<doc id="1942" url="https://en.wikipedia.org/wiki?curid=1942" title="Airline">
Airline

An airline is a company that provides air transport services for traveling passengers and freight. Airlines lease or own their aircraft with which to supply these services and may form partnerships or alliances with other airlines for mutual benefit. Generally, airline companies are recognized with an air operating certificate or license issued by a governmental aviation body.
Airlines vary from those with a single aircraft carrying mail or cargo, through full-service international airlines operating hundreds of aircraft. Airline services can be categorized as being intercontinental, domestic, regional, or international, and may be operated as scheduled services or charters.
History.
The first airlines.
DELAG, "Deutsche Luftschiffahrts-Aktiengesellschaft" was the world's first airline. It was founded on November 16, 1909 with government assistance, and operated airships manufactured by The Zeppelin Corporation. Its headquarters were in Frankfurt. The first fixed wing scheduled air service was started on January 1, 1914 from St. Petersburg, Florida to Tampa, Florida. The four oldest non-dirigible airlines that still exist are Netherlands' KLM, Colombia's Avianca, Australia's Qantas, Polish AeroTarg, and the Czech Republic's Czech Airlines. KLM first flew in May 1920, while Qantas (which stands for "Queensland and Northern Territory Aerial Services Limited") was founded in Queensland, Australia, in late 1920.
European airline industry.
Beginnings.
The earliest fixed wing airline in Europe was the Aircraft Transport and Travel, formed by George Holt Thomas in 1916. Using a fleet of former military Airco DH.4A biplanes that had been modified to carry two passengers in the fuselage, it operated relief flights between Folkestone and Ghent. On 15 July 1919, the company flew a proving flight across the English Channel, despite a lack of support from the British government. Flown by Lt. H Shaw in an Airco DH.9 between RAF Hendon and Paris - Le Bourget Airport, the flight took 2 hours and 30 minutes at £21 per passenger.
On 25 August 1919, the company used DH.16s to pioneer a regular service from Hounslow Heath Aerodrome to Le Bourget, the first regular international service in the world. The airline soon gained a reputation for reliability, despite problems with bad weather and began to attract European competition. In November 1919, it won the first British civil airmail contract. Six Royal Air Force Airco DH.9A aircraft were lent to the company, to operate the airmail service between Hawkinge and Cologne. In 1920, they were returned to the Royal Air Force.
Other British competitors were quick to follow - Handley Page Transport was established in 1919 and used the company's converted wartime Type O/400 bombers with a capacity for 19 passengers, to run a London-Paris passenger service.
The first French airline was Société des lignes Latécoère, later known as Aéropostale, which started its first service in late 1918 to Spain. The Société Générale des Transports Aériens was created in late 1919, by the Farman brothers and the Farman F.60 Goliath plane flew scheduled services from Toussus-le-Noble to Kenley, near Croydon, England. Another early French airline was the Compagnie des Messageries Aériennes, established in 1919 by Louis-Charles Breguet, offering a mail and freight service between Le Bourget Airport, Paris and Lesquin Airport, Lille.
The first German airline to use heavier than air aircraft was Deutsche Luft-Reederei established in 1917 which started operating in February 1919. In its first year, the D.L.R. operated regularly scheduled flights on routes with a combined length of nearly 1000 miles. By 1921 the D.L.R. network was more than 3000 km (1865 miles) long, and included destinations in the Netherlands, Scandinavia and the Baltic Republics. Another important German airline was Junkers Luftverkehr, which began operations in 1921. It was a division of the aircraft manufacturer Junkers, which became a separate company in 1924. It operated joint-venture airlines in Austria, Denmark, Estonia, Finland, Hungary, Latvia, Norway, Poland, Sweden and Switzerland.
The Dutch airline KLM made its first flight in 1920, and is the oldest continuously operating airline in the world. Established by aviator Albert Plesman, it was immediately awarded a "Royal" predicate from Queen Wilhelmina Its first flight was from Croydon Airport, London to Amsterdam, using a leased Aircraft Transport and Travel DH-16, and carrying two British journalists and a number of newspapers. In 1921, KLM started scheduled services.
In Finland, the charter establishing Aero O/Y (now Finnair) was signed in the city of Helsinki on September 12, 1923. Junkers F.13 D-335 became the first aircraft of the company, when Aero took delivery of it on March 14, 1924. The first flight was between Helsinki and Tallinn, capital of Estonia, and it took place on March 20, 1924, one week later.
In the Soviet Union, the Chief Administration of the Civil Air Fleet was established in 1921. One of its first acts was to help found Deutsch-Russische Luftverkehrs A.G. (Deruluft), a German-Russian joint venture to provide air transport from Russia to the West. Domestic air service began around the same time, when Dobrolyot started operations on 15 July 1923 between Moscow and Nizhni Novgorod. Since 1932 all operations had been carried under the name Aeroflot.
Early European airlines tend to favour comfort - the passenger cabins were often spacious with luxurious interiors - over speed and efficiency. The relatively basic navigational capabilities of pilots at the time also meant that delays due to the weather, especially during the winter in the south of England, were commonplace.
Rationalization.
By the early 1920s, small airlines were struggling to compete, and there was a movement towards increased rationalization and consolidation. In 1924, Imperial Airways was formed from the merger of Instone Air Line Company, British Marine Air Navigation, Daimler Airway and Handley Page Transport Co Ltd., to allow British airlines to compete with stiff competition from French and German airlines that were enjoying heavy government subsidies. The airline was a pioneer in surveying and opening up air routes across the world to serve far-flung parts of the British Empire and to enhance trade and integration.
The first new airliner ordered by Imperial Airways, was the Handley Page W8f "City of Washington", delivered on 3 November 1924. In the first year of operation the company carried 11,395 passengers and 212,380 letters. In April 1925, the film "The Lost World" became the first film to be screened for passengers on a scheduled airliner flight when it was shown on the London-Paris route.
Two French airlines also merged to form Air Union on 1 January 1923. This later merged with four other French airlines to become Air France, the country's flagship carrier to this day, on 7 October 1933.
Germany's Deutsche Luft Hansa was created in 1926 by merger of two airlines, one of them Junkers Luftverkehr. Luft Hansa, due to the Junkers heritage and unlike most other airlines at the time, became a major investor in airlines outside of Europe, providing capital to Varig and Avianca. German airliners built by Junkers, Dornier, and Fokker were among the most advanced in the world at the time.
Global expansion.
In 1926, Alan Cobham surveyed a flight route from the UK to Cape Town, South Africa, following this up with another proving flight to Melbourne, Australia. Other routes to British India and the Far East were also charted and demonstrated at this time. Regular services to Cairo and Basra began in 1927 and was extended to Karachi in 1929. The London-Australia service was inaugurated in 1932 with the Handley Page HP 42 airliners. Further services were opened up to Calcutta, Rangoon, Singapore, Brisbane and Hong Kong passengers departed London on 14 March 1936 following the establishment of a branch from Penang to Hong Kong.
Imperial's aircraft were small, most seating fewer than twenty passengers, and catered for the rich - only about 50,000 passengers used Imperial Airways in the 1930s. Most passengers on intercontinental routes or on services within and between British colonies were men doing colonial administration, business or research.
Like Imperial Airways, Air France and KLM's early growth depended heavily on the needs to service links with far-flung colonial possessions (North Africa and Indochina for the French and the East Indies for the Dutch). France began an air mail service to Morocco in 1919 that was bought out in 1927, renamed Aéropostale, and injected with capital to become a major international carrier. In 1933, Aéropostale went bankrupt, was nationalized and merged into Air France.
Although Germany lacked colonies, it also began expanding its services globally. In 1931, the airship Graf Zeppelin began offering regular scheduled passenger service between Germany and South America, usually every two weeks, which continued until 1937. In 1936, the airship Hindenburg entered passenger service and successfully crossed the Atlantic 36 times before crashing at Lakehurst, New Jersey on May 6, 1937.
From February 1934 until Germany started World War II in 1939 Deutsche Luft Hansa operated an airmail service from Stuttgart, Germany via Spain, the Canary Islands and West Africa to Natal in Brazil. This was the first time an airline flew across an ocean.
By the end of the 1930s Aeroflot had become the world's largest airline, employing more than 4,000 pilots and 60,000 other service personnel and operating around 3,000 aircraft (of which 75% were considered obsolete by its own standards). During the Soviet era Aeroflot was synonymous with Russian civil aviation, as it was the only air carrier. It became the first airline in the world to operate sustained regular jet services on 15 September 1956 with the Tupolev Tu-104.
EU airline deregulation.
Deregulation of the European Union airspace in the early 1990s has had substantial effect on the structure of the industry there. The shift towards 'budget' airlines on shorter routes has been significant. Airlines such as EasyJet and Ryanair have often grown at the expense of the traditional national airlines.
There has also been a trend for these national airlines themselves to be privatized such as has occurred for Aer Lingus and British Airways. Other national airlines, including Italy's Alitalia, have suffered - particularly with the rapid increase of oil prices in early 2008.
U.S. airline industry.
Early development.
Tony Jannus conducted the United States' first scheduled commercial airline flight on 1 January 1914 for the St. Petersburg-Tampa Airboat Line. The 23-minute flight traveled between St. Petersburg, Florida and Tampa, Florida, passing some above Tampa Bay in Jannus' Benoist XIV wood and muslin biplane flying boat. His passenger was a former mayor of St. Petersburg, who paid $400 for the privilege of sitting on a wooden bench in the open cockpit. The Airboat line operated for about four months, carrying more than 1,200 passengers who paid $5 each. Chalk's International Airlines began service between Miami and Bimini in the Bahamas in February 1919. Based in Ft. Lauderdale, Chalk's claimed to be the oldest continuously operating airline in the United States until its closure in 2008.
Following World War I, the United States found itself swamped with aviators. Many decided to take their war-surplus aircraft on barnstorming campaigns, performing aerobatic maneuvers to woo crowds. In 1918, the United States Postal Service won the financial backing of Congress to begin experimenting with air mail service, initially using Curtiss Jenny aircraft that had been procured by the United States Army Air Service. Private operators were the first to fly the mail but due to numerous accidents the US Army was tasked with mail delivery. During the Army's involvement they proved to be too unreliable and lost their air mail duties. By the mid-1920s, the Postal Service had developed its own air mail network, based on a transcontinental backbone between New York City and San Francisco. To supplant this service, they offered twelve contracts for spur routes to independent bidders. Some of the carriers that won these routes would, through time and mergers, evolve into Pan Am, Delta Air Lines, Braniff Airways, American Airlines, United Airlines (originally a division of Boeing), Trans World Airlines, Northwest Airlines, and Eastern Air Lines.
Service during the early 1920s was sporadic: most airlines at the time were focused on carrying bags of mail. In 1925, however, the Ford Motor Company bought out the Stout Aircraft Company and began construction of the all-metal Ford Trimotor, which became the first successful American airliner. With a 12-passenger capacity, the Trimotor made passenger service potentially profitable. Air service was seen as a supplement to rail service in the American transportation network.
At the same time, Juan Trippe began a crusade to create an air network that would link America to the world, and he achieved this goal through his airline, Pan American World Airways, with a fleet of flying boats that linked Los Angeles to Shanghai and Boston to London. Pan Am and Northwest Airways (which began flights to Canada in the 1920s) were the only U.S. airlines to go international before the 1940s.
With the introduction of the Boeing 247 and Douglas DC-3 in the 1930s, the U.S. airline industry was generally profitable, even during the Great Depression. This trend continued until the beginning of World War II.
Development since 1945.
As governments met to set the standards and scope for an emergent civil air industry toward the end of the war, the U.S. took a position of maximum operating freedom; U.S. airline companies were not as hard-hit as European and the few Asian ones had been. This preference for "open skies" operating regimes continues, with limitations, to this day.
World War II, like World War I, brought new life to the airline industry. Many airlines in the Allied countries were flush from lease contracts to the military, and foresaw a future explosive demand for civil air transport, for both passengers and cargo. They were eager to invest in the newly emerging flagships of air travel such as the Boeing Stratocruiser, Lockheed Constellation, and Douglas DC-6. Most of these new aircraft were based on American bombers such as the B-29, which had spearheaded research into new technologies such as pressurization. Most offered increased efficiency from both added speed and greater payload.
In the 1950s, the De Havilland Comet, Boeing 707, Douglas DC-8, and Sud Aviation Caravelle became the first flagships of the Jet Age in the West, while the Eastern bloc had Tupolev Tu-104 and Tupolev Tu-124 in the fleets of state-owned carriers such as Czechoslovak ČSA, Soviet Aeroflot and East-German Interflug. The Vickers Viscount and Lockheed L-188 Electra inaugurated turboprop transport.
The next big boost for the airlines would come in the 1970s, when the Boeing 747, McDonnell Douglas DC-10, and Lockheed L-1011 inaugurated widebody ("jumbo jet") service, which is still the standard in international travel. The Tupolev Tu-144 and its Western counterpart, Concorde, made supersonic travel a reality. Concorde first flew in 1969 and operated through 2003. In 1972, Airbus began producing Europe's most commercially successful line of airliners to date. The added efficiencies for these aircraft were often not in speed, but in passenger capacity, payload, and range. Airbus also features modern electronic cockpits that were common across their aircraft to enable pilots to fly multiple models with minimal cross-training.
US airline deregulation.
The 1978 U.S. airline industry deregulation lowered federally controlled barriers for new airlines just as a downturn in the nation's economy occurred. New start-ups entered during the downturn, during which time they found aircraft and funding, contracted hangar and maintenance services, trained new employees, and recruited laid off staff from other airlines.
Major airlines dominated their routes through aggressive pricing and additional capacity offerings, often swamping new start-ups. In the place of high barriers to entry imposed by regulation, the major airlines implemented an equally high barrier called loss leader pricing. In this strategy an already established and dominant airline stomps out its competition by lowering airfares on specific routes, below the cost of operating on it, choking out any chance a start-up airline may have. The industry side effect is an overall drop in revenue and service quality. Since deregulation in 1978 the average domestic ticket price has dropped by 40%. So has airline employee pay. By incurring massive losses, the airlines of the USA now rely upon a scourge of cyclical Chapter 11 bankruptcy proceedings to continue doing business. America West Airlines (which has since merged with US Airways) remained a significant survivor from this new entrant era, as dozens, even hundreds, have gone under.
In many ways, the biggest winner in the deregulated environment was the air passenger. Although not exclusively attributable to deregulation, indeed the U.S. witnessed an explosive growth in demand for air travel. Many millions who had never or rarely flown before became regular fliers, even joining frequent flyer loyalty programs and receiving free flights and other benefits from their flying. New services and higher frequencies meant that business fliers could fly to another city, do business, and return the same day, from almost any point in the country. Air travel's advantages put long distance intercity railroad travel and bus lines under pressure, with most of the latter having withered away, whilst the former is still protected under nationalization through the continuing existence of Amtrak.
By the 1980s, almost half of the total flying in the world took place in the U.S., and today the domestic industry operates over 10,000 daily departures nationwide.
Toward the end of the century, a new style of low cost airline emerged, offering a no-frills product at a lower price. Southwest Airlines, JetBlue, AirTran Airways, Skybus Airlines and other low-cost carriers began to represent a serious challenge to the so-called "legacy airlines", as did their low-cost counterparts in many other countries. Their commercial viability represented a serious competitive threat to the legacy carriers. However, of these, ATA and Skybus have since ceased operations.
Increasingly since 1978, US airlines have been reincorporated and spun off by newly created and internally led management companies, and thus becoming nothing more than operating units and subsidiaries with limited financially decisive control. Among some of these holding companies and parent companies which are relatively well known, are the UAL Corporation, along with the AMR Corporation, among a long list of airline holding companies sometime recognized worldwide. Less recognized are the private equity firms which often seize managerial, financial, and board of directors control of distressed airline companies by temporarily investing large sums of capital in air carriers, to rescheme an airlines assets into a profitable organization or liquidating an air carrier of their profitable and worthwhile routes and business operations.
Thus the last 50 years of the airline industry have varied from reasonably profitable, to devastatingly depressed. As the first major market to deregulate the industry in 1978, U.S. airlines have experienced more turbulence than almost any other country or region. In fact, no U.S. legacy carrier survived bankruptcy-free. Amongst the outspoken critics of deregulation, former CEO of American Airlines, Robert Crandall has publicly stated:
"Chapter 11 bankruptcy protection filing shows airline industry deregulation was a mistake."
The airline industry bailout.
Congress passed the Air Transportation Safety and System Stabilization Act (P.L. 107-42) in response to a severe liquidity crisis facing the already-troubled airline industry in the aftermath of the September 11th terrorist attacks. Through the ATSB Congress sought to provide cash infusions to carriers for both the cost of the four-day federal shutdown of the airlines and the incremental losses incurred through December 31, 2001 as a result of the terrorist attacks. This resulted in the first government bailout of the 21st century. Between 2000 and 2005 US airlines lost $30 billion with wage cuts of over $15 billion and 100,000 employees laid off.
In recognition of the essential national economic role of a healthy aviation system, Congress authorized partial compensation of up to $5 billion in cash subject to review by the Department of Transportation and up to $10 billion in loan guarantees subject to review by a newly created Air Transportation Stabilization Board (ATSB). The applications to DOT for reimbursements were subjected to rigorous multi-year reviews not only by DOT program personnel but also by the Government Accountability Office and the DOT Inspector General.
Ultimately, the federal government provided $4.6 billion in one-time, subject-to-income-tax cash payments to 427 U.S. air carriers, with no provision for repayment, essentially a gift from the taxpayers. (Passenger carriers operating scheduled service received approximately $4 billion, subject to tax.) In addition, the ATSB approved loan guarantees to six airlines totaling approximately $1.6 billion. Data from the US Treasury Department show that the government recouped the $1.6 billion and a profit of $339 million from the fees, interest and purchase of discounted airline stock associated with loan guarantees.
Asian airline industry.
Although Philippine Airlines (PAL) was officially founded on February 26, 1941, its license to operate as an airliner was derived from merged Philippine Aerial Taxi Company (PATCO) established by mining magnate Emmanuel N. Bachrach on December 3, 1930, making it Asia's oldest scheduled carrier still in operation. Commercial air service commenced three weeks later from Manila to Baguio, making it Asia's first airline route. Bachrach's death in 1937 paved the way for its eventual merger with Philippine Airlines in March 1941 and made it Asia's oldest airline. It is also the oldest airline in Asia still operating under its current name. Bachrach's majority share in PATCO was bought by beer magnate Andres R. Soriano in 1939 upon the advice of General Douglas MacArthur and later merged with newly formed Philippine Airlines with PAL as the surviving entity. Soriano has controlling interest in both airlines before the merger. PAL restarted service on March 15, 1941 with a single Beech Model 18 NPC-54 aircraft, which started its daily services between Manila (from Nielson Field) and Baguio, later to expand with larger aircraft such as the DC-3 and Vickers Viscount.
India was also one of the first countries to embrace civil aviation. One of the first West Asian airline companies was Air India, which had its beginning as Tata Airlines in 1932, a division of Tata Sons Ltd. (now Tata Group). The airline was founded by India's leading industrialist, JRD Tata. On October 15, 1932, J. R. D. Tata himself flew a single engined De Havilland Puss Moth carrying air mail (postal mail of Imperial Airways) from Karachi to Bombay via Ahmedabad. The aircraft continued to Madras via Bellary piloted by Royal Air Force pilot Nevill Vintcent. Tata Airlines was also one of the world's first major airlines which began its operations without any support from the Government.
With the outbreak of World War II, the airline presence in Asia came to a relative halt, with many new flag carriers donating their aircraft for military aid and other uses. Following the end of the war in 1945, regular commercial service was restored in India and Tata Airlines became a public limited company on July 29, 1946 under the name Air India. After the independence of India, 49% of the airline was acquired by the Government of India. In return, the airline was granted status to operate international services from India as the designated flag carrier under the name Air India International.
On July 31, 1946, a chartered Philippine Airlines (PAL) DC-4 ferried 40 American servicemen to Oakland, California, from Nielson Airport in Makati City with stops in Guam, Wake Island, Johnston Atoll and Honolulu, Hawaii, making PAL the first Asian airline to cross the Pacific Ocean. A regular service between Manila and San Francisco was started in December. It was during this year that the airline was designated as the flag carrier of Philippines.
During the era of decolonization, newly born Asian countries started to embrace air transport. Among the first Asian carriers during the era were Cathay Pacific of Hong Kong (founded in September 1946 ), Orient Airways (later Pakistan International Airlines; founded in October 1946), Air Ceylon (later SriLankan Airlines; founded in 1947), Malayan Airways Limited in 1947 (later Singapore and Malaysia Airlines), El Al in Israel in 1948, Garuda Indonesia in 1948, Japan Airlines in 1951, Thai Airways International in 1960, and Korean National Airlines in 1947.
Latin American airline industry.
Among the first countries to have regular airlines in Latin America were Bolivia with Lloyd Aéreo Boliviano, Cuba with Cubana de Aviación, Colombia with Avianca, Argentina with Aerolineas Argentinas, Chile with LAN Chile (today LAN Airlines), Brazil with Varig, Dominican Republic with Dominicana de Aviación, Mexico with Mexicana de Aviación, Trinidad and Tobago with BWIA West Indies Airways (today Caribbean Airlines), Venezuela with Aeropostal, and TACA based in El Salvador and representing several airlines of Central America (Costa Rica, Guatemala, Honduras and Nicaragua). All the previous airlines started regular operations well before World War II.
The air travel market has evolved rapidly over recent years in Latin America. Some industry estimates indicate that over 2,000 new aircraft will begin service over the next five years in this region.
These airlines serve domestic flights within their countries, as well as connections within Latin America and also overseas flights to North America, Europe, Australia, and Asia.
Only three airlines: Avianca, LAN, and TAM Airlines have international subsidiaries and cover many destinations within the Americas as well as major hubs in other continents. LAN with Chile as the central operation along with Peru, Ecuador, Colombia and Argentina and some operations in the Dominican Republic. The recently formed AviancaTACA group has control of Avianca Brazil, VIP Ecuador and a strategic alliance with AeroGal. And TAM with its Mercosur base in Asuncion, Paraguay. As of 2010, talks of uniting LAN and TAM have strongly developed to create a joint airline named LATAM.
Regulatory considerations.
National.
Many countries have national airlines that the government owns and operates. Fully private airlines are subject to a great deal of government regulation for economic, political, and safety concerns. For instance, governments often intervene to halt airline labor actions to protect the free flow of people, communications, and goods between different regions without compromising safety.
The United States, Australia, and to a lesser extent Brazil, Mexico, India, the United Kingdom, and Japan have "deregulated" their airlines. In the past, these governments dictated airfares, route networks, and other operational requirements for each airline. Since deregulation, airlines have been largely free to negotiate their own operating arrangements with different airports, enter and exit routes easily, and to levy airfares and supply flights according to market demand.
The entry barriers for new airlines are lower in a deregulated market, and so the U.S. has seen hundreds of airlines start up (sometimes for only a brief operating period). This has produced far greater competition than before deregulation in most markets. The added competition, together with pricing freedom, means that new entrants often take market share with highly reduced rates that, to a limited degree, full service airlines must match. This is a major constraint on profitability for established carriers, which tend to have a higher cost base.
As a result, profitability in a deregulated market is uneven for most airlines. These forces have caused some major airlines to go out of business, in addition to most of the poorly established new entrants.
International.
Groups such as the International Civil Aviation Organization establish worldwide standards for safety and other vital concerns. Most international air traffic is regulated by bilateral agreements between countries, which designate specific carriers to operate on specific routes. The model of such an agreement was the Bermuda Agreement between the US and UK following World War II, which designated airports to be used for transatlantic flights and gave each government the authority to nominate carriers to operate routes.
Bilateral agreements are based on the "freedoms of the air", a group of generalized traffic rights ranging from the freedom to overfly a country to the freedom to provide domestic flights within a country (a very rarely granted right known as cabotage). Most agreements permit airlines to fly from their home country to designated airports in the other country: some also extend the freedom to provide continuing service to a third country, or to another destination in the other country while carrying passengers from overseas.
In the 1990s, "open skies" agreements became more common. These agreements take many of these regulatory powers from state governments and open up international routes to further competition. Open skies agreements have met some criticism, particularly within the European Union, whose airlines would be at a comparative disadvantage with the United States' because of cabotage restrictions.
Economic considerations.
Historically, air travel has survived largely through state support, whether in the form of equity or subsidies. The airline industry as a whole has made a cumulative loss during its 100-year history, once the costs include subsidies for aircraft development and airport construction.
One argument is that positive externalities, such as higher growth due to global mobility, outweigh the microeconomic losses and justify continuing government intervention. A historically high level of government intervention in the airline industry can be seen as part of a wider political consensus on strategic forms of transport, such as highways and railways, both of which receive public funding in most parts of the world.
Although many countries continue to operate state-owned or parastatal airlines, many large airlines today are privately owned and are therefore governed by microeconomic principles to maximize shareholder profit.
Top airline groups by revenue.
for 2010, source : Airline Business August 2011, Flightglobal Data Research
Ticket revenue.
Airlines assign prices to their services in an attempt to maximize profitability. The pricing of airline tickets has become increasingly complicated over the years and is now largely determined by computerized yield management systems.
Because of the complications in scheduling flights and maintaining profitability, airlines have many loopholes that can be used by the knowledgeable traveler. Many of these airfare secrets are becoming more and more known to the general public, so airlines are forced to make constant adjustments.
Most airlines use differentiated pricing, a form of price discrimination, to sell air services at varying prices simultaneously to different segments. Factors influencing the price include the days remaining until departure, the booked load factor, the forecast of total demand by price point, competitive pricing in force, and variations by day of week of departure and by time of day. Carriers often accomplish this by dividing each cabin of the aircraft (first, business and economy) into a number of travel classes for pricing purposes.
A complicating factor is that of origin-destination control ("O&D control"). Someone purchasing a ticket from Melbourne to Sydney (as an example) for A$200 is competing with someone else who wants to fly Melbourne to Los Angeles through Sydney on the same flight, and who is willing to pay A$1400. Should the airline prefer the $1400 passenger, or the $200 passenger plus a possible Sydney-Los Angeles passenger willing to pay $1300? Airlines have to make hundreds of thousands of similar pricing decisions daily.
The advent of advanced computerized reservations systems in the late 1970s, most notably Sabre, allowed airlines to easily perform cost-benefit analyses on different pricing structures, leading to almost perfect price discrimination in some cases (that is, filling each seat on an aircraft at the highest price that can be charged without driving the consumer elsewhere).
The intense nature of airfare pricing has led to the term "fare war" to describe efforts by airlines to undercut other airlines on competitive routes. Through computers, new airfares can be published quickly and efficiently to the airlines' sales channels. For this purpose the airlines use the Airline Tariff Publishing Company (ATPCO), who distribute latest fares for more than 500 airlines to Computer Reservation Systems across the world.
The extent of these pricing phenomena is strongest in "legacy" carriers. In contrast, low fare carriers usually offer pre-announced and simplified price structure, and sometimes quote prices for each leg of a trip separately.
Computers also allow airlines to predict, with some accuracy, how many passengers will actually fly after making a reservation to fly. This allows airlines to overbook their flights enough to fill the aircraft while accounting for "no-shows," but not enough (in most cases) to force paying passengers off the aircraft for lack of seats, stimulative pricing for low demand flights coupled with overbooking on high demand flights can help reduce this figure. This is especially crucial during tough economic times as airlines undertake massive cuts to ticket prices to retain demand.
Operating costs.
Full-service airlines have a high level of fixed and operating costs to establish and maintain air services: labor, fuel, airplanes, engines, spares and parts, IT services and networks, airport equipment, airport handling services, sales distribution, catering, training, aviation insurance and other costs. Thus all but a small percentage of the income from ticket sales is paid out to a wide variety of external providers or internal cost centers.
Moreover, the industry is structured so that airlines often act as tax collectors. Airline fuel is untaxed because of a series of treaties existing between countries. Ticket prices include a number of fees, taxes and surcharges beyond the control of airlines. Airlines are also responsible for enforcing government regulations. If airlines carry passengers without proper documentation on an international flight, they are responsible for returning them back to the original country.
Analysis of the 1992–1996 period shows that every player in the air transport chain is far more profitable than the airlines, who collect and pass through fees and revenues to them from ticket sales. While airlines as a whole earned 6% return on capital employed (2-3.5% less than the cost of capital), airports earned 10%, catering companies 10-13%, handling companies 11-14%, aircraft lessors 15%, aircraft manufacturers 16%, and global distribution companies more than 30%. (Source: Spinetta, 2000, quoted in Doganis, 2002)
The widespread entrance of a new breed of low cost airlines beginning at the turn of the century has accelerated the demand that full service carriers control costs. Many of these low cost companies emulate Southwest Airlines in various respects, and like Southwest, they can eke out a consistent profit throughout all phases of the business cycle.
As a result, a shakeout of airlines is occurring in the U.S. and elsewhere. American Airlines, United Airlines, Continental Airlines (twice), US Airways (twice), Delta Air Lines, and Northwest Airlines have all declared Chapter 11 bankruptcy. Some argue that it would be far better for the industry as a whole if a wave of actual closures were to reduce the number of "undead" airlines competing with healthy airlines while being artificially protected from creditors via bankruptcy law. On the other hand, some have pointed out that the reduction in capacity would be short lived given that there would be large quantities of relatively new aircraft that bankruptcies would want to get rid of and would re-enter the market either as increased fleets for the survivors or the basis of cheap planes for new startups.
Where an airline has established an engineering base at an airport, then there may be considerable economic advantages in using that same airport as a preferred focus (or "hub") for its scheduled flights.
Assets and financing.
Airline financing is quite complex, since airlines are highly leveraged operations. Not only must they purchase (or lease) new airliner bodies and engines regularly, they must make major long-term fleet decisions with the goal of meeting the demands of their markets while producing a fleet that is relatively economical to operate and maintain. Compare Southwest Airlines and their reliance on a single airplane type (the Boeing 737 and derivatives), with the now defunct Eastern Air Lines which operated 17 different aircraft types, each with varying pilot, engine, maintenance, and support needs.
A second financial issue is that of hedging oil and fuel purchases, which are usually second only to labor in its relative cost to the company. However, with the current high fuel prices it has become the largest cost to an airline. Legacy airlines, compared with new entrants, have been hit harder by rising fuel prices partly due to the running of older, less fuel efficient aircraft. While hedging instruments can be expensive, they can easily pay for themselves many times over in periods of increasing fuel costs, such as in the 2000–2005 period.
In view of the congestion apparent at many international airports, the ownership of slots at certain airports (the right to take-off or land an aircraft at a particular time of day or night) has become a significant tradable asset for many airlines. Clearly take-off slots at popular times of the day can be critical in attracting the more profitable business traveler to a given airline's flight and in establishing a competitive advantage against a competing airline.
If a particular city has two or more airports, market forces will tend to attract the less profitable routes, or those on which competition is weakest, to the less congested airport, where slots are likely to be more available and therefore cheaper. For example, Reagan National Airport attracts profitable routes due partly to its congestion, leaving less-profitable routes to Baltimore-Washington International Airport and Dulles International Airport.
Other factors, such as surface transport facilities and onward connections, will also affect the relative appeal of different airports and some long distance flights may need to operate from the one with the longest runway. For example, LaGuardia Airport is the preferred airport for most of Manhattan due to its proximity, while long-distance routes must use John F. Kennedy International Airport's longer runways.
Airline partnerships.
Codesharing is the most common type of airline partnership; it involves one airline selling tickets for another airline's flights under its own airline code. An early example of this was Japan Airlines' (JAL) codesharing partnership with Aeroflot in the 1960s on Tokyo–Moscow flights; Aeroflot operated the flights using Aeroflot aircraft, but JAL sold tickets for the flights as if they were JAL flights. This practice allows airlines to expand their operations, at least on paper, into parts of the world where they cannot afford to establish bases or purchase aircraft. Another example was the Austrian–Sabena partnership on the Vienna–Brussels–New York/JFK route during the late '60s, using a Sabena Boeing 707 with Austrian livery.
Since airline reservation requests are often made by city-pair (such as "show me flights from Chicago to Düsseldorf"), an airline that can codeshare with another airline for a variety of routes might be able to be listed as indeed offering a Chicago–Düsseldorf flight. The passenger is advised however, that airline no. 1 operates the flight from say Chicago to Amsterdam, and airline no. 2 operates the continuing flight (on a different airplane, sometimes from another terminal) to Düsseldorf. Thus the primary rationale for code sharing is to expand one's service offerings in city-pair terms to increase sales.
A more recent development is the airline alliance, which became prevalent in the late 1990s. These alliances can act as virtual mergers to get around government restrictions. Alliances of airlines such as Star Alliance, Oneworld, and SkyTeam coordinate their passenger service programs (such as lounges and frequent-flyer programs), offer special interline tickets, and often engage in extensive codesharing (sometimes systemwide). These are increasingly integrated business combinations—sometimes including cross-equity arrangements—in which products, service standards, schedules, and airport facilities are standardized and combined for higher efficiency. One of the first airlines to start an alliance with another airline was KLM, who partnered with Northwest Airlines. Both airlines later entered the SkyTeam alliance after the fusion of KLM and Air France in 2004.
Often the companies combine IT operations, or purchase fuel and aircraft as a bloc to achieve higher bargaining power. However, the alliances have been most successful at purchasing invisible supplies and services, such as fuel. Airlines usually prefer to purchase items visible to their passengers to differentiate themselves from local competitors. If an airline's main domestic competitor flies Boeing airliners, then the airline may prefer to use Airbus aircraft regardless of what the rest of the alliance chooses.
Fuel hedging.
Southwest is credited with maintaining strong business profits between 1999 and the early 2000s due to its fuel hedging policy. Looking at the annual reports, many other airlines are replicating Southwest's hedging policy to control their fuel costs.
Environmental impacts.
Aircraft engines emit noise pollution, gases and particulate emissions, and contribute to global dimming.
Growth of the industry in recent years raised a number of ecological questions.
Domestic air transport grew in China at 15.5 percent annually from 2001 to 2006. The rate of air travel globally increased at 3.7 percent per year over the same time. In the EU greenhouse gas emissions from aviation increased by 87% between 1990 and 2006. However it must be compared with the flights increase, only in UK, between 1990 and 2006 terminal passengers increased from 100 000 thousands to 250 000 thousands., according to AEA reports every year, 750 million passengers travel by European airlines, which also share 40% of merchandise value in and out of Europe. Without even pressure from "green activists", targeting lower ticket prices, generally, airlines do what is possible to cut the fuel consumption (and gas emissions connected therewith). Further, according to some reports, it can be concluded that the last piston-powered aircraft were as fuel-efficient as the average jet in 2005.
Despite continuing efficiency improvements from the major aircraft manufacturers, the expanding demand for global air travel has resulted in growing greenhouse gas (GHG) emissions. Currently, the aviation sector, including US domestic and global international travel, make approximately 1.6 percent of global anthropogenic GHG emissions per annum. North America accounts for nearly 40 percent of the world's GHG emissions from aviation fuel use.
CO2 emissions from the jet fuel burned per passenger on an average airline flight is about 353 kilograms (776 pounds). Loss of natural habitat potential associated with the jet fuel burned per passenger on a airline flight is estimated to be 250 square meters (2700 square feet).
In the context of climate change and peak oil, there is a debate about possible taxation of air travel and the inclusion of aviation in an emissions trading scheme, with a view to ensuring that the total external costs of aviation are taken into account.
The airline industry is responsible for about 11 percent of greenhouse gases emitted by the U.S. transportation sector. Boeing estimates that biofuels could reduce flight-related greenhouse-gas emissions by 60 to 80 percent. The solution would be blending algae fuels with existing jet fuel:
There are projects on electric aircraft, and some of them are fully operational as of 2013.
Call signs.
Each operator of a scheduled or charter flight uses an airline call sign when communicating with airports or air traffic control centres. Most of these call-signs are derived from the airline's trade name, but for reasons of history, marketing, or the need to reduce ambiguity in spoken English (so that pilots do not mistakenly make navigational decisions based on instructions issued to a different aircraft), some airlines and air forces use call-signs less obviously connected with their trading name. For example, British Airways uses a "Speedbird" call-sign, named after the logo of its predecessor, BOAC, while SkyEurope used "Relax".
Airline personnel.
The various types of airline personnel include:
Flight operations personnel including flight safety personnel.
Airlines follow a corporate structure where each broad area of operations (such as maintenance, flight operations(including flight safety),
and passenger service) is supervised by a vice president. Larger airlines often appoint vice presidents to oversee each of the
airline's hubs as well. Airlines employ lawyers to deal with regulatory procedures and other administrative tasks.
Industry trends.
The pattern of ownership has been privatized in the recent years, that is, the ownership has gradually changed from governments to private and individual sectors or organizations. This occurs as regulators permit greater freedom and non-government ownership, in steps that are usually decades apart. This pattern is not seen for all airlines in all regions. 
The overall trend of demand has been consistently increasing. In the 1950s and 1960s, annual growth rates of 15% or more were common. Annual growth of 5-6% persisted through the 1980s and 1990s. Growth rates are not consistent in all regions, but countries with a de-regulated airline industry have more competition and greater pricing freedom. This results in lower fares and sometimes dramatic spurts in traffic growth. The U.S., Australia, Canada, Japan, Brazil, India and other markets exhibit this trend. The industry has been observed to be cyclical in its financial performance. Four or five years of poor earnings precede five or six years of improvement. But profitability even in the good years is generally low, in the range of 2-3% net profit after interest and tax. In times of profit, airlines lease new generations of airplanes and upgrade services in response to higher demand. Since 1980, the industry has not earned back the cost of capital during the best of times. Conversely, in bad times losses can be dramatically worse. Warren Buffett once said that despite all the money that has been invested in all airlines, the net profit is less than zero. He believes it is one of the hardest businesses to manage.
As in many mature industries, consolidation is a trend. Airline groupings may consist of limited bilateral partnerships, long-term, multi-faceted alliances between carriers, equity arrangements, mergers, or takeovers. Since governments often restrict ownership and merger between companies in different countries, most consolidation takes place within a country. In the U.S., over 200 airlines have merged, been taken over, or gone out of business since deregulation in 1978. Many international airline managers are lobbying their governments to permit greater consolidation to achieve higher economy and efficiency.

</doc>
<doc id="1943" url="https://en.wikipedia.org/wiki?curid=1943" title="Australian Democrats">
Australian Democrats

The Australian Democrats was a centrist political party in Australia with a social-liberal ideology. The party was formed in 1977, a merger of the Australia Party and the New Liberal Movement, with former Liberal minister Don Chipp as its high-profile leader. Though never achieving a seat in the House of Representatives, the party had considerable influence in the Senate for the following thirty years. Its representation in the Parliament of Australia ended on 30 June 2008, after loss of its four remaining Senate seats at the 2007 general election. , the organisation had disintegrated and control was contested by two factions associated with two former parliamentarians. The party was deregistered by the Australian Electoral Commission on 16 April 2015 due to the party's failure to demonstrate requisite 500 members to maintain registration.
Even before its deregistration and since it became extinct as a parliamentary party anywhere in Australia, the party saw many of its prominent members including former federal party leader Andrew Bartlett and former NSW MLC Arthur Chesterfield-Evans defect to the Greens.
Overview.
The party was founded on principles of honesty, tolerance, compassion and direct democracy through postal ballots of all members, so that "there should be no hierarchical structure ... by which a carefully engineered elite could make decisions for the members." From the outset, members' participation was fiercely protected in national and divisional constitutions prescribing internal elections, regular meeting protocols, annual conferences—and monthly journals for open discussion and balloting. Dispute resolution procedures were established, with final recourse to a party ombudsman and membership ballot.
Policies determined by the unique participatory method promoted environmental awareness and sustainability, opposition to the primacy of economic rationalism (Australian neoliberalism), preventative approaches to human health and welfare, animal rights, rejection of nuclear technology and weapons.
The Australian Democrats were the first representatives of green politics at the federal level in Australia. They played a key role in the "cause célèbre" of the Franklin River Dam.
The party's centrist role made it subject to criticism from both the right and left of the political spectrum. In particular, Chipp's former conservative affiliation was frequently recalled by opponents on the left. This problem was to torment later leaders and strategists who, by 1991, were proclaiming "the electoral objective" as a higher priority than the rigorous participatory democracy espoused by the party's founders.
Over three decades, the Australian Democrats achieved representation in the legislatures of the ACT, South Australia, New South Wales, Western Australia and Tasmania as well as Senate seats in all six states. However, at the 2004 and 2007 federal elections, all seven of its Senate seats were lost. The last remaining State parliamentarian, David Winderlich, left the party and was defeated as an independent in 2010.
History.
1977–79.
On the evening of 29 April 1977, Don Chipp addressed an overflowing Perth Town Hall meeting which unanimously passed a resolution to form a Centre-Line Party, which Chipp was invited to lead—but he firmly declined to reverse his avowed decision to quit politics, having resigned from the Liberal Party and been offered a lucrative position as a radio public affairs commentator. The Centre-Line Party was the provisional title of the Australian Democrats party. The occasion was a meeting at the Perth Town Hall to which Don Chipp had been invited in the hope that he would accept the position of leader of the new party, which would be an amalgamation of the Australia Party and the New Liberal Movement. On that occasion, Chipp declined to commit himself but did so at a corresponding public meeting in Melbourne on 9 May 1977. Chipp received a standing ovation from over 3,000 people, including former Prime Minister John Gorton, and decided to commit himself to leading the new party which was already being constructed by a national steering committee. The new party was eventually renamed the Australian Democrats by a ballot of its membership. "Fifty-six suggestions produced by members were listed on the ballot paper, including Uniting Australia Party, Australian Centre Line Party, Dinkum Democrats, Practical Idealists of Australia and People for Sanity Party!! After the ballot, the suggestion of the Steering Committee, 'Australian Democrats', was overwhelmingly accepted." The name "Australian Democrats" was already in informal currency before this decision.
The first Australian Democrats (AD) federal parliamentarian was Senator Janine Haines who filled Steele Hall's casual Senate vacancy for South Australia in 1977. Surprisingly, she was not a candidate when the party contested the 1977 federal elections after Don Chipp had agreed to be leader and figurehead. Members and candidates were not lacking in electoral experience, since the Australia Party had been contesting all federal elections since 1969 and the Liberal Movement, in 1974 and 1975. The party's broad aim was to achieve a balance of power in one or more parliaments and to exercise it responsibly in line with policies determined by membership.
The grassroot support attracted by Chipp's leadership was measurable at the party's first electoral test at the 1977 federal election on 10 December, when 9.38 per cent of the total Lower House vote was polled and 11.13 per cent of the Senate vote. At that time, with five Senate seats being contested in each state, the required quota was a daunting 16.66 per cent. However, the first 6-year-term seats were won by Don Chipp (Vic) and Colin Mason (NSW).
1980–82.
The Australian Democrats' first national conference, on 16–17 February 1980, was opened by the distinguished nuclear physicist and former governor of South Australia, Sir Mark Oliphant, who said: "I was privileged to be in the chair at the public meeting in Melbourne when on Chip announced formation of a new party, dedicated to preserve what freedoms we still retain, and to increase them. A party in which dictatorship from the top was replaced by consensus. A party not ordered about by big business and the rich, or by union bosses. A party where a man could retain freedom of conscience and not thereby be faced with expulsion. A party to which the intelligent individual could belong without having to subscribe to a dogmatic creed. In other words, a democratic party." 
At a Melbourne media conference on 19 September 1980, in the midst of the 1980 election campaign, Chipp described his party's aim as to "keep the bastards honest"—the "bastards" being the major parties and/or politicians in general. This became a long-lived slogan for the Democrats.
At the October 1980 election, the Democrats polled 9.25 per cent of the Senate vote, electing Janine Haines (SA) and two new senators Michael Macklin (Qld) and John Siddons (Vic), bringing the party's strength to five Senate seats from 1 July 1981 .
A by-election in the South Australian state seat of Mitcham (now Waite) saw Heather Southcott retain the seat for the Democrats in 1982. Since 1955 it had been held by conservative lawyer Robin Millhouse whose New Liberal Movement merged into the Democrats in 1977, and who was resigning to take up a senior judicial appointment. Southcott was defeated later that year at the 1982 state election. Mitcham was the only single-member lower-house seat anywhere in Australia to be won by the Democrats.
1986–90.
Don Chipp resigned from the Senate on 18 August 1986, being succeeded as party leader by Janine Haines and replaced as a senator for Victoria by Janet Powell.
At the 1987 election following a double dissolution, the reduced quota of 7.7% necessary to win a seat assisted the election of three new senators. 6-year terms were won by Paul McLean (NSW) and incumbents Janine Haines (South Australia) and Janet Powell (Victoria). In South Australia, a second senator, John Coulter, was elected for a 3-year term, as were incumbent Michael Macklin (Queensland) and Jean Jenkins (Western Australia).
1990–91.
1990 saw the voluntary departure from the Senate of Janine Haines (a step with which not all Democrats agreed) and the failure of her strategic goal of winning the House of Representatives seat of Kingston.
The casual vacancy was filled by Meg Lees several months before the election of Cheryl Kernot in place of retired deputy leader Michael Macklin. The ambitious Kernot immediately contested the party's national parliamentary deputy leadership. Being unemployed at the time, she requested and obtained party funds to pay for her travel to address members in all seven divisions. In the event, Victorian Janet Powell was elected as leader and John Coulter was chosen as deputy leader.
Despite the loss of Haines and the WA Senate seat (through an inconsistent national preference agreement with the ALP), the 1990 federal election heralded something of a rebirth for the party, with a dramatic rise in primary vote. This was at the same time as an economic recession was building, and events such as the Gulf War in Kuwait were beginning to shepherd issues of globalisation and transnational trade on to national government agendas.
Virtually alone on the Australian political landscape, Janet Powell consistently attacked both the government and opposition which had closed ranks in support of the Gulf War. Whereas the House of Representatives was thus able to avoid any debate about the war and Australia's participation, the Democrats took full advantage of the opportunity to move for a debate in the Senate.
Possibly because of the party's opposition to the Gulf War, there was mass-media antipathy and negative publicity which some construed as poor media performance by Janet Powell, the party's standing having stalled at about 10%. Before 12 months of her leadership had passed, the South Australian and Queensland divisions were circulating the party's first-ever petition to criticise and oust the parliamentary leader. The explicit grounds related to Powell's alleged responsibility for poor AD ratings in Gallup and other media surveys of potential voting support. When this charge was deemed insufficient, interested party officers and senators reinforced it with negative media 'leaks' concerning her openly established relationship with Sid Spindler and exposure of administrative failings resulting in excessive overtime to a staff member. With National Executive blessing, the party room pre-empted the ballot by replacing the leader with deputy John Coulter. In the process, severe internal divisions were generated. One major collateral casualty was the party whip Paul McLean who resigned and quit the Senate in disgust at what he perceived as in-fighting between close friends. The casual NSW vacancy created by his resignation was filled by Karin Sowada. Powell duly left the party, along with many leading figures of the Victorian branch of the party, and unsuccessfully stood as an Independent candidate when her term expired. In later years, she campaigned for the Australian Greens.
Electoral fortunes.
Because of their numbers on the cross benches during the Hawke and Keating governments, the Democrats were sometimes regarded as exercising a balance of power—which attracted electoral support from a significant sector of the electorate which had been alienated by both Labor and Coalition policies and practices. The party's parliamentary influence was weakened in 1996 after the Howard Government was elected, and a Labor senator, Mal Colston, resigned from the Labor Party. Since the Democrats now shared the parliamentary balance of power with two Independent senators, the Coalition government was able on occasion to pass legislation by negotiating with Colston and Brian Harradine. Following the 1998 election the Australian Democrats again held the balance of power, until the Coalition gained a Senate majority at the 2004 election.
The party's integrity as a neutral third party suffered a serious blow from the resignation and defection of leader Cheryl Kernot in October 1997, with revelations of her sexual relationship with Gareth Evans and her aspirations to a ministerial position in a Labor government.
Under Lees' leadership, in the 1998 federal election, the Democrats' candidate John Schumann came within 2 per cent of taking Liberal Foreign Minister Alexander Downer's seat of Mayo in the Adelaide Hills under Australia's preferential voting system. The party's representation increased to nine senators.
Internal conflict and leadership tensions from 2000 to 2002, blamed on the party's support for the Government's Goods and Services Tax (GST), was damaging to the Democrats. Opposed by the Labor Party, the Australian Greens and independent Senator Harradine, the GST required Democrat support to pass. In an election fought on tax, the Democrats publicly stated that they liked neither the Liberal (GST) tax package nor the Labor package, but pledged to work with whichever party was elected to make their tax package better. They campaigned with the slogan "No GST on food".
In 1999, after negotiations with Prime Minister Howard, Meg Lees, Andrew Murray and the party room Senators agreed to support the A New Tax System (ANTS) legislation with exemptions from GST for most food and some medicines, as well as many environmental and social concessions. Five Australian Democrats senators voted in favour. However, two dissident senators on the party's left Natasha Stott Despoja and Andrew Bartlett voted against the GST.
In 2001, a leadership spill saw Meg Lees replaced as leader by Natasha Stott Despoja after a very public and bitter leadership battle. Despite criticism of Stott Despoja's youth and lack of experience, the 2001 election saw the Democrats receive similar media coverage to the previous election. Despite the internal divisions, the Australian Democrats' election result in 2001 was quite good. However, it was not enough to prevent the loss of Vicki Bourne's Senate seat in NSW.
The 2002 South Australian state election was the last time an Australian Democrat would be elected to an Australian parliament. Sandra Kanck was re-elected to a second eight-year term from an upper house primary vote of 7.3 percent.
Resulting tensions between Stott Despoja and Lees led to Meg Lees leaving the party in 2002, becoming an independent and forming the Australian Progressive Alliance. Stott Despoja stood down from the leadership following a loss of confidence by her party room colleagues. It led to a protracted leadership battle in 2002, which eventually led to the election of Senator Andrew Bartlett as leader. While the public fighting stopped, the public support for the party remained at record lows.
On 6 December 2003, Bartlett stepped aside temporarily as leader of the party, after an incident in which he swore at Liberal Senator Jeannie Ferris on the floor of Parliament while intoxicated. The party issued a statement stating that deputy leader Lyn Allison would serve as the acting leader of the party. Bartlett apologised to the Democrats, Jeannie Ferris and the Australian public for his behaviour and assured all concerned that it would never happen again. On 29 January 2004, after seeking medical treatment, Bartlett returned to the Australian Democrats leadership, vowing to abstain from alcohol.
2004.
Support for the Australian Democrats fell significantly at the 2004 federal election in which they achieved only 2.4 per cent of the national vote. Nowhere was this more noticeable than in their key support base of suburban Adelaide in South Australia, where they received between 7 and 31 per cent of the Lower House vote in 2001, and between 1 and 4 per cent in 2004. Three incumbent senators were defeated—Aden Ridgeway (NSW), Brian Greig (WA) and John Cherry (Qld). Following the loss, the customary post-election leadership ballot installed Lyn Allison as leader and Andrew Bartlett as her deputy.
From 1 July 2005 the Australian Democrats lost official parliamentary party status, being represented by only four senators while the governing Liberal-National Coalition gained a majority and potential control of the Senate—the first time this advantage had been enjoyed by any government since 1980.
2006.
On 5 January 2006, the ABC reported that the Tasmanian Electoral Commission had de-registered that division of the party for failing to provide a list containing the required number of members to be registered for Tasmanian state and local elections.
On 18 March 2006, at the 2006 South Australian state election, the Australian Democrats were reduced to 1.7 per cent of the Legislative Council (upper house) vote. Their sole councillor up for re-election, Kate Reynolds, was defeated.
After the election, South Australian senator Natasha Stott Despoja denied rumours that she was considering quitting the party.
In early July, Richard Pascoe, national and South Australian party president, resigned, citing slumping opinion polls and the poor result in the 2006 South Australian election as well as South Australian parliamentary leader Sandra Kanck's comments regarding the drug MDMA which he saw as damaging to the party.
On 5 July 2006, Australian Democrats senator for Western Australia Andrew Murray announced his intention not to contest the 2007 federal election, citing frustration arising from the Howard Government's control of both houses and his unwillingness to serve another six-year term. His term ended on 30 June 2008.
On 28 August 2006, the founder of the Australian Democrats, Don Chipp, died. Former prime minister Bob Hawke said: "... there is a coincidental timing almost between the passing of Don Chipp and what I think is the death throes of the Democrats. "
On 22 October 2006, Australian Democrats Senator Natasha Stott Despoja announced her intention not to seek re-election at the 2007 federal election due to health concerns. Her term ended on 30 June 2008.
In November 2006, the Australian Democrats fared very poorly in the Victorian state election, receiving a Legislative Council vote tally of only 0.83%, less than half of the party's result in 2002 (1.79 per cent).
2007.
In the New South Wales state election of March 2007, the Australian Democrats lost their last remaining NSW Upper House representative, Arthur Chesterfield-Evans. The party fared poorly, gaining only 1.8 per cent of the Legislative Council vote. A higher vote was achieved in some of the Legislative Assembly seats selectively contested as compared to 2003. However, the statewide vote share fell because the party was unable to field as many candidates as in 2003.
In the Victorian state by-election in Albert Park District the Australian Democrats stood candidate Paul Kavanagh, who polled 5.75 per cent of the primary vote, despite a large number of candidates, and all media attention focusing on the battle between Labor and Greens candidates.
On 13 September 2007, the ACT Democrats (Australian Capital Territory Division of the party) was deregistered by the ACT Electoral Commissioner, being unable to demonstrate a minimum membership of 100 electors.
The Democrats had no success at the 2007 federal election. Two incumbent senators, Lyn Allison (Victoria) and Andrew Bartlett (Queensland), were defeated, their seats both reverting to major parties. Their two remaining colleagues, Andrew Murray (WA) and Natasha Stott Despoja (SA), did not run for new terms. All four senators' terms expired on 30 June 2008—leaving the Australian Democrats with no federal representation for the first time since its founding in 1977. An ABC report noted that "on the Australian Electoral Commission (AEC) website the party is now referred to just as 'other'".
Post-2007.
The last of the party's state upper-house members, David Winderlich, resigned from the party in October 2009 and was defeated as an independent at the 2010 election.
In March 2012, the Australian Electoral Commission queried a Democrats submission of 550 names of purported members and proposed deregistering the party for having fewer than 500 members, the threshold needed for registration. The Commission later satisfied itself that the party had sufficient membership to continue its registration.
The Democrats did not nominate a single candidate in the 2014 South Australian election, in the party's state of origin.
On 16 April 2015, the Australian Electoral Commission deregistered the Australian Democrats as a political party for failure to demonstrate the requisite 500 members to maintain registration.
The Australian Democrats have said they will appeal the AEC decision, which under the legislation is reviewable.
Policy.
The party's original support base consisted of voters alienated by perceived unproductive adversarial conflict between the two mainstream parties and an emerging new constituency of people with a desire to participate more effectively in government and to promote concerns for environmental protection and social justice. The party aimed to combine liberal social policies with centrist, particularly neo-Keynesian economics and a progressive environmental platform.
The original agenda included interventionist economic policies, commitment to environmental causes, support for reconciliation with Australia's indigenous population through such mechanisms as formal treaties, pacifist approaches to international relations, open government, constitutional reform, progressive approaches to social issues such as sexuality and drugs, and strong support for human rights and civil liberties. Its membership largely comprised tertiary-educated and middle-class constituents. The party also appealed to voters opposed to untrammeled government power and wishing to have alternative views aired in parliaments and media.
The party has a platform of participatory democracy, with policies supporting proportional representation and citizen-initiated referenda. Many important internal issues (such as electoral preselection and leadership) are decided by direct postal ballot of the membership. Although policies are theoretically set in a similar fashion, Australian Democrats parliamentarians generally had extensive freedom in interpreting them.
However, by 1980, the Australian Democrats had employed the postal-ballot method at both national at state levels to develop an extensive body of written policy covering not only the political agendas of the day but also innovative and far-sighted policies for environmental and economic sustainability, water and energy conservation, e.g., through development of alternative energy sources, expanded public transport, etc. To the community's growing concerns about human rights, the Australian Democrats added finely detailed policies on animal welfare and species preservation. The material is available in election manifestos and copies of the party's journals, obtainable in major public libraries.
In a 2009 "rebuild" process, the party announced creation of a new policy process, attempts to improve internal communication, and envisaged development of a new party constitution.
Prior to the 2013 federal election, the party, though factionally divided into two separate organisations, was able to publish a comprehensive package of member-balloted policies.
Support.
Support for the Democrats historically tended to fluctuate between about 5 and 10 per cent of the population and was geographically concentrated around the wealthy dense CBD and inner-suburban neighbourhoods of the capital cities (especially Adelaide). Therefore, they never managed to win a House of Representatives seat. During the 1980s, 1990s and early 2000s they typically held one or two Senate seats in each state, as well as having some representatives in state parliaments.
Following the internal conflict over GST (1998–2001) and resultant leadership changes, a dramatic decline occurred in the Democrats' membership and voting support in all states. Simultaneously, an increase was recorded in support for the Australian Greens who, by 2004, were supplanting the Democrats as a substantial third party. The trend was noted that year by political scientists Dean Jaensch et al. Elsewhere, Jaensch later suggested it was possible the Democrats could make a political comeback in the federal arena.
Following Tony Abbott's displacement of Malcolm Turnbull as federal leader of the Liberal Party in 2009, the Democrats sought to attract the support of "those Liberals who no longer feel they can support their party".
Federal parliamentary leaders.
Of the party's nine elected federal parliamentary leaders, six were women. Aboriginal senator Aden Ridgeway was deputy leader under Natasha Stott Despoja.
Ridgeway was technically leader between Stott Despoja's resignation and the appointment of Brian Greig as interim leader.

</doc>
<doc id="1944" url="https://en.wikipedia.org/wiki?curid=1944" title="Australian Capital Territory">
Australian Capital Territory

The Australian Capital Territory (ACT) (formerly, "The Territory for the Seat of Government" and, later, the "Federal Capital Territory") is a territory in the south east of Australia, enclaved within New South Wales. It is the smaller of the two self-governing internal territories in Australia. The only city and by far the most populous community is Canberra, the capital city of Australia.
The need for a national territory was flagged by colonial delegates during the Federation conventions of the late 19th century. Section 125 of the Australian Constitution provided that, following Federation in 1901, land would be ceded freely to the new Federal Government. The territory was transferred to the Commonwealth by the state of New South Wales in 1911, two years prior to the naming of Canberra as the national capital in 1913. The floral emblem of the ACT is the royal bluebell and the bird emblem is the gang-gang cockatoo.
Geography.
The ACT is bounded by the Goulburn-Cooma railway line in the east, the watershed of Naas Creek in the south, the watershed of the Cotter River in the west, and the watershed of the Molonglo River in the north-east. The ACT also has a small strip of territory around the southern end of the Beecroft Peninsula, which is the northern headland of Jervis Bay.
Apart from the city of Canberra, the Australian Capital Territory also contains agricultural land (sheep, dairy cattle, vineyards and small amounts of crops) and a large area of national park (Namadgi National Park), much of it mountainous and forested. Small townships and communities located within the ACT include Williamsdale, Naas, Uriarra, Tharwa and Hall.
Tidbinbilla is a locality to the south-west of Canberra that features the Tidbinbilla Nature Reserve and the Canberra Deep Space Communication Complex, operated by the United States' National Aeronautics and Space Administration (NASA) as part of its Deep Space Network.
There are a large range of mountains, rivers and creeks in the Namadgi National Park. These include the Naas and Murrumbidgee Rivers.
Climate.
Because of its elevation and distance from the coast, the Australian Capital Territory experiences four distinct seasons, unlike many other Australian cities whose climates are moderated by the sea. Canberra is noted for its warm to hot, dry summers, and cold winters with occasional fog and frequent frosts. Many of the higher mountains in the territory's south-west are snow-covered for at least part of the winter. Thunderstorms can occur between October and March, and annual rainfall is , with rainfall highest in spring and summer and lowest in winter.
The highest maximum temperature recorded in the ACT was at Acton on 11 January 1939. The lowest minimum temperature was at Gudgenby on 11 July 1971.
Geology.
Notable geological formations in the Australian Capital Territory include the "Canberra Formation", the "Pittman Formation", "Black Mountain Sandstone" and "State Circle Shale".
In the 1840s fossils of brachiopods and trilobites from the Silurian period were discovered at Woolshed Creek near Duntroon. At the time, these were the oldest fossils discovered in Australia, though this record has now been far surpassed. Other specific geological places of interest include the State Circle cutting and the Deakin anticline.
The oldest rocks in the ACT date from the Ordovician around 480 million years ago. During this period the region along with most of Eastern Australia was part of the ocean floor; formations from this period include the "Black Mountain Sandstone" formation and the "Pittman Formation" consisting largely of quartz-rich sandstone, siltstone and shale. These formations became exposed when the ocean floor was raised by a major volcanic activity in the Devonian forming much of the east coast of Australia.
Governance.
The ACT has internal self-government, but Australia's Constitution does not afford the territory government the full legislative independence provided to Australian states. Laws are made in a 17-member Legislative Assembly that combines both state and local government functions.
Members of the Legislative Assembly are elected via the Hare Clarke system. The ACT Chief Minister (currently Andrew Barr, Australian Labor Party) is elected by members of the ACT Assembly. The ACT Government Chief Minister is a member of the Council of Australian Governments.
Unlike other self-governing Australian territories (for example, the Northern Territory), the ACT does not have an Administrator. The Crown is represented by the Australian Governor-General in the government of the ACT. Until 4 December 2011, the decisions of the assembly could be overruled by the Governor-General (effectively by the national government) under section 35 of the Australian Capital Territory (Self-Government) Act 1988, although the federal parliament voted in 2011 to abolish this veto power, instead requiring a majority of both houses of the federal parliament to override an enactment of the ACT. The Chief Minister performs many of the roles that a state governor normally holds in the context of a state; however, the Speaker of the Legislative Assembly gazettes the laws and summons meetings of the Assembly.
In Australia's Federal Parliament, the ACT is represented by four federal members: two members of the House of Representatives; the Division of Fraser and the Division of Canberra and is one of only two territories to be represented in the Senate, with two Senators (the other being the Northern Territory). The Member for Fraser and the ACT Senators also represent the constituents of the Jervis Bay Territory.
In 1915 the "Jervis Bay Territory Acceptance Act 1915" created the Jervis Bay Territory as an annexe to the Australian Capital Territory. In 1988, when the ACT gained self-government, Jervis Bay became a separate territory administered by the Australian Government Minister responsible for Territories, presently the Minister for Home Affairs.
The ACT retains a small area of territory on the coast on the Beecroft Peninsula, consisting of a strip of coastline around the northern headland of Jervis Bay (not to be confused with the Jervis Bay Territory, which is on the southern headland of the Bay). The ACT's land on the Beecroft Peninsula is an "exclave", that is, an area of territory not physically connected to the main part of the ACT. Interestingly, this ACT exclave surrounds a small exclave of NSW territory, namely the Point Perpendicular lighthouse which is at the southern tip of the Beecroft Peninsula. The lighthouse and its grounds are New South Wales territory, but cut off from the rest of the state by the strip of ACT land. This is a geographic curiosity: an exclave of NSW land enclosed by an exclave of ACT land.
Administration.
ACT Ministers implement their executive powers through the following government directorates:
Demographics.
In the 2011 census the population of the ACT was 357,222 of whom most lived in Canberra. The ACT median weekly income for people aged over 15 was in the range $600–$699 while that for the population living outside Canberra was at the national average of $400–$499. The average level of degree qualification in the ACT is higher than the national average. Within the ACT 4.5% of the population have a postgraduate degree compared to 1.8% across the whole of Australia.
Urban structure.
Canberra is a planned city that was originally designed by Walter Burley Griffin, a major 20th century American architect. Major roads follow a wheel-and-spoke pattern rather than a grid. The city centre is laid out on two perpendicular axes: a water axis stretching along Lake Burley Griffin, and a ceremonial land axis stretching from Parliament House on Capital Hill north-eastward along ANZAC Parade to the Australian War Memorial at the foot of Mount Ainslie.
The area known as the Parliamentary Triangle is formed by three of Burley Griffin's axes, stretching from Capital Hill along Commonwealth Avenue to the Civic Centre around City Hill, along Constitution Avenue to the Defence precinct on Russell Hill, and along Kings Avenue back to Capital Hill.
The larger scheme of Canberra's layout is based on the three peaks surrounding the city, Mount Ainslie, Black Mountain, and Red Hill. The main symmetrical axis of the city is along ANZAC Parade and roughly on the line between Mount Ainslie and Bimberi Peak. Bimberi Peak being the highest mountain in the ACT approximately south west of Canberra . The precise alignment of ANZAC parade is between Mount Ainslie and Capital Hill (formally Kurrajong Hill).
The Griffins assigned spiritual values to Mount Ainslie, Black Mountain, and Red Hill and originally planned to cover each of these in flowers. That way each hill would be covered with a single, primary color which represented its spiritual value. This part of their plan never came to fruition. In fact, WWI interrupted the construction and some conflicts after the war made it a difficult process for the Griffins. Nevertheless, Canberra stands as an exemplary city design and is located halfway between the ski slopes and the beach. It enjoys a natural cooling from geophysical factors.
The urban areas of Canberra are organised into a hierarchy of districts, town centres, group centres, local suburbs as well as other industrial areas and villages. There are seven districts (with an eighth currently under construction), each of which is divided into smaller suburbs, and most of which have a town centre which is the focus of commercial and social activities. The districts were settled in the following chronological order:
The North and South Canberra districts are substantially based on Walter Burley Griffin's designs. In 1967 the then National Capital Development Commission adopted the "Y Plan" which laid out future urban development in Canberra around a series of central shopping and commercial area known as the 'town centres' linked by freeways, the layout of which roughly resembled the shape of the letter Y, with Tuggeranong at the base of the Y and Belconnen and Gungahlin located at the ends of the arms of the Y.
Development in Canberra has been closely regulated by government, both through the town planning process, but also through the use of crown lease terms that have tightly limited the use of parcels of land. All land in the ACT is held on 99 year leases from the national government, although most leases are now administered by the Territory government.
Most suburbs have their own local shops, and are located close to a larger shopping centre serving a group of suburbs. Community facilities and schools are often also located near local shops or group shopping centres. Many of Canberra's suburbs are named after former Prime Ministers, famous Australians, early settlers, or use Aboriginal words for their title.
Street names typically follow a particular theme; for example, the streets of Duffy are named after Australian dams and reservoirs, the streets of Dunlop are named after Australian inventions, inventors and artists and the streets of Page are named after biologists and naturalists. Most diplomatic missions are located in the suburbs of Yarralumla, Deakin and O'Malley. There are three light industrial areas: the suburbs of Fyshwick, Mitchell and Hume.
Education.
Almost all educational institutions in the Australian Capital Territory are located within Canberra. The ACT public education system schooling is normally split up into Pre-School, Primary School (K-6), High School (7–10) and College (11–12) followed by studies at university or CIT (Canberra Institute of Technology). Many private high schools include years 11 and 12 and are referred to as colleges. Children are required to attend school until they turn 17 under the ACT Government's "Learn or Earn" policy.
In February 2004 there were 140 public and non-governmental schools in Canberra; 96 were operated by the Government and 44 are non-Government. In 2005 there were 60,275 students in the ACT school system. 59.3% of the students were enrolled in government schools with the remaining 40.7% in non-government schools. There were 30,995 students in primary school, 19,211 in high school, 9,429 in college and a further 340 in special schools.
As of May 2004, 30% of people in the ACT aged 15–64 had a level of educational attainment equal to at least a bachelor's degree, significantly higher than the national average of 19%. The two main tertiary institutions are the Australian National University (ANU) in Acton and the University of Canberra (UC) in Bruce. There are also two religious university campuses in Canberra: Signadou is a campus of the Australian Catholic University and St Mark's Theological College is a campus of Charles Sturt University. Tertiary level vocational education is also available through the multi-campus Canberra Institute of Technology.
The Australian Defence Force Academy (ADFA) and the Royal Military College, Duntroon (RMC) are in the suburb of Campbell in Canberra's inner northeast. ADFA teaches military undergraduates and postgraduates and is officially a campus of the University of New South Wales while Duntroon provides Australian Army Officer training.
The Academy of Interactive Entertainment (AIE) offers courses in computer game development and 3D animation.

</doc>
<doc id="1946" url="https://en.wikipedia.org/wiki?curid=1946" title="Unit of alcohol">
Unit of alcohol

Units of alcohol are used in the United Kingdom (UK) as a measure to quantify the actual alcoholic content within a given volume of an alcoholic beverage, in order to provide guidance on total alcohol consumption.
A number of other countries (including Australia, Canada, New Zealand, and the US) use the concept of a "standard drink", the definition of which varies from country to country, for the same purpose. "Standard drinks" were referred to in the first UK guidelines (1984) that published "safe limits" for drinking, but these were replaced by references to "alcohol units" in the 1987 guidelines and the latter term has been used in all subsequent UK guidance.
One unit of alcohol (UK) is defined as 10 millilitres (8 grams) of pure alcohol. Typical drinks (i.e. typical quantities or servings of common alcoholic beverages) may contain 1–3 units of alcohol.
Containers of alcoholic beverages sold directly to UK consumers are normally labelled to indicate the number of units of alcohol in a typical serving of the beverage (optional) and in the full container (can or bottle), as well as information about responsible drinking. Additionally, the advent of smartphones has led to the creation of apps which report the number of units contained in an alcoholic drink.
As an approximate guideline, a typical healthy adult can metabolise (break down) about one unit of alcohol per hour, although this may vary depending on sex, age, weight, health, and many other factors.
Formula.
The number of UK units of alcohol in a drink can be determined by multiplying the volume of the drink (in millilitres) by its percentage ABV, and dividing by 1000.
For example, one imperial pint (568 ml) of beer at 4% alcohol by volume (ABV) contains:
formula_1
The formula uses . This results in exactly one unit per percentage point per litre, of any alcoholic beverage.
The formula can be simplified for everyday use by expressing the serving size in centilitres and the alcohol content literally as a percentage. Thus, a 750 ml bottle of wine at 12% ABV contains 75 cl * 12% (i.e. 0.12) = 9 units. Alternatively, the serving size in litres multiplied by the alcohol content as a number, the above example giving 0.75 * 12 = 9 units. 
Both pieces of input data are usually mentioned in this form on the bottle, so is easy to retrieve.
Labelling.
UK alcohol companies pledged in March 2011 to implement an innovative health labelling scheme to provide more information about responsible drinking on alcohol labels and containers. This voluntary scheme is the first of its kind in Europe and has been developed in conjunction with the UK Department of Health. The pledge stated:
At the end of 2014, 101 companies had committed to the pledge labelling scheme.
There are five elements included within the overall labelling scheme, the first three being mandatory, and the last two optional:
Drinks companies had pledged to display the three mandatory items on 80% of drinks containers on shelves in the UK off-trade by the end of December 2013. A report published in Nov 2014, confirmed that UK drinks producers had delivered on that pledge with a 79.3% compliance with the pledge elements as measured by products on shelf. Compared with labels from 2008 on a like-for-like basis, information on Unit alcohol content had increased by 46%; 91% of products displayed alcohol and pregnancy warnings (18% in 2008); and 75% showed the Chief Medical Officers’ lower risk daily guidelines (6% in 2008).
Quantities.
It is sometimes misleadingly stated that there is one unit per half-pint of beer, or small glass of wine, or single measure of spirits. However, such statements do not take into account the various strengths and volumes supplied in practice.
For example, the ABV of beer typically varies from 3.5% to 5.5%. A typical "medium" glass of wine with 175 ml at 12% ABV has 2.1 units. And spirits, although typically 35–40% ABV, have single measures of 25 ml or 35 ml (so 1 or 1.4 units) depending on location.
The misleading nature of "one unit per half-pint of beer, or small glass of wine, or single measure of spirits" can lead to people underestimating their alcohol intake.
Spirits.
Most spirits sold in the United Kingdom have 40% ABV or slightly less. In England a single pub measure (25 ml) of a spirit contains one unit. However, a larger 35 ml measure is increasingly used (and in particular is standard in Northern Ireland ), which contains 1.4 units of alcohol at 40% ABV. Sellers of spirits by the glass must state the capacity of their standard measure in ml.
Time to metabolise.
On average, it takes about one hour for the body to metabolise (break down) one unit of alcohol. However, this will vary with body weight, sex, age, personal metabolic rate, recent food intake, the type and strength of the alcohol, and
medications taken. Alcohol may be metabolised more slowly if liver function is impaired.
Recommended maximum.
From 1992 to 1995 the UK government advised that men should drink no more than 21 units per week, and women no more than 14. (The difference between the sexes was due to the typically lower weight and water-to-body-mass ratio of women.) The Times reported in October 2007 that these limits had been "plucked out of the air" and had no scientific basis.
This was changed after a government study showed that many people were in effect "saving up" their units and using them at the end of the week, a phenomenon referred to as binge drinking. Since 1995 the advice was that regular consumption of 3–4 units a day for men, or 2–3 units a day for women, would not pose significant health risks, but that consistently drinking four or more units a day (men), or three or more units a day (women), is not advisable.
An international study of about 6,000 men and 11,000 women for a total of 75,000 person-years found that people who reported that they drank more than a threshold value of 2 units of alcohol a day had a higher risk of fractures than non-drinkers. For example, those who drank over 3 units a day had nearly twice the risk of a hip fracture.

</doc>
<doc id="1947" url="https://en.wikipedia.org/wiki?curid=1947" title="Aotus">
Aotus

Aotus (the name is derived from the Ancient Greek words for "earless" in both cases: the monkey is missing external ears, and the pea is missing earlike bracteoles) may refer to:

</doc>
<doc id="1948" url="https://en.wikipedia.org/wiki?curid=1948" title="Ally McBeal">
Ally McBeal

Ally McBeal is an American legal comedy-drama television series, originally aired on Fox from September 8, 1997 to May 20, 2002. Created by David E. Kelley, the series stars Calista Flockhart in the title role as a young lawyer working in the fictional Boston law firm Cage and Fish, with other young lawyers whose lives and loves were eccentric, humorous and dramatic. The series placed #48 on "Entertainment Weekly" 2007 "New TV Classics" list.
Overview.
The series, set in the fictional Boston law firm Cage and Fish, begins with main character Allison Marie "Ally" McBeal joining the firm (co-owned by her law school classmate Richard Fish, played by Greg Germann) after leaving her previous job due to sexual harassment. On her first day Ally is horrified to find that she will be working alongside her ex-boyfriend Billy Thomas (Gil Bellows)—whom she has never gotten over. To make things worse, Billy is now married to fellow lawyer Georgia (Courtney Thorne-Smith), who also later joins Cage and Fish. The triangle among the three forms the basis for the main plot for the show's first three seasons.
Although ostensibly a legal drama, the main focus of the series was the romantic and personal lives of the main characters, often using legal proceedings as plot devices to contrast or reinforce a character's drama. For example, bitter divorce litigation of a client might provide a backdrop for Ally's decision to break up with a boyfriend. Legal arguments were also frequently used to explore multiple sides of various social issues.
Cage & Fish (which becomes Cage/Fish & McBeal or Cage, Fish, & Associates towards the end of the series), the fictional law firm where most of the characters work, is depicted as a highly sexualized environment symbolized by its unisex restroom. Lawyers and secretaries in the firm routinely date, flirt with, or have a romantic history with each other and frequently run into former or potential romantic interests in the courtroom or on the street outside.
The series had many offbeat and frequently surreal running gags and themes, such as Ally's tendency to immediately fall over whenever she met somebody she found attractive, or Richard Fish's wattle fetish and humorous mottos ("Fishisms" & "Bygones"), or John's gymnastic dismounts out of the office's unisex bathroom stalls, or the dancing twins (played by Eric & Steve Cohen) at the bar, that ran through the series. The show used vivid, dramatic fantasy sequences for Ally's and other characters' wishful thinking; particularly notable is the dancing baby.
The series also featured regular visits to a local bar where singer Vonda Shepard regularly performed (though occasionally handing over the microphone to the characters). The series also took place in the same continuity as David E. Kelley's legal drama "The Practice" (which aired on ABC), as the two shows crossed over with one another on occasion, a very rare occurrence for two shows that aired on different networks.
Episodes.
In Australia, "Ally McBeal" was aired by the Seven Network from 1997 to 2002. In 2010, it was aired repeatedly by Network Ten.
Crossovers with "The Practice".
Seymore Walsh, a stern judge often exasperated by the eccentricities of the Cage & Fish lawyers and played by actor Albert Hall, was also a recurring character on "The Practice". In addition, Judge Jennifer (Whipper) Cone appears on "The Practice" episode "Line of Duty" (S02E15), while Judge Roberta Kittelson, a recurring character on "The Practice,", has a featured guest role in the "Ally McBeal" episode "Do you Wanna Dance?"
Most of the primary "Practice" cast members guest starred in the "Ally McBeal" episode "The Inmates" (S01E20), in a storyline that concluded with the "Practice" episode "Axe Murderer" (S02E26), featuring Calista Flockhart and Gil Bellows reprising their "Ally" characters; what's unique about this continuing storyline is that "Ally McBeal" and "The Practice" happened to air on different networks. Bobby Donnell, the main character of "The Practice" played by Dylan McDermott, was featured heavily in both this crossover and another "Ally McBeal" episode, "These are the Days."
Regular "Practice" cast members Lara Flynn Boyle and Michael Badalucco each had a cameo in "Ally McBeal" (Boyle as a woman who trades insults with Ally in the episode "Making Spirits Bright" and Badalucco as one of Ally's dates in the episode "I Know him by Heart") but it remains ambiguous whether they were playing the same characters they play on "The Practice".
Reception.
Upon premiering in 1997, the show was an instant hit, average around 11 million viewers per episode. The shows second season saw an increase in ratings and soon became a top 20 show, averaging around 13 million viewers per episode. The show's ratings began to decline in the third season, but stabilized in the fourth season after Robert Downey, Jr. joined the regular cast as Ally's boyfriend Larry Paul, and a fresher aesthetic was created by new art director Matthew DeCoste. However, Downey's character was written out after the end of the season due to the actor's troubles with drug addiction.
The first two seasons, as well as the fourth, remain the most critically acclaimed and saw the most awards success at the Emmys, SAG Awards and the Golden Globes.
Along with "Dharma & Greg", "Ally McBeal" was one of the last two surviving shows to debut during the 1997-98 season, one of the weakest in United States television history for new shows. (Only seven shows to debut would be picked up for a second season, and only "Dharma & Greg" and "Ally McBeal" would last longer than three seasons, each providing enough episodes for syndication.) Both shows ended at the end of the 2001-02 season, five years after their debut. Both shows were produced by 20th Century Fox Television.
Feminist criticism.
Despite its success, "Ally McBeal" received some negative criticism from TV critics and feminists who found the title character annoying and demeaning to women (specifically professional women) because of her perceived flightiness, lack of demonstrated legal knowledge, short skirts, and emotional instability. Perhaps the most notorious example of the debate sparked by the show was the June 29, 1998 cover story of "Time" magazine, which juxtaposed McBeal with three pioneering feminists (Susan B. Anthony, Betty Friedan, Gloria Steinem) and asked "Is Feminism Dead?" In episode 12 of the second season of the show, Ally talks to her co-worker John Cage about a dream she had, saying "You know, I had a dream that they put my face on the cover of "Time" magazine as 'the face of feminism'."
Music.
"Ally McBeal" was a heavily music-oriented show. Vonda Shepard, a virtually unknown musician at the time, was featured continually on the show. Her song "Searchin' My Soul" became the show's theme song. Many of the songs Shepard performed were established hits with lyrics that paralleled the events of the episode, including "Both Sides Now", "Hooked on a Feeling" and "Tell Him". Besides recording background music for the show, Shepard frequently appeared at the ends of episodes as a musician performing at a local piano bar frequented by the main characters. On rare occasions, her character would have conventional dialogue. A portion of "Searchin' My Soul" was played at the beginning of each episode, but remarkably the song was never played in its entirety.
Several of the characters had a musical leitmotif that played when they appeared. John Cage's was "You're the First, the Last, My Everything", Ling Woo's was the Wicked Witch of the West theme from "The Wizard of Oz", and Ally McBeal herself picked "Tell Him", when told by a psychiatrist that she needed a theme.
Due to the popularity of the show and Shepard's music, a soundtrack titled "Songs from Ally McBeal" was released in 1998, as well as a successor soundtrack titled "Heart and Soul: New Songs From Ally McBeal" in 1999. Two compilation albums from the show featuring Shepard were also released in 2000 and 2001. A Christmas album was also released under the title "Ally McBeal: A Very Ally Christmas". The album received positive reviews, and Shephard’s version of Kay Starr’s Christmas song (Everybody's Waitin' For) The Man with the Bag, received considerable airplay during the holiday season.
Other artists featured on the show include Michael Jackson, Barry White, Al Green, Tina Turner, Macy Gray, Gloria Gaynor, Chayanne, Barry Manilow, Anastacia, Elton John, Sting and Mariah Carey. Josh Groban played the role of Malcolm Wyatt in the May 2001 season finale, performing "You're Still You". The series creator, David E. Kelley, was impressed with Groban's performance at The Family Celebration event and based on the audience reaction to Groban's singing, Kelley created a character for him in that finale. The background score for the show was composed by Danny Lux.
DVD releases.
Due to music licensing issues, none of the seasons of "Ally McBeal" were available on DVD in the United States (only 6 random episodes could be found on the R1 edition) until 2009, though the show had been available in Italy, Belgium, the Netherlands, Japan, Hong Kong, Portugal, Spain, France, Germany, the United Kingdom, Mexico, Taiwan, Australia, Brazil, and the Czech Republic with all the show's music intact since 2005. In the UK, Ireland, and Spain all seasons are available in a complete box set.
20th Century Fox released the complete first season on DVD in Region 1 on October 6, 2009. They also released a special complete series edition on the same day. Season 1 does not contain any special features, but the complete series set contains several bonus features, including featurettes, an all-new retrospective, the episode of "The Practice" in which Calista Flockhart guest starred, and a bonus disc entitled "The Best of Ally McBeal Soundtrack." In addition, both releases contain all of the original music. Season 2 was released on April 6, 2010. Seasons 3, 4, and 5 were all released on October 5, 2010.
"Ally" (1999).
In 1999, at the height of the show's popularity, a half-hour version entitled "Ally" began airing in parallel with the main program. This version, designed in a sitcom format, used re-edited scenes from the main program, along with previously unseen footage. The intention was to further develop the plots in the comedy-drama in a sitcom style. It also focused only on Ally's personal life, cutting all the courtroom plots. The repackaged show was cancelled partway through its initial run. While 13 episodes of "Ally" were produced, only 10 actually aired.
In popular culture.
McBeal and 1990s young affluent professional women were parodied in the song "Ally McBeal" (tune of "Like a Rolling Stone" by Bob Dylan) by a cappella group Da Vinci's Notebook on their album "The Life and Times of Mike Fanning", released in 2000.
In episode 2, season 3 of the British comedy "The Adam and Joe Show", the show was parodied as 'Ally McSqeal' using soft toys.

</doc>
<doc id="1949" url="https://en.wikipedia.org/wiki?curid=1949" title="Andreas Capellanus">
Andreas Capellanus

Andreas Capellanus ("Capellanus" meaning "chaplain"), also known as Andrew the Chaplain, and occasionally by a French translation of his name, André le Chapelain, was the 12th-century author of a treatise commonly known as "De amore" ("About Love"), and often known in English, somewhat misleadingly, as "The Art of Courtly Love", though its realistic, somewhat cynical tone suggests that it is in some measure an antidote to courtly love. Little is known of Andreas Capellanus's life, but he is presumed to have been a courtier of Marie de Champagne, and probably of French origin.
His work.
"De Amore" was written at the request of Marie de Champagne, daughter of King Louis VII of France and of Eleanor of Aquitaine. In it, the author informs a young pupil, Walter, of the pitfalls of love. A dismissive allusion in the text to the "wealth of Hungary" has suggested the hypothesis that it was written after 1184, at the time when Bela III of Hungary had sent to the French court a statement of his income and had proposed marriage to Marie's half-sister Marguerite of France, but before 1186, when his proposal was accepted.
"De Amore" is made up of three books. The first book covers the etymology and definition of love and is written in the manner of an academic lecture. The second book consists of sample dialogues between members of different social classes; it outlines how the romantic process between the classes should work. Book three is made of stories from actual courts of love presided over by noble women.
John Jay Parry, the editor of one modern edition of "De Amore", quotes critic Robert Bossuat as describing "De Amore" as "one of those capital works which reflect the thought of a great epoch, which explains the secret of a civilization". It may be viewed as didactic, mocking, or merely descriptive; in any event it preserves the attitudes and practices that were the foundation of a long and significant tradition in Western literature.
The social system of "courtly love", as gradually elaborated by the Provençal troubadours from the mid twelfth century, soon spread. One of the circles in which this poetry and its ethic were cultivated was the court of Eleanor of Aquitaine (herself the granddaughter of an early troubadour poet, William IX of Aquitaine). It has been claimed that "De Amore" codifies the social and sexual life of Eleanor's court at Poitiers between 1170 and 1174, though it was evidently written at least ten years later and, apparently, at Troyes. It deals with several specific themes that were the subject of poetical debate among late twelfth century troubadours and trobairitz.
The meaning of "De Amore" has been debated over the centuries. In the years immediately following its release many people took Andreas’ opinions concerning Courtly Love seriously. In more recent times, however, scholars have come to view the priest’s work as satirical. Many scholars now agree that Andreas was commenting on the materialistic, superficial nature of the nobles of the Middle Ages. Andreas seems to have been warning young Walter, his protege, about love in the Middle Ages.

</doc>
<doc id="1950" url="https://en.wikipedia.org/wiki?curid=1950" title="American Civil Liberties Union">
American Civil Liberties Union

The American Civil Liberties Union (ACLU) is a nonpartisan, non-profit organization whose stated mission is "to defend and preserve the individual rights and liberties guaranteed to every person in this country by the Constitution and laws of the United States." It works through litigation, lobbying, and community empowerment. Founded in 1920 by Roger Baldwin, Crystal Eastman, Walter Nelles, Morris Ernst, Albert DeSilver, Arthur Garfield Hays, Jane Addams, Felix Frankfurter, and Elizabeth Gurley Flynn, the ACLU has over 500,000 members and has an annual budget of over $100 million. Local affiliates of the ACLU are active in all 50 states and Puerto Rico. The ACLU provides legal assistance in cases when it considers civil liberties to be at risk. Legal support from the ACLU can take the form of direct legal representation, or preparation of "amicus curiae" briefs expressing legal arguments (when another law firm is already providing representation).
When the ACLU was founded in 1920, its focus was on freedom of speech, primarily for anti-war protesters. During the 1920s, the ACLU expanded its scope to include protecting the free speech rights of artists and striking workers, and working with the National Association for the Advancement of Colored People (NAACP) to decrease racism and discrimination. During the 1930s, the ACLU started to engage in work combating police misconduct and for Native American rights. Most of the ACLU's cases came from the Communist party and Jehovah's Witnesses. In 1940, the ACLU leadership was caught up in the Red Scare, and voted to exclude Communists from its leadership positions. During World War II, the ACLU defended Japanese-American citizens, unsuccessfully trying to prevent their forcible relocation to internment camps. During the Cold War, the ACLU headquarters was dominated by anti-communists, but many local affiliates defended members of the Communist Party.
By 1964, membership had risen to 80,000, and the ACLU participated in efforts to expand civil liberties. In the 1960s, the ACLU continued its decades-long effort to enforce separation of church and state. It defended several anti-war activists during the Vietnam War. The ACLU was involved in the "Miranda" case, which addressed misconduct by police during interrogations; and in the "New York Times" case, which established new protections for newspapers reporting on government activities. In the 1970s and 1980s, the ACLU ventured into new legal areas, defending homosexuals, students, prisoners, and the poor. In the twenty-first century, the ACLU has fought the teaching of creationism in public schools and challenged some provisions of anti-terrorism legislation as infringing on privacy and civil liberties.
In addition to representing persons and organizations in lawsuits, the ACLU lobbies for policies that have been established by its board of directors. Current positions of the ACLU include: opposing the death penalty; supporting same-sex marriage and the right of gays to adopt; supporting birth control and abortion rights; eliminating discrimination against women, minorities, and LGBT people; supporting the rights of prisoners and opposing torture; and opposing government preference for religion over non-religion, or for particular faiths over others.
Legally, the ACLU consists of two separate but closely affiliated nonprofit organizations: the American Civil Liberties Union, a 501(c)(4) social welfare group, and the ACLU Foundation, a 501(c)(3) public charity. Both organizations engage in civil rights litigation, advocacy, and education, but only donations to the 501(c)(3) foundation are tax deductible, and only the 501(c)(4) group can engage in unlimited political lobbying. The two organizations share office space and employees.
Organization.
Leadership.
The ACLU is led by a president and an executive director, Susan N. Herman and Anthony Romero, respectively, in 2015. The president acts as chairman of the ACLU's board of directors, leads fundraising, and facilitates policy-setting. The executive director manages the day-to-day operations of the organization. The board of directors consists of 80 persons, including representatives from each state affiliate, as well as at-large delegates. The organization has its headquarters in 125 Broad Street, a 40-story skyscraper located in Lower Manhattan, New York City.
The leadership of the ACLU does not always agree on policy decisions; differences of opinion within the ACLU leadership have sometimes grown into major debates. In 1937, an internal debate erupted over whether to defend Henry Ford's right to distribute anti-union literature. In 1939, a heated debate took place over whether to prohibit communists from serving in ACLU leadership roles. During the early 1950s the board was divided on whether to defend communists persecuted under McCarthyism. In 1968, a schism formed over whether to represent Dr. Spock's anti-war activism. In 1973, there was internal conflict over whether to call for the impeachment of Richard Nixon. In 2005, there was internal conflict about whether or not a gag rule should be imposed on ACLU employees to prevent publication of internal disputes.
Funding.
In the year ending March 31, 2014, the ACLU and the ACLU Foundation had a combined income from support and revenue of $100.4 million, originating from grants (50.0%), membership donations (25.4%), donated legal services (7.6%), bequests (16.2%), and revenue (.9%). Membership dues are treated as donations; members choose the amount they pay annually, averaging approximately $50 per member per year. In the year ending March 31, 2014, the combined expenses of the ACLU and ACLU Foundation were $133.4 million, spent on programs (86.2%), management (7.4%), and fundraising (8.2%). (After factoring in other changes in net assets of +$30.9 million, from sources such as investment income, the organization had an overall decrease in net assets of $2.1 million.) Over the period from 2011 to 2014 the ACLU Foundation, on the average, has accounted for roughly 70% of the combined budget, and the ACLU roughly 30%.
The ACLU solicits donations to its charitable foundation. The ACLU is accredited by the Better Business Bureau, and the Charity Navigator has ranked the ACLU with a four-star rating. The local affiliates solicit their own funding; however, some also receive funds from the national ACLU, with the distribution and amount of such assistance varying from state to state. At its discretion, the national organization provides subsidies to smaller affiliates that lack sufficient resources to be self-sustaining; for example, the Wyoming ACLU chapter received such subsidies until April 2015, when, as part of a round of layoffs at the national ACLU, the Wyoming office was closed.
In October 2004, the ACLU rejected $1.5 million from both the Ford Foundation and Rockefeller Foundation because the Foundations had adopted language from the USA PATRIOT Act in their donation agreements, including a clause stipulating that none of the money would go to "underwriting terrorism or other unacceptable activities." The ACLU views this clause, both in Federal law and in the donors' agreements, as a threat to civil liberties, saying it is overly broad and ambiguous.
Due to the nature of its legal work, the ACLU is often involved in litigation against governmental bodies, which are generally protected from adverse monetary judgments; a town, state or federal agency may be required to change its laws or behave differently, but not to pay monetary damages except by an explicit statutory waiver. In some cases, the law permits plaintiffs who successfully sue government agencies to collect money damages or other monetary relief. In particular, the Civil Rights Attorney's Fees Award Act of 1976 leaves the government liable in some civil rights cases. Fee awards under this civil rights statute are considered "equitable relief" rather than damages, and government entities are not immune from equitable relief. Under laws such as this, the ACLU and its state affiliates sometimes share in monetary judgments against government agencies. In 2006, the Public Expressions of Religion Protection Act sought to prevent monetary judgments in the particular case of violations of church-state separation.
The ACLU has received court awarded fees from opponents, for example, the Georgia affiliate was awarded $150,000 in fees after suing a county demanding the removal of a Ten Commandments display from its courthouse; a second Ten Commandments case in the State, in a different county, led to a $74,462 judgment. The State of Tennessee was required to pay $50,000, the State of Alabama $175,000, and the State of Kentucky $121,500, in similar Ten Commandments cases.
State affiliates.
Most of the organization's workload is performed by the 53 local affiliates. There is an affiliate in each state and in Puerto Rico. California has three affiliates. The affiliates operate autonomously from the national organization; each affiliate has its own staff, executive director, board of directors, and budget. Each affiliate consists of two non-profit corporations: a 501(c)(3) corporation that does not perform lobbying, and a 501(c)(4) corporation which is entitled to lobby.
ACLU affiliates are the basic unit of the ACLU's organization and engage in litigation, lobbying, and public education. For example, in a twenty-month period beginning January 2004, the ACLU's New Jersey chapter was involved in fifty-one cases according to their annual report—thirty-five cases in state courts, and sixteen in federal court. They provided legal representation in thirty-three of those cases, and served as amicus in the remaining eighteen. They listed forty-four volunteer attorneys who assisted them in those cases.
Positions.
The ACLU's official position statements, as of January 2012, included the following policies:
Support and opposition.
The ACLU is supported by a variety of persons and organizations. There were over 500,000 members in 2011, and the ACLU annually receives thousands of grants from hundreds of charitable foundations. Allies of the ACLU in legal actions have included the National Association for the Advancement of Colored People, the American Jewish Congress, People For the American Way, the National Rifle Association, the Electronic Frontier Foundation, Americans United for Separation of Church and State, and the National Organization for Women.
The ACLU has been criticized by liberals, such as when it excluded communists from its leadership ranks, when it defended Neo-Nazis, when it declined to defend Paul Robeson, or when it opposed the passage of the National Labor Relations Act. Conversely, it has been criticized by conservatives, such as when it argued against official prayer in public schools, or when it opposed the Patriot Act. The ACLU has supported conservative figures such as Rush Limbaugh, George Wallace, Henry Ford, and Oliver North; and it has supported liberal figures such as Dick Gregory, H. L. Mencken, Rockwell Kent, and Dr. Benjamin Spock.
A major source of criticism are legal cases in which the ACLU represents an individual or organization that promotes offensive or unpopular viewpoints, such as the Ku Klux Klan, Neo-Nazis, Nation of Islam, North American Man/Boy Love Association, or Westboro Baptist Church. The ACLU responded to these criticisms by stating "It is easy to defend freedom of speech when the message is something many people find at least reasonable. But the defense of freedom of speech is most critical when the message is one most people find repulsive."
Early years.
CLB era.
The ACLU developed from the National Civil Liberties Bureau (CLB), co-founded in 1917 during the Great War by Crystal Eastman, an attorney activist, and Roger Nash Baldwin. The focus of the CLB was on freedom of speech, primarily anti-war speech, and on supporting conscientious objectors who did not want to serve in World War I.
Three United States Supreme Court decisions in 1919 each upheld convictions under laws against certain kinds of anti-war speech. In 1919, the Court upheld the conviction of Socialist Party leader Charles Schenck for publishing anti-war literature. In "Debs v. United States," the court upheld the conviction of Eugene Debs. While the Court upheld a conviction a third time in "Abrams v. United States", Justice Oliver Wendell Holmes wrote an important dissent which has gradually been absorbed as an American principle: he urged the court to treat freedom of speech as a fundamental right, which should rarely be restricted.
In 1918 Crystal Eastman resigned from the organization due to health issues. After assuming sole leadership of the CLB, Baldwin insisted that the organization be reorganized. He wanted to change its focus from litigation to direct action and public education.
The CLB directors concurred, and on January 19, 1920, they formed an organization under a new name, the American Civil Liberties Union. Although a handful of other organizations in the United States at that time focused on civil rights, such as the National Association for the Advancement of Colored People (NAACP) and Anti-Defamation League (ADL), the ACLU was the first that did not represent a particular group of persons, or a single theme. Like the CLB, the NAACP pursued litigation to work on civil rights, including efforts to overturn the disfranchisement of African Americans in the South that had taken place since the turn of the century.
During the first decades of the ACLU, Baldwin continued as its leader. His charisma and energy attracted many supporters to the ACLU board and leadership ranks. Baldwin was ascetic, wearing hand-me-down clothes, pinching pennies, and living on a very small salary. The ACLU was directed by an executive committee, but it was not particularly democratic or egalitarian. The ACLU's base in New York resulted in its being dominated by people from the city and state. Most ACLU funding came from philanthropies, such as the Garland Fund.
Free speech era.
In the 1920s, government censorship was commonplace. Magazines were routinely confiscated under the anti-obscenity Comstock laws; permits for labor rallies were often denied; and virtually all anti-war or anti-government literature was outlawed. Right-wing conservatives wielded vast amounts of power, and activists that promoted unionization, socialism, or government reform were often denounced as un-American or unpatriotic. In one typical instance in 1923, author Upton Sinclair was arrested for trying to read the First Amendment during an Industrial Workers of the World rally.
ACLU leadership was divided on how to challenge the civil rights violations. One faction, including Baldwin, Arthur Garfield Hays and Norman Thomas, believed that direct, militant action was the best path. Hays was the first of many successful attorneys that relinquished their private practices to work for the ACLU. Another group, including Walter Nelles and Walter Pollak felt that lawsuits taken to the Supreme Court were the best way to achieve change. Both groups worked in tandem, but equally revered the Bill of Rights and the US Constitution.
During the 1920s, the ACLU's primary focus was on freedom of speech in general, and speech within the labor movement particularly. Because most of the ACLU's efforts were associated with the labor movement, the ACLU itself came under heavy attack from conservative groups, such as the American Legion, the National Civic Federation, and Industrial Defense Association and the Allied Patriotic Societies.
In addition to labor, the ACLU also led efforts in non-labor arenas, for example, promoting free speech in public schools. The ACLU itself was banned from speaking in New York public schools in 1921. The ACLU, working with the NAACP, also supported racial discrimination cases. The ACLU defended free speech regardless of the opinions being espoused. For example, the reactionary, anti-Catholic, anti-black Ku Klux Klan (KKK) was a frequent target of ACLU efforts, but the ACLU defended the KKK's right to hold meetings in 1923. There were some civil rights that the ACLU did not make an effort to defend in the 1920s, including censorship of the arts, government search and seizure issues, right to privacy, or wiretapping.
The Communist party of the United States was routinely harassed and oppressed by government officials, leading it to be the primary client of the ACLU. The Communists were very aggressive in their tactics, often engaging in illegal or unethical conduct, and this led to frequent conflicts between the Communists and ACLU. Communist leaders often attacked the ACLU, particularly when the ACLU defended the free speech rights of conservatives. This uneasy relationship between the two groups continued for decades.
Scopes trial.
When 1925 arrived – five years after the ACLU was formed – the organization had virtually no success to show for its efforts. That changed in 1925, when the ACLU persuaded John T. Scopes to defy Tennessee's anti-evolution law in a court test. Clarence Darrow, a member of the ACLU National Committee, headed Scopes' legal team. The prosecution, led by William Jennings Bryan, contended that the Bible should be interpreted literally in teaching creationism in school. The ACLU lost the case and Scopes was fined $100. The Tennessee Supreme Court later upheld the law but overturned the conviction on a technicality.
The Scopes trial was a phenomenal public relations success for the ACLU. The ACLU became well known across America, and the case led to the first endorsement of the ACLU by a major U.S. newspaper. The ACLU continued to fight for the separation of church and state in schoolrooms, decade after decade, including the 1982 case "McLean v. Arkansas" and the 2005 case "Kitzmiller v. Dover Area School District".
Baldwin himself was involved in an important free speech victory of the 1920s, after he was arrested for attempting to speak at a rally of striking mill workers in New Jersey. Although the decision was limited to the state of New Jersey, the appeals court's judgement in 1928 declared that constitutional guarantees of free speech must be given "liberal and comprehensive construction", and it marked a major turning point in the civil rights movement, signaling the shift of judicial opinion in favor of civil rights.
The most important ACLU case of the 1920s was "Gitlow v. New York", in which Benjamin Gitlow was arrested for violating a state law against inciting anarchy and violence, when he distributed literature promoting communism. Although the Supreme Court did not overturn Gitlow's conviction, it adopted the ACLU's stance (later termed the incorporation doctrine) that the First Amendment freedom of speech applied to state laws, as well as federal laws.
First victories.
Leaders of the ACLU were divided on the best tactics to use to promote civil liberties. Felix Frankfurter felt that legislation was the best long-term solution, because the Supreme Court could not (andin his opinionshould not) mandate liberal interpretations of the Bill of Rights. But Walter Pollack, Morris Ernst, and other leaders felt that Supreme Court decisions were the best path to guarantee civil liberties. A series of Supreme Court decisions in the 1920s foretold a changing national atmosphere; anti-radical emotions were diminishing, and there was a growing willingness to protect freedom of speech and assembly via court decisions.
Free speech.
Censorship was commonplace in the early 20th century. State laws and city ordinances routinely outlawed speech deemed to be obscene or offensive, and prohibited meetings or literature that promoted unions or labor organization. Starting in 1926, the ACLU began to expand its free speech activities to encompass censorship of art and literature. In that year, H. L. Mencken deliberately broke Boston law by distributing copies of his banned "American Mercury" magazine; the ACLU defended him and won an acquittal. The ACLU went on to win additional victories, including the landmark case "United States v. One Book Called Ulysses" in 1933, which reversed a ban by the Customs Department against the book "Ulysses" by James Joyce. The ACLU only achieved mixed results in the early years, and it was not until 1966 that the Supreme Court finally clarified the obscenity laws in the "Roth v. United States" and "Memoirs v. Massachusetts" cases.
The Comstock laws banned distribution of sex education information, based on the premise that it was obscene and led to promiscuous behavior Mary Ware Dennett was fined $300 in 1928, for distributing a pamphlet containing sex education material. The ACLU, led by Morris Ernst, appealed her conviction and won a reversal, in which judge Learned Hand ruled that the pamphlet's main purpose was to "promote understanding".
The success prompted the ACLU to broaden their freedom of speech efforts beyond labor and political speech, to encompass movies, press, radio and literature. The ACLU formed the National Committee on Freedom from Censorship in 1931 to coordinate this effort. By the early 1930s, censorship in the United States was diminishing.
Two major victories in the 1930s cemented the ACLUs campaign to promote free speech. In "Stromberg v. California", decided in 1931, the Supreme Court sided with the ACLU and affirmed the right of a communist party member to salute a communist flag. The result was the first time the Supreme Court used the Due Process Clause of the 14th amendment to subject states to the requirements of the First Amendment. In "Near v. Minnesota", also decided in 1931, the Supreme Court ruled that states may not exercise prior restraint and prevent a newspaper from publishing, simply because the newspaper had a reputation for being scandalous.
1930s.
The late 1930s saw the emergence of a new era of tolerance in the United States. National leaders hailed the Bill of Rights, particularly as it protected minorities, as the essence of democracy. The 1939 Supreme Court decision in "Hague v. Committee for Industrial Organization" affirmed the right of communists to promote their cause. Even conservative elements, such as the American Bar Association began to campaign for civil liberties, which were long considered to be the domain of left-leaning organizations. By 1940, the ACLU had achieved many of the goals it set in the 1920s, and many of its policies were the law of the land.
Expansion.
In 1929, after the Scopes and Dennett victories, Baldwin perceived that there was vast, untapped support for civil liberties in the United States. Baldwin proposed an expansion program for the ACLU, focusing on police brutality, Native American rights, African American rights, censorship in the arts, and international civil liberties. The board of directors approved Baldwin's expansion plan, except for the international efforts.
The ACLU played a major role in passing the 1932 Norris–La Guardia Act, a federal law which prohibited employers from preventing employees from joining unions, and stopped the practice of outlawing strikes, unions, and labor organizing activities with the use of injunctions. The ACLU also played a key role in initiating a nationwide effort to reduce misconduct (such as extracting false confessions) within police departments, by publishing the report "Lawlessness in Law Enforcement" in 1931, under the auspices of Herbert Hoover's Wickersham Commission. In 1934, the ACLU lobbied for the passage of the Indian Reorganization Act, which restored some autonomy to Native American tribes, and established penalties for kidnapping Native American children.
Although the ACLU deferred to the NAACP for litigation promoting civil liberties for African Americans, the ACLU did engage in educational efforts, and published "Black Justice" in 1931, a report which documented institutional racism throughout the South, including lack of voting rights, segregation, and discrimination in the justice system. Funded by the Garland Fund, the ACLU also participated in producing the influential Margold Report, which outlined a strategy to fight for civil rights for blacks. The ACLU's plan was to demonstrate that the "separate but equal" policies governing the Southern discrimination were illegal because blacks were never, in fact, treated equally.
Depression era and the New Deal.
In 1932twelve years after the ACLU was foundedit had achieved significant success; the Supreme Court had embraced the free speech principles espoused by the ACLU, and the general public was becoming more supportive of civil rights in general. But the Great Depression brought new assaults on civil liberties; the year 1930 saw a large increase in the number of free speech prosecutions, a doubling of the number of lynchings, and all meetings of unemployed persons were banned in Philadelphia.
The Franklin D. Roosevelt administration proposed the New Deal to combat the depression. ACLU leaders were of mixed opinions about the New Deal, since many felt that it represented an increase in government intervention into personal affairs, and because the National Recovery Administration suspended anti-trust legislation. Roosevelt was not personally interested in civil rights, but did appoint many civil libertarians to key positions, including Interior Secretary Harold Ickes, a member of the ACLU.
The economic policies of the New Deal leaders were often aligned with ACLU goals, but social goals were not. In particular, movies were subject to a barrage of local ordinances banning screenings that were deemed immoral or obscene. Even public health films portraying pregnancy and birth were banned; as was "Life" magazine's April 11, 1938 issue which included photos of the birth process. The ACLU fought these bans, but did not prevail.
The Catholic Church attained increasing political influence in the 1930s, and used its influence to promote censorship of movies, and to discourage publication of birth control information. This conflict between the ACLU and the Catholic Church led to the resignation of the last Catholic priest from ACLU leadership in 1934; a Catholic priest would not be represented there again until the 1970s.
The ACLU took no official position on president Franklin Delano Roosevelt's 1937 court-packing plan, which threatened to increase the number of Supreme Court justices, unless the Supreme Court reversed its course and began approving New Deal legislation. The Supreme Court responded by making a major shift in policy, and no longer applied strict constitutional limits to government programs, and also began to take a more active role in protecting civil liberties.
The first decision that marked the court's new direction was "De Jonge v. Oregon", in which a communist labor organizer was arrested for calling a meeting to discuss unionization. The ACLU attorney Osmond Fraenkel, working with International Labor Defense, defended De Jonge in 1937, and won a major victory when the Supreme Court ruled that "peaceable assembly for lawful discussion cannot be made a crime." The De Jonge case marked the start of an era lasting for a dozen years, during which Roosevelt appointees (led by Hugo Black, William O. Douglas, and Frank Murphy) established a body of civil liberties law. In 1938, Justice Harlan F. Stone wrote the famous "footnote four" in "United States v. Carolene Products Co." in which he suggested that state laws which impede civil liberties wouldhenceforthrequire compelling justification.
Senator Robert F. Wagner proposed the National Labor Relations Act in 1935, which empowered workers to unionize. Ironically, the ACLU, after 15 years of fighting for workers' rights, initially opposed the act (it later took no stand on the legislation) because some ACLU leaders feared the increased power the bill gave to the government. The newly formed National Labor Relations Board (NLRB) posed a dilemma for the ACLU, because in 1937 it issued an order to Henry Ford, prohibiting Ford from disseminating anti-union literature. Part of the ACLU leadership habitually took the side of labor, and that faction supported the NLRB's action. But part of the ACLU supported Ford's right to free speech. ACLU leader Arthur Garfield Hays proposed a compromise (supporting the auto workers union, yet also endorsing Ford's right to express personal opinions), but the schism highlighted a deeper divide that would become more prominent in the years to come.
The ACLU's support of the NLRB was a major development for the ACLU, because it marked the first time it accepted that a government agency could be responsible for upholding civil liberties. Until 1937, the ACLU felt that civil rights were best upheld by citizens and private organizations.
Some factions in the ACLU proposed new directions for the organization. In the late 1930s, some local affiliates proposed shifting their emphasis from civil liberties appellate actions, to becoming a legal aid society, centered on store front offices in low income neighborhoods. The ACLU directors rejected that proposal. Other ACLU members wanted the ACLU to shift focus into the political arena, and to be more willing to compromise their ideals in order to strike deals with politicians. This initiative was also rejected by the ACLU leadership.
Jehovah's Witnesses.
The ACLU's support of defendants with unpopular, sometimes extreme, viewpoints have produced many landmark court cases and established new civil liberties. One such defendant was the Jehovah's Witnesses, who were involved in a large number of Supreme Court cases. Cases that the ACLU supported included "Lovell v. City of Griffin" (which struck down a city ordinance that required a permit before a person could distribute "literature of any kind"); "Martin v. Struthers" (which struck down an ordinance prohibiting door-to-door canvassing); and "Cantwell v. Connecticut" (which reversed the conviction of a Witness who was reciting offensive speech on a street corner).
The most important cases involved statutes requiring flag salutes. The Jehovah's Witnesses felt that saluting a flag was contrary to their religious beliefs. Two children were convicted in 1938 of not saluting the flag. The ACLU supported their appeal to the Supreme Court, but the court affirmed the conviction, in 1940. But three years later, in "West Virginia State Board of Education v. Barnette", the Supreme court reversed itself and wrote "If there is any fixed star in our constitutional constellation, it is that no official, high or petty, can prescribe what shall be orthodox in politics, nationalism, religion, or other matters of opinion or force citizens to confess by word or act their faith therein." To underscore its decision, the Supreme Court announced it on Flag Day.
Communism and totalitarianism.
The rise of totalitarianism in Germany, Russia, and Italy during World War II had a tremendous impact on the civil liberties movement. On the one hand, the oppression of the totalitarian states put into sharp relief the virtue of freedom of speech and association in the United States; on the other hand, they prompted an anti-communist hysteria in America which eroded many civil liberties.
The ACLU leadership was divided over whether or not to defend pro-Nazi speech in the United States; pro-labor elements within the ACLU were hostile towards Nazism and fascism, and objected when the ACLU defended Nazis. Several states passed laws outlawing the hate speech directed at ethnic groups. The first person arrested under New Jersey's 1935 hate speech law was a Jehovah's Witness who was charged with disseminating anti-Catholic literature. The ACLU defended the Jehovah's Witnesses, and the charges were dropped. The ACLU proceeded to defend numerous pro-Nazi groups, defending their rights to free speech and free association.
In the late 1930s, the ACLU allied itself with the Popular Front, a coalition of liberal organizations coordinated by the United States Communist Party. The ACLU benefited because affiliates from the Popular Front could often fight local civil rights battles much more effectively than the New York-based ACLU. The association with the Communist Party led to accusations that the ACLU was a "communist front", particularly because Harry F. Ward was both chairman of the ACLU and chairman of the American League Against War and Fascism, a communist organization.
The House Unamerican Activities Committee (HUAC) was created in 1938 to uncover sedition and treason within the United States. When witnesses testified at its hearings, the ACLU was mentioned several times, leading the HUAC to mention the ACLU prominently in its 1939 report. This damaged the ACLU's reputation severely, even though the report said that it could not "definitely state whether or not" the ACLU was a communist organization.
While the ACLU rushed to defend its image against allegations of being a communist front, it also worked to protect witnesses who were being harassed by the HUAC. The ACLU was one of the few organizations to protest (unsuccessfully) against passage of the Smith Act in 1940, which would later be used to imprison many persons who supported Communism. The ACLU defended many persons who were prosecuted under the Smith Act, including labor leader Harry Bridges.
ACLU leadership was split on whether to purge its leadership of communists. Norman Thomas, John Haynes Holmes, and Morris Ernst were anti-communists who wanted to distance the ACLU from communism; opposing them were Harry Ward, Corliss Lamont and Elizabeth Flynn who rejected any political test for ACLU leadership. A bitter struggle ensued throughout 1939, and the anti-communists prevailed in February 1940, when the board voted to prohibit anyone who supported totalitarianism from ACLU leadership roles. Chairman Harry Ward immediately resigned, andfollowing a contentious six-hour debateElizabeth Flynn was voted off the ACLU's board. The 1940 resolution was a disaster for the ACLU, and considered by many to be a betrayal of its fundamental principles. The resolution was rescinded in 1968, and Flynn was posthumously reinstated to the ACLU in 1970.
Mid-century.
World War II.
When World War II engulfed the United States, the Bill of Rights was enshrined as a hallowed document, and numerous organizations defended civil liberties. Chicago and New York proclaimed "Civil Rights" weeks, and President Franklin Delano Roosevelt announced a national Bill of Rights day. Eleanor Roosevelt was the keynote speaker at the 1939 ACLU convention. In spite of this newfound respect for civil rights, Americans were becoming adamantly anti-communist, and believed that excluding communists from American society was an essential step to preserve democracy.
Contrasted with World War I, there was relatively little violation of civil liberties during World War II. President Roosevelt was a strong supporter of civil liberties, butmore importantlythere were few anti-war activists during World War II. The most significant exception was the internment of Japanese Americans. Two months after the Japanese attack on Pearl Harbor, Roosevelt authorized the creation of military "exclusion zones" with Executive Order 9066, paving the way for the detention of all West Coast Japanese Americans in inland camps. In addition to the non-citizen Issei (prohibited from naturalization as members of an "unassimilable" race), over two-thirds of those swept up were American-born citizens. The ACLU immediately protested to Roosevelt, comparing the evacuations to Nazi concentration camps. The ACLU was the only major organization to object to the internment plan, and their position was very unpopular, even within the organization. Not all ACLU leaders wanted to defend the Japanese Americans; Roosevelt loyalists such as Morris Ernst wanted to support Roosevelt's war effort, but pacifists such as Baldwin and Norman Thomas felt that Japanese Americans needed access to due process before they could be imprisoned. In a March 20, 1942 letter to Roosevelt, Baldwin called on the administration to allow Japanese Americans to prove their loyalty at individual hearings, describing the constitutionality of the planned removal "open to grave question." His suggestions went nowhere, and opinions within the organization became increasingly divided as the Army began the "evacuation" of the West Coast. In May, the two factions, one pushing to fight the exclusion orders then being issued, the other advocating support for the President's policy of removing citizens whose "presence may endanger national security," brought their opposing resolutions to a vote before the board and the ACLU's national leaders. They decided not to challenge the eviction of Japanese American citizens, and on June 22 instructions were sent to West Coast branches not to support cases that argued the government had no constitutional right to do so.
The ACLU offices on the West Coast had been more directly involved in addressing the tide of anti-Japanese prejudice from the start, as they were geographically closer to the issue, and were already working on cases challenging the exclusion by this time. The Seattle office, assisting in Gordon Hirabayashi's lawsuit, created an unaffiliated committee to continue the work the ACLU had started, while in Los Angeles, attorney A.L. Wirin continued to represent Ernest Kinzo Wakayama but without addressing the case's constitutional questions. (Wirin would lose private clients because of his defense of Wakayama and other Japanese Americans.) However, the San Francisco branch, led by Ernest Besig, refused to discontinue its support for Fred Korematsu, whose case had been taken on prior to the June 22 directive, and attorney Wayne Collins, with Besig's full support, centered his defense on the illegality of Korematsu's exclusion.
The West Coast offices had wanted a test case to take to court, but had a difficult time finding a Japanese American who was both willing to violate the internment orders and able to meet the ACLU's desired criteria of a sympathetic, Americanized plaintiff. Of the 120,000 Japanese Americans affected by the order, only 12 disobeyed, and Korematsu, Hirabayashi, and two others were the only resisters whose cases eventually made it to the Supreme Court. "Hirabayashi v. United States" came before the Court in May 1943, and the justices upheld the government's right to exclude Japanese Americans from the West Coast; although it had earlier forced its local office in L.A. to stop aiding Hirabayashi, the ACLU donated $1,000 to the case (over a third of the legal team's total budget) and submitted an "amicus" brief. Besig, dissatisfied with Osmond Fraenkel's tamer defense, filed an additional "amicus" brief that directly addressed Hirabayashi's constitutional rights. In the meantime, A.L. Wirin served as one of the attorneys in "Yasui v. United States" (decided the same day as the Hirabayashi case, and with the same results), but he kept his arguments within the perimeters established by the national office. The only case to receive a favorable ruling, "ex parte Endo", was also aided by two "amicus" briefs from the ACLU, one from the more conservative Fraenkel and another from the more putative Wayne Collins.
"Korematsu v. United States" proved to be the most controversial of these cases, as Besig and Collins refused to bow to national pressure to pursue the case without challenging the government's right to remove citizens from their homes. The ACLU board threatened to revoke the San Francisco branch's national affiliation, while Baldwin tried unsuccessfully to convince Collins to step down so he could replace him as lead attorney in the case. Eventually Collins agreed to present the case alongside Charles Horsky, although their arguments before the Supreme Court remained based in the unconstitutionality of the exclusion order Korematsu had disobeyed. The case was decided in December 1944, when the Court once again upheld the government's right to relocate Japanese Americans, although Korematsu's, Hirabayashi's and Yasui's convictions were later overturned in "coram nobis" proceedings in the 1980s.
Although the ACLU (somewhat unevenly) defended the Japanese Americans, it was more reluctant to defend anti-war protesters. A majority of the board passed a resolution in 1942 which declared the ACLU unwilling to defend anyone who interfered with the United States' war effort. Included in this group were the thousands of Nisei who renounced their U.S. citizenship during the war but later regretted the decision and tried to revoke their applications for "repatriation." (A significant number of those slated to "go back" to Japan had never actually been to the country and were in fact being deported rather than repatriated.) Ernest Besig had in 1944 visited the Tule Lake Segregation Center, where the majority of these "renunciants" were concentrated, and subsequently enlisted Wayne Collins' help to file a lawsuit on their behalf, arguing the renunciations had been given under duress. The national organization prohibited local branches from representing the renunciants, forcing Collins to pursue the case on his own, although Besig and the Northern California office provided some support.
When the war ended in 1945, the ACLU was 25 years old, and had accumulated an impressive set of legal victories. President Harry S. Truman sent a congratulatory telegram to the ACLU on the occasion of their 25th anniversary. American attitudes had changed since World War I, and dissent by minorities was tolerated with more willingness. The Bill of Rights was more respected, and minority rights were becoming more commonly championed. During their 1945 annual conference, the ACLU leaders composed a list of important civil rights issues to focus on in the future, and the list included racial discrimination and separation of church and state.
The ACLU supported the African-American defendants in "Shelley v. Kraemer", when they tried to occupy a house they had purchased in a neighborhood which had racially restrictive housing covenants. The African-American purchasers won the case in 1945.
Cold War era.
Anti-communist sentiment gripped the United States during the Cold War beginning in 1946. Federal investigations caused many persons with communist or left-leaning affiliations to lose their jobs, become blacklisted, or be jailed. During the Cold War, although the United States collectively ignored the civil rights of communists, other civil liberties—such as due process in law and separation of church and state—continued to be reinforced and even expanded.
The ACLU was internally divided when it purged communists from its leadership in 1940, and that ambivalence continued as it decided whether to defend alleged communists during the late 1940s. Some ACLU leaders were anti-communist, and felt that the ACLU should not defend any victims. Some ACLU leaders felt that communists were entitled to free speech protections, and the ACLU should defend them. Other ACLU leaders were uncertain about the threat posed by communists, and tried to establish a compromise between the two extremes. This ambivalent state of affairs would last until 1954, when the civil liberties faction prevailed, leading to the resignation of most of the anti-communist leaders.
In 1947, President Truman issued Executive Order 9835, which created the Federal Loyalty Program. This program authorized the Attorney General to create a list of organizations which were deemed to be subversive. Any association with these programs was ground for barring the person from employment. Listed organizations were not notified that they were being considered for the list, nor did they have an opportunity to present counterarguments; nor did the government divulge any factual basis for inclusion in the list. Although ACLU leadership was divided on whether to challenge the Federal Loyalty Program, some challenges were successfully made.
Also in 1947, the House Un-American Activities Committee (HUAC) subpoenaed ten Hollywood directors and writers, the "Hollywood Ten", intending to ask them to identify Communists, but the witnesses refused to testify. All were imprisoned for contempt of Congress. The ACLU supported the appeals of several of the artists, but lost on appeal. The Hollywood establishment panicked after the HUAC hearings, and created a blacklist which prohibited anyone with leftist associations from working. The ACLU supported legal challenges to the blacklist, but those challenges failed. The ACLU was more successful with an education effort; the 1952 report "The Judges and the Judged", prepared at the ACLU's direction in response to the blacklisting of actress Jean Muir, described the unfair and unethical actions behind the blacklisting process, and it helped gradually turn public opinion against McCarthyism.
The federal government took direct aim at the U.S. communist party in 1948 when it indicted its top twelve leaders in the Foley Square trial. The case hinged on whether or not mere membership in a totalitarian political party was sufficient to conclude that members advocated the overthrow of the United States government. The ACLU chose to not represent any of the defendants, and they were all found guilty and sentenced to three to five years in prison. Their defense attorneys were all cited for contempt, went to prison and were disbarred. When the government indicted additional party members, the defendants could not find attorneys to represent them. Communists protested outside the courthouse; a bill to outlaw picketing of courthouses was introduced in Congress, and the ACLU supported the anti-picketing law.
The ACLU, in a change of heart, supported the party leaders during their appeal process. The Supreme Court upheld the convictions in the "Dennis v. United States" decision by softening the free speech requirements from a "clear and present danger" test, to a "grave and probable" test. The ACLU issued a public condemnation of the "Dennis" decision, and resolved to fight it. One reason for the Supreme Court's support of cold war legislation was the 1949 deaths of Supreme Court justices Frank Murphy and Wiley Rutledge, leaving Hugo Black and William O. Douglas as the only remaining civil libertarians on the Court.
The "Dennis" decision paved the way for the prosecution of hundreds of other communist party members. The ACLU supported many of the communists during their appeals (although most of the initiative originated with local ACLU affiliates, not the national headquarters) but most convictions were upheld. The two California affiliates, in particular, felt the national ACLU headquarters was not supporting civil liberties strongly enough, and they initiated more cold war cases than the national headquarters did.
The ACLU also challenged many loyalty oath requirements across the country, but the courts upheld most of the loyalty oath laws. California ACLU affiliates successfully challenged the California state loyalty oath. The Supreme Court, until 1957, upheld nearly every law which restricted the liberties of communists.
The ACLU, even though it scaled back its defense of communists during the Cold War, still came under heavy criticism as a "front" for communism. Critics included the American Legion, Senator Joseph McCarthy, the HUAC, and the FBI. Several ACLU leaders were sympathetic to the FBI, and as a consequence, the ACLU rarely investigated any of the many complaints alleging abuse of power by the FBI during the Cold War.
Organizational change.
In 1950, the ACLU board of directors asked executive director Baldwin to resign, feeling that he lacked the organizational skills to lead the 9,000 (and growing) member organization. Baldwin objected, but a majority of the board elected to remove him from the position, and he was replaced by Patrick Murphy Malin. Under Malin's guidance, membership tripled to 30,000 by 1955the start of a 24-year period of continual growth leading to 275,000 members in 1974. Malin also presided over an expansion of local ACLU affiliates.
The ACLU, which had been controlled by an elite of a few dozen New Yorkers, became more democratic in the 1950s. In 1951, the ACLU amended its bylaws to permit the local affiliates to participate directly in voting on ACLU policy decisions. A bi-annual conference, open to the entire membership, was instituted in the same year, and in later decades it became a pulpit for activist members, who suggested new directions for the ACLU, including abortion rights, death penalty, and rights of the poor.
McCarthyism era.
During the early 1950s, the ACLU continued to steer a moderate course through the Cold War. When leftist singer Paul Robeson was denied a passport in 1950, even though he was not a communist and not accused of any illegal acts, the ACLU chose to not defend him. The ACLU later reversed their stance, and supported William Worthy and Rockwell Kent in their passport confiscation cases, which resulted in legal victories in the late 1950s.
In response to communist witch-hunts, many witnesses and employees chose to use the fifth amendment protection against self-incrimination to avoid divulging information about their political beliefs. Government agencies and private organizations, in response, established polices which inferred communist party membership for anyone who invoked the fifth amendment. The national ACLU was divided on whether to defend employees who had been fired merely for pleading the fifth amendment, but the New York affiliate successfully assisted teacher Harry Slochower in his Supreme Court case which reversed his termination.
The fifth amendment issue became the catalyst for a watershed event in 1954, which finally resolved the ACLU's ambivalence by ousting the anti-communists from ACLU leadership. In 1953, the anti-communists, led by Norman Thomas and James Fly, proposed a set of resolutions that inferred guilt of persons that invoked the fifth amendment. These resolutions were the first that fell under the ACLU's new organizational rules permitting local affiliates to participate in the vote; the affiliates outvoted the national headquarters, and rejected the anti-communist resolutions. Anti-communists leaders refused to accept the results of the vote, and brought the issue up for discussion again at the 1954 bi-annual convention. ACLU member Frank Graham, president of the University of North Carolina, attacked the anti-communists with a counter-proposal, which stated that the ACLU "stand against guilt by association, judgment by accusation, the invasion of privacy of personal opinions and beliefs, and the confusion of dissent with disloyalty." The anti-communists continued to battle Graham's proposal, but were outnumbered by the affiliates. The anti-communists finally gave up and departed the board of directors in late 1954 and 1955, ending an eight-year reign of ambivalence within the ACLU leadership ranks. Thereafter, the ACLU proceeded with firmer resolve against Cold War anti-communist legislation. The period from the 1940 resolution (and the purge of Elizabeth Flynn) to the 1954 resignation of the anti-communist leaders is considered by many to be an era in which the ACLU abandoned its core principles.
McCarthyism declined in late 1954 after television journalist Edward R. Murrow and others publicly chastised McCarthy. The controversies over the Bill of Rights that were generated by the Cold War ushered in a new era in American Civil liberties. In 1954 in "Brown v. Board of Education", the Supreme Court unanimously overturned state-sanctioned school segregation, and thereafter a flood of civil rights victories dominated the legal landscape.
The Supreme Court handed the ACLU two key victories in 1957, in "Watkins v. United States" and "Yates v. United States", both of which undermined the Smith Act and marked the beginning of the end of communist party membership inquiries. In 1965, the Supreme Court produced some decisions, including "Lamont v. Postmaster General" (in which the plaintiff was Corliss Lamont, a former ACLU board member), which upheld fifth amendment protections and brought an end to restrictions on political activity.
1960s.
The decade from 1954 to 1964 was the most successful period in the ACLU's history. Membership rose from 30,000 to 80,000, and by 1965 it had affiliates in seventeen states. During the ACLU's bi-annual conference in Colorado in 1964, the Supreme Court issued rulings on eight cases in which the ACLU was involved; the ACLU prevailed on seven of the eight. The ACLU played a role in Supreme Court decisions reducing censorship of literature and arts, protecting freedom of association, prohibiting racial segregation, excluding religion from public schools, and providing due process protection to criminal suspects. The ACLU's success arose from changing public attitudes; the American populace was more educated, more tolerant, and more willing to accept unorthodox behavior.
Separation of church and state.
Legal battles concerning the separation of church and state originated in laws dating to 1938 which required religious instruction in school, or provided state funding for religious schools. The Catholic church was a leading proponent of such laws; and the primary opponents (the "separationists") were the ACLU, Americans United for Separation of Church and State, and the American Jewish Congress. The ACLU led the challenge in the 1947 "Everson v. Board of Education" case, in which Justice Hugo Black wrote "he First Amendment has erected a wall between church and state…. That wall must be kept high and impregnable." It was not clear that the Bill of Rights forbid state governments from supporting religious education, and strong legal arguments were made by religious proponents, arguing that the Supreme Court should not act as a "national school board", and that the Constitution did not govern social issues. However, the ACLU and other advocates of church/state separation persuaded the Court to declare such activities unconstitutional. Historian Samuel Walker writes that the ACLU's "greatest impact on American life" was its role in persuading the Supreme Court to "constitutionalize" so many public controversies.
In 1948, the ACLU prevailed in the "McCollum v. Board of Education" case, which challenged public school religious classes taught by clergy paid for from private funds. The ACLU also won cases challenging schools in New Mexico which were taught by clergy and had crucifixes hanging in the classrooms. In the 1960s, the ACLU, in response to member insistence, turned its attention to in-class promotion of religion. In 1960, 42 percent of American schools included Bible reading. In 1962, the ACLU published a policy statement condemning in-school prayers, observation of religious holidays, and Bible reading. The Supreme Court concurred with the ACLU's position, when it prohibited New York's in-school prayers in the 1962 "Engel v. Vitale" decision. Religious factions across the country rebelled against the anti-prayer decisions, leading them to propose the School Prayer Constitutional Amendment, which declared in-school prayer legal. The ACLU participated in a lobbying effort against the amendment, and the 1966 congressional vote on the amendment failed to obtain the required two-thirds majority.
However, not all cases were victories; ACLU lost cases in 1949 and 1961 which challenged state laws requiring commercial businesses to close on Sunday, the Christian Sabbath. The Supreme court has never overturned such laws, although some states subsequently revoked many of the laws under pressure from commercial interests.
Freedom of expression.
During the 1940s and 1950s, the ACLU continued its battle against censorship of art and literature. In 1948, the New York affiliate of the ACLU received mixed results from the Supreme Court, winning the appeal of Carl Jacob Kunz, who was convicted for speaking without a police permit, but losing the appeal of Irving Feiner who was arrested to prevent a breach of the peace, based on his oration denouncing president Truman and the American Legion. The ACLU lost the case of Joseph Beahharnais, who was arrested for group libel when he distributed literature impugning the character of African Americans.
Cities across America routinely banned movies because they were deemed to be "harmful", "offensive", or "immoral"censorship which was validated by the 1915 "Mutual v. Ohio" Supreme Court decision which held movies to be mere commerce, undeserving of first amendment protection. The film "The Miracle" was banned in New York in 1951, at the behest of the Catholic Church, but the ACLU supported the film's distributor in an appeal of the ban, and won a major victory in the 1952 decision "Joseph Burstyn, Inc. v. Wilson". The Catholic Church led efforts throughout the 1950s attempting to persuade local prosecutors to ban various books and movies, leading to conflict with the ACLU when the ACLU published it statement condemning the church's tactics. Further legal actions by the ACLU successfully defended films such as "M" and "la Ronde", leading the eventual dismantling of movie censorship. Hollywood continued employing self-censorship with its own Production Code, but in 1956 the ACLU called on Hollywood to abolish the Code.
The ACLU defended beat generation artists, including Allen Ginsberg who was prosecuted for his poem "Howl"; andin an unorthodox case the ACLU helped a coffee house regain its restaurant license which was revoked because its Beat customers were allegedly disturbing the peace and quiet of the neighborhood.
The ACLU lost an important press censorship case when, in 1957, the Supreme Court upheld the obscenity conviction of publisher Samuel Roth for distributing adult magazines. As late as 1953, books such as "Tropic of Cancer" and "From Here to Eternity" were still banned. But public standards rapidly became more liberal though the 1960s, and obscenity was notoriously difficult to define, so by 1971 prosecutions for obscenity had halted.
Racial discrimination.
A major aspect of civil liberties progress after World War II was the undoing centuries of racism in federal, state, and local governments an effort generally known as the Civil Rights Movement. Several civil liberties organizations worked together for progress, including the National Association for the Advancement of Colored People (NAACP), the ACLU, and the American Jewish Congress. The NAACP took primary responsibility for Supreme Court cases (often led by lead NAACP attorney Thurgood Marshall), with the ACLU focusing on police misconduct, and supporting the NAACP with amicus briefs. The NAACP achieved a key victory in 1950 with the "Henderson v. United States" decision that ended segregation in interstate bus and rail transportation.
In 1954, the ACLU filed an amicus brief in the case of "Brown v. Board of Education", which led to the ban on racial segregation in U.S. public schools. Southern states instituted a McCarthyism-style witch-hunt against the NAACP, attempting it to disclose membership lists. The ACLU's fight against racism was not limited to segregation; in 1964 the ACLU provided key support to plaintiffs, primarily lower income urban residents, in "Reynolds v. Sims", which required states to establish the voting districts in accordance with the "one person, one vote" principle.
Police misconduct.
The ACLU regularly tackled police misconduct issues, starting with the 1932 case "Powell v. Alabama" (right to an attorney), and including 1942's "Betts v. Brady" (right to an attorney), and 1951's "Rochin v. California" (involuntary stomach pumping). In the late 1940s, several ACLU local affiliates established permanent committees to address policing issues. During the 1950s and 1960s, the ACLU was responsible for substantially advancing the legal protections against police misconduct. The Philadelphia affiliate was responsible for causing the City of Philadelphia, in 1958, to create the nation's first civilian police review board. In 1959, the Illinois affiliate published the first report in the nation, "Secret Detention by the Chicago Police", which documented unlawful detention by police.
Some of the most well known ACLU successes came in the 1960s, when the ACLU prevailed in a string of cases limiting the power of police to gather evidence; in 1961's "Mapp v. Ohio", the Supreme court required states to obtain a warrant before searching a person's home. The "Gideon v. Wainwright" decision in 1963 provided legal representation to indigents. In 1964, the ACLU persuaded the Court, in "Escobedo v. Illinois", to permit suspects to have an attorney present during questioning. And, in 1966, the "Miranda v. Arizona" decision required police to notify suspects of their constitutional rights. Although many law enforcement officials criticized the ACLU for expanding the rights of suspects, police officers themselves took advantage of the ACLU. For example, when the ACLU represented New York policemen in their lawsuit which objected to searches of their workplace lockers. In the late 1960s, civilian review boards in New York and Philadelphia were abolished, over the ACLU's objection.
Civil liberties revolution of the 1960s.
The 1960s was a tumultuous era in the United States, and public interest in civil liberties underwent an explosive growth. Civil liberties actions in the 1960s were often led by young people, and often employed tactics such as sit ins and marches. Protests were often peaceful, but sometimes employed militant tactics. The ACLU played a central role in all major civil liberties debates of the 1960s, including new fields such as gay rights, prisoner's rights, abortion, rights of the poor, and the death penalty. Membership in the ACLU increased from 52,000 at the beginning of the decade, to 104,000 in 1970. In 1960, there were affiliates in seven states, and by 1974 there were affiliates in 46 states. During the 1960s, the ACLU underwent a major transformation tactics; it shifted emphasis from legal appeals (generally involving amicus briefs submitted to the Supreme Court) to direct representation of defendants when they were initially arrested. At the same time, the ACLU transformed its style from "disengaged and elitist" to "emotionally engaged". The ACLU published a breakthrough document in 1963, titled "How Americans Protest", which was borne of frustration with the slow progress in battling racism, and which endorsed aggressive, even militant protest techniques.
African-American protests in the South accelerated in the early 1960s, and the ACLU assisted at every step. After four African-American college students staged a sit-in in a segregated North Carolina department store, the sit-in movement gained momentum across the United States. During 1960-61, the ACLU defended black students arrested for demonstrating in North Carolina, Florida, and Louisiana. The ACLU also provided legal help for the Freedom Rides in 1961, the integration of the University of Mississippi, the 1963 protests in Birmingham, Alabama, and the 1964 Freedom Summer.
The NAACP was responsible for managing most sit-in related cases that made it to the Supreme Court, winning nearly every decision. But it fell to the ACLU and other legal volunteer efforts to provide legal representation to hundreds of protestorswhite and blackwho were arrested while protesting in the South. The ACLU joined with other civil liberties groups to form the Lawyers Constitutional Defense Committee (LCDC) which subsequently provided legal representation to many of the protesters. The ACLU provided the majority of the funding for the LCDC.
In 1964, the ACLU opened up a major office in Atlanta, Georgia, dedicated to serving Southern issues. Much of the ACLU's progress in the South was due to Charles Morgan, Jr., the charismatic leader of the Atlanta office. He was responsible for desegregating juries ("Whitus v. Georgia"), desegregating prisons ("Lee v. Washington"), and reforming election laws. The ACLU's southern office also defended African-American congressman Julian Bond in "Bond v. Floyd", when the Georgia congress refused to formally induct Bond into the legislature. Another widely publicized case defended by Morgan was that of Army doctor Howard Levy, who was convicted of refusing to train Green Berets. Despite raising the defense that the Green Berets were committing war crimes in Vietnam, Levy lost on appeal in "Parker v. Levy", 417 U.S. 733 (1974).
In 1969, the ACLU won a major victory for free speech, when it defended Dick Gregory after he was arrested for peacefully protesting against the mayor of Chicago. The court ruled in "Gregory v. Chicago" that a speaker cannot be arrested for disturbing the peace when the hostility is initiated by someone in the audience, as that would amount to a "heckler's veto".
Vietnam war.
The ACLU was at the center of several legal aspects of the Vietnam war: defending draft resisters, challenging the constitutionality of the war, the potential impeachment of Richard Nixon, and the use of national security concerns to preemptively censor newspapers.
David J. Miller was the first person prosecuted for burning his draft card. The New York affiliate of the ACLU appealed his 1965 conviction (367 F.2d 72: "United States of America v. David J. Miller", 1966), but the Supreme Court refused to hear the appeal. Two years later, the Massachusetts affiliate took the card-burning case of David O'Brien to the Supreme Court, arguing that the act of burning was a form of symbolic speech, but the Supreme Court upheld the conviction in "United States v. O'Brien", 391 US 367 (1968). Thirteen-year-old Junior High student Mary Tinker wore a black armband to school in 1965 to object to the war, and was suspended from school. The ACLU appealed her case to the Supreme Court and won a victory in "Tinker v. Des Moines Independent Community School District". This critical case established that the government may not establish "enclaves" such as schools or prisons where all rights are forfeit.
The ACLU defended Sydney Street, who was arrested for burning an American flag to protest the reported assassination of civil rights leader James Meredith. In the "Street v. New York" decision, the court agreed with the ACLU that encouraging the country to abandon one of its national symbols was constitutionally protected form of expression. The ACLU successfully defended Paul Cohen, who was arrested for wearing a jacket with the words "fuck the draft" on its back, while he walked through the Los Angeles courthouse. The Supreme Court, in "Cohen v. California", held that the vulgarity of the wording was essential to convey the intensity of the message.
Non-war related free speech rights were also advanced during the Vietnam war era; in 1969, the ACLU defended a Ku Klux Klan member who advocated long-term violence against the government, and the Supreme Court concurred with the ACLU's argument in the landmark decision "Brandenburg v. Ohio", which held that only speech which advocated "imminent" violence could be outlawed.
A major crisis gripped the ACLU in 1968 when a debate erupted over whether to defend Benjamin Spock and the Boston Five against federal charges that they encouraged draftees to avoid the draft. The ACLU board was deeply split over whether to defend the activists; half the board harbored anti-war sentiments, and felt that the ACLU should lend its resources to the cause of the Boston Five. The other half of the board believed that civil liberties were not at stake, and the ACLU would be taking a political stance. Behind the debate was the longstanding ACLU tradition that it was politically impartial, and provided legal advice without regard to the political views of the defendants. The board finally agreed to a compromise solution that permitted the ACLU to defend the anti-war activists, without endorsing the activist's political views. Some critics of the ACLU suggest that the ACLU became a partisan political organization following the Spock case. After the Kent State shootings in 1970, ACLU leaders took another step towards politics by passing a resolution condemning the Vietnam war. The resolution was based in a variety of legal arguments, including civil liberties violations and a claim that the war was illegal.
Also in 1968, the ACLU held an internal symposium to discuss its dual roles: providing "direct" legal support (defense for accused in their initial trial, benefiting only the individual defendant), and appellate support (providing amicus briefs during the appeal process, to establish widespread legal precedent). Historically, the ACLU was known for its appellate work which led to landmark Supreme Court decisions, but by 1968, 90% of the ACLU's legal activities involved direct representation. The symposium concluded that both roles were valid for the ACLU.
1970s and 1980s.
Watergate era.
The ACLU supported "The New York Times" in its 1971 suit against the government, requesting permission to publish the Pentagon papers. The court upheld the "Times" and ACLU in the "New York Times Co. v. United States" ruling, which held that the government could not preemptively prohibit the publication of classified information and had to wait until after it was published to take action.
As the Watergate saga unfolded, the ACLU became the first national organization to call for Nixon's impeachment. This, following the resolution opposing the Vietnam war, was a second major decision that caused critics of the ACLU, particularly conservatives, to claim that the ACLU had evolved into a liberal political organization.
Enclaves and new civil liberties.
The decade from 1965 to 1975 saw an expansion of the field of civil liberties. Administratively, the ACLU responded by appointing Aryeh Neier to take over from Pemberton as Executive Director in 1970. Neier embarked on an ambitious program to expand the ACLU; he created the ACLU Foundation to raise funds, and he created several new programs to focus the ACLU's legal efforts. By 1974, ACLU membership had reached 275,000.
During those years, the ACLU led the way in expanding legal rights in three directions: new rights for persons within government-run "enclaves", new rights for victim groups, and privacy rights for mainstream citizens. At the same time, the organization grew substantially. The ACLU helped develop the field of constitutional law that governs "enclaves", which are groups of persons that live in conditions under government control. Enclaves include mental hospital patients, members of the military, and prisoners, and students (while at school). The term enclave originated with Supreme Court justice Abe Fortas's use of the phrase "schools may not be enclaves of totalitarianism" in the "Tinker v. Des Moines" decision.
The ACLU initiated the legal field of student's rights with the "Tinker v. Des Moines" case, and expanded it with cases such as "Goss v. Lopez" which required schools to provide students an opportunity to appeal suspensions.
As early as 1945, the ACLU had taken a stand to protect the rights of the mentally ill, when it drafted a model statute governing mental commitments. In the 1960s, the ACLU opposed involuntary commitments, unless it could be demonstrated that the person was a danger to himself or the community. In the landmark 1975 "O'Connor v. Donaldson" decision the ACLU represented a non-violent mental health patient who had been confined against his will for 15 years, and persuaded the Supreme Court to rule such involuntary confinements illegal. The ACLU has also defended the rights of mentally ill individuals who are not dangerous, but who create disturbances. The New York chapter of the ACLU defended Billie Boggs, a mentally ill woman who exposed herself and defecated and urinated in public.
Prior to 1960, prisoners had virtually no recourse to the court system, because courts considered prisoners to have no civil rights. That changed in the late 1950s, when the ACLU began representing prisoners that were subject to police brutality, or deprived of religious reading material. In 1968, the ACLU successfully sued to desegregate the Alabama prison system; and in 1969, the New York affiliate adopted a project to represent prisoners in New York prisons. Private attorney Phil Hirschkop discovered degrading conditions in Virginia prisons following the Virginia State Penitentiary strike, and won an important victory in 1971's "Landman v. Royster" which prohibited Virginia from treating prisoners in inhumane ways. In 1972, the ACLU consolidated several prison rights efforts across the nation and created the National Prison Project. The ACLU's efforts led to landmark cases such as "Ruiz v. Estelle" (requiring reform of the Texas prison system) and in 1996 U.S. Congress enacted the Prison Litigation Reform Act (PLRA) which codified prisoners' rights.
Victim groups.
The ACLU, during the 1960s and 1970s, expanded its scope to include what it referred to as "victim groups", namely women, the poor, and homosexuals. Heeding the call of female members, the ACLU endorsed the Equal Rights Amendment in 1970 and created the Women's Rights Project in 1971. The Women's Rights Project dominated the legal field, handling more than twice as many cases as the National Organization for Women, including breakthrough cases such as "Reed v. Reed", "Frontiero v. Richardson", and " Taylor v. Louisiana".
ACLU leader Harriet Pilpel raised the issue of the rights of homosexuals in 1964, and two years later the ACLU formally endorsed gay rights. In 1972, ACLU cooperating attorneys in Oregon filed the first federal civil rights case involving a claim of unconstitutional discrimination against a gay or lesbian public school teacher. The U.S. District Court held that a state statute that authorized school districts to fire teachers for "immorality" was unconstitutionally vague, and awarded monetary damages to the teacher. The court refused to reinstate the teacher, and the Ninth Circuit Court of Appeals affirmed that refusal by a 2 to 1 vote. Burton v. Cascade School District, 353 F. Supp. 254 (D. Or. 1972), aff'd 512 F.2d 850 (1975). In 1973 the ACLU created the Sexual Privacy Project (later the Gay and Lesbian Rights Project) which combated discrimination against homosexuals. This support continues even today. After then-Senator Larry Craig was arrested for soliciting sex in a public bathroom, the ACLU wrote an amicus brief for Craig, saying that sex between consenting adults in public places was protected under privacy rights.
Rights of the poor was another area that was expanded by the ACLU. In 1966 and again in 1968, activists within the ACLU encouraged the organization to adopt a policy overhauling the welfare system, and guaranteeing low-income families a baseline income; but the ACLU board did not approve the proposals. The ACLU played a key role in the 1968 "King v. Smith" decision, where the Supreme Court ruled that welfare benefits for children could not be denied by a state simply because the mother cohabited with a boyfriend.
Privacy.
The right to privacy is not explicitly identified in the U.S. Constitution, but the ACLU led the charge to establish such rights in the indecisive 1961 "Poe v. Ullman" case, which addressed a state statute outlawing contraception. The issue arose again in "Griswold v. Connecticut" (1965), and this time the Supreme Court adopted the ACLU's position, and formally declared a right to privacy. The New York affiliate of the ACLU pushed to eliminate anti-abortion laws starting in 1964, a year before "Griswold" was decided, and in 1967 the ACLU itself formally adopted the right to abortion as a policy. The ACLU led the defense in "United States v. Vuitch" which expanded the right of physicians to determine when abortions were necessary. These efforts culminated in one of the most controversial Supreme Court decisions of all time, "Roe v. Wade", which legalized abortion in the first three months of pregnancy. The ACLU successfully argued against state bans on interracial marriage, in the case of "Loving v. Virginia" (1967).
Related to privacy, the ACLU engaged in several battles to ensure that government records about individuals were kept private, and to give individuals the right to review their records. The ACLU supported several measures, including the 1970 Fair Credit Reporting Act required credit agencies to divulge credit information to individuals; the 1973 Family Educational Rights and Privacy Act, which provided students the right to access their records; and the 1974 Privacy Act which prevented the federal government from disclosing personal information without good cause.
Allegations of bias.
In the early 1970s, conservatives and libertarians began to criticize the ACLU for being too political and too liberal. Legal scholar Joseph W. Bishop wrote that the ACLU's trend to partisanship started with its defense of Dr. Spock's anti-war protests. Critics also blamed the ACLU for encouraging the Supreme Court to embrace judicial activism. Critics claimed that the ACLU's support of controversial decisions like "Roe v. Wade" and "Griswold v. Connecticut" violated the intention of the authors of the Bill of Rights. The ACLU became an issue in the 1988 presidential campaign, when Republican candidate George H. W. Bush accused Democratic candidate Michael Dukakis (a member of the ACLU) of being a "card carrying member of the ACLU".
The Skokie case.
It is the policy of the ACLU to support the civil liberties of defendants regardless of their ideological stance. The ACLU takes pride in defending individuals with unpopular viewpoints, such as George Wallace, George Lincoln Rockwell, and KKK members. The ACLU has defended American Nazis many times, and their actions often brought protests, particularly from American Jews.
In 1977, a small group of American Nazis, led by Frank Collin, applied to the town of Skokie, Illinois for permission to hold a demonstration in the town park. Skokie at the time had a majority population of Jews, totaling 40,000 of 70,000 citizens, some of whom were survivors of Nazi concentration camps. Skokie refused to grant permission, and an Illinois judge supported Skokie and prohibited the demonstration. Skokie immediately passed three ordinances aimed at preventing the group from meeting in Skokie. The ACLU assisted Collin and appealed to federal court. The appeal dragged on for a year, and the ACLU eventually prevailed in "Smith v. Collin", 447 F.Supp. 676.
The Skokie case was heavily publicized across America, partially because Jewish groups such as the Jewish Defense League and Anti Defamation League strenuously objected to the demonstration, leading many members of the ACLU to cancel their memberships. The Illinois affiliate of the ACLU lost about 25% of its membership and nearly one-third of its budget. The financial strain from the controversy led to layoffs at local chapters. After the membership crisis died down, the ACLU sent out a fund-raising appeal which explained their rationale for the Skokie case, and raised over $500,000 ($ in 2016 dollars).
Reagan era.
The inauguration of Ronald Reagan as president in 1981, ushered in an eight-year period of conservative leadership in the U.S. government. Under his leadership, the government pushed a conservative social agenda, including outlawing abortion, inserting prayer in schools, banning pornography, and resisting gay rights.
Fifty years after the Scopes trial, the ACLU found itself fighting another classroom case, the Arkansas 1981 creationism statute, which required schools to teach the biblical account of creation as a scientific alternative to evolution. The ACLU won the case in the "McLean v. Arkansas" decision.
In 1982, the ACLU became involved in a case involving the distribution of child pornography ("New York v. Ferber"). In an amicus brief, the ACLU argued that child pornography that violates the three prong obscenity test should be outlawed, but that the law in question was overly restrictive because it outlawed artistic displays and otherwise non-obscene material. The court did not adopt the ACLU's position.
During the 1988 presidential election, Vice President George H. W. Bush noted that his opponent Massachusetts Governor Michael Dukakis had described himself as a "card-carrying member of the ACLU" and used that as evidence that Dukakis was "a strong, passionate liberal" and "out of the mainstream". The phrase subsequently was used by the organization in an advertising campaign.
In 1990 the ACLU defended Lieutenant Colonel Oliver North, whose conviction was tainted by coerced testimonya violation of his fifth amendment rightsduring the Iran–Contra affair, where Oliver North was involved in illegal weapons sales to Iran in order to illegally fund the Contra guerillas.
Modern era.
1990 to 2000.
In 1997, ruling unanimously in the case of "Reno v. American Civil Liberties Union", the Supreme Court voted down anti-indecency provisions of the Communications Decency Act (the CDA), finding they violated the freedom of speech provisions of the First Amendment. In their decision, the Supreme Court held that the CDA's "use of the undefined terms 'indecent' and 'patently offensive' will provoke uncertainty among speakers about how the two standards relate to each other and just what they mean."
The ACLU's position on spam is considered controversial by a broad cross-section of political points of view. In 2000, Marvin Johnson, a legislative counsel for the ACLU, stated that proposed anti-spam legislation infringed on free speech by denying anonymity and by forcing spam to be labeled as such, "Standardized labeling is compelled speech." He also stated, "It's relatively simple to click and delete." The debate found the ACLU joining with the Direct Marketing Association and the Center for Democracy and Technology in criticizing a bipartisan bill in the House of Representatives in 2000. As early as 1997 the ACLU had taken a strong position that nearly all spam legislation was improper, although it has supported "opt-out" requirements in some cases. The ACLU opposed the 2003 CAN-SPAM act suggesting that it could have a chilling effect on speech in cyberspace.
In November 2000, 15 African-American residents of Hearne, Texas, were indicted on drug charges after being arrested in a series of "drug sweeps". The ACLU filed a class action lawsuit, "Kelly v. Paschall", on their behalf, alleging that the arrests were unlawful. The ACLU contended that 15 percent of Hearne's male African American population aged 18 to 34 were arrested based on the "uncorroborated word of a single unreliable confidential informant coerced by police to make cases." On May 11, 2005, the ACLU and Robertson County announced a confidential settlement of the lawsuit, an outcome which "both sides stated that they were satisfied with." The District Attorney dismissed the charges against the plaintiffs of the suit. The 2009 film American Violet depicts this case.
In 2000, the ACLU's Massachusetts affiliate represented the North American Man Boy Love Association (NAMBLA), on first amendment grounds, in the "Curley v. NAMBLA" wrongful death civil suit that was based solely on the fact that a man who raped and murdered a child had visited the NAMBLA website. Also In 2000, the ACLU lost the "Boy Scouts of America v. Dale" case, which had asked the Supreme Court to require the Boy Scouts of America to drop their policy of prohibiting homosexuals from becoming Boy Scout leaders.
Twenty-first century.
In March 2004, the ACLU, along with Lambda Legal and the National Center for Lesbian Rights, sued the state of California on behalf of six same-sex couples who were denied marriage licenses. That case, "Woo v. Lockyer", was eventually consolidated into "In re Marriage Cases", the California Supreme Court case which led to same-sex marriage being available in that state from June 16, 2008 until Proposition 8 was passed on November 4, 2008.
During the 2004 trial regarding allegations of Rush Limbaugh's drug abuse, the ACLU argued that his privacy should not have been compromised by allowing law enforcement examination of his medical records. In June 2004, the school district in Dover, Pennsylvania, required that its high school biology students listen to a statement which asserted that the theory of evolution is not fact and mentioning intelligent design as an alternative theory. Several parents called the ACLU to complain, because they believed that the school was promoting a religious idea in the classroom and violating the Establishment Clause of the First Amendment. The ACLU, joined by Americans United for Separation of Church and State, represented the parents in a lawsuit against the school district. After a lengthy trial, Judge John E. Jones III ruled in favor of the parents in the "Kitzmiller v. Dover Area School District" decision, finding that intelligent design is not science and permanently forbidding the Dover school system from teaching intelligent design in science classes.
In April 2006, Edward Jones and the ACLU sued the City of Los Angeles, on behalf of Robert Lee Purrie and five other homeless people, for the city's violation of the 8th and 14th Amendments to the U.S. Constitution, and Article I, sections 7 and 17 of the California Constitution (supporting due process and equal protection, and prohibiting cruel and unusual punishment). The Court ruled in favor of the ACLU, stating that, "the LAPD cannot arrest people for sitting, lying, or sleeping on public sidewalks in Skid Row." Enforcement of section 41.18(d) 24 hours a day against persons who have nowhere else to sit, lie, or sleep, other than on public streets and sidewalks, is breaking these amendments. The Court said that the anti-camping ordinance is "one of the most restrictive municipal laws regulating public spaces in the United States". Jones and the ACLU wanted a compromise in which the LAPD is barred from enforcing section 41.18(d) (arrest, seizure, and imprisonment) in Skid Row between the hours of 9:00 p.m. and 6:30 a.m. The compromise plan permits the homeless to sleep on the sidewalk, provided they are not "within 10 feet of any business or residential entrance" and only between these hours. One of the motivations for the compromise is the shortage of space in the prison system. Downtown development business interests and the Central City Association (CCA) were against the compromise. Police Chief William Bratton said the case had slowed the police effort to fight crime and clean up Skid Row, and that when he was allowed to clean up Skid Row, real estate profited. On September 20, 2006, the Los Angeles City Council voted to reject the compromise. On October 3, 2006, police arrested Skid Row's transients for sleeping on the streets for the first time in months.
In 2006, the ACLU of Washington State joined with a pro-gun rights organization, the Second Amendment Foundation, and prevailed in a lawsuit against the North Central Regional Library District (NCRL) in Washington for its policy of refusing to disable restrictions upon an adult patron's request. Library patrons attempting to access pro-gun web sites were blocked, and the library refused to remove the blocks. In 2012, the ACLU sued the same library system for refusing to temporarily, at the request of an adult patron, disable Internet filters which blocked access to Google Images.
In 2006, the ACLU challenged a Missouri law that prohibited picketing outside of veterans' funerals. The suit was filed in support of the Westboro Baptist Church and Shirley Phelps-Roper, who were threatened with arrest. The Westboro Baptist Church is well known for their picket signs that contain messages such as, "God Hates Fags", "Thank God for Dead Soldiers" and "Thank God for 9/11". The ACLU issued a statement calling the legislation a "law that infringes on Shirley Phelps-Roper's rights to religious liberty and free speech". The ACLU prevailed in the lawsuit. In 2008, the ACLU was part of a consortium of legal advocates, including Lambda Legal and the National Center for Lesbian Rights, that challenged California's Proposition 8, which declared same-sex marriages illegal. The ACLU and its allies prevailed.
In light of the Supreme Court's "Heller" decision recognizing that the Constitution protects an individual right to bear arms, ACLU of Nevada took a position of supporting "the individual's right to bear arms subject to constitutionally permissible regulations" and pledged to "defend this right as it defends other constitutional rights". Since 2008, the ACLU has increasingly assisted gun owners recover firearms that have been seized illegally by law enforcement.
In 2009, the ACLU filed an amicus brief in "Citizens United v. FEC", arguing that the Bipartisan Campaign Reform Act of 2002 violated the First Amendment right to free speech by curtailing political speech. This stance on the landmark "Citizens United" case caused considerable disagreement within the organization, resulting in a discussion about its future stance during a quarterly board meeting in 2010. On March 27, 2012, the ACLU reaffirmed its stance in support of the Supreme Court's "Citizens United" ruling, at the same time voicing support for expanded public financing of election campaigns and stating the organization would firmly oppose any future constitutional amendment limiting free speech.
In 2010 the ACLU of Illinois was inducted into the Chicago Gay and Lesbian Hall of Fame as a Friend of the Community.
In 2011 the ACLU started its Don't Filter Me project, countering LGBT-related Internet censorship in public schools in the United States.
On January 7, 2013, the ACLU reached a settlement with the federal government in "Collins v. United States" that provided for the payment of full separation pay to servicemembers discharged under "don't ask, don't tell" since November 10, 2004, who had previously been granted only half that. Some 181 were expected to receive about $13,000 each.
Anti-terrorism issues.
After the September 11, 2001 attacks, the federal government instituted a broad range of new measures to combat terrorism, including the passage of the Patriot Act. The ACLU challenged many of the measures, claiming that they violated rights regarding due process, privacy, illegal searches, and cruel and unusual punishment. An ACLU policy statement states:
Our way forward lies in decisively turning our backs on the policies and practices that violate our greatest strength: our Constitution and the commitment it embodies to the rule of law. Liberty and security do not compete in a zero-sum game; our freedoms are the very foundation of our strength and security. The ACLU's National Security Project advocates for national security policies that are consistent with the Constitution, the rule of law, and fundamental human rights. The Project litigates cases relating to detention, torture, discrimination, surveillance, censorship, and secrecy.
During the ensuing debate regarding the proper balance of civil liberties and security, the membership of the ACLU increased by 20%, bringing the group's total enrollment to 330,000. The growth continued, and by August 2008 ACLU membership was greater than 500,000. It remained at that level through 2011.
The ACLU has been a vocal opponent of the USA PATRIOT Act of 2001, the PATRIOT 2 Act of 2003, and associated legislation made in response to the threat of domestic terrorism. In response to a requirement of the USA PATRIOT Act, the ACLU withdrew from the Combined Federal Campaign charity drive. The campaign imposed a requirement that ACLU employees must be checked against a federal anti-terrorism watch list. The ACLU has stated that it would "reject $500,000 in contributions from private individuals rather than submit to a government 'blacklist' policy."
In 2004, the ACLU sued the federal government in "American Civil Liberties Union v. Ashcroft" on behalf of Nicholas Merrill, owner of an Internet service provider. Under the provisions of the Patriot Act, the government had issued national security letters to Merrill to compel him to provide private Internet access information from some of his customers. In addition, the government placed a gag order on Merrill, forbidding him from discussing the matter with anyone.
In January 2006, the ACLU filed a lawsuit, "ACLU v. NSA", in a federal district court in Michigan, challenging government spying in the NSA warrantless surveillance controversy. On August 17, 2006, that court ruled that the warrantless wiretapping program is unconstitutional and ordered it ended immediately. However, the order was stayed pending an appeal. The Bush administration did suspend the program while the appeal was being heard. In February 2008, the U.S. Supreme Court turned down an appeal from the ACLU to let it pursue a lawsuit against the program that began shortly after the September 11 terror attacks.
The ACLU and other organizations also filed separate lawsuits around the country against telecommunications companies. The ACLU filed a lawsuit in Illinois ("Terkel v. AT&T") which was dismissed because of the state secrets privilege and two others in California requesting injunctions against AT&T and Verizon. On August 10, 2006, the lawsuits against the telecommunications companies were transferred to a federal judge in San Francisco.
The ACLU represents a Muslim-American who was detained but never accused of a crime in "Ashcroft v. al-Kidd", a civil suit against former Attorney General John Ashcroft. In January 2010, the American military released the names of 645 detainees held at the Bagram Theater Internment Facility in Afghanistan, modifying its long-held position against publicizing such information. This list was prompted by a Freedom of Information Act lawsuit filed in September 2009 by the ACLU, whose lawyers had also requested detailed information about conditions, rules and regulations.
The ACLU has also criticized targeted killings of American citizens who fight against the United States. In 2011 the ACLU criticized the killing of radical Muslim cleric Anwar al-Awlaki on the basis that it was a violation of his Fifth Amendment right to not be deprived of life, liberty, or property without due process of law.

</doc>
<doc id="1955" url="https://en.wikipedia.org/wiki?curid=1955" title="Adobe Systems">
Adobe Systems

Adobe Systems Incorporated is an American transnational computer software company. The company is headquartered in San Jose, California, United States. Adobe has historically focused upon the creation of multimedia and creativity software products, with a more-recent foray towards rich Internet application software development. It is best known for Photoshop, an image editing software. Adobe Reader, the Portable Document Format (PDF) and Adobe Creative Suite, as well as its successor Adobe Creative Cloud.
Adobe was founded in February 1982 by John Warnock and Charles Geschke, who established the company after leaving Xerox PARC in order to develop and sell the PostScript page description language. In 1985, Apple Computer licensed PostScript for use in its LaserWriter printers, which helped spark the desktop publishing revolution.
, Adobe Systems has about 13,500 employees, about 40% of whom work in San Jose. Adobe also has major development operations in Newton, Massachusetts; New York City, New York; Orlando, Florida; Minneapolis, Minnesota; Lehi, Utah; Seattle, Washington; San Francisco and San Luis Obispo, California in the United States.
History.
The name of the company, "Adobe", comes from Adobe Creek in Los Altos, California, which ran behind the houses of both of the company's founders. Adobe's corporate logo features a stylized "A" and was designed by the wife of John Warnock, Marva Warnock, who is a graphic designer.
Adobe's first products after PostScript were digital fonts, which they released in a proprietary format called Type 1. Apple subsequently developed a competing standard, TrueType, which provided full scalability and precise control of the pixel pattern created by the font's outlines, and licensed it to Microsoft. Adobe responded by publishing the Type 1 specification and releasing Adobe Type Manager, software that allowed WYSIWYG scaling of Type 1 fonts on screen - like TrueType without the precise pixel-level control. But these moves were too late to stop the rise of TrueType. Although Type 1 remained the standard in the graphics/publishing market, TrueType became the standard for business and the average Windows user. In 1996, Adobe and Microsoft announced the OpenType font format, and in 2003 Adobe completed converting its Type 1 font library to OpenType.
In the mid-1980s, Adobe entered the consumer software market with Adobe Illustrator, a vector-based drawing program for the Apple Macintosh. Illustrator, which grew from the firm's in-house font-development software, helped popularize PostScript-enabled laser printers. Unlike MacDraw, the then standard Macintosh vector drawing program, Illustrator described shapes with more flexible Bézier curves, providing unprecedented accuracy. Font rendering in Illustrator, however, was left to the Macintosh's QuickDraw libraries and would not be superseded by a PostScript-like approach until Adobe released Adobe Type Manager.
Adobe Systems entered NASDAQ in 1986. Its revenue has grown from roughly $1 billion in 1999 to roughly $4 billion in 2012. Adobe's fiscal years run from December to November. For example, the 2007 fiscal year ended on November 30, 2007.
In 1989, Adobe introduced what was to become its flagship product, a graphics editing program for the Macintosh called Photoshop. Stable and full-featured, Photoshop 1.0 was ably marketed by Adobe and soon dominated the market.
In 1993, Adobe introduced PDF, the Portable Document Format, and its Adobe Acrobat and Reader software. PDF is now an International Standard: ISO 32000-1:2008. The technology is adopted worldwide as a common medium for electronic documents.
In December 1991, Adobe released Adobe Premiere, which Adobe rebranded to Adobe Premiere Pro in 2003. In 1994, Adobe acquired Aldus and added Adobe PageMaker and Adobe After Effects to its product line later in the year; it also controls the TIFF file format. In 1995, Adobe added Adobe FrameMaker, the long-document DTP application, to its product line after Adobe acquired Frame Technology Corp. In 1996, Adobe Systems Inc added Ares Software Corp. In 1999, Adobe introduced Adobe InCopy as a direct competitor to QuarkCopyDesk.
In 1992, Adobe acquired OCR Systems, Inc.; in 1994, the company acquired Aldus Corporation. On May 30, 1997, Adobe reincorporated in Delaware by merging with and into Adobe Systems (Delaware), which had incorporated on May 9, 1997. Adobe Systems Incorporated (Delaware), the surviving corporation, changed its name to Adobe Systems Incorporated concurrently with the merger.
The company acquired GoLive Systems, Inc. and released Adobe GoLive in 1999 and began shipping Adobe InDesign as a direct competitor to QuarkXPress and as an eventual replacement for PageMaker. In May 2003, Adobe acquired Syntrillium Software, adding Adobe Audition to its product line. In December 2004, French company OKYZ S.A., makers of 3D collaboration software, was acquired. This acquisition added 3D technology and expertise to the Adobe Intelligent Document Platform.
On December 12, 2005, Adobe acquired its main rival Macromedia in a stock swap valued at about $3.4 billion, adding Adobe ColdFusion, Adobe Contribute, Adobe Captivate, Adobe Acrobat Connect (formerly Macromedia Breeze), Adobe Director, Adobe Dreamweaver, Adobe Fireworks, Adobe Flash, FlashPaper, Adobe Flex, Adobe FreeHand, Adobe HomeSite, Adobe JRun, Adobe Presenter, and Adobe Authorware to Adobe's product line.
On November 12, 2007, CEO, Bruce Chizen resigned. Effective December 1, he was replaced by Shantanu Narayen, Adobe's current president and Chief Operating Officer. Bruce Chizen served out his term on Adobe's Board of Directors, and then continued in a strategic advisory role until the end of Adobe's 2008 fiscal year.
Adobe released Adobe Media Player in April 2008. On April 27, Adobe discontinued development and sales of its older HTML/web development software, GoLive in favor of Dreamweaver. Adobe offered a discount on Dreamweaver for GoLive users and supports those who still use GoLive with online tutorials and migration assistance. On June 1, Adobe launched Acrobat.com, a series of web applications geared for collaborative work. Creative Suite 4, which includes Design, Web, Production Premium and Master Collection came out in October 2008 in six configurations at prices from about USD $1,700 to $2,500 or by individual application. The Windows version of Photoshop includes 64-bit processing. On December 3, 2008, Adobe laid off 600 of its employees (8% of the worldwide staff) citing the weak economic environment.
Adobe announced two acquisitions in 2009: on August 29, it purchased Business Catalyst, and on September 15, it bought Omniture. On November 10, the company laid off 680 employees. Adobe announced it was investigating a "coordinated attack" against corporate network systems in China, managed by the company.
Adobe's 2010 was marked by continuing front-and-back arguments with Apple over the latter's non-support for Adobe Flash on its iPhone, iPad and other products. Apple CEO Steve Jobs claimed that Flash was not reliable or secure enough, while Adobe executives have argued that Apple wish to maintain control over the iOS platform. In April 2010, Steve Jobs published a post titled "Thoughts on Flash" where he outlined his thoughts on Adobe Flash and the rise of HTML 5.
In July 2010, Adobe bought Day Software integrating their line of CQ Products: WCM, DAM, SOCO, and Mobile
In January 2011, Adobe acquired DemDex, Inc. with the intent of adding DemDex's audience-optimization software to its online marketing suite. At Photoshop World 2011, Adobe unveiled a new mobile photo service. Carousel is a new application for iPhone, iPad and Mac that uses Photoshop Lightroom technology for users to adjust and fine-tune images on all platforms. Carousel will also allow users to automatically sync, share and browse photos. The service was later renamed to "Adobe Revel". On November 9, 2011, Adobe announced that they would cease development of Flash for mobile devices following version 11.1. Instead it would focus on HTML 5 for mobile devices. On December 1, 2011, Adobe announced that it entered into a definitive agreement to acquire privately held Efficient Frontier.
In December 2012, Adobe opened a new 280,000 square foot corporate campus in Lehi, UT.
In 2013, Adobe Systems endured a major security breach. Vast portions of the source code for the company's software were stolen and posted online and over 150 million records of Adobe's customers have been made readily available for download. In 2012, about 40 million sets of payment card information were compromised by a hack of Adobe.
A class-action lawsuit alleging that the company suppressed employee compensation was filed against Adobe, and three other Silicon Valley-based companies in a California federal district court in 2013. In May 2014, it was revealed the four companies, Adobe, Apple, Google, and Intel had reached agreement with the plaintiffs, 64,000 employees of the four companies, to pay a sum of $324.5 million to settle the suit.
Reception.
Since 1995, "Fortune" has ranked Adobe as an outstanding place to work. Adobe was rated the 5th best U.S. company to work for in 2003, 6th in 2004, 31st in 2007, 40th in 2008, 11th in 2009, 42nd in 2010, 65th in 2011, 41st in 2012, and 83rd in 2013.
In 2015, Adobe Systems India was ranked 21st of great places to work in India. In June 2014, it was ranked 6th of great places to work in India., In October 2008, Adobe Systems Canada Inc. was named one of "Canada's Top 100 Employers" by Mediacorp Canada Inc., and was featured in "Maclean's" newsmagazine.
Criticisms.
Pricing.
Adobe has been criticized for its pricing practices, with retail prices being as much as twice as high in non-US countries as in the US. As pointed out by many, it is significantly cheaper to pay for a return airfare ticket to the United States and purchase one particular collection of Adobe's software there than to buy it locally in Australia.
After Adobe revealed the pricing for the Creative Suite 3 Master Collection, which was £1,000 higher for European customers, a petition to protest over "unfair pricing" was published and signed by 10,000 users. In June 2009, Adobe further increased its prices in the UK by 10% in spite of weakening of the pound against the dollar, and UK users are not allowed to buy from the US store.
Adobe ranked no. 5 on a list of "Internet’s 9 Most Hated Companies", based on a 2013 survey on Reddit.com. Adobe's Reader and Flash were listed on "The 10 most hated programs of all time" on TechRadar.com.
Security.
Hackers have exploited vulnerabilities in Adobe programs, such as Adobe Reader, to gain unauthorized access to computers. Adobe's Flash Player has also been criticized for, among other things, suffering from performance, memory usage and security problems (see criticism of Flash Player). A report by security researchers from Kaspersky Lab criticized Adobe for producing the products having top 10 security vulnerabilities.
Observers noted that Adobe was spying on its customers by including spyware in the Creative Suite 3 software and quietly sending user data to a firm named Omniture. When users became aware, Adobe explained what the suspicious software did and admitted that they: "could and should do a better job taking security concerns into account". When a security flaw was later discovered in Photoshop CS5, Adobe sparked outrage by saying it would leave the flaw unpatched, so anyone who wanted to use the software securely would have to pay for an upgrade. Following a fierce backlash Adobe decided to provide the software patch.
Adobe has been criticized for pushing unwanted software including third-party browser toolbars and free virus scanners, usually as part of the Flash update process, and for pushing a third-party scareware program designed to scare users into paying for unneeded system repairs.
Customer data breach.
On October 3, 2013, the company initially revealed that 2.9 million customers' sensitive and personal data was stolen in security breach which included encrypted credit card information. Adobe later admitted that 38 million active users have been affected and the attackers obtained access to their IDs and encrypted passwords, as well as to many inactive Adobe accounts. The company did not make it clear if all the personal information was encrypted, such as email addresses and physical addresses, though data privacy laws in 44 states require this information to be encrypted.
A 3.8 GB file stolen from Adobe and containing 152 million usernames, reversibly encrypted passwords and unencrypted password hints was posted on AnonNews.org. LastPass, a password security firm, said that Adobe failed to use best practices for securing the passwords and has not salted them. Another security firm, Sophos, showed that Adobe used a weak encryption method permitting the recovery of a lot of information with very little effort. According to an IT expert, Adobe has failed its customers and ‘should hang their heads in shame’.
Many of the credit cards were tied to the Creative Cloud software-by-subscription service. Adobe offered its affected US customers a free membership in a credit monitoring service, but no similar arrangements have been made for non-US customers. When a data breach occurs in the US, penalties depend on the state where the victim resides, not where the company is based.
After stealing the customers' data, cyber-thieves also accessed Adobe's source code repository, likely in mid-August 2013. Because hackers acquired copies of the source code of Adobe proprietary products, they could find and exploit any potential weaknesses in its security, computer experts warned. Security researcher Alex Holden, chief information security officer of Hold Security, characterized this Adobe breach, which affected Acrobat, ColdFusion and numerous other applications, as "one of the worst in US history". Adobe also announced that hackers stole parts of the source code of Photoshop, which according to commentators could allow programmers to copy its engineering techniques and would make it easier to pirate Adobe's expensive products.
Published on a server of a Russian-speaking hacker group, the "disclosure of encryption algorithms, other security schemes, and software vulnerabilities can be used to bypass protections for individual and corporate data" and may have opened the gateway to new generation zero-day attacks. Hackers already used ColdFusion exploits to make off with usernames and encrypted passwords of PR Newswire's customers, which has been tied to the Adobe security breach. They also used a ColdFusion exploit to breach Washington state court and expose up to 160,000 Social Security numbers.

</doc>
<doc id="1957" url="https://en.wikipedia.org/wiki?curid=1957" title="Alexander technique">
Alexander technique

The Alexander technique (AT), named after Frederick Matthias Alexander, is an educational process that teaches people how to avoid unnecessary muscular and mental tension. It is based on Alexander's idea that a person's self awareness may be inaccurate, resulting in muscles being used in ways that unnecessarily cause tension. Examples of this include standing or sitting with weight unevenly distributed, holding one's head incorrectly, walking or running inefficiently, or responding to stressful stimuli in an exaggerated way. Alexander said that people who habitually "misused" their muscles in such ways could not trust their feelings (sensory appreciation) when carrying out activities or responding to situations emotionally.
The purpose of AT is to help people unlearn maladaptive psychophysical habits and return to a balanced state of rest and poise in which one's musculature is functioning as an integrated whole.
Alexander developed the technique's principles in the 1890s as a personal tool to alleviate breathing problems and hoarseness during public speaking. He credited the technique with allowing him to pursue his passion for Shakespearean acting.
There is little good medical evidence that the Alexander technique confers any health benefit.
History.
Frederick Matthias Alexander (1869-1955) was a Shakespearean orator who developed voice loss during his performances. After doctors found no physical cause, Alexander reasoned that he was inadvertently doing something to himself while speaking to cause his problem. His self-observation in multiple mirrors revealed that he was contracting his entire stature prior to phonation in preparation for all verbal response. He developed the hypothesis that this habitual pattern of pulling the head backwards and downwards needlessly disrupted the normal working of the total postural, breathing and vocal mechanisms. After experimenting to develop his ability to stop the unnecessary and habitual contracting in his neck, displacement of his head, and shortening his stature, he found that his problem with recurrent voice loss was resolved. While on a recital tour in New Zealand (1895) he began to realise the wider significance of head carriage for overall physical functioning. Further, Alexander observed that many individuals commonly tightened their musculature in the same pattern as he had done, in anticipation of many other activities besides speech.
Alexander believed his work could be applied to improve individual health and well being. He further refined his technique of self-observation and re-training to teach his discoveries to others. As part of his teaching method, he also developed a unique way of imparting the improved kinesthetic and proprioceptive experience to his students. This approach to using the hands also allowed him to re-arrange the working of a person's entire supportive musculature as it functions in relation to gravity from moment to moment. He explained his reasoning in four books published in 1918, 1923, 1931 (1932 in the UK) and 1942. He also trained teachers to teach his work and to use their hands in this unique way from 1930 until his death in 1955. Teacher training was continued during World War II between 1941 and 1943, when Alexander accompanied children and teachers of the Little School to Stow, Massachusetts to join his brother, A. R. Alexander, who also taught his brother's technique. The American teacher training course included Frank Pierce Jones, who went on to conduct research work to explore aspects of the Alexander Technique at the Tufts Institute for Psychological Research, and he published many of his studies in professional journals.
Since the 1960s, numerous training schools for teachers of the Alexander Technique have started up in the United States—some based upon the standards of training laid down by the Society of Teachers of the Alexander Technique established in England after Alexander's death in 1955 and others established by those who had not undergone the required length of training. In 1987, The North American Society of Teachers of the Alexander Technique was established to maintain high teaching standards. It is now called The American Society of Teachers of the Alexander Technique and is affiliated with the original Alexander society in London.
Influence.
The American philosopher and educator John Dewey became impressed with the Alexander technique after his headaches, neck pains, blurred vision, and stress symptoms largely improved during the time he used Alexander's advice to change his posture. In 1923, Dewey wrote the introduction to Alexander's "Constructive Conscious Control of the Individual".
Aldous Huxley had transformative lessons with Alexander, and continued doing so with other teachers after moving to the US. He rated Alexander's work highly enough to base the character of the doctor who saves the protagonist in 'Eyeless in Gaza' (an experimental form of autobiographical work) on F.M. Alexander, putting many of his phrases into the character's mouth. Huxley's work 'The Art of Seeing' also discusses his views on the technique.
Sir Stafford Cripps, George Bernard Shaw, Henry Irving and other stage grandees, Lord Lytton and other eminent people of the era also wrote positive appreciations of his work after taking lessons with Alexander.
Since Alexander's work in the field came at the start of the 20th century, his ideas influenced many originators in the field of mind-body improvement. Fritz Perls, who originated Gestalt therapy, credited Alexander as an inspiration for his psychological work. The Feldenkrais Method and the Mitzvah Technique were both influenced by the Alexander technique.
Process.
Alexander's approach emphasizes mindful action. The technique is applied dynamically to everyday movements, as well as actions selected by students.
Actions such as sitting, squatting, lunging or walking are often selected by the teacher. Other actions may be selected by the student, tailored to their interests or work activities such as hobbies, computer use, lifting, driving or performance in acting, sports, speech or music. Alexander teachers often use themselves as examples. They demonstrate, explain, and analyze a student's moment to moment responses as well as using mirrors, video feedback or classmate observations. Guided modelling with a highly skilled hand contact is the primary tool for detecting and guiding the student into a more coordinated state in movement and at rest. Suggestions for improvements are often student-specific.
Exercise as a teaching tool is deliberately omitted because of a common mistaken assumption that there exists a "correct" position. There are only two specific procedures that are practiced by the student; the first is lying semi-supine; resting in this way uses "mechanical advantage" as a means of redirecting long-term and short-term accumulated muscular tension into a more integrated and balanced state. This position is sometimes referred to as "constructive rest", or "the balanced resting state". It's also a specific time to practice Alexander's principle of conscious "directing" without "doing." The second exercise is the "Whispered Ah," which is used to co-ordinate and free breathing & vocal production.
Freedom, efficiency and patience are the prescribed values. Proscribed are unnecessary effort, self-limiting habits as well as mistaken perceptual assumptions. Students are led to change their largely automatic routines that are interpreted by the teacher to currently or cumulatively be physically limiting, inefficient, or not in keeping with best use of themselves as a whole. The Alexander teacher provides verbal coaching while monitoring, guiding and preventing unnecessary habits at their source with a specialized hands-on assistance. This specialized hands-on skill also allows Alexander teachers to bring about a balanced working of the student's supportive musculature as it relates to gravity's downward pull from moment to moment. Often, students require a great deal of hands-on work in order to experience a fully poised relation to gravity in both movement and at rest as they react to all life's stimuli. The hands-on skill requires Alexander teachers to maintain in themselves from moment to moment the improved psycho-physical co-ordination they are communicating to the student.
Alexander developed terminology to describe his methods, outlined in his four books that explain the sometimes paradoxical experience of learning and substituting new improvements.
Uses.
According to Alexander Technique instructor Michael J. Gelb, people tend to study the Alexander Technique either to rid themselves of pain, to increase their performance abilities, or for reasons of personal development and transformation.
As an example among performance-art applications, the Alexander technique is used and taught by classically trained vocal coaches and musicians. Its advocates claim that it allows for a balanced use of all aspects of the vocal tract by consciously increasing air-flow, allowing improved vocal technique and tone. Because the technique has allegedly been used to improve breathing and stamina in general, advocates also claim that athletes, people with asthma, tuberculosis, and panic attacks have also found improvements. The technique has been used by actors to reduce stage fright and to increase spontaneity. By improving stress-management, the technique can be an adjunct to psychotherapy for people with disabilities, Post-traumatic Stress Disorder, panic attacks, stuttering, and chronic pain.
The technique is a frequent component in acting training. It is part of the curriculum in many conservatory programs.
Method.
The Alexander Technique is most commonly taught privately in a series of 10 to 40 private lessons which may last from 30 minutes to an hour. Students are often performers, such as actors, dancers, musicians, athletes and public speakers, or people who work on computers, or who are in frequent pain for other reasons. Instructors observe their students, then show them how to hold themselves and move with better poise and less strain. Sessions include chair work and table work, often in front of a mirror, during which the instructor and the student will stand, sit and lie down, moving efficiently while maintaining a correct relationship between the head, neck and spine.
To qualify as a teacher of Alexander Technique, instructors are required to complete at least 1,600 hours, spanning at least three years, of supervised teacher training. The result must be satisfactory to qualified peers to gain membership in professional societies.
Effectiveness.
In 2015 the Australian Government's Department of Health published the results of a review of alternative therapies that had sought to determine if any were suitable for being covered by health insurance; Alexander technique was one of 17 practices evaluated for which no clear evidence of effectiveness was found – no good body of supporting research existed.
A 2011 review said that The Alexander technique was cost-effective in the management of chronic pain.
There is moderate evidence that the technique helps reduce the disability associated with symptoms of Parkinson's disease.
Evidence suggests that Alexander technique lessons may help performance anxiety in musicians, but studies of the technique were inconclusive in improving music performance, respiratory function and the posture of musicians.
No adequately designed clinical trials exist that allow evaluation of claims that the technique helps people with their asthma.

</doc>
<doc id="1960" url="https://en.wikipedia.org/wiki?curid=1960" title="Andrea Alciato">
Andrea Alciato

Andrea Alciato (8 May 1492 – 12 January 1550), commonly known as Alciati (Andreas Alciatus), was an Italian jurist and writer. He is regarded as the founder of the French school of legal humanists.
Biography.
Alciati was born in Alzate Brianza, near Milan, and settled in France in the early 16th century. He displayed great literary skill in his exposition of the laws, and was one of the first to interpret the civil law by the history, languages and literature of antiquity, and to substitute original research for the servile interpretations of the glossators. He published many legal works, and some annotations on Tacitus and accumulated a sylloge of Roman inscriptions from Milan and its territories, as part of his preparation for his history of Milan, written in 1504-05.
Alciati is most famous for his "Emblemata," published in dozens of editions from 1531 onward. This collection of short Latin verse texts and accompanying woodcuts created an entire European genre, the emblem book, which attained enormous popularity in continental Europe and Great Britain.
Alciati died at Pavia in 1550.

</doc>
<doc id="1962" url="https://en.wikipedia.org/wiki?curid=1962" title="Apparent magnitude">
Apparent magnitude

The apparent magnitude (m) of a celestial object is a number that is a measure of its brightness as seen by an observer on Earth. The smaller the number, the brighter a star appears. The sun, at apparent magnitude of -27, is the brightest object in the sky. It is adjusted to the value it would have in the absence of the atmosphere. The brighter an object appears, the lower its magnitude value (i.e. inverse relation). In addition, the magnitude scale is logarithmic: a difference of one in magnitude corresponds to a change in brightness by a factor of formula_1 or about 2.512.
Generally, the visible spectrum (vmag) is used as a basis for the apparent magnitude. However, other spectra are also used (e.g. the near-infrared J-band). In the visible spectrum, Sirius is the brightest star after the Sun. In the near-infrared J-band, Betelgeuse is the brightest. The apparent magnitude of stars is measured with a bolometer.
History.
The scale used to indicate magnitude originates in the Hellenistic practice of dividing stars visible to the naked eye into six "magnitudes". The brightest stars in the night sky were said to be of first magnitude ("m" = 1), whereas the faintest were of sixth magnitude ("m" = 6), which is the limit of human visual perception (without the aid of a telescope). Each grade of magnitude was considered twice the brightness of the following grade (a logarithmic scale), although that ratio was subjective as no photodetectors existed. This rather crude scale for the brightness of stars was popularized by Ptolemy in his "Almagest", and is generally believed to have originated with Hipparchus.
In 1856, Norman Robert Pogson formalized the system by defining a first magnitude star as a star that is 100 times as bright as a sixth-magnitude star, thereby establishing the logarithmic scale still in use today. This implies that a star of magnitude "m" is 2.512 times as bright as a star of magnitude "m+1". This figure, the fifth root of 100, became known as "Pogson's Ratio". The zero point of Pogson's scale was originally defined by assigning Polaris a magnitude of exactly 2. Astronomers later discovered that Polaris is slightly variable, so they switched to Vega as the standard reference star, assigning the brightness of Vega as the definition of zero magnitude at any specified wavelength.
Apart from small corrections, the brightness of Vega still serves as the definition of zero magnitude for visible and near infrared wavelengths, where its spectral energy distribution (SED) closely approximates that of a black body for a temperature of 11,000 K. However, with the advent of infrared astronomy it was revealed that Vega's radiation includes an Infrared excess presumably due to a circumstellar disk consisting of dust at warm temperatures (but much cooler than the star's surface). At shorter (e.g. visible) wavelengths, there is negligible emission from dust at these temperatures. However, in order to properly extend the magnitude scale further into the infrared, this peculiarity of Vega should not affect the definition of the magnitude scale. Therefore, the magnitude scale was extrapolated to "all" wavelengths on the basis of the black body radiation curve for an ideal stellar surface at 11,000 K uncontaminated by circumstellar radiation. On this basis the spectral irradiance (usually expressed in janskys) for the zero magnitude point, as a function of wavelength can be computed (see ). Small deviations are specified between systems using measurement appartuses developed independently so that data obtained by different astronomers can be properly compared; of greater practical importance is the definition of magnitude not at a single wavelength but applying to the response of standard spectral filters used in photometry over various wavelength bands.
With the modern magnitude systems, brightness over a very wide range is specified according to the logarithmic definition detailed below, using this zero reference. In practice such apparent magnitudes do not exceed 30 (for detectable measurements). The brightness of Vega is exceeded by four stars in the night sky at visible wavelengths (and more at infrared wavelengths) as well as bright planets such as Venus, Mars, and Jupiter, and these must be described by "negative" magnitudes. For example, Sirius, the brightest star of the celestial sphere, has an apparent magnitude of −1.4 in the visible; negative magnitudes for other very bright astronomical objects can be found in the table below.
Calculations.
As the amount of light received actually depends on the thickness of the Earth's atmosphere in the line of sight to the object, the apparent magnitudes are adjusted to the value they would have in the absence of the atmosphere. The dimmer an object appears, the higher the numerical value given to its apparent magnitude. Note that brightness varies with distance; an extremely bright object may appear quite dim, if it is far away. More exactly, brightness varies inversely with the square of the distance. The absolute magnitude, "M", of a celestial body (outside the Solar System) is the apparent magnitude it would have if it were at 10 parsecs (~32.6 light years) and that of a planet (or other Solar System body) is the apparent magnitude it would have if it were 1 astronomical unit from both the Sun and Earth. The absolute magnitude of the Sun is 4.83 in the V band (yellow) and 5.48 in the B band (blue).
The apparent magnitude, "m", in the band, "x", can be defined as,
where formula_3 is the observed flux in the band x, and formula_4 and formula_5 are a reference magnitude, and reference flux in the same band x, such as that of Vega. An increase of 1 in the magnitude scale corresponds to a decrease in brightness by a factor of formula_6. Based on the properties of logarithms, a difference in magnitudes, formula_7, can be converted to a variation in brightness as formula_8.
Example: Sun and Moon.
"What is the ratio in brightness between the Sun and the full moon?"
The apparent magnitude of the Sun is −26.74 (brighter), and the mean apparent magnitude of the full moon is −12.74 (dimmer).
Difference in magnitude : 
Variation in Brightness : 
The Sun appears about 400,000 times brighter than the full moon.
Magnitude addition.
Sometimes, it might be useful to add magnitudes. For example, to determine the combined magnitude of a double star when the magnitudes of the individual components are known. This can be done by setting an equation using the brightness (in linear units) of each magnitude.
Solving for formula_12 yields
where formula_12 is the resulting magnitude after adding formula_15 and formula_16. Note that the negative of each magnitude is used because greater intensities equate to lower magnitudes.
Standard reference values.
It is important to note that the scale is logarithmic: the relative brightness of two objects is determined by the difference of their magnitudes. For example, a difference of 3.2 means that one object is about 19 times as bright as the other, because Pogson's Ratio raised to the power 3.2 is approximately 19.05.
A common misconception is that the logarithmic nature of the scale is because the human eye itself has a logarithmic response. In Pogson's time this was thought to be true (see Weber-Fechner law), but it is now believed that the response is a power law (see Stevens' power law).
Magnitude is complicated by the fact that light is not monochromatic. The sensitivity of a light detector varies according to the wavelength of the light, and the way it varies depends on the type of light detector. For this reason, it is necessary to specify how the magnitude is measured for the value to be meaningful. For this purpose the UBV system is widely used, in which the magnitude is measured in three different wavelength bands: U (centred at about 350 nm, in the near ultraviolet), B (about 435 nm, in the blue region) and V (about 555 nm, in the middle of the human visual range in daylight). The V band was chosen for spectral purposes and gives magnitudes closely corresponding to those seen by the light-adapted human eye, and when an apparent magnitude is given without any further qualification, it is usually the V magnitude that is meant, more or less the same as visual magnitude.
Because cooler stars, such as red giants and red dwarfs, emit little energy in the blue and UV regions of the spectrum their power is often under-represented by the UBV scale. Indeed, some L and T class stars have an estimated magnitude of well over 100, because they emit extremely little visible light, but are strongest in infrared.
Measures of magnitude need cautious treatment and it is extremely important to measure like with like. On early 20th century and older orthochromatic (blue-sensitive) photographic film, the relative brightnesses of the blue supergiant Rigel and the red supergiant Betelgeuse irregular variable star (at maximum) are reversed compared to what human eyes perceive, because this archaic film is more sensitive to blue light than it is to red light. Magnitudes obtained from this method are known as photographic magnitudes, and are now considered obsolete.
For objects within the Milky Way with a given absolute magnitude, 5 is added to the apparent magnitude for every tenfold increase in the distance to the object. This relationship does not apply for objects at very great distances (far beyond the Milky Way), because a correction for general relativity must then be taken into account due to the non-Euclidean nature of space. 
For planets and other Solar System bodies the apparent magnitude is derived from its phase curve and the distances to the Sun and observer.
Table of notable celestial objects.
Some of the above magnitudes are only approximate. Telescope sensitivity also depends on observing time, optical bandpass, and interfering light from scattering and airglow.

</doc>
<doc id="1963" url="https://en.wikipedia.org/wiki?curid=1963" title="Absolute magnitude">
Absolute magnitude

Absolute magnitude is the measure of intrinsic brightness of a celestial object. It is the hypothetical apparent magnitude of an object at a standard distance of exactly 10 parsecs (32.6 light years) from the observer, assuming no astronomical extinction of starlight. This places the objects on a common basis and allows the true energy output of astronomical objects to be compared without the distortion introduced by distance. As with all astronomical magnitudes, the absolute magnitude can be specified for different wavelength intervals; for stars the most commonly quoted absolute magnitude is the absolute visual magnitude, which uses only the visual (V) band of the spectrum (UBV system). Also commonly used is the absolute bolometric magnitude, which is the total luminosity expressed in magnitude units that takes into account energy radiated at all wavelengths, whether visible or not.
The brighter the celestial object the smaller its absolute magnitude. The magnitude scale extends downward through the positive numbers and into the negative numbers as brightness increases. A difference of 1.0 in absolute magnitude corresponds to a difference of 2.512 ≈ 10 of absolute brightness. Therefore a star of magnitude -2 is 100 (or 2.512) times brighter than a star of magnitude 3. The Milky Way, for example, has an absolute magnitude of about −20.5, so a quasar with an absolute magnitude of −25.5 is 100 times brighter than the Milky Way. If this particular quasar and the Milky Way could be seen side by side at the same distance of one parsec and the Milky Way's stars reduced to a single point, the quasar would be 5 magnitudes (or 100 times) brighter than the Milky Way. Similarly, Canopus has an absolute visual magnitude of about −5.5, whereas Ross 248 has an absolute visual magnitude of +14.8, for a difference of about 20 magnitudes, i.e., Canopus would be seen as about 20 magnitudes brighter; stated another way, Canopus emits more than 100 million (10) times more visual power than Ross 248.
Stars and galaxies ("M").
In stellar and galactic astronomy, the standard distance is 10 parsecs (about 32.616 light years, 308.57 petameters or 308.57 trillion kilometres).
A star at 10 parsecs has a parallax of 0.1" (100 milli arc seconds).
Galaxies (and other extended objects) are much larger than 10 parsecs, their light is radiated over an extended patch of sky, and their overall brightness cannot be directly observed from relatively short distances, but the same convention is used. A galaxy's magnitude is defined by measuring all the light radiated over the entire object, treating that integrated brightness as the brightness of a single point-like or star-like source, and computing the magnitude of that point-like source as it would appear if observed at the standard 10 parsecs distance. Consequently, the absolute magnitude of any object "equals" the apparent magnitude it "would have" if it were 10 parsecs away.
The measurement of absolute magnitude is made with an instrument called a bolometer. When using an absolute magnitude, one must specify the type of electromagnetic radiation being measured. When referring to total energy output, the proper term is bolometric magnitude. The bolometric magnitude usually is computed from the visual magnitude plus a bolometric correction, formula_1. This correction is needed because very hot stars radiate mostly ultraviolet radiation, whereas very cool stars radiate mostly infrared radiation (see Planck's law).
Many stars visible to the naked eye have such a low absolute magnitude that they would appear bright enough to cast shadows if they were at 10 parsecs from the Earth: Rigel (−7.0), Deneb (−7.2), Naos (−6.0), and Betelgeuse (−5.6). For comparison, Sirius has an absolute magnitude of 1.4, which is brighter than the Sun, whose absolute visual magnitude is 4.83 (it actually serves as a reference point). The Sun's absolute bolometric magnitude is set arbitrarily, usually at 4.75.
Absolute magnitudes of stars generally range from −10 to +17. The absolute magnitudes of galaxies can be much lower (brighter). For example, the giant elliptical galaxy M87 has an absolute magnitude of −22 (i.e. as bright as about 60,000 stars of magnitude −10).
Computation.
If extinction by gas and dust is not significant, one can compute the absolute magnitude formula_2 of an object given its apparent magnitude formula_3 and luminosity distance formula_4:
where formula_4 is the star's actual distance in parsecs (1 parsec is 206,265 astronomical units, approximately 3.2616 light-years). For very large distances, the cosmological redshift complicates the relation between absolute and apparent magnitude, because the radiation observed was shifted into the red range of the spectrum. To compare the magnitudes of very distant objects with those of local objects, a k correction might have to be applied to the magnitudes of the distant objects.
For nearby astronomical objects (such as stars in the Milky Way) luminosity distance "D" is almost identical to the real distance to the object, because spacetime within the Milky Way is almost Euclidean. For much more distant objects the Euclidean approximation is not valid, and general relativity must be taken into account when calculating the luminosity distance of an object.
In the Euclidean approximation for nearby objects, the absolute magnitude formula_2 of a star can be calculated from its apparent magnitude formula_8 and the star's parallax formula_9 in arcseconds:
You can also compute the absolute magnitude formula_2 of an object given its apparent magnitude formula_3 and distance modulus formula_13:
Examples.
Rigel has a visual magnitude of formula_15 and distance about 860 light-years
Vega has a parallax of 0.129", and an apparent magnitude of +0.03
Alpha Centauri A has a parallax of 0.742" and an apparent magnitude of −0.01
The Black Eye galaxy has a visual magnitude of m=+9.36 and a distance modulus of 31.06.
Apparent magnitude.
Given the absolute magnitude formula_2, for objects within the Milky Way you can also calculate the apparent magnitude formula_3 from any distance formula_22 (in parsecs):
For objects at very great distances (outside the Milky Way) the luminosity distance "D" must be used instead of "d" (in parsecs).
Given the absolute magnitude formula_2, you can also compute apparent magnitude formula_3 from its parallax formula_26:
Also calculating absolute magnitude formula_2 from distance modulus formula_13:
Bolometric magnitude.
Bolometric magnitude corresponds to luminosity, expressed in magnitude units; that is, after taking into account all electromagnetic wavelengths, including those unobserved due to instrumental pass-band, the Earth's atmospheric absorption, and extinction by interstellar dust. In the case of stars with few observations, it usually must be computed assuming an effective temperature.
Classically, the difference in bolometric magnitude is related to the luminosity ratio according to:
which makes by inversion:
where
In August 2015, the International Astronomical Union passed Resolution B2 defining the zero points of the absolute and apparent bolometric magnitude scales in SI units for power (watts) and irradiance (formula_37), respectively. Although bolometric magnitudes had been used by astronomers for many decades, there had been systematic differences in the absolute magnitude-luminosity scales presented in various astronomical references, and no international standardization. This led to systematic differences in bolometric corrections scales, which when combined with incorrect assumed absolute bolometric magnitudes for the Sun could lead to systematic errors in estimated stellar luminosities (and stellar properties calculated which rely on stellar luminosity, like radii, ages, etc.).
IAU 2015 Resolution B2 defines an absolute bolometric magnitude scale where formula_38 corresponds to luminosity watts, with the zero point luminosity formula_39 set such that the Sun (with nominal luminosity watts) corresponds to absolute bolometric magnitude formula_40. Placing a radiation source (e.g. star) at the standard distance of 10 parsecs, it follows that the zero point of the apparent bolometric magnitude scale formula_41 corresponds to irradiance formula_42 = formula_37. Using the IAU 2015 scale, the nominal total solar irradiance ("Solar constant") measured at 1 astronomical unit (formula_44) corresponds to an apparent bolometric magnitude of the Sun of formula_45.
Following IAU 2015 Resolution B2 system, the relation between a star's absolute bolometric magnitude and its luminosity is no longer directly tied to the Sun's (variable) luminosity:
where
The new IAU absolute magnitude scale permanently disconnects the scale from the variable Sun. However, on this SI power scale, the nominal solar luminosity corresponds closely to formula_49 = 4.74, a value that was commonly adopted by astronomers before the 2015 IAU resolution.
The luminosity of the star in watts can be calculated as a function of its absolute bolometric magnitude formula_49 as:
using the variables as defined previously.
Solar System bodies ("H").
For planets and asteroids a definition of absolute magnitude that is more meaningful for nonstellar objects is used.
In this case, the absolute magnitude (H) is defined as the apparent magnitude that the object would have if it were one astronomical unit (AU) from both the Sun and the observer. Because the object is illuminated by the Sun, absolute magnitude is a function of phase angle and this relationship is referred to as the phase curve.
To convert a stellar or galactic absolute magnitude into a planetary one, subtract 31.57. A comet's nuclear magnitude (M2) is a different scale and can not be used for a size comparison with an asteroid's (H) magnitude.
Apparent magnitude.
The absolute magnitude can be used to help calculate the apparent magnitude of a body under different conditions.
where formula_54 is 1 AU, formula_55 is the phase angle, the angle between the Sun–body and body–observer lines. By the law of cosines, we have:
formula_57 is the phase integral (integration of reflected light; a number in the 0 to 1 range).
Example: Ideal diffuse reflecting sphere. A reasonable first approximation for planetary bodies
A full-phase diffuse sphere reflects as much light as a diffuse disc of the same diameter.
Distances:
Note: because Solar System bodies are never perfect diffuse reflectors, astronomers use empirically derived relationships to predict apparent magnitudes when accuracy is required.
Example.
Moon:
How bright is the Moon from Earth?
Meteors.
For a meteor, the standard distance for measurement of magnitudes is at an altitude of at the observer's zenith.

</doc>
<doc id="1965" url="https://en.wikipedia.org/wiki?curid=1965" title="Apollo 1">
Apollo 1

Apollo 1 (initially designated AS-204) was the first manned mission of the U.S. Apollo manned lunar landing program. The planned low Earth orbital test of the Apollo Command/Service Module never made its target launch date of February 21, 1967, because a cabin fire during a launch rehearsal test on January 27 at Cape Kennedy Air Force Station Launch Complex 34 killed all three crew members—Command Pilot Virgil I. "Gus" Grissom, Senior Pilot Edward H. White II, and Pilot Roger B. Chaffee—and destroyed the Command Module (CM). The name "Apollo 1", chosen by the crew, was officially retired by NASA in commemoration of them on April 24, 1967.
Immediately after the fire, NASA convened the "Apollo 204 Accident Review Board" to determine the cause of the fire, and both houses of the United States Congress conducted their own committee inquiries to oversee NASA's investigation. The ignition source for the fire was determined to be electrical, fed by combustible nylon material and the high pressure pure oxygen cabin atmosphere. The astronauts' rescue was prevented by the plug door hatch which could not be opened against the internal pressure, and hampered by poor emergency preparedness caused by a failure to identify the test as hazardous, based on the fact that the rocket was unfueled.
During the Congressional investigation, then-Senator Walter F. Mondale publicly revealed a NASA internal document citing problems with prime Apollo contractor North American Aviation, which became known as the "Phillips Report". This disclosure embarrassed NASA Administrator James E. Webb, who was unaware of the document's existence, and attracted controversy to the Apollo program. Despite congressional displeasure at NASA's lack of openness, both congressional committees ruled that the issues raised in the report had no bearing on the accident.
Manned Apollo flights were suspended for 20 months while the Command Module's hazards were corrected, and development and unmanned testing of the Lunar Module (LM) and Saturn V Moon rocket continued. The Saturn IB launch vehicle for Apollo 1, AS-204, was used for the first LM test flight, Apollo 5. The first successful manned Apollo mission was flown by Apollo 1's backup crew on Apollo 7 in October 1968.
Apollo manned test flight plans.
AS-204 was to be the first manned test flight of the Apollo Command/Service Module (CSM) to Earth orbit, launched on a Saturn IB rocket. AS-204 was to test launch operations, ground tracking and control facilities and the performance of the Apollo-Saturn launch assembly and would have lasted up to two weeks, depending on how the spacecraft performed.
The CSM for this flight, number 012 built by North American Aviation (NAA), was a Block I version designed before the lunar orbit rendezvous landing strategy was chosen; therefore it lacked capability of docking with the Lunar Module. This was incorporated into the Block II CSM design, along with lessons learned in Block I. Block II would be test-flown with the LM when the latter was ready, and would be used on the Moon landing flights.
Deke Slayton, the Mercury astronaut who was grounded and became Director of Flight Crew Operations, selected the first Apollo crew in January 1966, with Grissom as Command Pilot, White as Senior Pilot, and rookie Donn F. Eisele as Pilot. But Eisele dislocated his shoulder twice aboard the KC135 weightlessness training aircraft, and had to undergo surgery on January 27. Slayton replaced him with Chaffee, and NASA announced the crew selection on March 21, 1966. James McDivitt, David Scott and Russell Schweickart were named as the backup crew. On September 29, Walter Schirra, Eisele, and Walter Cunningham were named as the prime crew for a second Block I CSM flight, AS-205. NASA planned to follow this with an unmanned test flight of the LM (AS-206), then the third manned mission would be a dual flight designated AS-278 (or AS-207/208), in which AS-207 would launch the first manned Block II CSM, which would then rendezvous and dock with the LM launched unmanned on AS-208.
In March, NASA was studying the possibility of flying the first Apollo mission as a joint space rendezvous with the final Project Gemini mission, Gemini 12 in November 1966. But by May, delays in making Apollo ready for flight just by itself, and the extra time needed to incorporate compatibility with the Gemini, made that impractical. This became moot when slippage in readiness of the AS-204 spacecraft caused the last-quarter 1966 target date to be missed, and the mission was rescheduled for February 21, 1967.
Mission background.
Grissom declared his intent to keep his craft in orbit for a full 14 days. A newspaper article published on August 4, 1966, referred to the flight as "Apollo 1". CM-012 arrived at the Kennedy Space Center on August 26, labeled "Apollo One" by NAA on its packaging.
In October 1966, NASA announced the flight would carry a small television camera to broadcast live from the Command Module. The camera would also be used to allow flight controllers to monitor the spacecraft's instrument panel in flight. Television cameras were carried aboard all manned Apollo missions.
By December 1966, the second Block I flight AS-205 was canceled as unnecessary; and Schirra, Eisele and Cunningham were reassigned as the backup crew for Apollo 1. McDivitt's crew was now promoted to prime crew of the Block II / LM mission, re-designated AS-258 because the AS-205 launch vehicle would be used in place of AS-207. A third manned mission was planned to launch the CSM and LM together on a Saturn V (AS-503) to an elliptical medium Earth orbit (MEO), to be crewed by Frank Borman, Michael Collins and William Anders. McDivitt, Scott and Schweickart had started their training for AS-258 in CM-101 at the NAA plant in Downey, California, when the Apollo 1 accident occurred.
Insignia.
Grissom's crew received approval in June 1966 to design a mission patch with the name "Apollo 1". The design's center depicts a Command/Service Module flying over the southeastern United States with Florida (the launch point) prominent. The Moon is seen in the distance, symbolic of the eventual program goal. A yellow border carries the mission and astronaut names with another border set with stars and stripes, trimmed in gold. The insignia was designed by the crew, with the artwork done by North American Aviation employee Allen Stevens.
Spacecraft preparation.
The Apollo Command/Service Module was much bigger and far more complex than any previously implemented spacecraft design. In October 1963, Joseph F. Shea was named Apollo Spacecraft Program Office (ASPO) manager, responsible for managing the design and construction of both the CSM and the LM.
In a spacecraft review meeting held with Shea on August 19, 1966 (a week before delivery), the crew expressed concern about the amount of flammable material (mainly nylon netting and Velcro) in the cabin, which both astronauts and technicians found convenient for holding tools and equipment in place. Though Shea gave the spacecraft a passing grade, after the meeting they gave him a crew portrait they had posed with heads bowed and hands clasped in prayer, with the inscription:
It isn't that we don't trust you, Joe, but this time we've decided to go over your head.
Shea gave his staff orders to tell North American to remove the flammables from the cabin, but did not supervise the issue personally.
North American shipped spacecraft CM-012 to Kennedy Space Center on August 26, 1966 under a conditional Certificate of Flight Worthiness: 113 significant incomplete planned engineering changes had to be completed at KSC. But that was not all; an additional 623 engineering change orders were made and completed after delivery. Grissom became so frustrated with the inability of the training simulator engineers to keep up with the spacecraft changes, that he took a lemon from a tree by his house and hung it on the simulator.
The Command and Service Modules were mated in the KSC altitude chamber in September, and combined system testing was performed. Altitude testing was performed first unmanned, then with both the prime and backup crews, from October 10 through December 30. During this testing, the Environmental Control Unit in the Command Module was found to have a design flaw, and was sent back to the manufacturer for design changes and rework. The returned ECU then leaked water/glycol coolant, and had to be returned a second time. Also during this time, a propellant tank in Service Module 017 had ruptured during testing at NAA, prompting the separation of the modules and removal from the chamber so the Service Module could be tested for signs of the tank problem. These tests were negative, and once all outstanding hardware problems were fixed, the reassembled spacecraft finally completed a successful altitude test with Schirra's backup crew.
According to the final report of the accident investigation board, "At the post-test debriefing the backup flight crew expressed their satisfaction with the condition and performance of the spacecraft." This would appear to contradict the account given in "Lost Moon: The Perilous Voyage of Apollo 13" by Jeffrey Kluger and astronaut James Lovell, that "When the trio climbed out of the ship, … Schirra made it clear that he was not pleased with what he had seen," and that he later warned Grissom and Shea that "there's nothing wrong with this ship that I can point to, but it just makes me uncomfortable. Something about it just doesn't ring right," and that Grissom should get out at the first sign of trouble.
Following the successful altitude tests, the spacecraft was removed from the altitude chamber on January 3, 1967, and mated to its Saturn IB launch vehicle on pad 34 on January 6.
Accident.
Plugs-out test.
The launch simulation on January 27, 1967, on pad 34, was a "plugs-out" test to determine whether the spacecraft would operate nominally on (simulated) internal power while detached from all cables and umbilicals. Passing this test was essential to making the February 21 launch date. The test was considered non-hazardous because neither the launch vehicle nor the spacecraft was loaded with fuel or cryogenics, and all pyrotechnic systems were disabled.
At 1:00 pm EST (1800 GMT) on January 27, first Grissom, then Chaffee, and White entered the Command Module fully pressure-suited, and were strapped into their seats and hooked up to the spacecraft's oxygen and communication systems. Grissom immediately noticed a strange odor in the air circulating through his suit which he compared to "sour buttermilk", and the simulated countdown was held at 1:20 pm, while air samples were taken. No cause of the odor could be found, and the countdown was resumed at 2:42 pm. The accident investigation found this odor not to be related to the fire.
Three minutes after the count was resumed, the hatch installation was started. The hatch consisted of three parts: a removable inner hatch, which stayed inside the cabin; a hinged outer hatch, which was part of the spacecraft's heat shield; and an outer hatch cover, which was part of the boost protective cover enveloping the entire Command Module to protect it from aerodynamic heating during launch and from launch escape rocket exhaust in the event of a launch abort. The boost hatch cover was partially but not fully latched in place, because the flexible boost protective cover was slightly distorted by some cabling run under it to provide the simulated internal power. (The spacecraft's fuel cell reactants were not loaded for this test.) After the hatches were sealed, the air in the cabin was replaced with pure oxygen at , higher than atmospheric pressure.
Movement by the astronauts was detected by the spacecraft's inertial measurement unit and the astronaut's biomedical sensors, and also indicated by increases in oxygen spacesuit flow, and sounds from Grissom's stuck-open microphone. There was no evidence to identify the movement, or whether it was related to the fire. The stuck microphone was part of a problem with the communications loop connecting the crew, the Operations and Checkout Building, and the Complex 34 blockhouse control room. The poor communications led Grissom to remark: "How are we going to get to the Moon if we can't talk between two or three buildings?" The simulated countdown was held again at 5:40 pm while attempts were made to troubleshoot the communications problem. All countdown functions up to the simulated internal power transfer had been successfully completed by 6:20 pm, but at 6:30 the count remained on hold at T minus 10 minutes.
Fire.
The crew members were using the time to run through their checklist again, when a voltage transient was recorded at 6:30:54 (23:30:54 GMT). Ten seconds later (at 6:31:04.7), one of the astronauts exclaimed "Hey!" or "Fire!"; this was followed by two seconds of scuffling sounds through Grissom's open microphone. This was followed at 6:31:06.2 (23:31:06.2 GMT) by someone (believed to be Chaffee) reporting, "'ve, or We'v got a fire in the cockpit." The next transmission occurred 6.8 seconds later and was badly garbled, interpreted by various listeners as "They're fighting a bad fire—Let's get out ...Open 'er up", "We've got a bad fire—Let's get out ...We're burning up", or "I'm reporting a bad fire ...I'm getting out ..."; the speaker could not be conclusively identified. The last transmission ended abruptly at 6:31:21.8, 15.6 seconds after the first report of fire.
Some blockhouse witnesses said that they saw White on the television monitors, reaching for the inner hatch release handle as flames in the cabin spread from left to right and licked the window.
The intensity of the fire fed by pure oxygen caused the pressure to rise in that 15 seconds to , which ruptured the Command Module's inner wall (initial phase of the fire). Flames and gases then rushed outside the Command Module through open access panels to two levels of the pad service structure. Intense heat, dense smoke, and ineffective gas masks designed for toxic fumes rather than heavy smoke hampered the ground crew's attempts to rescue the men. There were fears the Command Module had exploded, or soon would, and that the fire might ignite the solid fuel rocket in the launch escape tower above the Command Module, which would have likely killed nearby ground personnel, and possibly have destroyed the pad.
As the pressure was released by the cabin rupture, the convective rush of air caused the flames to spread across the cabin, beginning the second phase. The third phase began when most of the oxygen was consumed and was replaced with atmospheric air, essentially quenching the fire, but causing massive amounts of smoke, dust, carbon monoxide, and fumes to fill the cabin.
It took five minutes for the pad workers to open all three hatch layers, and they could not drop the inner hatch to the cabin floor as intended, so they pushed it out of the way to one side. Although the cabin lights remained lit, they were at first unable to find the astronauts through the dense smoke. As the smoke cleared, they found the bodies, but were not able to remove them. The fire had partly melted Grissom's and White's nylon space suits and the hoses connecting them to the life support system. Grissom had removed his restraints and was lying on the floor of the spacecraft. White's restraints were burned through, and he was found lying sideways just below the hatch. It was determined that he had tried to open the hatch per the emergency procedure, but was not able to do so against the internal pressure. Chaffee was found strapped into his right-hand seat, as procedure called for him to maintain communication until White opened the hatch. Because of the large strands of melted nylon fusing the astronauts to the cabin interior, removing the bodies took nearly 90 minutes.
Investigation.
As a result of the in-flight failure of the Gemini 8 mission on March 17, 1966, NASA Deputy Administrator Robert Seamans wrote and implemented "Management Instruction 8621.1" on April 14, 1966, defining "Mission Failure Investigation Policy And Procedures". This modified NASA's existing accident procedures, based on military aircraft accident investigation, by giving the Deputy Administrator the option of performing independent investigations of major failures, beyond those for which the various Program Office officials were normally responsible. It declared, "It is NASA policy to investigate and document the causes of all major mission failures which occur in the conduct of its space and aeronautical activities and to take appropriate corrective actions as a result of the findings and recommendations."
Immediately after the Apollo 1 fire, to avoid appearance of a conflict of interest, NASA Administrator James E. Webb asked President Lyndon B. Johnson to allow NASA to handle the investigation according to its established procedure, promising to be truthful in assessing blame, and to keep the appropriate leaders of Congress informed. Seamans then directed establishment of the "Apollo 204 Review Board" chaired by Langley Research Center director Floyd L. Thompson, which included astronaut Frank Borman, spacecraft designer Maxime Faget, and six others.
Seamans immediately ordered all Apollo 1 hardware and software impounded, to be released only under control of the Board. On February 3, two members, a Cornell University professor and North American's Chief engineer for Apollo, left the Board, and a U.S. Bureau of Mines professor joined. After thorough stereo photographic documentation of the CM-012 interior, the board ordered its disassembly using procedures tested by disassembling the identical CM-014, and conducted a thorough investigation of every part. The board also reviewed the astronauts' autopsy results and interviewed witnesses. Seamans sent Webb weekly status reports of the investigation's progress, and the Board issued its final report on April 5, 1967.
According to the Board, Grissom suffered severe third degree burns on over one-third of his body and his spacesuit was mostly destroyed. White suffered third degree burns on almost half of his body and a quarter of his spacesuit had melted away. Chaffee suffered third degree burns over almost a quarter of his body and a small portion of his spacesuit was damaged. The autopsy report confirmed that the primary cause of death for all three astronauts was cardiac arrest caused by high concentrations of carbon monoxide. Burns suffered by the crew were not believed to be major factors, and it was concluded that most of them had occurred postmortem. Asphyxiation happened after the fire melted the astronauts' suits and oxygen tubes, exposing them to the lethal atmosphere of the cabin.
The review board identified five major factors which combined to cause the fire and the astronauts' deaths:
Ignition source.
The review board determined that the electrical power momentarily failed at 23:30:55 GMT, and found evidence of several electric arcs in the interior equipment. However, they were unable to conclusively identify a single ignition source. They determined that the fire most likely started near the floor in the lower left section of the cabin, close to the Environmental Control Unit. It spread from the left wall of the cabin to the right, with the floor being affected only briefly.
The board noted that a silver-plated copper wire running through an environmental control unit near the center couch had become stripped of its Teflon insulation and abraded by repeated opening and closing of a small access door.
This weak point in the wiring also ran near a junction in an ethylene glycol/water cooling line that had been prone to leaks. The electrolysis of ethylene glycol solution with the silver anode was discovered at MSC on May 29, 1967 to be a hazard capable of causing a violent exothermic reaction, igniting the ethylene glycol mixture in the CM's pure oxygen atmosphere. Experiments at the Illinois Institute of Technology confirmed the hazard existed for silver-plated wires, but not for copper-only or nickel-plated copper. In July, ASPO directed both North American and Grumman to ensure no silver or silver-coated electrical contacts existed in the vicinity of possible glycol spills in the Apollo spacecraft.
Pure oxygen atmosphere.
The plugs-out test had been run to simulate the launch procedure, with the cabin pressurized with pure oxygen at the nominal launch level of , above standard sea level atmospheric pressure. This is more than five times the partial pressure of oxygen in the atmosphere, and provides an environment in which materials not normally considered highly flammable will burst into flame.
The high-pressure oxygen atmosphere was consistent with that used in the Mercury and Gemini programs. The pressure before launch was deliberately greater than ambient in order to drive out the nitrogen-containing air and replace it with pure oxygen, and also to seal the plug door hatch cover. During launch, the pressure would have been gradually reduced to the in-flight level of , providing sufficient oxygen for the astronauts to breathe while reducing the fire risk. The Apollo 1 crew had tested this procedure with their spacecraft in the Operations and Checkout Building altitude (vacuum) chamber on October 18 and 19, 1966, and the backup crew of Schirra, Eisele and Cunningham had repeated it on December 30. The investigation board noted that, during these tests, the Command Module had been fully pressurized with pure oxygen four times, for a total of six hours and fifteen minutes, two and a half hours longer than it had been during the plugs-out test.
Flammable materials in the cabin.
The review board cited "many types and classes of combustible material" close to ignition sources. The NASA crew systems department had installed of Velcro throughout the spacecraft, almost like carpeting. This Velcro was found to be flammable in a high-pressure 100% oxygen environment.
Buzz Aldrin states in his book "Men From Earth" that the flammable material had been removed (per the crew's August 19 complaints and Joseph Shea's order), but was replaced prior to the August 26 delivery to Cape Kennedy.
Hatch design.
The inner hatch cover used a plug door design, sealed by higher pressure inside the cabin than outside. The normal pressure level used for launch ( above ambient) created sufficient force to prevent removing the cover until the excess pressure was vented. Emergency procedure called for Grissom to open the cabin vent valve first, allowing White to remove the cover, but Grissom was prevented from doing this because the valve was located to the left, behind the initial wall of flames. Also, while the system could easily vent the normal pressure, its flow capacity was utterly incapable of handling the rapid increase to absolute caused by the intense heat of the fire.
North American had originally suggested the hatch open outward and use explosive bolts to blow the hatch in case of emergency, as had been done in Project Mercury. NASA did not agree, arguing the hatch could accidentally open, as it had on Grissom's "Liberty Bell 7" flight, so the Manned Spacecraft Center designers rejected the explosive design in favor of a mechanically operated one for the Gemini and Apollo programs.
Before the fire, the Apollo astronauts had recommended changing the design to an outward-opening hatch, and this was already slated for inclusion in the Block II Command Module design. According to Donald K. Slayton's testimony before the House investigation of the accident, this was based on ease of exit for spacewalks and at the end of flight, rather than for emergency exit.
Emergency preparedness.
The board noted that: the test planners had failed to identify the test as hazardous; the emergency equipment (such as gas masks) were inadequate to handle this type of fire; that fire, rescue, and medical teams were not in attendance; and that the spacecraft work and access areas contained many hindrances to emergency response such as steps, sliding doors, and sharp turns.
Choice of pure oxygen atmosphere.
When designing the Mercury spacecraft, NASA had considered using a nitrogen/oxygen mixture to reduce the fire risk near launch, but rejected it based on two considerations. First, nitrogen used with the in-flight pressure reduction carried the clear risk of decompression sickness (known as "the bends"). But the decision to eliminate the use of any gas but oxygen was crystalized when a serious accident occurred on April 21, 1960, in which McDonnell Aircraft test pilot G.B. North passed out and was seriously injured when testing a Mercury cabin / spacesuit atmosphere system in a vacuum chamber. The problem was found to be nitrogen-rich (oxygen-poor) air leaking from the cabin into his spacesuit feed. North American Aviation had suggested using an oxygen/nitrogen mixture for Apollo, but NASA overruled this. The pure oxygen design also carried the benefit of saving weight, by eliminating the need for nitrogen tanks.
In his monograph "Project Apollo: The Tough Decisions", Deputy Administrator Seamans wrote that NASA's single worst mistake in engineering judgment was not to run a fire test on the Command Module prior to the plugs-out test. In the first episode of the 2009 BBC documentary series "NASA: Triumph and Tragedy", Jim McDivitt said that NASA had no idea how a 100% oxygen atmosphere would influence burning. Similar remarks by other astronauts were expressed in the 2007 documentary film "In the Shadow of the Moon".
Other oxygen incidents.
Several fires in high-oxygen test environments had occurred prior to the Apollo fire. In 1962, USAF Colonel B. Dean Smith was conducting a test of the Gemini space suit with a colleague in a pure oxygen chamber at Brooks Air Force Base in San Antonio, Texas when a fire broke out, destroying the chamber. Smith and his partner narrowly escaped.
Other oxygen fire occurrences are documented in reports archived in the National Air and Space Museum, such as:
Incidents had also occurred in the Soviet space program, but due to the government's policy of secrecy, these were not disclosed to the West until well after the Apollo 1 fire. Cosmonaut Valentin Bondarenko died on March 23, 1961, from burns sustained in a fire while participating in a 15-day endurance experiment in a high-oxygen isolation chamber, less than three weeks before the first Vostok manned space flight; this was disclosed on January 28, 1986.
During the Voskhod 2 mission in March 1965, cosmonauts Pavel Belyayev and Alexey Leonov could not completely seal the spacecraft hatch after Leonov's historic first walk in space. The spacecraft's environmental control system responded to the leaking air by adding more oxygen to the cabin, causing the concentration level to rise as high as 45%. The crew and ground controllers worried about the possibility of fire, remembering Bondarenko's death four years earlier.
Political fallout.
Committees in both houses of the United States Congress with oversight of the space program soon launched investigations, including the Senate Committee on Aeronautical and Space Sciences, chaired by Senator Clinton P. Anderson. Seamans, Webb, Manned Space Flight Administrator Dr. George E. Mueller, and Apollo Program Director Maj Gen Samuel C. Phillips were called to testify before Anderson's committee.
In the February 27 hearing, Senator Walter F. Mondale asked Webb if he knew of a "report" of extraordinary problems with the performance of North American Aviation on the Apollo contract. Webb replied he did not, and deferred to his subordinates on the witness panel. Mueller and Phillips responded they too were unaware of any such "report".
However, in late 1965, just over a year before the accident, Phillips had headed a "tiger team" investigating the causes of inadequate quality, schedule delays, and cost overruns in both the Apollo CSM and the Saturn V second stage (for which North American was also prime contractor.) He gave an oral presentation (with transparencies) of his team's findings to Mueller and Seamans, and also presented them in a memo to North American president John L. Atwood, to which Mueller appended his own strongly worded memo to Atwood.
During Mondale's 1967 questioning about what was to become known as the "Phillips Report", Seamans was afraid Mondale might actually have seen a hard copy of Phillips' presentation, and responded that contractors have occasionally been subjected to on-site progress reviews; perhaps this was what Mondale's information referred to. Mondale continued to refer to "the Report" despite Phillips' refusal to characterize it as such, and angered by what he perceived as Webb's deception and concealment of important program problems from Congress, he questioned NASA's selection of North American as prime contractor. Seamans later wrote that Webb roundly chastised him in the cab ride leaving the hearing, for volunteering information which led to the disclosure of Phillips' memo.
On May 11, Webb issued a statement defending NASA's November 1961 selection of North American as the prime contractor for Apollo. This was followed on June 9 by Seamans filing a seven-page memorandum documenting the selection process. Webb eventually provided a controlled copy of Phillips' memo to Congress. The Senate committee noted in its final report NASA's testimony that "the findings of the hillip task force had no effect on the accident, did not lead to the accident, and were not related to the accident", but stated in its recommendations:
"Notwithstanding that in NASA's judgment the contractor later made significant progress in overcoming the problems, the committee believes it should have been informed of the situation. The committee does not object to the position of the Administrator of NASA, that all details of Government/contractor relationships should not be put in the public domain. However, that position in no way can be used as an argument for not bringing this or other serious situations to the attention of the committee."
Freshman Senators Edward W. Brooke III and Charles H. Percy jointly wrote an "Additional Views" section appended to the committee report, chastising NASA more strongly than Anderson for not having disclosed the Phillips review to Congress. Mondale wrote his own, even more strongly worded Additional View, accusing NASA of "evasiveness, … lack of candor, … patronizing attitude toward Congress, … refusal to respond fully and forthrightly to legitimate Congressional inquiries, and … solicitous concern for corporate sensitivities at a time of national tragedy."
The potential political threat to Apollo blew over, due in large part to the support of President Lyndon B. Johnson, who at the time still wielded a measure of influence with the Congress from his own Senatorial experience. He was a staunch supporter of NASA since its inception, had even recommended the Moon program to President John F. Kennedy in 1961, and was skilled at portraying it as part of Kennedy's legacy.
Internal acrimony developed between NASA and North American over assignment of blame. North American argued unsuccessfully it was not responsible for the fatal error in spacecraft atmosphere design. Finally, Webb contacted Atwood, and demanded either he or Chief Engineer Harrison A. Storms resign. Atwood elected to fire Storms.
On the NASA side, Joseph Shea became unfit for duty in the aftermath and was removed from his position, although not fired.
Program recovery.
Gene Kranz called a meeting of his staff in Mission Control three days after the accident, delivering a speech which has subsequently become one of NASA's principles. Speaking of the errors and overall attitude surrounding the Apollo program before the accident, he stated: "We were too 'gung-ho' about the schedule and we blocked out all of the problems we saw each day in our work. Every element of the program was in trouble and so were we." He reminded the team of the perils and mercilessness of their endeavor, and stated the new requirement that every member of every team in mission control be "tough and competent", requiring nothing less than perfection throughout NASA's programs. In 2003, following the Space Shuttle "Columbia" disaster, NASA administrator Sean O'Keefe quoted Kranz's speech, applying it to the "Columbia" crew.
Command Module redesign.
After the fire, the Apollo program was grounded for review and redesign. The Command Module was found to be extremely hazardous and in some instances, carelessly assembled (for example, a misplaced socket wrench was found in the cabin).
It was decided that remaining Block I spacecraft would be used only for unmanned Saturn V test flights. All manned missions would use the Block II spacecraft, to which many Command Module design changes were made:
Thorough protocols were implemented for documenting spacecraft construction and maintenance.
New mission naming scheme.
The astronauts' widows asked that "Apollo 1" be reserved for the flight their husbands never made, and on April 24, 1967, Associate Administrator for Manned Space Flight, Dr. George E. Mueller, announced this change officially: AS-204 would be recorded as Apollo 1, "first manned Apollo Saturn flight – failed on ground test". Since three unmanned Apollo missions (AS-201, AS-202, and AS-203) had previously occurred, the next mission, the first unmanned Saturn V test flight (AS-501) would be designated Apollo 4, with all subsequent flights numbered sequentially in the order flown. The first three flights would not be renumbered, and the names "Apollo 2" and "Apollo 3" would go unused.
The manned flight hiatus allowed work to catch up on the Saturn V and Lunar Module, which were encountering their own delays. Apollo 4 flew in November 1967. Apollo 1's (AS-204) Saturn IB rocket was taken down from Launch Complex 34, later reassembled at Launch complex 37B and used to launch Apollo 5, an unmanned Earth orbital test flight of the first Lunar Module LM-1, in January 1968. A second unmanned Saturn V AS-502 flew as Apollo 6 in April 1968, and Grissom's backup crew of Wally Schirra, Don Eisele, and Walter Cunningham, finally flew the orbital test mission as Apollo 7 (AS-205), in a Block II CSM in October 1968.
Memorials.
Gus Grissom and Roger Chaffee were buried at Arlington National Cemetery. Ed White was buried at West Point Cemetery on the grounds of the United States Military Academy in West Point, New York.
Their names are among those of several astronauts and cosmonauts who have died in the line of duty, listed on the Space Mirror Memorial at the Kennedy Space Center Visitor Complex in Merritt Island, Florida.
An Apollo 1 mission patch was left on the Moon's surface after the first manned lunar landing by Apollo 11 crew members Neil Armstrong and Buzz Aldrin.
The Apollo 15 mission left on the surface of the Moon a tiny memorial statue, "Fallen Astronaut", along with a plaque containing the names of the Apollo 1 astronauts, among others including Soviet cosmonauts, who perished in the pursuit of human space flight.
President Jimmy Carter awarded the Congressional Space Medal of Honor posthumously to Grissom on October 1, 1978. President Bill Clinton awarded it to White and Chaffee on December 17, 1997.
Launch Complex 34.
After the Apollo 1 fire, Launch Complex 34 was subsequently used only for the launch of Apollo 7 and later dismantled down to the concrete launch pedestal, which remains at the site () along with a few other concrete and steel-reinforced structures. The pedestal bears two plaques commemorating the crew. Each year the families of the Apollo 1 crew are invited to the site for a memorial, and the Kennedy Space Center Visitor Center includes the site in its tour of the historic Cape Canaveral launch sites.
In January 2005, three granite benches, built by a college classmate of one of the astronauts, were installed at the site on the southern edge of the launch pad. Each bears the name of one of the astronauts and his military service insignia.
Remains of CM-012.
The Apollo 1 Command Module has never been on public display. After the accident, the spacecraft was removed and taken to Kennedy Space Center to facilitate the review board's disassembly in order to investigate the cause of the fire. When the investigation was complete, it was moved to the NASA Langley Research Center in Hampton, Virginia, and placed in a secured storage warehouse.
On February 17, 2007, the parts of CM-012 were moved approximately to a newer, environmentally controlled warehouse. Only a few weeks earlier, Gus Grissom's brother Lowell publicly suggested CM-012 be permanently entombed in the concrete remains of Launch Complex 34.

</doc>
<doc id="1966" url="https://en.wikipedia.org/wiki?curid=1966" title="Apollo 10">
Apollo 10

Apollo 10 was the fourth manned mission in the United States Apollo space program, and the second (after Apollo 8) to orbit the Moon. Launched on May 18, 1969, it was the F mission: a "dress rehearsal" for the first Moon landing, testing all of the components and procedures, just short of actually landing. The Lunar Module (LM) came to within of the lunar surface, the point where the powered descent to the lunar surface would begin. Its success enabled the first landing to be attempted on Apollo 11 in July, 1969.
According to the 2002 "Guinness World Records", Apollo 10 set the record for the highest speed attained by a manned vehicle at 39,897 km/h (11.08 km/s or 24,791 mph) during the return from the Moon on May 26, 1969.
Due to the use of their names as call signs, the "Peanuts" characters Charlie Brown and Snoopy became semi-official mascots for the mission. "Peanuts" creator Charles Schulz also drew some special mission-related artwork for NASA.
Crew.
Crew notes.
Apollo 10 was the first of only two Apollo missions with an entirely flight-experienced crew (the other being Apollo 11). Thomas P. Stafford had flown on Gemini 6 and Gemini 9; John W. Young had flown on Gemini 3 and Gemini 10, and Eugene A. Cernan had flown with Stafford on Gemini 9.
In addition, Apollo 10 marked the only Saturn V flight from Launch Complex 39B, as preparations for Apollo 11 at LC-39A had begun in March almost immediately after Apollo 9's launch.
They were also the only Apollo crew all of whose members went on to fly subsequent missions aboard Apollo spacecraft: Young later commanded Apollo 16, Cernan commanded Apollo 17 and Stafford commanded the US vehicle on the Apollo–Soyuz Test Project.
The Apollo 10 crew holds the distinction of being the humans who have traveled to the farthest point away from home, some from their homes and families in Houston. While most Apollo missions orbited the Moon at the same from the lunar surface, timing makes this distinction possible as the distance between the Earth and Moon varies by approximately (between perigee and apogee) throughout the year, and the Earth's rotation make the distance to Houston vary by another each day. The Apollo 10 crew reached the farthest point in their orbit around the far side of the Moon at approximately the same time Earth had rotated around putting Houston nearly a full Earth diameter away. The Apollo 13 crew holds the distinction of being the farthest any human has traveled from the Earth's surface.
By the normal rotation in place during Apollo, the backup crew would have been scheduled to fly on Apollo 13. However, Alan Shepard was given the Apollo 13 command slot instead. L. Gordon Cooper, Jr., Commander of the Apollo 10 backup crew, was enraged and resigned from NASA. Later, Shepard's crew was forced to switch places with Jim Lovell's tentative Apollo 14 crew.
Deke Slayton wrote in his memoirs that Cooper and Donn F. Eisele were never intended to rotate to another mission as both were out of favor with NASA management for various reasons (Cooper for his lax attitude towards training and Eisele for incidents aboard Apollo 7 and an extramarital affair) and were assigned to the backup crew simply because of a lack of qualified manpower in the Astronaut Office at the time the assignment needed to be made. Cooper, Slayton noted, had a very small chance of receiving the Apollo 13 command if he did an outstanding job with the assignment, which he did not. Eisele, despite his issues with management, was always intended for future assignment to the Apollo Applications Program (which was eventually cut down to only the Skylab component) and not a lunar mission.
On May 22, 1969 at 20:35:02 UTC, a 27.4 second LM descent propulsion system burn inserted the LM into a descent orbit of so that the resulting lowest point in the orbit occurred about 15° from lunar landing site 2 (the Apollo 11 landing site). The lowest measured point in the trajectory was above the lunar surface at 21:29:43 UTC.
Mission highlights.
This dress rehearsal for a Moon landing brought the Apollo Lunar Module to from the lunar surface, at the point where powered descent would begin on the actual landing. Practicing this approach orbit would refine knowledge of the lunar gravitational field needed to calibrate the powered descent guidance system to within (LR altitude update lock) needed for a landing. Earth-based observations, unmanned spacecraft, and Apollo 8 had respectively allowed calibration to within , , and . Except for this final stretch, the mission went exactly as a landing would have gone, both in space and on the ground, putting NASA's flight controllers and extensive tracking and control network through a rehearsal.
Shortly after trans-lunar injection, the Command/Service Module (CSM) separated from the S-IVB stage, turned around, and docked its nose to the top of the Lunar Module (LM) still nestled in the S-IVB. The CSM/LM stack then separated from the S-IVB for the trip to the Moon.
Apollo 10 was the first mission to carry a color television camera inside the spacecraft, and made the first live color TV transmissions from space.
Upon reaching lunar orbit, Young remained alone in the Command Module (CM) "Charlie Brown" while Stafford and Cernan flew separately in the LM "Snoopy". The LM crew demonstrated their craft's radar and engines, rode out a momentary gyration in the lunar lander's motion (due to a faulty switch setting), and surveyed the Apollo 11 landing site in the Sea of Tranquility. The ascent stage was loaded with the amount of fuel it would have had remaining if it had lifted off from the surface and reached the altitude at which the Apollo 10 ascent stage fired. The fueled LM weighed , compared to for the Apollo 11 LM which made the first landing. Historian Craig Nelson wrote that NASA took special precaution to ensure Stafford and Cernan would not attempt to make the first landing. Nelson quoted Cernan as saying "A lot of people thought about the kind of people we were: 'Don't give those guys an opportunity to land, 'cause they might!' So the ascent module, the part we lifted off the lunar surface with, was short-fueled. The fuel tanks weren't full. So had we literally tried to land on the Moon, we couldn't have gotten off." In his own memoir, Cernan wrote "Our lander, LM-4...was still too heavy to guarantee safe margins for a moon landing."
Upon separation of the descent stage and ascent engine ignition, the Lunar Module began to roll violently due to the crew accidentally duplicating commands into the flight computer which took the LM out of abort mode, the correct configuration for this maneuver. The live network broadcasts caught Cernan and Stafford uttering several expletives before regaining control of the LM. Cernan has said he observed the horizon spinning eight times over, indicating eight rolls of the spacecraft under ascent engine power. While the incident was downplayed by NASA, the roll was just several revolutions from being unrecoverable, which would have resulted in the LM crashing into the lunar surface.
Splashdown occurred in the Pacific Ocean on May 26, 1969, at 16:52:23 UTC, approximately east of American Samoa. The astronauts were recovered by the USS "Princeton", and subsequently flown to Pago Pago International Airport in Tafuna for a greeting reception, before being flown on a C-141 cargo plane to Honolulu.
Hardware disposition.
The LM "Snoopy"'s descent stage was left in orbit, but eventually crashed onto the lunar surface because of the Moon's non-uniform gravitational field; its location was not tracked.
After being jettisoned, "Snoopy's" ascent stage engine was fired to fuel depletion, sending it on a trajectory past the Moon into a heliocentric orbit. The Apollo 11 ascent stage was left in lunar orbit to eventually crash; all subsequent ascent stages were intentionally steered into the Moon to obtain readings from seismometers placed on the surface, except for the one on Apollo 13, which did not land but was used as a "life boat" to get the crew back to Earth, and burned up in Earth's atmosphere.
"Snoopy"'s ascent stage orbit was not tracked after 1969, and its current location is unknown. In 2011, a group of amateur astronomers in the UK started a project to search for it.
The Command Module "Charlie Brown" is currently on loan to the Science Museum in London, where it is on display. "Charlie Brown"'s Service Module (SM) was jettisoned just before re-entry and burned up in the Earth's atmosphere.
After Apollo 10, NASA required astronauts to choose more "dignified" names for their command and lunar module. The requirement was unenforceable: Apollo 16 astronauts Young, Mattingly and Duke chose "Casper", as in Casper the Friendly Ghost, for their Command Module name. The idea was to give children a way to identify with the mission by using humor.
After the insertion into trans-Lunar orbit, the Saturn IVB third stage became a object where it would continue to orbit the Sun for many years. , it remains in orbit.
Mission insignia.
The shield-shaped emblem for the flight shows a large, three-dimensional Roman numeral X sitting on the Moon's surface, in Stafford's words, "to show that we had left our mark." Although it did not land on the Moon, the prominence of the number represents the significant contributions the mission made to the Apollo program. A CSM circles the Moon as an LM ascent stage flies up from its low pass over the lunar surface with its engine firing. The Earth is visible in the background. On the mission patch, a wide, light blue border carries the word APOLLO at the top and the crew names around the bottom. The patch is trimmed in gold. The insignia was designed by Allen Stevens of Rockwell International.
NASA reports
Multimedia

</doc>
<doc id="1967" url="https://en.wikipedia.org/wiki?curid=1967" title="Apollo 12">
Apollo 12

Apollo 12 was the sixth manned flight in the United States Apollo program and the second to land on the Moon (an H type mission). It was launched on November 14, 1969 from the Kennedy Space Center, Florida, four months after Apollo 11. Mission commander Charles "Pete" Conrad and Lunar Module Pilot Alan L. Bean performed just over one day and seven hours of lunar surface activity while Command Module Pilot Richard F. Gordon remained in lunar orbit. The landing site for the mission was located in the southeastern portion of the Ocean of Storms.
Unlike the first landing on Apollo 11, Conrad and Bean achieved a precise landing at their expected location, the site of the "Surveyor 3" unmanned probe, which had landed on April 20, 1967. They carried the first color television camera to the lunar surface on an Apollo flight, but transmission was lost after Bean accidentally destroyed the camera by pointing it at the Sun. On one of two moonwalks, they visited the "Surveyor" and removed some parts for return to Earth. The mission ended on November 24 with a successful splashdown.
Mission highlights.
Launch and transfer.
Apollo 12 launched on schedule from Kennedy Space Center, during a rainstorm. It was the first rocket launch attended by an incumbent US president, Richard Nixon. Thirty-six-and-a-half seconds after lift-off, the vehicle triggered a lightning discharge through itself and down to the Earth through the Saturn's ionized plume. Protective circuits on the fuel cells in the Service Module (SM) falsely detected overloads and took all three fuel cells offline, along with much of the Command/Service Module (CSM) instrumentation. A second strike at 52 seconds after launch knocked out the "8-ball" attitude indicator. The telemetry stream at Mission Control was garbled. However, the vehicle continued to fly correctly; the strikes had not affected the Saturn V Instrument Unit.
The loss of all three fuel cells put the CSM entirely on batteries, which were unable to maintain normal 75-ampere launch loads on the 28-volt DC bus. One of the AC inverters dropped offline. These power supply problems lit nearly every warning light on the control panel and caused much of the instrumentation to malfunction.
Electrical, Environmental and Consumables Manager (EECOM) John Aaron remembered the telemetry failure pattern from an earlier test when a power supply malfunctioned in the CSM Signal Conditioning Equipment (SCE), which converted raw signals from instrumentation to standard voltages for the spacecraft instrument displays and telemetry encoders.
Aaron made a call, "Try SCE to aux," which switched the SCE to a backup power supply. The switch was fairly obscure, and neither Flight Director Gerald Griffin, CAPCOM Gerald Carr, nor Mission Commander Pete Conrad immediately recognized it. Lunar Module Pilot Alan Bean, flying in the right seat as the spacecraft systems engineer, remembered the SCE switch from a training incident a year earlier when the same failure had been simulated. Aaron's quick thinking and Bean's memory saved what could have been an aborted mission, and earned Aaron the reputation of a "steely-eyed missile man". Bean put the fuel cells back on line, and with telemetry restored, the launch continued successfully. Once in Earth parking orbit, the crew carefully checked out their spacecraft before re-igniting the S-IVB third stage for trans-lunar injection. The lightning strikes had caused no serious permanent damage.
Initially, it was feared that the lightning strike could have caused the Command Module's (CM) parachute mechanism to prematurely fire, disabling the explosive bolts that open the parachute compartment to deploy them. If they were indeed disabled, the Command Module would have crashed uncontrollably into the Pacific Ocean and killed the crew instantly. Since there was no way to figure out whether or not this was the case, ground controllers decided not to tell the astronauts about the possibility. The parachutes deployed and functioned normally at the end of the mission.
After Lunar Module (LM) separation, the S-IVB was intended to fly into solar orbit. The S-IVB auxiliary propulsion system was fired, and the remaining propellants vented to slow it down to fly past the Moon's trailing edge (the Apollo spacecraft always approached the Moon's leading edge). The Moon's gravity would then slingshot the stage into solar orbit. However, a small error in the state vector in the Saturn's guidance system caused the S-IVB to fly past the Moon at too high an altitude to achieve Earth escape velocity. It remained in a semi-stable Earth orbit after passing the Moon on November 18, 1969. It finally escaped Earth orbit in 1971 but was briefly recaptured in Earth orbit 31 years later. It was discovered by amateur astronomer Bill Yeung who gave it the temporary designation J002E3 before it was determined to be an artificial object.
Moon Landing.
The Apollo 12 mission landed on November 19, 1969, on an area of the Ocean of Storms that had been visited earlier by several unmanned missions ("Luna 5", "Surveyor 3", and "Ranger 7"). The International Astronomical Union, recognizing this, christened this region "Mare Cognitum (Known Sea)". The Lunar coordinates of the landing site were 3.01239° S latitude, 23.42157° W longitude. The landing site would thereafter be listed as "Statio Cognitum" on lunar maps. Conrad and Bean did not formally name their landing site, though Conrad nicknamed the intended touchdown area "Pete's Parking Lot".
The second lunar landing was an exercise in precision targeting, which would be needed for future Apollo missions. Most of the descent was automatic, with manual control assumed by Conrad during the final few hundred feet of descent. Unlike Apollo 11, where Neil Armstrong had to use the manual control to direct his lander downrange of the computer's target which was strewn with boulders, Apollo 12 succeeded in landing at its intended target – within walking distance of the "Surveyor 3" probe, which had landed on the Moon in April 1967. This was the first – and, to date, only – occasion in which humans have "caught up" to a probe sent to land on another world.
Conrad actually landed "Intrepid" short of "Pete's Parking Lot", because it looked rougher during final approach than anticipated, and was a little under from "Surveyor 3", a distance that was chosen to eliminate the possibility of lunar dust (being kicked up by "Intrepid's" descent engine during landing) from covering "Surveyor 3". But the actual touchdown point–approximately from "Surveyor 3"–did cause high velocity sandblasting of the probe. It was later determined that the sandblasting removed more dust than it delivered onto the "Surveyor", because the probe was covered by a thin layer that gave it a tan hue as observed by the astronauts, and every portion of the surface exposed to the direct sandblasting was lightened back toward the original white color through the removal of lunar dust.
EVAs.
When Conrad, who was somewhat shorter than Neil Armstrong, stepped onto the lunar surface, his first words were "Whoopie! Man, that may have been a small one for Neil, but that's a long one for me." This was not an off-the-cuff remark: Conrad had made a bet with reporter Oriana Fallaci he would say these words, after she had queried whether NASA had instructed Neil Armstrong what to say as he stepped onto the Moon. Conrad later said he was never able to collect the money.
To improve the quality of television pictures from the Moon, a color camera was carried on Apollo 12 (unlike the monochrome camera that was used on Apollo 11). Unfortunately, when Bean carried the camera to the place near the Lunar Module where it was to be set up, he inadvertently pointed it directly into the Sun, destroying the Secondary Electron Conduction (SEC) tube. Television coverage of this mission was thus terminated almost immediately. See also: Apollo TV camera.
Apollo 12 successfully landed within walking distance of the "Surveyor 3" probe. Conrad and Bean removed pieces of the probe to be taken back to Earth for analysis. It is claimed that the common bacterium "Streptococcus mitis" was found to have accidentally contaminated the spacecraft's camera prior to launch and survived dormant in this harsh environment for two and a half years. However, this finding has since been disputed: see Reports of "Streptococcus mitis" on the Moon.
Astronauts Conrad and Bean also collected rocks and set up equipment that took measurements of the Moon's seismicity, solar wind flux and magnetic field, and relayed the measurements to Earth. The instruments were part of the first complete nuclear-powered ALSEP station set up by astronauts on the Moon to relay long-term data from the lunar surface. The instruments on Apollo 11 were not as extensive or designed to operate long term. The astronauts also took photographs, although by accident Bean left several rolls of exposed film on the lunar surface. Meanwhile, Gordon, on board the "Yankee Clipper" in lunar orbit, took multi-spectral photographs of the surface.
The lunar plaque attached to the descent stage of "Intrepid" is unique in that unlike the other plaques, it (a) did not have a depiction of the Earth, and (b) it was textured differently (the other plaques had black lettering on polished stainless steel while the Apollo 12 plaque had the lettering in polished stainless steel while the background was brushed flat).
Return.
"Intrepid's" ascent stage was dropped (per normal procedures) after Conrad and Bean rejoined Gordon in orbit. It impacted the Moon on November 20, 1969, at . The seismometers the astronauts had left on the lunar surface registered the vibrations for more than an hour.
The crew stayed an extra day in lunar orbit taking photographs, for a total lunar surface stay of 31 and a half hours and a total time in lunar orbit of eighty-nine hours.
On the return flight to Earth after leaving lunar orbit, the crew of Apollo 12 witnessed (and photographed) a solar eclipse, though this one was of the Earth eclipsing the Sun.
Splashdown.
"Yankee Clipper" returned to Earth on November 24, 1969 at 20:58 UTC (3:58pm EST, 10:58am HST), in the Pacific Ocean, approximately 500 nautical miles (800 km) east of American Samoa. During splashdown, a 16 mm film camera dislodged from storage and struck Bean in the forehead, rendering him briefly unconscious. He suffered a mild concussion and needed six stitches. After recovery by the USS "Hornet", they were flown to Pago Pago International Airport in Tafuna for a reception, before being flown on a C-141 cargo plane to Honolulu.
Mission insignia.
The Apollo 12 mission patch shows the crew's navy background; all three astronauts at the time of the mission were U.S. Navy commanders. It features a clipper ship arriving at the Moon, representing the Command Module "Yankee Clipper". The ship trails fire, and flies the flag of the United States. The mission name APOLLO XII and the crew names are on a wide gold border, with a small blue trim. Blue and gold are traditional U.S. Navy colors. The patch has four stars on it — one each for the three astronauts who flew the mission and one for Clifton Williams, a U.S. naval aviator and astronaut who was killed on October 5, 1967, after a mechanical failure caused the controls of his T-38 trainer to stop responding and crash. He trained with Conrad and Gordon as part of the backup crew for what would be the Apollo 9 mission, and would have been assigned as Lunar Module Pilot for Apollo 12.
Spacecraft location.
The Apollo 12 Command Module "Yankee Clipper" is on display at the Virginia Air and Space Center in Hampton, Virginia.
In 2002, astronomers thought they might have discovered another moon orbiting Earth, which they designated J002E3, that turned out to be the third stage of the Apollo 12 Saturn V rocket.
The Lunar Module "Intrepid" impacted the Moon November 20, 1969 at 22:17:17.7 UT (5:17 PM EST) . In 2009, the Lunar Reconnaissance Orbiter (LRO) photographed the Apollo 12 landing site. The "Intrepid" Lunar Module descent stage, experiment package (ALSEP), "Surveyor 3" spacecraft, and astronaut footpaths are all visible. In 2011, the LRO returned to the landing site at a lower altitude to take higher resolution photographs.
Depiction in media.
Portions of the Apollo 12 mission are dramatized in the miniseries "From the Earth to the Moon" episode entitled "That's All There Is". Conrad, Gordon, and Bean were portrayed by Paul McCrane, Tom Verica, and Dave Foley, respectively. Conrad had been portrayed by a different actor, Peter Scolari, in the first episode.
NASA reports
Multimedia

</doc>
<doc id="1968" url="https://en.wikipedia.org/wiki?curid=1968" title="Apollo 14">
Apollo 14

Apollo 14 was the eighth manned mission in the United States Apollo program, and the third to land on the Moon. It was the last of the "H missions," targeted landings with two-day stays on the Moon with two lunar EVAs, or moonwalks.
Commander Alan Shepard, Command Module Pilot Stuart Roosa, and Lunar Module Pilot Edgar Mitchell launched on their nine-day mission on January 31, 1971 at 4:04:02 p.m. local time after a 40-minute, 2 second delay due to launch site weather restrictions, the first such delay in the Apollo program. Shepard and Mitchell made their lunar landing on February 5 in the Fra Mauro formation - originally the target of the aborted Apollo 13 mission. During the two lunar EVAs, of Moon rocks were collected, and several scientific experiments were performed. Shepard hit two golf balls on the lunar surface with a makeshift club he had brought from Earth. Shepard and Mitchell spent 33½ hours on the Moon, with almost 9½ hours of EVA.
In the aftermath of Apollo 13, several modifications were made to the Service Module electrical power system to prevent a repeat of that accident, including redesign of the oxygen tanks and addition of a third tank.
While Shepard and Mitchell were on the surface, Roosa remained in lunar orbit aboard the Command/Service Module "Kitty Hawk", performing scientific experiments and photographing the Moon, including the landing site of the future Apollo 16 mission. He took several hundred seeds on the mission, many of which were germinated on return, resulting in the so-called Moon trees. Shepard, Roosa, and Mitchell landed in the Pacific Ocean on February 9.
Crew.
Shepard was the oldest U.S. astronaut when he made his trip aboard Apollo 14. He is the only astronaut from Project Mercury (the original Mercury Seven astronauts) to reach the Moon. Another of the original seven, Gordon Cooper, had (as Apollo 10's backup commander) tentatively been scheduled to command the mission, but according to author Andrew Chaikin, his casual attitude toward training, along with problems with NASA hierarchy (reaching all the way back to the Mercury-Atlas 9 flight), resulted in his removal.
The mission was a personal triumph for Shepard, who had battled back from Ménière's disease which grounded him from 1964 to 1968. He and his crew were originally scheduled to fly on Apollo 13, but in 1969 NASA officials switched the scheduled crews for Apollos 13 and 14. This was done to allow Shepard more time to train for his flight, as he had been grounded for four years.
All three crew members are now dead, making Apollo 14 the first of the six successful moon landing missions whose crew has all died. Roosa died in 1994 from pancreatitis; Shepard in 1998 from leukemia and Mitchell in 2016.
Mission parameters.
Geocentric:
Selenocentric:
Mission highlights.
Launch and flight to lunar orbit.
Apollo 14 launched during heavy cloud cover and the Saturn V booster quickly disappeared from view. NASA's long-range cameras, based 60 miles south in Vero Beach, had a clear shot of the remainder of the launch. Following the launch, the Launch Control Center at Kennedy Space Center was visited by U.S. Vice President Spiro T. Agnew, Prince Juan Carlos of Spain, and his wife, Princess Sofía.
At the beginning of the mission, the CSM "Kitty Hawk" had difficulty achieving capture and docking with the LM "Antares". Repeated attempts to dock went on for 1 hour and 42 minutes, until it was suggested that Roosa hold "Kitty Hawk" against "Antares" using its thrusters, then the docking probe would be retracted out of the way, hopefully triggering the docking latches. This attempt was successful, and no further docking problems were encountered during the mission.
Lunar descent.
After separating from the Command Module in lunar orbit, the LM "Antares" also had two serious problems. First, the LM computer began getting an ABORT signal from a faulty switch. NASA believed that the computer might be getting erroneous readings like this if a tiny ball of solder had shaken loose and was floating between the switch and the contact, closing the circuit. The immediate solution — tapping on the panel next to the switch — did work briefly, but the circuit soon closed again. If the problem recurred after the descent engine fired, the computer would think the signal was real and would initiate an auto-abort, causing the ascent stage to separate from the descent stage and climb back into orbit. NASA and the software teams at the Massachusetts Institute of Technology scrambled to find a solution, and determined the fix would involve reprogramming the flight software to ignore the false signal. The software modifications were transmitted to the crew via voice communication, and Mitchell manually entered the changes (amounting to over 80 keystrokes on the LM computer pad) just in time.
A second problem occurred during the powered descent, when the LM landing radar failed to lock automatically onto the Moon's surface, depriving the navigation computer of vital information on the vehicle's altitude and vertical descent speed (this was not a result of the modifications to the ABORT command; rather, the post-mission report indicated it was an unrelated bug in the radar's operation). After the astronauts cycled the landing radar breaker, the unit successfully acquired a signal near , again just in time. Shepard then manually landed the LM closer to its intended target than any of the other six Moon landing missions. Mitchell believes that Shepard would have continued with the landing attempt without the radar, using the LM inertial guidance system and visual cues. A post-flight review of the descent data showed the inertial system alone would have been inadequate, and the astronauts probably would have been forced to abort the landing as they approached the surface.
Lunar surface operations.
Shepard and Mitchell named their landing site "Fra Mauro Base", and this designation is recognized by the International Astronomical Union (depicted in Latin on lunar maps as "Statio Fra Mauro").
Shepard's first words, after stepping onto the lunar surface were, "And it's been a long way, but we're here." Unlike Neil Armstrong on Apollo 11 and Pete Conrad on Apollo 12, Shepard had already stepped off the LM footpad and was a few yards (meters) away before he spoke.
Shepard's moonwalking suit was the first to utilize red stripes on the arms and legs and on the top of the lunar EVA sunshade "hood," so as to allow easy identification between the commander and LM pilot on the surface; on the Apollo 12 pictures, it had been almost impossible to distinguish between the two crewmen, causing a great deal of confusion. This feature was included on Jim Lovell's Apollo 13 suit; because no landing was made on that mission, Apollo 14 was the first to make use of it. This feature was used for the remaining Apollo missions, and for the EVAs of Space Shuttle flights afterwards, and it is still in use today on both the U.S. and Russian space suits on the International Space Station.
After landing in the Fra Mauro formation—the destination for Apollo 13—Shepard and Mitchell took two moonwalks, adding new seismic studies to the by now familiar Apollo Lunar Surface Experiments Package (ALSEP), and using the Modular Equipment Transporter (MET), a pull-cart for carrying equipment and samples, nicknamed "lunar rickshaw". Roosa, meanwhile, took pictures from on board Command Module "Kitty Hawk" in lunar orbit.
The second moonwalk, or EVA, was intended to reach the rim of the wide Cone Crater. The two astronauts were not able to find the rim amid the rolling terrain of the crater's slopes. They became physically exhausted from the attempt and with their suits' oxygen supplies starting to run low, the effort was called off. Later analysis, using the pictures that they took, determined that they had come within an estimated of the crater's rim. Images from the Lunar Reconnaissance Orbiter (LRO) show the tracks of the astronauts and the MET come to within 30 m of the rim.
Shepard and Mitchell deployed and activated various scientific instruments and experiments and collected almost of lunar samples for return to Earth. Other Apollo 14 achievements included the only use of MET; longest distance traversed by foot on the lunar surface; first use of shortened lunar orbit rendezvous techniques; and the first extensive orbital science period conducted during CSM solo operations.
The astronauts also engaged in less serious activities on the Moon. Shepard brought along a six iron golf club head which he could attach to the handle of a lunar excavation tool, and two golf balls, and took several one-handed swings (due to the limited flexibility of the EVA suit). He exuberantly exclaimed that the second ball went "miles and miles and miles" in the low lunar gravity, but later estimated the distance as . Mitchell then threw a lunar scoop handle as if it were a javelin.
Return, splashdown and quarantine.
On the way back to Earth, the crew conducted the first U.S. materials processing experiments in space.
The Command Module "Kitty Hawk" splashed down in the South Pacific Ocean on February 9, 1971 at 21:05 T, approximately south of American Samoa. After recovery by the ship USS "New Orleans", the crew was flown to Pago Pago International Airport in Tafuna for a reception before being flown on a C-141 cargo plane to Honolulu. The Apollo 14 astronauts were the last lunar explorers to be quarantined on their return from the Moon.
Roosa, who worked in forestry in his youth, took several hundred tree seeds on the flight. These were germinated after the return to Earth, and widely distributed around the world as commemorative Moon trees.
Mission insignia.
The oval insignia shows a gold NASA Astronaut Pin, given to U.S. astronauts upon completing their first space flight, traveling from the Earth to the Moon. A gold band around the edge includes the mission and astronaut names. The designer was Jean Beaulieu.
The backup crew spoofed the patch with its own version, with revised artwork showing a Wile E. Coyote cartoon character depicted as gray-bearded (for Shepard, who was 47 at the time of the mission and the oldest man on the Moon), pot-bellied (for Mitchell, who had a pudgy appearance) and red furred (for Roosa's red hair), still on the way to the Moon, while Road Runner (for the backup crew) is already on the Moon, holding a U.S. flag and a flag labeled "1st Team." The flight name is replaced by "BEEP BEEP" and the backup crew's names are given. Several of these patches were hidden by the backup crew and found during the flight by the crew in notebooks and storage lockers in both the CSM "Kitty Hawk" and the LM "Antares" spacecraft, and one patch was even stored on the MET lunar hand cart.
Spacecraft location.
The Apollo 14 Command Module "Kitty Hawk" is on display at the Apollo/Saturn V Center building at the Kennedy Space Center after being on display at the United States Astronaut Hall of Fame near Titusville, Florida, for several years.
The ascent stage of Lunar Module "Antares" impacted the Moon on February 7, 1971 at 00:45:25.7 UT (February 6, 7:45 PM EST) . "Antares"' descent stage and the mission's other equipment remain at Fra Mauro at .
Photographs taken in 2009 by the Lunar Reconnaissance Orbiter were released on July 17, and the Fra Mauro equipment was the most visible Apollo hardware at that time, owing to particularly good lighting conditions. In 2011, the LRO returned to the landing site at a lower altitude to take higher resolution photographs.
NASA reports
Multimedia

</doc>
<doc id="1969" url="https://en.wikipedia.org/wiki?curid=1969" title="Apollo 15">
Apollo 15

Apollo 15 was the ninth manned mission in the United States' Apollo program, the fourth to land on the Moon, and the eighth successful manned mission. It was the first of what were termed "J missions", long stays on the Moon, with a greater focus on science than had been possible on previous missions. It was also the first mission on which the Lunar Roving Vehicle was used.
The mission began on July 26, 1971, and ended on August 7. At the time, NASA called it the most successful manned flight ever achieved.
Commander David Scott and Lunar Module Pilot James Irwin spent three days on the Moon, including 18½ hours outside the spacecraft on lunar extra-vehicular activity (EVA). The mission landed near Hadley rille, in an area of the Mare Imbrium called "Palus Putredinus" (Marsh of Decay). The crew explored the area using the first lunar rover, which allowed them to travel much farther from the Lunar Module (LM) than had been possible on missions without the rover. They collected of lunar surface material. At the same time, Command Module Pilot Alfred Worden orbited the Moon, using a Scientific Instrument Module (SIM) in the Service Module (SM) to study the lunar surface and environment in great detail with a panoramic camera, a gamma-ray spectrometer, a mapping camera, a laser altimeter, a mass spectrometer, and a lunar sub-satellite deployed at the end of Apollo 15's stay in lunar orbit (an Apollo program first).
The mission successfully accomplished its objectives, but was marred by negative publicity that accompanied disclosure of the crew carrying unauthorized postage stamps which they had planned to sell after their return. Ironically, this mission was one of very few that had been honored with the issue of a commemorative US stamp, with this first use of a lunar rover happening one decade after the first Mercury astronaut launch.
Crew.
All three astronauts on the all-United States Air Force crew received an honorary degree or master's degree from the University of Michigan, including Scott's honorary degree, awarded in the spring of 1971, months before the launch. Scott had attended the University of Michigan, but left before graduating to accept an appointment to the United States Military Academy. The crewmen did their undergraduate work at either the United States Military Academy or the United States Naval Academy.
Backup crew.
Schmitt was the first member of Group 4 to be selected as a prime or backup crew member for an Apollo flight; from Group 4 he was the only astronaut to make it to the Moon, with the last Apollo mission at the end of 1972.
Planning and training.
The crew for Apollo 15 had previously served as the backup crew for Apollo 12. There had been a friendly rivalry between that prime and backup crew on that mission, with the prime being all United States Navy, and the backup all United States Air Force.
Originally Apollo 15 would have been an H mission, like Apollos 12, 13 and 14. But on September 2, 1970, NASA announced it was canceling what were to be the current incarnations of the Apollo 15 and Apollo 19 missions. To maximize the return from the remaining missions, Apollo 15 would now fly as a J mission and have the honor of carrying the first lunar rover.
One of the major changes in the training for Apollo 15 was the geology training. Although on previous flights the crews had been trained in field geology, for the first time Apollo 15 would make it a high priority. Scott and Irwin would train with Leon Silver, a Caltech geologist who on Earth was interested in the Precambrian. Silver had been suggested by Harrison Schmitt as an alternative to the classroom lecturers that NASA had previously used. Among other things, Silver had made important refinements to the methods for dating rocks using the decay of uranium into lead in the late 1950s.
At first Silver would take the prime and backup crews to various geological sites in Arizona and New Mexico as if for a normal field geology lesson, but as launch time approached, these trips became more realistic. Crews began to wear mock-ups of the backpacks they would carry, and communicate using walkie-talkies to a CAPCOM in a tent. (During a mission the Capsule Communicators (CAPCOMs), always fellow astronauts, were the only people who normally would speak to the crew.) The CAPCOM was accompanied by a group of geologists unfamiliar with the area who would rely on the astronauts' descriptions to interpret the findings.
The decision to land at Hadley came in September 1970. The Site Selection Committees had narrowed the field down to two sites — Hadley Rille or the crater Marius, near which were a group of low, possibly volcanic, domes. Although not ultimately his decision, the commander of a mission always held great sway. To David Scott the choice was clear, with Hadley, being "exploration at its finest."
Command Module Pilot Alfred Worden undertook a different kind of geology training. Working with an Egyptian-born geologist, Farouk El-Baz, he flew over areas in an airplane simulating the speed at which terrain would pass below him while in the Apollo Command/Service Module (CSM) in orbit. He became quite adept at making geologic observations as objects passed below.
Mission highlights.
Launch and outbound trip.
Apollo 15 was launched on July 26, 1971, at 9:34 AM EDT from the Kennedy Space Center, at Cape Canaveral, Florida. During the launch, the S-IC did not completely shut off following staging for four seconds, creating the possibility of the spent stage banging into the S-II engines, damaging them and forcing an abort (the S-II exhaust also struck a telemetry package on the S-IC and caused it to fail). Despite this, the third stage and spacecraft reached its planned Earth parking orbit. A couple of hours into the mission, the third stage reignited to propel the spacecraft out of Earth orbit and on to the Moon.
A few days after launching from Florida, the spacecraft passed behind the far side of the Moon, where the Service Propulsion System (SPS) engine on the CSM ignited for a six-minute burn, to slow the craft down into an initial lunar orbit. Once the lowest point of altitude in the orbit was reached, the SPS engine was fired again, to place the spacecraft into the proper descent orbit for the Lunar Module landing at Hadley.
Moon Landing.
Most of the first part of the day after arriving in lunar orbit on July 30 was spent in preparing the Lunar Module for descent to the lunar surface later on that day. When preparations were complete, un-docking from the CSM was attempted; it did not occur, because of a faulty seal in the hatch mechanism. The Command Module Pilot, Alfred Worden, re-sealed the hatch; the LM then separated from the CSM. David Scott and James Irwin continued preparations for the descent while Worden remained in the CSM, returning to a higher orbit to perform lunar observations and await his crewmates' return a few days later.
Soon, Scott and Irwin began the descent to the Hadley landing site. Several minutes after descent was initiated, at pitch-over and the beginning of the approach phase of the landing, the LM was six kilometers east of the pre-selected landing target. On learning this, Scott altered the flight path of the LM. They touched down at 22:16:29 UTC on July 30 at Hadley, within a few hundred meters of the planned landing site. One of the legs of the LM landed in a small crater so the module was tilted by 10° – the maximum acceptable was 15°. While previous crews had exited the Lunar Module shortly after landing, the crew of Apollo 15 elected to spend the rest of the day inside the LM, waiting until the next day to perform the first of three EVAs, or moonwalks, in order to preserve their sleep rhythm on a mission on which they were to spend a significantly longer time on the surface than previous crews had spent. Before they slept, Scott performed a stand-up EVA, during which the LM was depressurized and he photographed their surroundings from the top docking hatch.
Lunar surface.
Throughout the sleep period, Mission Control, in Houston, monitored a slow but steady oxygen leak. The data output of the onboard telemetry computers was limited during the night to conserve energy, so controllers could not determine the exact cause of the leak without awaking the crew. Scott and Irwin eventually were awakened an hour early, and the source of the leak was found to be an open valve on the urine transfer device. After the problem was solved, the crew began preparation for the first Moon walk.
Four hours later, Scott and Irwin became the seventh and eighth humans, respectively, to walk on the Moon. After unloading the Lunar Roving Vehicle (LRV), the two drove to the first moonwalk's primary destination, Elbow Crater, along the edge of Hadley Rille. On returning to the LM "Falcon", Scott and Irwin deployed the Apollo Lunar Surface Experiments Package (ALSEP). The first EVA lasted about 6½ hours.
The target of the second EVA, the next day, was the edge of Mount Hadley Delta, where the pair sampled boulders and craters along the Apennine Front. During this moonwalk, the astronauts recovered what came to be one of the more famous lunar samples collected on the Moon during Apollo, sample #15415, more commonly known as the "Genesis Rock." Once back at the landing site, Scott continued to try to drill holes for an experiment at the ALSEP site, with which he had struggled the day before. After conducting soil-mechanics experiments and erecting a U.S. flag, Scott and Irwin returned to the LM. EVA 2 lasted 7 hours and 12 minutes.
During EVA 3, the third and final moonwalk of the mission, the crew again ventured to the edge of Hadley Rille, this time to the northwest of the immediate landing site. After returning to the LM's location, Scott performed an experiment in view of the television camera, using a feather and hammer to demonstrate Galileo's theory that all objects in a given gravity field fall at the same rate, regardless of mass (in the absence of aerodynamic drag). He dropped the hammer and feather at the same time; because of the negligible lunar atmosphere, there was no drag on the feather, which hit the ground at the same time as the hammer.
Scott then drove the rover to a position away from the LM, where the television camera could be used to observe the lunar liftoff. Before the mission, the crew had contacted Belgian sculptor Paul Van Hoeydonck to create a small aluminum statuette called "Fallen Astronaut" to commemorate those astronauts and cosmonauts who lost their lives in the pursuit of space exploration. Scott left the sculpture by the rover, along with a plaque bearing the names of 14 American astronauts and Soviet cosmonauts who were known up to that time. The memorial was left while the television camera was turned off; only Irwin knew what Scott was doing at the time. Scott told mission control he was doing some cleanup activities around the rover.
The EVA lasted 4 hours and 50 minutes. In total, the two astronauts spent 18½ hours outside the LM and collected approximately of lunar samples.
Return to Earth.
After lifting off from the lunar surface 2 days and 18 hours after landing, the LM ascent stage rendezvoused and re-docked with the CSM with Worden aboard in orbit. After transferring samples and other items from the LM to the CSM, the LM was sealed off, jettisoned, and intentionally crashed into the lunar surface. After completing more observations of the Moon from orbit and releasing the sub-satellite, the three-person crew departed lunar orbit with another burn of the SPS engine.
The next day, on the return trip to Earth, Worden performed a spacewalk in deep space, the first of its kind, to retrieve exposed film from the SIM bay. Later on in the day, the crew set a record for the longest Apollo flight to that point.
On approach to Earth the next day, August 7, the Service Module was jettisoned, and the Command Module (CM) reentered the Earth's atmosphere. Although one of the three parachutes on the CM failed to deploy properly, only two were required for a safe landing (one extra for redundancy). Upon landing in the North Pacific Ocean, the crew were recovered and taken aboard the recovery ship, the USS "Okinawa" after a mission lasting 12 days, 7 hours, 11 minutes, and 53 seconds.
Hardware.
Spacecraft.
Apollo 15 used Command/Service Module CSM-112, which was given the call sign "Endeavour", named after the HMS "Endeavour" and Lunar Module LM-10, call sign "Falcon", named after the United States Air Force Academy mascot. If Apollo 15 had flown as an H mission, it would have been with CSM-111 and LM-9. That CSM was used by the Apollo–Soyuz Test Project in 1975, but the Lunar Module went unused and is now on display at the Kennedy Space Center Visitor Complex.
After re-entry, one of "Endeavour"'s three main parachutes collapsed after opening. Only two of the three parachutes were required for safe splashdown; the third was a contingency. "Endeavour" ultimately splashed down safely to end the mission.
Technicians at the Kennedy Space Center had many problems with the SIM bay in the Service Module. It was the first time it had flown and experienced problems from the start. Problems came from the fact the instruments were designed to operate in zero gravity, but had to be tested in the 1 g on the surface of the Earth. As such, things like the 7.5 m booms for the mass and gamma ray spectrometers could only be tested using railings that tried to mimic the space environment, and so they never worked particularly well. When the technicians tried to integrate the entire bay into the rest of the spacecraft, data streams would not synchronize, and lead investigators of the instruments would want to make last minute checks and changes. When it came time to test the operation of the gamma-ray spectrometer, it was necessary to stop every engine within of the test site.
On the Lunar Module, the fuel and oxidizer tanks were enlarged on both the descent and ascent stages and the engine bell on the descent stage was extended. Batteries and solar cells were added for increased electrical power. In all this increased the weight of the Lunar Module to , heavier than previous models.
"Endeavour" is currently on display at the National Museum of the United States Air Force at Wright-Patterson Air Force Base in Dayton, Ohio.
Lunar Rover.
The Lunar Roving Vehicle had been in development since May 1969, with the contract awarded to Boeing. It could be folded into a space 5 ft by 20 in (1.5 m by 0.5 m). Unloaded it weighed 460 lb (209 kg) and when carrying two astronauts and their equipment, 1500 lb (700 kg). Each wheel was independently driven by a ¼ horsepower (200 W) electric motor. Although it could be driven by either astronaut, the Commander always drove. Travelling at speeds up to 6 to 8 mph (10 to 12 km/h), it meant that for the first time the astronauts could travel far afield from their lander and still have enough time to do some scientific experiments.
Lunar subsatellite.
The Apollo 15 subsatellite (PFS-1) was a small satellite released into lunar orbit from the SIM bay. Its main objectives were to study the plasma, particle, and magnetic field environment of the Moon and map the lunar gravity field. Specifically, it measured plasma and energetic particle intensities and vector magnetic fields, and facilitated tracking of the satellite velocity to high precision. A basic requirement was that the satellite acquire fields and particle data everywhere on the orbit around the Moon. The Moon's roughly circular orbit about the Earth at ~380,000 km (60 Earth radii) carried the subsatellite into both interplanetary space and various regions of the Earth's magnetosphere. The satellite orbited the Moon and returned data from August 4, 1971 until January 1973.
In later years, through a study of many lunar orbiting satellites, scientists came to discover that most low lunar orbits (LLO) are unstable. Fortunately, PFS-1 had been placed, unknown to mission planners at the time, very near to one of only four lunar "frozen orbits", where a lunar satellite may remain indefinitely.
Releasing the subsatellite was the crew's final activity in lunar orbit, occurring an hour before the burn to take them back to Earth. A virtually identical subsatellite was deployed by Apollo 16.
Launch vehicle.
The Saturn V that launched Apollo 15 was designated SA-510, the tenth flight-ready model of the rocket. As the payload of the rocket was greater, changes were made to its launch trajectory and Saturn V itself. The rocket was launched in a more southerly direction (80–100 degrees azimuth) and the Earth parking orbit lowered to above the Earth's surface. These two changes meant more could be launched. The propellant reserves were reduced and the number of retrorockets on the S-IC first stage (used to separate the spent first stage from the S-II second stage) reduced from eight to four. The four outboard engines of the S-IC would be burned longer and the center engine would also burn longer before being shut down (see Saturn V for more information on the launch sequence). Changes were also made to the S-II to stop pogo oscillations.
Once all the various components had been installed on the Saturn V, it was moved to the launch site, Launch Complex 39A. During late June and early July 1971, the rocket and Launch Umbilical Tower (LUT) were struck by lightning at least four times. All was well however, with only minor damage suffered.
Space suits.
The astronauts themselves wore new space suits. On all previous Apollo flights, including the non-lunar flights, the commander and lunar module pilot had worn suits with the life support, liquid cooling, and communications connections in two parallel rows of three. On Apollo 15, the new suits, dubbed the "A7LB," had the connectors situated in triangular pairs. This new arrangement, along with the relocation of the entry zipper (which went in an up-down motion on the old suits), from the right shoulder to the left hip, allowed the inclusion of a new waist joint, allowing the astronauts to bend completely over and to sit on the rover. Upgraded backpacks allowed for longer-duration moonwalks, and the Command Module Pilot, who wore a suit with three connectors, would wear a five-connector version of the old Moon suit — the liquid cooling water connector being removed, as the Command Module Pilot would make a "deep-space EVA" to retrieve film cartridges on the flight home.
Scandals.
After a successful mission, the reputations of the crew and NASA were tarnished by a deal the crew had made with a German stamp dealer. H. Walter Eiermann, who had many professional and social contacts with NASA employees and the astronaut corps, arranged for Scott to carry unauthorized commemorative postal covers in his space suit, in addition to the postal covers NASA had contracted to carry for the United States Postal Service. Eiermann had promised each astronaut $7,000 in the form of savings accounts in return for 100 covers signed after having been on the Moon. He told the astronauts that he would not advertise or sell the covers until the end of the Apollo program. Irwin wrote in his book "To Rule the Night" that the astronauts had agreed to the deal as a way to help finance their children's college tuition.
Another controversy arose after the flight, caused by the "Fallen Astronaut" statuette that Scott had left on the Moon. The crew claim they had agreed with the sculptor, Paul Van Hoeydonck, that no replicas were to be made, in order to satisfy NASA's aversion to commercial exploitation of the space program. After the sculpture's existence was publicly disclosed during their post-flight press conference, the National Air and Space Museum contacted the crew asking for a replica made for the museum. Van Hoeydonck, whose account of the agreement contradicts Scott's, subsequently advertised replicas for sale to the public. Under pressure from NASA, Van Hoeydonck withdrew the sale offer.
Mission insignia.
The three astronauts of Apollo 15 were all United States Air Force active duty officers, and their patch carries Air Force motifs (just as the Apollo 12 all-Navy crew's patch had featured a sailing ship). The circular patch features stylized red, white and blue birds flying over the Hadley Rille section of the Moon. Immediately behind the birds, a line of craters form the Roman numeral XV. The artwork is circled in red, with a white band giving the mission and crew names and a blue border. Scott contacted fashion designer Emilio Pucci to design the patch, who came up with the basic idea of the three-bird motif on a square patch. The crew changed the shape to round and the colors from blues and greens to a patriotic red, white and blue. Worden stated that each bird also represented an astronaut, white being his own color (and as Command Module Pilot, uppermost), with Scott the blue bird and Irwin the red. The Roman numeral design was created when NASA insisted that the mission number be displayed in Arabic numerals.
Visibility from space.
The halo area of the Apollo 15 landing site, generated by the LM's exhaust plume, was observed by a camera aboard the Japanese lunar orbiter SELENE and confirmed by comparative analysis of photographs in May 2008. This corresponds well to photographs taken from the Apollo 15 Command Module showing a change in surface reflectivity due to the plume, and was the first visible trace of manned landings on the Moon seen from space since the close of the Apollo program.
NASA reports
Multimedia

</doc>
<doc id="1970" url="https://en.wikipedia.org/wiki?curid=1970" title="Apollo 16">
Apollo 16

Apollo 16 was the tenth manned mission in the United States Apollo space program, the fifth and penultimate to land on the Moon and the first to land in the lunar highlands. The second of the so-called "J missions," it was crewed by Commander John Young, Lunar Module Pilot Charles Duke and Command Module Pilot Ken Mattingly. Launched from the Kennedy Space Center in Florida at 12:54 PM EST on April 16, 1972, the mission lasted 11 days, 1 hour, and 51 minutes, and concluded at 2:45 PM EST on April 27.
John Young and Charles Duke spent 71 hours—just under three days—on the lunar surface, during which they conducted three extra-vehicular activities or moonwalks, totaling 20 hours and 14 minutes. The pair drove the Lunar Roving Vehicle (LRV), the second produced and used on the Moon, . On the surface, Young and Duke collected of lunar samples for return to Earth, while Command Module Pilot Ken Mattingly orbited in the Command/Service Module (CSM) above to perform observations. Mattingly spent 126 hours and 64 revolutions in lunar orbit. After Young and Duke rejoined Mattingly in lunar orbit, the crew released a subsatellite from the Service Module (SM). During the return trip to Earth, Mattingly performed a one-hour spacewalk to retrieve several film cassettes from the exterior of the Service Module.
Apollo 16's landing spot in the highlands was chosen to allow the astronauts to gather geologically older lunar material than the samples obtained in the first four landings, which were in or near lunar maria. Samples from the Descartes Formation and the Cayley Formation disproved a hypothesis that the formations were volcanic in origin.
Crew.
Mattingly had originally been assigned to the prime crew of Apollo 13, but was exposed to the measles through Duke, at that time on the back-up crew for Apollo 13, who had caught it from one of his children. He never contracted the illness, but was nevertheless removed from the crew and replaced by his backup, Jack Swigert, three days before the launch. Young, a captain in the United States Navy, had flown on three spaceflights prior to Apollo 16: Gemini 3, Gemini 10 and Apollo 10, which orbited the Moon. One of 19 astronauts selected by NASA in April 1966, Duke had never flown in space before Apollo 16. He served on the support crew of Apollo 10 and was a Capsule Communicator (CAPCOM) for Apollo 11.
Backup crew.
Although not officially announced, the original backup crew consisted of Fred W. Haise (CDR), William R. Pogue (CMP) and Gerald P. Carr (LMP), who were targeted for the prime crew assignment on Apollo 19. However, after the cancellations of Apollos 18 and 19 were finalized in September 1970 this crew would not rotate to a lunar mission as planned. Subsequently, Roosa and Mitchell were recycled to serve as members of the backup crew after returning from Apollo 14, while Pogue and Carr were reassigned to the Skylab program where they flew on Skylab 4.
Mission insignia.
The insignia of Apollo 16 is dominated by a rendering of an American eagle and a red, white and blue shield, representing the people of the United States, over a gray background representing the lunar surface. Overlaying the shield is a gold NASA vector, orbiting the Moon. On its gold-outlined blue border, there are 16 stars, representing the mission number, and the names of the crew members: Young, Mattingly, Duke. The insignia was designed from ideas originally submitted by the crew of the mission.
Planning and training.
Landing site selection.
Apollo 16 was the second of the Apollo type J missions, featuring the use of the Lunar Roving Vehicle, increased scientific capability, and lunar surface stays of three days. As Apollo 16 was the penultimate mission in the Apollo program and there was no new hardware or procedures to test on the lunar surface, the last two missions (the other being Apollo 17) presented opportunities for astronauts to clear up some uncertainties in understanding the Moon's properties. Although previous Apollo expeditions, including Apollo 14 and Apollo 15, obtained samples of pre-mare lunar material, before lava began to upwell from the Moon's interior and flood the low areas and basins, none had actually visited the lunar highlands.
Apollo 14 had visited and sampled a ridge of material that had been ejected by the impact that created the Mare Imbrium impact basin. Likewise, Apollo 15 had also sampled material in the region of Imbrium, visiting the basin's edge. There remained the possibility, because the Apollo 14 and Apollo 15 landing sites were closely associated with the Imbrium basin, that different geologic processes were prevalent in areas of the lunar highlands far from Mare Imbrium. Several members of the scientific community remarked that the central lunar highlands resembled regions on Earth that were created by volcanic processes and hypothesized the same might be true on the Moon. They had hoped that scientific output from the Apollo 16 mission would provide an answer.
Two locations on the Moon were given primary consideration for exploration by the Apollo 16 expedition: the Descartes Highlands region west of Mare Nectaris and the crater Alphonsus. At Descartes, the Cayley and Descartes formations were the primary areas of interest in that scientists suspected, based on telescopic and orbital imagery, that the terrain found there was formed by magma more viscous than that which formed the lunar maria. The Cayley Formation's age was approximated to be about the same as Mare Imbrium based on the local frequency of impact craters. The considerable distance between the Descartes site and previous Apollo landing sites would be beneficial for the network of geophysical instruments, portions of which were deployed on each Apollo expedition beginning with Apollo 12.
At the Alphonsus, three scientific objectives were determined to be of primary interest and paramount importance: the possibility of old, pre-Imbrium impact material from within the crater's wall, the composition of the crater's interior and the possibility of past volcanic activity on the floor of the crater at several smaller "dark halo" craters. Geologists feared, however, that samples obtained from the crater might have been contaminated by the Imbrium impact, thus preventing Apollo 16 from obtaining samples of pre-Imbrium material. There also remained the distinct possibility that this objective had already been satisfied by the Apollo 14 and Apollo 15 missions, as the Apollo 14 samples had not yet been completely analyzed and samples from Apollo 15 had not yet been obtained.
It was decided to target the Apollo 16 mission for the Descartes site. Following the decision, the Alphonsus site was considered the most likely candidate for Apollo 17, but was eventually rejected. With the assistance of orbital photography obtained on the Apollo 14 mission, the Descartes site was determined to be safe enough for a manned landing. The specific landing site was between two young impact craters, North Ray and South Ray craters – in diameter, respectively – which provided "natural drill holes" which penetrated through the lunar regolith at the site, thus leaving exposed bedrock that could be sampled by the crew.
After selecting the landing site for Apollo 16, sampling the Descartes and Cayley formations, two geologic units of the lunar highlands, was determined by mission planners to be the primary sampling interest of the mission. It was these formations that the scientific community widely suspected were formed by lunar volcanism, but this hypothesis was proven incorrect by the composition of lunar samples from the mission.
Training.
In preparing for their mission, in addition to the usual Apollo spacecraft training, Young and Duke, along with backup commander Fred Haise, underwent an extensive geological training program that included several field trips to introduce them to concepts and techniques they would use in analyzing features and collecting samples on the lunar surface. During these trips, they visited and provided scientific descriptions of geologic features they were likely to encounter. In July 1971, they visited Sudbury, Ontario, Canada for geology training exercises, the first time U.S. astronauts did so. Geologists chose the area because of a wide crater created about 1.8 billion years ago by a large meteorite. The Sudbury Basin shows evidence of shatter cone geology familiarizing the Apollo crew with geologic evidence of a meteor impact. During the training exercises the astronauts did not wear space suits, but carried radio equipment to converse with each other and scientist-astronaut Anthony W. England, practicing procedures they would use on the lunar surface.
In addition to the field geology training, Young and Duke also trained to use their EVA space suits, adapt to the reduced lunar gravity, collect samples, and drive the Lunar Roving Vehicle. They also received survival training and preparation for other technical aspects of the mission.
Command Module pilot Mattingly also received training in recognizing geological features from orbit by flying over the field areas in an airplane, and trained to operate the Scientific Instrument Module from lunar orbit.
Mission highlights.
Launch and outbound trip.
The launch of Apollo 16 was delayed one month from March 17 to April 16. This was the first launch delay in the Apollo program due to a technical problem. During the delay, the space suits, a spacecraft separation mechanism and batteries in the Lunar Module (LM) were modified and tested. There were concerns that the explosive mechanism designed to separate the docking ring from the Command Module (CM) would not create enough pressure to completely sever the ring. This, along with a dexterity issue in Young's space suit and fluctuations in the capacity of the Lunar Module batteries, required investigation and trouble-shooting. In January 1972, three months before the planned April launch date, a fuel tank in the Command Module was accidentally damaged during a routine test. The rocket was returned to the Vertical Assembly Building (VAB) and the fuel tank replaced, and the rocket returned to the launch pad in February in time for the scheduled launch.
The official mission countdown began on Monday, April 10, 1972, at 8:30 AM, six days before the launch. At this point the Saturn V rocket's three stages were powered up and drinking water was pumped into the spacecraft. As the countdown began, the crew of Apollo 16 was participating in final training exercises in anticipation of a launch on April 16. The astronauts underwent their final preflight physical examination on April 11. On April 15, liquid hydrogen and liquid oxygen propellants were pumped into the spacecraft, while the astronauts rested in anticipation of their launch the next day.
The Apollo 16 mission launched from the Kennedy Space Center in Florida at 12:54 PM EST on April 16, 1972. The launch was nominal; the crew experienced vibration similar to that of previous crews. The first and second stages of the Saturn V rocket performed nominally; the spacecraft entered orbit around Earth just under 12 minutes after lift-off. After reaching orbit, the crew spent time adapting to the zero-gravity environment and preparing the spacecraft for Trans Lunar Injection (TLI), the burn of the third-stage rocket that would propel them to the Moon. In Earth orbit, the crew faced minor technical issues, including a potential problem with the environmental control system and the S-IVB third stage's attitude control system, but eventually resolved or compensated for them as they prepared to depart towards the Moon. After two orbits, the rocket's third stage reignited for just over five minutes, propelling the craft towards the Moon at about . Six minutes after the burn of the S-IVB, the Command/Service Module, containing the crew, separated from the rocket and traveled for before turning around and retrieving the Lunar Module from inside the expended rocket stage. The maneuver, known as transposition, went smoothly and the LM was extracted from the S-IVB. Following transposition and docking, the crew noticed the exterior surface of the Lunar Module was giving off particles from a spot where the LM's skin appeared torn or shredded; at one point, Duke estimated they were seeing about five to ten particles per second. The crew entered the Lunar Module through the docking tunnel connecting it with the Command Module to inspect its systems, at which time they did not spot any major issues. Once on course towards the Moon, the crew put the spacecraft into a rotisserie "barbecue" mode in which the craft rotated along its long axis three times per hour to ensure even heat distribution about the spacecraft from the Sun. After further preparing the craft for the voyage, the crew began the first sleep period of the mission just under 15 hours after launch.
By the time Mission Control issued the wake-up call to the crew for flight day two, the spacecraft was about away from the Earth, traveling at about . As it was not due to arrive in lunar orbit until flight day four, flight days two and three were largely preparatory days, consisting of spacecraft maintenance and scientific research. On day two, the crew performed an electrophoresis experiment, also performed on Apollo 14, in which they attempted to prove the higher purity of particle migrations in the zero-gravity environment. The remainder of day two included a two-second mid-course correction burn performed by the Command/Service Module's Service Propulsion System engine to tweak the spacecraft's trajectory. Later in the day, the astronauts entered the Lunar Module for the second time in the mission to further inspect the landing craft's systems. The crew reported they had observed additional paint peeling from a portion of the LM's outer aluminum skin. Despite this, the crew discovered that the spacecraft's systems were performing nominally. Following the LM inspection, the crew reviewed checklists and procedures for the following days in anticipation of their arrival and the Lunar Orbit Insertion burn. Command Module Pilot Mattingly reported a "gimbal lock" warning light, indicating the craft was not reporting an attitude. Mattingly alleviated this by realigning the guidance system using the Sun and Moon. At the end of day two, Apollo 16 was about away from Earth.
At the beginning of day three, the spacecraft was about away from the Earth. The velocity of the craft steadily decreased, as it had not yet reached the lunar sphere of gravitational influence. The early part of day three was largely housekeeping, spacecraft maintenance and exchanging status reports with Mission Control in Houston. The crew performed the Apollo light flash experiment, or ALFMED, to investigate "light flashes" that were seen by the astronauts when the spacecraft was dark, regardless of whether or not their eyes were open, on Apollo lunar flights. This was thought to be caused by the penetration of the eye by cosmic ray particles. During the second half of the day, Young and Duke again entered the Lunar Module to power it up and check its systems, and perform housekeeping tasks in preparation for lunar landing. The systems were found to be functioning as expected. Following this, the crew donned their space suits and rehearsed procedures that would be used on landing day. Just before the end of flight day three at 59 hours, 19 minutes, 45 seconds after liftoff, while from the Earth and from the Moon, the spacecraft's velocity began increasing as it accelerated towards the Moon after entering the lunar sphere of influence.
After waking up on flight day four, the crew began preparations for the maneuver that would brake the spacecraft into orbit around the Moon, or lunar orbit insertion. At a distance of from the Moon, the Scientific Instrument Module (SIM) bay cover was jettisoned. At just over 74 hours into the mission, the spacecraft passed behind the Moon, losing direct contact with Mission Control. While over the far side of the Moon, the Command/Service Module's Service Propulsion System engine burned for 6 minutes and 15 seconds, braking the spacecraft into an orbit around the Moon with a low point (pericynthion) of 58.3 and a high point (apocynthion) of 170.4 nautical miles (108.0 and 315.6 km, respectively). After entering lunar orbit, the crew began preparations for the Descent Orbit Insertion (DOI) maneuver to further modify the spacecraft's orbital trajectory. The maneuver was successful, decreasing the craft's pericynthion to . The remainder of flight day four was spent making observations and preparing for activation of the Lunar Module, undocking, and landing the next day.
Lunar surface.
The crew continued preparing for Lunar Module activation and undocking shortly after waking up to begin flight day five. The boom that extended the mass spectrometer out from the Command/Service Module's Scientific Instruments Bay was stuck in a semi-deployed position. It was decided that Young and Duke would visually inspect the boom after undocking from the CSM in the LM. They entered the LM for activation and checkout of the spacecraft's systems. Despite entering the LM 40 minutes ahead of schedule, they completed preparations only 10 minutes early due to numerous delays in the process. With the preparations finished, they undocked in the LM "Orion" from Mattingly in the Command/Service Module "Casper" 96 hours, 13 minutes, 13 seconds into the mission. For the rest of the two crafts' passes over the near side of the Moon, Mattingly prepared to shift "Casper" to a circular orbit while Young and Duke prepared "Orion" for the descent to the lunar surface. At this point, during tests of the CSM's steerable rocket engine in preparation for the burn to modify the craft's orbit, a malfunction occurred in the engine's backup system. According to mission rules, "Orion" would have then re-docked with "Casper", in case Mission Control decided to abort the landing and use the Lunar Module's engines for the return trip to Earth. After several hours of analysis, however, mission controllers determined that the malfunction could be worked around and Young and Duke could proceed with the landing. As a result of this, powered descent to the lunar surface began about six hours behind schedule. Because of the delay, Young and Duke began their descent to the surface at an altitude higher than that of any previous mission, at . At an altitude of about , Young was able to view the landing site in its entirety. Throttle-down of the LM's landing engine occurred on time and the spacecraft tilted forward to its landing orientation at an altitude of . The LM landed north and west of the planned landing site at 104 hours, 29 minutes, and 35 seconds into the mission, at 2:23:35 UTC on April 21.
After landing, Young and Duke began powering down some of the LM's systems to conserve battery power. Upon completing their initial adjustments, the pair configured "Orion" for their three-day stay on the lunar surface, removed their space suits and took initial geological observations of the immediate landing site. They then settled down for their first meal on the surface. After eating, they configured the cabin for their first sleep period on the Moon. The landing delay caused by the malfunction in the Command/Service Module's main engine necessitated significant modifications to the mission schedule. Apollo 16 would spend one less day in lunar orbit after surface exploration had been completed to afford the crew contingency time to compensate for any further problems and to conserve expendables. In order to improve Young's and Duke's sleep schedule, the third and final moonwalk of the mission was trimmed from seven hours to five.
The next morning, flight day five, Young and Duke ate breakfast and began preparations for the first extra-vehicular activity (EVA), or moonwalk. After the pair donned and pressurized their space suits and depressurized the Lunar Module cabin, Young climbed out onto the "porch" of the LM, a small platform above the ladder. Duke handed Young a jettison bag full of trash to dispose of on the surface. Young then lowered the equipment transfer bag (ETB), containing equipment for use during the EVA, to the surface. Young descended the ladder and, upon setting foot on the lunar surface, became the ninth human to walk on the Moon. Upon stepping onto the surface, Young expressed his sentiments about being there: "There you are: Mysterious and Unknown Descartes. Highland plains. Apollo 16 is gonna change your image. I'm sure glad they got ol' Brer Rabbit, here, back in the briar patch where he belongs." Duke soon descended the ladder and joined Young on the surface, becoming the tenth and youngest human to walk on the Moon, at age 36. After setting foot on the lunar surface, Duke expressed his excitement, commenting: "Fantastic! Oh, that first foot on the lunar surface is super, Tony!" The pair's first task of the moonwalk was to unload the Lunar Roving Vehicle, the Far Ultraviolet Camera/Spectrograph (UVC), and other equipment, from the Lunar Module. This was done without problems. On first driving the lunar rover, Young discovered that the rear steering was not working. He alerted Mission Control to the problem before setting up the television camera and planting the flag of the United States with Duke. The day's next task was to deploy the Apollo Lunar Surface Experiments Package (ALSEP); while they were parking the lunar rover, on which the TV camera was mounted, to observe the deployment, the rear steering began functioning without explanation. While deploying a heat-flow experiment (that had burned up with the Lunar Module "Aquarius" on Apollo 13 and had been attempted with limited success on Apollo 15), a cable was inadvertently snapped after getting caught around Young's foot. After ALSEP deployment, they collected samples in the vicinity. About four hours after the beginning of EVA-1, they mounted the lunar rover and drove to the first geologic stop, Plum Crater, a crater on the rim of Flag crater, about across. There, at a distance of from the LM, they sampled material from the vicinity of Flag Crater, which scientists believed penetrated through the upper regolith layer to the underlying Cayley Formation. It was there that Young retrieved, at the request of Mission Control, the largest rock returned by an Apollo mission, a breccia nicknamed Big Muley after mission geology principal investigator William R. Muehlberger. The next stop of the day was Buster Crater, about from the LM. There, Duke took pictures of Stone Mountain and South Ray Crater while Young deployed a magnetic field experiment. At that point, scientists began to reconsider their pre-mission hypothesis that Descartes had been the setting of ancient volcanic activity, as the two astronauts had yet to find any volcanic material. Following their stop at Buster, Young did a demonstration drive of the lunar rover while Duke filmed with a 16 mm movie camera. After completing more tasks at the ALSEP, they returned to the LM to close out the moonwalk. They reentered the LM 7 hours, 6 minutes, and 56 seconds after the start of the EVA. Once inside, they pressurized the LM cabin, went through a half-hour briefing with scientists in Mission Control, and configured the cabin for the sleep period.
Shortly after waking up on the morning of flight day six three and a half minutes early, they discussed with Mission Control in Houston the day's timeline of events. The second lunar excursion's primary objective was to visit Stone Mountain to climb up the slope of about 20 degrees to reach a cluster of five craters known as "Cinco Craters." After preparations for the day's moonwalk were completed, the astronauts climbed out of the Lunar Module. After departing the immediate landing site in the lunar rover, they arrived at the day's first destination, the Cinco Craters, from the LM. At above the valley floor, the pair were at the highest elevation above the LM of any Apollo mission. After marveling at the view (including South Ray) from the side of Stone Mountain, which Duke described as "spectacular," the astronauts gathered samples in the vicinity. After spending 54 minutes on the slope, they climbed aboard the lunar rover en route to the day's second stop, station five, a crater across. There, they hoped to find Descartes material that had not been contaminated by ejecta from South Ray Crater, a large crater south of the landing site. The samples they collected there, although their origin is still not certain, are, according to geologist Don Wilhelms, "a reasonable bet to be Descartes." The next stop, station six, was a blocky crater, where the astronauts believed they could sample the Cayley Formation as evidenced by the firmer soil found there. Bypassing station seven to save time, they arrived at station eight on the lower flank of Stone Mountain, where they sampled material on a ray from South Ray Crater for about an hour. There, they collected black and white breccias and smaller, crystalline rocks rich in plagioclase. At station nine, an area known as the "Vacant Lot," which was believed to be free of ejecta from South Ray, they spent about 40 minutes gathering samples. Twenty-five minutes after departing station nine, they arrived at the final stop of the day, halfway between the ALSEP site and the LM. There, they dug a double core and conducted several penetrometer tests along a line stretching east of the ALSEP. At the request of Young and Duke, the moonwalk was extended by ten minutes. After returning to the LM to wrap up the second lunar excursion, they climbed back inside the landing craft's cabin, sealing and pressurizing the interior after 7 hours, 23 minutes, and 26 seconds of EVA time, breaking a record that had been set on Apollo 15. After eating a meal and proceeding with a debriefing on the day's activities with Mission Control, they reconfigured the LM cabin and prepared for the sleep period.
Flight day seven was their third and final day on the lunar surface, returning to orbit to rejoin Mattingly in the Command/Service Module following the day's moonwalk. During the third and final lunar excursion, they were to explore North Ray Crater, the largest of any of the craters any Apollo expedition had visited. After exiting "Orion", the pair drove the lunar rover away from the LM before adjusting their heading to travel to North Ray Crater. The drive was smoother than that of the previous day, as the craters were shallower and boulders were less abundant north of the immediate landing site. Boulders gradually became larger and more abundant as they approached North Ray in the lunar rover. Upon arriving at the rim of North Ray crater, they were away from the LM. After their arrival, the duo took photographs of the wide and deep crater. They visited a large boulder, taller than a four-story building, which became known as 'House Rock'. Samples obtained from this boulder delivered the final blow to the pre-mission volcanic hypothesis, proving it incorrect. House Rock had numerous bullet hole-like marks where micrometeoroids from space had impacted the rock. About 1 hour and 22 minutes after arriving, they departed for station 13, a large boulder field about from North Ray. On the way, they set a lunar speed record, traveling at an estimated downhill. They arrived at a high boulder, which they called 'Shadow Rock'. Here, they sampled permanently shadowed soil. During this time, Mattingly was preparing the Command/Service Module in anticipation of their return approximately six hours later. After three hours and six minutes, they returned to the LM, where they completed several experiments and offloaded the rover. A short distance from the LM, Duke placed a photograph of his family and a United States Air Force commemorative medallion on the surface. Young drove the rover to a point about east of the LM, known as the 'VIP site,' so its television camera, controlled remotely by Mission Control, could observe Apollo 16's liftoff from the Moon. They then reentered the LM after a 5-hour and 40 minute final excursion. After pressurizing the LM cabin, the crew began preparing to return to lunar orbit.
Return to Earth.
Eight minutes before departing the lunar surface, CAPCOM James Irwin notified Young and Duke from Mission Control that they were go for liftoff. Two minutes before launch, they activated the "Master Arm" switch and then the "Abort Stage" button, after which they awaited ignition of "Orion"’s ascent stage engine. When the ascent stage ignited, small explosive charges severed the ascent stage from the descent stage and cables connecting the two were severed by a guillotine-like mechanism. Six minutes after liftoff, at a speed of about , Young and Duke reached lunar orbit. Young and Duke successfully rendezvoused and re-docked with Mattingly in the Command/Service Module. To minimize the transfer of lunar dust from the LM cabin into the CSM, Young and Duke cleaned the cabin before opening the hatch separating the two spacecraft. After opening the hatch and reuniting with Mattingly, the crew transferred the samples Young and Duke had collected on the surface into the CSM for transfer to Earth. After transfers were completed, the crew would sleep before jettisoning the empty Lunar Module ascent stage the next day, when it was to be crashed intentionally into the lunar surface.
The next day, after final checks were completed, the expended LM ascent stage was jettisoned. Because of a failure by the crew to activate a certain switch in the LM before sealing it off, it initially tumbled after separation and did not execute the rocket burn necessary for the craft's intentional de-orbit. The ascent stage eventually crashed into the lunar surface nearly a year after the mission. The crew's next task, after jettisoning the Lunar Module ascent stage, was to release a subsatellite into lunar orbit from the CSM's Scientific Instrument Bay. The burn to alter the CSM's orbit to that desired for the subsatellite had been cancelled; as a result, the subsatellite lasted half of its anticipated lifetime. Just under five hours later, on the CSM's 65th orbit around the Moon, its Service Propulsion System main engine was reignited to propel the craft on a trajectory that would return it to Earth. The SPS engine performed the burn flawlessly despite the malfunction that had delayed the lunar landing several days before.
At a distance of about from Earth, Mattingly performed a "deep-space" extra-vehicular activity, or spacewalk, during which he retrieved several film cassettes from the CSM's SIM bay. While outside the spacecraft, Mattingly set up a biological experiment, the Microbial Ecology Evaluation Device (MEED). The MEED experiment was only performed on Apollo 16. The crew carried out various housekeeping and maintenance tasks aboard the spacecraft and ate a meal before concluding the day.
The penultimate day of the flight was largely spent performing experiments, aside from a twenty-minute press conference during the second half of the day. During the press conference, the astronauts answered questions pertaining to several technical and non-technical aspects of the mission prepared and listed by priority at the Manned Spacecraft Center in Houston by journalists covering the flight. In addition to numerous housekeeping tasks, the astronauts prepared the spacecraft for its atmospheric reentry the next day. At the end of the crew's final full day in space, the spacecraft was approximately from Earth and closing at a rate of about .
When the wake-up call was issued to the crew for their final day in space by CAPCOM Tony England, it was about out from Earth, traveling just over . Just over three hours before splashdown in the Pacific Ocean, the crew performed a final course correction burn, changing their velocity by . Approximately ten minutes before reentry into Earth's atmosphere, the cone-shaped Command Module containing the three crewmembers separated from the Service Module, which would burn up during reentry. At 265 hours and 37 minutes into the mission, at a velocity of about , Apollo 16 began atmospheric reentry. At its maximum, the temperature of the heat shield was between . After successful parachute deployment and less than 14 minutes after reentry began, the Command Module splashed down in the Pacific Ocean southeast of the island of Kiritimati (or "Christmas Island"), 290 hours, 37 minutes, 6 seconds after liftoff. The spacecraft and its crew was retrieved by the . They were safely aboard the "Ticonderoga" 37 minutes after splashdown.
Lunar subsatellite PFS-2.
The Apollo 16 subsatellite (PFS-2) was a small satellite released into lunar orbit from the Service Module. Its principal objective was to measure charged particles and magnetic fields all around the Moon as the Moon orbited Earth, similar to its sister spacecraft, PFS-1, released eight months earlier by Apollo 15. "The low orbits of both subsatellites were to be similar ellipses, ranging from above the lunar surface."
"Instead, something bizarre happened. The orbit of PFS-2 rapidly changed shape and distance from the Moon. In 2-1/2 weeks the satellite was swooping to within a hair-raising of the lunar surface at closest approach. As the orbit kept changing, PFS-2 backed off again, until it seemed to be a safe 30 miles away. But not for long: inexorably, the subsatellite's orbit carried it back toward the Moon. And on May 29, 1972—only 35 days and 425 orbits after its release"—PFS-2 crashed into the Lunar surface.
In later years, through a study of many lunar orbiting satellites, scientists came to discover that most low lunar orbits (LLO) are unstable. PFS-2 had been placed, unknown to mission planners at the time, squarely into one of the most unstable of orbits, at 11 degrees orbital inclination, far from the four "frozen lunar orbits" discovered only later at 27°, 50°, 76°, and 86° inclination.
Spacecraft locations.
The aircraft carrier USS "Ticonderoga" delivered the Apollo 16 Command Module to the North Island Naval Air Station, near San Diego, California, on Friday, May 5, 1972. On Monday, May 8, 1972, ground service equipment being used to empty the residual toxic reaction control system fuel in the Command Module tanks exploded in a Naval Air Station hangar. Forty-six people were sent to the hospital for 24 to 48 hours observation, most suffering from inhalation of toxic fumes. Most seriously injured was a technician who suffered a fractured kneecap when the GSE cart overturned on him. A hole was blown in the hangar roof 250 feet above; about 40 windows in the hangar were shattered. The Command Module suffered a three-inch gash in one panel.
The Apollo 16 Command Module "Casper" is on display at the U.S. Space & Rocket Center in Huntsville, Alabama. The Lunar Module ascent stage separated 24 April 1972 but a loss of attitude control rendered it out of control. It orbited the Moon for about a year. Its impact site on the Moon is unknown.
Duke donated some flown items, including a lunar map, to Kennesaw State University in Kennesaw, Georgia. He left two items on the Moon, both of which he photographed. The most famous is a plastic-encased photo portrait of his family (NASA Photo AS16-117-18841). The reverse of the photo is signed by Duke's family and bears this message: "This is the family of Astronaut Duke from Planet Earth. Landed on the Moon, April 1972." The other item was a commemorative medal issued by the United States Air Force, which was celebrating its 25th anniversary in 1972. He took two medals, leaving one on the Moon and donating the other to the Wright-Patterson Air Force Base museum.
In 2006, shortly after Hurricane Ernesto affected Bath, North Carolina, eleven-year-old Kevin Schanze discovered a piece of metal debris on the ground near his beach home. Schanze and a friend discovered a "stamp" on the flat metal sheet, which upon further inspection turned out to be a faded copy of the Apollo 16 mission insignia. NASA later confirmed the object to be a piece of the first stage of the Saturn V rocket that launched Apollo 16 into space. In July 2011, after returning the piece of debris at NASA's request, 16-year-old Schanze was given an all-access tour of the Kennedy Space Center and VIP seating for the launch of STS-135, the final mission of the Space Shuttle program.
In January 2016, the Apollo 16 S-IVB impact site was discovered within the Mare Insularum, approximately southwest of Copernicus Crater, by the Lunar Reconnaissance Orbiter.

</doc>
<doc id="1971" url="https://en.wikipedia.org/wiki?curid=1971" title="Apollo 17">
Apollo 17

Apollo 17 was the final mission of NASA's Apollo program, the enterprise that landed the first humans on the Moon. Launched at 12:33 am Eastern Standard Time (EST) on December 7, 1972, with a crew made up of Commander Eugene Cernan, Command Module Pilot Ronald Evans, and Lunar Module Pilot Harrison Schmitt, it was the last use of Apollo hardware for its original purpose; after Apollo 17, extra Apollo spacecraft were used in the Skylab and Apollo–Soyuz programs.
Apollo 17 was the first night launch of a U.S. human spaceflight and the final manned launch of a Saturn V rocket. It was a "J-type mission" which included three days on the lunar surface, extended scientific capability, and the third Lunar Roving Vehicle (LRV). While Evans remained in lunar orbit in the Command/Service Module (CSM), Cernan and Schmitt spent just over three days on the moon in the Taurus–Littrow valley and completed three moonwalks, taking lunar samples and deploying scientific instruments. Evans took scientific measurements and photographs from orbit using a Scientific Instruments Module mounted in the Service Module.
The landing site was chosen with the primary objectives of Apollo 17 in mind: to sample lunar highland material older than the impact that formed Mare Imbrium, and investigate the possibility of relatively new volcanic activity in the same area. Cernan, Evans and Schmitt returned to Earth on December 19 after a 12-day mission.
Apollo 17 is the most recent manned Moon landing and was the last time humans travelled beyond low Earth orbit. It was also the first mission to be commanded by a person with no background as a test pilot, and the first to have no one on board who had been a test pilot; X-15 test pilot Joe Engle lost the lunar module pilot assignment to Schmitt, a scientist. The mission broke several records: the longest moon landing, longest total extravehicular activities (moonwalks), largest lunar sample, and longest time in lunar orbit.
Crew.
Eugene Cernan, Ronald Evans, and former X-15 pilot Joe Engle were assigned to the backup crew of Apollo 14. Engle flew sixteen X-15 flights, three of which exceeded the border of space. Following the rotation pattern that a backup crew would fly as the prime crew three missions later, Cernan, Evans, and Engle would have flown Apollo 17. Harrison Schmitt served on the backup crew of Apollo 15 and, following the crew rotation cycle, was slated to fly as Lunar Module Pilot on Apollo 18. However, Apollo 18 was cancelled in September 1970. Following this decision, the scientific community pressured NASA to assign a geologist to an Apollo landing, as opposed to a pilot trained in geology. In light of this pressure, Harrison Schmitt, a professional geologist, was assigned the Lunar Module Pilot position on Apollo 17. Scientist-astronaut Curt Michel believed that it was his own decision to resign, after it became clear that he would not be given a flight assignment, that mobilized this action.
Subsequent to the decision to assign Schmitt to Apollo 17, there remained the question of which crew (the full backup crew of Apollo 15, Dick Gordon, Vance Brand, and Schmitt, or the backup crew of Apollo 14) would become prime crew of the mission. NASA Director of Flight Crew Operations Deke Slayton ultimately assigned the backup crew of Apollo 14 (Cernan and Evans), along with Schmitt, to the prime crew of Apollo 17.
Backup crew.
Replacement.
The Apollo 15 prime crew received the backup assignment since this was to be the last lunar mission and the backup crew would not rotate to another mission. However, when the Apollo 15 postage stamp incident became public in early 1972 the crew was reprimanded by NASA and the United States Air Force (they were active duty officers). Director of Flight Crew Operations Deke Slayton removed them from flight status and replaced them with Young and Duke from the Apollo 16 prime crew and Roosa from the Apollo 14 prime and Apollo 16 backup crews.
Mission insignia.
The insignia's most prominent feature is an image of the Greek sun god Apollo backdropped by a rendering of an American eagle, the red bars on the eagle mirroring those on the flag of the United States. Three white stars above the red bars represent the three crewmen of the mission. The background includes the Moon, the planet Saturn and a galaxy or nebula. The wing of the eagle partially overlays the Moon, suggesting man's established presence there. The gaze of Apollo and the direction of the eagle's motion embody man's intention to explore further destinations in space.
The patch includes, along with the colors of the U.S. flag (red, white, and blue), the color gold, representative of a "golden age" of spaceflight that was to begin with Apollo 17. The image of Apollo in the mission insignia is a rendering of the "Apollo Belvedere" sculpture. The insignia was designed by Robert McCall, with input from the crew.
Planning and training.
Like Apollo 15 and Apollo 16, Apollo 17 was slated to be a "J-mission," an Apollo mission type that featured lunar surface stays of three days, higher scientific capability, and the usage of the Lunar Roving Vehicle. Since Apollo 17 was to be the final lunar landing of the Apollo program, high-priority landing sites that had not been visited previously were given consideration for potential exploration. A landing in the crater Copernicus was considered, but was ultimately rejected because Apollo 12 had already obtained samples from that impact, and three other Apollo expeditions had already visited the vicinity of Mare Imbrium. A landing in the lunar highlands near the crater Tycho was also considered, but was rejected because of the rough terrain found there and a landing on the lunar far side in the crater Tsiolkovskiy was rejected due to technical considerations and the operational costs of maintaining communication during surface operations. A landing in a region southwest of Mare Crisium was also considered, but rejected on the grounds that a Soviet spacecraft could easily access the site; Luna 20 eventually did so shortly after the Apollo 17 site selection was made.
After the elimination of several sites, three sites made the final consideration for Apollo 17: Alphonsus crater, Gassendi crater, and the Taurus-Littrow valley. In making the final landing site decision, mission planners took into consideration the primary objectives for Apollo 17: obtaining old highlands material from a substantial distance from Mare Imbrium, sampling material from young volcanic activity (i.e., less than three billion years), and having minimal ground overlap with the orbital ground tracks of Apollo 15 and Apollo 16 to maximize the amount of new data obtained.
The Taurus-Littrow site was selected with the prediction that the crew would be able to obtain samples of old highland material from the remnants of a landslide event that occurred on the south wall of the valley and the possibility of relatively young, explosive volcanic activity in the area. Although the valley is similar to the landing site of Apollo 15 in that it is on the border of a lunar mare, the advantages of Taurus-Littrow were believed to outweigh the drawbacks, thus leading to its selection as the Apollo 17 landing site.
Apollo 17 was the only lunar landing mission to carry the Traverse Gravimeter Experiment (TGE), an experiment built by Draper Laboratory at the Massachusetts Institute of Technology designed to provide relative gravity measurements throughout the landing site at various locations during the mission's moonwalks. Scientists would then use this data to gather information about the geological substructure of the landing site and the surrounding vicinity.
As with previous lunar landings, the Apollo 17 astronauts underwent an extensive training program that included training to collect samples on the surface, usage of the spacesuits, navigation in the Lunar Roving Vehicle, field geology training, survival training, splashdown and recovery training, and equipment training.
Mission hardware and experiments.
Traverse Gravimeter.
Apollo 17 was the only Apollo lunar landing mission to carry the Traverse Gravimeter Experiment. As gravimeters had proven to be useful in the geologic investigation of the Earth, the objective of this experiment was to determine the feasibility of using the same techniques on the Moon to learn about its internal structure. The gravimeter was used to obtain readings at the landing site in the immediate vicinity of the Lunar Module (LM), as well as various locations on the mission's traverse routes. The TGE was carried on the Lunar Roving Vehicle; measurements were taken by the astronauts while the LRV was not in motion or after the gravimeter was placed on the surface.
A total of twenty-six measurements were taken with the TGE during the mission's three moonwalks, with productive results. As part of the Apollo Lunar Surface Experiments Package (ALSEP), the astronauts also deployed the Lunar Surface Gravimeter, a similar experiment, which ultimately failed to function properly.
Scientific Instrument Module.
Sector one of the Apollo 17 Service Module (SM) contained the Scientific Instrument Module (SIM) bay. The SIM bay housed three experiments for use in lunar orbit: a lunar sounder, an infrared scanning radiometer, and a far-ultraviolet spectrometer. A mapping camera, panoramic camera, and a laser altimeter were also included in the SIM bay.
The lunar sounder beamed electromagnetic impulses toward the lunar surface, which were designed with the objective of obtaining data to assist in developing a geological model of the interior of the Moon to an approximate depth of .
The Infrared Scanning Radiometer was designed with the objective of generating a temperature map of the lunar surface to aid in locating surface features such as rock fields, structural differences in the lunar crust, and volcanic activity.
The Far-Ultraviolet Spectrometer was to be used to obtain data pertaining to the composition, density, and constituency of the lunar atmosphere. The spectrometer was also designed to detect far-UV radiation emitted by the Sun that has been reflected off the lunar surface.
The Laser Altimeter was designed with the intention of measuring the altitude of the spacecraft above the lunar surface within approximately two meters (6.5 feet), and providing altitude information to the panoramic and mapping cameras.
Light flash phenomenon.
Throughout the Apollo lunar missions, the crew members observed light flashes that penetrated closed eyelids. These flashes, described as "streaks" or "specks" of light, were usually observed by astronauts while the spacecraft was darkened during a sleep period. These flashes, while not observed on the lunar surface, would average about two per minute and were observed by the crew members during the trip out to the Moon, back to Earth, and in lunar orbit.
The Apollo 17 crew conducted an experiment, also conducted on Apollo 16, with the objective of linking these light flashes with cosmic rays. As part of an experiment conducted by NASA and the University of Houston, one astronaut wore a device that recorded the time, strength, and path of high-energy atomic particles that penetrated the device. Analysis of the results concluded that the evidence supported the hypothesis that the flashes occurred when charged particles travelled through the retina in the eye.
Surface Electrical Properties Experiment.
Apollo 17 was the only lunar surface expedition to include the Surface Electrical Properties (SEP) experiment. The experiment included two major components: a transmitting antenna deployed near the Lunar Module and a receiving antenna located on the Lunar Roving Vehicle. At different stops during the mission's traverses, electrical signals traveled from the transmitting device, through the ground, and received at the LRV. The electrical properties of the lunar soil could be determined by comparison of the transmitted and received electrical signals. The results of this experiment, which are consistent with lunar rock composition, show that the top of the Moon are extremely dry.
Lunar Roving Vehicle.
Apollo 17 was the third mission (the others being Apollo 15 and Apollo 16) to make use of a Lunar Roving Vehicle. The LRV, in addition to being used by the astronauts for transport from station to station on the mission's three moonwalks, was used to transport the astronauts' tools, communications equipment, and samples. The Apollo 17 LRV was also used to carry experiments unique to the mission, such as the Traverse Gravimeter and Surface Electrical Properties experiment. The Apollo 17 LRV traveled a cumulative distance of approximately in a total drive time of about four hours and twenty-six minutes; the greatest distance Eugene Cernan and Harrison Schmitt traveled from the Lunar Module was about .
Biological cosmic ray experiment.
Apollo 17 included a biological cosmic ray experiment (BIOCORE), carrying mice that had been implanted with radiation monitors to see whether they suffered damage from cosmic rays.
Five pocket mice ("Perognathus longimembris") were implanted with radiation monitors under their scalps and flown on the mission. The species was chosen because it was well-documented, small, easy to maintain in an isolated state (not requiring drinking water for the duration of the mission and with highly concentrated waste), and for its ability to withstand environmental stress. Four of the five mice survived the flight; the cause of death of the fifth mouse was not determined.
The study found lesions in the scalp itself and liver. The scalp lesions and liver lesions appeared to be unrelated to one another, and were not thought to be the result of cosmic rays. No damage was found in the mice's retinas or viscera. At the time of the publication of the Apollo 17 Preliminary Science Report, the mouse brains had not yet been examined. However, subsequent studies showed no significant effect on the brains.
Mission highlights.
Launch and outbound trip.
Apollo 17 was launched at 12:33 am EST on December 7, 1972, from launch pad 39-A at the Kennedy Space Center. It was the last manned Saturn V launch and the only night launch. The launch was delayed by two hours and forty minutes due to an automatic cutoff in the launch sequencer at the T-30 second mark in the countdown. The issue was quickly determined to be a minor technical error. The clock was reset and held at the T-22 minute mark while technicians worked around the malfunction in order to continue with the launch. This pause was the only launch delay in the Apollo program caused by this type of hardware failure. The count resumed and the rocket lifted off achieving a normal low Earth orbit.
Approximately 500,000 people were estimated to have observed the launch in the immediate vicinity of Kennedy Space Center, despite the early morning hour. The launch was visible as far away as ; observers in Miami, Florida, saw a "red streak" crossing the northern sky.
At 3:46 am EST, the S-IVB third stage was re-ignited to propel the spacecraft towards the Moon.
At approximately 2:47 pm EST on December 10, the Service Propulsion System engine on the Command/Service Module ignited to slow down the CSM/Lunar Module stack into lunar orbit. Following orbit insertion and orbital stabilization, the crew began preparations for landing in the Taurus-Littrow valley.
Moon Landing.
After separating from the Command/Service Module, the Lunar Module "Challenger" and its crew of two, Eugene Cernan and Harrison Schmitt, adjusted their orbit and began preparations for the descent to Taurus-Littrow. While Cernan and Schmitt prepared for landing, Command Module Pilot Ron Evans remained in orbit to take observations, perform experiments and await the return of his crew-mates a few days later.
Soon after completing their preparations for landing, Cernan and Schmitt began their descent to the Taurus-Littrow valley on the lunar surface. Several minutes after the descent phase was initiated, the Lunar Module pitched over, giving the crew their first look at the landing site during the descent phase and allowing Cernan to guide the spacecraft to a desirable landing target while Schmitt provided data from the flight computer essential for landing. The LM touched down on the lunar surface at 2:55 pm EST on December 11. Shortly thereafter, the two astronauts began re-configuring the LM for their stay on the surface and began preparations for the first moonwalk of the mission, or EVA-1.
Lunar surface.
The first moonwalk (EVA) of the mission began approximately four hours after landing, at about 6:55 pm on December 11. The first task of the first lunar excursion was to offload the Lunar Roving Vehicle and other equipment from the Lunar Module. While working near the rover, a fender was accidentally broken off when Gene Cernan brushed up against it, his hammer getting caught under the right-rear fender, breaking off the rear extension. The same incident had also occurred on Apollo 16 as Commander John Young maneuvered around the rover. Although this was not a mission-critical issue, the loss of the fender caused Cernan and Schmitt to be covered with dust thrown up when the rover was in motion. The crew used duct tape to fix the problem by attaching a map to the damaged fender, but the dust picked up on the tape surface prevented it from sticking properly and the first fix was short lived. After an overnight rethink by the flight controllers, a better method of applying the tape resulted in a satisfactory fix that lasted for the length of the exploration. The crew then deployed the Apollo Lunar Surface Experiments Package (ALSEP) west of the immediate landing site. After completing this, Cernan and Schmitt departed on the first geologic traverse of the mission towards Steno crater to the south of the landing site, during which they gathered of samples; took seven gravimeter measurements; and deployed two explosive packages, which were later detonated remotely to test geophones that had been placed by the astronauts and seismometers that had been placed on previous Apollo missions. The EVA ended after seven hours and twelve minutes.
On December 12, at 6:28 pm EST, Cernan and Schmitt began their second lunar excursion. One of the first tasks of the EVA was repairing the right-rear fender on the LRV, the rearward extension of which had been broken off the previous day. The pair did this by taping together four cronopaque maps with duct tape and clamping the replacement fender extension to the fender, thus providing a means of preventing dust from raining down upon them while in motion. During this EVA, the pair sampled several different types of geologic deposits found in the valley, including the avalanche at the base of the South Massif, orange-colored soil at Shorty crater, and ejecta of Camelot crater. The crew completed this moonwalk after seven hours and thirty-seven minutes. They collected of samples, deployed three more explosive packages and took seven gravimeter measurements.
The third moonwalk, the last of the Apollo program, began at 5:26 pm EST on December 13. During this excursion, the crew collected of lunar samples and took nine gravimeter measurements. They drove the rover to the north and east of the landing site and explored the base of the North Massif, the Sculptured Hills, and the unusual crater Van Serg. Before ending the moonwalk, the crew collected a rock, a breccia, and dedicated it to several different nations which were represented in Mission Control Center in Houston, Texas, at the time. A plaque located on the Lunar Module, commemorating the achievements made during the Apollo program, was then unveiled. Before reentering the LM for the final time, Gene Cernan expressed his thoughts:
Cernan then followed Schmitt into the Lunar Module after spending approximately seven hours and 15 minutes outside during the mission's final lunar excursion.
Return to Earth.
Eugene Cernan and Harrison Schmitt successfully lifted off from the lunar surface in the ascent stage of the Lunar Module on December 14, at 5:55 pm EST. After a successful rendezvous and docking with Ron Evans in the Command/Service Module in orbit, the crew transferred equipment and lunar samples between the LM and the CSM for return to Earth. Following this, the LM ascent stage was sealed off and jettisoned at 1:31 am on December 15. The ascent stage was then deliberately crashed into the Moon in a collision recorded by seismometers deployed on Apollo 17 and previous Apollo expeditions.
On December 17, during the trip back to Earth, at 3:27 pm EST, Ron Evans successfully conducted a one-hour and seven minute spacewalk to retrieve exposed film from the instrument bay on the exterior of the CSM.
On December 19, the crew jettisoned the no-longer-needed Service Module, leaving only the Command Module for return to Earth. The Apollo 17 spacecraft reentered Earth's atmosphere and landed safely in the Pacific Ocean at 2:25 pm, from the recovery ship, the USS "Ticonderoga". Cernan, Evans and Schmitt were then retrieved by a recovery helicopter and were safely aboard the recovery ship 52 minutes after landing.
Spacecraft locations.
The Command Module "America" is currently on display at Space Center Houston at the Lyndon B. Johnson Space Center in Houston, Texas.
The ascent stage of lunar module "Challenger" impacted the Moon December 15, 1972 at 06:50:20.8 UT (1:50 am EST), at . The descent stage remains on the Moon at the landing site, .
In 2009 and again in 2011, the Lunar Reconnaissance Orbiter photographed the landing site from increasingly low orbits.
Depiction of mission in fiction and popular culture.
Portions of the Apollo 17 mission are dramatized in the 1998 HBO miniseries "From the Earth to the Moon" episode entitled "Le Voyage dans la Lune."
The prologue to the 1999 novel "Back to the Moon", by Homer Hickam, begins with a dramatized depiction of the end of the second Apollo 17 EVA. The orange soil then becomes the major driver of the plot of the rest of the story.
The 2005 novel "Tyrannosaur Canyon" by Douglas Preston opens with a depiction of the Apollo 17 moonwalks using quotes taken from the official mission transcript.
Additionally, there have been fictional astronauts in film, literature and television who have been described as "the last man to walk on the Moon," implying they were crew members on Apollo 17. One such character was Steve Austin in the television series "The Six Million Dollar Man". In the 1972 novel "Cyborg", upon which the series was based, Austin remembers watching the Earth "fall away during Apollo XVII." In the 1998 film "Deep Impact" fictional astronaut Spurgeon "Fish" Tanner, portrayed by Robert Duvall, was described at a Presidential press conference as the "last man to walk on the moon" by the President of the United States, portrayed by Morgan Freeman.
In the Anime Aldnoah.Zero, the Apollo 17 mission locates an ancient transporter gate leading to Mars left by an unknown, extinct alien race. This discovery is the divergence point for the story's alternate history.

</doc>
<doc id="1973" url="https://en.wikipedia.org/wiki?curid=1973" title="American Revolution">
American Revolution

The American Revolution was a political upheaval that took place between 1765 and 1783 during which colonists in the Thirteen American Colonies rejected the British monarchy and aristocracy, overthrew the authority of Great Britain, and founded the United States of America.
Starting in 1765, members of American colonial society rejected the authority of the British Parliament to tax them without colonial representatives in the government. During the following decade, protests by colonists—known as Patriots—continued to escalate, as in the Boston Tea Party in 1773 during which patriots destroyed a consignment of taxed tea from the Parliament-controlled and favored East India Company. The British responded by imposing punitive laws—the Coercive Acts—on Massachusetts in 1774, following which Patriots in the other colonies rallied behind Massachusetts. In late 1774 the Patriots set up their own alternative government to better coordinate their resistance efforts against Great Britain, while other colonists, known as Loyalists, preferred to remain aligned to the British Crown.
Tensions escalated to the outbreak of fighting between Patriot militia and British regulars at Lexington and Concord in April 1775. The conflict then evolved into a global war, during which the Patriots (and later their French, Spanish, and Dutch allies) fought the British and Loyalists in what became known as the American Revolutionary War (1775–1783). Patriots in each of the thirteen colonies formed a Provincial Congress that assumed power from the old colonial governments and suppressed Loyalism, and from there built a Continental Army under the leadership of General George Washington. Claiming King George III's rule to be tyrannical and infringing the colonists' "rights as Englishmen", the Continental Congress declared the colonies free and independent states in July 1776. The Patriot leadership professed the political philosophies of liberalism and republicanism to reject monarchy and aristocracy, and proclaimed that all men are created equal. Congress rejected British proposals requiring allegiance to the monarchy and abandonment of independence.
The British were forced out of Boston in 1776, but then captured and held New York City for the duration of the war. The British blockaded the ports and captured other cities for brief periods, but failed to defeat Washington's forces. In early 1778, following a failed patriot invasion of Canada, a British army was captured at the Battle of Saratoga, following which the French openly entered the war as allies of the United States. The war later turned to the American South, where the British captured an army at South Carolina, but failed to enlist enough volunteers from Loyalist civilians to take effective control. A combined American–French force captured a second British army at Yorktown in 1781, effectively ending the war in the United States. The Treaty of Paris in 1783 formally ended the conflict, confirming the new nation's complete separation from the British Empire. The United States took possession of nearly all the territory east of the Mississippi River and south of the Great Lakes, with the British retaining control of Canada and Spain taking Florida.
Among the significant results of the revolution was the creation of a new Constitution of the United States. The 'Three-Fifths Compromise' allowed the southern slaveholders to consolidate power and maintain slavery in America for another eighty years, but through the expansion of voting rights and liberties over subsequent decades the elected government became responsible to the will of the people. The new Constitution established a relatively strong federal national government that included an executive, national judiciary, a bicameral Congress that represented both states in the Senate and population in the House of Representatives.
Origins.
Historians typically begin their histories of the American Revolution with the British victory in the French and Indian War in 1763, which removed France as a major player in North American affairs. Lawrence Henry Gipson, the historian of the British Empire, states:
For the prior history see Thirteen Colonies.
1764–1766: Taxes imposed and withdrawn.
In 1764 Parliament passed the Currency Act to restrain the use of paper money that British merchants saw as a means to evade debt payments. Parliament also passed the Sugar Act imposing customs duties on a number of articles. That same year Prime Minister George Grenville proposed to impose direct taxes on the colonies to raise revenue, but delayed action to see if the colonies would propose some way to raise the revenue themselves. None did, and in March 1765 Parliament passed the Stamp Act which imposed direct taxes on the colonies for the first time. All official documents, newspapers, almanacs and pamphlets—even decks of playing cards—were required to have the stamps.
The colonists objected chiefly on the grounds not that the taxes were high (they were low), but because they had no representation in the Parliament. Benjamin Franklin testified in Parliament in 1766 that Americans already contributed heavily to the defense of the Empire. He said local governments had raised, outfitted and paid 25,000 soldiers to fight France—as many as Britain itself sent—and spent many millions from American treasuries doing so in the French and Indian War alone. Stationing a standing army in Great Britain during peacetime was politically unacceptable. London had to deal with 1,500 politically well-connected British officers who became redundant; it would have to discharge them or station them in North America.
In 1765 the Sons of Liberty formed. They used public demonstrations, boycott, violence and threats of violence to ensure that the British tax laws were unenforceable. While openly hostile to what they considered an oppressive Parliament acting illegally, colonists persisted in sending numerous petitions and pleas for intervention from a monarch to whom they still claimed loyalty. In Boston, the Sons of Liberty burned the records of the vice admiralty court and looted the home of the chief justice, Thomas Hutchinson. Several legislatures called for united action, and nine colonies sent delegates to the Stamp Act Congress in New York City in October 1765. Moderates led by John Dickinson drew up a "Declaration of Rights and Grievances" stating that taxes passed without representation violated their rights as Englishmen. Colonists emphasized their determination by boycotting imports of British merchandise.
The Parliament at Westminster saw itself as the supreme lawmaking authority throughout all British possessions and thus entitled to levy any tax without colonial approval. They argued that the colonies were legally British corporations that were completely subordinate to the British parliament and pointed to numerous instances where Parliament had made laws binding on the colonies in the past. They did not see anything in the unwritten British constitution that made taxes special and noted that Parliament had taxed American trade for decades. Parliament insisted that the colonies effectively enjoyed a "virtual representation" like most British people did, as only a small minority of the British population elected representatives to Parliament. Americans such as James Otis maintained the Americans were not in fact virtually represented.
In London, the Rockingham government came to power (July 1765) and Parliament debated whether to repeal the stamp tax or to send an army to enforce it. Benjamin Franklin made the case for repeal, explaining the colonies had spent heavily in manpower, money, and blood in defense of the empire in a series of wars against the French and Indians, and that further taxes to pay for those wars were unjust and might bring about a rebellion. Parliament agreed and repealed the tax (February 21, 1766), but in the Declaratory Act of March 1766 insisted that parliament retained full power to make laws for the colonies "in all cases whatsoever". The repeal nonetheless caused widespread celebrations in the colonies.
Briggs says unnamed modern American economic historians have challenged the view that Great Britain was placing a heavy burden on the North American colonies and have suggested the cost of defending them from the possibility of invasion by France or Spain was £400,000 – five times the maximum income from them. Briggs rejects the analysis, saying that issue was not invoked at the time.
1767–1773: Townshend Acts and the Tea Act.
In 1767 the Parliament passed the Townshend Acts, which placed duties on a number of essential goods including paper, glass, and tea and established a Board of Customs in Boston to more rigorously execute trade regulations. The new taxes were enacted on the belief that Americans only objected to internal taxes and not external taxes like custom duties. The Americans, however, argued against the constitutionality of the act because its purpose was to raise revenue and not regulate trade. Colonists responded by organizing new boycotts of British goods. These boycotts were less effective, however, as the Townshend goods were widely used.
In February 1768 the Assembly of Massachusetts Bay issued a circular letter to the other colonies urging them to coordinate resistance. The governor dissolved the assembly when it refused to rescind the letter. Meanwhile, in June 1768 a riot broke out in Boston over the seizure of the sloop "Liberty", owned by John Hancock, for alleged smuggling. Custom officials were forced to flee, prompting the British to deploy troops to Boston. A Boston town meeting declared no obedience was due to parliamentary laws and called for the convening of a convention. A convention assembled but only issued a mild protest before dissolving itself. In January 1769 Parliament responded to the unrest by reactivating the Treason Act 1543 which permitted subjects outside the realm to face trials for treason in England. The governor of Massachusetts was instructed to collect evidence of said treason, and although the threat was not carried out it caused widespread outrage.
On March 5, 1770 a large mob gathered around a group of British soldiers. The mob grew more and more threatening, throwing snowballs, rocks and debris at the soldiers. One soldier was clubbed and fell. There was no order to fire but the soldiers fired into the crowd anyway. They hit 11 people; three civilians died at the scene of the shooting, and two died after the incident. The event quickly came to be called the Boston Massacre. Although the soldiers were tried and acquitted (defended by John Adams), the widespread descriptions soon became propaganda to turn colonial sentiment against the British. This in turn began a downward spiral in the relationship between Britain and the Province of Massachusetts.
A new ministry under Lord North came to power in 1770 and Parliament withdrew all taxes except the tax on tea, giving up its efforts to raise revenue while maintaining the right to tax. This temporarily resolved the crisis and the boycott of British goods largely ceased, with only the more radical patriots such as Samuel Adams continuing to agitate.
In June 1772, in what became known as the "Gaspee" Affair, American patriots including John Brown burned a British warship that had been vigorously enforcing unpopular trade regulations. The affair was investigated for possible treason, but no action was taken.
In 1772 it became known that the Crown intended to pay fixed salaries to the governors and judges in Massachusetts. Samuel Adams in Boston set about creating new Committees of Correspondence, which linked Patriots in all 13 colonies and eventually provided the framework for a rebel government. In early 1773 Virginia, the largest colony, set up its Committee of Correspondence, on which Patrick Henry and Thomas Jefferson served.
A total of about 7000 to 8000 Patriots served on "Committees of Correspondence" at the colonial and local levels, comprising most of the leadership in their communities — Loyalists were excluded. The committees became the leaders of the American resistance to British actions, and largely determined the war effort at the state and local level. When the First Continental Congress decided to boycott British products, the colonial and local Committees took charge, examining merchant records and publishing the names of merchants who attempted to defy the boycott by importing British goods.
In 1773 private letters were published where Massachusetts Governor Thomas Hutchinson claimed the colonists could not enjoy all English liberties, and Lieutenant Governor Andrew Oliver called for the direct payment of colonial officials. The letters, whose contents were used as evidence of a systematic plot against American rights, discredited Hutchinson in the eyes of the people the Assembly petitioned for his recall. Benjamin Franklin, post-master general for the colonies, acknowledged that he leaked the letters which led to him being berated by British officials and fired from his job.
Meanwhile, Parliament passed the Tea Act to lower the price of taxed tea exported to the colonies in order to help the East India Company undersell smuggled Dutch tea. Special consignees were appointed to sell the tea in order to bypass colonial merchants. The act was opposed not only by those who resisted the taxes but also by smugglers who stood to lose business. In most instances the consignees were forced to resign and the tea was turned back, but Massachusetts governor Hutchinson refused to allow Boston merchants to give into pressure. A town meeting in Boston determined that the tea would not be landed, and ignored a demand from the governor to disperse. On December 16, 1773 a group of men, led by Samuel Adams and dressed to evoke American Indians, boarded the ships of the British East India Company and dumped £10,000 worth of tea from their holds (approximately £636,000 in 2008) into Boston Harbor. Decades later this event became known as the Boston Tea Party and remains a significant part of American patriotic lore.
1774–1775: Intolerable Acts and the Quebec Act.
The British government responded by passing several Acts which came to be known as the Intolerable Acts, which further darkened colonial opinion towards the British. They consisted of four laws enacted by the British parliament. The first, the Massachusetts Government Act, altered the Massachusetts charter and restricted town meetings. The second Act, the Administration of Justice Act, ordered that all British soldiers to be tried were to be arraigned in Britain, not in the colonies. The third Act was the Boston Port Act, which closed the port of Boston until the British had been compensated for the tea lost in the Boston Tea Party. The fourth Act was the Quartering Act of 1774, which allowed royal governors to house British troops in the homes of citizens without requiring permission of the owner.
In response, Massachusetts patriots issued the Suffolk Resolves and formed an alternative shadow government known as the "Provincial Congress" which began training militia outside British-occupied Boston. In September 1774, the First Continental Congress convened, consisting of representatives from each of the colonies, to serve as a vehicle for deliberation and collective action. During secret debates conservative Joseph Galloway proposed the creation of a colonial Parliament that would be able to approve or disapprove of acts of the British Parliament but his idea was not accepted. The Congress instead endorsed the proposal of John Adams that Americans would obey Parliament voluntarily but would resist all taxes in disguise. Congress called for a boycott beginning on 1 December 1774 of all British goods; it was enforced by new committees authorized by the Congress.
The Quebec Act of 1774 extended Quebec's boundaries to the Ohio River, shutting out the claims of the 13 colonies. By then, however, the Americans had little regard for new laws from London; they were drilling militia and organizing for war.
The British retaliated by confining all trade of the New England colonies to Britain and excluding them from the Newfoundland fisheries. Lord North advanced a compromise proposal in which Parliament would not tax so long as the colonies made fixed contributions for defense and to support civil government. This would also be rejected.
Creating new state constitutions.
Following the Battle of Bunker Hill in June 1775, the Patriots had control of Massachusetts outside the Boston city limits; the Loyalists suddenly found themselves on the defensive with no protection from the British army. In all 13 colonies, Patriots had overthrown their existing governments, closing courts and driving British officials away. They had elected conventions and "legislatures" that existed outside any legal framework; new constitutions were drawn up in each state to supersede royal charters. They declared that they were states now, not colonies.
On January 5, 1776, New Hampshire ratified the first state constitution. In May 1776, Congress voted to suppress all forms of crown authority, to be replaced by locally created authority. Virginia, South Carolina, and New Jersey created their constitutions before July 4. Rhode Island and Connecticut simply took their existing royal charters and deleted all references to the crown. The new states were all committed to republicanism, with no inherited offices. They decided not only what form of government to create, and also how to select those who would craft the constitutions and how the resulting document would be ratified. But there would be no universal suffrage and real power, including the right to elect the future President would still lay in the hands of a few selected elites for many years. On 26 May 1776 John Adams wrote James Sullivan from Philadelphia;
"Depend upon it, sir, it is dangerous to open so fruitful a source of controversy and altercation, as would be opened by attempting to alter the qualifications of voters. There will be no end of it. New claims will arise. Women will demand a vote. Lads from twelve to twenty one will think their rights not enough attended to, and every man, who has not a farthing, will demand an equal voice with any other in all acts of state. It tends to confound and destroy all distinctions, and prostrate all ranks, to one common level".
In states where the wealthy exerted firm control over the process, such as Maryland, Virginia, Delaware, New York and Massachusetts – the last-mentioned of these state's constitutions still being in force in the 21st century, continuously since its ratification on June 15, 1780 – the results were constitutions that featured:
In states where the less affluent had organized sufficiently to have significant power—especially Pennsylvania, New Jersey, and New Hampshire—the resulting constitutions embodied
The radical provisions of Pennsylvania's constitution lasted only 14 years. In 1790, conservatives gained power in the state legislature, called a new constitutional convention, and rewrote the constitution. The new constitution substantially reduced universal white-male suffrage, gave the governor veto power and patronage appointment authority, and added an upper house with substantial wealth qualifications to the unicameral legislature. Thomas Paine called it a constitution unworthy of America.
Military hostilities begin.
Massachusetts was declared in a state of rebellion in February 1775 and the British garrison received orders to disarm the rebels and arrest their leaders, leading to the Battles of Lexington and Concord on 19 April 1775. The Patriots set siege to Boston, expelled royal officials from all the colonies, and took control through the establishment of Provincial Congresses. The Battle of Bunker Hill followed on June 17, 1775. While a British victory, it was at a great cost; about 1,000 British casualties from a garrison of about 6,000, as compared to 500 American casualties from a much larger force. First ostensibly loyal to the king and desiring to govern themselves while remaining in the empire, the repeated pleas by the First Continental Congress for royal intervention on their behalf with Parliament resulted in the declaration by the King that the states were "in rebellion", and the members of Congress were traitors.
In the winter of 1775, the Americans invaded Canada. General Richard Montgomery captured Montreal but a joint attack on Quebec was a total failure; many Americans were captured or died of smallpox.
In March 1776, with George Washington as the commander of the new army, the Continental Army forced the British to evacuate Boston. The revolutionaries were now in full control of all 13 colonies and were ready to declare independence. While there still were many Loyalists, they were no longer in control anywhere by July 1776, and all of the Royal officials had fled.
Prisoners.
In August 1775, George III declared Americans in arms against royal authority to be traitors to the Crown. Following their surrender at the Battles of Saratoga in October 1777, there were thousands of British and Hessian soldiers in American hands. Although Lord Germain took a hard line, the British generals on the scene never held treason trials; they treated captured enemy soldiers as prisoners of war. The dilemma was that tens of thousands of Loyalists were under American control and American retaliation would have been easy. The British built much of their strategy around using these Loyalists. Therefore, no Americans were put on trial for treason. The British maltreated the prisoners they held, resulting in more deaths to American sailors and soldiers than from combat operations. At the end of the war, both sides released their surviving prisoners.
Independence and Union.
In April 1776 the North Carolina Provincial Congress issued the Halifax Resolves, explicitly authorizing its delegates to vote for independence. In May Congress called on all the states to write constitutions, and eliminate the last remnants of royal rule.
By June nine colonies were ready for independence; one by one the last four—Pennsylvania, Delaware, Maryland and New York—fell into line. Richard Henry Lee was instructed by the Virginia legislature to propose independence, and he did so on June 7, 1776. On the 11th a committee was created to draft a document explaining the justifications for separation from Britain. After securing enough votes for passage, independence was voted for on July 2. The Declaration of Independence, drafted largely by Thomas Jefferson and presented by the committee, was slightly revised and unanimously adopted by the entire Congress on July 4, marking the formation of a new sovereign nation, which called itself the United States of America.
The Second Continental Congress approved a new constitution, the "Articles of Confederation," for ratification by the states on November 15, 1777, and immediately began operating under their terms. The Articles were formally ratified on March 1, 1781. At that point, the Continental Congress was dissolved and on the following day a new government of the United States in Congress Assembled took its place, with Samuel Huntington as presiding officer.
Defending the Revolution.
British return: 1776–1777.
According to British historian Jeremy Black, the British had significant advantages including a highly trained army, the world's largest navy and a highly efficient system of public finance that could easily fund the war. However, the British were seriously handicapped by their misunderstanding of the depth of support for the Patriot position. Ignoring the advice of General Gage, they misinterpreted the situation as merely a large-scale riot. London decided that by sending a large military and naval force they could overawe the Americans and force them to be loyal again:
After Washington forced the British out of Boston in the spring of 1776, neither the British nor the Loyalists controlled any significant areas. The British, however, were massing forces at their naval base at Halifax, Nova Scotia. They returned in force in July 1776, landing in New York and defeating Washington's Continental Army at the Battle of Brooklyn in August. After winning the Battle of Brooklyn, the British requested a meeting with representatives from Congress to negotiate an end to hostilities.
A delegation including John Adams and Benjamin Franklin met Howe on Staten Island in New York Harbor on September 11, in what became known as the Staten Island Peace Conference. Howe demanded a retraction of the Declaration of Independence, which was refused, and negotiations ended. The British then quickly seized New York City and nearly captured Washington's army. They made New York their main political and military base of operations in North America, holding it until November 1783. The city became the destination for Loyalist refugees, and a focal point of Washington's intelligence network.
The British also took New Jersey, pushing the Continental Army into Pennsylvania. In a surprise attack in late December 1776 Washington crossed the Delaware River back into New Jersey and defeated Hessian and British armies at Trenton and Princeton, thereby regaining control of most of New Jersey. The victories gave an important boost to Patriots at a time when morale was flagging, and have become iconic events of the war.
In 1777, as part of a grand strategy to end the war, the British sent an invasion force from Canada to seal off New England, which the British perceived as the primary source of agitators. In a major case of mis-coordination, the British army in New York City went to Philadelphia which it captured from Washington. The invasion army under Burgoyne waited in vain for reinforcements from New York, and became trapped in northern New York state. It surrendered after the Battle of Saratoga in October 1777. From early October 1777 until November 15 a pivotal siege at Fort Mifflin, Philadelphia, Pennsylvania distracted British troops and allowed Washington time to preserve the Continental Army by safely leading his troops to harsh winter quarters at Valley Forge.
American alliances after 1778.
The capture of a British army at Saratoga encouraged the French to formally enter the war in support of Congress, as Benjamin Franklin negotiated a permanent military alliance in early 1778, significantly becoming the first country to officially recognize the Declaration of Independence. On February 6, 1778, a Treaty of Amity and Commerce and a Treaty of Alliance were signed between the United States and France. William Pitt spoke out in parliament urging Britain to make peace in America, and unite with America against France, while other British politicians who had previously sympathised with colonial grievances now turned against the American rebels for allying with Britain's international rival and enemy.
Later Spain (in 1779) and the Dutch (1780) became allies of the French, leaving the British Empire to fight a global war alone without major allies, and requiring it to slip through a combined blockade of the Atlantic. The American theater thus became only one front in Britain's war. The British were forced to withdraw troops from continental America to reinforce the valuable sugar-producing Caribbean colonies, which were considered more important.
Because of the alliance with France and the deteriorating military situation, Sir Henry Clinton, the British commander, evacuated Philadelphia to reinforce New York City. General Washington attempted to intercept the retreating column, resulting in the Battle of Monmouth Court House, the last major battle fought in the north. After an inconclusive engagement, the British successfully retreated to New York City. The northern war subsequently became a stalemate, as the focus of attention shifted to the smaller southern theater.
The British move South, 1778–1783.
The British strategy in America now concentrated on a campaign in the southern states. With fewer regular troops at their disposal, the British commanders saw the "southern strategy" as a more viable plan, as the south was perceived as being more strongly Loyalist, with a large population of recent immigrants as well as large numbers of slaves who might be captured or run away to join the British.
Beginning in late December 1778, the British captured Savannah and controlled the Georgia coastline. In 1780 they launched a fresh invasion and took Charleston as well. A significant victory at the Battle of Camden meant that royal forces soon controlled most of Georgia and South Carolina. The British set up a network of forts inland, hoping the Loyalists would rally to the flag.
Not enough Loyalists turned out, however, and the British had to fight their way north into North Carolina and Virginia, with a severely weakened army. Behind them much of the territory they had already captured dissolved into a chaotic guerrilla war, fought predominantly between bands of Loyalist and American militia, which negated many of the gains the British had previously made.
Surrender at Yorktown (1781).
The British army under Cornwallis marched to Yorktown, Virginia where they expected to be rescued by a British fleet. The fleet showed up but so did a larger French fleet, so the British fleet after the Battle of the Chesapeake returned to New York for reinforcements, leaving Cornwallis trapped. In October 1781 under a combined siege by the French and Continental armies under Washington, the British surrendered their second invading army of the war.
The end of the war.
Historians continue to debate whether the odds for American victory were long or short. John E. Ferling says the odds were so long that the American victory was "Almost A Miracle." On the other hand, Joseph Ellis says the odds favored the Americans, and asks whether there ever was any realistic chance for the British to win. He argues that this opportunity came only once, in the summer of 1776 and the British failed that test. Admiral Howe and his brother General Howe, "missed several opportunities to destroy the Continental Army...Chance, luck, and even the vagaries of the weather played crucial roles." Ellis's point is that the strategic and tactical decisions of the Howes were fatally flawed because they underestimated the challenges posed by the Patriots. Ellis concludes that once the Howe brothers failed, the opportunity for a British victory "would never come again."
Support for the conflict had never been strong in Britain, where many sympathized with the rebels, but now it reached a new low. Although King George III personally wanted to fight on, his supporters lost control of Parliament, and no further major land offensives were launched in the American Theater.
Washington could not know that after Yorktown the British would not reopen hostilities. They still had 26,000 troops occupying New York City, Charleston and Savannah, together with a powerful fleet. The French army and navy departed, so the Americans were on their own in 1782–83. The treasury was empty, and the unpaid soldiers were growing restive, almost to the point of mutiny or possible "coup d'état". The unrest among officers of the Newburgh Conspiracy was personally dispelled by Washington in 1783, and Congress subsequently created the promise of a five years bonus for all officers.
Peace treaty.
During negotiations in Paris, the American delegation discovered that France would support independence, but no territorial gains. The new nation would be confined to the area east of the Appalachian Mountains. The American delegation opened direct secret negotiations with London, cutting the French out. British Prime Minister Lord Shelburne was in full charge of the British negotiations. He now saw a chance make the United States a valuable economic partner. The U.S. obtained all the land east of the Mississippi River, south of Canada, and north of Florida. It gained fishing rights off Canadian coasts, and agreed to allow British merchants and Loyalists to try to recover their property. It was a highly favorable treaty for the United States, and deliberately so from the British point of view. Prime Minister Shelburne foresaw highly profitable two-way trade between Britain and the rapidly growing United States, as indeed came to pass. Since the blockade was lifted and the old imperial restrictions were gone, American merchants were free to trade with any nation anywhere in the world, and their businesses flourished.
The British largely abandoned the Indian allies living in the new nation. They were not a party to this treaty and did not recognize it until they were defeated militarily by the United States. However, the British did promise to support the Indians. They sold them munitions and maintained forts in American territory until the Jay Treaty of 1795.
Impact on Britain.
Losing the war and the 13 colonies was a shock to Britain. The war revealed the limitations of Britain's fiscal-military state when it discovered it suddenly faced powerful enemies, with no allies, and dependent on extended and vulnerable transatlantic lines of communication. The defeat heightened dissension and escalated political antagonism to the King's ministers. Inside parliament, the primary concern changed from fears of an over-mighty monarch to the issues of representation, parliamentary reform, and government retrenchment. Reformers sought to destroy what they saw as widespread institutional corruption.
The result was a powerful crisis, 1776–1783. The peace in 1783 left France financially prostrate, while the British economy boomed thanks to the return of American business. The crisis ended after 1784 thanks to the King's shrewdness in outwitting Charles James Fox (the leader of the Fox-North Coalition), and renewed confidence in the system engendered by the leadership of the new Prime Minister, William Pitt. Historians conclude that loss of the American colonies enabled Britain to deal with the French Revolution with more unity and better organization than would otherwise have been the case. Britain turned towards Asia, the Pacific and later Africa with subsequent exploration leading to the rise of the Second British Empire.
Finance.
Britain's war against the Americans, French and Spanish cost about £100 million. The Treasury borrowed 40% of the money it needed. Heavy spending brought France to the verge of bankruptcy and revolution, while the British had relatively little difficulty financing their war, keeping their suppliers and soldiers paid, and hiring tens of thousands of German soldiers.
Britain had a sophisticated financial system based on the wealth of thousands of landowners, who supported the government, together with banks and financiers in London. The efficient British tax system collected about 12 percent of the GDP in taxes during the 1770s.
In sharp contrast, Congress and the American states had no end of difficulty financing the war. In 1775 there was at most 12 million dollars in gold in the colonies, not nearly enough to cover current transactions, let alone finance a major war. The British made the situation much worse by imposing a tight blockade on every American port, which cut off almost all imports and exports. One partial solution was to rely on volunteer support from militiamen, and donations from patriotic citizens.
Another was to delay actual payments, pay soldiers and suppliers in depreciated currency, and promise it would be made good after the war. Indeed, in 1783 the soldiers and officers were given land grants to cover the wages they had earned but had not been paid during the war. Not until 1781, when Robert Morris was named Superintendent of Finance of the United States, did the national government have a strong leader in financial matters.
Morris used a French loan in 1782 to set up the private Bank of North America to finance the war. Seeking greater efficiency, Morris reduced the civil list, saved money by using competitive bidding for contracts, tightened accounting procedures, and demanded the national government's full share of money and supplies from the confederated states.
Congress used four main methods to cover the cost of the war, which cost about 66 million dollars in specie (gold and silver). Congress made two issues of paper money, in 1775–1780, and in 1780–81. The first issue amounted to 242 million dollars. This paper money would supposedly be redeemed for state taxes, but the holders were eventually paid off in 1791 at the rate of one cent on the dollar. By 1780, the paper money was "not worth a Continental", as people said.
The skyrocketing inflation was a hardship on the few people who had fixed incomes—but 90 percent of the people were farmers, and were not directly affected by that inflation. Debtors benefited by paying off their debts with depreciated paper.The greatest burden was borne by the soldiers of the Continental Army, whose wages—usually in arrears—declined in value every month, weakening their morale and adding to the hardships of their families.
Beginning in 1777, Congress repeatedly asked the states to provide money. But the states had no system of taxation either, and were little help. By 1780 Congress was making requisitions for specific supplies of corn, beef, pork and other necessities—an inefficient system that kept the army barely alive.
Starting in 1776, the Congress sought to raise money by loans from wealthy individuals, promising to redeem the bonds after the war. The bonds were in fact redeemed in 1791 at face value, but the scheme raised little money because Americans had little specie, and many of the rich merchants were supporters of the Crown. Starting in 1776, the French secretly supplied the Americans with money, gunpowder, and munitions in order to weaken its arch enemy, Great Britain. When France officially entered the war in 1778, the subsidies continued, and the French government, as well as bankers in Paris and Amsterdam loaned large sums to the American war effort. These loans were repaid in full in the 1790s.
Concluding the Revolution.
Creating a "more perfect union" and guaranteeing rights.
After the war finally ended in 1783, there was a period of prosperity. The national government, still operating under the Articles of Confederation, was able to settle the issue of the western territories, which were ceded by the states to Congress. American settlers moved rapidly into those areas, with Vermont, Kentucky and Tennessee becoming states in the 1790s.
However, the national government had no money to pay either the war debts owed to European nations and the private banks, or to pay Americans who had been given millions of dollars of promissory notes for supplies during the war. Nationalists, led by Washington, Alexander Hamilton and other veterans, feared that the new nation was too fragile to withstand an international war, or even internal revolts such as the Shays's Rebellion of 1786 in Massachusetts.
Calling themselves "Federalists," the nationalists convinced Congress to call the Philadelphia Convention in 1787. It adopted a new Constitution that provided for a much stronger federal government, including an effective executive in a check-and-balance system with the judiciary and legislature. After a fierce debate in the states over the nature of the proposed new government, the Constitution was ratified in 1788. The new government under President George Washington took office in New York in March 1789. As assurances to those who were cautious about federal power, amendments to the Constitution guaranteeing many of the inalienable rights that formed a foundation for the revolution were spearheaded in Congress by James Madison, and later ratified by the states in 1791.
National debt.
The national debt after the American Revolution fell into three categories. The first was the $12 million owed to foreigners—mostly money borrowed from France. There was general agreement to pay the foreign debts at full value. The national government owed $40 million and state governments owed $25 million to Americans who had sold food, horses, and supplies to the revolutionary forces. There were also other debts that consisted of promissory notes issued during the Revolutionary War to soldiers, merchants, and farmers who accepted these payments on the premise that the new Constitution would create a government that would pay these debts eventually.
The war expenses of the individual states added up to $114 million compared to $37 million by the central government. In 1790, at the recommendation of first Secretary of the Treasury, Alexander Hamilton, Congress combined the remaining state debts with the foreign and domestic debts into one national debt totaling $80 million. Everyone received face value for wartime certificates, so that the national honor would be sustained and the national credit established.
Ideology and factions.
The population of the 13 Colonies was far from homogeneous, particularly in their political views and attitudes. Loyalties and allegiances varied widely not only within regions and communities, but also within families and sometimes shifted during the course of the Revolution.
Ideology behind the Revolution.
The ideological movement known as the American Enlightenment was a critical precursor to the American Revolution. Chief among the ideas of the American Enlightenment were the concepts of liberalism, republicanism and fear of corruption. Collectively, the acceptance of these concepts by a growing number of American colonists began to foster an intellectual environment which would lead to a new sense of political and social identity.
Natural rights and republicanism.
John Locke's (1632–1704) ideas on liberty greatly influenced the political thinking behind the revolution, especially through his indirect influence on English writers such as John Trenchard, Thomas Gordon, and Benjamin Hoadly, whose political ideas in turn had a strong influence on the American revolutionaries. Locke is often referred to as "the philosopher of the American Revolution", and is credited with leading Americans to the critical concepts of social contract, natural rights, and "born free and equal." Locke's Two Treatises of Government, published in 1689, were especially influential; Locke in turn was influenced by Protestant theology. He argued that, as all humans were created equally free, governments needed the "consent of the governed." Both Lockean concepts were central to the United States Declaration of Independence, which deduced human equality, "life, liberty, and the pursuit of happiness" from the biblical belief in creation: "All men are "created" equal, ... they are endowed by their "Creator" with certain unalienable Rights." In late eighteenth-century America, belief in "equality by creation" and "rights by creation" was still widespread.
The Declaration also referred to the "Laws of Nature and of Nature's God" as justification for the Americans' separation from the British monarchy. Most eighteenth-century Americans believed that nature, the entire universe, was God's creation. Therefore, he was "Nature's God." Everything, including man, was part of the "universal order of things", which began with God and was pervaded and directed by his providence. Accordingly, the signers of the Declaration professed their "firm reliance on the Protection of divine Providence." And they appealed to "the Supreme Judge o for the rectitude of hei intentions." Like most of his countrymen, George Washington was firmly convinced that he was an instrument of providence, to the benefit not only of the American people but of all of humanity.
The theory of the "social contract" influenced the belief among many of the Founders that among the "natural rights" of man was the right of the people to overthrow their leaders, should those leaders betray the historic rights of Englishmen. In terms of writing state and national constitutions, the Americans heavily used Montesquieu's analysis of the wisdom of the "balanced" British Constitution (mixed government).
A motivating force behind the revolution was the American embrace of a political ideology called "republicanism", which was dominant in the colonies by 1775, but of minor importance back in Great Britain. The republicanism was inspired by the "country party" in Great Britain, whose critique of British government emphasized that corruption was a terrible reality in Great Britain. Americans feared the corruption was crossing the Atlantic; the commitment of most Americans to republican values and to their rights, energized the revolution, as Britain was increasingly seen as hopelessly corrupt and hostile to American interests. Britain seemed to threaten the established liberties that Americans enjoyed. The greatest threat to liberty was depicted as corruption—not just in London but at home as well. The colonists associated it with luxury and, especially, inherited aristocracy, which they condemned.
The Founding Fathers were strong advocates of republican values, particularly Samuel Adams, Patrick Henry, John Adams, Benjamin Franklin, Thomas Jefferson, Thomas Paine, George Washington, James Madison and Alexander Hamilton, which required men to put civic duty ahead of their personal desires. Men had a civic duty to be prepared and willing to fight for the rights and liberties of their countrymen and countrywomen. John Adams, writing to Mercy Otis Warren in 1776, agreed with some classical Greek and Roman thinkers in that "Public Virtue cannot exist without private, and public Virtue is the only Foundation of Republics." He continued:
For women, "republican motherhood" became the ideal, exemplified by Abigail Adams and Mercy Otis Warren; the first duty of the republican woman was to instill republican values in her children and to avoid luxury and ostentation.
Fusing republicanism and liberalism.
While some republics had emerged throughout history, such as the Roman Republic of the ancient world, one based on liberal principles had never existed. Thomas Paine's best-seller pamphlet "Common Sense" appeared in January 1776, after the Revolution had started. It was widely distributed and loaned, and often read aloud in taverns, contributing significantly to spreading the ideas of republicanism and liberalism together, bolstering enthusiasm for separation from Great Britain, and encouraging recruitment for the Continental Army.
Paine provided a new and widely accepted argument for independence, by advocating a complete break with history. "Common Sense" is oriented to the future in a way that compels the reader to make an immediate choice. It offered a solution for Americans disgusted and alarmed at the threat of tyranny.
Impact of Great Awakening.
Dissenting (i.e. Protestant, non-Church of England) churches of the day were the "school of democracy." President John Witherspoon of the College of New Jersey (now Princeton University) wrote widely circulated sermons linking the American Revolution to the teachings of the Hebrew Bible. Throughout the colonies, dissenting Protestant ministers (Congregationalist, Baptist, and Presbyterian) preached Revolutionary themes in their sermons, while most Church of England clergymen preached loyalty to the king, the titular head of the state church. Religious motivation for fighting tyranny reached across socioeconomic lines to encompass rich and poor, men and women, frontiersmen and townsmen, farmers and merchants.
Historian Bernard Bailyn argues that the evangelicalism of the era challenged traditional notions of natural hierarchy by preaching that the Bible taught all men are equal, so that the true value of a man lies in his moral behavior, not his class. Kidd argues that religious disestablishment, belief in a God as the source of human rights, and shared convictions about sin, virtue, and divine providence worked together to unite rationalists and evangelicals and thus encouraged American defiance of the Empire, whereas Bailyn denied that religion played such a critical role. Alan Heimert argued, however, that New Light antiauthoritarianism was essential to the further democratization of colonial American society, and set the stage for a confrontation with British monarchical and aristocratic rule.
Class and psychology of the factions.
Looking back, John Adams concluded in 1818:
In terms of class, Loyalists tended to have longstanding social and economic connections to British merchants and government; for instance, prominent merchants in major port cities such as New York, Boston and Charleston tended to be Loyalists, as did men involved with the fur trade along the northern frontier. In addition, officials of colonial government and their staffs, those who had established positions and status to maintain, favored maintaining relations with Great Britain. They often were linked to British families in England by marriage as well.
By contrast, Patriots by number tended to be yeomen farmers, especially in the frontier areas of New York and the backcountry of Pennsylvania, Virginia and down the Appalachian mountains. They were craftsmen and small merchants. Leaders of both the Patriots and the Loyalists were men of educated, propertied classes. The Patriots included many prominent men of the planter class from Virginia and South Carolina, for instance, who became leaders during the Revolution, and formed the new government at the national and state levels.
To understand the opposing groups, historians have assessed evidence of their hearts and minds. In the mid-20th century, historian Leonard Woods Labaree identified eight characteristics of the Loyalists that made them essentially conservative; traits to those characteristic of the Patriots. Older and better established men, Loyalists tended to resist innovation. They thought resistance to the Crown—which they insisted was the only legitimate government—was morally wrong, while the Patriots thought morality was on their side.
Loyalists were alienated when the Patriots resorted to violence, such as burning houses and tarring and feathering. Loyalists wanted to take a centrist position and resisted the Patriots' demand to declare their opposition to the Crown. Many Loyalists, especially merchants in the port cities, had maintained strong and long-standing relations with Britain (often with business and family links to other parts of the British Empire).
Many Loyalists realized that independence was bound to come eventually, but they were fearful that revolution might lead to anarchy, tyranny or mob rule. In contrast, the prevailing attitude among Patriots, who made systematic efforts to use mob violence in a controlled manner, was a desire to seize the initiative. Labaree also wrote that Loyalists were pessimists who lacked the confidence in the future displayed by the Patriots.
Historians in the early 20th century, such as J. Franklin Jameson, examined the class composition of the Patriot cause, looking for evidence of a class war inside the revolution. In the last 50 years, historians have largely abandoned that interpretation, emphasizing instead the high level of ideological unity. Just as there were rich and poor Loyalists, the Patriots were a 'mixed lot', with the richer and better educated more likely to become officers in the Army.
Ideological demands always came first: the Patriots viewed independence as a means to gain freedom from British oppression and taxation and, above all, to reassert what they considered to be their rights as English subjects. Most yeomen farmers, craftsmen, and small merchants joined the Patriot cause to demand more political equality. They were especially successful in Pennsylvania but less so in New England, where John Adams attacked Thomas Paine's "Common Sense" for the "absurd democratical notions" it proposed.
King George III.
The war became a personal issue for the king, fueled by his growing belief that British leniency would be taken as weakness by the Americans. The king also sincerely believed he was defending Britain's constitution against usurpers, rather than opposing patriots fighting for their natural rights.
Patriots.
At the time, revolutionaries were called "Patriots", "Whigs", "Congress-men", or "Americans". They included a full range of social and economic classes, but were unanimous regarding the need to defend the rights of Americans and uphold the principles of republicanism in terms of rejecting monarchy and aristocracy, while emphasizing civic virtue on the part of the citizens. Newspapers were strongholds of patriotism (although there were a few Loyalist papers), and printed many pamphlets, announcements, patriotic letters and pronouncements.
According to historian Robert Calhoon, the consensus of historians is that 40–45% of the white population in the Thirteen Colonies supported the Patriots' cause, 15–20% supported the Loyalists, and the remainder were neutral or kept a low profile. Mark Lender explores why ordinary folk became insurgents against the British even though they were unfamiliar with the ideological rationales being offered. They held very strongly a sense of "rights" that they felt the British were violating – rights that stressed local autonomy, fair dealing, and government by consent. They were highly sensitive to the issue of tyranny, which they saw manifested in the British response to the Boston Tea Party. The arrival in Boston of the British Army heightened their sense of violated rights, leading to rage and demands for revenge. They had faith that God was on their side.
Loyalists.
The consensus of scholars is that about 15–20% of the white population remained loyal to the British Crown. Those who actively supported the king were known at the time as "Loyalists", "Tories", or "King's men". The Loyalists never controlled territory unless the British Army occupied it. Loyalists were typically older, less willing to break with old loyalties, often connected to the Church of England, and included many established merchants with strong business connections across the Empire, as well as royal officials such as Thomas Hutchinson of Boston. There were 500 to 1000 black loyalists who were held as slaves by patriots, escaped to British lines and joined the British army. Most died of disease but Britain took the survivors to Canada as free men.
The revolution could divide families. The most dramatic example was when William Franklin, son of Benjamin Franklin and royal governor of the Province of New Jersey, remained loyal to the Crown throughout the war; they never spoke again. Recent immigrants who had not been fully Americanized were also inclined to support the King, such as recent Scottish settlers in the back country; among the more striking examples of this, see Flora MacDonald.
After the war, the great majority of the 450,000–500,000 Loyalists remained in America and resumed normal lives. Some, such as Samuel Seabury, became prominent American leaders. Estimates vary, but about 62,000 Loyalists relocated to Canada, and others to Britain (7,000) or to Florida or the West Indies (9,000). The exiles represented approximately 2% of the total population of the colonies. Nearly all black loyalists left for Nova Scotia, Florida, or England, where they could remain free. When Loyalists left the South in 1783, they took thousands of their slaves with them to be slaves in the British West Indies.
Neutrals.
A minority of uncertain size tried to stay neutral in the war. Most kept a low profile, but the Quakers, especially in Pennsylvania, were the most important group to speak out for neutrality. As Patriots declared independence, the Quakers, who continued to do business with the British, were attacked as supporters of British rule, "contrivers and authors of seditious publications" critical of the revolutionary cause.
Role of women.
Women contributed to the American Revolution in many ways, and were involved on both sides. While formal Revolutionary politics did not include women, ordinary domestic behaviors became charged with political significance as Patriot women confronted a war that permeated all aspects of political, civil, and domestic life. They participated by boycotting British goods, spying on the British, following armies as they marched, washing, cooking, and tending for soldiers, delivering secret messages, and in a few cases like Deborah Samson, fighting disguised as men. Also, Mercy Otis Warren held meetings in her house and cleverly attacked Loyalists with her creative plays and histories. Above all, they continued the agricultural work at home to feed their families and the armies. They maintained their families during their husbands' absences and sometimes after their deaths.
American women were integral to the success of the boycott of British goods, as the boycotted items were largely household items such as tea and cloth. Women had to return to knitting goods, and to spinning and weaving their own cloth — skills that had fallen into disuse. In 1769, the women of Boston produced 40,000 skeins of yarn, and 180 women in Middletown, Massachusetts wove of cloth.
A crisis of political loyalties could disrupt the fabric of colonial America women's social worlds: whether a man did or did not renounce his allegiance to the King could dissolve ties of class, family, and friendship, isolating women from former connections. A woman's loyalty to her husband, once a private commitment, could become a political act, especially for women in America committed to men who remained loyal to the King. Legal divorce, usually rare, was granted to Patriot women whose husbands supported the King.
Other participants.
France.
In early 1776, France set up a major program of aid to the Americans, and the Spanish secretly added funds. Each country spent one million "livres tournaises" to buy munitions. A dummy corporation run by Pierre Beaumarchais concealed their activities. American rebels obtained some munitions through the Dutch Republic as well as French and Spanish ports in the West Indies.
Spain.
Spain did not officially recognize the U.S. but became an informal ally when it declared war on Britain on June 21, 1779. Bernardo de Gálvez y Madrid, general of the Spanish forces in New Spain, also served as governor of Louisiana. He led an expedition of colonial troops to force the British out of Florida and keep open a vital conduit for supplies.
Native Americans.
Most Native Americans rejected pleas that they remain neutral and supported the British Crown, both because of trading relationships and Britain's effort to establish an Indian reserve and prohibit colonial settlement west of the Appalachian Mountains. The great majority of the 200,000 Native Americans east of the Mississippi distrusted the colonists and supported the British cause, hoping to forestall continued colonial encroachment on their territories. Those tribes that were more closely involved in colonial trade tended to side with the revolutionaries, although political factors were important as well.
Except for warriors and bands associated with four of the Iroquois nations in New York and Pennsylvania, which allied with the British, most Native Americans did not participate directly in the war. The British did have other allies especially in the upper Midwest. They provided Indians with funding and weapons to attack American outposts. Some Indians tried to remain neutral, seeing little value in joining a European conflict and fearing reprisals from whichever side they opposed. The Oneida and Tuscarora, among the Iroquois of central and western New York, supported the American cause.
The British provided arms to Indians, who were led by Loyalists in war parties to raid frontier settlements from the Carolinas to New York. They killed many settlers on the frontier, especially in Pennsylvania and New York's Mohawk Valley.
In 1776 Cherokee war parties attacked American colonists all along the southern frontier of the uplands throughout the Washington District, North Carolina (now Tennessee) and the Kentucky wilderness area. While the Cherokee launched raids numbering a couple hundred warriors, as seen in the Cherokee–American wars, they could not mobilize enough forces to fight a major invasion of colonial areas without the help of allies, most often the Creek. The Chickamauga Cherokee under Dragging Canoe allied themselves closely with the British, and fought on for an additional decade after the signing of the Treaty of Paris.
Joseph Brant of the powerful Mohawk nation, part of the Iroquois Confederacy based in New York, was the most prominent Native American leader against the rebel forces. In 1778 and 1780, he led 300 Iroquois warriors and 100 white Loyalists in multiple attacks on small frontier settlements in New York and Pennsylvania, killing many settlers and destroying villages, crops and stores. The Seneca, Onondaga and Cayuga of the Iroquois Confederacy also allied with the British against the Americans.
In 1779 the Continentals retaliated with an American army under John Sullivan, which raided and destroyed 40 empty Iroquois villages in central and western New York. Sullivan's forces systematically burned the villages and destroyed about 160,000 bushels of corn that comprised the winter food supply. Facing starvation and homeless for the winter, the Iroquois fled to the Niagara Falls area and to Canada, mostly to what became Ontario. The British resettled them there after the war, providing land grants as compensation for some of their losses.
At the peace conference following the war, the British ceded lands which they did not really control, and did not consult their Indian allies. They "transferred" control to the United States of all the land east of the Mississippi and north of Florida. The historian Calloway concludes:
The British did not give up their forts in the West (what is now the eastern Midwest, stretching from Ohio to Wisconsin) until 1796; they kept alive the dream of forming a satellite Indian nation there, which they called a Neutral Indian Zone. That goal was one of the causes of the War of 1812.
African Americans.
Free blacks in the North and South fought on both sides of the Revolution, but most fought for the patriots. Gary Nash reports that recent research concludes there were about 9000 black Patriot soldiers, counting the Continental Army and Navy, and state militia units, as well as privateers, wagoneers in the Army, servants to officers, and spies. Ray Raphael notes that while thousands did join the Loyalist cause, "A far larger number, free as well as slave, tried to further their interests by siding with the patriots." Crispus Attucks, who was shot dead by British soldiers in the Boston Massacre in 1770, is an iconic martyr to Patriots. Both sides offered freedom and re-settlement to slaves who were willing to fight for them, recruiting slaves whose owners supported the opposing cause.
Many African-American slaves sided with the Loyalists. Tens of thousands in the South used the turmoil of war to escape, and the southern plantation economies of South Carolina and Georgia especially were disrupted. During the Revolution, the British tried to turn slavery against the Americans.
Historian David Brion Davis explains the difficulties with a policy of wholesale arming of the slaves:
Davis underscored the British dilemma: "Britain, when confronted by the rebellious American colonists, hoped to exploit their fear of slave revolts while also reassuring the large number of slave-holding Loyalists and wealthy Caribbean planters and merchants that their slave property would be secure". The colonists accused the British of encouraging slave revolts.
American advocates of independence were commonly lampooned in Britain for what was termed their hypocritical calls for freedom, at the same time that many of their leaders were planters who held hundreds of slaves. Samuel Johnson snapped, "how is it we hear the loudest yelps for liberty among the lav drivers of the Negroes?" Benjamin Franklin countered by criticizing the British self-congratulation about "the freeing of one Negro" (Somersett) while they continued to permit the Slave Trade.
Phyllis Wheatley, a black poet who popularized the image of Columbia to represent America, came to public attention when her "Poems on Various Subjects, Religious and Moral" appeared in 1773.
During the war, slaves escaped from across New England and the mid-Atlantic area to British-occupied cities, such as New York. The effects of the war were more dramatic in the South. In Virginia the royal governor Lord Dunmore recruited black men into the British forces with the promise of freedom, protection for their families, and land grants. Tens of thousands of slaves escaped to British lines throughout the South, causing dramatic losses to slaveholders and disrupting cultivation and harvesting of crops. For instance, South Carolina was estimated to lose about 25,000 slaves, or one third of its slave population, to flight, migration or death. From 1770 to 1790, the black proportion of the population (mostly slaves) in South Carolina dropped from 60.5 percent to 43.8 percent; and in Georgia from 45.2 percent to 36.1 percent.
When the British evacuated its forces from Savannah and Charleston, it also gave transportation to 10,000 slaves, carrying through on its commitment to them. They evacuated and resettled more than 3,000 "Black Loyalists" from New York to Nova Scotia, Upper and Lower Canada. Others sailed with the British to England or were resettled as freedmen in the West Indies of the Caribbean. But slaves who were carried to the Caribbean under control of Loyalist masters generally remained slaves until British abolition in its colonies in 1834. More than 1200 of the Black Loyalists of Nova Scotia later resettled in the British colony of Sierra Leone, where they became leaders of the Krio ethnic group of Freetown and the later national government. Many of their descendants still live in Sierra Leone, as well as other African countries.
Effects of the Revolution.
Loyalist expatriation.
About 60,000 to 70,000 Loyalists left the newly founded republic; some migrated to Britain. The remainder, known as United Empire Loyalists, received land and subsidies for resettlement in British colonies in North America, especially Quebec (concentrating in the Eastern Townships), Prince Edward Island, and Nova Scotia. The new colonies of Upper Canada (now Ontario) and New Brunswick were expressly created by Britain for their benefit, where the Crown awarded land to Loyalists as compensation for losses in the United States. Britain wanted to develop the frontier of Upper Canada on a British colonial model. But about 80% of the Loyalists stayed in the United States and became full, loyal citizens; some of the exiles later returned to the U.S.
Interpretations.
Interpretations about the effect of the Revolution vary. Contemporary participants referred to the events as "the revolution". Greene argues that the events were not "revolutionary", as the relationships and property rights of colonial society were not transformed: a distant government was simply replaced with a local one; the Revolution is still sometimes known outside the United States as the American War of Independence.
Historians such as Bernard Bailyn, Gordon Wood, and Edmund Morgan accept the contemporary view of the participants that the American Revolution was a unique and radical event that produced deep changes and had a profound effect on world affairs, based on an increasing belief in the principles of the Enlightenment, as reflected in how liberalism was understood during the period, and republicanism. These were demonstrated by a leadership and government that espoused protection of natural rights, and a system of laws chosen by the people. However, what was then considered "the people" was still mostly restricted to free white males who were able to pass a property-qualification. Such a restriction made a significant gain of the revolution in the short term irrelevant to women, African Americans and slaves, poor white men, youth, and Native Americans. Only with the development of the American system over the following centuries would "a government by the people", promised by the revolution, be won for a greater proportion of the population.
Morgan has argued that in terms of long-term impact on American society and values:
Inspiring all colonies.
After the Revolution, genuinely democratic politics became possible in the former colonies. The rights of the people were incorporated into state constitutions. Concepts of liberty, individual rights, equality among men and hostility toward corruption became incorporated as core values of liberal republicanism. The greatest challenge to the old order in Europe was the challenge to inherited political power and the democratic idea that government rests on the consent of the governed. The example of the first successful revolution against a European empire, and the first successful establishment of a republican form of democratically elected government, provided a model for many other colonial peoples who realized that they too could break away and become self-governing nations with directly elected representative government.
The Dutch Republic, also at war with Britain, was the next country to sign a treaty with the United States, on October 8, 1782. On April 3, 1783, Ambassador Extraordinary Gustaf Philip Creutz, representing King Gustav III of Sweden, and Benjamin Franklin, signed a Treaty of Amity and Commerce with the U.S.
The American Revolution was the first wave of the Atlantic Revolutions: the French Revolution, the Haitian Revolution, and the Latin American wars of independence. Aftershocks reached Ireland in the Irish Rebellion of 1798, in the Polish-Lithuanian Commonwealth, and in the Netherlands.
The Revolution had a strong, immediate influence in Great Britain, Ireland, the Netherlands, and France. Many British and Irish Whigs spoke in favor of the American cause. In Ireland, there was a profound impact; the Protestants who controlled Ireland were demanding more and more self-rule. Under the leadership of Henry Grattan, the so-called "Patriots" forced the reversal of mercantilist prohibitions against trade with other British colonies. The King and his cabinet in London could not risk another rebellion on the American model, and made a series of concessions to the Patriot faction in Dublin. Armed Protestant volunteer units were set up to protect against an invasion from France. As in America, so too in Ireland the King no longer had a monopoly of lethal force.
The Revolution, along with the Dutch Revolt (end of the 16th century) and the 17th century English Civil War, was among the examples of overthrowing an old regime for many Europeans who later were active during the era of the French Revolution, such as Marquis de Lafayette. The American Declaration of Independence influenced the French Declaration of the Rights of Man and the Citizen of 1789.
The spirit of the Declaration of Independence led to laws ending slavery in all the Northern states and the Northwest Territory, with New Jersey the last in 1804—long before the British Parliament acted in 1833 to abolish slavery in its colonies. States such as New Jersey and New York adopted gradual emancipation, which kept some people as slaves for more than two decades longer.
Status of American women.
The democratic ideals of the Revolution inspired changes in the roles of women.
The concept of republican motherhood was inspired by this period and reflects the importance of Republicanism as the dominant American ideology. It assumed that a successful republic rested upon the virtue of its citizens. Women were considered to have the essential role of instilling their children with values conducive to a healthy republic. During this period, the wife's relationship with her husband also became more liberal, as love and affection instead of obedience and subservience began to characterize the ideal marital relationship. In addition, many women contributed to the war effort through fundraising and running family businesses in the absence of husbands.
The traditional constraints gave way to more liberal conditions for women. Patriarchy faded as an ideal; young people had more freedom to choose their spouses and more often used birth control to regulate the size of their families. Society emphasized the role of mothers in child rearing, especially the patriotic goal of raising republican children rather than those locked into aristocratic value systems. There was more permissiveness in child-rearing. Patriot women married to Loyalists who left the state could get a divorce and obtain control of the ex-husband's property.
Whatever gains they had made, however, women still found themselves subordinated, legally and socially, to their husbands, disfranchised and usually with only the role of mother open to them. But, some women earned livelihoods as midwives and in other roles in the community, which were not originally recognized as significant by men.
Abigail Adams expressed to her husband, the president, the desire of women to have a place in the new republic:
Zagarri in 2007 argued that the American Revolution created a continuing debate on the rights of woman and an environment favorable to women's participation in U.S. politics. She asserts that for a brief decade, a "comprehensive transformation in women's rights, roles, and responsibilities seemed not only possible but perhaps inevitable." But, the changes also engendered a backlash that set back the cause of women's rights and led to a greater rigidity that marginalized women from political life.
Status of African Americans.
In the first two decades after the American Revolution, state legislatures and individuals took actions to free numerous slaves, in part based on revolutionary ideals. Northern states passed new constitutions that contained language about equal rights or specifically abolished slavery; some states, such as New York and New Jersey, where slavery was more widespread, passed laws by the end of the 18th century to abolish slavery by a gradual method; in New York, the last slaves were freed in 1827.
While no southern state abolished slavery, for a period individual owners could free their slaves by personal decision, often providing for manumission in wills but sometimes filing deeds or court papers to free individuals. Numerous slaveholders who freed their slaves cited revolutionary ideals in their documents; others freed slaves as a reward for service. Records also suggest that some slaveholders were freeing their own mixed-race children, born into slavery to slave mothers.
Memory.
The American Revolution has a central place in the American memory. As the founding story, it is covered in the schools, memorialized by a national holiday, and commemorated in innumerable monuments. Thus Independence Day (the "Fourth of July") is a major national holiday celebrated annually. Besides local sites such as Bunker Hill, one of the first national pilgrimages for memorial tourists was Mount Vernon, George Washington's estate (near Washington City), which attracted ten thousand visitors a year by the 1850s.
Crider points out that in the 1850s, editors and orators both North and South claimed their region was the true custodian of the legacy of 1776, as they used the Revolution symbolically in their rhetoric. Ryan, noting that the Bicentennial was celebrated a year after the United States' humiliating 1975 withdrawal from Vietnam, says the Ford administration stressed the themes of renewal and rebirth based on a restoration of traditional values, and presented a nostalgic approach to 1776 that made it seem eternally young and fresh.
Albanese argues that the Revolution became the main source of the non-denominational "American civil religion" that has shaped patriotism, and the memory and meaning of the nation's birth ever since. She says that specific battles are not central (as they are for the Civil War) but rather certain events and people have been celebrated as icons of certain virtues (or vices). Thus she points out the Revolution produced a Moses-like leader (George Washington), prophets (Thomas Jefferson, Tom Paine), disciples (Alexander Hamilton, James Madison) and martyrs (Boston Massacre, Nathan Hale), as well as devils (Benedict Arnold), sacred places (Valley Forge, Bunker Hill), rituals (Boston Tea Party), emblems (the new flag), sacred holidays (Independence Day), and a holy scripture whose every sentence is carefully studied and applied in current law cases (The Declaration of Independence, the Constitution and the Bill of Rights).

</doc>
<doc id="1974" url="https://en.wikipedia.org/wiki?curid=1974" title="April 17">
April 17


</doc>
<doc id="1975" url="https://en.wikipedia.org/wiki?curid=1975" title="Alan Ayckbourn">
Alan Ayckbourn

Sir Alan Ayckbourn, CBE (born 12 April 1939) is a prolific English playwright and director. He has written and produced more than seventy full-length plays in Scarborough and London and was, between 1972 and 2009, the artistic director of the Stephen Joseph Theatre in Scarborough, where all but four of his plays have received their first performance. More than 40 have subsequently been produced in the West End, at the Royal National Theatre or by the Royal Shakespeare Company since his first hit "Relatively Speaking" opened at the Duke of York's Theatre in 1967.
Major successes include "Absurd Person Singular" (1975), "The Norman Conquests" trilogy (1973), "Bedroom Farce" (1975), "Just Between Ourselves" (1976), "A Chorus of Disapproval" (1984), "Woman in Mind" (1985), "A Small Family Business" (1987), "Man Of The Moment" (1988), "House" & "Garden" (1999) and "Private Fears in Public Places" (2004). His plays have won numerous awards, including seven London "Evening Standard" Awards. They have been translated into over 35 languages and are performed on stage and television throughout the world. Ten of his plays have been staged on Broadway, attracting two Tony nominations, and one Tony award.
Life.
Childhood.
Ayckbourn was born in Hampstead, London. His mother Irene Worley ("Lolly") was a writer of short stories who published under the name "Mary James". His father, Horace Ayckbourn, was an orchestral violinist, at one time deputy leader of the London Symphony Orchestra. His parents, who separated shortly after World War II, never married, and Ayckbourn's mother divorced her "first" husband to marry again in 1948.
Ayckbourn wrote his first play at Wisborough Lodge (a preparatory school in the village of Wisborough Green) when he was about 10. Whilst at prep school as a boarder, his mother wrote to tell him she was marrying Cecil Pye, a bank manager. When he went home for the holidays his new family consisted of his mother, his stepfather and Christopher, his stepfather's son by an earlier marriage. This relationship too, reportedly ran into difficulties early on.
Ayckbourn attended Haileybury and Imperial Service College, in the village of Hertford Heath, and whilst there toured Europe and America with the school's Shakespeare company.
Adult life.
After leaving school at 17, Ayckbourn's career took several temporary jobs in various places before starting a temporary job at the Scarborough Library Theatre, where he was introduced to the artistic director, Stephen Joseph. It is said that Joseph became both a mentor and father figure for Ayckbourn until his untimely death in 1967, and he has consistently spoken highly of him.
Ayckbourn's career was briefly interrupted when he was called for National Service. He was swiftly discharged, officially on medical grounds, but it is suggested that a doctor who noticed his reluctance to join the Armed Forces deliberately failed the medical as a favour. Although Ayckbourn continued to move where his career took him, he settled in Scarborough, eventually buying Longwestgate House, the house formerly owned by Stephen Joseph.
In 1957, Ayckbourn married Christine Roland, another member of the Library Theatre company, and indeed Ayckbourn's first two plays were written jointly with her under the pseudonym of "Roland Allen". They had two sons, Steven and Philip. However, the marriage had difficulties which eventually led to their separation in 1971. Alan Ayckbourn said that his relationship with Christine became easy once they agreed their marriage was over. Around this time, he started to share a home with Heather Stoney, an actress he had first met ten years earlier. Like his mother, neither he nor Christine sought a divorce for the next thirty years and it was only in 1997 that they formally divorced; Ayckbourn married Heather Stoney. One side-effect of the timing is that, as Alan was awarded a knighthood a few months before the divorce, both his first and second wife are entitled to take the title of Lady Ayckbourn.
In February 2006, he suffered a stroke in Scarborough, and stated: "I hope to be back on my feet, or should I say my left leg, as soon as possible, but I know it is going to take some time. In the meantime I am in excellent hands and so is the Stephen Joseph Theatre." He left hospital after eight weeks and returned to directing after six months, but the following year he announced he would step down as artistic director of the Stephen Joseph Theatre. Ayckbourn, however, continues to write and direct his own work at the theatre.
Influence on plays.
Since Alan Ayckbourn's plays started becoming established in the West End, interviewers have raised the question of whether his work is autobiographical. There is no clear answer to this question. There has only been one biography, written by Paul Allen, and this primarily covers his career in the theatre. Ayckbourn has frequently said he sees aspects of himself in all his characters. For example, in "Bedroom Farce" (1975), he admitted to being, in some respects, all four of the men in the play. It has been suggested that, after Ayckbourn himself, the person who is used the most in his plays is his mother, particularly as Susan in "Woman in Mind" (1985).
What is less clear is how much influence events in Ayckbourn's life have had on his writing. It is true that the theme of marriages in various difficulties was heavily present throughout his plays in the early seventies, around the time his own marriage was coming to an end. However, by this time, he had also witnessed the failures of his parents' relationships as well as those of some of his friends. Which relationships, if any, he drew on for his plays, is unclear. In Paul Allen’s biography, Ayckbourn is briefly compared to Dafydd and Guy in "A Chorus of Disapproval" (1984). Both characters feel themselves in trouble, and there was speculation that Alan Ayckbourn himself may have felt himself to be in trouble. At the time, he had reportedly become seriously involved with another actress, which threatened his relationship with Heather Stoney. But again, it is unclear whether this had any effect on the writing, and Paul Allen's view is that it is not current experience that Ayckbourn uses for his plays.
It could be that Ayckbourn had written plays with himself and his own issues in mind, but as Ayckbourn is portrayed as a guarded and private man, it is hard to imagine him exposing his own life in his plays to any great degree. In the biography, Paul Allen wrote, regarding a suggestion in "Cosmopolitan" that his plays were becoming autobiographical: "If we take that to mean that his plays tell his own life story, he still hasn't started."
Career.
Early career and acting.
On leaving school his theatrical career started immediately, with an introduction to Sir Donald Wolfit by his French master. Ayckbourn joined Wolfit on tour to the Edinburgh Festival Fringe as an acting assistant stage manager (meaning a role that involved both acting and stage management) for three weeks, with his first role on the professional stage being various parts in "The Strong are Lonely" by Fritz Hochwälder. In the following year, Ayckbourn appeared in six other plays at the Connaught Theatre, Worthing, and the Thorndyke theatre, Leatherhead.
In 1957, Ayckbourn was employed by the director Stephen Joseph at the Library Theatre, Scarborough, the predecessor to the modern Stephen Joseph Theatre. His role, again, was initially an acting stage manager. This employment led to Ayckbourn's first professional script commission, in 1958. When he complained about the quality of a script he was performing, Joseph challenged him to write a better one. The result was "The Square Cat", written under the pseudonym Roland Allen and first performed in 1959. In this play, Ayckbourn himself played the character Jerry Watiss.
After thirty-four appearances in plays at the Library Theatre, including four of his own, in 1962 Ayckbourn moved to Stoke-on-Trent to help set up the Victoria Theatre, (now the New Vic), where he appeared in a further eighteen plays. His final appearance in one of his own plays was as the Crimson Gollywog in the disastrous children's play "Christmas v Mastermind". He left the Stoke company in 1964, officially to commit his time to the London production of "Mr. Whatnot", but reportedly because was having trouble working with the artistic director, Peter Cheeseman. By now, his career as a writer was coming to fruition, and his acting career was sidelined.
His final role on stage was as Jerry in "Two for the Seesaw" by William Gibson, at the Civic Theatre in Rotherham. He was left stranded on stage because Heather Stoney was unable to re-appear because the props had been left unpacked, and this led him to decide acting was more trouble than it was worth. The assistant stage manager on the production, Bill Kenwright, would become one of the UK's most successful producers.
Writing.
Alan Ayckbourn's earliest plays were written and produced at a time when the Scarborough Library theatre, like most regional theatres, regularly commissioned work from their own actors to keep costs down (the other notable actor whose work was being commissioned being David Campton). His first play, "The Square Cat", was sufficiently popular locally to secure further commissions, but neither this nor the following three plays had any major impact outside of Scarborough. But, after his transfer to Victoria Theatre in Stoke-on-Trent, there came "Christmas v Mastermind", which flopped and is now universally regarded as Ayckbourn's greatest disaster.
His fortunes began to revive in 1963 with "Mr. Whatnot", again premièring at the Victoria Theatre. This was the first play that Ayckbourn was sufficiently happy with to allow performances today, and the first play to receive a West End performance. However, the West End production flopped, in part down to misguided casting. After this, Ayckbourn experimented by collaborating with comedians, first writing a monologue for Tommy Cooper, and later with Ronnie Barker, who played Lord Slingsby-Craddock in the London production of "Mr Whatnot" in 1964, for the scripts of for LWT's "Hark at Barker". Ayckbourn used the pseudonym 'Peter Caulfield' because he was under exclusive contract to the BBC at the time.
Then, in 1965, back at the Scarborough Library Theatre, "Meet my Father" was produced, later retitled "Relatively Speaking". This time, the play was a massive success, both in Scarborough and the West End, making Alan Ayckbourn rich and earning him a congratulatory telegram from Noël Coward. This was not quite the end of Ayckbourn's hit-and-miss record, because his following play, "The Sparrow" only ran for three weeks at Scarborough. However, the following play, "How the Other Half Loves", secured his runaway success as a playwright.
The height of Ayckbourn's commercial success included "Absurd Person Singular" (1975), "The Norman Conquests" trilogy (1973), "Bedroom Farce" (1975) and "Just Between Ourselves" (1976), all plays that focused heavily on marriage in the British middle classes. The only failure during this period was a 1975 musical with Andrew Lloyd Webber, "Jeeves", and even this did little to dent Ayckbourn's popularity. Although his plays have received major West End productions almost from the beginning of his writing career, and hence have been reviewed in British newspapers, Ayckbourn's work was for years routinely dismissed as being too slight for serious study. Recently, scholars have begun to view Ayckbourn as an important commentator on the lifestyles of the British suburban middle class, and as a stylistic innovator who experiments with theatrical styles within the boundaries set by popular tastes.
From the 1980s, Ayckbourn began to move away from the recurring themes of marriage and explore other contemporary themes, one example being "Woman in Mind", a play performed entirely from the perspective of a Woman going through a nervous breakdown. He also experimented with several more unconventional ways of writing plays, such as "Intimate Exchanges", which has one beginning and sixteen possible endings, and "House & Garden", where two plays take place simultaneously of two different stages, as well as diversifying into children's theatre (such as "Mr A's Amazing Maze Plays" and musical plays, such as "By Jeeves" (a more successful rewrite of the original "Jeeves").
With a résumé of over seventy plays, of which more than forty have played at the National Theatre or in the West End, Alan Ayckbourn is one of England’s most successful living playwrights. Despite his success, honours and awards (which include a prestigious Laurence Olivier Award), Alan Ayckbourn remains a relatively anonymous figure dedicated to regional theatre. Throughout his writing career, all but four of his plays were premièred at the Stephen Joseph Theatre in Scarborough in its three different locations.
Alan Ayckbourn received the CBE in 1987 and was knighted in the 1997 New Year Honours. It is frequently claimed (but not proven) that Alan Ayckbourn is the most performed living English playwright, and the second most performed of all time after Shakespeare.
Although Alan Ayckbourn's plays no longer dominate the theatrical scene on the scale of his earlier works, he continues to write, his most recent major success being "Private Fears in Public Places" that had a hugely successful Off-Broadway run at 59E59 Theaters, and in 2006 was made into a film "Cœurs", directed by Alain Resnais. After suffering a stroke, there was uncertainly as to whether he could continue to write (the Ayckbourn play premièred immediately after the stroke, "If I Were You", was written before his illness), but his first play written afterwards, "Life and Beth", was premièred in the summer of 2008. Ayckbourn continues to write for the Stephen Joseph Theatre on invitation of his successor as artistic director, Chris Monks, with the first new play under this arrangement, "My Wonderful Day", performed in October 2009. His latest play, "Roundelay" is scheduled to open in September 2014; the order in which each of the five acts is played in each performance is to be left to chance (allowing 120 possible permutations), with members of the audience being invited to extract five coloured ping pong balls from a bag beforehand.
Many of Ayckbourn's plays have had their New York premiere at 59E59 Theaters as part of their annual Brits Off Broadway Festitval including "Private Fears in Public Places", "Intimate Exchanges", "My Wonderful Day" and "Neighbourhood Watch" among others.
Directing.
Although Alan Ayckbourn is best known as a writer, it is said that he only spends 10% of his time writing plays. Most of the rest of his time is spent directing.
Alan Ayckbourn began directing at the Scarborough Library Theatre in 1961, with a production of "Gas Light" by Patrick Hamilton. He directed five other plays that year and the following year in Scarborough, and after transferring to the Victoria Theatre, directed a further six plays in 1963. Between 1964 and 1967 (when much of his time was taken up by various productions of his early successes "Mr. Whatnot" and "Relatively Speaking") he only directed one play ("The Sparrow", written by himself, later withdrawn), but in 1968 he resumed regularly directing plays, mostly at Scarborough. At this time he also worked as a radio drama producer for the BBC, based in Leeds.
At first, his directing career was separate from his writing career. It was not until 1963 that Ayckbourn directed a play of his own (a revival of "Standing Room Only"), 1967 that Ayckbourn directed a première of his own ("The Sparrow"). The London premières remained in the hands of other directors for longer, with the first play of his both written and directed by him in London ("Bedroom Farce") waiting until 1977.
After the death of Stephen Joseph in 1967, the position of Director of Productions was appointed on an annual basis. Alan Ayckbourn was offered this position in 1969 and 1970, succeeding Rodney Wood, but he handed the position over to Caroline Smith in 1971 (having spent most of his time that year in the USA with "How the Other Half Loves"). He became Director of Productions again in 1972, and this time, on 12 November that same year, he was made the permanent artistic director of the theatre.
In mid-1986, Ayckbourn accepted an invitation to work as a visiting director for two years at the Royal National Theatre in London, form his own company, and perform a play in each of the three auditoria provided at least one was a new play of his own. Using a stock company that included established performers like Michael Gambon, Polly Adams and Simon Cadell. The three plays became four, and were: "Tons of Money" by Will Evans and Valentine, with adaptations by Ayckbourn (Lyttelton), Arthur Miller's "A View From the Bridge" (Cottesloe), his own "A Small Family Business" (Olivier) and John Ford's "'Tis Pity She's a Whore" (Olivier again). During this time, Alan Ayckbourn shared his role of artistic director of the Stephen Joseph Theatre with Robin Herford and returned in 1987 to direct the première of "Henceforward...".
He announced in 1999 that he would step back from directing the work of other playwrights, in order to concentrate on his own plays, the last one being Rob Shearman's "Knights in Plastic Armour" in 1999; the exception being in 2002 when he directed the world première of Tim Firth's "The Safari Party".
In 2002, following a dispute over the Duchess Theatre's handling of "Damsels in Distress", Ayckbourn sharply criticised both this and the West End's treatment of theatre in general, in particular their casting of celebrities. Although he did not explicitly say he would boycott the West End, he did not return to direct in the West End again until 2009 with a revival of "Woman in Mind" (although he did allow other West End producers to revive "Absurd Person Singular" in 2007 and "The Norman Conquests" in 2008).
After Ayckbourn suffered a stroke in February 2006, he returned to work in September and premièred his 70th play "If I Were You" at the Stephen Joseph Theatre the following month.
He announced in June 2007 that he would retire as artistic director of the Stephen Joseph Theatre after the 2008 season. His successor, Chris Monks, took over at the start of the 2009–2010 season, but Ayckbourn remained to direct premières and revivals of his work at the theatre, beginning with "How the Other Half Loves" in June 2009.
In March 2010 he directed an in-the-round revival of his play "Taking Steps" at the Orange Tree Theatre, winning universal press acclaim.
In July 2014, Ayckbourn directed a musical adaptation of "The Boy Who Fell Into A Book", with musical adaptation and lyrics by Paul James and music by Eric Angus and Cathy Shostak. The show ran in The Stephen Joseph Theatre and received critical acclaim.
Ayckbourn also sits on the Council of the Society of Authors.
Works.
One-act plays.
There are seven one-act plays written by Alan Ayckbourn. Five of them ("Mother Figure", "Drinking Companion", "Between Mouthfuls", "Gosforth’s Fete" and "Widows Might") were written for "Confusions", first performed in 1974.
The other two one-act plays were:
Film adaptations of Ayckbourn plays.
Plays adapted as films include:

</doc>
<doc id="1979" url="https://en.wikipedia.org/wiki?curid=1979" title="Alpha Centauri">
Alpha Centauri

Alpha Centauri (α Cen), also known as Rigil Kent ()(the "Centaur's Foot") or Toliman, is the closest star system to the Solar System at . It consists of three stars: the pair Alpha Centauri A and Alpha Centauri B and a small and faint red dwarf, Alpha Centauri C, better known as Proxima Centauri, that may be gravitationally bound to the other two. (Beta Centauri, or β Centauri, should not be confused with Alpha Centauri B, and is a separate, trinary, system of its own.) To the unaided eye, the two main components appear as a single object of an apparent visual magnitude of −0.27, forming the brightest star in the southern constellation Centaurus and the third-brightest star in the night sky, only outshone by Sirius and Canopus.
Alpha Centauri A (α Cen A) has 110% of the mass and 151.9% the luminosity of the Sun, and Alpha Centauri B (α Cen B) is smaller and cooler, at 90.7% of the Sun's mass and 44.5% of its visual luminosity. During the pair's 79.91-year orbit about a common center, the distance between them varies from about that between Pluto and the Sun to that between Saturn and the Sun. Proxima is at the slightly smaller distance of 1.29 parsecs or 4.24 light years from the Sun, making it the closest star to the Sun, even though it is not visible to the naked eye. The separation of Proxima from Alpha Centauri AB is about 0.06 parsecs, 0.2 light years or 15,000 astronomical units (AU), equivalent to 500 times the size of Neptune's orbit.
Nature and components.
Alpha Centauri is the name given to what appears as a single star to the naked eye and the brightest star in the southern constellation of Centaurus. At −0.27 apparent visual magnitude (calculated from A and B magnitudes), it is fainter only than Sirius and Canopus. The next-brightest star in the night sky is Arcturus. Alpha Centauri is a multiple-star system, with its two main stars being Alpha Centauri A and Alpha Centauri B , usually defined to identify them as the different components of the binary . A third companion—Proxima Centauri (or Proxima or )—has a distance much greater than the observed separation between stars A and B and is probably gravitationally associated with the AB system. As viewed from Earth, it is located at an angular separation of 2.2° from the two main stars. If it were bright enough to be seen without a telescope, Proxima Centauri would appear to the naked eye as a star separate from . Alpha Centauri AB and Proxima Centauri form a "visual" double star. Direct evidence that Proxima Centauri has an elliptical orbit typical of binary stars has yet to be found. Together all three components make a triple star system, referred to by double-star observers as the triple star (or multiple star), .
Alpha Centauri A is the principal member, or "primary", of the binary system, being slightly larger and more luminous than the Sun. It is a solar-like main-sequence star with a similar yellowish color, whose stellar classification is spectral type G2 V. From the determined mutual orbital parameters, Alpha Centauri A is about 10% more massive than the Sun, with a radius about 23% larger. The projected rotational velocity of this star is , resulting in an estimated rotational period of 22 days, which gives it a slightly faster rotational period than the Sun's 25 days. When considered among the individual brightest stars in the sky (excluding the Sun), Alpha Centauri A is the fourth brightest at an apparent visual magnitude of +0.01, being fractionally fainter than Arcturus at an apparent visual magnitude of −0.04.
Alpha Centauri B is the companion star, or "secondary", of the binary system, and is slightly smaller and less luminous than the Sun. It is a main-sequence star of spectral type K1 V, making it more an orange color than the primary star. Alpha Centauri B is about 90% the mass of the Sun and 14% smaller in radius. The projected rotational velocity is , resulting in an estimated rotational period of 41 days. (An earlier, 1995 estimate gave a similar rotation period of 36.8 days.) Although it has a lower luminosity than component A, star B emits more energy in the X-ray band. The light curve of B varies on a short time scale and there has been at least one observed flare. Alpha Centauri B at an apparent visual magnitude of 1.33 would be twenty-first in brightness if it could be seen independently of Alpha Centauri A.
Alpha Centauri C, also known as Proxima Centauri, is of spectral class M6 Ve, a small main-sequence star (Type V) with emission lines. Its B−V color index is +1.82 and its mass is about 0.123 solar masses (), or 129 Jupiter masses.
Together, the bright visible components of the binary star system are called Alpha Centauri AB . This "AB" designation denotes the apparent gravitational centre of the main binary system relative to other companion star(s) in any multiple star system. "AB-C" refers to the orbit of Proxima around the central binary, being the distance between the centre of gravity and the outlying companion. Some older references use the confusing and now discontinued designation of A×B. Because the distance between the Sun and Alpha Centauri AB does not differ significantly from either star, gravitationally this binary system is considered as if it were one object.
Asteroseismic studies, chromospheric activity, and stellar rotation (gyrochronology), are all consistent with the α Cen system being similar in age to, or slightly older than, the Sun, with typical ages quoted between 4.5 and 7 billion years (Gyr). Asteroseismic analyses that incorporate the tight observational constraints on the stellar parameters for α Cen A and/or B have yielded age estimates of 4.85 ± 0.5 Gyr, 5.0 ± 0.5 Gyr, 5.2–7.1 Gyr, 6.4 Gyr, and 6.52 ± 0.3 Gyr. Age estimates for stars A and B based on chromospheric activity (Calcium H & K emission) yield 4.4–6.5 Gyr, whereas gyrochronology yields 5.0 ± 0.3 Gyr.
Observation.
The two Alpha Centauri AB binary stars are too close together to be resolved by the naked eye, because the angular separation varies between 2 and 22 arcsec, but through much of the orbit, both are easily resolved in binoculars or small telescopes.
In the southern hemisphere, Alpha Centauri forms the outer star of "The Pointers" or "The Southern Pointers", so called because the line through Beta Centauri (Hadar/Agena),
some 4.5° west, points directly to the constellation Crux — the Southern Cross. The Pointers easily distinguish the true Southern Cross from the fainter asterism known as the False Cross.
South of about 29° S latitude, Alpha Centauri is circumpolar and never sets below the horizon. Both stars, including Crux, are too far south to be visible for mid-latitude northern observers. Below about 29° N latitude to the equator (roughly Hermosillo, Chihuahua City in Mexico, Galveston, Texas, Ocala, Florida and Lanzarote, the Canary Islands of Spain) during the northern summer, Alpha Centauri lies close to the southern horizon. The star culminates each year at midnight on 24 April or 9 p.m. on 8 June.
As seen from Earth, Proxima Centauri lies 2.2° southwest from Alpha Centauri AB. This is about four times the angular diameter of the Full Moon, and almost exactly half the distance between Alpha Centauri AB and Beta Centauri. Proxima usually appears as a deep-red star of an apparent visual magnitude of 13.1 in a sparsely populated star field, requiring moderately sized telescopes to see. Listed as V645 Cen in the "General Catalogue of Variable Stars (G.C.V.S.) Version 4.2", this UV Ceti-type flare star can unexpectedly brighten rapidly by as much as 0.6 magnitudes at visual wavelengths, then fade after only a few minutes. Some amateur and professional astronomers regularly monitor for outbursts using either optical or radio telescopes.
Observational history.
English explorer Robert Hues brought Alpha Centauri to the attention of European observers in his 1592 work "Tractatus de Globis", along with Canopus and Achernar, noting "Now, therefore, there are but three Stars of the first magnitude that I could perceive in all those parts which are never seene here in England. The first of these is that bright Star in the sterne of Argo which they call Canobus. The second is in the end of Eridanus. The third lpha Centaur is in the right foote of the Centaure."
The binary nature of Alpha Centauri AB was first recognized in December 1689 by astronomer and Jesuit priest Jean Richaud. The finding was made incidentally while observing a passing comet from his station in Puducherry. Alpha Centauri was only the second binary star system to be discovered, preceded only by Alpha Crucis.
By 1752, French astronomer Abbé Nicolas Louis de Lacaille made astrometric positional measurements using state-of-the-art instruments of that time. Its large proper motion was discovered by Manuel John Johnson, observing from Saint Helena, who informed Thomas Henderson at the Royal Observatory, Cape of Good Hope of it. The parallax of Alpha Centauri was subsequently determined by Henderson from many exacting positional observations of the AB system between April 1832 and May 1833. However, he withheld his results because he suspected they were too large to be true, but eventually published them in 1839 after Friedrich Wilhelm Bessel released his own accurately determined parallax for 61 Cygni in 1838. For this reason, Alpha Centauri is sometimes considered as the second star to have its distance measured because Henderson's work was not fully recognized at first. (The distance of Alpha is now reckoned at 4.396 light-years, or about 41.6 trillion kilometres).
Later, John Herschel made the first micrometrical observations in 1834. Since the early 20th century, measures have been made with photographic plates.
By 1926, South African astronomer William Stephen Finsen calculated the approximate orbit elements close to those now accepted for this system. All future positions are now sufficiently accurate for visual observers to determine the relative places of the stars from a binary star ephemeris. Others, like the Belgian astronomer D. Pourbaix (2002), have regularly refined the precision of any new published orbital elements.
Alpha Centauri is inside the G-cloud, and the nearest known system to it is Luhman 16 at 3.6 light years.
Scottish astronomer Robert Innes discovered Proxima Centauri in 1915 by blinking photographic plates taken at different times during a dedicated proper motion survey. This showed the large proper motion and parallax of the star was similar in both size and direction to those of Alpha Centauri AB, suggesting immediately it was part of the system and slightly closer to us than Alpha Centauri AB. Lying 4.24 light-years away, Proxima Centauri is the nearest star to the Sun. All current derived distances for the three stars are from the parallaxes obtained from the Hipparcos star catalog (HIP) and the Hubble Space Telescope.
Binary system.
With the orbital period of 79.91 years, the A and B components of this binary star can approach each other to 11.2 astronomical units, equivalent to 1.67 billion km or about the mean distance between the Sun and Saturn, or may recede as far as 35.6 AU (5.3 billion km—approximately the distance from the Sun to Pluto). This is a consequence of the binary's moderate orbital eccentricity "e" = 0.5179. From the orbital elements, the total mass of both stars is about —or twice that of the Sun. The average individual stellar masses are and , respectively, though slightly higher masses have been quoted in recent years, such as and , or totalling . Alpha Centauri A and B have absolute magnitudes of +4.38 and +5.71, respectively. Stellar evolution theory implies both stars are slightly older than the Sun at 5 to 6 billion years, as derived by both mass and their spectral characteristics.
Viewed from Earth, the "apparent orbit" of this binary star means that its separation and position angle (P.A.) are in continuous change throughout its projected orbit. Observed stellar positions in 2010 are separated by 6.74 arcsec through the P.A. of 245.7°, reducing to 6.04 arcsec through 251.8° in 2011. The closest approach in the future will be in February 2016, at 4.0 arcsec through 300°. The observed maximum separation of these stars is about 22 arcsec, while the minimum distance is 1.7 arcsec. The widest separation occurred during February 1976 and the next will be in January 2056.
In the "true orbit", closest approach or periastron was in August 1955, and next in May 2035. Furthest orbital separation at apastron last occurred in May 1995 and the next will be in 2075. The apparent distance between the two stars is rapidly decreasing, at least until 2019.
Proxima Centauri.
The much fainter red dwarf Proxima Centauri, or simply Proxima, is about 15,000 AU away from Alpha Centauri AB. This is equivalent to 0.24 light years or 2.2 trillion kilometres—about 5% the distance between Alpha Centauri AB and the Sun. Proxima is likely gravitationally bound to Alpha Centauri AB, orbiting it with a period between 100,000 and 500,000 years. However, it is also possible that Proxima is not gravitationally bound and thus moving along a hyperbolic trajectory with respect to Alpha Centauri AB.
The main evidence for a bound orbit is that Proxima's association with Alpha Centauri AB is unlikely to be coincidental, because they share approximately the same motion through space. Theoretically, Proxima could leave the system after several million years. It is not yet certain whether Proxima and Alpha Centauri are truly gravitationally bound.
Proxima is a red dwarf of spectral class M6 Ve with an absolute magnitude of +15.60, which is only a small fraction of the Sun's luminosity. By mass, Proxima is calculated as (rounded to ) or about one-eighth that of the Sun.
Kinematics.
All components of Alpha Centauri display significant proper motions against the background sky, similar to the first-magnitude stars Sirius and Arcturus. Over the centuries, this causes the apparent stellar positions to slowly change. Such motions define the "high-proper-motion stars". These stellar motions were unknown to ancient astronomers. Most assumed that all stars were immortal and permanently fixed on the celestial sphere, as stated in the works of the philosopher Aristotle.
Edmond Halley in 1718 found that some stars had significantly moved from their ancient astrometric positions. For example, the bright star Arcturus (α Boo) in the constellation of Boötes showed an almost 0.5° difference in 1800 years, as did the brightest star, Sirius, in Canis Major (α CMa). Halley's positional comparison was Ptolemy's catalogue of stars contained in the Almagest whose original data included portions from an earlier catalog by Hipparchos during the . Halley's proper motions were mostly for northern stars, so the southern star Alpha Centauri was not determined until the early 19th century.
Scottish-born observer Thomas James Henderson in the 1830s at the Royal Observatory at the Cape of Good Hope discovered the true distance to Alpha Centauri. He soon realised this system displayed an unusually high proper motion, and therefore its observed true velocity through space should be much larger. In this case, the apparent stellar motion was found using Abbé Nicolas Louis de Lacaille's astrometric observations of 1751–1752, by the observed differences between the two measured positions in different epochs. Using the Hipparcos Star Catalogue (HIP) data, the mean individual proper motions are −3678 mas/yr or −3.678 arcsec per year in right ascension and +481.84 mas/yr or 0.48184 arcsec per year in declination. As proper motions are cumulative, the motion of Alpha Centauri is about 6.1 arcmin each century, and 61.3 arcmin or 1.02° each millennium. These motions are about one-fifth and twice, respectively, the diameter of the full moon. Using spectroscopy the mean radial velocity has been determined to be around 20 km/s towards the Solar System.
As the stars of Alpha Centauri approach the Solar System, the measured proper motion and trigonometric parallax slowly increase. Changes are also observed in the size of the semi-major axis of the orbital ellipse, increasing by 0.03 arcsec per century. This change slightly shortens the observed orbital period of by some 0.006 years per century. This small effect is gradually decreasing until the star system is at its closest to us, and is then reversed as the distance increases again. Consequently, the observed position angles of the stars are subject to changes in the orbital elements over time, as first determined by W. H. van den Bos in 1926. Some slight differences of about 0.5% in the measured proper motions are caused by Alpha Centauri AB's orbital motion.
Based on these observed proper motions and radial velocities, Alpha Centauri will continue to gradually brighten, passing just north of the Southern Cross or Crux, before moving northwest and up towards the celestial equator and away from the galactic plane. By about 29,700 AD, in the present-day constellation of Hydra, Alpha Centauri will be 1.00 pc or 3.26 ly away. Then it will reach the stationary radial velocity (RVel) of 0.0 km/s and the maximum apparent magnitude of −0.86v (which is comparable to present-day magnitude of Canopus). However, even during the time of this nearest approach, the apparent magnitude of Alpha Centauri will still not surpass that of Sirius (which will brighten incrementally over the next 60,000 years, and will continue to be the brightest star as seen from Earth for the next 210,000 years).
The Alpha Centauri system will then begin to move away from the Solar System, showing a positive radial velocity. Due to visual perspective, about 100,000 years from now, these stars will reach a final vanishing point and slowly disappear among the countless stars of the Milky Way. Here this once bright yellow star will fall below naked-eye visibility somewhere in the faint present day southern constellation of Telescopium (this unusual location results from the fact that Alpha Centauri's orbit around the galactic centre is highly tilted with respect to the plane of the Milky Way).
In about 4000 years, the proper motion of Alpha Centauri will mean that from the point of view of Earth it will appear close enough to Beta Centauri to form an optical double star. Beta Centauri is in reality far more distant than Alpha Centauri.
Planets.
Until the 1990s, technologies did not exist that could detect planets outside the Solar System. Since then, exoplanet-detection capabilities have steadily improved to the point where Earth-mass planets can be detected.
Alpha Centauri Bb.
In 2012 a planet around Alpha Centauri B was announced, but in 2015 a new analysis concluded that it almost certainly does not exist and it is just a spurious artifact of the data analysis.
Alpha Centauri Bc.
On 25 March 2015, a scientific paper by Demory and colleagues published transit results for Alpha Centauri B using the Hubble Space Telescope for a total of 40 hours. They evidenced a transit event possibly corresponding to a planetary body. This planet would most likely orbit Alpha Centauri B with an orbital period of 20.4 days or less, with only a 5% chance of it having a longer orbit. The median average of the likely orbits is 12.4 days with an impact parameter of around 0–0.3. Its orbit would likely have an eccentricity of 0.24 or less. If confirmed, this planet would be called Alpha Centauri Bc. This planet would also still be far too close to its parent star to harbour life.
Possibility of additional planets.
The discovery of planets orbiting other star systems, including similar binary systems (Gamma Cephei), raises the possibility that additional planets may exist in the Alpha Centauri system. Such planets could orbit Alpha Centauri A or Alpha Centauri B individually, or be on large orbits around the binary Alpha Centauri AB. Because both the principal stars are fairly similar to the Sun (for example, in age and metallicity), astronomers have been especially interested in making detailed searches for planets in the Alpha Centauri system. Several established planet-hunting teams have used various radial velocity or star transit methods in their searches around these two bright stars. All the observational studies have so far failed to find any evidence for brown dwarfs or gas giants.
In 2009, computer simulations (then unaware of the close-in planet Bb) showed that a planet might have been able to form near the inner edge of Alpha Centauri B's habitable zone, which extends from 0.5 to 0.9 AU from the star. Certain special assumptions, such as considering that Alpha Centauri A and B may have initially formed with a wider separation and later moved closer to each other (as might be possible if they formed in a dense star cluster) would permit an accretion-friendly environment farther from the star. Bodies around A would be able to orbit at slightly farther distances due to A's stronger gravity. In addition, the lack of any brown dwarfs or gas giants in close orbits around A or B make the likelihood of terrestrial planets greater than otherwise. Theoretical studies on the detectability via radial velocity analysis have shown that a dedicated campaign of high-cadence observations with a 1-meter class telescope can reliably detect a hypothetical planet of in the habitable zone of B within three years.
Radial velocity measurements of Alpha Centauri B with High Accuracy Radial Velocity Planet Searcher spectrograph ruled out planets of more than to the distance of the habitable zone of the star (orbital period P = 200 days).
A sub-millimetre source detected in 2014, nominally tagged as "αCen D", may be a substantial companion, or an extreme trans-Neptunian object.
Theoretical planets.
Early computer-generated models of planetary formation predicted the existence of terrestrial planets around both Alpha Centauri A and B, but most recent numerical investigations have shown that the gravitational pull of the companion star renders the accretion of planets very difficult. Despite these difficulties, given the similarities to the Sun in spectral types, star type, age and probable stability of the orbits, it has been suggested that this stellar system could hold one of the best possibilities for harbouring extraterrestrial life on a potential planet.
In the Solar System both Jupiter and Saturn were probably crucial in perturbing comets into the inner Solar System. Here, the comets provided the inner planets with their own source of water and various other ices. In the Alpha Centauri system Proxima Centauri may have influenced the planetary disk as the Alpha Centauri system was forming, enriching the area around Alpha Centauri A and B with volatile materials. This would be discounted if, for example, Alpha Centauri B happened to have gas giants orbiting Alpha Centauri A (or conversely, Alpha Centauri A for Alpha Centauri B), or if the stars B and A themselves were able to perturb comets into each other's inner system as Jupiter and Saturn presumably have done in the Solar System. Such icy bodies probably also reside in Oort clouds of other planetary systems, when they are influenced gravitationally by either the gas giants or disruptions by passing nearby stars many of these icy bodies then travel starwards. There is no direct evidence yet of the existence of such an Oort cloud around Alpha Centauri AB, and theoretically this may have been totally destroyed during the system's formation.
To be in the star's habitable zone, any suspected planet around Alpha Centauri A would have to be placed about 1.25 AU away  – about halfway between the distances of Earth's orbit and Mars's orbit in the Solar System – so as to have similar planetary temperatures and conditions for liquid water to exist. For the slightly less luminous and cooler Alpha Centauri B, the habitable zone would lie closer at about 0.7 AU , approximately the distance that Venus is from the Sun.
With the goal of finding evidence of such planets, both Proxima Centauri and Alpha Centauri AB were among the listed "Tier 1" target stars for NASA's Space Interferometry Mission (SIM). Detecting planets as small as three Earth-masses or smaller within two astronomical units of a "Tier 1" target would have been possible with this new instrument. The SIM mission, however, was cancelled due to financial issues in 2010.
View from this system.
Viewed from near the Alpha Centauri system, the sky would appear very much as it does for an observer on Earth, except that Centaurus would be missing its brightest star. The Sun would be a yellow star of an apparent visual magnitude of +0.5 in eastern Cassiopeia, at the antipodal point of Alpha Centauri's current right ascension and declination, at (2000). This place is close to the 3.4-magnitude star ε Cassiopeiae. An interstellar or alien observer would find the \/\/ of Cassiopeia had become a /\/\/ shape nearly in front of the Heart Nebula in Cassiopeia. Sirius lies less than a degree from Betelgeuse in the otherwise unmodified Orion and is with −1.2 a little fainter than from Earth but still the brightest star in the Alpha Centauri sky. Procyon is also displaced into the middle of Gemini, outshining Pollux, whereas both Vega and Altair are shifted northwestward relative to Deneb (which barely moves, due to its great distance), giving the Summer Triangle a more equilateral appearance.
From Proxima itself, Alpha Centauri AB would appear like two close bright stars with the combined apparent magnitude of −6.8. Depending on the binary's orbital position, the bright stars would appear noticeably divisible to the naked eye, or occasionally, but briefly, as single unresolved star. Based on the calculated absolute magnitudes, the visual apparent magnitudes of Alpha Centauri A and B would be −6.5 and −5.2, respectively.
View from a hypothetical planet.
An observer on a hypothetical planet orbiting around either Alpha Centauri A or Alpha Centauri B would see the other star of the binary system as an intensely bright object in the night sky, showing a small but discernible disk while near periapse: A up to 210 arc seconds, B up to 155 arc seconds. Near apoapse, the disc would shrink to 60 arc seconds for A, 43 arc seconds for B, being too small to resolve by naked eye. In any case, the dazzling surface brightness could make the discs harder to resolve than a similarly sized less bright object.
For example, some theoretical planet orbiting about 1.25 AU from Alpha Centauri A (so that the star appears roughly as bright as the Sun viewed from the Earth) would see Alpha Centauri B orbit the entire sky once roughly every one year and three months (or 1.3(4) a), the planet's own orbital period. Added to this would be the changing apparent position of Alpha Centauri B during its long eighty-year elliptical orbit with respect to Alpha Centauri A (The average speed, at 4,5 degrees per Earth year, is comparable in speed to Uranus here. With the eccentricity of the orbit, the maximum speed near periapse, about 18 degrees per Earth year, is faster than Saturn, but slower than Jupiter. The minimum speed near apoapse, about 1,8 degrees per Earth year, is slower than Neptune.). Depending on its and planet´s position on their respective orbits, Alpha Centauri B would vary in apparent magnitude between −18.2 (dimmest) and −21.0 (brightest). These visual apparent magnitudes are much dimmer than the apparent magnitude of the Sun as viewed from the Earth (−26.7). The difference of 5.7 to 8.6 magnitudes means Alpha Centauri B would appear, on a linear scale, 2500 to 190 times dimmer than Alpha Centauri A (or the Sun viewed from the Earth), but also 190 to 2500 times brighter than the full Moon as seen from the Earth (−12.5).
Also, if another similar planet orbited at 0.71 AU from Alpha Centauri B (so that in turn Alpha Centauri B appeared as bright as the Sun seen from the Earth), this hypothetical planet would receive slightly more light from the more luminous Alpha Centauri A, which would shine 4.7 to 7.3 magnitudes dimmer than Alpha Centauri B (or the Sun seen from the Earth), ranging in apparent magnitude between −19.4 (dimmest) and −22.1 (brightest). Thus Alpha Centauri A would appear between 830 and 70 times dimmer than the Sun but some 580 to 6900 times brighter than the full Moon. During such planet's orbital period of 0.6(3) a, an observer on the planet would see this intensely bright companion star circle the sky just as we see with the Solar System's planets. Furthermore, Alpha Centauri A's sidereal period of approximately eighty years means that this star would move through the local ecliptic as slowly as Uranus with its eighty-four year period, but as the orbit of Alpha Centauri A is more elliptical, its apparent magnitude will be far more variable. Although intensely bright to the eye, the overall illumination would not significantly affect climate nor influence normal plant photosynthesis.
An observer on the hypothetical planet would notice a change in orientation to VLBI reference points commensurate with the binary orbit periodicity plus or minus any local effects such as precession or nutation.
Assuming this hypothetical planet had a low orbital inclination with respect to the mutual orbit of Alpha Centauri A and B, then the secondary star would start beside the primary at 'stellar' conjunction. Half the period later, at 'stellar' opposition, both stars would be opposite each other in the sky. Then, for about half the planetary year the appearance of the night sky would be a darker blue – similar to the sky during totality at any total solar eclipse. Humans could easily walk around and clearly see the surrounding terrain, and reading a book would be quite possible without any artificial light. After another half period in the stellar orbit, the stars would complete their orbital cycle and return to the next stellar conjunction, and the familiar day and night cycle would return.
Traditional names.
The colloquial name of Alpha Centauri is "Rigel Kent" or "Rigil Kent", short for "Rigil/Rigel Kentaurus", the romanization of the Arabic name رجل القنطورس "Rijl Qanṭūris", from the phrase "Rijl al-Qanṭūris" "the foot of the Centaur". This is sometimes further abbreviated to "Rigel", though that is ambiguous with Beta Orionis, which is also called Rigel. Although the short form "Rigel Kent" is common in English, the stars are most often referred to by their Bayer designation "Alpha Centauri."
A medieval name is "Toliman", whose etymology may be Arabic الظلمان "al-Ẓulmān" "the ostriches". During the 19th century, the northern amateur popularist Elijah H. Burritt used the now-obscure name Bungula, possibly coined from "β" and the Latin "ungula" ("hoof"). Together, Alpha and Beta Centauri form the "Southern Pointers" or "The Pointers", as they point towards the Southern Cross, the asterism of the constellation of Crux.
In Mandarin, "Nán Mén", meaning "Southern Gate", refers to an asterism consisting of α Centauri and ε Centauri. Consequently, α Centauri itself is known as "Nán Mén Èr", the Second Star of the Southern Gate.
To the Australian aboriginal Boorong people of northwestern Victoria, Alpha and Beta Centauri are "Bermbermgle", two brothers noted for their courage and destructiveness, who speared and killed "Tchingal" "The Emu" (the Coalsack Nebula). The form in Wotjobaluk is "Bram-bram-bult".
Space travel.
Alpha Centauri is envisioned as a likely first target for manned or unmanned interstellar exploration. Crossing the huge distance between the Sun and Alpha Centauri using current spacecraft technologies would take several millennia, though the possibility of solar sail or nuclear pulse propulsion technology could cut this down to a matter of decades.

</doc>
<doc id="1980" url="https://en.wikipedia.org/wiki?curid=1980" title="Amiga">
Amiga

The Amiga is a family of personal computers sold by Commodore in the 1980s and 1990s. Based on the Motorola 68000 family of microprocessors, the machine has a custom chipset with graphics and sound capabilities that were unprecedented for the price, and a pre-emptive multitasking operating system called AmigaOS. The Amiga provided a significant upgrade from earlier 8-bit home computers, including Commodore's own C64.
The Amiga 1000 was officially released in July 1985, but a series of production problems meant it did not become widely available until early 1986. The best selling model, the Amiga 500, was introduced in 1987 and became one of the leading home computers of the late 1980s and early 1990s with four to six million sold. The A3000, introduced in 1990, started the second generation of Amiga systems, followed by the A500+ and the A600. Finally, as the third generation, the A1200 and the A4000 were released in 1992. The platform became particularly popular for gaming and programming demos. It also found a prominent role in the desktop video, video production, and show control business, leading to affordable video editing systems such as the Video Toaster. The Amiga's native ability to simultaneously play back multiple digital sound samples made it a popular platform for early "tracker" music software. The relatively powerful processor and ability to access several megabytes of memory led to the development of several 3D rendering packages, including LightWave 3D and Aladdin 4D.
Although early Commodore advertisements attempt to cast the computer as an all-purpose business machine, especially when outfitted with the Amiga Sidecar PC compatibility addon, the Amiga was most commercially successful as a home computer, with a wide range of games and creative software. It was also a less expensive alternative to the Apple Macintosh and IBM PC as a general-purpose business or home computer. Initially, the Amiga was developed alongside various Commodore PC clones, but Commodore later left the PC market. Poor marketing and the failure of the later models to repeat the technological advances of the first systems meant that the Amiga quickly lost its market share to competing platforms, such as the fourth generation game consoles, Apple Macintosh, and later IBM PC compatibles. Commodore ultimately went bankrupt in April 1994 after the "make or break" Amiga CD32 model failed in the marketplace.
Since the demise of Commodore, various groups have marketed successors to the original Amiga line, including Genesi, Eyetech, ACube Systems and A-EON Technology. Likewise, AmigaOS has influenced replacements, clones and compatible systems such as MorphOS, AmigaOS 4 and AROS. The demise of Commodore has been commonly attributed to numerous factors such as poor marketing, a lack of sufficient third party developers, and a failure to compete with cheaper PC clones with "multimedia" features and low-cost color-capable Macintosh models such as the Macintosh LC.
History.
Concept and early development.
Jay Miner joined Atari in the 1970s to develop custom integrated circuits, and led development of the Atari 2600's TIA. Almost as soon as its development was complete, the team began developing a much more sophisticated set of chips, CTIA, ANTIC and POKEY, that formed the basis of the Atari 8-bit family.
With the 8-bit line's launch in 1979, Miner again started looking at a next generation chipset. Nolan Bushnell had sold the company to Warner Communications in 1978, and the new management was much more interested in the existing lines than development of new products that might cut into their sales. Miner wanted to start work with the new Motorola 68000, but management was only interested in another MOS 6502 based system. Miner left the company, and the industry.
Shortly thereafter, in 1982, Larry Kaplan was approached by a number of investors who wanted to develop a new game platform. Kaplan hired Miner to run the hardware side of the newly formed company, "Hi-Toro". The system was code-named "Lorraine" in keeping with Miner's policy of giving systems female names, in this case the company president's wife, Lorraine Morse. When Kaplan left the company late in 1982 to rejoin Atari, Miner was promoted to head engineer and the company relaunched as Amiga Corporation.
A breadboard prototype was largely completed by late 1983, and shown at the January 1984 Consumer Electronics Show (CES) with the famous Boing Ball demo. A further developed version was demonstrated at the June 1984 CES and shown to many companies in hopes of garnering further funding, but found little interest in a market that was in the final stages of the North American video game crash of 1983.
Atari was also in turmoil as a result of the crash, and Warner sold the computer division to Jack Tramiel in June 1984. Amiga and Atari had been in talks about a licensing deal since March, but these were going nowhere.
Commodore launch.
Tramiel's move to Atari resulted in a considerable number of Commodore employees moving as well, including a number of the senior technical staff. This left Commodore with no workable path to a next generation computer. The company approached Amiga offering to fund development as a home computer system. They quickly arranged to repay the Atari loan, ending that threat. The two companies were initially arranging a $4 million license agreement before Commodore offered $24 million to purchase Amiga outright.
By late 1984 the prototype breadboard chipset had successfully been turned into IC's, and the system hardware was being readied for production. At this time the operating system (OS) was not as ready, and led to a deal to port an OS known as TRIPOS to the platform. TRIPOS was a multitasking system that had been written in BCPL during the 1970s for minicomputer systems like the PDP-11, but later experimentally ported to the 68000. This early version was known as AmigaDOS and the GUI as Workbench. The BCPL parts were later rewritten in the C language, and the entire system became AmigaOS.
The system originally looked like an early pizza box form factor workstation, but a late change was the introduction of vertical supports on either side of the case to provide a "garage" under the main section of the system where the keyboard could be stored.
The first model was announced in 1985 as simply "The Amiga from Commodore", later to be retroactively dubbed the Amiga 1000. They were first offered for sale in August, but by October only 50 had been built, all of which were used by Commodore. Machines only began to arrive in quantity in mid-November, meaning they missed the Christmas buying rush. By the end of the year, they had sold 35,000 machines, and severe cashflow problems made the company pull out of the January 1986 CES. Bad or entirely missing marketing, forcing the development team to move to the east coast, notorious stability problems and other blunders limited sales in early 1986 to 10 to 15,000 units a month.
Commercial success.
In late 1985 Thomas Rattigan was promoted to COO of Commodore, and then to CEO in February 1986. He immediately implemented an ambitious plan that covered almost all of the company's operations. Among these were the long overdue cancelation of the now outdated PET and VIC-20 lines, a variety of poorly selling Commodore 64 offshoots, and the Commodore 900 workstation effort.
Another of the changes was to split the Amiga into two products, a new high-end version of the Amiga aimed at the creative market, and a cost-reduced version that would take over for the Commodore 64 in the low-end market. These new designs were released in 1987 as the Amiga 2000 and Amiga 500, the latter of which went on to widespread success and became their best selling model.
Similar high-end/low-end models would make up the Amiga line for the rest of its history; follow-on designs included the Amiga 3000/Amiga 500+/Amiga 600, and the Amiga 4000/Amiga 1200. These models incorporated a series of technical upgrades known as the ECS and AGA, which added higher resolution displays among many other improvements and simplifications.
Ultimately the Amiga line would sell an estimated 4,850,000 machines over its lifetime. The machines were most popular in the UK and Germany, with about 1.5 million sold in each country, and sales in the high hundreds of thousands in other European nations. The machine was less popular in North America, where an estimated 700,000 were sold.
Bankruptcy.
In spite of his successes in making the company profitable and bringing the Amiga line success, Rattigan was soon forced out of the company in a power struggle with majority shareholder, Irving Gould. This is widely regarded as the turning point in the line's success, as further improvements in the systems were eroded by rapid improvements in other platforms.
In 1994, Commodore filed for bankruptcy and its assets were purchased by Escom, a German PC manufacturer, who created the subsidiary company Amiga Technologies. They re-released the A1200 and A4000T, and introduced a new 68060 version of the A4000T. However, Escom in turn went bankrupt in 1997.
The Amiga brand was then sold to another PC manufacturer, Gateway 2000, which had announced grand plans for it. However, in 2000, Gateway sold the Amiga brand without having released any products. The current owner of the trademark, Amiga, Inc, licensed the rights to sell hardware using the Amiga or AmigaOne brand to Eyetech Group, Hyperion Entertainment and Commodore USA.
Hardware.
At its core, the Amiga has a custom chipset consisting of several coprocessors, which handle audio, video and direct memory access independently of the Central Processing Unit (CPU). This architecture freed up the Amiga's processor for other tasks and gave the Amiga a performance edge over its competitors, particularly in terms of video-intensive applications and games.
The general Amiga architecture uses two distinct bus subsystems, namely, the chipset bus and the CPU bus. The chipset bus allows the custom coprocessors and CPU to address "Chip RAM". The CPU bus provides addressing to other subsystems, such as conventional RAM, ROM and the Zorro II or Zorro III expansion subsystems. This architecture enables independent operation of the subsystems; the CPU "Fast" bus can be much faster than the chipset bus. CPU expansion boards may provide additional custom buses. Additionally, "busboards" or "bridgeboards" may provide ISA or PCI buses.
Central processing unit.
The Motorola 68000 series of microprocessors was used in all Amiga models from Commodore. While the 68000 family has a 32-bit design, the 68000 used in several early models is generally referred to as 16-bit. The 68000 has a 16-bit external data bus so must transfer 32 bits of data in two consecutive steps, a technique called multiplexing: all this is transparent to the software, which was 32-bit from the beginning. The 68000 can address 16 MB of physical memory. Later Amiga models featured full 32-bit CPUs with a larger address space and instruction pipeline facilities.
CPU upgrades were offered by both Commodore and third-party manufacturers. Most Amiga models can be upgraded either by direct CPU replacement or through expansion boards. Such boards often featured faster and higher capacity memory interfaces and hard disk controllers.
Towards the end of Commodore's time in charge of Amiga development there were suggestions that Commodore intended to move away from the 68000 series to higher performance RISC processors, such as the PA-RISC. However, these ideas were never developed before Commodore filed for bankruptcy. Despite this, third-party manufacturers designed upgrades featuring a combination of 68000 series and PowerPC processors along with a PowerPC native microkernel and software. Later Amiga clones featured PowerPC processors only.
Custom chipset.
The custom chipset at the core of the Amiga design appeared in three distinct generations, with a large degree of backward-compatibility. The Original Chip Set (OCS) appeared with the launch of the A1000 in 1985. OCS was eventually followed by the modestly improved Enhanced Chip Set (ECS) in 1990 and finally by the partly 32-bit Advanced Graphics Architecture (AGA) in 1992. Each chipset consists of several coprocessors which handle graphics acceleration, digital audio, direct memory access and communication between various peripherals (e.g., CPU, memory and floppy disks). In addition, some models featured auxiliary custom chips which performed tasks such as SCSI control and display de-interlacing.
Graphics.
All Amiga systems can display full-screen animated graphics with 2, 4, 8, 16, 32, 64 (EHB Mode) or 4096 colors (HAM Mode). Models with the AGA chipset (A1200 and A4000) also have non-EHB 64, 128, 256 and (HAM Mode) color modes and a palette expanded from 4096 to 16.8 million colors.
The Amiga chipset can "genlock", which is the ability to adjust its own screen refresh timing to match an NTSC or PAL video signal. When combined with setting transparency, this allows an Amiga to overlay an external video source with graphics. This ability made the Amiga popular for many applications, and provides the ability to do character generation and CGI effects far more cheaply than earlier systems. This ability has been frequently utilized by wedding videographers, TV stations and their weather forecasting divisions (for weather graphics and radar), advertising channels, music video production, and desktop videographers. The NewTek Video Toaster was made possible by the genlock ability of the Amiga.
In 1988, the release of the Amiga A2024 fixed-frequency monochrome monitor with built-in framebuffer and flicker fixer hardware provided the Amiga with a choice of high-resolution graphic modes (1024×800 for NTSC and 1024×1024 for PAL).
ReTargetable Graphics.
ReTargetable Graphics is an API for device drivers mainly used by 3rd party graphics hardware to interface with AmigaOS via a set of libraries. The software libraries may include software tools to adjust resolution, screen colors, pointers and screenmodes. It uses available hardware and does not extend the capabilities in any way. Amiga intuition.library is limited to display depths of 8-bits but RTG libraries makes is possible to handle higher depths like 24-bits.
Sound.
The sound chip, named Paula, supports four PCM-sample-based sound channels (two for the left speaker and two for the right) with 8-bit resolution for each channel and a 6-bit volume control per channel. The analog output is connected to a low-pass filter, which filters out high-frequency aliases when the Amiga is using a lower sampling rate (see Nyquist limit). The brightness of the Amiga's power LED is used to indicate the status of the Amiga's low-pass filter. The filter is active when the LED is at normal brightness, and deactivated when dimmed (or off on older A500 Amigas). On Amiga 1000 (and first Amiga 500 and Amiga 2000 model), the power LED had no relation to the filter's status, and a wire needed to be manually soldered between pins on the sound chip to disable the filter. Paula can read directly from the system's RAM, using direct memory access (DMA), making sound playback without CPU intervention possible.
Although the hardware is limited to four separate sound channels, software such as "OctaMED" uses software mixing to allow eight or more virtual channels, and it was possible for software to mix two hardware channels to achieve a single 14-bit resolution channel by playing with the volumes of the channels in such a way that one of the source channels contributes the most significant bits and the other the least.
The quality of the Amiga's sound output, and the fact that the hardware is ubiquitous and easily addressed by software, were standout features of Amiga hardware unavailable on PC platforms for years. Third-party sound cards exist that provide DSP functions, multi-track direct-to-disk recording, multiple hardware sound channels and 16-bit and beyond resolutions. A retargetable sound API called AHI was developed allowing these cards to be used transparently by the OS and software.
Kickstart firmware.
Kickstart is the firmware upon which AmigaOS is bootstrapped. Its purpose is to initialize the Amiga hardware and core components of AmigaOS and then attempt to boot from a bootable volume, such as a floppy disk or hard disk drive. Most models (excluding the Amiga 1000) come equipped with Kickstart on an embedded ROM-chip.
Keyboard and mouse.
The keyboard on Amiga computers is similar to that found on a mid 80s IBM PC: Ten function keys, a numeric keypad, and four separate directional arrow keys. Caps Lock and Control share space to the left of A. Missing are the Home, End, Page Up, and Page Down keys: These are accomplished on Amigas by pressing shift and the appropriate arrow key. The Amiga keyboard adds a Help key, which a function key usually acts as on PCs (usually F1). In addition to the Control and Alt modifier keys, the Amiga has 2 'Amiga' keys, rendered as 'Open Amiga' and 'Closed Amiga' similar to the Open/Closed Apple logo keys on Apple II keyboards. The left is used to manipulate the operating system (moving screens and the like) and the right delivered commands to the application. The absence of Num lock frees space for more math symbols around the number pad. Contemporary Macintosh computers, for comparison, lack function keys completely.
The mouse has two buttons like Windows, but unlike Windows pressing and holding the right button replaces the system status line at the top of the screen with a Maclike menu bar. As with Apple's Mac OS prior to Mac OS 8, menu options are selected by releasing the button over that option, not by left clicking.
The mouse plugs into one of two controller ports also used for joysticks, game paddles, and graphics tablets. Although compatible with analog joysticks, Atari 2600-style digital joysticks became standard.
Other peripherals and expansions.
The Amiga was one of the first computers for which inexpensive sound sampling and video digitization accessories were available. As a result of this and the Amiga's audio and video capabilities, the Amiga became a popular system for editing and producing both music and video.
Many expansion boards were produced for Amiga computers to improve the performance and capability of the hardware, such as memory expansions, SCSI controllers, CPU boards, and graphics boards. Other upgrades include genlocks, network cards for Ethernet, modems, sound cards and samplers, video digitizers, extra serial ports, and IDE controllers. Additions after the demise of Commodore company are USB cards. The most popular upgrades were memory, SCSI controllers and CPU accelerator cards. These were sometimes combined into the one device.
Early CPU accelerator cards feature full 32-bit CPUs of the 68000 family such as the Motorola 68020 and Motorola 68030, almost always with 32-bit memory and usually with FPUs and MMUs or the facility to add them. Later designs feature the Motorola 68040 or Motorola 68060. Both CPUs feature integrated FPUs and MMUs. Many CPU accelerator cards also had integrated SCSI controllers.
Phase5 designed the PowerUP boards (Blizzard PPC and CyberStorm PPC) featuring both a 68k (a 68040 or 68060) and a PowerPC (603 or 604) CPU, which are able to run the two CPUs at the same time and share the system memory. The PowerPC CPU on PowerUP boards is usually used as a coprocessor for heavy computations; a powerful CPU is needed to run MAME for example, but even decoding JPEG pictures and MP3 audio was considered heavy computation at the time. It is also possible to ignore the 68k CPU and run Linux on the PPC via project Linux APUS, but a PowerPC-native AmigaOS promised by Amiga Technologies GmbH was not available when the PowerUP boards first appeared.
24-bit graphics cards and video cards were also available. Graphics cards were designed primarily for 2D artwork production, workstation use, and later, gaming. Video cards are designed for inputting and outputting video signals, and processing and manipulating video.
In the North American market, the "NewTek Video Toaster" was a video effects board which turned the Amiga into an affordable video processing computer which found its way into many professional video environments. One particularly well-known use was to create the special effects in early series of "Babylon 5". Due to its NTSC-only design, it did not find a market in countries that used the PAL standard, such as in Europe. In those countries, the "OpalVision" card was popular, although less featured and supported than the Video Toaster. Low-cost timebase correctors (TBC) specifically designed to work with the Toaster quickly came to market, most of which were designed as standard Amiga bus cards.
Various manufacturers started producing PCI busboards for the A1200, A3000 and A4000, allowing standard Amiga computers to use PCI cards such as graphics cards, Sound Blaster sound cards, 10/100 Ethernet cards, USB cards, and television tuner cards. Other manufacturers produced hybrid boards which contained an Intel x86 series chip, allowing the Amiga to emulate a PC.
PowerPC upgrades with Wide SCSI controllers, PCI busboards with Ethernet, sound and 3D graphics cards, and tower cases allowed the A1200 and A4000 to survive well into the late nineties.
Expansion boards were made by Richmond Sound Design that allow their show control and sound design software to communicate with their custom hardware frames either by either ribbon cable or fiber optic cable for long distances, allowing the Amiga to control up to eight million digitally controlled external audio, lighting, automation, relay and voltage control channels spread around a large theme park, for example. See Amiga software for more information on these applications.
Other popular devices included the following:
Serial ports.
The Commodore A2232 board provides serial ports in addition to the Amiga's built-in serial port. Each port can be driven independently at speeds of There is however a driver available on Aminet that allows two of the serial ports to be driven at The serial card used the 65CE02 CPU clocked at . This CPU was also part of the CSG 4510 CPU core that was used in the Commodore 65 computer.
Networking.
Amiga has three networking interface APIs:
Different network media were used:
Models and variants.
The original Amiga models were produced from 1985 to 1996. They are, in order of production: 1000, 2000, 500, 1500, 2500, 3000, 3000UX, 3000T, CDTV, 500+, 600, 4000, 1200, CD32, and 4000T. The PowerPC based AmigaOne computers were later marketed since 2002. Several companies and private persons have also released Amiga clones and still do so today.
Commodore Amiga.
The first Amiga model, the Amiga 1000, was launched in 1985 and became popular for its impressive graphics, video and audio capabilities. In 2006, PC World rated the Amiga 1000 as the seventh greatest PC of all time, stating "Years ahead of its time, the Amiga was the world's first multimedia, multitasking personal computer".
Following the A1000, Commodore updated the desktop line of Amiga computers with the Amiga 2000 in 1987, the Amiga 3000 in 1990, and the Amiga 4000 in 1992, each offering improved capabilities and expansion options. However, the best selling models were the budget models, particularly the highly successful Amiga 500 (1987) and the Amiga 1200 (1992). The Amiga 500+ (1991) was the shortest lived model, replacing the Amiga 500 and lasting only six months until it was phased out and replaced with the Amiga 600 (1992), which in turn was also quickly replaced by the Amiga 1200.
The CDTV, launched in 1991, was a CD-ROM based all-in-one multimedia system. It was an early attempt at a multi-purpose multimedia appliance in an era before multimedia consoles and CD-ROM drives were common. Unfortunately for Commodore, the system never achieved any real commercial success. Like the Commodore 64GS that was a video game console based on a computer, the CDTV was designed as a video game console and multimedia platform. It had existed before the Sony PlayStation and Sega Saturn, but had influenced them. It competed with the Turbo-Grafx CD and Sega CD system add ons when it was being sold.
Commodore's last Amiga offering before filing for bankruptcy was an attempt to capture a portion of the highly competitive 1990s console market with the Amiga CD32 (1993), a 32-bit CD-ROM games console. Though discontinued after Commodore's demise it met with moderate commercial success in Europe. The CD32 was a next generation CDTV, and it was designed to save Commodore by entering the growing video game console market.
Following purchase of Commodore's assets by Escom in 1995, the A1200 and A4000T continued to be sold in small quantities until 1996, though the ground lost since the initial launch and the prohibitive expense of these units meant that the Amiga line never regained any real popularity.
Several Amiga models contained references to songs by the rock band The B-52s. Early A500 units, at least, had the words "B52/ROCK LOBSTER" silk-screen printed onto their printed circuit board, a reference to the popular song "Rock Lobster" The Amiga 600 referenced "JUNE BUG" (after the song "Junebug") and the Amiga 1200 had "CHANNEL Z" (after "Channel Z")., and the CD-32 had "Spellbound".
Most original casing was made from ABS plastics which may become brown with time. This can be reversed by using the public domain chemical mix "Retr0bright".
AmigaOS 4 systems.
AmigaOS 4 is designed for PowerPC Amiga systems. It is mainly based on AmigaOS 3.1 source code, with some parts of version 3.9. Currently runs on both Amigas equipped with CyberstormPPC or BlizzardPPC accelerator boards, on the Teron series based AmigaOne computers built by Eyetech under license by Amiga Inc, on the Pegasos II from Genesi/bPlan GmbH, on the Acube Systems Sam440ep / Sam460ex / AmigaOne 500 systems and on the A-EON AmigaOne X1000.
AmigaOS 4.0 had been available only in developer pre-releases for numerous years until it was officially released in December 2006. Due to the nature of some provisions of the contract between Amiga Inc. and Hyperion Entertainment (the Belgian company which is developing the OS), the commercial AmigaOS 4 had been available only to licensed buyers of AmigaOne motherboards.
AmigaOS 4.0 for Amigas equipped with PowerUP accelerator boards was released in November 2007. Version 4.1 was released in August 2008 for AmigaOne systems, and in May 2011 for Amigas equipped with PowerUP accelerator boards. The most recent release of AmigaOS for all supported platforms is 4.1 update 5. Starting with release 4.1 update 4 there is an Emulation drawer containing official AmigaOS 3.x ROMs (all classic Amiga models including CD32) and relative Workbench files.
Acube Systems entered an agreement with Hyperion under which it has ported AmigaOS 4 to its Sam440ep and Sam460ex line of PowerPC-based motherboards. In 2009 a version for Pegasos II was released in co-operation with Acube Systems. In 2012, A-EON Technology Ltd manufactured and released the AmigaOne X1000 to consumers through their distributor, AmigaKit.com.
Amiga hardware clones.
Long-time Amiga developer MacroSystem entered the Amiga-clone market with their DraCo non-linear video editing system. It appears in two versions, initially a tower model and later a cube. DraCo expanded upon and combined a number of earlier expansion cards developed for Amiga (VLabMotion, Toccata, WarpEngine, RetinaIII) into a true Amiga-clone powered by the Motorola 68060 processor. The DraCo can run AmigaOS 3.1 up through AmigaOS 3.9. It is the only Amiga-based system to support FireWire for video I/O. DraCo also offers an Amiga-compatible Zorro-II expansion bus and introduced a faster custom DraCoBus, capable of transfer rates (faster than Commodore's Zorro-III). The technology was later used in the Casablanca system, a set-top-box also designed for non-linear video editing.
In 1998, Index Information released the Access, an Amiga-clone similar to the Amiga 1200, but on a motherboard which could fit into a standard 5¼" drive bay. It features either a 68020 or 68030 CPU, with a redesigned AGA chipset, and runs AmigaOS 3.1.
In 1998, former Amiga employees (John Smith, Peter Kittel, Dave Haynie and Andy Finkel to mention few) formed a new company called PIOS. Their hardware platform, PIOS One, was aimed at Amiga, Atari and Macintosh users. The company was renamed to Met@box in 1999 until it folded.
The NatAmi (short for "Native Amiga") hardware project began in 2005 with the aim of designing and building an Amiga clone motherboard that is enhanced with modern features. The NatAmi motherboard is a standard Mini-ITX-compatible form factor computer motherboard, powered by a Motorola/Freescale 68060 and its chipset. It is compatible with the original Amiga chipset, which has been inscribed on a programmable FPGA Altera chip on the board. The NatAmi is the second Amiga clone project after the Minimig motherboard, and its history is very similar to that of the C-One mainboard developed by Jeri Ellsworth and Jens Schönfeld. From a commercial point of view, Natami's circuitry and design are currently closed source. One goal of the NatAmi project is to design an Amiga-compatible motherboard that includes up-to-date features but that does not rely on emulation (as in WinUAE), modern PC Intel components, or a modern PowerPC mainboard. As such, NatAmi is not intended to become another evolutionary heir to classic Amigas, such as with AmigaOne or Pegasos computers. This "purist" philosophy essentially limits the resulting processor speed but puts the focus on bandwidth and low latencies. The developers also recreated the entire Amiga chipset, freeing it from legacy Amiga limitations such as two megabytes of audio and video graphics RAM as in the AGA chipset, and rebuilt this new chipset by programming a modern FPGA Altera Cyclone IV chip. Later, the developers decided to create from scratch a new software-form processor chip, codenamed "N68050" that resides in the physical Altera FPGA programmable chip.
In 2006, two new Amiga clones were announced, both using FPGA based hardware synthesis to replace the Amiga OCS custom chipset. The first, the Minimig, is a personal project of Dutch engineer Dennis van Weeren. Referred to as "new Amiga hardware", the original model was built on a Xilinx Spartan-3 development board, but soon a dedicated board was developed. The minimig uses the FPGA to reproduce the custom Denise, Agnus, Paula and Gary chips as well as both 8520 CIAs and implements a simple version of Amber. The rest of the chips are an actual 68000 CPU, ram chips, and a PIC microcontroller for BIOS control. The design for Minimig was released as open source on July 25, 2007. In February 2008, an Italian company Acube Systems began selling Minimig boards. A third party upgrade replaces the PIC microcontroller with a more powerful ARM processor, providing more functionality such as write access and support for hard disk images. The Minimig core has been ported to the FPGArcade "Replay" board. The Replay uses an FPGA with about more capacity and which does support the AGA chipset and a 68020 soft core with 68030 capabilities. The Replay board is designed to implement many older computers and classic arcade machines.
The second is the Clone-A system announced by Individual Computers. As of mid 2007 it has been shown in its development form, with FPGA-based boards replacing the Amiga chipset and mounted on an Amiga 500 motherboard.
In 2011, by ArcadeRetroGaming, called the Multiple Classic Computer, which emulates the Commodore 64. Support for Amiga software is planned.
Emulation.
Like many popular but discontinued platforms, the Amiga has been emulated so that software developed for the Amiga can be run on other computer platforms without the original hardware. Such emulators attempt to replicate the functionality of the Amiga architecture in software. As mentioned above, attempts have also been made to replicate the Amiga chipset in FPGA chips.
One of the most challenging aspects of emulation is the design of the Amiga chipset, which relies on cycle-critical timings. As a result, early emulators did not always achieve the intended results though later emulator versions can now accurately reproduce the behavior of Amiga systems.
Operating systems.
AmigaOS.
AmigaOS is a single-user multitasking operating system. It was developed first by Commodore International, and initially introduced in 1985 with the Amiga 1000. Original versions run on the Motorola 68000 series of microprocessors, while AmigaOS 4 runs only on PowerPC microprocessors. At the time of release AmigaOS put an operating system that was well ahead of its time into the hands of the average consumer. It was one of the first commercially available consumer operating systems for personal computers to implement preemptive multitasking.
Another notable feature was the combined use of both a command-line interface and graphical user interface. AmigaDOS was the disk operating system and command line portion of the OS and Workbench the native graphical windowing, icons, menu and pointer environment for file management and launching applications. Notably, AmigaDOS allowed long filenames (up to 107 characters) with whitespace and did not require file extensions. The windowing system and user interface engine which handles all input events is called Intuition.
The multi-tasking kernel is called Exec. It acts as a scheduler for tasks running on the system, providing pre-emptive multitasking with prioritised round-robin scheduling. It enabled true pre-emptive multitasking in as little as 256 KB of free memory.
AmigaOS is one of the few microkernel-based operating systems not to implement memory protection, though this lack is common amongst many of its contemporary operating systems. The lack of memory protection is because the 68000 CPU does not include a memory management unit and therefore there is no way to enforce protection of memory. Although this speeds and eases inter-process communication because programs can communicate by simply passing a pointer back and forth, the lack of memory protection made the AmigaOS more vulnerable to crashes from badly behaving programs than other multitasking systems that did implement memory protection, and Amiga OS is fundamentally incapable of enforcing any form of security model since any program had full access to the system. A co-operational memory protection feature was implemented in AmigaOS 4 and could be retrofitted to old AmigaOS systems using Enforcer or CyberGuard tools.
The problem was somewhat exacerbated by Commodore's initial decision to release documentation relating not only to the OS's underlying software routines, but also to the hardware itself, enabling intrepid programmers who had developed their skills on the Commodore 64 to POKE the hardware directly, as was done on the older platform. While the decision to release the documentation was a popular one and allowed the creation of fast, sophisticated sound and graphics routines in games and demos, it also contributed to system instability as some programmers lacked the expertise to program at this level. For this reason, when the new AGA chipset was released, Commodore declined to release low-level documentation in an attempt to force developers into using the approved software routines.
Influence on other operating systems.
AmigaOS directly or indirectly inspired the development of various operating systems. MorphOS and AROS clearly inherit heavily from the structure of AmigaOS as explained directly in articles regarding these two operating systems. AmigaOS also influenced BeOS, which featured a centralized system of Datatypes, similar to that present in AmigaOS. Likewise, DragonFlyBSD was also inspired by AmigaOS as stated by Dragonfly developer Matthew Dillon who is a former Amiga developer. WindowLab and amiwm are among several window managers for the X Window System seek to mimic the Workbench interface. IBM licensed the Amiga GUI from Commodore in exchange for the REXX language license. This allowed OS/2 to have the WPS (Work Place Shell) GUI shell for OS/2 2.0 a 32-bit operating system.
Unix and Unix-like systems.
Commodore-Amiga produced Amiga Unix, informally known as Amix, based on AT&T SVR4. It supports the Amiga 2500 and Amiga 3000 and is included with the Amiga 3000UX. Among other unusual features of Amix is a hardware-accelerated windowing system which can scroll windows without copying data. Amix is not supported on the later Amiga systems based on 68040 or 68060 processors.
Other, still maintained, operating systems are available for the classic Amiga platform, including Linux and NetBSD. Both require a CPU with MMU such as the 68020 with 68851 or full versions of the 68030, 68040 or 68060. There is also a version of Linux for Amigas with PowerPC accelerator cards. Debian and Yellow Dog Linux can run on the AmigaOne.
There is an official, older version of OpenBSD. The last Amiga release is 3.2. Minix 1.5.10 also runs on Amiga.
Emulating other systems.
The Amiga is able to emulate other computer platforms ranging from many 8-bit systems such as the Sinclair ZX Spectrum, Commodore 64, Nintendo Game Boy, Nintendo Entertainment System, Apple II and the TRS-80. The Commodore PC-Transformer software emulated an IBM 5150 at 1 MHz in Monochrome mode. Later PC-Bridgecards were a full hardware PC on a card with 8086/80286/80386 Intel chips running MS-DOS and Windows in an Amiga window. A-Max emulated an Apple Macintosh using a serial port dongle that had a Macintosh ROM on it. The Amiga had the same 68000 CPU as the Macintosh and, using a Macintosh emulator, could run Mac 68K operating systems and programs. However, the Amiga could not directly read Macintosh 3.5" floppies due to their proprietary form. Further, it required a compatible Macintosh for a copy of its ROM. The Atari ST was also emulated. MAME (the arcade machine emulator) is also available for Amiga systems with PPC accelerator card upgrades.
Amiga software.
In the late 1980s and early 1990s the platform became particularly popular for gaming, demoscene activities and creative software uses. During this time commercial developers marketed a wide range of games and creative software, often developing titles simultaneously for the Atari ST due to the similar hardware architecture. Popular creative software included 3D rendering (ray-tracing) packages, bitmap graphics editors, desktop video software, software development packages and "tracker" music editors.
Until the late 1990s the Amiga remained a popular platform for non-commercial software, often developed by enthusiasts, and much of which was freely redistributable. An on-line archive, Aminet, was created in 1992 and until around 1996 was the largest public archive of software, art and documents for any platform.
Marketing.
The name "Amiga" was chosen by the developers from the Spanish word for a female friend, because they knew Spanish, and because it occurred before Apple and Atari alphabetically. It also conveyed the message that the Amiga computer line was "user friendly" as a pun or play on words.
The first official Amiga logo was a rainbow-colored double checkmark. In later marketing material Commodore largely dropped the checkmark and used logos styled with various typefaces. Though it was never adopted as a trademark by Commodore, the "Boing Ball" has been synonymous with Amiga since its launch. It became an unofficial and enduring theme after a visually impressive animated demonstration at the 1984 Winter Consumer Electronics Show in January 1984 showing a checkered ball bouncing and rotating. Following Escom's purchase of Commodore in 1996, the Boing Ball theme was incorporated into a new logo.
Early Commodore advertisements attempted to cast the computer as an all-purpose business machine, though the Amiga was most commercially successful as a home computer. Throughout the 1980s and early 1990s Commodore primarily placed advertising in computer magazines and occasionally in national newspapers and on television.
Legacy.
Since the demise of Commodore, various groups have marketed successors to the original Amiga line:
AmigaOS and MorphOS are commercial proprietary operative systems. AmigaOS 4, based on AmigaOS 3.1 source code with some parts of version 3.9, is developed by Hyperion Entertainment and runs on PowerPC based hardware. MorphOS, based on some parts of AROS source code, is developed by MorphOS Team and is continued on Apple and other PowerPC based hardware.
There is also AROS, a free and open source operative system (re-implementation of the AmigaOS 3.1 APIs), for Amiga 68k, x86 and ARM hardware (one version runs Linux-hosted on the Raspberry Pi). In particular, AROS for Amiga 68k hardware aims to create an open source Kickstart ROM replacement for emulation purpose and/or for use on real "classic" hardware.
Amiga community.
After Commodore went bankrupt in 1994, there remained a very active Amiga community, which continued to support the platform long after mainstream commercial vendors abandoned it. The most popular Amiga magazine, "Amiga Format", continued to publish editions until 2000, some six years after Commodore filed for bankruptcy. Another magazine, "Amiga Active", was launched in 1999 and was published until 2001. Several notable magazines are in publication today: "Amiga Future", which is available in both English and German; "Bitplane.it", a bi-monthly magazine in Italian; and "AmigaPower", a long-running French magazine.
In spite of declining interest in the platform there was a bi-weekly specialist column in the UK weekly magazine Micro Mart. There is also a fan website, that has served as a community discussion and support resource since the 1994 bankruptcy. Other popular English-language "forums" also exist, particularly amiga.org since 1994 and amigaworld.net and English Amiga Board.
Notable historic uses.
The Amiga series of computers found a place in early computer graphic design and television presentation. Below are some examples of notable uses and users:
In addition, many other celebrities and notable individuals have made use of the Amiga:
Other uses.
The Amiga was also used in a number of special purpose applications:

</doc>
<doc id="1985" url="https://en.wikipedia.org/wiki?curid=1985" title="Absorption">
Absorption

Absorption may refer to:

</doc>
<doc id="1986" url="https://en.wikipedia.org/wiki?curid=1986" title="Actinophryid">
Actinophryid

The actinophryids are small, familiar group of heliozoan protists. They are the most common heliozoa in fresh water, and are especially frequent in lakes and rivers, but a few are found in marine and soil habitats as well. Each actinophryid are unicellular and roughly spherical in shape, without any shell or test, and with many pseudopodia supported by axopods radiating outward from the cell body, which adhere to passing prey and allows it to roll or float about. The outer portion of the cell, or ectoplasm, is distinct and is filled with many tiny vacuoles, which assist in flotation. This is very similar to the process of osmosis. The movement of water from inside the cell to the outside is not because of water concentration in this case. It is the cell pushing the excess water out. A few contractile vacuoles around the periphery of the cell expel excess water, and are visible as clear bulges when full.
There are two genera included here. "Actinophrys" have a single, central nucleus. Most are around 40-50 μm in diameter, with axopods up to 100 μm in length, though this varies. "Actinosphaerium" are several times larger, from 200-1000 μm in diameter, with many nuclei, and are found exclusively in fresh water. Two other genera, "Echinosphaerium" and "Camptonema", have been described but appear to be synonyms.
Reproduction takes place by fission, with open mitosis. Under unfavourable conditions, the organism will form a cyst, which is multi-walled and covered in spikes. While encysted it may undergo a peculiar process of autogamy or self-fertilization, where it goes through meiosis and divides to form two gametes, which then fuse together again. This is the only form of sexual reproduction that occurs within the group, though it is really more genetic reorganization than reproduction.
The axopods are supported by microtubules arranged in a unique and characteristic double-coil pattern. In "Actinophrys", these arise from the nuclear membrane, while in "Actinosphaerium" some do and others don't. Other heliozoa where the microtubules arise from the nucleus have been considered possible relatives, and it now appears that the actinophryids developed from axodines such as "Pedinella". These are specialized heterokont algae, related to golden algae, diatoms, brown algae, and the like, which have microtubule-supported tentacles.
As far as the diet of the Actinophyrys goes, the protist feeds on small flagellates, diminutive cilates, microscopic algae, etc.

</doc>
<doc id="1988" url="https://en.wikipedia.org/wiki?curid=1988" title="Abel Tasman">
Abel Tasman

Abel Janszoon Tasman (; 1603 – 10 October 1659) was a Dutch seafarer, explorer, and merchant, best known for his voyages of 1642 and 1644 in the service of the Dutch East India Company (VOC). He was the first known European explorer to reach the islands of Van Diemen's Land (now Tasmania) and New Zealand, and to sight the Fiji islands. 
First Pacific voyage.
Abel Jans Tasman was born in 1603 in Lutjegast in what is now the province of Groningen, the Netherlands. The oldest available source mentioning him dates 27 December 1631 when, as a widower living in Amsterdam, he became engaged to marry 21-year-old Jannetje Tjaers from the Jordaan district of the city. In 1633 he sailed from Texel to Batavia in the service of the Dutch East India Company (VOC), taking the southern Brouwer Route. Tasman helped to punish the people from Seram Island who had sold spices to others than the Dutch. In August 1637 he was back in Amsterdam, and the following year he signed on for another ten years and took his wife with him to Batavia. On 25 March 1638 he tried to sell his property in the Jordaan, but the purchase was cancelled. In 1639 he was second-in-command of an exploration expedition in the north Pacific under Matthijs Quast. The fleet included the ships "Engel" and "Gracht" and reached Fort Zeelandia (Dutch Formosa) and Deshima.
In August 1642, the Council of the Indies, consisting of Antonie van Diemen, Cornelis van der Lijn, Joan Maetsuycker, Justus Schouten, Salomon Sweers, Cornelis Witsen, and Pieter Boreel in Batavia despatched Tasman and Franchoijs Visscher on a voyage of which one of the objects was to obtain knowledge of "all the totally unknown provinces of Beach". This expedition used two small ships, the "Heemskerck" and the "Zeehaen".
Beach and Terra Australis.
"Beach" appeared on maps of the time, notably that of Abraham Ortelius of 1570 and that of Jan Huygen van Linschoten of 1596, as the northernmost part of the southern continent, the "Terra Australis", along with "Locach". According to Marco Polo, "Locach" was a kingdom where gold was “so plentiful that no one who did not see it could believe it”. "Beach" was in fact a mistranscription of "Locach". Locach was Marco Polo’s name for the southern Thai kingdom of Lavo, or Lop Buri, the “city of Lavo”, (ลพบร, after Lavo, the son of Rama in Hindu mythology). In Chinese (Cantonese), Lavo was pronounced “Lo-huk” (羅斛), from which Marco Polo took his rendition of the name. In the German cursive script, “Locach” and “Boeach” look similar, and in the 1532 edition of Marco Polo’s "Travels" his Locach was changed to "Boëach", later shortened to "Beach".
They seem to have drawn on the map of the world published in Florence in 1489 by Henricus Martellus, in which "provincia boëach" appears as the southern neighbour of "provincia ciamba". Book III of Marco Polo’s "Il Milione" described his journey by sea from China to India by way of Champa (= Southern Vietnam), Java (which he called "Java Major"), Locach and Sumatra (called "Java Minor"). After a chapter describing the kingdom of Champa there follows a chapter describing Java (which he did not visit himself). The narrative then resumes, describing the route southward from Champa toward Sumatra, but by a slip of the pen the name “Java” was substituted for “Champa” as the point of departure, locating Sumatra 1,300 miles to the south of Java instead of Champa. Locach, located between Champa and Sumatra, was likewise misplaced far to the south of Java, by some geographers on or near an extension of the "Terra Australis".
As explained by Sir Henry Yule, the editor of an English edition of Marco Polo’s "Travels": “Some geographers of the 16th century, following the old editions which carried the travellers south-east of Java to the land of “Boeach” (or Locac), introduced in their maps a continent in that situation”. Gerard Mercator did just that on his 1541 globe, placing "Beach provincia aurifera" (“Beach the gold-bearing province”) in the northernmost part of the "Terra Australis" in accordance with the faulty text of Marco Polo’s "Travels".
It remained in this location on his world map of 1569, with the amplified description, quoting Marco Polo, "Beach provincia aurifera quam pauci ex alienis regionibus adeunt propter gentis inhumanitatem" (“Beach the gold-bearing province, wither few go from other countries because of the inhumanity of its people”) with "Lucach regnum" shown somewhat to its south west. Following Mercator, Abraham Ortelius also showed "BEACH" and "LVCACH" in these locations on his world map of 1571. Likewise, Linschoten’s very popular 1596 map of the East Indies showed "BEACH" projecting from the map’s southern edge, leading (or misleading) Visscher and Tasman in their voyage of 1642 to seek Beach with its plentiful gold in a location to the south of the Solomon Islands somewhere between Staten Land near Cape Horn and the Cape of Good Hope.
Confirmation that land existed where the maps showed "Beach" to be had come from Dirk Hartog’s landing in October 1616 on its west coast, which he called Eendrachtsland after the name of his ship.
Mauritius.
In accordance with Visscher's directions, Tasman sailed from Batavia on 14 August 1642 and arrived at Mauritius on 5 September 1642. The reason for this was the crew could be fed well on the island; there was plenty of fresh water and timber to repair the ships. Tasman got the assistance of the governor Adriaan van der Stel. Because of the prevailing winds Mauritius was chosen as a turning point. After a four-week stay on the island both ships left on 8 October using the Roaring Forties to sail east as fast as possible. (No one had gone as far as Pieter Nuyts in 1626/27.) On 7 November snow and hail influenced the ship's council to alter course to a more north-eastern direction, expecting to arrive one day at the Solomon Islands.
Tasmania.
On 24 November 1642 Abel Tasman reached and sighted the west coast of Tasmania, north of Macquarie Harbour. He named his discovery Van Diemen's Land after Antonio van Diemen, Governor-General of the Dutch East Indies. Proceeding south he skirted the southern end of Tasmania and turned north-east, Tasman then tried to work his two ships into Adventure Bay on the east coast of South Bruny Island where he was blown out to sea by a storm, this area he named Storm Bay. Two days later Tasman anchored to the North of Cape Frederick Hendrick just North of the Forestier Peninsula. Tasman then landed in Blackman Bay – in the larger Marion Bay. The next day, an attempt was made to land in North Bay; however, because the sea was too rough the carpenter swam through the surf and planted the Dutch flag in North Bay. Tasman then claimed formal possession of the land on 3 December 1642.
New Zealand.
After some exploration, Tasman had intended to proceed in a northerly direction but as the wind was unfavourable he steered east. Tasman endured a very rough journey from Tasmania to New Zealand. In one of his diary entries Tasman credits his compass, claiming it was the only thing that kept him alive. On 13 December they sighted land on the north-west coast of the South Island, New Zealand, becoming the first Europeans to do so. Tasman named it "Staten Landt" on the assumption that it was connected to an island (Staten Island, Argentina) at the south of the tip of South America. He sailed north, then east and 5 days later anchored about 7 km from the coast. He sent ship's boats to gather water, but one of his boats was attacked by Māori in a double hulled waka (canoe) and four of his men were attacked and killed by mere. He made it to land but was unwelcomely forced away - unable to claim the country for the Dutch. 
As Tasman sailed out of the bay he was again attacked, this time by 11 waka. The waka approached the "Zeehaen" which fired and hit one Maori who fell down. Canister shot hit the side of a waka. Archeological research has shown the Dutch had tried to land at a major agricultural area, which the Māori may have been trying to protect. Tasman named the bay "Murderers' Bay" (now known as Golden Bay) and sailed north, but mistook Cook Strait for a bight (naming it "Zeehaen's Bight"). Two names he gave to New Zealand landmarks still endure, Cape Maria van Diemen and Three Kings Islands, but "Kaap Pieter Boreels" was renamed by Cook 125 years later to Cape Egmont.
The return voyage.
On route back to Batavia, Tasman came across the Tongan archipelago on 20 January 1643. While passing the Fiji Islands Tasman's ships came close to being wrecked on the dangerous reefs of the north-eastern part of the Fiji group. He charted the eastern tip of Vanua Levu and Cikobia before making his way back into the open sea. He eventually turned north-west to New Guinea, and arrived at Batavia on 15 June 1643.
Second Pacific voyage.
With three ships on his second voyage ("Limmen", "Zeemeeuw" and the tender "Braek") in 1644, he followed the south coast of New Guinea eastwards. He missed the Torres Strait between New Guinea and Australia, and continued his voyage westwards along the north Australian coast. He mapped the north coast of Australia making observations on the land, called New Holland, and its people.
From the point of view of the Dutch East India Company, Tasman's explorations were a disappointment: he had neither found a promising area for trade nor a useful new shipping route. Although received modestly, the company was upset to a degree that Tasman didn't fully explore the lands he found, and decided that a more "persistent explorer" should be chosen for any future expeditions. For over a century, until the era of James Cook, Tasmania and New Zealand were not visited by Europeans – mainland Australia was visited, but usually only by accident.
Later life.
On 2 November 1644 Abel Tasman was appointed a member of the Council of Justice at Batavia. He went to Sumatra in 1646, and in August 1647 to Siam (now Thailand) with letters from the company to the King. In May 1648 he was in charge of an expedition sent to Manila to try to intercept and loot the Spanish silver ships coming from America, but he had no success and returned to Batavia in January 1649. In November 1649 he was charged and found guilty of having in the previous year hanged one of his men without trial, was suspended from his office of commander, fined, and made to pay compensation to the relatives of the sailor. On 5 January 1651 he was formally reinstated in his rank and spent his remaining years at Batavia. He was in good circumstances, being one of the larger landowners in the town. He died at Batavia on 10 October 1659 and was survived by his second wife and a daughter by his first wife. His property was divided between his wife and his daughter by his first marriage. In his testimony (dating from 1657) he left only 25 guilders to the poor of his village.
Legacy.
Multiple places have been named after Tasman, including:
Also named after Tasman are:
His portrait has been on 4 New Zealand postage stamp issues, on a 1992 5 NZD coin, and on one 1985 Australian postage stamp
Tasman Map.
The original Tasman Map is held in the collection of the State Library of New South Wales. The map shows a general outline of parts of the coastline of Australia. In the foyer of the Mitchell wing at the State Library, the map is reproduced in a marble floor.

</doc>
<doc id="1990" url="https://en.wikipedia.org/wiki?curid=1990" title="August 5">
August 5


</doc>
<doc id="1991" url="https://en.wikipedia.org/wiki?curid=1991" title="Angula">
Angula

The word Angula may refer to one of the following:

</doc>
<doc id="1994" url="https://en.wikipedia.org/wiki?curid=1994" title="ASP">
ASP

ASP may refer to:

</doc>
<doc id="1997" url="https://en.wikipedia.org/wiki?curid=1997" title="Algebraic geometry">
Algebraic geometry

Algebraic geometry is a branch of mathematics, classically studying zeros of multivariate polynomials. Modern algebraic geometry is based on the use of abstract algebraic techniques, mainly from commutative algebra, for solving geometrical problems about these sets of zeros.
The fundamental objects of study in algebraic geometry are algebraic varieties, which are geometric manifestations of solutions of systems of polynomial equations. Examples of the most studied classes of algebraic varieties are: plane algebraic curves, which include lines, circles, parabolas, ellipses, hyperbolas, cubic curves like elliptic curves and quartic curves like lemniscates, and Cassini ovals. A point of the plane belongs to an algebraic curve if its coordinates satisfy a given polynomial equation. Basic questions involve the study of the points of special interest like the singular points, the inflection points and the points at infinity. More advanced questions involve the topology of the curve and relations between the curves given by different equations.
Algebraic geometry occupies a central place in modern mathematics and has multiple conceptual connections with such diverse fields as complex analysis, topology and number theory. Initially a study of systems of polynomial equations in several variables, the subject of algebraic geometry starts where equation solving leaves off, and it becomes even more important to understand the intrinsic properties of the totality of solutions of a system of equations, than to find a specific solution; this leads into some of the deepest areas in all of mathematics, both conceptually and in terms of technique.
In the 20th century, algebraic geometry split into several subareas.
Much of the development of the main stream of algebraic geometry in the 20th century occurred within an abstract algebraic framework, with increasing emphasis being placed on "intrinsic" properties of algebraic varieties not dependent on any particular way of embedding the variety in an ambient coordinate space; this parallels developments in topology, differential and complex geometry. One key achievement of this abstract algebraic geometry is Grothendieck's scheme theory which allows one to use sheaf theory to study algebraic varieties in a way which is very similar to its use in the study of differential and analytic manifolds. This is obtained by extending the notion of point: In classical algebraic geometry, a point of an affine variety may be identified, through Hilbert's Nullstellensatz, with a maximal ideal of the coordinate ring, while the points of the corresponding affine scheme are all prime ideals of this ring. This means that a point of such a scheme may be either a usual point or a subvariety. This approach also enables a unification of the language and the tools of classical algebraic geometry, mainly concerned with complex points, and of algebraic number theory. Wiles's proof of the longstanding conjecture called Fermat's last theorem is an example of the power of this approach.
Basic notions.
Zeros of simultaneous polynomials.
In classical algebraic geometry, the main objects of interest are the vanishing sets of collections of polynomials, meaning the set of all points that simultaneously satisfy one or more polynomial equations. For instance, the two-dimensional sphere in three-dimensional Euclidean space R could be defined as the set of all points ("x","y","z") with
A "slanted" circle in R can be defined as the set of all points ("x","y","z") which satisfy the two polynomial equations
Affine varieties.
First we start with a field "k". In classical algebraic geometry, this field was always the complex numbers C, but many of the same results are true if we assume only that "k" is algebraically closed. We consider the affine space of dimension "n" over "k", denoted A("k") (or more simply A, when "k" is clear from the context). When one fixes a coordinates system, one may identify A("k") with "k". The purpose of not working with "k" is to emphasize that one "forgets" the vector space structure that "k" carries.
A function "f" : A → A is said to be "polynomial" (or "regular") if it can be written as a polynomial, that is, if there is a polynomial "p" in "kx"...,"x"} vanishes. Like for affine algebraic sets, there is a bijection between the projective algebraic sets and the reduced homogeneous ideals which define them. The "projective varieties" are the projective algebraic sets whose defining ideal is prime. In other words, a projective variety is a projective algebraic set, whose homogeneous coordinate ring is an integral domain, the "projective coordinates ring" being defined as the quotient of the graded ring or the polynomials in "n"+1 variables by the homogeneous (reduced) ideal defining the variety. Every projective algebraic set may be uniquely decomposed into a finite union of projective varieties.
The only regular functions which may be defined properly on a projective variety are the constant functions. Thus this notion is not used in projective situations. On the other hand, the "field of the rational functions" or "function field " is a useful notion, which, similarly as in the affine case, is defined as the set of the quotients of two homogeneous elements of the same degree in the homogeneous coordinate ring.
Real algebraic geometry.
The real algebraic geometry is the study of the real points of the algebraic geometry.
The fact that the field of the reals number is an ordered field may not be occulted in such a study. For example, the curve of equation formula_4 is a circle if formula_5, but does not have any real point if formula_6. It follows that real algebraic geometry is not only the study of the real algebraic varieties, but has been generalized to the study of the "semi-algebraic sets", which are the solutions of systems of polynomial equations and polynomial inequalities. For example, a branch of the hyperbola of equation formula_7 is not an algebraic variety, but is a semi-algebraic set defined by formula_8 and formula_9 or by formula_8 and formula_11.
One of the challenging problems of real algebraic geometry is the unsolved Hilbert's sixteenth problem: Decide which respective positions are possible for the ovals of a nonsingular plane curve of degree 8.
Computational algebraic geometry.
One may date the origin of computational algebraic geometry to meeting EUROSAM'79 (International Symposium on Symbolic and Algebraic Manipulation) held at Marseille, France in June 1979. At this meeting,
Since then, most results in this area are related to one or several of these items either by using or improving one of these algorithms, or by finding algorithms whose complexity is simply exponential in the number of the variables.
Gröbner basis.
A Gröbner basis is a system of generators of a polynomial ideal whose computation allows the deduction of many properties of the affine algebraic variety defined by the ideal.
Given an ideal "I" defining an algebraic set "V":
Gröbner basis computations do not allow to compute directly the primary decomposition of "I" nor the prime ideals defining the irreducible components of "V", but most algorithms for this involve Gröbner basis computation. The algorithms which are not based on Gröbner bases use regular chains but may need Gröbner bases in some exceptional situations.
Gröbner base are deemed to be difficult to compute. In fact they may contain, in the worst case, polynomials whose degree is doubly exponential in the number of variables and a number of polynomials which is also doubly exponential. However, this is only a worst case complexity, and the complexity bound of Lazard's algorithm of 1979 may frequently apply. Faugère's F4 and F5 algorithms realize this complexity, as F5 algorithm may be viewed as an improvement of Lazard's 1979 algorithm. It follows that the best implementations allow to compute almost routinely with algebraic sets of degree more than 100. This means that, presently, the difficulty of computing a Gröbner basis is strongly related to the intrinsic difficulty of the problem.
Cylindrical algebraic decomposition (CAD).
CAD is an algorithm which had been introduced in 1973 by G. Collins to implement with an acceptable complexity the Tarski–Seidenberg theorem on quantifier elimination over the real numbers.
This theorem concerns the formulas of the first-order logic whose atomic formulas are polynomial equalities or inequalities between polynomials with real coefficients. These formulas are thus the formulas which may be constructed from the atomic formulas by the logical operators "and" (∧), "or" (∨), "not" (¬), "for all" (∀) and "exists" (∃). Tarski's theorem asserts that, from such a formula, one may compute an equivalent formula without quantifier (∀, ∃).
The complexity of CAD is doubly exponential in the number of variables. This means that CAD allow, in theory, to solve every problem of real algebraic geometry which may be expressed by such a formula, that is almost every problem concerning explicitly given varieties and semi-algebraic sets.
While Gröbner basis computation has doubly exponential complexity only in rare cases, CAD has almost always this high complexity. This implies that, unless if most polynomials appearing in the input are linear, it may not solve problems with more than four variables.
Since 1973, most of the research on this subject is devoted either to improve CAD or to find alternate algorithms in special cases of general interest.
As an example of the state of art, there are efficient algorithms to find at least a point in every connected component of a semi-algebraic set, and thus to test if a semi-algebraic set is empty. On the other hand, CAD is yet, in practice, the best algorithm to count the number of connected components.
Asymptotic complexity vs. practical efficiency.
The basic general algorithms of computational geometry have a double exponential worst case complexity. More precisely, if "d" is the maximal degree of the input polynomials and "n" the number of variables, their complexity is at most formula_12 for some constant "c", and, for some inputs, the complexity is at least formula_13 for another constant "c"′.
During the last 20 years of 20th century, various algorithms have been introduced to solve specific subproblems with a better complexity. Most of these algorithms have a complexity formula_14.
Among these algorithms which solve a sub problem of the problems solved by Gröbner bases, one may cite "testing if an affine variety is empty" and "solving nonhomogeneous polynomial systems which have a finite number of solutions." Such algorithms are rarely implemented because, on most entries Faugère's F4 and F5 algorithms have a better practical efficiency and probably a similar or better complexity ("probably" because the evaluation of the complexity of Gröbner basis algorithms on a particular class of entries is a difficult task which has been done only in a few special cases).
The main algorithms of real algebraic geometry which solve a problem solved by CAD are related to the topology of semi-algebraic sets. One may cite "counting the number of connected components", "testing if two points are in the same components" or "computing a Whitney stratification of a real algebraic set". They have a complexity of
formula_14, but the constant involved by "O" notation is so high that using them to solve any nontrivial problem effectively solved by CAD, is impossible even if one could use all the existing computing power in the world. Therefore, these algorithms have never been implemented and this is an active research area to search for algorithms with have together a good asymptotic complexity and a good practical efficiency.
Abstract modern viewpoint.
The modern approaches to algebraic geometry redefine and effectively extend the range of basic objects in various levels of generality to schemes, formal schemes, ind-schemes, algebraic spaces, algebraic stacks and so on. The need for this arises already from the useful ideas within theory of varieties, e.g. the formal functions of Zariski can be accommodated by introducing nilpotent elements in structure rings; considering spaces of loops and arcs, constructing quotients by group actions and developing formal grounds for natural intersection theory and deformation theory lead to some of the further extensions.
Most remarkably, in late 1950s, algebraic varieties were subsumed into Alexander Grothendieck's concept of a scheme. Their local objects are affine schemes or prime spectra which are locally ringed spaces which form a category which is antiequivalent to the category of commutative unital rings, extending the duality between the category of affine algebraic varieties over a field "k", and the category of finitely generated reduced "k"-algebras. The gluing is along Zariski topology; one can glue within the category of locally ringed spaces, but also, using the Yoneda embedding, within the more abstract category of presheaves of sets over the category of affine schemes. The Zariski topology in the set theoretic sense is then replaced by a Grothendieck topology. Grothendieck introduced Grothendieck topologies having in mind more exotic but geometrically finer and more sensitive examples than the crude Zariski topology, namely the étale topology, and the two flat Grothendieck topologies: fppf and fpqc; nowadays some other examples became prominent including Nisnevich topology. Sheaves can be furthermore generalized to stacks in the sense of Grothendieck, usually with some additional representability conditions leading to Artin stacks and, even finer, Deligne-Mumford stacks, both often called algebraic stacks.
Sometimes other algebraic sites replace the category of affine schemes. For example, Nikolai Durov has introduced commutative algebraic monads as a generalization of local objects in a generalized algebraic geometry. Versions of a tropical geometry, of an absolute geometry over a field of one element and an algebraic analogue of Arakelov's geometry were realized in this setup.
Another formal generalization is possible to Universal algebraic geometry in which every variety of algebras has its own algebraic geometry. The term "variety of algebras" should not be confused with "algebraic variety".
The language of schemes, stacks and generalizations has proved to be a valuable way of dealing with geometric concepts and became cornerstones of modern algebraic geometry.
Algebraic stacks can be further generalized and for many practical questions like deformation theory and intersection theory, this is often the most natural approach. One can extend the Grothendieck site of affine schemes to a higher categorical site of derived affine schemes, by replacing the commutative rings with an infinity category of differential graded commutative algebras, or of simplicial commutative rings or a similar category with an appropriate variant of a Grothendieck topology. One can also replace presheaves of sets by presheaves of simplicial sets (or of infinity groupoids). Then, in presence of an appropriate homotopic machinery one can develop a notion of derived stack as such a presheaf on the infinity category of derived affine schemes, which is satisfying certain infinite categorical version of a sheaf axiom (and to be algebraic, inductively a sequence of representability conditions). Quillen model categories, Segal categories and quasicategories are some of the most often used tools to formalize this yielding the "derived algebraic geometry", introduced by the school of Carlos Simpson, including Andre Hirschowitz, Bertrand Toën, Gabrielle Vezzosi, Michel Vaquié and others; and developed further by Jacob Lurie, Bertrand Toën, and Gabrielle Vezzosi. Another (noncommutative) version of derived algebraic geometry, using A-infinity categories has been developed from early 1990-s by Maxim Kontsevich and followers.
History.
Prehistory: before the 16th century.
Some of the roots of algebraic geometry date back to the work of the Hellenistic Greeks from the 5th century BC. The Delian problem, for instance, was to construct a length "x" so that the cube of side "x" contained the same volume as the rectangular box "a""b" for given sides "a" and "b". Menaechmus (circa 350 BC) considered the problem geometrically by intersecting the pair of plane conics "ay" = "x" and "xy" = "ab". The later work, in the 3rd century BC, of Archimedes and Apollonius studied more systematically problems on conic sections, and also involved the use of coordinates. The Arab mathematicians were able to solve by purely algebraic means certain cubic equations, and then to interpret the results geometrically. This was done, for instance, by Ibn al-Haytham in the 10th century AD. Subsequently, Persian mathematician Omar Khayyám (born 1048 A.D.) discovered the general method of solving cubic equations by intersecting a parabola with a circle. Each of these early developments in algebraic geometry dealt with questions of finding and describing the intersections of algebraic curves.
Renaissance.
Such techniques of applying geometrical constructions to algebraic problems were also adopted by a number of Renaissance mathematicians such as Gerolamo Cardano and Niccolò Fontana "Tartaglia" on their studies of the cubic equation. The geometrical approach to construction problems, rather than the algebraic one, was favored by most 16th and 17th century mathematicians, notably Blaise Pascal who argued against the use of algebraic and analytical methods in geometry. The French mathematicians Franciscus Vieta and later René Descartes and Pierre de Fermat revolutionized the conventional way of thinking about construction problems through the introduction of coordinate geometry. They were interested primarily in the properties of "algebraic curves", such as those defined by Diophantine equations (in the case of Fermat), and the algebraic reformulation of the classical Greek works on conics and cubics (in the case of Descartes).
During the same period, Blaise Pascal and Gérard Desargues approached geometry from a different perspective, developing the synthetic notions of projective geometry. Pascal and Desargues also studied curves, but from the purely geometrical point of view: the analog of the Greek "ruler and compass construction". Ultimately, the analytic geometry of Descartes and Fermat won out, for it supplied the 18th century mathematicians with concrete quantitative tools needed to study physical problems using the new calculus of Newton and Leibniz. However, by the end of the 18th century, most of the algebraic character of coordinate geometry was subsumed by the "calculus of infinitesimals" of Lagrange and Euler.
19th and early 20th century.
It took the simultaneous 19th century developments of non-Euclidean geometry and Abelian integrals in order to bring the old algebraic ideas back into the geometrical fold. The first of these new developments was seized up by Edmond Laguerre and Arthur Cayley, who attempted to ascertain the generalized metric properties of projective space. Cayley introduced the idea of "homogeneous polynomial forms", and more specifically quadratic forms, on projective space. Subsequently, Felix Klein studied projective geometry (along with other types of geometry) from the viewpoint that the geometry on a space is encoded in a certain class of transformations on the space. By the end of the 19th century, projective geometers were studying more general kinds of transformations on figures in projective space. Rather than the projective linear transformations which were normally regarded as giving the fundamental Kleinian geometry on projective space, they concerned themselves also with the higher degree birational transformations. This weaker notion of congruence would later lead members of the 20th century Italian school of algebraic geometry to classify algebraic surfaces up to birational isomorphism.
The second early 19th century development, that of Abelian integrals, would lead Bernhard Riemann to the development of Riemann surfaces.
In the same period began the algebraization of the algebraic geometry through commutative algebra. The prominent results in this direction are Hilbert's basis theorem and Hilbert's Nullstellensatz, which are the basis of the connexion between algebraic geometry and commutative algebra, and Macaulay's multivariate resultant, which is the basis of elimination theory. Probably because of the size of the computation which is implied by multivariate resultants, elimination theory was forgotten during the middle of the 20th century until it was renewed by singularity theory and computational algebraic geometry.
20th century.
B. L. van der Waerden, Oscar Zariski and André Weil developed a foundation for algebraic geometry based on contemporary commutative algebra, including valuation theory and the theory of ideals. One of the goals was to give a rigorous framework for proving the results of Italian school of algebraic geometry. In particular, this school used systematically the notion of generic point without any precise definition, which was first given by these authors during the 1930s.
In the 1950s and 1960s Jean-Pierre Serre and Alexander Grothendieck recast the foundations making use of sheaf theory. Later, from about 1960, and largely led by Grothendieck, the idea of schemes was worked out, in conjunction with a very refined apparatus of homological techniques. After a decade of rapid development the field stabilized in the 1970s, and new applications were made, both to number theory and to more classical geometric questions on algebraic varieties, singularities and moduli.
An important class of varieties, not easily understood directly from their defining equations, are the abelian varieties, which are the projective varieties whose points form an abelian group. The prototypical examples are the elliptic curves, which have a rich theory. They were instrumental in the proof of Fermat's last theorem and are also used in elliptic curve cryptography.
In parallel with the abstract trend of the algebraic geometry, which is concerned with general statements about varieties, methods for effective computation with concretely-given varieties have also been developed, which lead to the new area of computational algebraic geometry. One of the founding methods of this area is the theory of Gröbner bases, introduced by Bruno Buchberger in 1965. Another founding method, more specially devoted to real algebraic geometry, is the cylindrical algebraic decomposition, introduced by George E. Collins in 1973.
Analytic geometry.
An analytic variety is defined locally as the set of common solutions of several equations involving analytic functions. It is analogous to the included concept of real or complex algebraic variety. Any complex manifold is an analytic variety. Since analytic varieties may have singular points, not all analytic varieties are manifolds.
Modern analytic geometry is essentially equivalent to real and complex algebraic geometry, as has been shown by Jean-Pierre Serre in his paper "GAGA", the name of which is French for "Algebraic geometry and analytic geometry". Nevertheless, the two fields remain distinct, as the methods of proof are quite different and algebraic geometry includes also geometry in finite characteristic.
Applications.
Algebraic geometry now finds applications in statistics, control theory, robotics, error-correcting codes, phylogenetics and geometric modelling. There are also connections to string theory, game theory, graph matchings, solitons and integer programming.

</doc>
<doc id="1998" url="https://en.wikipedia.org/wiki?curid=1998" title="Austin, Texas">
Austin, Texas

Austin () is the capital of the US state of Texas and the seat of Travis County. Located in , Austin is the city in the United States and the city in Texas. It is the fastest growing of the largest 50 US cities. Austin is also the second largest state capital in the United States, after Phoenix, Arizona. As of July 1, 2014, Austin had a population of 912,791 (U.S. Census Bureau estimate). The city is the cultural and economic center of the metropolitan area, which had an estimated population of 1,943,299 as of July 1, 2014.
In the 1830s, pioneers began to settle the area in central Austin along the Colorado River. After Republic of Texas Vice President Mirabeau B. Lamar visited the area during a buffalo-hunting expedition between 1837 and 1838, he proposed that the republic's capital, then located in Houston, be relocated to the area situated on the north bank of the Colorado River near the present-day Congress Avenue Bridge. In 1839, the site was officially chosen as the republic's new capital (the republic's seventh and final location) and was incorporated under the name Waterloo. Shortly thereafter, the name was changed to Austin in honor of Stephen F. Austin, the "Father of Texas" and the republic's first secretary of state.
The city grew throughout the 19th century and became a center for government and education with the construction of the Texas State Capitol and the University of Texas at Austin. After a lull in growth from the Great Depression, Austin resumed its development into a major city and, by the 1980s, it emerged as a center for technology and business. A number of Fortune 500 companies have headquarters or regional offices in Austin including Advanced Micro Devices, Apple Inc., ARM Holdings, Cisco, eBay, Google, IBM, Intel, Texas Instruments, 3M, Oracle Corporation and Whole Foods Market. Dell's worldwide headquarters is located in nearby Round Rock, a suburb of Austin.
Residents of Austin are known as Austinites. They include a diverse mix of government employees (e.g., university faculty and staff, law enforcement, political staffers); foreign and domestic college students; musicians; high-tech workers; blue-collar workers and businesspeople. The city is home to development centers for many technology corporations; it adopted the "Silicon Hills" nickname in the 1990s. However, the current official slogan promotes Austin as "The Live Music Capital of the World", a reference to the many musicians and live music venues within the area, and the long-running PBS TV concert series "Austin City Limits". In recent years, some Austinites have also adopted the unofficial slogan "Keep Austin Weird". This interpretation of the classic "Texas-style" sense of independence refers to a desire to protect small, unique, local businesses from being overrun by large corporations. In the late 1800s, Austin also became known as the City of the "Violet Crown" for the wintertime violet glow of color across the hills just after sunset. Even today, many Austin businesses use the term "violet crown" in their name. Austin is known as a "clean-air city" for the city's stringent no-smoking ordinances that apply to all public places and buildings, including restaurants and bars. The FBI ranked Austin as the second-safest major city in the U.S. for the year 2012.
History.
Austin, Travis County and Williamson County have been the site of human habitation since at least 9200 BC. The earliest known inhabitants of the area lived during the late Pleistocene (Ice Age) and are linked to the Clovis culture around 9200 BC (11,200 years ago), based on evidence found throughout the area and documented at the much-studied Gault Site, midway between Georgetown and Fort Hood.
When settlers first arrived from Europe, the area was inhabited by the Tonkawa tribe, and the Comanches and Lipan Apaches were known to travel through the area as well. Spanish colonists, including the Espinosa-Olivares-Aguirre expedition, traveled through the area for centuries, though few permanent settlements were created for some time. In 1730, three missions from East Texas were combined and reestablished as one mission on the south side of the Colorado River, in what is now Zilker Park, in Austin. The mission was in this area for only about seven months, and then was moved to San Antonio de Béxar and split into three missions. In the mid-18th century, the San Xavier missions were located along the Colorado River, in what is now western Milam County, to facilitate exploration.
Early in the 19th century, Spanish forts were established in what are now Bastrop and San Marcos. Following the independence of Mexico, new settlements were established in Central Texas, but growth in the region was stagnant because of conflicts with the regional Native Americans.
In 1835–1836, Texans fought and won independence from Mexico. Texas thus became its own independent country with its own president, congress, and monetary system. In 1839, the Texas Congress formed a commission to seek a site for a new capital to be named for Stephen F. Austin. Mirabeau B. Lamar, second president of the newly formed Republic of Texas, advised the commissioners to investigate the area named Waterloo, noting the area's hills, waterways, and pleasant surroundings. Waterloo was selected and the name "Austin" was chosen as the town's new name. The location was seen as a convenient crossroads for trade routes between Santa Fe and Galveston Bay, as well as routes between northern Mexico and the Red River. Austin is also the site where the southern leg of the Chisholm Trail leads to the Colorado River.
Edwin Waller was picked by Lamar to survey the village and draft a plan laying out the new capital. The original site was narrowed to that fronted the Colorado River between two creeks, Shoal Creek and Waller Creek, which was later named in his honor. The 14-block grid plan was bisected by a broad north-south thoroughfare, Congress Avenue, running up from the river to Capital Square, where the new Texas State Capitol was to be constructed. A temporary one-story capitol was erected on the corner of Colorado and 8th Streets. On August 1, 1839, the first auction of 217 out of 306 lots total was held. The grid plan Waller designed and surveyed now forms the basis of downtown Austin.
In 1840, a series of conflicts between the Texas Rangers and the Comanches, known as the Council House Fight and the Battle of Plum Creek, finally pushed the Comanches westward, mostly ending conflicts in Central Texas. Settlement in the area began to expand quickly. Travis County was established in 1840, and the surrounding counties were mostly established within the next two decades.
Initially, the new capital thrived. But Lamar's political enemy, Sam Houston, used two Mexican army incursions to San Antonio as an excuse to move the government. Sam Houston fought bitterly against Lamar's decision to establish the capital in such a remote wilderness. The men and women who traveled mainly from Houston to conduct government business were intensely disappointed as well. By 1840, the population had risen to 856, of whom nearly half fled from Austin when Congress recessed. The resident Black population listed in January of this same year was 176. The fear of Austin's proximity to the Indians and Mexico, which still considered Texas a part of their land, created an immense motive for Sam Houston, the first and third President of the Republic of Texas, to relocate the capital once again in 1841. Upon threats of Mexican troops in Texas, Houston raided the Land Office to transfer all official documents to Houston for safe keeping in what was later known as the Archive War, but the people of Austin would not allow this unaccompanied decision to be executed. The documents stayed, but the capital would temporarily move from Austin to Houston to Washington-on-the-Brazos. Without the governmental body, Austin's population declined to a low of only a few hundred people throughout the early 1840s. The voting by the fourth President of the Republic, Anson Jones, and Congress, who reconvened in Austin in 1845, settled the issue to keep Austin the seat of government as well as annex the Republic of Texas into the United States.
In 1860, 38% of Travis County residents were slaves. In 1861, with the outbreak of the American Civil War, voters in Austin and other Central Texas communities voted against secession. However, as the war progressed and fears of attack by Union forces increased, Austin contributed hundreds of men to the Confederate forces. The African American population of Austin swelled dramatically after the enforcement of the Emancipation Proclamation in Texas by Union General Gordon Granger at Galveston in an event commemorated as Juneteenth. Black communities such as Wheatville, Pleasant Hill, and Clarksville were established with Clarksville being the oldest surviving freedomtown ‒ the original post-Civil War settlements founded by former African-American slaves ‒ west of the Mississippi River. In 1870, blacks made up 36.5% of Austin's population. The postwar period saw dramatic population and economic growth. The opening of the Houston and Texas Central Railway (H&TC) in 1871 turned Austin into the major trading center for the region with the ability to transport both cotton and cattle. The Missouri, Kansas, and Texas (MKT) line followed close behind. Austin was also the terminus of the southernmost leg of the Chisholm Trail and "drovers" pushed cattle north to the railroad. Cotton was one of the few crops produced locally for export and a cotton gin engine was located downtown near the trains for "ginning" cotton of its seeds and turning the product into bales for shipment. However, as other new railroads were built through the region in the 1870s, Austin began to lose its primacy in trade to the surrounding communities. In addition, the areas east of Austin took over cattle and cotton production from Austin, especially in towns like Hutto and Taylor that sit over the blackland prairie, with its deep, rich soils for producing cotton and hay.
In September 1881, Austin public schools held their first classes. The same year, Tillotson Collegiate and Normal Institute (now part of Huston-Tillotson University) opened its doors. The University of Texas at Austin held its first classes in 1883, although classes had been held in the original wooden state Capitol for four years before.
During the 1880s, Austin gained new prominence as the state capitol building was completed in 1888 and claimed as the seventh largest building in the world. In the late 19th century, Austin expanded its city limits to more than three times its former area, and the first granite dam was built on the Colorado River to power a new street car line and the new "moon towers". Unfortunately, the first dam washed away in a flood on April 7, 1900.
In the 1920s and 1930s, Austin launched a series of civic development and beautification projects that created much of the city's infrastructure and many of its parks. In addition, the state legislature established the Lower Colorado River Authority (LCRA) that, along with the city of Austin, created the system of dams along the Colorado River to form the Highland Lakes. These projects were enabled in large part because the Public Works Administration provided Austin with greater funding for municipal construction projects than other Texas cities.
During the early twentieth century, a three-way system of social segregation emerged in Austin, with Anglos, African Americans and Mexicans being separated by custom or law in most aspects of life, including housing, health care, and education. Many of the municipal improvement programs initiated during this period—such as the construction of new roads, schools, and hospitals—were deliberately designed to institutionalize this system of segregation. Racial segregation actually increased in Austin during the first half of the twentieth century, with African Americans and Mexicans experiencing high levels of discrimination and social marginalization.
In 1940, the destroyed granite dam on the Colorado River was finally replaced by a hollow concrete dam that formed Lake McDonald (now called Lake Austin) and which has withstood all floods since. In addition, the much larger Mansfield Dam was built by the LCRA upstream of Austin to form the flood-control lake, Lake Travis. In the early 20th century, the Texas Oil Boom took hold, creating tremendous economic opportunities in Southeast Texas and North Texas. The growth generated by this boom largely passed by Austin at first, with the city slipping from fourth largest to 10th largest in Texas between 1880 and 1920.
After the mid-20th century, Austin became established as one of Texas' major metropolitan centers. In 1970, the United States Census Bureau reported Austin's population as 14.5% Hispanic, 11.9% black, and 73.4% non-Hispanic white. In the late 20th century, Austin emerged as an important high tech center for semiconductors and software. The University of Texas at Austin emerged as a major university.
The 1970s saw Austin's emergence in the national music scene, with local artists such as Willie Nelson, Asleep at the Wheel, and Stevie Ray Vaughan and iconic music venues such as the Armadillo World Headquarters. Over time, the long-running television program "Austin City Limits", its namesake Austin City Limits Festival, and the South by Southwest music festival solidified the city's place in the music industry.
Geography.
The most southerly of the capitals of the contiguous forty-eight states, Austin is located in Central Texas, along the Balcones Escarpment and Interstate 35, 150 miles northwest of Houston. It is also 160 miles south of Dallas and 75 miles north of San Antonio. Its elevation varies from to approximately above sea level. In 2010, the city occupied a total area of . Approximately of this area is water.
Austin is situated on the Colorado River, with three man-made (artificial) lakes within the city limits: Lady Bird Lake (formerly known as Town Lake), Lake Austin (both created by dams along the Colorado River), and Lake Walter E. Long that is partly used for cooling water for the Decker Power Plant. Mansfield Dam and the foot of Lake Travis are located within the city's limits. Lady Bird Lake, Lake Austin, and Lake Travis are each on the Colorado River. As a result of its straddling the Balcones Fault, much of the eastern part of the city is flat, with heavy clay and loam soils, whereas, the western part and western suburbs consist of rolling hills on the edge of the Texas Hill Country. Because the hills to the west are primarily limestone rock with a thin covering of topsoil, portions of the city are frequently subjected to flash floods from the runoff caused by thunderstorms. To help control this runoff and to generate hydroelectric power, the Lower Colorado River Authority operates a series of dams that form the Texas Highland Lakes. The lakes also provide venues for boating, swimming, and other forms of recreation within several parks on the lake shores.
Austin is located at the intersection of four major ecological regions, and is consequently a temperate-to-hot green oasis with a highly variable climate having some characteristics of the desert, the tropics, and a wetter climate. The area is very diverse ecologically and biologically, and is home to a variety of animals and plants. Notably, the area is home to many types of wildflowers that blossom throughout the year but especially in the spring, including the popular bluebonnets, some planted in an effort by "Lady Bird" Johnson, wife of former President Lyndon Johnson.
A popular point of prominence in Austin is Mount Bonnell. At about above sea level, it is a natural limestone formation overlooking Lake Austin on the Colorado River, with an observation deck about below its summit.
The soils of Austin range from shallow, gravelly clay loams over limestone in the western outskirts to deep, fine sandy loams, silty clay loams, silty clays or clays in the city's eastern part. Some of the clays have pronounced shrink-swell properties and are difficult to work under most moisture conditions. Many of Austin's soils, especially the clay-rich types, are slightly to moderately alkaline and have free calcium carbonate.
Cityscape.
Buildings that make up most of Austin's skyline are modest in height and somewhat spread out. The latter characteristic is partly due to a restriction that preserves the view of the Texas State Capitol building from various locations around Austin (known as the Capitol View Corridor). However, many new high-rise towers have been constructed and the downtown area is looking more modern and dense. The city's tallest building, The Austonian, was topped out on September 17, 2009. Austin is currently undergoing a skyscraper boom, which includes recent construction on the now complete 360 Condominiums at , Spring (condominiums), the Austonian at , and several others that are mainly for residential use.
At night, parts of Austin are lit by "artificial moonlight" from Moonlight Towers built to illuminate the central part of the city. The moonlight towers were built in the late 19th century and are now recognized as historic landmarks. Only 15 of the 31 original innovative towers remain standing in Austin, and none remain in any of the other cities where they were installed. The towers are featured in the 1993 film "Dazed and Confused".
Downtown.
The central business district of Austin is home to some of the tallest condo towers in the state, with The Austonian topping out at 56 floors (the tallest residential building in the U.S. west of the Mississippi River) and 360 at 44 floors. Former Mayor Will Wynn set out a goal for having up to 25,000 people living Downtown by 2015, and the city provided incentives for building residential units in the urban core. Because of this, the city has been driven to increase density in Austin's urban core. The skyline has drastically changed in recent years, and the residential real estate market has remained relatively strong.
Downtown growth has been aided by the presence of a popular live music and nightlife scene, museums, restaurants, and Lady Bird Lake, considered one of the city's best recreational spots. The 2nd Street District consists of several new residential projects, restaurants, upscale boutiques and other entertainment venues, as well as Austin's City Hall. Across 2nd Street from Austin's City Hall is the new ACL Live @ the Moody Theatre where the long-running PBS program "Austin City Limits", is filmed. It is located at the base of the new W Hotel. The annual South by Southwest (SXSW) Music, Film and Interactive Festival is located in downtown Austin and includes one of the world's largest music festivals; with more than 3,000 acts from every continent except Antarctica, playing in more than 100 venues, over five days, in March.
Climate.
Under the Köppen climate classification, Austin has a humid subtropical climate. The city is characterized by hot summers and mild winter days and usually cool to cold winter nights. Austin is usually at least partially sunny, receiving nearly 2650 hours, or 60.3% of the possible total, of bright sunshine per year. Summer dewpoint averages at around .
Austin summers are usually hot, with average July and August highs in the high-90s °F (34–36 °C). Highs reach on 116 days per year, and on 18 days per year. The highest recorded temperature was occurring on September 5, 2000, and August 28, 2011.
Winters in Austin are mild and relatively dry. For the entire year, Austin averages 88 days below and 19 days when the minimum temperature falls at or below freezing. The lowest recorded temperature was on January 31, 1949. About every two years or so, Austin experiences an ice storm that freezes roads over and affects much of the city for 24 to 48 hours. When Austin received of ice on January 24, 2014, there were 278 vehicular accidents. 
Snowfall is rare in Austin. A snowfall of on February 4, 2011, caused more than 300 car accidents. A snowstorm brought the city to a near standstill in 1985.
Monthly averages for Austin's weather data are shown in a graphical format to the right, and in a more detailed tabular format below.
2011 drought.
From October 2010 through September 2011, both major reporting stations in Austin, Camp Mabry and Bergstrom Int'l, had the least rainfall of a water year on record, receiving less than a third of normal precipitation. This was a result of La Niña conditions in the eastern Pacific Ocean where water was significantly cooler than normal. David Brown, a regional official with the National Oceanic and Atmospheric Administration, has explained that "these kinds of droughts will have effects that are even more extreme in the future, given a warming and drying regional climate."
Demographics.
According to the 2010 United States Census, the racial composition of Austin is:
At the 2000 United States Census, there were people, households, and families residing in the city (roughly comparable in size to San Francisco, Leeds, UK; and Ottawa, Canada). The population density was . There were housing units at an average density of . There were households out of which 26.8% had children under the age of 18 living with them, 38.1% were married couples living together, 10.8% had a female householder with no husband present, and 46.7% were non-families. 32.8% of all households were made up of individuals and 4.6% had someone living alone who was 65 years of age or older. The average household size was 2.40 and the average family size was 3.14.
In the city the population was spread out with 22.5% under the age of 18, 16.6% from 18 to 24, 37.1% from 25 to 44, 17.1% from 45 to 64, and 6.7% who were 65 years of age or older. The median age was 30 years. For every 100 females there were 105.8 males.
The median income for a household in the city was , and the median income for a family was $. Males had a median income of $ vs. $ for females. The per capita income for the city was $. About 9.1% of families and 14.4% of the population were below the poverty line, including 16.5% of those under age 18 and 8.7% of those age 65 or over. The median house price was $ in 2009, and it has increased every year since 2004. The median value of a house in which the owner occupies it was $227,800 in 2014, which is higher than the average American home value of $175,700. [Census]
A 2014 University of Texas study stated that Austin was the only U.S. city with a fast growth rate between 2000 and 2010 with a net loss in African-Americans. As of 2014, Austin's African-American and Non-Hispanic White share of the total population is declining despite the absolute number of both ethnic groups increasing. Austin's Non-Hispanic White population first dropped below 50% in 2005. The rapid growth of the Hispanic and Asian population has outpaced all other ethnic groups in the city.
According to one survey completed in 2014, it is estimated that at least 5.3% (48,000+) of Austin's residents identify as Lesbian, Gay, Bisexual, or Transgender. Austin had the third highest rate in the nation.
Economy.
The Greater Austin metropolitan statistical area had a Gross Domestic Product of $86 billion in 2010. Austin is considered to be a major center for high tech. Thousands of graduates each year from the engineering and computer science programs at the University of Texas at Austin provide a steady source of employees that help to fuel Austin's technology and defense industry sectors. The region's rapid growth has led "Forbes" to rank the Austin metropolitan area number one among all big cities for jobs for 2012 in their annual survey and WSJ Marketwatch to rank the area number one for growing businesses. By 2013, Austin ranked No. 14 on "Forbes"' list of the Best Places for Business and Careers (directly below Dallas, No. 13 on the list). As a result of the high concentration of high-tech companies in the region, Austin was strongly affected by the dot-com boom in the late 1990s and subsequent bust. Austin's largest employers include the Austin Independent School District, the City of Austin, Dell, the U.S. Federal Government, Freescale Semiconductor (spun off from Motorola in 2004), IBM, St. David's Healthcare Partnership, Seton Family of Hospitals, the State of Texas, the Texas State University, and the University of Texas at Austin.
Other high-tech companies with operations in Austin include 3M, Apple, AMD, Applied Materials, ARM Holdings, Bigcommerce, Bioware, Blizzard Entertainment, Buffalo Technology, Cirrus Logic, Cisco Systems, eBay/PayPal, Electronic Arts, Flextronics, Facebook, Google, Hewlett-Packard, Hoover's, HomeAway, Hostgator, Intel Corporation, National Instruments, Nvidia, Oracle, Polycom, Qualcomm, Inc., Rackspace, RetailMeNot, Rooster Teeth, Samsung Group, Silicon Laboratories, Spansion, Troux Technologies, United Devices, and Xerox. In 2010, Facebook accepted a grant to build a downtown office that could bring as many as 200 jobs to the city. The proliferation of technology companies has led to the region's nickname, "the Silicon Hills", and spurred development that greatly expanded the city.
Austin is also emerging as a hub for pharmaceutical and biotechnology companies; the city is home to about 85 of them. The city was ranked by the Milken Institute as the No.12 biotech and life science center in the United States. Companies such as Hospira, Pharmaceutical Product Development, and ArthroCare Corporation are located there.
Whole Foods Market (often called just "Whole Foods") is an upscale, international grocery store chain specializing in fresh and packaged food products—many having an organic-/local-/"natural"-theme. It was founded and is headquartered in Austin.
Other companies based in Austin include Freescale Semiconductor, Temple-Inland, Sweet Leaf Tea Company, Keller Williams Realty, National Western Life, GSD&M, Dimensional Fund Advisors, Golfsmith, Forestar Group, and EZCorp.
In addition to national and global corporations, Austin features a strong network of independent, unique, locally owned firms and organizations.
Arts and culture.
"Keep Austin Weird" has been a local motto for years, featured on bumper stickers and T-shirts. This motto has not only been used in promoting Austin's eccentricity and diversity, but is also meant to bolster support of local independent businesses. According to the 2010 book, "Weird City", the phrase was begun by a local Austin Community College librarian, Red Wassenich, and his wife, Karen Pavelka, who were concerned about Austin's "rapid descent into commercialism and overdevelopment." The slogan has been interpreted many ways since its inception, but remains an important symbol for many Austinites who wish to voice concerns over rapid growth and irresponsible development. Austin has a long history of vocal citizen resistance to development projects perceived to degrade the environment, or to threaten the natural and cultural landscapes.
According to the Nielsen Company, adults in Austin read and contribute to blogs more than those in any other U.S. metropolitan area. Austin residents have the highest internet usage in all of Texas. Austin was selected as the No. 2 Best Big City in "Best Places to Live" by "Money" magazine in 2006, and No. 3 in 2009, and also the "Greenest City in America" by MSN. According to "Travel & Leisure" magazine, Austin ranks No. 1 on the list of cities with the best people, referring to the personalities and attributes of the citizens. In 2012, the city was listed among the 10 best places to retire in the U.S. by CBS Money Watch.
Recently in 2015, Forbes listed Austin as #1 Boom Town because of its economic strength, including jobs among other appealing attributes.
South Congress is a shopping district stretching down South Congress Avenue from Downtown. This area is home to coffee shops, eccentric stores, restaurants, food trucks, trailers and festivals. It prides itself on "Keeping Austin Weird", especially with development in the surrounding area(s).
Old Austin.
"Old Austin" is an adage often used by the native citizens in Austin, Texas when being nostalgic to refer to the olden days of the capital city of Texas. Although Austin is also known internationally as the live music capital of the world and its catch phrase/slogan Keep Austin Weird can be heard echoed in places as far as Buffalo, NY and Santa Monica, CA - the term "Old Austin" refers to a time when the city was smaller and better known for its lack of traffic, hipsters, and urban sprawl. It is often employed by longtime residents expressing displeasure at the rapidly changing culture.
The growth and popularity of Austin can be seen by the expansive development taking place in its downtown landscape. Forbes ranked Austin as the second fastest-growing city in 2015.
Annual cultural events.
The O. Henry House Museum hosts the annual O. Henry Pun-Off, a pun contest where the successful contestants exhibit wit akin to that of the author William Sydney Porter.
Other annual events include Eeyore's Birthday Party, Spamarama, Austin Gay Pride, the Austin Reggae Festival, Kite Festival, Art City Austin in April, East Austin Studio Tour in November, and Carnaval Brasileiro in February. Sixth Street features annual festivals such as the Pecan Street Festival and Halloween night. The three-day Austin City Limits Music Festival has been held in Zilker Park every year since 2002. Every year around the end of March and the beginning of April, Austin is home to "Texas Relay Weekend."
Austin's Zilker Park Tree is a Christmas display made of lights strung from the top of a Moonlight tower in Zilker Park. The Zilker Tree is lit in December along with the "Trail of Lights," an Austin Christmas tradition. In 2010 and 2011, the Trail of Lights was canceled due to budget shortfalls, but the trail was turned back on for the 2012 holiday season.
Music.
As Austin's official slogan is "The Live Music Capital of the World", the city has a vibrant live music scene with more music venues per capita than any other U.S. city. Austin's music revolves around the many nightclubs on 6th Street and an annual film/music/interactive festival known as South by Southwest (SXSW). The concentration of restaurants, bars, and music venues in the city's downtown core is a major contributor to Austin's live music scene, as the zip code encompassing the downtown entertainment district hosts the most bar or alcohol-serving establishments in the U.S.
The longest-running concert music program on American television, "Austin City Limits", is recorded at ACL Live at The Moody Theater. "Austin City Limits" and C3 Presents produce the Austin City Limits Music Festival, an annual music and art festival held at Zilker Park in Austin. Other music events include the Urban Music Festival, Fun Fun Fun Fest, Chaos In Tejas and Old Settler's Music Festival. Austin Lyric Opera performs multiple operas each year (including the 2007 opening of Philip Glass's "Waiting for the Barbarians", written by University of Texas at Austin alumnus J. M. Coetzee). The Austin Symphony Orchestra performs a range of classical, pop and family performances and is led by Music Director and Conductor Peter Bay.
Film.
Austin hosts several film festivals including SXSW Film Festival and Austin Film Festival, which draws films of many different types from all over the world. In 2004 the city was first in "MovieMaker Magazine's" annual top ten cities to live and make movies.
Austin has been the location for a number of motion pictures, partly due to the influence of The University of Texas at Austin Department of Radio-Television-Film. Films produced in Austin include "The Texas Chain Saw Massacre" (1974), "Songwriter" (1984), "Man of the House", "Secondhand Lions", "Chainsaw Massacre 2", "Nadine", "Waking Life", "Spy Kids","The Faculty", "Dazed and Confused", "Wild Texas Wind", "Office Space", "The Life of David Gale", "Miss Congeniality", "Doubting Thomas", "Slacker", "Idiocracy", "The New Guy", "Hope Floats", "The Alamo", "Blank Check", "The Wendall Baker Story", "School of Rock", "A Slipping-Down Life", "A Scanner Darkly", "Saturday Morning Massacre", and most recently, the Coen brothers' "True Grit", "Grindhouse", "Machete", "How to Eat Fried Worms" and "Bandslam". In order to draw future film projects to the area, the Austin Film Society has converted several airplane hangars from the former Mueller Airport into filmmaking center Austin Studios. Projects that have used facilities at Austin Studios include music videos by The Flaming Lips and feature films such as "25th Hour" and "Sin City". Austin also hosted the MTV series, "" in 2005. The film review websites Spill.com and Ain't It Cool News are based in Austin. Rooster Teeth Productions, creator of popular web series such as "Red vs. Blue", and "RWBY" is also located in Austin.
Theater.
Austin has a strong theater culture, with dozens of itinerant and resident companies producing a variety of work. The city also has live performance theater venues such as the Zachary Scott Theatre Center, Vortex Repertory Company, Salvage Vanguard Theater, Rude Mechanicals' the Off Center, Austin Playhouse, Scottish Rite Children's Theater, Hyde Park Theatre, the Blue Theater, The Hideout Theatre, and Esther's Follies. The Victory Grill was a renowned venue on the Chitlin' circuit. Public art and performances in the parks and on bridges are popular. Austin hosts the Fuse Box Festival each April featuring international, leading-edge theater artists.
The Paramount Theatre, opened in downtown Austin in 1915, contributes to Austin's theater and film culture, showing classic films throughout the summer and hosting regional premieres for films such as "Miss Congeniality". The Zilker Park Summer Musical is a long-running outdoor musical.
The Long Center for the Performing Arts is a 2,300-seat theater built partly with materials reused from the old Lester E. Palmer Auditorium.
Ballet Austin is the fourth largest ballet academy in the country. Each year Ballet Austin's 20-member professional company performs ballets from a wide variety of choreographers, including their international award winning artistic director, Stephen Mills. The city is also home to the Ballet East Dance Company, a modern dance ensemble, and the Tapestry Dance Company which performs a variety of dance genres.
The Austin improvisational theatre scene has several theaters: ColdTowne Theater, The Hideout Theater, The New Movement Theater, and The Institution Theater. Austin also hosts the Out of Bounds Improv Festival, which draws comedic artists in all disciplines to Austin.
Museums and other points of interest.
Museums in Austin include the Texas Memorial Museum, the Blanton Museum of Art (reopened in 2006), the Bob Bullock Texas State History Museum across the street (which opened in 2000), the Austin Museum of Art (AMOA), the Elisabet Ney Museum and the galleries at the Harry Ransom Center. The Texas State Capitol itself is also a major tourist attraction. The Driskill Hotel built in 1886, once owned by George W. Littlefield, and located at 6th and Brazos streets, was finished just before the construction of the Capitol building. Sixth Street is a musical hub for the city. The Enchanted Forest, a multi-acre outdoor music, art, and performance art space in South Austin hosts events such as fire-dancing and circus-like-acts. Austin is also home to the Lyndon Baines Johnson Library and Museum, which houses documents and artifacts related to the Johnson administration, including LBJ's limousine and a re-creation of the Oval Office.
Locally produced art is featured at the South Austin Museum of Popular Culture. The Mexic-Arte Museum is a Latin American art museum founded in 1983. Austin is also home to the O. Henry House Museum, which served as the residence of O. Henry from 1893 to 1895. Farmers' markets are popular attractions, providing a variety of locally grown and often organic foods.
Austin also has many odd statues and landmarks, such as the Stevie Ray Vaughan statue, the Willie Nelson statue, the Mangia dinosaur, the Loca Maria lady at Taco Xpress, the Hyde Park Gym's giant flexed arm, and Daniel Johnston's "Hi, How are You?" Jeremiah the Innocent frog mural.
The Ann W. Richards Congress Avenue Bridge houses the world's largest urban population of Mexican free-tailed bats. Starting in March, up to 1.5 million bats take up residence inside the bridge's expansion and contraction zones as well as in long horizontal grooves running the length of the bridge's underside, an environment ideally suited for raising their young. Every evening around sunset, the bats emerge in search of insects, an exit visible on weather radar. Watching the bat emergence is an event that is popular with locals and tourists, with more than 100,000 viewers per year. The bats migrate to Mexico each winter.
The Austin Zoo, located in unincorporated western Travis County, is a rescue zoo that provides sanctuary to displaced animals from a variety of situations, including those involving neglect.
Sports.
Many Austinites support the athletic programs of the University of Texas at Austin known as the Texas Longhorns. During the 2005–06 academic term, Longhorns football team was named the NCAA Division I FBS National Football Champion, and Longhorns baseball team won the College World Series. The Texas Longhorns play home games in the state's second-largest sports stadium, Darrell K Royal-Texas Memorial Stadium, seating over 101,000 fans. Baseball games are played at UFCU Disch–Falk Field.
Austin is the most populous city in the United States without a club in a major-league professional sports team. Minor-league professional sports came to Austin in 1996, when the Austin Ice Bats began playing at the Travis County Expo Center; they were later replaced by the AHL Texas Stars. Austin now hosts a number of other professional teams, including the Austin Spurs of the NBA Development League, the Austin Aztex of the United Soccer League, the Austin Outlaws in WFA football, and the Austin Aces in WTT tennis.
Natural features like the bicycle-friendly Texas Hill Country and generally mild climate make Austin the home of several endurance and multi-sport races and communities. The Capitol 10,000 is the largest race in Texas, and approximately fifth largest in the United States. The Austin Marathon has been run in the city every year since 1992. Additionally the city is home to the largest 5 mile race in Texas, named the Turkey Trot as it is run annually on thanksgiving. Started in 1991 by Thundercloud Subs, a local sandwich chain (who still sponsors the event), the event has grown to host over 20,000 runners. All proceeds are donated to Caritas of Austin, a local charity.
The Austin-founded American Swimming Association hosts several swim races around town. Austin is also the hometown of several cycling groups and the former seven-time Tour de France champion cyclist Lance Armstrong. Combining these three disciplines is a growing crop of triathlons, including the Capital of Texas Triathlon held every Memorial Day on and around Lady Bird Lake, Auditorium Shores, and Downtown Austin.
In June 2010 it was announced that the Austin area would host the Formula One, United States Grand Prix, from 2012 until 2021. The State pledged $25 million in public funds annually for 10 years to pay the sanctioning fees for the race. A Formula One circuit was built at an estimated cost of $250 to $300 million, and is located just east of the Austin Bergstrom International Airport. Circuit of the Americas also plays host to MotoGP World Championships from 2013.
The summer of 2014 marked the inaugural season for World TeamTennis team Austin Aces, formerly Orange County Breakers of the southern California region. Austin Aces play their matches at the Cedar Park Center northwest of Austin, and feature former professionals Andy Roddick and Marion Bartoli, as well as current WTA tour player Vera Zvonareva.
Parks and recreation.
The Austin Parks and Recreation Department received the Excellence in Aquatics award in 1999 and the Gold Medal Awards in 2004 from the National Recreation and Park Association. Home to more than 50 public swimming pools, Austin has parks and pools throughout the city. There are several well-known swimming locations. These include Deep Eddy Pool, Texas' oldest man-made swimming pool, and Barton Springs Pool, the nation's largest natural swimming pool in an urban area. Barton Springs Pool is spring-fed while Deep Eddy is well-fed. Both range in temperature from about during the winter to about during the summer. Hippie Hollow Park, a county park situated along Lake Travis, is the only officially sanctioned clothing-optional public park in Texas. Activities include rockclimbing, kayaking, swimming, mountain biking, exploring, and hiking along the greenbelt, a long-spanning area that runs through the city. Some well known naturally forming swimming holes along Austin's greenbelt include Twin Falls, Sculpture Falls and Campbell's Hole. Zilker Park, a large green area close to downtown, forms part of the greenbelt along the Colorado River. Hamilton Pool is a pool and wildlife park located about 30 minutes from the city.
To strengthen the region’s parks system, which spans more than , The Austin Parks Foundation (APF) was established in 1992 to develop and improve parks in and around Austin. APF works to fill the city’s park funding gap by leveraging volunteers, philanthropists, park advocates and strategic collaborations to develop, maintain and enhance Austin's parks, trails and green spaces. APF fosters innovative public/private partnerships and since 2006, has given over 145 grants totaling more than $2 million in service to the greater Austin community.
Government and law.
City government.
Austin is administered by an 11-member city council (10 council members elected by geographic district plus a mayor elected at large). The council is accompanied by a hired city manager under the manager-council system of municipal governance. Council and mayoral elections are non-partisan, with a runoff in case there is no majority winner. A referendum approved by voters on November 6, 2012 changed the council composition from six council members plus a mayor elected at large to the current "10+1" district system. November 2014 marked the first election under the new system.
Austin formerly operated its city hall at 128 West 8th Street. Antoine Predock and Cotera Kolar Negrete & Reed Architects designed a new city hall building, which was intended to reflect what "The Dallas Morning News" referred to as a "crazy-quilt vitality, that embraces everything from country music to environmental protests and high-tech swagger." The new city hall, built from recycled materials, has solar panels in its garage. The city hall, at 301 West Second Street, opened in November 2004. The mayor of Austin is Steve Adler.
Law enforcement in Austin is provided by the Austin Police Department, except for state government buildings, which are patrolled by the Texas Department of Public Safety. The University of Texas Police operate from the University of Texas.
Fire protection within the city limits is provided by the Austin Fire Department, while the surrounding county is divided into twelve geographical areas known as Emergency Services Districts, which are covered by separate regional fire departments. Emergency Medical Services are provided for the whole county by "Austin-Travis County Emergency Medical Services".
State and federal representation.
The Texas Department of Transportation operates the Austin District Office in Austin.
The Texas Department of Criminal Justice (TDCJ) operates the Austin I and Austin II district parole offices in Austin.
The United States Postal Service operates several post offices in Austin.
Politics.
Austin is known as an enclave of liberal politics in a generally conservative state—so much so, that the city is sometimes sarcastically called the "People's Republic of Austin" by residents of other parts of Texas, and conservatives in the Texas Legislature.
Since redistricting following the 2010 United States Census, Austin has been divided between six congressional districts at the federal level: Texas's 35th, Texas's 25th, Texas's 10th, Texas's 21st, Texas's 17th, and Texas's 31st. Texas's 35th congressional district is represented by Democrat Lloyd Doggett. The other five districts are represented by Republicans, of whom only one, Michael McCaul of the 10th district, lives in Austin.
As a result of the major party realignment that began in the 1970s, central Austin became a stronghold of the Democratic Party, while the suburbs tend to vote Republican. A controversial turning point in the political history of the Austin area was the 2003 Texas redistricting. Opponents characterized the resulting district layout as excessively partisan gerrymandering, and the plan was challenged in court by Democratic and minority activists; of note, the Supreme Court of the United States has never struck down a redistricting plan for being excessively partisan. The plan was subsequently upheld by a three-judge federal panel in late 2003, and on June 28, 2006, the matter was largely settled when the Supreme Court, in a 7–2 decision, upheld the entire congressional redistricting plan with the exception of a Hispanic-majority district in southwest Texas. This affected Austin's districting, as U.S. Rep. Lloyd Doggett's district (U.S. Congressional District 25) was found to be insufficiently compact to compensate for the reduced minority influence in the southwest district; it was redrawn so that it took in most of southeastern Travis County and several counties to its south and east.
Overall, the city is a blend of downtown liberalism and suburban conservatism but leans to the political left as a whole. The city last went to a Republican candidate in 2000 when former Texas Governor George W. Bush successfully ran for President. In 2004, the Democrats rebounded strongly as John Kerry enjoyed a 14.0% margin over Bush, who once again won Texas.
City residents have been supportive of alternative candidates; for example, Ralph Nader won 10.4% of the vote in Austin in 2000.
In 2003, the city adopted a resolution against the USA PATRIOT Act that reaffirmed constitutionally guaranteed rights. Of Austin's six state legislative districts, three are strongly Democratic and three are swing districts, two of which are held by Democrats and one of which is held by a Republican. However, two of its three congressional districts (the 10th and the 21st) are presently held by Republicans, with only the 25th held by a Democrat. This is largely due to the 2003 redistricting, which left downtown Austin without an exclusive congressional seat of its own. Travis County was also the only county in Texas to reject Texas Constitutional Amendment Proposition 2 that effectively outlawed gay marriage and status equal or similar to it and did so by a wide margin (40% for, 60% against).
Two of the candidates for president in the 2004 race called Austin home. Michael Badnarik, the Libertarian Party candidate, and David Cobb of the Green Party both had lived in Austin. During the run up to the election in November, a presidential debate was held at the University of Texas at Austin student union involving the two candidates. While the Commission on Presidential Debates only invites Democrats and Republicans to participate in televised debates, the debate at UT was open to all presidential candidates. Austin also hosted one of the last presidential debates between Barack Obama and Hillary Clinton during their heated race for the Democratic nomination in 2008.
In the 2012 Presidential election, Travis County, which contains the majority of Austin, voted to re-elect President Barack Obama (D) by a 24-point margin (60.1% to 36.2%).
Environmental movement.
The distinguishing political movement of Austin politics has been that of the environmental movement, which spawned the parallel neighborhood movement, then the more recent conservationist movement (as typified by the Hill Country Conservancy), and eventually the current ongoing debate about "sense of place" and preserving the Austin quality of life. Much of the environmental movement has matured into a debate on issues related to saving and creating an Austin "sense of place." In 2012, Austin became just one of a few cities in Texas to ban the sale and use of plastic bags.
Education.
Researchers at Central Connecticut State University ranked Austin the 16th most literate city in the United States for 2008. The Austin Public Library operates the John Henry Faulk Library and various library branches. In addition, the University of Texas at Austin operates the seventh-largest academic library in the nation.
Austin was voted "America's No.1 College Town" by the Travel Channel. Over 43 percent of Austin residents age 25 and over hold a bachelor's degree, while 16 percent hold a graduate degree. In 2009, greater Austin ranked eighth among metropolitan areas in the United States for bachelor's degree attainment with nearly 39 percent of area residents over 25 holding a bachelor's degree.
Higher education.
Austin is home to the University of Texas at Austin, the flagship institution of the University of Texas System with over 38,000 undergraduate students and 12,000 graduate students. In 2015 rankings, the university was ranked 53rd among "National Universities" (17th among public universities) by "U.S. News & World Report." UT has annual research expenditures of over $595 million and has the highest-ranked business, engineering, and law programs of any university in the state of Texas.
Other institutions of higher learning in Austin include St. Edward's University, Huston-Tillotson University, Austin Community College, Concordia University, the Seminary of the Southwest, the Acton School of Business, Texas Health and Science University, Austin Graduate School of Theology, Austin Presbyterian Theological Seminary, Virginia College's Austin Campus, The Art Institute of Austin, Southern Careers Institute of Austin, Austin Conservatory and a branch of Park University.
Public primary and secondary education.
The Austin area has 29 public school districts, 17 charter schools and 69 private schools. Most of the city is served by the Austin Independent School District. This district includes notable schools such as the magnet Liberal Arts and Science Academy High School of Austin, Texas (LASA), which, by test scores, has consistently been within the top thirty high schools in the nation, as well as The Ann Richards School for Young Women Leaders. Some parts of Austin are served by other districts, including Round Rock, Pflugerville, Leander, Manor, Del Valle, Lake Travis, Dripping Springs, Hays, and Eanes ISDs. Four of the metro's major public school systems, representing 54% of area enrollment, are included in "Expansion Management" magazine's latest annual education quality ratings of nearly 2,800 school districts nationwide. Two districts—Eanes and Round Rock—are rated "gold medal", the highest of the magazine's cost-performance categories.
Private and alternative education.
Private and alternative education institutions for children in preschool-12th grade include ACE Academy, Regents School of Austin, Redeemer Lutheran School, Garza (public), Austin Discovery School (public charter), Austin Jewish Academy, Austin Peace Academy, The Austin School for the Performing and Visual Arts, The Austin Waldorf School, The Griffin School, The Khabele School, Concordia Academy, Kirby Hall School, St. Ignatius Martyr Catholic School,Holy Family Catholic School, San Juan Diego Catholic High School, Brentwood Christian School, Renaissance Academy, St. Austin Catholic School, St. Francis School, St. Stephen's Episcopal School, St. Mary's, St. Theresa's, St. Michael's Catholic Academy, St. Gabriel's Catholic School, St. Andrew's Episcopal School, St. Francis Episcopal School, St. Paul Lutheran School, Trinity Episcopal School, Huntington-Surrey, Cleaview Sudbury School, Inside Outside School, Paragon Preparatory Middle School, Austin International School, Progress School, Bronze Doors Academy, and a number of Montessori schools.
Along with homeschooling & "unschooling" communities, Austin is home to a number of part-time learning environments designed to offer basic academics and inspired mentoring. Such current resources include the Whole Life Learning Center and AHB Community School.
Austin is also home to child developmental institutions including the Center for Autism and Related Disorders, the Central Texas Autism Center, Johnson Center for Child Health and Development and many more.
Media.
Austin's main daily newspaper is the "Austin American-Statesman". "The Austin Chronicle" is Austin's alternative weekly, while "The Daily Texan" is the student newspaper of the University of Texas at Austin. Austin's business newspaper is the weekly "Austin Business Journal". Austin also has numerous smaller special interest or sub-regional newspapers such as the "Oak Hill Gazette", "Westlake Picayune", "Hill Country News", "Round Rock Leader", "NOKOA", and "The Villager" among others. "Texas Monthly", a major regional magazine, is also headquartered in Austin. The "Texas Observer", a muckraking biweekly political magazine, has been based in Austin for over five decades. The weekly "Community Impact Newspaper" newspaper published by John Garrett, former publisher of the "Austin Business Journal" has five regional editions and is delivered to every house and business within certain zip codes and all of the news is specific to those zip codes. The most recent entrant on the Austin news scene is "The Texas Tribune", an on-line publication focused on Texas and Austin politics. The "Tribune" is "user-supported" through donations, a business model similar to public radio. The Editor is Evan Smith, former Editor of "Texas Monthly". Smith co-founded the "Texas Tribune", a nonprofit, non-partisan public media organization, with Austin venture capitalist John Thornton and veteran journalist Ross Ramsey.
Commercial radio stations include KASE-FM (country), KVET (sports), KVET-FM (country), KKMJ-FM (adult contemporary), KLBJ (talk), KLBJ-FM (classic rock), KTAE (Christian talk), KFMK (contemporary Christian), KOKE-FM (progressive country) and KPEZ (rhythmic contemporary). KUT is the leading public radio station in Texas and produces the majority of its content locally. KOOP (FM) is a volunteer-run radio station with more than 60 locally produced programs. KVRX is the student-run college radio station of the University of Texas at Austin with a focus on local and non-mainstream music and community programming. Other listener-supported stations include KAZI (urban contemporary), and KMFA (classical)
Network television stations (affiliations in parentheses) include KTBC (Fox O&O), KVUE (ABC), KXAN (NBC), KEYE-TV (CBS), KLRU (PBS), KNVA (The CW), KBVO (My Network TV), and KAKW (Univision O&O). KLRU produces several award winning locally produced programs such as "Austin City Limits".
Alex Jones, journalist, radio show host and filmmaker, produces his talk show The Alex Jones Show in Austin which broadcasts nationally on more than 60 AM and FM radio stations in the United States, WWCR Radio shortwave and XM Radio: Channel 166.
Transportation.
Of all the people who work in Austin, 73% drive alone, 10% carpool, 6% work from home, 5% take the bus, 2% walk, and 1% bicycle.
Highways.
Central Austin lies between two major north-south freeways: Interstate 35 to the east and the Mopac Expressway (Loop 1) to the west. U.S. Highway 183 runs from northwest to southeast, and State Highway 71 crosses the southern part of the city from east to west, completing a rough "box" around central and north-central Austin. Austin is the largest city in the United States to be served by only one Interstate Highway.
U.S. Highway 290 enters Austin from the east and merges into Interstate 35. Its highway designation continues south on I-35 and then becomes part of Highway 71, continuing to the west. Highway 290 splits from Highway 71 in southwest Austin, in an interchange known as "The Y." Highway 71 continues to Brady, Texas, and Highway 290 continues west to intersect Interstate 10 near Junction. Interstate 35 continues south through San Antonio to Laredo on the Texas-Mexico border. Interstate 35 is the highway link to the Dallas-Fort Worth metroplex in northern Texas. There are two links to Houston, Texas (Highway 290 and State Highway 71/Interstate 10). Highway 183 leads northwest of Austin toward Lampasas.
In the mid-1980s, construction was completed on Loop 360, a scenic highway that curves through the hill country from near the 71/Mopac interchange in the south to near the 183/Mopac interchange in the north. The iconic Pennybacker Bridge, also known as the "360 Bridge", crosses Lake Austin to connect the northern and southern portions of Loop 360.
Tollways.
State Highway 130 is a bypass route designed to relieve traffic congestion, starting from Interstate 35 just north of Georgetown and running along a parallel route to the east, where it bypasses Round Rock, Austin, San Marcos and New Braunfels before ending at Interstate 10 east of Seguin, where drivers could drive west to return to Interstate 35 in San Antonio. The first segment was opened in November 2006, which was located east of Austin–Bergstrom International Airport at Austin's southeast corner on State Highway 71. Highway 130 runs concurrently with Highway 45 from Pflugerville on the north until it reaches US 183 well south of Austin, where it splits off and goes west. The entire route of State Highway 130 is now complete with last leg, which opened on November 1, 2012. The highway is noted for having the entire route with a speed limit of at least . The 41-mile section of the toll road between Mustang Ridge and Seguin has a posted speed limit of , the highest posted speed limit in the United States.
State Highway 45 runs east-west from just south of Highway 183 in Cedar Park to 130 inside Pflugerville (just east of Round Rock). A tolled extension of State Highway Loop 1 was also created. A new southeast leg of Highway 45 has recently been completed, running from US 183 and the south end of Segment 5 of TX-130 south of Austin due west to I-35 at the FM 1327/Creedmoor exit between the south end of Austin and Buda. The 183A Toll Road opened March 2007, providing a tolled alternative to U.S. 183 through the cities of Leander and Cedar Park. Currently under construction is a change to East US 290 from US 183 to the town of Manor. Officially, the tollway will be dubbed Tollway 290 with the Manor Expressway as a nickname.
Despite the overwhelming initial opposition to the toll road concept when it was first announced, all three toll roads have exceeded revenue projections.
Airports.
Austin's airport is Austin–Bergstrom International Airport (ABIA) ( AUS), located southeast of the city. The airport is on the site of the former Bergstrom Air Force Base, which was closed in 1993 as part of the Base Realignment and Closure process. Previously, Robert Mueller Municipal Airport was the commercial airport of Austin. Austin Executive Airport serves the general aviation coming into the city, as well as other smaller airports outside of the city centre.
Intercity bus service.
Greyhound Lines operates the Austin Station at 916 East Koenig Lane, just east of Airport Boulevard and adjacent to Highland Mall. Turimex Internacional operates bus service from Austin to Nuevo Laredo and on to many destinations in Mexico. The Turimex station is located at 5012 East 7th Street, near Shady Lane.
Megabus offers daily service to San Antonio, Dallas/Fort Worth and Houston from a stop at Dobie Center.
Public transportation.
Capital Metropolitan Transportation Authority Capital Metro provides public transportation to the city, primarily by bus. Capital Metro is planning to change some routes to "Rapid Lines." The lines will feature long, train-like, high-tech buses. This addition is going to be implemented to help reduce congestion. Capital Metro opened a commuter rail system known as Capital MetroRail on March 22, 2010. The system was built on existing freight rail lines and serves downtown Austin, East Austin, North Central Austin, Northwest Austin, and Leander in its first phase. Future expansion could include a line to Manor and another to Round Rock. Capital Metro is also looking into a light rail system to connect most of Downtown, the University of Texas at Austin, and the Mueller Airport Redevelopment. The light rail system would help connect the MetroRail line to key destinations in Central Austin. On August 7, 2014, the Austin City Council unanimously voted to place a $600 million light rail bond proposal on the November 4, 2014 ballot. Implementation of this package is contingent on matching funding from Federal transit grants. If Federal funding is available, then Austin would begin construction of a light rail line that would run from Riverside Drive to the Highland Austin Community College Campus.
Capital Area Rural Transportation System connects Austin with outlying suburbs.
An Amtrak "Texas Eagle" station is located west of downtown. Segments of the Amtrak route between Austin and San Antonio are under evaluation for a future regional passenger rail corridor as an alternative to the traffic congestion of Interstate 35. This is a multi jurisdictional project called Lone Star Rail. Austin is also home to Car2Go, a carsharing program. Austin was chosen as the first city in the western hemisphere to host this company's business, which is based in Germany.
Cycling.
Austin is known as the most bike-friendly city in Texas, and was ranked the #11 city in the US by Bicycling Magazine in 2010.
Austin has a Silver-level rating from the League of American Bicyclists. There are over 80 miles of bike lanes in Austin. Over 2% of commuters get to work by bike and many more Austinites ride for daily transportation needs, according to the American Community Survey. The North Loop neighborhood along with the Manor Road area have the highest bike commuting rates, with over 13% of residents biking to work in 2012. Biking is also very popular recreationally with the extensive network of trails in the city.
The city's bike advocacy organization is Bike Austin. Bike Texas, a state-level advocacy also has its main office in Austin.
Bicycles are a popular transportation choice among students, faculty, and staff at the University of Texas, Austin. According to a survey done at UT, 9% of commuters bike to campus.
Walkability.
A 2013 study by Walk Score ranked Austin 35th most walkable of the 50 largest U.S. cities. This is considered a medium low ranking.
Sister cities.
List of sister cities of Austin, Texas, designated by Sister Cities International.
The cities of Belo Horizonte, Brazil and Elche, Spain were formerly sister cities, but upon a vote of the Austin City Council in 1991, their status was de-activated.

</doc>
<doc id="2003" url="https://en.wikipedia.org/wiki?curid=2003" title="Argument from morality">
Argument from morality

The argument from morality is an argument for the existence of God. Arguments from morality tend to be based on moral normativity or moral order. Arguments from moral normativity observe some aspect of morality and argue that God is the best or only explanation for this, concluding that God must exist. Argument from moral order are based on the asserted need for moral order to exist in the universe. They claim that, for this moral order to exist, God must exist to support it. The argument from morality is noteworthy in that one cannot evaluate the soundness of the argument without attending to almost every important philosophical issue in metaethics.
German philosopher Immanuel Kant devised an argument from morality based on practical reason. Kant argued that the goal of humanity is to achieve perfect happiness and virtue (the summum bonum) and believed that an afterlife must exist in order for this to be possible, and that God must exist to provide this. In his book "Mere Christianity", C. S. Lewis argued that "conscience reveals to us a moral law whose source cannot be found in the natural world, thus pointing to a supernatural Lawgiver." Lewis argued that accepting the validity of human reason as a given must include accepting the validity of practical reason, which could not be valid without reference to a higher cosmic moral order which could not exist without a God to create and/or establish it. A related argument is from conscience; John Henry Newman argued that the conscience supports the claim that objective moral truths exist because it drives people to act morally even when it is not in their own interest. Newman argued that, because the conscience suggests the existence of objective moral truths, God must exist to give authority to these truths.
General form.
All variations of the argument from morality begin with an observation about moral thought or experiences and conclude with the existence of God. Some of these arguments propose moral facts which they claim evident through human experience, arguing that God is the best explanation for these. Other versions describe some end which humans should strive to attain, only possible if God exists.
Many arguments from morality are based on moral normativity, which suggests that objective moral truths exist and require God's existence to give them authority. Often, they consider that morality seems to be binding – obligations are seen to convey more than just a preference, but imply that the obligation will stand, regardless of other factors or interests. For morality to be binding, God must exist. In its most general form, the argument from moral normativity is:
Some arguments from moral order suggest that morality is based on rationality and that this can only be the case if there is a moral order in the universe. The arguments propose that only the existence of God as orthodoxly conceived could support the existence of moral order in the universe, so God must exist. Alternative arguments from moral order have proposed that we have an obligation to attain the perfect good of both happiness and moral virtue. They attest that whatever we are obliged to do must be possible, and achieving the perfect good of both happiness and moral virtue is only possible if a natural moral order exists. A natural moral order requires the existence of God as orthodoxly conceived, so God must exist.
Variations.
Practical Reason.
In his "Critique of Pure Reason", German philosopher Immanuel Kant stated that no successful argument for God's existence arises from reason alone. In his "Critique of Practical Reason" he went on to argue that, despite the failure of these arguments, morality requires that God's existence is assumed, owing to practical reason. Rather than proving the existence of God, Kant was attempting to demonstrate that all moral thought requires the assumption that God exists. Kant argued that humans are obliged to bring about the "summum bonum": the two central aims of moral virtue and happiness, where happiness arises out of virtue. As ought implies can, Kant argued, it must be possible for the "summum bonum" to be achieved. He accepted that it is not within the power of humans to bring the "summum bonum" about, because we cannot ensure that virtue always leads to happiness, so there must be a higher power who has the power to create an afterlife where virtue can be rewarded by happiness.
Philosopher G. H. R. Parkinson notes a common objection to Kant's argument: that what ought to be done does not necessarily entail that it is possible. He also argues that alternative conceptions of morality exist which do not rely on the assumptions that Kant makes – he cites utilitarianism as an example which does not require the "summum bonum". Nichola Everitt argues that much moral guidance is unattainable, such as the Biblical command to be Christ-like. She proposes that Kant's first two premises only entail that we must try to achieve the perfect good, not that it is actually attainable.
Argument from objective moral truths.
Both theists and non-theists have accepted that the existence of objective moral truths might entail the existence of God. Atheist philosopher J. L. Mackie accepted that, if objective moral truths existed, they would warrant a supernatural explanation. Scottish philosopher W. R. Sorley presented the following argument:
Many critics have challenged the second premise of this argument, by offering a biological and sociological account of the development of human morality which suggests that it is neither objective nor absolute. This account, supported by biologist E. O. Wilson and philosopher Michael Ruse, proposes that the human experience of morality is a by-product of natural selection, a theory philosopher Mark D. Linville calls evolutionary naturalism. According to the theory, the human experience of moral obligations was the result of evolutionary pressures, which attached a sense of morality to human psychology because it was useful for moral development; this entail that moral values do not exist independently of the human mind. Morality might be better understood as an evolutionary imperative in order to propagate genes and ultimately reproduce. No human society today advocates immorality, such as theft or murder, because it would undoubtedly lead to the end of that particular society and any chance for future survival of offspring. Scottish empiricist David Hume made a similar argument, that belief in objective moral truths is unwarranted and to discuss them is meaningless.
Because evolutionary naturalism proposes an empirical account of morality, it does not require morality to exist objectively; Linville considers the view that this will lead to moral scepticism or antirealism. C. S. Lewis argued that, if evolutionary naturalism is accepted, human morality cannot be described as absolute and objective because moral statements cannot be right or wrong. Despite this, Lewis argued, those who accept evolutionary naturalism still act as if objective moral truths exist, leading Lewis to reject naturalism as incoherent. As an alternative ethical theory, Lewis offered a form of divine command theory which equated God with goodness and treated goodness as an essential part of reality, thus asserting God's existence.
J.C.A. Gaskin challenges the first premise of the argument from moral objectivity, arguing that it must be shown why absolute and objective morality entails that morality is commanded by God, rather than simply a human invention. It could be the consent of humanity that gives it moral force, for example. American philosopher Michael Martin argues that it is not necessarily true that objective moral truths must entail the existence of God, suggesting that there could be alternative explanations: he argues that naturalism may be an acceptable explanation and, even if a supernatural explanation is necessary, it does not have to be God (polytheism is a viable alternative). Martin also argues that a non-objective account of ethics might be acceptable and challenges the view that a subjective account of morality would lead to moral anarchy.
Conscience.
Related to the argument from morality is the argument from conscience, associated with eighteenth-century bishop Joseph Butler and nineteenth-century cardinal John Henry Newman. Newman proposed that the conscience, as well as giving moral guidance, provides evidence of objective moral truths which must be supported by the divine. He argued that emotivism is an inadequate explanation of the human experience of morality because people avoid acting immorally, even when it might be in their interests. Newman proposed that, to explain the conscience, God must exist.
British philosopher John Locke argued that moral rules cannot be established from conscience because the differences in people's consciences would lead to contradictions. Locke also noted that the conscience is influenced by "education, company, and customs of the country", a criticism mounted by J. L. Mackie, who argued that the conscience should be seen as an "introjection" of other people into an agent's mind. Michael Martin challenges the argument from conscience with a naturalistic account of conscience, arguing that naturalism provides an adequate explanation for the conscience without the need for God's existence. He uses the example of the internalisation by humans of social pressures, which leads to the fear of going against these norms. Even if a supernatural cause is required, he argues, it could be something other than God; this would mean that the phenomenon of the conscience is no more supportive of monotheism than polytheism.

</doc>
<doc id="2004" url="https://en.wikipedia.org/wiki?curid=2004" title="ASL (disambiguation)">
ASL (disambiguation)

ASL is a common initialism for American Sign Language, the sign language of the United States and Canada, and may also refer to:

</doc>
<doc id="2006" url="https://en.wikipedia.org/wiki?curid=2006" title="Auschwitz concentration camp">
Auschwitz concentration camp

Auschwitz concentration camp ( ) was a network of German Nazi concentration camps and extermination camps built and operated by the Third Reich in Polish areas annexed by Nazi Germany during World War II. It consisted of Auschwitz I (the original camp), Auschwitz II–Birkenau (a combination concentration/extermination camp), Auschwitz III–Monowitz (a labor camp to staff an IG Farben factory), and 45 satellite camps.
Auschwitz I was first constructed to hold Polish political prisoners, who began to arrive in May 1940. The first extermination of prisoners took place in September 1941, and Auschwitz II–Birkenau went on to become a major site of the Nazi "Final Solution to the Jewish question". From early 1942 until late 1944, transport trains delivered Jews to the camp's gas chambers from all over German-occupied Europe, where they were killed with the pesticide Zyklon B. At least 1.1 million prisoners died at Auschwitz, around 90 percent of them Jewish; approximately 1 in 6 Jews killed in the Holocaust died at the camp. Others deported to Auschwitz included 150,000 Poles, 23,000 Romani and Sinti, 15,000 Soviet prisoners of war, 400 Jehovah's Witnesses, and tens of thousands of others of diverse nationalities, including an unknown number of homosexuals. Many of those not killed in the gas chambers died of starvation, forced labor, infectious diseases, individual executions, and medical experiments.
In the course of the war, the camp was staffed by 7,000 members of the German "Schutzstaffel" (SS), approximately 12 percent of whom were later convicted of war crimes. Some, including camp commandant Rudolf Höss, were executed. The Allied Powers refused to believe early reports of the atrocities at the camp, and their failure to bomb the camp or its railways remains controversial. One hundred forty-four prisoners are known to have escaped from Auschwitz successfully, and on October 7, 1944, two "Sonderkommando" units—prisoners assigned to staff the gas chambers—launched a brief, unsuccessful uprising.
As Soviet troops approached Auschwitz in January 1945, most of its population was evacuated and sent on a death march. The prisoners remaining at the camp were liberated on January 27, 1945, a day now commemorated as International Holocaust Remembrance Day. In the following decades, survivors, such as Primo Levi, Viktor Frankl, and Elie Wiesel, wrote memoirs of their experiences in Auschwitz, and the camp became a dominant symbol of the Holocaust. In 1947, Poland founded a museum on the site of Auschwitz I and II, and in 1979, it was named a UNESCO World Heritage Site.
History.
Background.
Discrimination against Jews began immediately after the Nazi seizure of power in Germany on January 30, 1933. The Law for the Restoration of the Professional Civil Service, passed on April 7 that year, excluded most Jews from the legal profession and the civil service. Similar legislation soon deprived Jewish members of other professions of the right to practise. Violence and economic pressure were used by the regime to encourage Jews to leave the country voluntarily. Jewish businesses were denied access to markets, forbidden to advertise in newspapers, and deprived of access to government contracts. Citizens were harassed and subjected to violent attacks and boycotts of their businesses.
In September 1935 the Nuremberg Laws were enacted. These laws prohibited marriages between Jews and people of Germanic extraction, extramarital relations between Jews and Germans, and the employment of German women under the age of 45 as domestic servants in Jewish households. The Reich Citizenship Law stated that only those of Germanic or related blood were defined as citizens. Thus Jews and other minority groups were stripped of their German citizenship. By the start of World War II in 1939, around 250,000 of Germany's 437,000 Jews emigrated to the United States, Palestine, the United Kingdom, and other countries.
The ideology of Nazism brought together elements of antisemitism, racial hygiene, and eugenics, and combined them with pan-Germanism and territorial expansionism with the goal of obtaining more "Lebensraum" (living space) for the Germanic people. Nazi Germany attempted to obtain this new territory by invading Poland and the Soviet Union, intending to deport or kill the Jews and Slavs living there, who were viewed as being inferior to the Aryan master race. After the invasion of Poland in September 1939, German dictator Adolf Hitler ordered that the Polish leadership and intelligentsia should be destroyed. Approximately 65,000 civilians were killed by the end of 1939. In addition to leaders of Polish society, the Nazis killed Jews, prostitutes, Romani, and the mentally ill. SS-"Obergruppenführer" (Senior Group Leader) Reinhard Heydrich, then head of the Gestapo, ordered on September 21 that Jews should be rounded up and concentrated into cities with good rail links. Initially the intention was to deport the Jews to points further east, or possibly to Madagascar.
Auschwitz I.
After this part of Poland was annexed by Nazi Germany, Oświęcim (Auschwitz) was located administratively in Germany, Province of Upper Silesia, Regierungsbezirk Kattowitz, Landkreis Bielitz. It was first suggested as a site for a concentration camp for Polish prisoners by "SS-Oberführer" Arpad Wigand, an aide to Higher SS and Police Leader for Silesia, Erich von dem Bach-Zelewski. Bach-Zelewski had been searching for a site to house prisoners in the Silesia region, as the local prisons were filled to capacity. Richard Glücks, head of the Concentration Camps Inspectorate, sent former Sachsenhausen concentration camp commandant Walter Eisfeld to inspect the site, which already held sixteen dilapidated one-story buildings that had once served as an Austrian and later Polish Army barracks and a camp for transient workers. "Reichsführer-SS" Heinrich Himmler, head of the "Schutzstaffel" (SS), approved the site in April 1940, intending to use the facility to house political prisoners. SS-"Obersturmbannführer" (lieutenant colonel) Rudolf Höss oversaw the development of the camp and served as the first commandant. SS-"Obersturmführer" (senior lieutenant) Josef Kramer was appointed Höss's deputy. Auschwitz I, the original camp, became the administrative center for the whole complex.
Local residents were evicted, including 1,200 people who lived in shacks around the barracks. Around 300 Jewish residents of Oświęcim were brought in to lay foundations. From 1940 to 1941, 17,000 Polish and Jewish residents of the western districts of Oświęcim were expelled from places adjacent to the camp. The Germans also ordered expulsions from the villages of Broszkowice, Babice, Brzezinka, Rajsko, Pławy, Harmęże, Bór, and Budy. German citizens were offered tax concessions and other benefits if they would relocate to the area. By October 1943, more than 6,000 Reich Germans had arrived. The Nazis planned to build a model modern residential area for incoming Germans, including schools, playing fields, and other amenities. Some of the plans went forward, including the construction of several hundred apartments, but many were never fully implemented. Basic amenities such as water and sewage disposal were inadequate, and water-borne illnesses were commonplace.
The first prisoners (30 German criminal prisoners from the Sachsenhausen concentration camp) arrived in May 1940, intended to act as functionaries within the prison system. The first mass transport to Auschwitz concentration camp, which included Catholic prisoners, suspected members of the resistance, and 20 Jews, arrived from the prison in Tarnów, Poland, on June 14, 1940. They were interned in the former building of the Polish Tobacco Monopoly, adjacent to the site, until the camp was ready.
The inmate population grew quickly as the camp absorbed Poland's intelligentsia and dissidents, including the Polish underground resistance. By March 1941, 10,900 were imprisoned there, most of them Poles. By the end of 1940, the SS had confiscated land in the surrounding area to create a "zone of interest" surrounded by a double ring of electrified barbed wire fences and watchtowers. Like other Nazi concentration camps, the gates to Auschwitz I displayed the motto "Arbeit macht frei" ("Work brings freedom").
Auschwitz II-Birkenau.
The initial victories of Operation Barbarossa in the summer and fall of 1941 against Hitler's new enemy, the Soviet Union, led to dramatic changes in Nazi anti-Jewish ideology and the profile of prisoners brought to Auschwitz. Construction on Auschwitz II-Birkenau began in October 1941 to ease congestion at the main camp. "Reichsführer-SS" Heinrich Himmler, head of the "Schutzstaffel" (SS), intended the camp to house 50,000 prisoners of war, who would be interned as forced laborers. Plans called for the expansion of the camp first to house 150,000 and eventually as many as 200,000 inmates. An initial contingent of 10,000 Soviet prisoners of war arrived at Auschwitz I in October 1941, but by March 1942 only 945 were still alive, and these were transferred to Birkenau, where most of them died from disease or starvation by May. By this time Hitler had decided to annihilate the Jewish people, so Birkenau was repurposed as a combination labor camp / extermination camp.
The chief of construction of Auschwitz II-Birkenau was Karl Bischoff. Unlike his predecessor, he was a competent and dynamic bureaucrat who, in spite of the ongoing war, carried out the construction deemed necessary. The Birkenau camp, the four crematoria, a new reception building, and hundreds of other buildings were planned and realized. Bischoff's plans initially called for each barrack to have an occupancy of 550 prisoners (one-third of the space allotted in other Nazi concentration camps). He later changed this to 744 prisoners per barrack. The SS designed the barracks not so much to house people as to destroy them.
The first gas chamber at Birkenau was the "red house" (called Bunker 1 by SS staff), a brick cottage converted into a gassing facility by tearing out the inside and bricking up the windows. It was operational by March 1942. A second brick cottage, the "white house" or Bunker 2, was converted some weeks later. These structures were in use for mass killings until early 1943. Himmler visited the camp in person on July 17 and 18, 1942. He was given a demonstration of a mass killing using the gas chamber in Bunker 2 and toured the building site of the new IG Farben plant being constructed at the nearby town of Monowitz.
In early 1943, the Nazis decided to increase greatly the gassing capacity of Birkenau. Crematorium II, originally designed as a mortuary, with morgues in the basement and ground-level incinerators, was converted into a killing factory by installing gas-tight doors, vents for the Zyklon B (a highly lethal cyanide-based pesticide) to be dropped into the chamber, and ventilation equipment to remove the gas thereafter. It went into operation in March. Crematorium III was built using the same design. Crematoria IV and V, designed from the start as gassing centers, were also constructed that spring. By June 1943, all four crematoria were operational. Most of the victims were killed using these four structures.
The Gypsy camp.
On December 10, 1942, Himmler issued an order to send all Sinti and Roma (Gypsies) to concentration camps, including Auschwitz. A separate camp for Roma was set up at Auschwitz II-Birkenau known as the "Zigeunerfamilienlager" (Gypsy Family Camp). The first transport of German Gypsies arrived on February 26, 1943, and was housed in Section B-IIe of Auschwitz II. Approximately 23,000 Gypsies had been brought to Auschwitz by 1944, 20,000 of whom died there. One transport of 1,700 Polish Sinti and Roma was killed upon arrival, as they were suspected to be ill with spotted fever.
Gypsy prisoners were used primarily for construction work. Thousands died of typhus and noma due to overcrowding, poor sanitary conditions, and malnutrition. Anywhere from 1,400 to 3,000 prisoners were transferred to other concentration camps before the murder of the remaining population.
On August 2, 1944, the SS cleared the Gypsy camp. A witness in another part of the camp later told of the Gypsies unsuccessfully battling the SS with improvised weapons before being loaded into trucks. The surviving population of 2,897 was then killed en masse in the gas chambers. The murder of the Romani people by the Nazis during World War II is known in the Romani language as the Porajmos (devouring).
Auschwitz III.
After examining several sites for a new plant to manufacture buna, a type of synthetic rubber essential to the war effort, chemicals manufacturer IG Farben chose a site near the towns of Dwory and Monowice (Monowitz in German), about east of Auschwitz I and east of the town of Oświęcim. Financial support in the form of tax exemptions was available to corporations prepared to develop industries in the frontier regions under the Eastern Fiscal Assistance Law, passed in December 1940. In addition to its proximity to the concentration camp, which could be used as a source of cheap labor, the site had good railway connections and access to raw materials. In February 1941, Himmler ordered that the Jewish population of Oświęcim should be expelled to make way for skilled laborers that would be brought in to work at the plant. All Poles able to work were to remain in the town and were forced to work building the factory. Himmler visited in person in March and decreed an immediate expansion of the parent camp to house 30,000 persons. Development of the camp at Birkenau began about six months later. Construction of IG Auschwitz began in April, with an initial force of 1,000 workers from Auschwitz I assigned to work on the construction. This number increased to 7,000 in 1943 and 11,000 in 1944. Over the course of its history, about 35,000 inmates in total worked at the plant; 25,000 died as a result of malnutrition, disease, and the physically impossible workload. In addition to the concentration camp inmates, who comprised a third of the work force, IG Auschwitz employed slave laborers from all over Europe.
Initially the laborers walked the seven kilometers from Auschwitz I to the plant each day, but as this meant they had to rise at 3:00 am, many arrived exhausted and unable to work. The camp at Monowitz (also called Monowitz-Buna or Auschwitz III) was constructed and began housing inmates on October 30, 1942, the first concentration camp to be financed and built by private industry. In January 1943 the "ArbeitsausbildungLager" (labor education camp) was moved from the parent camp to Monowitz. These prisoners were also forced to work on the building site. The SS charged IG Farben three Reichsmarks per hour for unskilled workers, four for skilled workers. Although the camp administrators expected the prisoners to work at 75 percent of the capacity of a free worker, the inmates were only able to perform 20 to 50 percent as well. Site managers constantly threatened inmates with transportation to Birkenau for death in the gas chambers as a way to try to increase productivity. Deaths and transfers to the gas chambers at Birkenau reduced the prisoner population of Monowitz by nearly a fifth each month; numbers were made up with new arrivals. Life expectancy of inmates at Monowitz averaged about three months. Though the factory was initially expected to begin production in 1943, shortages of labor and raw materials meant start-up had to be postponed repeatedly. The plant was almost ready to commence production when it was overrun by Soviet troops in 1945.
Subcamps.
Various other German industrial enterprises, such as Krupp and Siemens-Schuckert, built factories with their own subcamps. There were 45 such satellite camps, 28 of which served corporations involved in the armaments industry. Prisoner populations ranged from several dozen to several thousand. Subcamps were built at Blechhammer, Jawiszowice, Jaworzno, Lagisze, Mysłowice, Trzebinia, and other centers as far afield as the Protectorate of Bohemia and Moravia. Satellite camps were designated as "Aussenlager" (external camp), "Nebenlager" (extension or subcamp), or "Arbeitslager" (labor camp). Industries with satellite camps included coal mines, foundries and other metal works, chemical plants, and other industries. Prisoners were also made to work in forestry and farming.
Evacuation, death marches, and liberation.
In November 1944, with the Soviet Red Army approaching through Poland, Himmler ordered gassing operations to cease across the Reich. Crematoria II, III, and IV were dismantled, while Crematorium I was transformed into an air raid shelter. The "Sonderkommando" were ordered to remove other evidence of the killings, including the mass graves. The SS destroyed written records, and in the final week before the camp's liberation, burned or demolished many of its buildings.
Himmler ordered the evacuation of all camps in January 1945, charging camp commanders with "making sure that not a single prisoner from the concentration camps falls alive into the hands of the enemy." On January 17, 58,000 Auschwitz detainees were evacuated under guard, largely on foot; thousands of them died in the subsequent death march west towards Wodzisław Śląski. Approximately 20,000 Auschwitz prisoners made it to Bergen-Belsen concentration camp in Germany, where they were liberated by the British in April 1945.
Those too weak or sick to walk were left behind. When the 322nd Rifle Division of the Red Army arrived at the camp on January 27 they found around 7,500 prisoners and about 600 corpses had been left behind. Among the items found by the Soviet soldiers were 370,000 men's suits, 837,000 women's garments, and of human hair.
The camp's liberation received little press attention at the time. Rees attributes this to three factors: the previous discovery of similar crimes at Majdanek concentration camp, competing news from the Allied summit at Yalta, and the Soviet Union's interest, for propaganda purposes, in minimizing attention to Jewish suffering. Due to the vast extent of the camp area, at least four divisions took part in liberating the camp: 100th Rifle Division (established in Vologda, Russia), 322nd Rifle Division (Gorky, Russia), 286th Rifle Division (Leningrad), and 107th Motor Rifle Division (Tambov, Russia). 
After the war.
After liberation, parts of Auschwitz I served first as a hospital for liberated prisoners. Soviet and Polish investigators worked in the initial months to document the war crimes of the SS. In the two years that followed, the Soviets dismantled and exported the IG Farben factories, and the Birkenau barracks were looted by Polish civilians. Area residents sifted the mass graves and ashes for gold. Until 1947, some of the facilities were used as a prison camp of the Soviet NKVD.
After the site became a museum in 1947, exhumation work lasted for more than a decade. Antoni Dobrowolski, the oldest known survivor of Auschwitz, died aged 108 on October 21, 2012, in Dębno, Poland.
Camp commandant Rudolf Höss was pursued by the British Intelligence Corps, who arrested him at a farm near Flensburg, Germany, on March 11, 1946. Höss confessed to his role in the mass killings at Auschwitz in his memoirs and in his trial before the Supreme National Tribunal in Warsaw, Poland. He was convicted of murder and hanged at the camp on April 16, 1947.
Around 12 percent of Auschwitz's 6,500 staff who survived the war were eventually brought to trial. Poland was more active than other nations in investigating war crimes, prosecuting 673 of the total 789 Auschwitz staff brought to trial. On November 25, 1947, the Auschwitz Trial began in Kraków, when Poland's Supreme National Tribunal brought to court 40 former Auschwitz staff. The trial's defendants included commandant Arthur Liebehenschel, women's camp leader Maria Mandel, and camp leader Hans Aumeier. The trials ended on December 22, 1947, with 23 death sentences, 7 life sentences, and 9 prison sentences ranging from three to fifteen years. Hans Münch, an SS doctor who had several former prisoners testify on his behalf, was the only person to be acquitted.
Other former staff were hanged for war crimes in the Dachau Trials and the Belsen Trial, including camp leaders Josef Kramer, Franz Hössler, and Vinzenz Schöttl; doctor Friedrich Entress; and guards Irma Grese and Elisabeth Volkenrath. The Frankfurt Auschwitz Trials, held in West Germany from December 20, 1963 to August 20, 1965, convicted 17 of 22 defendants, giving them prison sentences ranging from life to three years and three months. Bruno Tesch and Karl Weinbacher, the owner and the chief executive officer of the firm Tesch & Stabenow, one of the suppliers of Zyklon B, were executed for knowingly supplying the chemical for use on humans.
Command and control.
Camp guards were members of the "SS-Totenkopfverbände" (Death's Head Units). Around 7,000 SS personnel in total were posted to Auschwitz during the war. Of these, 4 percent of SS personnel were officers and 26 percent were non-commissioned officers, while the remainder were rank-and-file members. Approximately three in four SS personnel worked in security. Others worked in the medical or political departments, in the camp headquarters, or in the economic administration, which was responsible for the property of dead prisoners. SS personnel at the camp included 200 women, who worked as guards, nurses, or messengers. The overall command authority for the entire camp was Department D (the Concentration Camps Inspectorate) of the "SS-Wirtschafts-Verwaltungshauptamt" (SS Economics Main Office; SS-WVHA).
Auschwitz was considered a comfortable posting by many SS members, due to many amenities and the abundance of slave labor. Of the various prisoner groups, SS officers preferred Jehovah's Witnesses for household slaves because of their nonviolent behavior. Höss lived with his wife and children in a villa just outside the camp grounds. Other SS personnel were also initially allowed to bring fiancees, wives, and children to live at the camp, but when the SS camp grew more crowded, Höss restricted further arrivals. Facilities for the SS personnel and their families included a library, swimming pool, coffee house, and a theater that hosted regular performances.
One prisoner in each work detail or prisoner block—usually an Aryan—was appointed as a "Kapo" ("head" or "overseer"). The "Kapos" received better rations and lodging and wielded tremendous power over other prisoners, whom they often abused. Very few "Kapos" were prosecuted after the war, however, due to the difficulty in determining which "Kapo" atrocities had been performed under SS orders and which had been individual actions.
About 120 SS personnel were assigned to the gas chambers and lived on site at the crematoria. Several SS personnel oversaw the killings at each gas chamber, while the bulk of the work was done by the mostly Jewish prisoners known as "Sonderkommando" (special squad). "Sonderkommando" responsibilities included guiding victims to the gas chambers and removing, looting, and cremating the corpses.
The "Sonderkommado" were housed separately from other prisoners, in somewhat better conditions. Their quality of life was further improved by access to the goods taken from murdered prisoners, which "Sonderkommando" were sometimes able to steal for themselves and to trade on Auschwitz's black market. Hungarian doctor Miklós Nyiszli reported that the "Sonderkommando" numbered around 860 prisoners when the Hungarian Jews were being killed in 1944. Many "Sonderkommando" committed suicide due to the horrors of their work; those who did not generally were shot by the SS in a matter of weeks, and new "Sonderkommando" units were then formed from incoming transports. Almost none of the 2,000 prisoners placed in these units survived to the camp's liberation.
Life in the camps.
The prisoners' day began at 4:30 am (an hour later in winter) with morning roll call. Dr. Miklós Nyiszli describes roll call as beginning 3:00 am and lasting four hours. The weather was cold in Auschwitz at that time of day, even in summer. The prisoners were ordered to line up outdoors in rows of five and had to stay there until 7:00 am, when the SS officers arrived. Meanwhile, the guards would force the prisoners to squat for an hour with their hands above their heads or levy punishments such as beatings or detention for infractions such as having a missing button or an improperly cleaned food bowl. The inmates were counted and re-counted. Nyiszli describes how even the dead had to be present at roll call, standing supported by their fellow inmates until the ordeal was over. When he was a prisoner in 1944–45, five to ten men were found dead in the barracks each night. The prisoners assigned to Mengele's staff slept in a separate barracks and were awoken at 7:00 am for a roll call that only took a few minutes.
After roll call, the "Kommando", or work details, walked to their place of work, five abreast, wearing striped camp fatigues, no underwear, and ill-fitting wooden shoes without socks. A prisoner's orchestra (such as the Women's Orchestra of Auschwitz) was forced to play cheerful music as the workers left the camp. "Kapos" were responsible for the prisoners' behavior while they worked, as was an SS escort. The working day lasted 12 hours during the summer and a little less in the winter. Much of the work took place outdoors at construction sites, gravel pits, and lumber yards. No rest periods were allowed. One prisoner was assigned to the latrines to measure the time the workers took to empty their bladders and bowels. Sunday was not a work day, but the prisoners did not rest; they were required to clean the barracks and take their weekly shower. Prisoners were allowed to write (in German) to their families on Sundays. Inmates who did not speak German would trade some of their bread to another inmate for help composing their letters. Members of the SS censored the outgoing mail.
A second mandatory roll call took place in the evening. If a prisoner was missing, the others had to remain standing in place until he was either found or the reason for his absence discovered, regardless of the weather conditions, even if it took hours. After roll call, individual and collective punishments were meted out, depending on what had happened during the day, before the prisoners were allowed to retire to their blocks for the night and receive their bread rations and water. Curfew was two or three hours later. The prisoners slept in long rows of wooden bunks, lying in and on their clothes and shoes to prevent them from being stolen.
According to Nyiszli, "Eight hundred to a thousand people were crammed into the superimposed compartments of each barracks. Unable to stretch out completely, they slept there both lengthwise and crosswise, with one man's feet on another's head, neck, or chest. Stripped of all human dignity, they pushed and shoved and bit and kicked each other in an effort to get a few more inches' space on which to sleep a little more comfortably. For they did not have long to sleep".
The types of prisoners were distinguishable by triangular pieces of cloth, called "Winkel", sewn onto on their jackets below their prisoner number. Political prisoners had a red triangle, Jehovah's Witnesses had purple, criminals had green, and so on. The nationality of the inmate was indicated by a letter stitched onto the "Winkel". Jews had a yellow triangle, overlaid by a second "Winkel" if they also fit into a second category. Uniquely at Auschwitz, prisoners were tattooed with their prisoner number, on the chest for Soviet prisoners of war and on the left arm for civilians.
Prisoners received a hot drink in the morning, but no breakfast, and a thin meatless vegetable soup at noon. In the evening they received a small ration of moldy bread. Most prisoners saved some of the bread for the following morning. Nyiszli notes the daily intake did not exceed 700 calories, except for prisoners being subjected to live medical experimentation, who were better fed and clothed. Sanitary arrangements were poor, with inadequate latrines and a lack of fresh water. In Auschwitz II-Birkenau, latrines were not installed until 1943, two years after camp construction began. The camps were infested with vermin such as disease-carrying lice, and the inmates suffered and died in epidemics of typhus and other diseases. Noma, a bacterial infection occurring among the malnourished, was a common cause of death among children in the Gypsy camp.
Block 11 of Auschwitz I was the prison within the prison, where violators of the numerous rules were punished. Some prisoners were made to spend the nights in standing cells. These cells were about , and held four men; they could do nothing but stand, and were forced during the day to work with the other prisoners. Prisoners sentenced to death for attempting to escape were confined in a dark cell and given neither food nor water while being left to die. 
In the basement were the "dark cells", which had only a very tiny window and a solid door. Prisoners placed in these cells gradually suffocated as they used up all the oxygen in the cell; sometimes the SS lit a candle in the cell to use up the oxygen more quickly. Many were subjected to hanging with their hands behind their backs for hours, even days, thus dislocating their shoulder joints.
Selection and extermination process.
On July 31, 1941, Hermann Göring gave written authorization to Heydrich, Chief of the Reich Main Security Office (RSHA), to prepare and submit a plan for "Die Endlösung der Judenfrage" (the Final Solution of the Jewish question) in territories under German control and to coordinate the participation of all involved government organizations. The resulting "Generalplan Ost" (General Plan for the East) called for deporting the population of occupied Eastern Europe and the Soviet Union to Siberia, for use as slave labour or to be murdered. In addition to eliminating Jews, the Nazis also planned to reduce the population of the conquered territories by 30 million people through starvation in an action called the Hunger Plan. Food supplies would be diverted to the German army and German civilians. Cities would be razed and the land allowed to return to forest or resettled by German colonists.
Plans for the total eradication of the Jewish population of Europe—eleven million people—were formalized at the Wannsee Conference on January 20, 1942. Some would be worked to death and the rest would be killed. Initially the victims were killed with gas vans or by "Einsatzgruppen" firing squads, but these methods proved impracticable for an operation of this scale. By 1942, killing centers at Auschwitz, Sobibór, Treblinka, and other Nazi extermination camps replaced "Einsatzgruppen" as the primary method of mass killing.
The first mass exterminations at Auschwitz took place in early September 1941, when 900 inmates were killed by gathering them in the basement of Block 11 and gassing them with Zyklon B. This building proved unsuitable for mass gassings, so the site of the killings was moved to the crematorium at Auschwitz I (Crematorium I, which operated until July 1942). There, more than 700 victims could be killed at once. In order to keep the victims calm, they were told they were to undergo disinfection and de-lousing. They were ordered to undress outside and then were locked in the building and gassed. After its decommissioning as a gas chamber, the building was converted to a storage facility and later served as an air raid shelter for the SS. The gas chamber and crematorium were reconstructed after the war using the original components, which remained on site. Some 60,000 people were killed at Crematorium I.
Mass exterminations were moved to two provisional gas chambers (Bunkers 1 and 2), where the killings continued while the larger Crematoria II, III, IV, and V were under construction. Bunker 2 was temporarily reactivated from May to November 1944, when large numbers of Hungarian Jews were exterminated. In summer 1944 the capacity of the crematoria and outdoor incineration pits was 20,000 bodies per day. A planned sixth facility—Crematorium VI—was never built.
Prisoners were transported from all over German-occupied Europe by rail, arriving in daily convoys. By July 1942, the SS were conducting "selections". Incoming Jews were segregated; those deemed able to work were sent to the right and admitted into the camp, and those deemed unfit for labor were sent to the left and immediately gassed. The group selected to die, about three-quarters of the total, included almost all children, women with small children, all the elderly, and all those who appeared on brief and superficial inspection by an SS doctor not to be completely fit. After the selection process was complete, those too ill or too young to walk to the crematoria were transported there on trucks or killed on the spot with a bullet to the head. The belongings of the arrivals were seized by the SS and sorted in an area of the camp called "Canada", so called because Canada was seen as a land of plenty. Many of the SS at the camp enriched themselves by pilfering the confiscated property.
SS officers told the victims they were to take a shower and undergo delousing. The victims undressed in an outer chamber and walked into the gas chamber, which was disguised as a shower facility. Some were even issued soap and a towel. The Zyklon B was delivered by ambulance to the crematoria by a special SS bureau known as the Hygienic Institute. The actual delivery of the gas to the victims was always handled by the SS, on the order of the supervising SS doctor. After the doors were shut, SS men dumped in the Zyklon B pellets through vents in the roof or holes in the side of the chamber. The victims were dead within 20 minutes. Despite the thick concrete walls, screaming and moaning from within could be heard outside. In one failed attempt to muffle the noise, two motorcycle engines were revved up to full throttle nearby, but the sound of yelling could still be heard over the engines.
"Sonderkommando" wearing gas masks then dragged the bodies from the chamber. The victims' glasses, artificial limbs, jewelry, and hair were removed, and any dental work was extracted so the gold could be melted down. The corpses were burned in the nearby incinerators, and the ashes were buried, thrown in the river, or used as fertilizer.
The gas chambers worked to their fullest capacity from April–July 1944, during the massacre of Hungary's Jews. Hungary was an ally of Germany during the war, but it had resisted turning over its Jews until Germany invaded that March. A rail spur leading directly into Birkenau was completed that May to deliver the victims closer to the gas chambers. From 14 May until early July 1944, 437,000 Hungarian Jews, half of the pre-war population, were deported to Auschwitz, at a rate of 12,000 a day for a considerable part of that period. The incoming volume was so great that the SS resorted to burning corpses in open-air pits as well as in the crematoria. The last selection took place on October 30, 1944.
Medical experiments.
German doctors performed a wide variety of experiments on prisoners at Auschwitz. SS doctors tested the efficacy of X-rays as a sterilization device by administering large doses to female prisoners. Prof. Dr. Carl Clauberg injected chemicals into women's uteruses in an effort to glue them shut. Bayer, then a subsidiary of IG Farben, bought prisoners to use as research subjects for testing new drugs. Prisoners were also deliberately infected with spotted fever for vaccination research and exposed to toxic substances to study the effects.
The most infamous doctor at Auschwitz was Josef Mengele, known as the "Angel of Death". Particularly interested in research on identical twins, Mengele performed cruel experiments on them, such as inducing diseases in one twin and killing the other when the first died to perform comparative autopsies. He also took a special interest in dwarfs, and he deliberately induced noma in twins, dwarfs, and other prisoners to study the effects.
Kurt Heissmeyer took twenty Jewish children from Auschwitz to use in pseudoscientific medical experiments at the Neuengamme concentration camp. In April 1945, the children were killed by hanging to conceal the project.
A skeleton collection was obtained from among a pool of 115 Jewish Auschwitz inmates, chosen for their perceived stereotypical racial characteristics. Rudolf Brandt and Wolfram Sievers, general manager of the "Ahnenerbe" (a Nazi research institute), were responsible for delivering the skeletons to the collection of the Anatomy Institute at the Reich University of Strasbourg in the Alsace region of Occupied France. The collection was sanctioned by Himmler and under the direction of August Hirt. Ultimately 87 of the inmates were shipped to Natzweiler-Struthof and killed in August 1943. Brandt and Sievers were later convicted in the Doctors' Trial in Nuremberg.
Death toll.
The exact number of victims at Auschwitz is difficult to fix with certainty, because many prisoners were never registered and much evidence was destroyed by the SS in the final days of the war. As early as 1942, Himmler visited the camp and ordered that "all mass graves were to be opened and the corpses burned. In addition the ashes were to be disposed of in such a way that it would be impossible at some future time to calculate the number of corpses burned."
Shortly following the camp's liberation, the Soviet government stated that four million people had been killed on the site, a figure now regarded as greatly exaggerated. While under interrogation, Höss said that Adolf Eichmann told him that two and a half million Jews had been killed in gas chambers and about half a million more had died of other causes. Later he wrote, "I regard the figure of two and a half million as far too high. Even Auschwitz had limits to its destructive possibilities". Gerald Reitlinger's 1953 book "The Final Solution" estimated the number killed to be 800,000 to 900,000, and Raul Hilberg's 1961 work "The Destruction of the European Jews" estimated the number killed to be a maximum of 1,000,000 Jewish victims.
In 1983, French scholar George Wellers was one of the first to use German data on deportations to estimate the number killed at Auschwitz, arriving at a figure of 1,471,595 deaths, including 1.35 million Jews and 86,675 Poles. A larger study started by Franciszek Piper used timetables of train arrivals combined with deportation records to calculate at least 960,000 Jewish deaths and at least 1.1 million total deaths, a figure adopted as official by the Auschwitz-Birkenau State Museum in the 1990s. Piper also stated that a figure of as many as 1.5 million total deaths was possible.
By nation, the greatest number of Auschwitz's Jewish victims were from Hungary, accounting for 438,000 deaths, followed by Polish Jews (300,000 deaths), French (69,000), Dutch (60,000), and Greek (55,000). Fewer than one percent of Soviet Jews murdered in the Holocaust were killed in Auschwitz, as German forces had already been driven from Russia when the killing at Auschwitz reached its peak in 1944. Approximately 1 in 6 Jews killed in the Holocaust died at the camp.
The next largest group of victims were non-Jewish Poles, who accounted for 70,000 to 75,000 deaths. Twenty-one thousand Roma and Sinti were killed, along with 15,000 Soviet POWs and 10,000 to 15,000 peoples of other nations. Around 400 Jehovah's Witnesses were imprisoned at Auschwitz, at least 152 of whom died. An estimated 5,000 to 15,000 gay men prosecuted under German Penal Code Section 175 (proscribing sexual acts between men) were detained in concentration camps of which an unknown number were sent to Auschwitz; of those sent to Auschwitz 80 percent died.
Escapes, resistance, and the Allies' knowledge of the camps.
Inmates were at times able to distribute information from the camp via messages and shortwave radio transmissions. The Polish government-in-exile in London first reported the gassing of prisoners on July 21, 1942. However, these reports were for a long time discarded as exaggerated or unreliable by the Allied Powers, Germany's opponents.
Information regarding Auschwitz was also available to the Allies during the years 1940–43 by the accurate and frequent reports of Polish Home Army (Armia Krajowa) Captain Witold Pilecki. Pilecki was the only known person to volunteer to be imprisoned at Auschwitz concentration camp, spending 945 days there. He gathered evidence of genocide and organized resistance structures known as Związek Organizacji Wojskowej (ZOW) at the camp. His first report was smuggled to the outside world in November 1940, through an inmate who was released from the camp. He eventually escaped on April 27, 1943, but his personal report of mass killings was dismissed as exaggeration by the Allies, as were his previous reports.
The first information about Auschwitz concentration camp was published in winter 1940–41 in the Polish underground newspapers "Polska żyje" ("Poland lives") and "Biuletyn Informacyjny" ("Newsletter"). From 1942 members of the Bureau of Information and Propaganda of the Warsaw area Home Army published in occupied Poland a few brochures based on the accounts of escapees. The first of these was a fictional memoir "Oświęcim. Pamiętnik więźnia" ("Auschwitz: Diary of a prisoner"), written by Halina Krahelska and published in April 1942 in Warsaw. Also published in 1942 were the books "Auschwitz: obóz śmierci" ("Auschwitz: camp of death") written by Natalia Zarembina, and "W piekle" ("In Hell") by Zofia Kossak-Szczucka, the Polish writer, social activist, and founder of Żegota.
In 1943, the "Kampfgruppe Auschwitz" (Combat Group Auschwitz) was organized with the aim of sending out information about what was happening. "Sonderkommandos" buried notes in the ground, hoping they would be found by the camp's liberators. The group also took and smuggled out photographs of corpses and preparations for mass killings in mid-1944.
The attitude of the Allies changed with receipt of the detailed, 32-page Vrba–Wetzler report, compiled by two Jewish prisoners, Rudolf Vrba and Alfréd Wetzler, who escaped on April 7, 1944. This report finally convinced Allied leaders that mass killings were taking place in Auschwitz.
Details from the Vrba-Wetzler report were released to the Swiss press by diplomat George Mantello and printed on June 6 by "The New York Times". Auschwitz Plans originating with the Polish government were provided to the U.K foreign ministry in August 1944.
Starting with a plea from the Slovakian rabbi Chaim Michael Dov Weissmandl in May 1944, there was a growing campaign by Jewish organizations to persuade the Allies to bomb Auschwitz or the railway lines leading to it. At one point British Prime Minister Winston Churchill ordered that such a plan be prepared, but he was told that precision bombing the camp to free the prisoners or disrupt the railway was not technically feasible.
In 1978, historian David S. Wyman published an essay titled "Why Auschwitz Was Never Bombed", arguing that the US Air Force had the capability to attack Auschwitz and should have done so; books by Bernard Wasserstein and Martin Gilbert raised similar questions about British inaction. Since the 1990s, other historians have argued that Allied bombing accuracy was not sufficient for Wyman's proposed attack, and that counterfactual history is an inherently problematic endeavor. The controversy over this decision has lasted to the present day in both countries.
Individual escape attempts.
At least 802 prisoners attempted to escape from the Auschwitz camps, mostly Polish or Soviet prisoners fleeing from work sites outside the camp. 144 were successful. The fates of 331 of the escapees are unknown. A common punishment for escape attempts was death by starvation; the families of successful escapees were sometimes arrested and interned in Auschwitz and prominently displayed to deter others. If someone did manage to escape, the SS picked ten people at random from the prisoner's block and starved them to death.
A daring escape from Auschwitz was staged on June 20, 1942 by four Polish prisoners: Eugeniusz Bendera (auto mechanic at the camp), Kazimierz Piechowski, Stanisław Gustaw Jaster, and Józef Lempart. After breaking into a warehouse, the four dressed as members of the "SS-Totenkopfverbände" (the SS units responsible for concentration camps), armed themselves, and stole an SS staff car, which they then drove unchallenged through the main gate.
On June 24, 1944, a Belgian-Polish Jew, Mala Zimetbaum, escaped with her Polish boyfriend, Edek Galiński dressed in a stolen prisoner-guard uniform. They were later recaptured, tortured, and executed by the SS. On July 21, 1944, inmate Jerzy Bielecki, dressed in an SS uniform and using a faked pass, managed to cross the camp's gate together with his Jewish girlfriend, Cyla. Both survived the war.
Birkenau revolt.
The "Sonderkommando" units were aware that as witnesses to the killings, they themselves would eventually be killed to hide Nazi crimes. Though they knew that it would mean their deaths, the "Sonderkommando" of Birkenau "Kommando" III staged an uprising on October 7, 1944, following an announcement that some of them would be selected to be "transferred to another camp"—a common Nazi ruse for the murder of prisoners. The "Sonderkommando" attacked the SS guards with stones, axes, and makeshift hand grenades. As the SS set up machine guns to attack the prisoners in Crematorium IV, the "Sonderkommando" in Crematorium II also revolted, some of them managing to escape the compound. The rebellion was suppressed by nightfall.
Ultimately, three SS guards were killed—one of whom was burned alive by the prisoners in the oven of Crematorium II—and 250 "Sonderkommando" were killed. Hundreds of prisoners escaped, but were all soon captured and executed, along with an additional group who participated in the revolt. Crematorium IV was destroyed in the fighting, and a group of prisoners in the gas chamber of Crematorium V was spared in the chaos.
Legacy.
In the decades since its liberation, Auschwitz has become a primary symbol of the Holocaust. Historian Timothy D. Snyder attributes this to the camp's high death toll as well as to its "unusual combination of an industrial camp complex and a killing facility", which left behind far more witnesses than single-purpose killing facilities such as Chełmno or Treblinka. The United Nations General Assembly has designated January 27, the date of the camp's liberation, as International Holocaust Remembrance Day. In a speech on the fiftieth anniversary of the liberation, German chancellor Helmut Kohl described Auschwitz as the "darkest and most horrific chapter of German history".
Notable memoirists of the camp include Primo Levi, Elie Wiesel, and Tadeusz Borowski. In "If This Is a Man", Levi wrote that the concentration camps represented the epitome of the totalitarian system:
Psychiatrist Viktor Frankl drew on his imprisonment at Auschwitz in composing "Man's Search for Meaning" (1946), one of the most widely read works about the camp. An existentialist work, the book argues that individuals can find purpose even among great suffering, and that this sense of purpose sustains them. Wiesel wrote about his own imprisonment at Auschwitz in "Night" (1960) and other works, and became a prominent spokesman against ethnic violence. In 1986, he was awarded the Nobel Peace Prize.
Camp survivor Simone Veil was later elected President of the European Parliament, serving from 1979–82. Two Auschwitz victims—Maximilian Kolbe, a priest who volunteered to die by starvation in place of a stranger, and Edith Stein, a Jewish convert to Catholicism—were later named saints of the Roman Catholic Church.
Auschwitz-Birkenau State Museum.
On July 2, 1947, the Polish government passed a law establishing a state memorial to the victims of Nazism on the site of the camp. In 1955, an exhibition opened displaying prisoner mug shots; hair, suitcases, and shoes taken from murdered prisoners; canisters of Zyklon B pellets; and other objects related to the killings. UNESCO added the camp to its list of World Heritage Sites in 1979. In 2011, the museum drew 1,400,000 visitors.
Pope John Paul II performed mass over the train tracks leading to the camp on June 7, 1979. In the decades following his visit, controversies erupted over a group of Carmelite nuns founding a convent on the site and erecting a large cross originally used in the pope's mass. Protesters objected to what they saw as Christianization of the site, while others argued that the cross's presence effectively recognized the camp's Catholic victims.
On September 4, 2003, three Israeli Air Force F-15 Eagles performed a fly-over of Auschwitz-Birkenau during a ceremony at the camp below. The flight was led by Major-General Amir Eshel, the son of Holocaust survivors. On January 27, 2015, some 300 Auschwitz survivors and other guests gathered under a giant tent at the entrance to Auschwitz II Birkenau to commemorate the 70th anniversary of the camp's liberation. Attendees included president of the World Jewish Congress Ronald Lauder, film director Steven Spielberg, and world leaders such as Polish president Bronisław Komorowski and King Willem-Alexander of the Netherlands. As the number of remaining survivors decreases each year, the attendance at the event is unlikely to be surpassed at future major anniversaries. Commemorations also took place at Yad Vashem in Israel, Theresienstadt concentration camp, and in Berlin and Moscow.
Museum curators note that some visitors try to take artefacts as souvenirs, which is strictly prohibited and usually leads to criminal charges. For example, on June 22, 2015, two British men were convicted of theft for stealing apparel buttons and shards of decorative glass they found on the ground near the area where camp victims' confiscated personal effects were stored. The men, both 17 years old, received probation and were fined £170. Curators said that similar thefts happen once or twice a year.

</doc>
<doc id="2007" url="https://en.wikipedia.org/wiki?curid=2007" title="Archery">
Archery

Archery is the sport, practice or skill of using a bow to propel arrows. The word comes from the Latin "arcus". Historically, archery has been used for hunting and combat. In modern times, it is mainly a competitive sport and recreational activity. A person who participates in archery is typically called an "archer" or a "bowman", and a person who is fond of or an expert at archery is sometimes called a toxophilite.
History.
The bow seems to have been invented in the later Paleolithic or early Mesolithic periods. The oldest signs of its use in Europe come from the in the north of Hamburg, Germany and dates from the late Paleolithic, about 10,000–9000 BC. The arrows were made of pine and consisted of a mainshaft and a long fore shaft with a flint point. There are no definite earlier bows; previous pointed shafts are known, but may have been launched by spear-throwers rather than bows. The oldest bows known so far come from the Holmegård swamp in Denmark. Bows eventually replaced the spear-thrower as the predominant means for launching shafted projectiles, on every continent except Australasia, though spear-throwers persisted alongside the bow in parts of the Americas, notably Mexico and among the Inuit.
Bows and arrows have been present in Egyptian culture since its predynastic origins. In the Levant, artifacts that could be arrow-shaft straighteners are known from the Natufian culture, (c. 12,800–10,300 BP (before present)) onwards. The Khiamian and PPN A shouldered Khiam-points may well be arrowheads.
Classical civilizations, notably the Assyrians, Armenians, Persians, Parthians, Indians, Koreans, Chinese, and Japanese fielded large numbers of archers in their armies. The Welsh longbow proved its worth for the first time in Continental warfare at the Battle of Crécy. In the Americas archery was widespread at European contact.
Archery was highly developed in Asia. The Sanskrit term for archery, dhanurveda, came to refer to martial arts in general. In East Asia, Goguryeo, one of the Three Kingdoms of Korea was well known for its regiments of exceptionally skilled archers.
Mounted archery.
Central Asian tribesmen (after the domestication of the horse) and American Plains Indians (after gaining access to horses) became extremely adept at archery on horseback. Lightly armoured, but highly mobile archers were excellently suited to warfare in the Central Asian steppes, and they formed a large part of armies that repeatedly conquered large areas of Eurasia. Shorter bows are more suited to use on horseback, and the composite bow enabled mounted archers to use powerful weapons. Empires throughout the Eurasian landmass often strongly associated their respective "barbarian" counterparts with the usage of the bow and arrow, to the point where powerful states like the Han Dynasty referred to their neighbours, the Xiong-nu, as "Those Who Draw the Bow" For example, Xiong-nu mounted bowmen made them more than a match for the Han military, and their threat was at least partially responsible for Chinese expansion into the Ordos region, to create a stronger, more powerful buffer zone against them. It is possible that "barbarian" peoples were responsible for introducing archery or certain types of bows to their "civilized" counterparts—the Xiong-nu and the Han being one example. Similarly, short bows seem to have been introduced to Japan by northeast Asian groups.
Decline of archery.
The development of firearms rendered bows obsolete in warfare, albeit efforts were sometimes made to preserve archery practice. In Wales and England, for example, the government tried to enforce practice with the Longbow until the end of the 16th century. This was because it was recognised that the bow had been instrumental to military success during the Hundred Years' War. Despite the high social status, ongoing utility, and widespread pleasure of archery in Armenia, China, Egypt, England, America, India, Japan, Korea, Turkey, Wales and elsewhere, almost every culture that gained access to even early firearms used them widely, to the neglect of archery. Early firearms were vastly inferior in rate-of-fire, and were very susceptible to wet weather. However, they had longer effective range and were tactically superior in the common situation of soldiers shooting at each other from behind obstructions. They also required significantly less training to use properly, in particular penetrating steel armour without any need to develop special musculature. Armies equipped with guns could thus provide superior firepower, and highly trained archers became obsolete on the battlefield. However, the bow and arrow is still an effective weapon, and archers have seen action in the 21st century. Traditional archery remains in use for sport, and for hunting in many areas.
Eighteenth-century revival.
Early recreational archery societies included the Finsbury Archers and the Kilwinning Papingo, established in 1688. The latter held competitions in which archers had to dislodge a wooden parrot from the top of an abbey tower. The Company of Scottish Archers was formed in 1676 and is one of the oldest sporting bodies in the world. It remained a small and scattered pastime, however, until the late 18th century when it experienced a fashionable revival among the aristocracy. Sir Ashton Lever, an antiquarian and collector, formed the Toxophilite Society in London in 1781, with the patronage of George, the Prince of Wales.
Archery societies were set up across the country, each with its own strict entry criteria and outlandish costumes. Recreational archery soon became extravagant social and ceremonial events for the nobility, complete with flags, music and 21 gun salutes for the competitors. The clubs were "the drawing rooms of the great country houses placed outside" and thus came to play an important role in the social networks of the local upper class. As well as its emphasis on display and status, the sport was notable for its popularity with females. Young women could not only compete in the contests but retain and show off their sexuality while doing so. Thus, archery came to act as a forum for introductions, flirtation and romance. It was often consciously styled in the manner of a Medieval tournament with titles and laurel wreaths being presented as a reward to the victor. General meetings were held from 1789, in which local lodges convened together to standardise the rules and ceremonies. Archery was also co-opted as a distinctively British tradition, dating back to the lore of Robin Hood and it served as a patriotic form of entertainment at a time of political tension in Europe. The societies were also elitist, and the new middle class bourgeoisie were excluded from the clubs due to their lack of social status.
After the Napoleonic Wars, the sport became increasingly popular among all classes, and it was framed as a nostalgic reimagining of the preindustrial rural Britain. Particularly influential was Sir Walter Scott's 1819 novel, "Ivanhoe" that depicted the heroic character Lockseley winning an archery tournament.
A modern sport.
The 1840s saw the first attempts at turning the recreation into a modern sport. The first Grand National Archery Society meeting was held in York in 1844 and over the next decade the extravagant and festive practices of the past were gradually whittled away and the rules were standardised as the 'York Round' - a series of shoots at 60, 80, and 100 yards. Horace A. Ford helped to improve archery standards and pioneered new archery techniques. He won the Grand National 11 times in a row and published a highly influential guide to the sport in 1856.
Towards the end of the 19th century, the sport experienced declining participation as alternative sports such as croquet and tennis became more popular among the middle class. By 1889, just 50 archery clubs were left in Britain, but it was still included as a sport at the 1900 Paris Olympics.
In the United States, primitive archery was revived in the early 20th century. The last of the Yahi Indian tribe, a native known as Ishi, came out of hiding in California in 1911. His doctor, Saxton Pope, learned many of Ishi's traditional archery skills, and popularized them. The Pope and Young Club, founded in 1961 and named in honor of Pope and his friend, Arthur Young, became one of North America's leading bowhunting and conservation organizations. Founded as a nonprofit scientific organization, the Club was patterned after the prestigious Boone and Crockett Club and advocated responsible bowhunting by promoting quality, fair chase hunting, and sound conservation practices.
From the 1920s, professional engineers took an interest in archery, previously the exclusive field of traditional craft experts. They led the commercial development of new forms of bow including the modern recurve and compound bow. These modern forms are now dominant in modern Western archery; traditional bows are in a minority. In the 1980s, the skills of traditional archery were revived by American enthusiasts, and combined with the new scientific understanding. Much of this expertise is available in the "Traditional Bowyer's Bibles" (see Additional reading). Modern game archery owes much of its success to Fred Bear, an American bow hunter and bow manufacturer.
Mythology.
Deities and heroes in several mythologies are described as archers, including the Greek Artemis and Apollo, the Roman Diana and Cupid, the Germanic Agilaz, continuing in legends like those of Wilhelm Tell, Palnetoke, or Robin Hood. Armenian Hayk and Babylonian Marduk, Indian Arjuna, Abhimanyu, Eklavya, Karna, Bhishma, Drona, Parashurama Rama, and Shiva, and Persian Arash were all archers. Earlier Greek representations of Heracles normally depict him as an archer.
The Nymphai Hyperboreioi (Νύμφαι Ὑπερβόρειοι) were worshipped on the Greek island of Delos as attendants of Artemis, presiding over aspects of archery; Hekaerge (Ἑκαέργη), represented distancing, Loxo (Λοξώ), trajectory, and Oupis (Οὖπις), aim.
In East Asia, Yi the archer and his apprentice Feng Meng appear in several early Chinese myths, and the historical character of Zhou Tong features in many fictional forms. Jumong, the first Taewang of the Goguryeo kingdom of the Three Kingdoms of Korea, is claimed by legend to have been a near-godlike archer. Archery features in the story of Oguz Khagan.
In West African Yoruba belief, Osoosi is one of several deities of the hunt who are identified with bow and arrow iconography and other insignia associated with archery.
Equipment.
Types of bows.
While there is great variety in the construction details of bows (both historic and modern), all bows consist of a string attached to elastic limbs that store mechanical energy imparted by the user drawing the string. Bows may be broadly split into two categories: those drawn by pulling the string directly and those that use a mechanism to pull the string.
Directly drawn bows may be further divided based upon differences in the method of limb construction, notable examples being self bows, laminated bows and composite bows. Bows can also be classified by the bow shape of the limbs when unstrung; in contrast to traditional European straight bows, a recurve bow and some types of Longbow have tips that curve away from the archer when the bow is unstrung. The cross-section of the limb also varies; the classic longbow is a tall bow with narrow limbs that are D-shaped in cross section, and the flatbow has flat wide limbs that are approximately rectangular in cross-section. The classic D-shape comes from the use of the wood of the yew tree. The sap-wood is best suited to the tension on the back of the bow, and the heart-wood to the compression on the belly. Hence, a cross-section of a yew longbow shows the narrow, light-coloured sap-wood on the 'straight' part (riser) of the D, and the red/orange heartwood forms the curved part of the D, to balance the mechanical tension/compression stress. Cable-backed bows use cords as the back of the bow; the draw weight of the bow can be adjusted by changing the tension of the cable. They were widespread among Inuit who lacked easy access to good bow wood. One variety of cable-backed bow is the Penobscot bow or Wabenaki bow, invented by Frank Loring (Chief Big Thunder) about 1900. It consists of a small bow attached by cables on the back of a larger main bow.
In different cultures, the arrows are released from either the left or right side of the bow, and this affects the hand grip and position of the bow. In Arab archery, Turkish archery and Kyūdō, the arrows are released from the right hand side of the bow, and this affects construction of the bow. In western archery, the arrow is usually released from the left hand side of the bow.
Compound bows are designed to reduce the force required to hold the string at full draw, hence allowing the archer more time to aim with less muscular stress. Most compound designs use cams or elliptical wheels on the ends of the limbs to achieve this. A typical let-off is anywhere from 65%–80%. For example, a 60-pound bow with 80% let-off only requires 12 pounds of force to hold at full draw. Up to 99% let-off is possible. The compound bow was invented by Holless Wilbur Allen in the 1960s (a US patent was filed in 1966 and granted in 1969) and it has become the most widely used type of bow for all forms of archery in North America.
Mechanically drawn bows typically have a stock or other mounting, such as the crossbow. Crossbows typically have shorter draw lengths compared to compound bows. Because of this, heavier draw weights are required to achieve the same energy transfer to the arrow. These mechanically drawn bows also have devices to hold the tension when the bow is fully drawn. They are not limited by the strength of a single archer and larger varieties have been used as siege engines.
Types of arrows and fletchings.
The most common form of arrow consists of a shaft with an arrowhead attached to the front end and with fletchings and a nock attached to the other end. Arrows across time and history are normally carried in a container known as a quiver, which can take many different forms. Shafts of arrows are typically composed of solid wood, bamboo, fiberglass, aluminium alloy, carbon fiber, or composite materials. Wooden arrows are prone to warping. Fiberglass arrows are brittle, but can be produced to uniform specifications easily. Aluminium shafts were a very popular high-performance choice in the latter half of the 20th century due to their straightness, lighter weight, and subsequently higher speed and flatter trajectories. Carbon fiber arrows became popular in the 1990s and are very light, flying even faster and flatter than aluminium arrows. Today, arrows made up of composite materials are the most popular tournament arrows at Olympic Events, especially the Easton X10 and A/C/E.
The arrowhead is the primary functional component of the arrow. Some arrows may simply use a sharpened tip of the solid shaft, but separate arrowheads are far more common, usually from metal, stone, or other hard materials. The most commonly used forms are target points, field points, and broadheads, although there are also other types, such as bodkin, judo, and blunt heads.
Fletching is traditionally made from bird feathers. Also solid plastic vanes and thin sheetlike spin vanes are used. They are attached near the nock (rear) end of the arrow with thin double sided tape, glue, or, traditionally, sinew. Three fletches is the most common configuration in all cultures, though as many as six have been used. Two makes the arrow unstable in flight. When "three-fletched" the fletches are equally spaced around the shaft with one placed such that it is perpendicular to the bow when nocked on the string (though with modern equipment, variations are seen especially when using the modern spin vanes). This fletch is called the "index fletch" or "cock feather" (also known as "the odd vane out" or "the nocking vane") and the others are sometimes called the "hen feathers". Commonly, the cock feather is of a different color. However, if archers are using fletching made of feather or similar material, they may use same color vanes, as different dyes can give varying stiffness to vanes, resulting in less precision. When "four-fletched", often two opposing fletches are cock feathers and occasionally the fletches are not evenly spaced.
The fletching may be either "parabolic" (short feathers in a smooth parabolic curve) or "shield" (generally shaped like half of a narrow shield) cut and is often attached at an angle, known as "helical" fletching, to introduce a stabilizing spin to the arrow while in flight. Whether helicial or straight fletched, when natural fletching (bird feathers) are used it is critical that all feathers come from the same side of the bird. Oversized fletchings can be used to accentuate drag and thus limit the range of the arrow significantly; these arrows are called "flu-flus". Misplacement of fletchings can often change the arrow's flight path dramatically.
Bow string.
Dacron and other modern materials offer high strength for their weight and are used on most modern bows. Linen and other traditional materials are still used on traditional bows. Almost any fiber can be made into a bow string. The author of "Arab Archery" suggests the hide of a young, emaciated camel. Njál's saga describes the refusal of a wife, Hallgerður, to cut her hair to make an emergency bowstring for her husband, Gunnar Hámundarson, who is then killed.
Protective equipment.
Most archers wear a bracer (also known as an arm-guard) to protect the inside of the bow arm from being hit by the string and prevent clothing from catching the bow string. The bracer does not brace the arm; the word comes from the armoury term "brassard", meaning an armoured sleeve or badge. The Navajo people have developed highly ornamented bracers as non-functional items of adornment. Some archers (mostly women) also wear protection on their chests, called chestguards or plastrons. The myth of the Amazons was that they had one breast removed to solve this problem. Roger Ascham mentions one archer, presumably with an unusual shooting style, who wore a leather guard for his face.
The drawing digits are normally protected by a leather tab, glove, or thumb ring. A simple tab of leather is commonly used, as is a skeleton glove. Medieval Europeans probably used a complete leather glove.
Eurasiatic archers who used the thumb or Mongolian draw protected their thumbs, usually with leather according to the author of "Arab Archery", but also with special rings of various hard materials. Many surviving Turkish and Chinese examples are works of considerable art. Some are so highly ornamented that the users could not have used them to loose an arrow. Possibly these were items of personal adornment, and hence value, remaining extant whilst leather had virtually no intrinsic value and would also deteriorate with time. In traditional Japanese archery a special glove is used that has a ridge to assist in drawing the string.
Release aids.
A release aid is a mechanical device designed to give a crisp and precise loose of arrows from a compound bow. In the most commonly used, the string is released by a finger-operated trigger mechanism, held in the archer's hand or attached to their wrist. In another type, known as a back-tension release, the string is automatically released when drawn to a pre-determined tension.
Stabilizers.
Stabilizers are mounted on the front of the bow below the handle and on the right side, below the handle to help aiming by keeping the bow steady.
Shooting technique and form.
The standard convention on teaching archery is to hold the bow depending upon eye dominance (though in modern Kyudo all archers are trained to hold the bow in the left hand). Therefore, if one is right-eye dominant, they would hold the bow in the left hand and draw the string with the right hand. However, not everyone agrees with this line of thought. A smoother, and more fluid release of the string will produce the most consistently repeatable shots, and therefore may provide greater accuracy of the arrow flight. Some believe that the hand with the greatest dexterity should therefore be the hand that draws and releases the string. Either eye can be used for aiming, and the less dominant eye can be trained over time to become more effective for use. To assist with this, an eye patch can be temporarily worn over the dominant eye.
The hand that holds the bow is referred to as the "bow hand" and its arm the "bow arm". The opposite hand is called the "drawing hand" or "string hand". Terms such as "bow shoulder" or "string elbow" follow the same convention.
If shooting according to eye dominance, right-eye-dominant archers shooting conventionally hold the bow with their left hand. If shooting according to hand dexterity, the archer draws the string with the hand that possesses the greatest dexterity, regardless of eye dominance.
Modern form.
To shoot an arrow, an archer first assumes the correct stance. The body should be at or nearly perpendicular to the target and the shooting line, with the feet placed shoulder-width apart. As an archer progresses from beginner to a more advanced level other stances such as the "open stance" or the "closed stance" may be used, although many choose to stick with a "neutral stance". Each archer has a particular preference, but mostly this term indicates that the leg furthest from the shooting line is a half to a whole foot-length from the other foot, on the ground.
To load, the bow is pointed toward the ground, tipped slightly clockwise of vertical (for a right handed shooter) and the shaft of the arrow is placed on the arrow rest or shelf. The back of the arrow is attached to the bowstring with the nock (a small locking groove located at the proximal end of the arrow). This step is called "nocking the arrow". Typical arrows with three vanes should be oriented such that a single vane, the "cock feather", is pointing away from the bow, to improve the clearance of the arrow as it passes the arrow rest.
A compound bow is fitted with a special type of arrow rest, known as a launcher, and the arrow is usually loaded with the cock feather/vane pointed either up, or down, depending upon the type of launcher being used.
The bowstring and arrow are held with three fingers, or with a mechanical arrow release. Most commonly, for finger shooters, the index finger is placed above the arrow and the next two fingers below, although several other techniques have their adherents around the world, involving three fingers below the arrow, or an arrow pinching technique. "Instinctive" shooting is a technique eschewing sights and is often preferred by traditional archers (shooters of longbows and recurves). In either the split finger or three finger under case, the string is usually placed in the first or second joint, or else on the pads of the fingers.
Another type of string hold, used on traditional bows, is the type favoured by the Mongol warriors, known as the "thumb release", style. This involves using the thumb to draw the string, with the fingers curling around the thumb to add some support. To release the string, the fingers are opened out and the thumb relaxes to allow the string to slide off the thumb. When using this type of release, the arrow should rest on the same side of the bow as the drawing hand i.e. Left hand draw = arrow on left side of bow.
The archer then raises the bow and draws the string, with varying alignments for vertical versus slightly canted bow positions. This is often one fluid motion for shooters of recurves and longbows, which tend to vary from archer to archer. Compound shooters often experience a slight jerk during the drawback, at around midpoint where the draw weight is at its maximum—before relaxing into a comfortable stable full draw position. The archer draws the string hand towards the face, where it should rest lightly at a fixed "anchor point". This point is consistent from shot to shot, and is usually at the corner of the mouth, on the chin, to the cheek, or to the ear, depending on preferred shooting style. The archer holds the bow arm outwards, toward the target. The elbow of this arm should be rotated so that the inner elbow is perpendicular to the ground, though archers with hyper extendable elbows tend to angle the inner elbow toward the ground, as exemplified by the Korean archer Jang Yong-Ho. This keeps the forearm out of the way of the bowstring.
In modern form, the archer stands erect, forming a "T". The archer's lower trapezius muscles are used to pull the arrow to the anchor point. Some modern bows are equipped with a mechanical device, called a clicker, which produces a clicking sound when the archer reaches the correct draw length. In contrast, traditional English Longbow shooters step "into the bow", exerting force with both the bow arm and the string hand arm simultaneously, especially when using bows having draw weights from 100 lbs to over 175 lbs. Heavily-stacked traditional bows (recurves, long bows, and the like) are released immediately upon reaching full draw at maximum weight, whereas compound bows reach their maximum weight in or around mid-draw, dropping holding weight significantly at full draw. Compound bows are often held at full draw for a short time to achieve maximum accuracy.
The arrow is typically released by relaxing the fingers of the drawing hand (see Bow draw), or triggering the mechanical release aid. Usually the release aims to keep the drawing arm rigid, the bow hand relaxed, and the arrow is moved back using the back muscles, as opposed to using just arm motions. An archer should also pay attention to the recoil or "follow through" of his or her body, as it may indicate problems with form (technique) that affect accuracy.
Aiming methods.
There are two main forms of aiming in archery: using a mechanical or fixed sight, or barebow.
Mechanical sights can be affixed to the bow to aid in aiming. They can be as simple as a pin, or may use optics with magnification. They usually also have a peep sight (rear sight) built into the string, which aids in a consistent anchor point. Modern compound bows automatically limit the draw length to give a consistent arrow velocity, while traditional bows allow great variation in draw length. Some bows use mechanical methods to make the draw length consistent. Barebow archers often use a sight picture, which includes the target, the bow, the hand, the arrow shaft and the arrow tip, as seen at the same time by the archer. With a fixed "anchor point" (where the string is brought to, or close to, the face), and a fully extended bow arm, successive shots taken with the sight picture in the same position fall on the same point. This lets the archer adjust aim with successive shots to achieve accuracy.
Modern archery equipment usually includes sights. Instinctive aiming is used by many archers who use traditional bows. The two most common forms of a non-mechanical release are split-finger and three-under. Split-finger aiming requires the archer to place the index finger above the nocked arrow, while the middle and ring fingers are both placed below. Three-under aiming places the index, middle, and ring fingers under the nocked arrow. This technique allows the archer to better look down the arrow since the back of the arrow is closer to the dominant eye, and is commonly called "gun barreling" (referring to common aiming techniques used with firearms).
When using short bows or shooting from horseback, it is difficult to use the sight picture. The archer may look at the target, but without including the weapon in the field of accurate view. Aiming then involves hand-eye coordination—which includes proprioception and motor-muscle memory, similar to that used when throwing a ball. With sufficient practice, such archers can normally achieve good practical accuracy for hunting or for war. Aiming without a sight picture may allow more rapid shooting.
Instinctive shooting is a style of shooting that includes the barebow aiming method that relies heavily upon the subconscious mind, proprioception, and motor/muscle memory to make aiming adjustments; the term used to refer to a general category of archers who did not use a mechanical or fixed sight.
Physics.
When a projectile is thrown by hand, the speed of the projectile is determined by the kinetic energy imparted by the thrower's muscles performing work. However, the energy must be imparted over a limited distance (determined by arm length) and therefore (because the projectile is accelerating) over a limited time, so the limiting factor is not work but rather power, which determined how much energy can be added in the limited time available. Power generated by muscles, however, is limited by force–velocity relationship, and even at the optimal contraction speed for power production, total work by the muscle is less than half of what it would be if the muscle contracted over the same distance at slow speeds, resulting in less than 1/4 the projectile launch velocity possible without the limitations of the force–velocity relationship.
When a bow is used, the muscles are able to perform work much more slowly, resulting in greater force and greater work done. This work is stored in the bow as elastic potential energy, and when the bowstring is released, this stored energy is imparted to the arrow much more quickly than can be delivered by the muscles, resulting in much higher velocity and, hence, greater distance. This same process is employed by frogs, which use elastic tendons to increase jumping distance. In archery, some energy dissipates through elastic hysteresis, reducing the overall amount released when the bow is shot. Of the remaining energy, some is dampened both by the limbs of the bow and the bowstring. Depending on the arrow's elasticity, some of the energy is also absorbed by compressing the arrow, primarily because the release of the bowstring is rarely in line with the arrow shaft, causing it to flex out to one side. This is because the bowstring accelerates faster than the archer's fingers can open, and consequently some sideways motion is imparted to the string, and hence arrow nock, as the power and speed of the bow pulls the string off the opening fingers.
Even with a release aid mechanism some of this effect is usually experienced, since the string always accelerates faster than the retaining part of the mechanism. This makes the arrow oscillate in flight—its center flexing to one side and then the other repeatedly, gradually reducing as the arrow's flight proceeds. This is clearly visible in high-speed photography of arrows at discharge. A direct effect of these energy transfers can clearly be seen when dry firing. Dry firing refers to releasing the bow string without a nocked arrow. Because there is no arrow to receive the stored potential energy, all the energy stays in the bow. Some have suggested that dry firing may cause physical damage to the bow, such as cracks and fractures—and because most bows are not specifically made to handle the high amounts of energy dry firing produces, should never be done.
Modern arrows are made to a specified 'spine', or stiffness rating, to maintain matched flexing and hence accuracy of aim. This flexing can be a desirable feature, since, when the spine of the shaft is matched to the acceleration of the bow(string), the arrow bends or flexes around the bow and any arrow-rest, and consequently the arrow, and fletchings, have an un-impeded flight. This feature is known as the archer's paradox. It maintains accuracy, for if part of the arrow struck a glancing blow on discharge, some inconsistency would be present, and the excellent accuracy of modern equipment would not be achieved.
The accurate flight of an arrow is dependent on its fletching. The arrow's manufacturer (a "fletcher") can arrange fletching to cause the arrow to rotate along its axis. This improves accuracy by evening pressure buildups that would otherwise cause the arrow to "plane" on the air in a random direction after shooting. Even with a carefully made arrow, the slightest imperfection or air movement causes some unbalanced turbulence in air flow. Consequently, rotation creates an equalization of such turbulence, which, overall, maintains the intended direction of flight i.e. accuracy. This rotation is not to be confused with the rapid gyroscopic rotation of a rifle bullet. Fletching that is not arranged to induce rotation still improves accuracy by causing a restoring drag any time the arrow tilts from its intended direction of travel.
The innovative aspect of the invention of the bow and arrow was the amount of power delivered to an extremely small area by the arrow. The huge ratio of length vs. cross sectional area, coupled with velocity, made the arrow more powerful than any other hand held weapon until firearms were invented. Arrows can spread or concentrate force, depending on the application. Practice arrows, for instance, have a blunt tip that spreads the force over a wider area to reduce the risk of injury or limit penetration. Arrows designed to pierce armor in the Middle Ages used a very narrow and sharp tip ("bodkinhead") to concentrate the force. Arrows used for hunting used a narrow tip ("broadhead") that widens further, to facilitate both penetration and a large wound.
Hunting.
Using archery to take game animals is known as "bow hunting". Bow hunting differs markedly from hunting with firearms, as distance between hunter and prey must be much shorter to ensure a humane kill. The skills and practices of bow hunting therefore emphasize very close approach to the prey, whether by still hunting, stalking, or waiting in a blind or tree stand. In many countries, including much of the United States, bow hunting for large and small game is legal. Bow hunters generally enjoy longer seasons than are allowed with other forms of hunting such as black powder, shotgun, or rifle. Usually, compound bows are used for large game hunting and may feature fiber optic sights and other enhancements. Using a bow and arrow to take fish is known as "bow fishing".
Modern competitive archery.
Competitive archery involves shooting arrows at a target for accuracy from a set distance or distances. This is the most popular form of competitive archery worldwide and is called target archery. A form particularly popular in Europe and America is field archery, shot at targets generally set at various distances in a wooded setting. Competitive archery in the United States is governed by USA Archery and National Field Archery Association (NFAA), which also certifies instructors.
Para-Archery is an adaptation of archery for athletes with a disability. It is governed by the World Archery Federation (WA), and is one of the sports in the Summer Paralympic Games. There are also several other lesser-known and historical forms of archery, as well as archery novelty games and flight archery, where the aim is to shoot the greatest distance.
Additionally, a game known variously by names such as "Archery Golf" or "Gol-Archery" adopts some of the rules of golf to play a game that uses traditional or modern archery tools. Players shoot arrows or similar projectiles into a series of holes, goals or targets on a course, with the objective being to do so using the fewest number of shots.
Archery golf is mentioned in the United States media as early as 1923 and has been played in tournaments ever since.
Archery golf shares some qualities with clout archery and flight archery, and is especially similar to the oldest form of competitive archery, roving marks, but it is unique in that a player may shoot at or toward a single target as many times as necessary to complete that "hole".

</doc>
<doc id="2009" url="https://en.wikipedia.org/wiki?curid=2009" title="Alvar Aalto">
Alvar Aalto

Hugo Alvar Henrik Aalto (; 3 February 1898 – 11 May 1976) was a Finnish architect and designer, as well as a sculptor and painter. His work includes architecture, furniture, textiles and glassware. Aalto's early career runs in parallel with the rapid economic growth and industrialization of Finland during the first half of the twentieth century and many of his clients were industrialists; among these were the Ahlström-Gullichsen family. The span of his career, from the 1920s to the 1970s, is reflected in the styles of his work, ranging from Nordic Classicism of the early work, to a rational International Style Modernism during the 1930s to a more organic modernist style from the 1940s onwards. His furniture designs were considered Scandinavian Modern. What is typical for his entire career, however, is a concern for design as a Gesamtkunstwerk, a "total work of art"; whereby he – together with his first wife Aino Aalto – would design not just the building, but give special treatments to the interior surfaces and design furniture, lamps, and furnishings and glassware. The Alvar Aalto Museum, designed by Aalto himself, is located in what is regarded as his home city Jyväskylä.
Biography.
Life.
Hugo Alvar Henrik Aalto was born in Kuortane, Finland. His father, Johan Henrik Aalto, was a Finnish-speaking land-surveyor and his mother, Selly (Selma) Matilda (née Hackstedt) was a Swedish-speaking postmistress. When Aalto was 5 years old, the family moved to Alajärvi, and from there to Jyväskylä in Central Finland. Aalto studied at the Jyväskylä Lyceum school, where he completed his basic education in 1916 and took drawing lessons from a local artist named Jonas Heiska. In 1916 he then enrolled to study architecture at the Helsinki University of Technology. His studies were interrupted by the Finnish Civil War, which he fought in. He fought on the side of the "White Army" and fought at the Battle of Länkipohja and the Battle of Tampere. He built his first piece of architecture while still a student, a house for his parents, at Alajärvi. Afterwards, he continued his education, graduating in 1921. In the summer of 1922 he began his official military service, finishing at the Hamina reserve officer training school, and was promoted to reserve second lieutenant in June 1923.
In 1920, while still a student, Aalto made his first trip abroad, travelling via Stockholm to Gothenburg, where he even briefly found work with the architect Arvid Bjerke. In 1922, he accomplished his first independent piece at the Industrial Exposition in Tampere. In 1923 he returned to Jyväskylä, where he opened his first architectural office, under the name 'Alvar Aalto, Architect and Monumental Artist'. At that same time he also wrote articles for the Jyväskylä newspaper "Sisä-Suomi" under the pseudonym Remus. During this time, he designed a number of small single-family houses in Jyväskylä, and the office's workload steadily increased. In 1925, he married architect Aino Marsio. Their honeymoon journey to Italy was Aalto's first trip there, though Aino had previously made a study trip there. The latter trip together sealed an intellectual bond with the culture of the Mediterranean region that was to remain important to Aalto for the rest of his life. On their return, they continued with a number of local projects, notably the Jyväskylä Worker's Club. However, the Aaltos moved their office to Turku in 1927, and started collaborating with architect Erik Bryggman. The office moved again in 1933 to Helsinki.
The Aaltos designed and built a joint house-office (1935–36) for themselves in Munkkiniemi, Helsinki, but later (1954–56) had a purpose-built office erected in the same neighbourhood – nowadays the former is a "house museum" and the latter the premises of the Alvar Aalto Academy. In 1926 the young Aaltos designed and had built a summer cottage in Alajärvi, Villa Flora. In 1938, the Aaltos visited the United States for the first time, ostensibly to visit the Finnish Pavilion, which they had designed, for the New York World Fair of the following year. Aino Aalto died of cancer in 1949. Aino and Alvar Aalto had 2 children, a daughter Johanna "Hanni" Alanen, born Aalto, 1925, and a son Hamilkar Aalto, 1928. In 1952 Aalto married architect Elissa Mäkiniemi (died 1994), who had been working as an assistant in his office. In 1952 Aalto designed and had built a summer cottage, the so-called Experimental House, for himself and his new wife in Muuratsalo in Central Finland. Alvar Aalto died on 11 May 1976, in Helsinki, and is buried in the Hietaniemi cemetery in Helsinki.
Architecture career.
Early career: classicism.
Although he is sometimes regarded as among the first and most influential architects of Nordic modernism, a closer examination of the historical facts reveals that Aalto (while a pioneer in Finland) closely followed and had personal contacts with other pioneers in Sweden, in particular Gunnar Asplund and Sven Markelius. What they and many others of that generation in the Nordic countries had in common was that they started off from a classical education and were first designing classical architecture, though what historians now call Nordic Classicism – a style that had been a reaction to the previous dominant style of National Romanticism – before moving, in the late 1920s, towards Modernism. On returning to Jyväskylä in 1923 to establish his own architect's office, Aalto busied himself with a number of single-family homes, all designed in the Nordic Classicism style, such as the manor-like house for his mother's cousin Terho Manner in Töysa in 1923, a summer villa for the Jyväskylä chief constable in 1923 and the Alatalo farmhouse in Tarvaala in 1924. During this period he also completed his first public buildings, the Jyväskylä Workers' Club in 1925, the Jyväskylä Defence Corps building in 1926 and the Seinäjoki Defence Corp building in 1924–29. Aalto also entered several architectural competitions for prestigious state public buildings, both in Finland and abroad, including the two competitions for the Finnish Parliament building in 1923 and 1924, the extension to the University of Helsinki in 1931, and the building to house the League of Nations in Geneva, Switzerland, in 1926–27. Furthermore, this was the period when Aalto was most prolific in his writings, with articles for professional journals and newspapers. Among his most well-known essays from this period are "Urban culture" (1924), "Temple baths on Jyväskylä ridge" (1925), "Abbé Coignard's sermon" (1925), and "From doorstep to living room" (1926).
Early career: functionalism.
The shift in Aalto's design approach from classicism to modernism is epitomised by the Viipuri Library (1927–35), which went through a transformation from an originally classical competition entry proposal to the completed high-modernist building. Yet his humanistic approach is in full evidence in the library: the interior displays natural materials, warm colours, and undulating lines. Due to problems over financing and a change of site, the Viipuri Library project lasted eight years, and during that same time he also designed the Turun Sanomat Building (1929–30) and Paimio Sanatorium (1929–32). Thus, the Turun Sanomat Building first heralded Aalto's move towards modernism, and this was then carried forward both in the Paimio Sanatorium and in the ongoing design for the library. Although the Turun Sanomat Building and Paimio Sanatorium are comparatively pure modernist works, they too carried the seeds of his questioning of such an orthodox modernist approach and a move to a more daring, synthetic attitude. It has been said that his work on two of these three buildings (not the Viipuri Library) showed similarities to Walter Gropius' style, in particular his work on the Bauhaus school of design in Dessau. His work on the Viipuri building started to show his individuality in a departure from the European norms.
Through Sven Markelius, Aalto became a member of the Congres Internationaux d'Architecture Moderne (CIAM), attending the second congress in Frankfurt in 1929 and the fourth congress in Athens in 1933, where he established a close friendship with László Moholy-Nagy, Sigfried Giedion and Philip Morton Shand. It was during this time that he followed closely the work of the main driving force behind the new modernism, Le Corbusier, and visited him in his Paris office several times in the following years.
It was not until the completion of the Paimio Sanatorium (1932) and Viipuri Library (1935) that Aalto first achieved world attention in architecture. His reputation grew in the USA following the critical reception of his design for the Finnish Pavilion at the 1939 New York World's Fair, described by Frank Lloyd Wright as a "work of genius". It could be said that Aalto's international reputation was sealed with his inclusion in the second edition of Sigfried Giedion's influential book on Modernist architecture, "Space, Time and Architecture: The growth of a new tradition" (1949), in which Aalto received more attention than any other Modernist architect, including Le Corbusier. In his analysis of Aalto, Giedion gave primacy to qualities that depart from direct functionality, such as mood, atmosphere, intensity of life and even national characteristics, declaring that "Finland is with Aalto wherever he goes".
In 1938, the Museum of Modern Art, in New York organized an exhibit that eventually went on a 12-city tour. Afterwards he visited America for the first time and gave a series of lectures at Yale.
Mid career: experimentation.
During the 1930s Alvar spent some time experimenting with laminated wood, making sculptures, and abstract reliefs, characterized by irregular curved forms. Utilizing this knowledge he was able to solve technical problems concerning the flexibility of wood and also of working out spatial issues in his designs. Aalto's early experiments with wood and his move away from a purist modernism would be tested in built form with the commission to design Villa Mairea (1939) in Noormarkku, the luxury home of the young industrialist couple Harry and Maire Gullichsen. It was Maire Gullichsen who acted as the main client, and she worked closely not only with Alvar but also Aino Aalto on the design, inspiring them to be more daring in their work. The original design was to include a private art gallery, but this was never built. The building forms a U-shape around a central inner "garden" the central feature of which is a kidney-shaped swimming pool. Adjacent to the pool is a sauna executed in a rustic style, alluding to both Finnish and Japanese precedents. The design of the house is a synthesis of numerous stylistic influences, from traditional Finnish vernacular to purist modernism, as well as influences from English and Japanese architecture. While the house is clearly intended for a wealthy family, Aalto nevertheless argued that it was also an experiment that would prove useful in the design of mass housing. It created zones for different activities within the structure.
His increased fame led to offers and commissions outside Finland. In 1941 he accepted an invitation as a visiting professor to Massachusetts Institute of Technology in the USA. This was during the Second World War, and he involved his students in designing low-cost, small-scale housing for the reconstruction of war-torn Finland. While teaching at MIT, Aalto also designed the student dormitory, Baker House, completed in 1948. The dormitory lay along the Charles River and its undulating form provided maximum view and ventilation for each resident. This building was the first building of Aalto's redbrick period. Originally used in Baker House to signify the Ivy League university tradition, on his return to Finland Aalto used it in a number of key buildings, in particular, in several of the buildings in the new Helsinki University of Technology campus (starting in 1950), Säynätsalo Town Hall (1952), Helsinki Pensions Institute (1954), Helsinki House of Culture (1958), as well as in his own summer house, the so-called Experimental House in Muuratsalo (1957).
In the 50's he immersed himself in his sculpting, be it with bronze, marble, or mixed media. This paid off as he produced an outstanding piece for the memorial of the Battle of Suomussalmi (1960), located on the battlefield. It consists of a leaning bronze pillar on a pedestal.
Mature career: monumentalism.
The early 1960s and 1970s (up until his death in 1976) were marked by key works in Helsinki, in particular the huge town plan for the void in centre of Helsinki adjacent to Töölö Bay and the vast railway yards, and marked on the edges by significant buildings such as the National Museum and the main railway station, both by Eliel Saarinen. In his town plan Aalto proposed a line of separate marble-clad buildings fronting the bay which would house various cultural institutions, including a concert hall, opera, museum of architecture and headquarters for the Finnish Academy. The scheme also extended into the Kamppi district with a series of tall office blocks. Aalto first presented his scheme in 1961, but it went through various modifications during the early 1960s. Only two fragments of the overall plan were ever realized: the Finlandia Hall concert hall (1976) fronting Töölö Bay, and an office building in the Kamppi district for the Helsinki Electricity Company (1975). The Miesian formal language of geometric grids employed in the buildings was also used by Aalto for other sites in Helsinki, including the Enso-Gutzeit building (1962), the Academic Bookstore (1962) and the SYP Bank building (1969).
Following Aalto's death in 1976 his office continued to operate under the direction of his widow, Elissa, completing works already to some extent designed. These works include the Jyväskylä City Theatre and Essen opera house. Since the death of Elissa Aalto the office has continued to operate as the Alvar Aalto Academy, giving advice on the restoration of Aalto buildings and organising the vast archive material.
Furniture career.
Whereas Aalto was famous for his architecture, his furniture designs were well thought of and are still popular today. He studied Josef Hoffmann and the Wiener Werkstätte, and for a period of time, worked under Eliel Saarinen. He also gained inspiration from Gebrüder Thonet. During the late 1920s and 1930s he, working closely with Aino Aalto, also focused a lot of his energy on furniture design, partly due to the decision to design much of the individual furniture pieces and lamps for the Paimio Sanatorium. Of particular significance was the experimentation in bent plywood chairs, most notably the so-called Paimio chair, which had been designed for the sitting tuberculosis patient. The Aaltos, together with visual arts promoter Maire Gullichsen and art historian Nils-Gustav Hahl founded the Artek company in 1935, ostensibly to sell Aalto products, but also other imported products. He became the first furniture designer to use the cantilever principle in chair design using wood.
Awards.
Aalto's awards included the Prince Eugen Medal in 1954, the Royal Gold Medal for Architecture from the Royal Institute of British Architects in 1957 and the Gold Medal from the American Institute of Architects in 1963. He was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1957. He also was a member of the Academy of Finland, and was its president from 1963 to 1968. From 1925 to 1956 he was a member of the Congrès International d'Architecture Moderne. In 1960 he received an honorary doctorate at the Norwegian University of Science and Technology (NTNU).
Works.
Aalto's career spans the changes in style from (Nordic Classicism) to purist International Style Modernism to a more personal, synthetic and idiosyncratic Modernism. Aalto's wide field of design activity ranges from the large scale of city planning and architecture to interior design, furniture and glassware design and painting. It has been estimated that during his entire career Aalto designed over 500 individual buildings, approximately 300 of which were built, the vast majority of which are in Finland. He also has a few buildings in France, Germany, Italy and the USA.
Aalto's work with wood, was influenced by early Scandinavian architects; however, his experiments and departure from the norm brought attention to his ability to make wood do things not previously done. His techniques in the way he cut the beech tree, for example, and also his ability to use plywood as structural and aesthetic. Other examples include the rough-hewn vertical placement of logs at his pavilion at the Lapua expo, looking similar to a medieval barricade, at the orchestra platform at turku and the Paris expo at the World Fair, he used varying sizes and shapes of planks. Also at Paris and at Villa Mairea he utilized birch boarding in a vertical arrangement. Also his famous undulating walls and ceilings made of red pine. In his roofing, he created massive spans (155-foot at the covered statium at Otaniemi) all without tie rods. His stairway at Villa Mairea, he evokes feelings of a natural forest by binding beech wood with withes into columns.
Aalto claimed that his paintings were not made as individual artworks but as part of his process of architectural design, and many of his small-scale "sculptural" experiments with wood led to later larger architectural details and forms. These experiments also led to a number of patents: for example, he invented a new form of laminated bent-plywood furniture in 1932. His experimental method had been influenced by his meetings with various members of the Bauhaus design school, especially László Moholy-Nagy, whom he first met in 1930. Aalto's furniture was exhibited in London in 1935, to great critical acclaim, and to cope with the consumer demand Aalto, together with his wife Aino, Maire Gullichsen and Nils-Gustav Hahl founded the company Artek that same year. Aalto glassware (Aino as well as Alvar) is manufactured by Iittala. Aalto was one of the first architects outside of Germany, France, and the Netherlands to master modern architecture.
Aalto's 'High Stool' and 'Stool E60' (manufactured by Artek) are currently used in Apple Stores across the world to serve as seating for customers. Finished in black lacquer, the stools are used to seat customers at the 'Genius Bar' and also in other areas of the store at times when seating is required for a product workshop or special event. Aalto was also influential in bringing modern art to the Finnish people, in particular the work of his friends, Alexander Milne Calder and Fernand Léger.
Critique of Aalto's architecture.
As already mentioned, Aalto's international reputation was sealed with his inclusion in the second edition of Sigfried Giedion's influential book on Modernist architecture, "Space, Time and Architecture: The growth of a new tradition" (1949), in which Aalto received more attention than any other Modernist architect, including Le Corbusier. In his analysis of Aalto, Giedion gave primacy to qualities that depart from direct functionality, such as mood, atmosphere, intensity of life and even national characteristics, declaring that "Finland is with Aalto wherever he goes". However, a few more recent architecture critics and historians have questioned Aalto's position of influence in the canonic history. Italian Marxist architecture historians Manfredo Tafuri and Francesco Dal Co put forward the viewpoint that Aalto's "historical significance has perhaps been rather exaggerated; with Aalto we are outside of the great themes that have made the course of contemporary architecture so dramatic. The qualities of his works have a meaning only as masterful distractions, not subject to reproduction outside the remote reality in which they have their roots." Their viewpoint was propounded by their own priority given to urbanism, seeing Aalto as an anti-urban, and thus consequently disparaging what they regarded as peripheral non-urban areas of the world: "Essentially his architecture is not appropriate to urban typologies." Similarly concerned with the appropriateness of Aalto's form language, at the other end of the political spectrum, American postmodernist critic Charles Jencks made a claim for the need for buildings to signify meaning; however, he then lifted out Aalto's Pensions Institute building as an example of what he termed Aalto's 'soft paternalism': "Conceived as a fragmented mass to break up the feeling of bureaucracy, it succeeds all too well in being humane and killing the pensioner with kindness. The forms are familiar red brick and ribbon-strip windows broken by copper and bronze elements – all carried through with a literal-mindedness that borders on the soporific." But also during Aalto's lifetime he faced critique from his fellow architects in Finland, most notably Kirmo Mikkola and Juhani Pallasmaa; by the last decade of his life Aalto's work was seen as idiosyncratic and individualistic, when the opposing tendencies of rationalism and constructivism – often championed under left-wing politics – argued for anonymous virtually non-aesthetic architecture. Mikkola wrote of Aalto's late works: "Aalto has moved to his present baroque line..."
Memorials.
Aalto has been commemorated in a number of ways:
Further reading.
Göran Schildt has written and edited many books on Aalto, the most well-known being the three-volume biography, usually referred to as the definitive biography on Aalto.

</doc>
<doc id="2011" url="https://en.wikipedia.org/wiki?curid=2011" title="Comparison of American and British English">
Comparison of American and British English

This is one of a series of articles about the differences between British English and American English, which, "for the purposes of these articles", are defined as follows:
Written forms of British and American English as found in newspapers and textbooks vary little in their essential features, with only occasional noticeable differences in comparable media (comparing American newspapers with British newspapers, for example). This kind of formal English, particularly written English, is often called "standard English".
The spoken forms of British English vary considerably, reflecting a long history of dialect development amid isolated populations. In the United Kingdom, dialects, word use and accents vary not only between England, Northern Ireland, Scotland and Wales, but also within them. "Received Pronunciation" (RP) refers to a way of pronouncing standard English that is actually used by about two percent of the UK population. It remains the accent upon which dictionary pronunciation guides are based, and for teaching English as a foreign language. It is referred to colloquially as "the Queen's English", "Oxford English" and "BBC English", although by no means do all graduates of the university speak with such an accent and the BBC no longer requires it or uses it exclusively. The present monarch uses a hyperlect of the Queen's English.
An unofficial standard for "spoken" American English has also developed, as a result of mass media and geographic and social mobility, and broadly describes the English typically heard from network newscasters, commonly referred to as non-regional diction, although local newscasters tend toward more parochial forms of speech. Despite this unofficial standard, regional variations of American English have not only persisted but have actually intensified, according to linguist William Labov.
Regional dialects in the United States typically reflect some elements of the language of the main immigrant groups in any particular region of the country, especially in terms of pronunciation and vernacular vocabulary. Scholars have mapped at least four major regional variations of spoken American English: Northern, Southern, Midland, and Western. After the American Civil War, the settlement of the western territories by migrants from the east led to dialect mixing and levelling, so that regional dialects are most strongly differentiated in the eastern parts of the country that were settled earlier. Localized dialects also exist with quite distinct variations, such as in Southern Appalachia, Boston and the New York City area.
British and American English are the reference norms for English as spoken, written, and taught in the rest of the world, excluding countries where English is spoken natively such as Australia, Canada, Ireland and New Zealand. In many former British Empire countries where English is not spoken natively, British English forms are closely followed, alongside numerous AmE usages which have become widespread throughout the Anglosphere. Conversely, in many countries historically influenced by the United States where English is not spoken natively, American English forms are closely followed. Many of these countries, while retaining strong BrE or AmE influences, have developed their own unique dialects, which include Indian English and Philippine English.
Chief among other native English dialects are Canadian English and Australian English, which rank third and fourth in the number of native speakers. For the most part, Canadian English, while featuring numerous British forms alongside indigenous Canadianisms, shares vocabulary, phonology and syntax with American English, leading many to recognize "North American English" as an organic grouping of dialects. Australian English likewise shares many American and British English usages alongside plentiful features unique to Australia, and retains a significantly higher degree of distinctiveness from both the larger varieties than does Canadian English. South African English, New Zealand English and the Hiberno-English of Ireland are also distinctive and rank fifth, sixth and seventh in the number of native speakers.
Historical background.
The English language was first introduced to the Americas by British colonization, beginning in 1607 in Jamestown, Virginia. Similarly, the language spread to numerous other parts of the world as a result of British trade and colonization elsewhere and the spread of the former British Empire, which, by 1921, held sway over a population of 470–570 million people, approximately a quarter of the world's population at that time.
Over the past 400 years the form of the language used in the Americas—especially in the United States—and that used in the United Kingdom have diverged in a few minor ways, leading to the versions now occasionally referred to as American English and British English. Differences between the two include pronunciation, grammar, vocabulary (lexis), spelling, punctuation, idioms, and formatting of dates and numbers, although the differences in written and most spoken grammar structure tend to be much less than those of other aspects of the language in terms of mutual intelligibility. A small number of words have completely different meanings in the two versions or are even unknown or not used in one of the versions. One particular contribution towards formalizing these differences came from Noah Webster, who wrote the first American dictionary (published 1828) with the intention of showing that people in the United States spoke a different dialect from Britain, much like a regional accent.
This divergence between American English and British English has provided opportunities for humorous comment, e.g., George Bernard Shaw has a character say that the United States and United Kingdom are "two countries divided by a common language"; and Oscar Wilde that "We have really everything in common with America nowadays, except, of course, the language" ("The Canterville Ghost", 1888). Henry Sweet incorrectly predicted in 1877 that within a century American English, Australian English and British English would be mutually unintelligible. It may be the case that increased worldwide communication through radio, television, the Internet and globalization has reduced the tendency towards regional variation. This can result either in some variations becoming extinct (for instance, "the wireless", being progressively superseded by "the radio") or in the acceptance of wide variations as "perfectly good English" everywhere.
Although spoken American and British English are generally mutually intelligible, there are occasional differences which might cause embarrassment—for example, in American English a "rubber" is usually interpreted as a "condom" rather than an "eraser"; and a British "fanny" refers to the female pubic area, while the American "fanny" refers to an "ass" (US) or an "arse" (UK). Likewise the Australian "root" means to have sexual intercourse whilst in American English it means to support someone for success.
Grammar.
Nouns.
Formal and notional agreement.
In British English (BrE), collective nouns can take either singular ("formal agreement") or plural ("notional agreement") verb forms, according to whether the emphasis is on the body as a whole or on the individual members respectively; compare "a committee was appointed" with "the committee were unable to agree". The term "the Government" always takes a plural verb in British civil service convention, perhaps to emphasise the principle of cabinet collective responsibility. Compare also the following lines of Elvis Costello's song "Oliver's Army": "Oliver's Army is here to stay / Oliver's Army are on their way ". Some of these nouns, for example "staff", actually combine with plural verbs most of the time.
In American English (AmE), collective nouns are almost always singular in construction: "the committee was unable to agree". However, when a speaker wishes to emphasize that the individuals are acting separately, a plural pronoun may be employed with a singular or plural verb: "the team takes their seats", rather than "the team takes its seats". Such a sentence would most likely be recast as "the team members take their seats". Despite exceptions such as usage in "The New York Times", the names of sports teams are usually treated as plurals even if the form of the name is singular.
The difference occurs for all nouns of multitude, both general terms such as "team" and "company" and proper nouns (for example where a place name is used to refer to a sports team). For instance,
Proper nouns that are plural in form take a plural verb in both AmE and BrE; for example, "The Beatles are a well-known band"; "The Patriots are the champions", with one major exception: in American English, "the United States" is almost universally used with a singular verb. Although the construction "the United States are" was more common early in the history of the country, as the singular federal government exercised more authority and a singular national identity developed (especially following the American Civil War) it became standard to treat "the United States" as a singular noun.
Verbs.
Transitivity.
The following verbs show differences in transitivity between BrE and AmE:
Vocabulary.
Most of the differences in lexis or vocabulary between British and American English are in connection with concepts originating from the 19th century to the mid 20th century, when new words were coined independently. Almost the entire vocabularies of the car/automobile and railway/railroad industries (see Rail terminology) are different between the UK and US, for example. Other sources of difference are slang or vulgar terms (where frequent new coinage occurs) and idiomatic phrases, including phrasal verbs. The differences most likely to create confusion are those where the same word or phrase is used for two different concepts. Regional variations, even within the US or the UK, can create the same problems. From the mid-20th century, movies and television have spread new words in both countries, usually from the US to the UK.
It is not a straightforward matter to classify differences of vocabulary. David Crystal identifies some of the problems of classification on the facing page to his list of American English/British English lexical variation and states "this should be enough to suggest caution when working through an apparently simple list of equivalents".
Overview of lexical differences.
"Note: A lexicon is not made up of different words but different "units of meaning" (lexical units or lexical items e.g., "fly ball" in baseball), including idioms and figures of speech. This makes it easier to compare the dialects."
Though the influence of cross-culture media has done much to familiarize BrE and AmE speakers with each other's regional words and terms, many words are still recognized as part of a single form of English. Though the use of a British word would be acceptable in AmE (and vice versa), most listeners would recognize the word as coming from the other form of English and treat it much the same as a word borrowed from any other language.
Words and phrases that have their origins in BrE.
Most speakers of AmE are aware of some BrE terms, although they may not generally use them or may be confused as to whether someone intends the American or British meaning (such as for "biscuit"). It is generally very easy to guess what some words, such as "driving licence", mean. However, use of many other British words such as "naff" (slang but commonly used to mean "not very good") are unheard of in American English.
Words and phrases that have their origins in AmE.
Speakers of BrE are likely to understand most common AmE terms, examples such as "sidewalk" (pavement), "gas (gasoline/petrol)", "counterclockwise" (anticlockwise) or "elevator (lift)", without any problem, thanks in part to considerable exposure to American popular culture and literature. Certain terms that are heard less frequently, especially those likely to be absent or rare in American popular culture, e.g., "copacetic (satisfactory)", are unlikely to be understood by most BrE speakers.
Divergence.
Words and phrases with different meanings.
Words such as "bill" and "biscuit" are used regularly in both AmE and BrE but mean different things in each form. In AmE a bill is usually paper money (as in "dollar bill") though it can mean the same as in BrE, an invoice (as in "the repair bill was £250"). In AmE a biscuit is what in BrE is called a scone. In BrE a biscuit is what AmE calls a cookie. As chronicled by Winston Churchill, the opposite meanings of the verb "to table" created a misunderstanding during a meeting of the Allied forces; in BrE to table an item on an agenda means to "open it up" for discussion whereas in AmE, it means to "remove" it from discussion, or at times, to suspend or delay discussion.
The word "football" in BrE refers to Association football, also known as soccer. In AmE, "football" means American football. The standard AmE term "soccer", a contraction of "association (football)", is of British origin, derived from the formalization of different codes of football in the 19th century, and was a fairly unremarkable usage (possibly marked for class) in BrE until relatively recently; it has lately become perceived incorrectly as an Americanism. In international (i.e. non-American) context, particularly in sports news outside English-speaking North America, American (or US branches of foreign) news agencies also use "football" to mean "soccer", especially in direct quotes.
Similarly, the word "hockey" in BrE refers to field hockey and in AmE, "hockey" means ice hockey.
Other ambiguity (complex cases).
Words with completely different meanings are relatively few; most of the time there are either (1) words with one or more shared meanings and one or more meanings unique to one variety (for example, bathroom and toilet) or (2) words the meanings of which are actually common to both BrE and AmE but that show differences in frequency, connotation or denotation (for example, "smart", "clever", "mad").
Some differences in usage and/or meaning can cause confusion or embarrassment. For example the word "fanny" is a slang word for vulva in BrE but means buttocks in AmE—the AmE phrase "fanny pack" is "bum bag" in BrE. In AmE the word "fag" (short for "faggot") is a highly offensive term for a homosexual male but in BrE it is a normal and well-used term for a cigarette, for hard work, or for a chore, while a faggot itself is a sort of meatball. In AmE the word "pissed" means being annoyed whereas in BrE it is a coarse word for being drunk (in both varieties, "pissed off" means irritated).
Similarly, in AmE the word "pants" is the common word for the BrE "trousers" and "knickers" refers to a variety of half-length trousers, while the majority of BrE speakers would understand "pants" to mean "underpants" and "knickers" to mean "female underpants".
Sometimes the confusion is more subtle. In AmE the word "quite" used as a qualifier is generally a reinforcement: for example, "I'm quite hungry" means "I'm very hungry". In BrE "quite" (which is much more common in conversation) may have this meaning, as in "quite right" or "quite mad", but it more commonly means "somewhat", so that in BrE "I'm quite hungry" can mean "I'm somewhat hungry". This divergence of use can lead to misunderstanding.
Social and cultural differences.
Lexical items that reflect separate social and cultural development.
Education.
School.
The US has a more uniform nationwide system of terms than does the U.K., but the division by grades varies somewhat among the states and even among local school districts. For example, "elementary school" often includes kindergarten, and may include sixth grade, with "middle school" including only two grades or extending to ninth grade.
In the UK the US equivalent of a "high school" is often referred to as a "secondary school" regardless of whether it is state funded or private. Secondary education in the United States also includes "middle school" or "junior high school", a two- or three-year transitional school between elementary school and high school. "Middle school" is sometimes used in the UK as a synonym for the younger "junior school", covering the second half of the primary curriculum—current years four to six in some areas. However, in Dorset (South England) it is used to describe the second school in the three-tier system, which is normally from year five to year eight. In other regions, such as Evesham and the surrounding area in Worcestershire, the second tier goes from year six to year eight, and both starting secondary school in year nine. In Kirklees, West Yorkshire in the villages of the Dearne Valley there is a three tier system: first schools year reception to year five, middle school (Scissett/Kirkburton Middle School) year six to year eight and high school ()year 9 to year 13.
A "public school" has opposite meanings in the two countries. In AmE this is a government-owned institution open to all students, supported by public funding. The BrE use of the term is in contrast with "private" education, i.e., to be educated privately with a tutor. In England and Wales the term strictly refers to an ill-defined group of prestigious private independent schools funded by students' fees, although it is often more loosely used to refer to any independent school. Independent schools are also known as "private schools", and the latter is the term used in Scotland and Northern Ireland for all such fee-funded schools. Strictly, the term "public school" is not used in Scotland and Northern Ireland in the same sense as in England, but nevertheless Gordonstoun, the Scottish private school, is sometimes referred to as a "public school", as are some other Scottish private schools. Government-funded schools in Scotland and Northern Ireland are properly referred to as "state schools"—but are sometimes confusingly referred to as "public schools" (with the same meaning as in the US); whereas in the US, where most public schools are administered by local governments, a "state school" is typically a college or university run by one of the states.
Speakers in both the United States and the United Kingdom use several additional terms for specific types of secondary school. A US "prep school" or "preparatory school" is an independent school funded by tuition fees; the same term is used in the UK for a private school for pupils under thirteen, designed to prepare them for fee-paying public schools. An American "catholic schools" cover costs through tuition and have affiliations with a religious institution, most often a Catholic church or diocese. In England, where the state-funded education system grew from parish schools organized by the local established church, the Church of England (C. of E., or C.E.), and many schools, especially primary schools (up to age 11) retain a church connection and are known as "church schools", "C.E. schools" or "C.E. (aided) schools". There are also "faith schools" associated with the Roman Catholic Church and other major faiths, with a mixture of funding arrangements.
In the US, a "magnet school" receives government funding and has special admission requirements: in some cases pupils gain admission through superior performance on admission tests, while other magnet schools admit students through a lottery. The UK has city academies, which are independent privately sponsored schools run with public funding and which can select up to 10% of pupils by aptitude. Moreover in the UK 36 local education authorities retain selection by ability at 11. They maintain grammar schools (state funded secondary schools), which admit pupils according to performance in an examination (known as the 11+) and comprehensive schools that take pupils of all abilities. Grammar schools select the most academically able 10% to 23% of those who sit the exam. Students who fail the exam go to a secondary modern school, sometimes called a "high school", or increasingly an "academy". In areas where there are no grammar schools the comprehensives likewise may term themselves high schools or academies. Nationally only 6% of pupils attend grammar schools, mainly in four distinct counties. Some private schools are called "grammar schools", chiefly those that were grammar schools long before the advent of state education.
University.
In the UK a university student is said to "study", to "read" or, informally, simply to "do" a subject. In the recent past the expression 'to read a subject' was more common at the older universities such as Oxford and Cambridge. In the US a student "studies" or "majors in" a subject (although "concentration" or "emphasis" is also used in some US colleges or universities to refer to the major subject of study). "To major in" something refers to the student's principal course of study; "to study" may refer to any class being taken.
BrE:
AmE:
At university level in BrE, each "module" is taught or facilitated by a "lecturer" or "tutor"; "professor" is the job-title of a senior academic (in AmE, at some universities, the equivalent of the BrE lecturer is instructor, especially when the teacher has a lesser degree or no university degree, though the usage may become confusing according to whether the subject being taught is considered technical or not; it is also different from adjunct instructor/professor). In AmE each "class" is generally taught by a "professor" (although some US tertiary educational institutions follow the BrE usage), while the position of "lecturer" is occasionally given to individuals hired on a temporary basis to teach one or more classes and who may or may not have a doctoral degree.
The word "course" in American use typically refers to the study of a restricted topic or individual subject (for example, "a course in Early Medieval England", "a course in integral calculus") over a limited period of time (such as a semester or term) and is equivalent to a "module" or sometimes "unit" at a British university. In the UK, a "course of study" or simply "course" is likely to refer to the entire programme of study, which may extend over several years and be made up of any number of "modules," hence it is also practically synonymous to a degree programme. A few university-specific exceptions exist: for example, at Cambridge the word "paper" is used to refer to a "module", while the whole course of study is called "tripos".
A "dissertation" in AmE refers to the final written product of a doctoral student to fulfil the requirement of that program. In BrE, the same word refers to the final written product of a student in an undergraduate or taught master's programme. A dissertation in the AmE sense would be a thesis in BrE, though "dissertation" is also used.
Another source of confusion is the different usage of the word "college". (See a full international discussion of the various meanings at college.) In the US this refers to a post-high school institution that grants either associate's or bachelor's degrees, while in the UK it refers to any post-secondary institution that is not a university (including "sixth form college" after the name in secondary education for years 12 and 13, the "sixth form") where intermediary courses such as A levels or NVQs can be taken and GCSE courses can be retaken. College may sometimes be used in the UK or in Commonwealth countries as part of the name of a secondary or high school (for example, Dubai College). In the case of Oxford, Cambridge, Aberdeen, London, Lancaster, Durham, Kent and York universities, all members are also members of a college which is part of the university, for example, one is a member of King's College, Cambridge and hence the university.
In both the US and UK "college" can refer to some division within a university that comprises related academic departments such as the "college of business and economics" though in the UK "faculty" is more often used. Institutions in the US that offer two to four years of post-high school education often have the word "college" as part of their name, while those offering more advanced degrees are called a "university". (There are exceptions: Boston College, Dartmouth College and the College of William & Mary are examples of colleges that offer advanced degrees, while Vincennes University is an unusual example of a "university" that offers only associate degrees in the vast majority of its academic programs.) American students who pursue a "bachelor's degree" (four years of higher education) or an "associate degree" (two years of higher education) are "college students" regardless of whether they attend a college or a university and refer to their educational institutions informally as "colleges." A student who pursues a master's degree or a doctorate degree in the arts and sciences is in AmE a "graduate student"; in BrE a "postgraduate student" although "graduate student" is also sometimes used. Students of advanced professional programs are known by their field ("business student", "law student", "medical student"). Some universities also have a residential college system, the details of which may vary but generally involve common living and dining spaces as well as college-organized activities. Nonetheless, when it comes to the level of education, AmE generally uses the word "college" (e.g., going to college) whereas BrE generally uses the word "university" (e.g., going to university) regardless of the institution's official designation/status in both countries.
In the context of higher education, the word "school" is used slightly differently in BrE and AmE. In BrE, except for the University of London, the word school is used to refer to an academic department in a university. In AmE, the word school is used to refer to a collection of related academic departments and is headed by a dean. When referring to a division of a university, school is practically synonymous to a college.
"Professor" has different meanings in BrE and AmE. In BrE it is the highest academic rank, followed by reader, senior lecturer and lecturer. In AmE "professor" refers to academic staff of all ranks, with (full) professor (largely equivalent to the UK meaning) followed by associate professor and assistant professor.
"Tuition" has traditionally had separate meaning in each variation. In BrE it is the educational content transferred from teacher to student at a university. In AmE it is the money (the fees) paid to receive that education (BrE: tuition fees).
General terms.
In both the US and the UK, a student "takes" an exam, but in BrE a student can also be said to "sit" an exam. When preparing for an exam students "revise" (BrE)/"review" (AmE) what they have studied; the BrE idiom "to revise for" has the equivalent "to review for" in AmE.
Examinations are supervised by "invigilators" in the UK and "proctors" (or "(exam) supervisors") in the US (a "proctor" in the UK is an official responsible for student discipline at the University of Oxford or Cambridge). In the UK a teacher "sets" an exam, while in the US, a teacher "writes" (prepares) and then "gives" (administers) an exam.
BrE:
AmE:
In BrE, students are awarded "marks" as credit for requirements (e.g., tests, projects) while in AmE, students are awarded "points" or "grades" for the same. Similarly, in BrE, a candidate's work is being "marked", while in AmE it is said to be "graded" to determine what mark or grade is given.
There is additionally a difference between American and British usage in the word "school". In British usage "school" by itself refers only to primary (elementary) and secondary (high) schools and to "sixth forms" attached to secondary schools—if one "goes to school", this type of institution is implied. By contrast an American student at a university may talk of "going to school" or "being in school". US and British law students and medical students commonly speak in terms of going to "law school" and "medca school", respectively. However, the word is used in BrE in the context of higher education to describe a division grouping together several related subjects within a university, for example a "School of European Languages" containing "departments" for each language and also in the term "art school". It is also the name of some of the constituent colleges of the University of London, for example, School of Oriental and African Studies, London School of Economics.
Among high-school and college students in the United States, the words "freshman" (or the gender-neutral terms "frosh" or "first year", sometimes "freshie"), "sophomore", "junior" and "senior" refer to the first, second, third, and fourth years respectively. For first-year students, "frosh" and "freshie" are another gender-neutral terms that can be used as a qualifier, for example "Frosh class elections". It is important that the context of either high school or college first be established or else it must be stated directly (that is, "She is a high school freshman". "He is a college junior."). Many institutes in both countries also use the term "first-year" as a gender-neutral replacement for "freshman", although in the US this is recent usage, formerly referring only to those in the first year as a graduate student. One exception is the University of Virginia; since its founding in 1819 the terms "first-year", "second-year", "third-year", and "fourth-year" have been used to describe undergraduate university students. At the United States service academies, at least those operated by the federal government directly, a different terminology is used, namely "fourth class", "third class", "second class" and "first class" (the order of numbering being the reverse of the number of years in attendance). In the UK first-year university students are sometimes called "freshers" early in the academic year; however, there are no specific names for those in other years nor for school pupils. Graduate and professional students in the United States are known by their year of study, such as a "second-year medical student" or a "fifth-year doctoral candidate." Law students are often referred to as "1L", "2L", or "3L" rather than "nth-year law students"; similarly, medical students are frequently referred to as "M1", "M2", "M3", or "M4".
While anyone in the US who finishes studying at any educational institution by passing relevant examinations is said to "graduate" and to be a "graduate", in the UK only degree and above level students can "graduate". "Student" itself has a wider meaning in AmE, meaning any person of any age studying at any educational institution or level, whereas in BrE it tends to be used for people studying at a post-secondary educational institution and the term "pupil" is more widely used for a young person at primary or secondary school, though the use of "student" for secondary school pupils in the UK is increasingly used, particularly for "sixth form" (years 12 and 13).
The names of individual institutions can be confusing. There are several "University High Schools" in the United States that are not affiliated with any post-secondary institutions and cannot grant degrees, and there is one public high school, Central High School of Philadelphia, which does grant bachelor's degrees to the top ten per cent of graduating seniors. British secondary schools occasionally have the word "college" in their names.
When it comes to the admissions process, applicants are usually asked to solicit "letters of reference" or reference forms from referees in BrE. In AmE, these are called "letters of recommendation" or recommendation forms. Consequently, the writers of these letters are known as "referees" and "recommenders", respectively.
In the context of education, for AmE, the word "staff" mainly refers to school personnel who are neither administrators nor have teaching loads or academic responsibilities; personnel who have academic responsibilities are referred to as members of their institution's "faculty." In BrE, the word "staff" refers to both academic and non-academic school personnel. As mentioned previously, the term "faculty" in BrE refers more to a collection of related academic departments.
Government and Politics.
In Britain, political candidates "stand for election", while in the US, they "run for office". There is virtually no crossover between BrE and AmE in the use of these terms. Also, the document which contain's a party's positions/principles is referred to as a "party platform" in AmE, whereas it is in BrE commonly known as a "party manifesto". The term "general election" is used slightly differently in British and American English. In BrE, it refers exclusively to a nationwide parliamentary election, whereas in AmE, it refers to any election in the US which involves a race between at least two parties (e.g., Democrat vs Republican candidate).
In AmE, the term "swing state", "swing county", "swing district" is used to denote a jurisdiction/constituency where results are expected to be close but crucial to the overall outcome of the general election. In BrE, the term "marginal constituency" is more often used for the same and "swing" is more commonly used to refer to how much one party has gained (or lost) an advantage over another.
In Britain, the term "government" only refers to what is commonly known in America as the executive branch of government.
Business/Finance.
In financial statements, what is referred to in AmE as "revenue" or "sales" is known in BrE as "turnover."
A bankrupt firm "goes into administration" in BrE; in AmE it "goes bankrupt", or "files for Chapter 7" (liquidation) or "Chapter 11" (reorganization). An insolvent individual or partnership "goes bankrupt" in both jurisdictions.
If a finance company takes possession of a mortgaged property from a debtor, it is called "foreclosure" in AmE and "repossession" in BrE. In some, limited scenarios, "repossession" may be used in AmE, but it is much less common compared to "foreclosure".
Employment/Recruitment.
In BrE, the term "curriculum vitae" (commonly abbreviated to "CV") is used to describe the document prepared by applicants containing their credentials required for a job. In AmE, the term "résumé" is more commonly used, with "CV" primarily used in academic or research contexts, and is usually more comprehensive than the "résumé".
Transport/Transportation.
Americans refer to "transportation" and British people to "transport". ("Transportation" in Britain has traditionally meant the punishment of criminals by deporting them to an overseas penal colony.) In AmE, the word "transport" is mainly used only as a verb, seldom as a noun or adjective except in reference to certain specialized objects, such as a "tape transport" or a "military transport" (e.g., a troop transport, a kind of vehicle, not an act of transporting).
Road transport.
Differences in terminology are especially obvious in the context of roads. The British term "dual carriageway", in American parlance, would be "divided highway". The "central reservation" on a "motorway" or "dual carriageway" in the UK would be the "median" or "center divide" on a "freeway", "expressway", "highway" or "parkway" in the US. The one-way lanes that make it possible to enter and leave such roads at an intermediate point without disrupting the flow of traffic are known as "slip roads" in the UK but US civil engineers call them "ramps" and both further distinguish between "on-ramps" or "entry-slips" (for entering) and "off-ramps" or "exit-slips" (for leaving). When American engineers speak of "slip roads", they are referring to a street that runs alongside the main road (separated by a berm) to allow off-the-highway access to the premises that are there, sometimes also known as a frontage road—in both the US and UK this is also known as a "service road".
In the UK, the term "outside lane" refers to the higher-speed "overtaking lane" ("passing lane" in the US) closest to the centre of the road, while "inside lane" refers to the lane closer to the edge of the road. In the US "outside lane" is used only in the context of a turn, in which case it depends in which direction the road is turning (i.e., if the road bends right the left lane is the "outside lane" but if the road bends left it is the right lane). Both also refer to "slow" and "fast" lanes (even though all actual traffic speeds may be at or around the legal speed limit).
In the UK "drink driving" is against the law, while in the US, where the action is also outlawed, the term is "drunk driving". The legal term in the US is "driving while intoxicated" (DWI) or "driving under the influence (of alcohol)" (DUI). The equivalent legal phrase in the UK is "drunk in charge of a motor vehicle" (DIC) or more commonly "driving with excess alcohol".
Specific auto parts and transport terms have different names in the two dialects, for example:<br>
Rail transport.
There are also differences in terminology in the context of rail transport. The best known is "railway" in Britain and "railroad" in America, but there are several others. A "railway station" in the UK is a "railroad station" or "train station" in the US; trains have "drivers" (often called "engine drivers") in Britain, while in America trains are driven by "engineers"; trains have "guards" in the UK and "conductors" in the US; a place where two tracks meet is called a set of "points" in the UK and a "switch" in the US; and a place where a road crosses a railway line at ground level is called a "level crossing" in Britain and a "grade crossing" in America. In Britain, the term "sleeper" is used for the devices that bear the weight of the rails and are known as "ties" or "crossties" in the United States. The British term "platform" in the sense "The train is at Platform 1" would be known in the US by the term "track", and used in the phrase "The train is on Track 1". Also, the British term "Brake Van" or "Guard's Van", is a "Caboose" in the US. Finally the American English phrase "All aboard!" when getting on a train is rarely used in Britain; the nearest British equivalent is "Take your seats!", and when the train reaches its final stop, in Britain the phrase used by announcers is "All change!" while in America it is "All out!"
Television.
Traditionally, a "show" on British television would have referred to a light-entertainment program (BrE "programme") with one or more performers and a participative audience, whereas in American television, the term is used for any type of program. British English traditionally referred to other types of program by their type, such as drama, serial etc., but the term "show" has now taken on the American meaning. In American television the episodes of a program first broadcast in a particular year constitute a "season", while the entire run of the program—which may span several seasons—is called a "series". In British television, on the other hand, the word "series" may apply to the episodes of a program in one particular year, for example, "The 1998 series of "Grange Hill"", as well as to the entire run. However, the entire run may occasionally be referred to as a "show".
The term "telecast", meaning television broadcast, is not used in British English. A television program would be "broadcast", "aired" or "shown" in Britain.
Telecommunications.
A long-distance call is a "trunk call" in British English, but is a "toll call" in American English. The distinction is a result of historical differences in the way local service was billed; the Bell System traditionally flat-rated local calls in all but a few markets, subsidising local service by charging higher rates, or tolls, for intercity calls, allowing local calls to appear to be free. British Telecom (and the British Post Office before it) charged for all calls, local and long distance, so labelling one class of call as "toll" would have been meaningless.
Similarly, a toll-free number in America is a freephone number in Great Britain. The term "freefone" is a BT trademark.
Levels of buildings.
There are also variations in floor numbering between the US and UK. In most countries, including the UK, the "first floor" is one above the entrance level, while the entrance level is the "ground floor". In the US the ground floor is considered the first floor. In a British lift one would press the "G" or "0" button to return to the ground floor, whereas in an American elevator, one would push the "1", "G", or "L" (for Lobby) button to return to the ground floor. The "L" button (or sometimes "-1") in a British lift would take you to the lower ground floor, which implies that the building is built on a slope and thus there are two ground floors - there would similarly be a "U" button (or "0") for upper ground floor.
American (AmE) "apartment buildings" / (BrE) "blocks of flats" are frequently exceptions to this rule. The ground floor often contains the lobby and parking area for the tenants, while the numbered floors begin one level above and contain only the flats (AmE "apartments") themselves.
Units and measurement.
Numbers.
When saying or writing out numbers, the British insert an "and" before the tens and units, as in "one hundred and sixty-two" or "two thousand and three". In the United States it is considered correct to drop the "and", as in "one hundred sixty-two" or "two thousand three". However, the "and" is also retained even in AmE speech, for emphasis or as another acceptable variant.
Some American and Canadian schools teach students to pronounce decimally written fractions (for example, ".5") as though they were longhand fractions ("five tenths"), such as "thirteen and seven tenths" for 13.7. This formality is often dropped in common speech and is steadily disappearing in instruction in mathematics and science as well as in international American schools. In the UK, and among most Americans, 13.7 would be read "thirteen point seven".
In counting, it is common in both varieties of English to count in hundreds up to 1,900—so "1,200" may be "twelve hundred". However, Americans use this pattern for much higher numbers than is the norm in British English, referring to twenty-four hundred where British English would most often use two thousand four hundred. Even below 2,000, Americans are more likely than the British are to read numbers like 1,234 as "twelve hundred thirty-four" instead of "one thousand two hundred (and) thirty-four".
In the case of years, however, "twelve thirty-four" would be the norm on both sides of the Atlantic for the year 1234. The years 2000 to 2009 are most often read as "two thousand", "two thousand (and) one" and the like by both British and American speakers. For years after 2009, "twenty eleven", "twenty fourteen", etc. are more common, even in years earlier than 2009 BC/BCE. Likewise, the years after 1009 (until 1099) are also read in the same manner (e.g. 1015 is either "ten fifteen" or, rarely, "one thousand fifteen"). Some Britons read years within the 1000s to 9000s BC/BCE in the American manner, that is, 1234 BC is read as "twelve (hundred and) thirty-four" BC, while 2400 BC can be read as either "two thousand four hundred" or "twenty four hundred" BC.
There is also a historical difference between billions, trillions, and so forth. Americans use "billion" to mean one thousand million (1,000,000,000), whereas in the UK, until the latter part of the 20th century, it was used to mean one million million (1,000,000,000,000). In 1974 the British prime minister, Harold Wilson, told the House of Commons that UK government statistics would now use the short scale, followed by the Chancellor, Denis Healey, in 1975, that the treasury would now adopt the US billion version. One thousand million was sometimes described as a "milliard", the definition adopted by most other European languages. However, the "American" version has since been adopted for all published writing, and the word "milliard" is obsolete in English, as are "billiard" (but not "billiards", the game), "trilliard", and so on. All major British publications and broadcasters, including BBC, which long used "thousand million" to avoid ambiguity, now use "billion" to mean thousand million.
Many people have no direct experience of manipulating numbers this large, and many non-American readers may interpret "billion" as 10 (even if they are young enough to have been taught otherwise at school); moreover, usage of the "long" billion is standard in some non-English speaking countries. For these reasons, defining the word may be advisable when writing for the public. See long and short scales for a more detailed discussion of the evolution of these terms in English and other languages.
When referring to the numeral 0, British people would normally use "nought", "oh", or "zero", although "nil" is common in sports scores. Americans use the term "zero" most frequently; "oh" is also often used (though never when the quantity in question is nothing), and occasionally slang terms such as "zilch" or "zip" are used. Phrases such as "the team won two–zip" or "the team leads the series two–nothing" are heard when reporting sports scores. In the case of association football—known as "football" in Britain and "soccer" in America—Americans will sometimes use "nil" as in Britain, although this usage is mostly confined to soccer journalists and hardcore fans and is not universal among either group. The digit 0, for example, when a phone or account number is being read aloud, is nearly always pronounced "oh" in both language varieties for the sake of convenience. In the internet age the use of the term "oh" can cause certain inconveniences when one is referencing an email address, causing confusion as to whether the character in question is a zero or the letter "O".
When reading numbers in a sequence, such as a telephone or serial number, British people will usually use the terms "double" followed by the repeated number. Hence 007 is "double oh seven". Exceptions are the emergency telephone number 999, which is always "nine nine nine" and the apocalyptic "Number of the Beast", which is always "six six six". In the US, 911 (the US emergency telephone number) is usually read "nine one one", while 9/11 (in reference to the September 11, 2001, attacks) is usually read "nine eleven".
Dates.
Dates are usually written differently in the short (numerical) form. Christmas Day 2000, for example, is 25/12/00 or 25.12.00 in the UK and 12/25/00 in the US, although the formats 25/12/2000, 25.12.2000, and 12/25/2000 now have more currency than they had before the Year 2000 problem. Occasionally other formats are encountered, such as the ISO 8601 2000-12-25, popular among programmers, scientists and others seeking to avoid ambiguity, and to make alphanumerical order coincide with chronological order. The difference in short-form date order can lead to misunderstanding. For example 06/04/05 could mean either June 4, 2005 (if read as US format), 6 April 2005 (if seen as in UK format) or even 5 April 2006 if taken to be an older ISO 8601-style format where 2-digit years were allowed.
When the output of a computer printer has a date in the header or footer this can cause problems since the date style depends on the software, not the country where the printer is located.
When using the name of the month rather than the number to write a date in the UK, the recent standard style is for the day to precede the month, e. g., 21 April. Month preceding date is almost invariably the style in the US, and was common in the UK until the late twentieth century. British usage often changes the day from an integer to an ordinal, i.e., 21st instead of 21. In speech, "of" and "the" are used in the UK, as in "the 21st of April". In written language, the words "the" and "of" may be and are usually dropped, i.e., 21 April. The US would say this as "April 21st", and this form is still common in the UK. One of the few exceptions in American English is saying "the Fourth of July" as a shorthand for the United States Independence Day. In the US military the British forms are used, but the day is read cardinally.
Phrases such as the following are common in Britain but are generally unknown in the US: "A week today", "a week tomorrow", "a week Tuesday" and "Tuesday week"; these all refer to a day more than a week in the future. "A fortnight Friday" and "Friday fortnight" refer to a day two weeks after the coming Friday). "A week on Tuesday" and "a fortnight on Friday" could refer either to a day in the past ("it's a week on Tuesday, you need to get another one") or in the future ("see you a week on Tuesday"), depending on context. In the US the standard construction is "a week from today", "a week from tomorrow", etc. BrE speakers may also say "Thursday last" or "Thursday gone" where AmE would prefer "last Thursday". "I'll see you (on) Thursday coming" or "let's meet this coming Thursday" in BrE refer to a meeting later this week, while "not until Thursday next" would refer to next week.
Time.
The 24-hour clock ("18:00", "18.00" or "1800") is considered normal in the UK and Europe in many applications including air, rail and bus timetables; it is largely unused in the US outside of military, police, aviation and medical applications. British English tends to use the full stop or period (.) when telling time, compared to American English which uses Colons (:) (i.e., 11:15 PM or 23:15 for AmE and 11.15 pm or 23.15 for BrE). Usually in the military (and sometimes in the police, aviation and medical) applications on both sides of the Atlantic "0800" and "1800" are read as "(oh/zero) eight hundred" and "eighteen hundred" hours respectively.
Fifteen minutes after the hour is called "quarter past" in British usage and "a quarter after" or, less commonly, "a quarter past" in American usage. Fifteen minutes before the hour is usually called "quarter to" in British usage and "a quarter of", "a quarter to" or "a quarter 'til" in American usage; the form "a quarter to" is associated with parts of the Northern United States, while "a quarter 'til" is found chiefly in the Appalachian region. Thirty minutes after the hour is commonly called "half past" in both BrE and AmE; "half after" used to be more common in the US. In informal British speech, the preposition is sometimes omitted, so that 5:30 may be referred to as "half five". The AmE formations "top of the hour" and "bottom of the hour" are not used in BrE. Forms such as "eleven forty" are common in both dialects. To be simple and direct in telling time, no terms relating to fifteen or thirty minutes before/after the hour are used; rather the time is told exactly as for example "nine fifteen", "ten forty-five".
Mass.
In British usage, human body mass is colloquially expressed in stones (equal to 14 pounds). People normally describe themselves as weighing, for example, "11 stone 4" (11 stones and 4 pounds) and not "158 pounds" (the conventional way of expressing the same weight in the United States). Stones are never used in the United States, and most Americans are unfamiliar with the term. Kilogrammes (note the difference from the U.S. spelling, "kilograms") are the official measurement in the United Kingdom, although very few people know their weight in kilogrammes. This is rarely noticed by the British (one such occasion might be a weight measurement at a hospital).
When used as the unit of measurement the plural form of "stone" is correctly "stone" (as in "11 stone"). When describing the units, the correct plural is "stones" (as in "Please enter your weight in stones and pounds").
Mathematics.
Besides the differences between the shorthand word for the subject itself (i.e., "Maths" for BrE and "Math" for AmE), there are also differences in terms within the subject.
In geometry, what is referred to as a "trapezoid" (a quadrilateral with exactly 1 pair of parallel sides) in US textbooks is a "trapezium" in its UK counterparts. The "slope" of the line in AmE is said to be the "gradient" of a line in BrE. The skill of "factoring" polynomials in AmE is called "factorisation" in BrE; likewise, the words "factor" and "factorise," respectively refer to their present tense forms.
In BrE the term mathematics is not commonly used for simple arithmetic. "2 + 2 = 4" is referred as arithmetic, not mathematics.
Holiday greetings.
When people greet one another at Christmas in North America they say “Merry Christmas!”, whereas, in the UK, “Happy Christmas!” is an exceptionally common greeting. However, there are still a wide number of Britons who alternatively say "Merry Christmas!" to greet each other. It is increasingly common for Americans to say "Happy holidays", referring to all, or at least multiple, winter holidays (Christmas, Hanukkah, Winter solstice, Kwanzaa, etc.) especially when the subject's religious observances are not known; the phrase is rarely heard in the U.K. In Britain, the phrases "holiday season" and "holiday period" refer to the period in the summer when most people take time off from work, and travel; AmE does not use "holiday" in this sense, instead using "vacation" for recreational excursions.
Idiosyncratic differences.
Figures of speech.
Both BrE and AmE use the expression "I couldn't care less" to mean the speaker does not care at all. Many Americans use "I could care less" to mean the same thing. This variant is frequently derided as sloppy, as the literal meaning of the words is that the speaker "does" care to some extent.
In both areas, saying, "I don't mind" often means, "I'm not annoyed" (for example, by someone's smoking), while "I don't care" often means, "The matter is trivial or boring". However, in answering a question such as "Tea or coffee?", if either alternative is equally acceptable an American may answer, "I don't care", while a British person may answer, "I don't mind". Either sounds odd to the other.
Equivalent idioms.
A number of English idioms that have essentially the same meaning show lexical differences between the British and the American version; for instance:
Writing.
Spelling.
Before the early 18th century English spelling was not standardized. Different standards became noticeable after the publishing of influential dictionaries. For the most part current BrE spellings follow those of Samuel Johnson's "Dictionary of the English Language" (1755), while AmE spellings follow those of Noah Webster's "An American Dictionary of the English Language" (1828). In Britain, the influences of those who preferred the French spellings of certain words proved decisive. In many cases AmE spelling deviated from mainstream British spelling; on the other hand it has also often retained older forms. Many of the now characteristic AmE spellings were popularized, although often not created, by Noah Webster. Webster chose already-existing alternative spellings "on such grounds as simplicity, analogy or etymology". Webster did attempt to introduce some reformed spellings, as did the Simplified Spelling Board in the early 20th century, but most were not adopted. Later spelling changes in the UK had little effect on present-day US spelling, and vice versa.
Punctuation.
Full stops and periods in abbreviations.
There have been some trends of transatlantic difference in use of periods in some abbreviations. These are discussed at "Abbreviation § Periods (full stops) and spaces". Unit symbols such as "kg" and "Hz" are never punctuated.
Restrictive and non-restrictive modifiers.
In American English, restrictive and non-restrictive modifying phrases require different words and sentence structures. In particular, a non-restrictive modifying phrase must be set off by commas, and it generally uses "which" as its pronoun. A restrictive modifying phrase, by contrast, is not set off by commas, and uses the pronoun "that." An example of the first in American English is: "The dog, which bit the man, was brown." In that sentence the phrase "which bit the man" is non-restrictive: it is merely providing background information about a dog whose identity is otherwise not in question. The contrasting sentence in American English would be: "The dog that bit the man was brown." In this sentence, the phrase "that bit the man" is restrictive: it tells the reader that, of several dogs that might have bitten the man, the actual biter was brown. Interchanging the two structures is grammatically incorrect in American English because they have different meanings.
British English, by contrast, generally does not require its writers to construct sentences in a manner that distinguishes between the restrictive and non-restrictive forms of modifiers. Thus, a writer of British English might write: "The dog which bit the man was brown." In this sentence, it is ambiguous whether the phrase "which bit the man" is serving to identify a particular dog among several candidates or just to provide background information about a dog whose identity is otherwise not in doubt. The reader must try to infer the distinction from context or from his own knowledge.
H. W. Fowler, in "A Dictionary of Modern English Usage" of 1926, recommends “that”, without a preceding comma, for restrictive (“defining”) use and “which”, with a comma, for descriptive (“non-defining”) use. However, he notes that it was not (then) commonly British English usage, and that British and American usages differed, without explicitly identifying this usage as American. He also notes several problems with this usage “The most important of these is its 'that"' insistence on being the first word of its clause ; it cannot, like "whom" & "which", endure that a preposition governing it should, by coming before it, part it from the antecedent or the main sentence ; such a preposition has to go, instead, at the end of the clause ; that is quite in harmony with the closer connexion between a defining, (or "that"-) clause & the antecedent than between a non-defining (or "which"-) clause & the antecedent ; but it forces the writer to choose between ending his sentence or clause with a preposition & and giving up "that" for "which".” However, Fowler also goes on to reprise his assertion that prepositional endings are acceptable: “to shrink with horror from ending with a preposition is no more than a foolish superstition”.
Quoting.
Americans begin their quotations with double quotation marks (") and use single quotation marks (') for quotations within quotations (nested quotations). BrE usage varies, with some authoritative sources such as "The Economist" and "The Times" recommending the same usage as in the US, whereas other authoritative sources, such as "The King's English", recommend single quotation marks. In journals and newspapers, quotation mark double/single use depends on the individual publication's house style.
American guides almost always recommend placing commas and periods inside adjacent quotation marks. Specific exceptions are made for cases in which the addition of a period or comma could create confusion, such as the quotation of web addresses or certain types of data strings. In both styles, question marks and exclamation marks are placed inside the quotation marks if they belong to the quotation and outside otherwise. With narration of direct speech, both styles retain punctuation inside the quotation marks, with a full stop changing into a comma if followed by explanatory text, also known as a dialogue tag. Americans tend to apply quotations when signifying doubt of veracity (sarcastically or seriously), to imply another meaning to a word or to imply a cynical take on a paraphrased quotation, without punctuation at all.
The American style is used by most American newspapers, publishing houses and style guides in the United States and Canada (including the Modern Language Association's "MLA Style Manual", the American Psychological Association's "APA Publication Manual", the University of Chicago's "The Chicago Manual of Style", the American Institute of Physics's "AIP Style Manual", the American Medical Association's "AMA Manual of Style", the American Political Science Association's "APSA Style Manual", the Associated Press' "The AP Guide to Punctuation" and the Canadian Public Works' "The Canadian Style").
"Hart's Rules" and the "Oxford Dictionary for Writers and Editors" call the British style "new" quoting. It is also similar to the use of quotation marks in many other languages (including Portuguese, Spanish, French, Italian, Catalan, Dutch and German). A few US professional societies whose professions frequently employ various non-word characters, such as chemistry and computer programming, use the British form in their style guides (see "ACS Style Guide"). According to the "Jargon File", American hackers switched to what they later discovered to be the British quotation system because placing a period inside a quotation mark can change the meaning of data strings that are meant to be typed character-for-character. (It may be noted that the current American system places periods and commas outside the quotation marks in these cases anyway.)
Parentheses/brackets.
In British English, "( )" marks are often referred to as brackets, whereas "" are called square brackets and "{ }" are called curly brackets. In formal British English and in American English "( )" marks are parentheses (singular: parenthesis), "" are called brackets, and "{ }" can be called either curly brackets or curly braces. In both countries, standard usage is to place punctuation outside the parenthesis, unless the entire sentence is contained within them:
In the case of a parenthetical expression which is itself a complete sentence, the final punctuation may be placed inside the parenthesis, particularly if not a period:
Titles and headlines.
Use of capitalization varies.
Sometimes the words in titles of publications and newspaper headlines as well as chapter and section headings are capitalized in the same manner as in normal sentences (sentence case). That is, only the first letter of the first word is capitalised, along with proper nouns, etc.
However, publishers sometimes require additional words in titles and headlines to have the initial capital, for added emphasis, as it is often perceived as appearing more professional. In AmE this is common in titles but less so in newspaper headlines. The exact rules differ between publishers and are often ambiguous; a typical approach is to capitalise all words other than short articles, prepositions, and conjunctions. This should probably be regarded as a common stylistic difference rather than a linguistic difference, as neither form would be considered incorrect or unusual in either the UK or the US. Many British tabloid newspapers (such as "The Sun", "The Daily Sport") use fully capitalised headlines for impact as opposed to readability (for example, BERLIN WALL FALLS or BIRD FLU PANIC). On the other hand the broadsheets (such as "The Guardian", "The Times", and "The Independent") usually follow the sentence style of having only the first letter of the first word capitalised.
American newspapers commonly use a comma as a shorthand for "and" in headlines. For example, "The Washington Post" had the headline "A TRUE CONSERVATIVE: For McCain, Bush Has Both Praise, Advice."

</doc>
<doc id="2014" url="https://en.wikipedia.org/wiki?curid=2014" title="Atomic semantics">
Atomic semantics

Atomic semantics is a term which describes a type of guarantee provided by a data register shared by several processors in a parallel machine or in a network of computers working together.
Atomic semantics are very strong. An atomic register provides strong guarantees even when there is concurrency and failures.
A read/write register R stores a value and is accessed by two basic operations: read and write(v). A read returns the value stored in R and write(v) changes the value stored in R to v.
A register is called atomic if it satisfies the two following properties:
1) Each invocation op of a read or write operation:
•Must appear as if it were executed at a single point τ(op) in time.
•τ (op) works as follow:
τb(op) ≤ τ (op) ≤ τe(op): where τb(op) and τe(op) indicate the time when the operation op begins and ends.
•If op1 ≠ op2, then τ (op1)≠τ (op2)
2) Each read operation returns the value written by the last write operation before the read, in the sequence where all operations are ordered by their τ values.
Atomic/Linearizable register:
Termination: when a node is correct,sooner or later each read and write operation will complete.
Safety Property (Linearization points for read and write and failed operations):
Read operation:It appears as if happened at all nodes at some times between the invocation and response time.
Write operation: Similar to read operation,it appears as if happened at all nodes at some times between the invocation and response time.
Failed operation(The atomic term comes from this notion):It appears as if it is completed at every single node or it never happened at any node.
Example : We know that an atomic register is one that is linearizable to a sequential safe register.
The following picture shows where we should put the linearization point for each operation:
An atomic register could be defined for a variable with a single writer but multi- readers(SWMR),single-writer/single-reader (SWSR),or multi-writer/multi-reader(MWMR). Here is an example of a multi-reader multi-writer atomic register which is accessed by three processes (P1,P2,P3).Note that R.read() → v means that the corresponding read operation returns v, which is the value of the register. Therefore, the following execution of the register R could satisfies the definition of the atomic registers:
R.write(1), R.read()→1, R.write(3), R.write(2), R.read()→2, R.read()→2.

</doc>
<doc id="2015" url="https://en.wikipedia.org/wiki?curid=2015" title="Antarctic Circumpolar Current">
Antarctic Circumpolar Current

The Antarctic Circumpolar Current (ACC) is an ocean current that flows clockwise from west to east around Antarctica. An alternative name for the ACC is the West Wind Drift. The ACC is the dominant circulation feature of the Southern Ocean and has a mean transport of 100-150 Sverdrups (Sv, million m³/s), making it the largest ocean current. The current is circumpolar due to the lack of any landmass connecting with Antarctica and this keeps warm ocean waters away from Antarctica, enabling that continent to maintain its huge ice sheet.
Associated with the Circumpolar Current is the Antarctic Convergence, where the cold Antarctic waters meet the warmer waters of the subantarctic, creating a zone of upwelling nutrients. These nurture high levels of phytoplankton with associated copepods and krill, and resultant foodchains supporting fish, whales, seals, penguins, albatrosses and a wealth of other species.
The ACC has been known to sailors for centuries; it greatly speeds up any travel from west to east, but makes sailing extremely difficult from east to west; though this is mostly due to the prevailing westerly winds. The circumstances preceding the mutiny on the "Bounty" and Jack London's story "Make Westing" poignantly illustrated the difficulty it caused for mariners seeking to round Cape Horn on the clipper ship route between New York and California. The clipper route, which is the fastest sailing route around the world, follows the ACC around three continental capes – Cape Agulhas (Africa), South East Cape (Australia) and Cape Horn (South America).
The current creates the Ross and Weddell gyres.
Structure.
The ACC connects the Atlantic, Pacific and Indian Oceans, and serves as a principal pathway of exchange between them. The current is strongly constrained by landform and bathymetric features. To trace it starting arbitrarily at South America, it flows through the Drake Passage between South America and the Antarctic Peninsula and then is split by the Scotia Arc to the east, with a shallow warm branch flowing to the north in the Falkland Current and a deeper branch passing through the Arc more to the east before also turning to the north. Passing through the Indian Ocean, the current first retroflects the Agulhas Current to form the Agulhas Return Current before it is split by the Kerguelen Plateau, and then moving northward again. Deflection is also seen as it passes over the mid-ocean ridge in the Southeast Pacific.
Fronts.
The current is accompanied by three fronts: the Subantarctic front (SAF), the Polar front (PF), and the Southern ACC front (SACC). Furthermore, the waters of the Southern Ocean are separated from the warmer and saltier subtropical waters by the subtropical front (STF).
The northern boundary of the ACC is defined by the northern edge of the SAF, this being the most northerly water to pass through Drake Passage and therefore be circumpolar. Much of the ACC transport is carried in this front, which is defined as the latitude at which a subsurface salinity minimum or a thick layer of unstratified Subantarctic mode water first appears, allowed by temperature dominating density stratification. Still further south lies the PF, which is marked by a transition to very cold, relatively fresh, Antarctic Surface Water at the surface. Here a temperature minimum is allowed by salinity dominating density stratification, due to the lower temperatures. Further south still is the SACC, which is determined as the southernmost extent of Circumpolar Deep Water (temperature of about 2 °C at 400 m). This water mass flows along the shelfbreak of the western Antarctic Peninsula and thus marks the most southerly water flowing through Drake Passage and therefore circumpolar. The bulk of the transport is carried in the middle two fronts.
The total transport of the ACC at Drake Passage is estimated to be around 135 Sv, or about 135 times the transport of all the world's rivers combined. There is a relatively small addition of flow in the Indian Ocean, with the transport south of Tasmania reaching around 147 Sv, at which point the current is probably the largest on the planet.
Dynamics.
The circumpolar current is driven by the strong westerly winds in the latitudes of the Southern Ocean.
In latitudes where there are continents, winds blowing on light surface water can simply pile up light water against these continents. But in the Southern Ocean, the momentum imparted to the surface waters cannot be offset in this way. There are different theories on how the Circumpolar Current balance the momentum imparted by the winds. The increasing eastward momentum imparted by the winds causes water parcels to drift outwards from the axis of the Earth's rotation (in other words, northward) as a result of the Coriolis force. This northward Ekman transport is balanced by a southward, pressure-driven flow below the depths of the major ridge systems. Some theories connect these flows directly, implying that there is significant upwelling of dense deep waters within the Southern Ocean, transformation of these waters into light surface waters, and a transformation of waters in the opposite direction to the north. Such theories link the magnitude of the Circumpolar Current with the global thermohaline circulation, particularly the properties of the North Atlantic.
Alternatively, ocean eddies, the oceanic equivalent of atmospheric storms, or the large-scale meanders of the Circumpolar Current may directly transport momentum downwards in the water column. This is because such flows can produce a net southward flow in the troughs and a net northward flow over the ridges without requiring any transformation of density. In practice both the thermohaline and the eddy/meander mechanisms are likely to be important.
The current flows at a rate of about over the Macquarie Ridge south of New Zealand. The ACC varies with time. Evidence of this is the Antarctic Circumpolar Wave, a periodic oscillation that affects the climate of much of the southern hemisphere. There is also the Antarctic oscillation, which involves changes in the location and strength of Antarctic winds. Trends in the Antarctic Oscillation have been hypothesized to account for an increase in the transport of the Circumpolar Current over the past two decades.
Formation.
Published estimates of the onset of the Antarctic Circumpolar Current vary, but it is commonly considered to have started at the Eocene/Oligocene boundary. The isolation of Antarctica and formation of the ACC occurred with the openings of the Tasmanian Seaway and the Drake Passage. The Tasmanian Seaway separates East Antarctica and Australia, and is reported to have opened to water circulation 33.5 Ma. The timing of the opening of the Drake Passage, between South America and the Antarctic Peninsula, is more disputed; tectonic and sediment evidence show that it could have been open as early as pre 34 Ma, estimates of the opening of the Drake passage are between 20 and 40 Ma. The isolation of Antarctica by the current is credited by many researchers with causing the glaciation of Antarctica and global cooling in the Eocene epoch. Oceanic models have shown that the opening of these two passages limited polar heat convergence and caused a cooling of sea surface temperatures by several degrees; other models have shown that CO levels also played a significant role in the glaciation of Antarctica.
Phytoplankton.
Antarctic sea ice cycles seasonally, in February–March the amount of sea ice is lowest, and in August–September the sea ice is at its greatest extent. Ice levels have been monitored by satellite since 1973. Upwelling of deep water under the sea ice brings substantial amounts of nutrients. As the ice melts, the melt water provides stability and the critical depth is well below the mixing depth, which allows for a positive net primary production. As the sea ice recedes epontic algae dominate the first phase of the bloom, and a strong bloom dominate by diatoms follows the ice melt south.
Another phytoplankton bloom occurs more to the north near the antarctic convergence, here nutrients are present from thermohaline circulation. Phytoplankton blooms are dominated by diatoms and grazed by copepods in the open ocean, and by krill closer to the continent. Diatom production continues through the summer, and populations of krill are sustained, bringing large numbers of cetaceans, cephalopods, seals, birds and fish to the area.
Phytoplankton blooms are believed to be limited by irradiance in the austral (southern hemisphere) spring, and by biologically available iron in the summer. Much of the biology in the area occurs along the major fronts of the current, the Subtropical, Subantarctic, and the Antarctic Polar fronts, these are areas associated with well defined temperature changes. Size and distribution of phytoplankton are also related to fronts. Microphytoplankton (>20μm) are found at fronts and at sea ice boundaries, while nanophytoplankton (<20μm) are found between fronts.
Studies of phytoplankton stocks in the southern sea have shown that the Antarctic Circumpolar Current is dominated by diatoms, while the Weddell Sea has abundant coccolithophorids and silicoflagellates. Surveys of the SW Indian Ocean have shown phytoplankton group variation based on their location relative to the Polar Front, with diatoms dominating South of the front, and dinoflagellates and flagellates in higher populations North of the front.
Some research has been done on Antarctic phytoplankton as a carbon sink. Areas of open water left from ice melt are good areas for phytoplankton blooms. The phytoplankton takes carbon from the atmosphere during photosynthesis. As the blooms die and sink, the carbon can be stored in sediments for thousands of years. This natural carbon sink is estimated to remove 3.5 million tonnes from the ocean each year. 3.5 million tonnes of carbon taken from the ocean and atmosphere is equivalent to 12.8 million tonnes of carbon dioxide.
Studies.
An expedition in May 2008 by 19 scientists studied the geology and biology of eight Macquarie Ridge sea mounts, as well as the Antarctic Circumpolar Current to investigate the effects of climate change of the southern Ocean. The circumpolar current merges the waters of the Atlantic, Indian, and Pacific Oceans and carries up to 150 times the volume of water flowing in all of the world's rivers. The study found that any damage on the cold-water corals nourished by the current will have a long-lasting impact. After studying the circumpolar current it is clear that it strongly influences regional and global climate as well as underwater biodiversity.
Dr Adrian Glover of the Natural History Museum, London says that the current helps preserve wooden shipwrecks by preventing wood-boring "ship worms" from reaching targets such as Ernest Shackleton's ship the "Endurance".

</doc>
<doc id="2017" url="https://en.wikipedia.org/wiki?curid=2017" title="Arbor Day">
Arbor Day

Arbor Day (or Arbour; from the Latin "arbor", meaning tree) is a holiday in which individuals and groups are encouraged to plant and care for trees. Today, many countries observe such a holiday. Though usually observed in the spring, the date varies, depending on climate and suitable planting season.
Origins.
First Arbor Day in the world.
The Spanish village of Mondoñedo held the first documented arbor plantation festival in the world organized by its mayor in 1594. The place remains as Alameda de los Remedios and it is still planted with lime and horsenut trees. A humble granite and a bronze plate recalls the event. Additionally, the small Spanish village of Villanueva de la Sierra held the first modern Arbor Day, an initiative launched in 1805 by the local priest with the enthusiastic support of the entire population.
First American Arbor Day.
The first American Arbor Day was originated in Nebraska City, Nebraska, United States by J. Sterling Morton. On April 10, 1872, an estimated one million trees were planted in Nebraska.
Birdsey Northrop of Connecticut was responsible for globalizing it when he visited Japan in 1883 and delivered his Arbor Day and Village Improvement message. In that same year, the American Forestry Association made Northrop the Chairman of the committee to campaign for Arbor Day nationwide. He also brought his enthusiasm for Arbor Day to Australia, Canada, and Europe.
McCreight and Roosevelt.
Beginning in 1906, Pennsylvania conservationist Major Israel McCreight of DuBois, Pennsylvania, argued that President Theodore Roosevelt’s conservation speeches were limited to businessmen in the lumber industry and recommended a campaign of youth education and a national policy on conservation education. McCreight urged President Roosevelt to make a public statement to school children about trees and the destruction of American forests. Conservationist Gifford Pinchot, Chief of the United States Forest Service, embraced McCreight’s recommendations and asked the President to speak to the public school children of the United States about conservation. On April 15, 1907, Roosevelt issued an "Arbor Day Proclamation to the School Children of the United States" about the importance of trees and that forestry deserves to be taught in U.S. schools. Pinchot wrote McCreight, "we shall all be indebted to you for having made the suggestion."
Around the world.
Australia.
National Schools Tree Day is held on the last Friday of July for schools and National Tree Day the last Sunday in July throughout Australia. Many states have Arbor Day although only Victoria has Arbor Week, which was suggested by Premier Dick Hamer in the 1980s. Arbor Day has been observed in Australia since 20 June 1889.
Belgium.
International Day of Treeplanting is celebrated in Flanders on or around 21 March as a theme-day/educational-day/observance, not as public holidays. Tree planting is sometimes combined with awareness campaigns of the fight against cancer: "Kom Op Tegen Kanker".
Brazil.
The Arbor Day (Dia da Árvore) is celebrated on September 21. It's not a national holiday. However, schools nationwide celebrate this day with environment-related activities, namely tree planting.
British Virgin Islands.
Arbour Day is celebrated on November 22. It is sponsored by the National Parks Trust of the Virgin Islands. Activities include an annual national Arbour Day Poetry Competition and tree planting ceremonies throughout the territory.
Cambodia.
Cambodia celebrates arbor day on 9 July with a tree planting ceremony attended by the king.
Canada.
Founded by Don Clark of Schomberg, Ontario for his wife Margret Clark in 1906. In Canada, Maple Leaf Day falls on the last Wednesday in September during National Forest Week. Ontario celebrates Arbor Week from the last Friday in April to the first Sunday in May. Nova Scotia celebrates Arbor Day on the Thursday during National Forest Week, which is the first full week in May. Prince Edward Island celebrates Arbor Day on the 3rd Friday in May during Arbor Week.
Central African Republic.
National Tree Planting Day is on July 20.
China.
In 1981, the fourth session of the Fifth National People's Congress of the People's Republic of China adopted the Resolution on the Unfolding of a Nationwide Voluntary Tree-planting Campaign. This resolution established the Arbor Day () and stipulated that every able-bodied citizen between the ages of 11 and 60 should plant three to five trees per year or do the equivalent amount of work in seedling, cultivation, tree tending or other services. Supporting documentation instructs all units to report population statistics to the local afforestation committees as the basis for workload allocation. Moreover, those failing to do their duty are expected to make up planting requirements, provide funds equivalent to the value of labor required or pay heavy fines. Therefore, the tree-planting campaign is actually compulsory, or at least obligatory (that is, an obligation to the community). The "voluntary" in the title referred to the fact that the tree-planters would "volunteer" their labour. The People's Republic of China celebrates Arbor Day on March 12, a day founded by Lin Daoyang, continue to use following the date of Arbor Day of Republic of China.
Costa Rica.
"Día del Árbol" is on June 15.
Egypt.
Arbor Day is on January 15.
Germany.
Arbor Day ("Tag des Baumes") is on April 25. First celebration was in 1952.
India.
Van Mahotsav is an annual pan-Indian tree planting festival, occupying a week in the month of July. During this event millions of trees are planted. It was initiated in 1950 by K. M. Munshi, the then Union Minister for Agriculture and Food to create an enthusiasm in the mind of the populace for the conservation of forests and planting of trees.
The name Van Mahotsava (the festival of trees) originated in July 1947 after a successful tree-planting drive was undertaken in Delhi, in which national leaders like Jawaharlal Nehru, Dr Rajendra Prasad and Abul Kalam Azad participated. Paryawaran Sachetak Samiti, a leading environmental organization conducts mass events & concrete activities on this special day celebration each year. The week was simultaneously celebrated in a number of states in the country.
Iran.
In Iran it is known as National Tree Planting Day. By Solar Hijri calendar, it is on the 15th day of month Esfand which usually corresponds with March 5.
This day is the first day of the Natural Recyclable Resources week (March 5 to 12).
This is the time in which the saplings of the all kinds in terms of different climates of different parts of Iran would be shared among the people. They also are going to be taught the ways of planting trees.
Israel.
The Jewish holiday Tu Bishvat, the new year for trees, is on the 15th day of the month of Shvat, which usually falls in January or February. Originally based on the date used to calculate the age of fruit trees for tithing as mandated in Leviticus 19:23–25, the holiday now is most often observed by planting trees, or raising money to plant trees. Tu Bishvat is a semi official holiday in Israel, schools are open but Hebrew speaking schools will often go on tree planting excursions.
Japan.
Japan celebrates a similarly themed Greenery Day, held on May 4. Although it has a similar theme to Arbor Day, its roots lie in celebration of the birthday of Emperor Hirohito.
Kenya.
National Tree Planting Day is on April 21. Often people plant palm trees and coconut trees along the Indian Ocean that borders the East coast of Kenya.
Lesotho.
National Tree Planting Day is on March 21.
Luxembourg.
National Tree Planting Day is in November since 1991. It is organized by natur&ëmwelt.
Republic of Congo.
National Tree Planting Day is on November 6.
Republic of Macedonia.
Having in mind the bad condition of the forest fund, and in particular the catastrophic wildfires which occurred in the summer of 2007, a citizen's initiative for afforestation was started in the Republic of Macedonia. The campaign by the name 'Tree Day-Plant Your Future' was first organized on 12 March 2008, when an official non-working day was declared and more than 150,000 Macedonians planted 2 million trees in one day (symbolically, one for each citizen). Six million more were planted in November the same year, and another 12,5 million trees in 2009. This has been established as a tradition and takes place every year.
Malawi.
National Tree Planting Day is on the 2nd Monday of December.
Mexico.
The "Día del Árbol" was established in Mexico in 1959 with President Adolfo López Mateos issuing a decree that it should be observed on the 2nd Thursday of July.
Mongolia.
National Tree Planting Day is on the 2nd Saturday of May and October. It is first National Tree Planting Day was celebrated on 2010-05-08
Namibia.
Its first Arbor Day was celebrated on October 8, 2004.
Netherlands.
Since conference and of the Food and Agriculture Organization's publication "World Festival of Trees", and a resolution of the United Nations in 1954: "The Conference, recognising the need of arousing mass consciousness of the aesthetic, physical and economic value of trees, recommends a World Festival of Trees to be celebrated annually in each member country on a date suited to local conditions"; it has been adopted by the Netherlands. In 1957, the National Committee Day of Planting Trees/Foundation of National Festival of Trees ("Nationale Boomplantdag"/"Nationale Boomfeestdag") was created.
On the third Wednesday in March each year (near the spring equinox), three quarters of Dutch schoolchildren aged 10/11 and Dutch celebrities plant trees. Stichting Nationale Boomfeestdag organizes all the activities in the Netherlands for this day. Some municipalities however plant the trees around 21 September because of the planting season.
In 2007, the 50th anniversary was celebrated with special golden jubilee-activities.
New Zealand.
New Zealand's first Arbor Day planting was in Greytown in the Wairarapa on 3 July 1890. The first official celebration will take place in Wellington in August 2012, with the planting of pohutukawa and Norfolk pines along Thorndon Esplanade.
Born in 1855, Dr Leonard Cockayne (generally recognised as the greatest botanist who has lived, worked, and died in New Zealand) worked extensively on native plants throughout New Zealand and wrote many notable botanical texts. Even as early as the 1920s he held a vision for school students of New Zealand to be involved in planting native trees and plants in their school grounds. This vision bore fruit and schools in New Zealand have long planted native trees on Arbor Day.
Since 1977, New Zealand has celebrated Arbor Day on June 5, which is also World Environment Day, prior to then Arbor Day, in New Zealand, was celebrated on August 4 – which is rather late in the year for tree planting in New Zealand hence the date change.
What the Department of Conservation (DOC) does for Arbor Day:
Many of DOC's Arbor Day activities focus on ecological restoration projects using native plants to restore habitats that have been damaged or destroyed by humans or invasive pests and weeds. There are great restoration projects underway around New Zealand and many organisations including community groups, landowners, conservation organisations, iwi, volunteers, schools, local businesses, nurseries and councils are involved in them. These projects are part of a vision to protect and restore the indigenous biodiversity.
Niger.
Since 1975, Niger has celebrated Arbor Day as part of its Independence Day: 3 August. On this day, aiding the fight against desertification, each Nigerien plants a tree.
Pakistan.
National tree plantation day of Pakistan ( قومی شجر کاری دن ) is celebrated on 18 August.
Philippines.
Since 1947, Arbor Day in the Philippines has been institutionalized to be observed throughout the nation by planting trees and ornamental plants and other forms of relevant activities. 
Arbor Day in the Philippines has been commemorated in the Philippines since 1947 when its practice was instituted through Proclamation No. 30. It was subsequently revised by Proclamation No. 41, issued in the same year. In 1955, the commemoration was extended from a day to a week and was moved to the last full week of July. Over two decades later, its commemoration was moved to the second week of June. In 2003, the commemorations were reduced from a week to a day and was moved to June 25 per Proclamation No. 396. The same proclamation directed "the active participation of all government agencies, including government-owned and controlled corporations, private sector, schools, civil society groups and the citizenry in tree planting activity". It was subsequently revised by Proclamation 643 in the succeeding year.
In 2012, Republic Act 10176 was passed, which revived tree planting events "as yearly event for local government units". Since 2012, many local arbor day celebrations have been commemorated, as in the cases of Natividad and Tayug in Pangasinan and Santa Rita in Pampanga.
Poland.
In Poland, Arbor Day is celebrated since 2002. Each year, on October 10. lots of Polish people plant trees as well as participate in events organized by ecological foundations. Moreover, Polish Forest Inspectorates and schools give special lectures and lead ecological awareness campaigns.
Portugal.
Arbor Day is celebrated on March 21. It's not a national holiday but instead schools nationwide celebrate this day with environment-related activities, namely tree planting.
South Africa.
Arbor Day was celebrated from 1945 until 2000 in South Africa, when the national government extended it to National Arbor Week, which lasts from 1–7 September. Two trees, one common and one rare, are highlighted to increase public awareness of indigenous trees, while various "greening" activities are undertaken by schools, businesses and other organizations.
South Korea.
Arbor Day (Sikmogil, 식목일) was a public holiday in South Korea on April 5 until 2005. The day is still celebrated, though. On non-leap years, the day coincides with Hansik.
Spain.
In Spain is usually held on the International Forest Day on 21 March, but following the 1915 decree spirit that forced to celebrate the Arbor Day throughout Spain, each municipality or collective decides the date for its Arbor Day, usually between February and May. In Villanueva de la Sierra (Extremadura), where the first Arbor Day in the world was held in 1805, it is celebrated, as on that occasion, on Tuesday Carnaval. It is a great day in the local festive calendar.
As an example of commitment to nature, the small town of Pescueza, with only 180 inhabitants, organizes every spring a large plantation of holm oaks, which is called the "Festivalino" promoted by city council, several foundations and citizen participation where several thousand people together repopulates naked lands and regaining life. All wrapped up in a fun party atmosphere, joy, music and renew.
Sri Lanka.
National Tree Planting Day is on November 15.
Taiwan.
Arbor Day (植樹節) has been a traditional holiday in the Republic of China since 1927. In 1914, the founder of the agricultural college at Nanking University suggested to the now-defunct Ministry of Agriculture and Forestry that China should imitate the practice in the United States of Arbor Day. The holiday would be held the same day as the Qingming Festival. However, for unknown reasons, the suggestion was not made through the formal process, so nothing came from this original request. After the successful conclusion of the Northern Expedition, the now-defunct Ministry of Agriculture and Minerals formally petitioned the Executive Yuan to establish Arbor Day to commemorate the passing of Dr. Sun Yat-sen, the Father of Modern China. He had been a major advocate of afforestation in his life, because it would increase people's livelihoods. The Executive Yuan approved Arbor Day in the spirit of Dr. Sun that year and has since been celebrated on March 12 for this purpose.
Tanzania.
National Tree Planting Day is on April 1
Uganda.
National Tree Planting Day is on March 24.
United Kingdom.
First mounted in 1975, National Tree Week is a celebration of the start of the winter tree planting season. Around a million trees are planted each year by schools, community organizations and local authorities.
United States.
Arbor Day was founded in 1872 by Julius Sterling Morton in Nebraska City, Nebraska. By the 1920s, each state in the United States had passed public laws that stipulated a certain day to be Arbor Day or Arbor and Bird Day observance.
National Arbor Day is celebrated every year on the last Friday in April; in Nebraska, it is a civic holiday. Each state celebrates its own state holiday. The customary observance is to plant a tree. On the first Arbor Day, April 10, 1872, an estimated one million trees were planted.
Venezuela.
Venezuela recognizes "Día del Arbol" on the last Sunday of May.

</doc>
<doc id="2018" url="https://en.wikipedia.org/wiki?curid=2018" title="A. J. Ayer">
A. J. Ayer

Sir Alfred Jules "Freddie" Ayer (; 29 October 1910 – 27 June 1989) was a British philosopher known for his promotion of logical positivism, particularly in his books "Language, Truth, and Logic" (1936) and "The Problem of Knowledge" (1956).
He was educated at Eton College and Oxford University, after which he studied the philosophy of Logical Positivism at the University of Vienna. From 1933 to 1940 he lectured on philosophy at Christ Church, Oxford. 
During the Second World War Ayer was a Special Operations Executive and MI6 agent. 
He was Grote Professor of the Philosophy of Mind and Logic at University College London from 1946 until 1959, after which he returned to Oxford to become Wykeham Professor of Logic. He was president of the Aristotelian Society from 1951 to 1952 and knighted in 1970.
Early life.
Ayer was born in St John's Wood, in north west London, to a wealthy family from continental Europe. His mother, Reine Citroën, was from the Dutch Jewish family who founded the Citroën car company in France. His father, Jules Ayer, was a Swiss Calvinist financier who worked for the Rothschild family.
Education.
Ayer was educated at Ascham St Vincent's School, a former boarding preparatory school for boys in the seaside town of Eastbourne in Sussex, in which he started boarding at the comparatively early age of seven, due to the First World War, and Eton College, a boarding school in Eton (near Windsor) in Berkshire. It was at Eton that Ayer first became known for his characteristic bravado and precocity. Although primarily interested in furthering his intellectual pursuits, he was very keen on sports, particularly rugby, and reputedly played the Eton Wall Game very well. In the final examinations at Eton, Ayer came second in his year, and first in classics. In his final year, as a member of Eton's senior council, he unsuccessfully campaigned for the abolition of corporal punishment at the school. He won a classics scholarship to Christ Church, Oxford.
Life and career.
After graduation from Oxford University Ayer spent a year in Vienna, returned to England and published his first book, "Language, Truth and Logic" in 1936. The first exposition in English of Logical Positivism as newly developed by the Vienna Circle, this made Ayer at age 26 the 'enfant terrible' of British philosophy. In the Second World War he served as an officer in the Welsh Guards, chiefly in intelligence (Special Operations Executive (SOE) and MI6). Ayer was commissioned second lieutenant into the Welsh Guards from Officer Cadet Training Unit on 21 September 1940
After the war he briefly returned to Oxford University where he became a fellow and Dean of Wadham College. He thereafter taught philosophy at London University from 1946 until 1959, when he also started to appear on radio and television. He was an extrovert, and social mixer, and was married four times, including to Dee Wells and Vanessa Salmon (thus becoming stepfather to Nigella Lawson). Reputedly he liked dancing and attending the clubs in London and New York. He was also obsessed with sport: he had played rugby for Eton, and was a noted cricketer and a keen supporter of the Tottenham Hotspur football team. For an academic, Ayer was an unusually well-connected figure in his time, with close links to 'high society' and the establishment. Presiding over Oxford high-tables, he is often described as charming, but at times he could also be intimidating.
Philosophical ideas.
In "Language, Truth and Logic" (1936), Ayer presents the verification principle as the only valid basis for philosophy. Unless logical or empirical verification is possible, statements like "God exists" or "charity is good" are not true or untrue but meaningless, and may thus be excluded or ignored. Religious language in particular was unverifiable and as such literally nonsense. He also criticises C. A. Mace's opinion that metaphysics is a form of intellectual poetry. The stance of a person who believes "God" denotes no verifiable hypothesis is sometimes referred to as igtheism (for example, by Paul Kurtz). In later years Ayer reiterated that he did not believe in God and began to refer to himself as an atheist. He followed in the footsteps of Bertrand Russell by debating with the Jesuit scholar Frederick Copleston on the topic of religion.
Ayer's version of emotivism divides "the ordinary system of ethics" into four classes:
He focuses on propositions of the first class—moral judgments—saying that those of the second class belong to science, those of the third are mere commands, and those of the fourth (which are considered in normative ethics as opposed to meta-ethics) are too concrete for ethical philosophy. While class three statements were irrelevant to Ayer's brand of emotivism, they would later play a significant role in Stevenson's.
Ayer argues that moral judgments cannot be translated into non-ethical, empirical terms and thus cannot be verified; in this he agrees with ethical intuitionists. But he differs from intuitionists by discarding appeals to intuition of non-empirical moral truths as "worthless" since the intuition of one person often contradicts that of another. Instead, Ayer concludes that ethical concepts are "mere pseudo-concepts":
Between 1945 and 1947, together with Russell and George Orwell, he contributed a series of articles to "Polemic", a short-lived British "Magazine of Philosophy, Psychology, and Aesthetics" edited by the ex-Communist Humphrey Slater.
Ayer was closely associated with the British humanist movement. He was an Honorary Associate of the Rationalist Press Association from 1947 until his death. He was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1963. In 1965, he became the first president of the Agnostics' Adoption Society and in the same year succeeded Julian Huxley as president of the British Humanist Association, a post he held until 1970. In 1968 he edited "The Humanist Outlook", a collection of essays on the meaning of humanism. In addition he was one of the signers of the Humanist Manifesto.
He taught or lectured several times in the United States, including serving as a visiting professor at Bard College in the fall of 1987. At a party that same year held by fashion designer Fernando Sanchez, Ayer, then 77, confronted Mike Tyson who was forcing himself upon the (then) little-known model Naomi Campbell. When Ayer demanded that Tyson stop, the boxer said: "Do you know who the fuck I am? I'm the heavyweight champion of the world," to which Ayer replied: "And I am the former Wykeham Professor of Logic. We are both pre-eminent in our field. I suggest that we talk about this like rational men". Ayer and Tyson then began to talk, while Naomi Campbell slipped out.
Later career.
From 1959 to his retirement in 1978, Sir Alfred held the Wykeham Chair, Professor of Logic at Oxford. He was knighted in 1970.
Ayer died on 27 June 1989. From 1980 – 1989, Ayer lived at 51 York Street, Marylebone, where a memorial plaque was unveiled on 19 November 1995.
Personal life.
Ayer was married four times to three women. His first marriage was from 1932–1941 to (Grace Isabel) Renée (d. 1980), who subsequently married philosopher Stuart Hampshire, Ayer's friend and colleague. In 1960 he married Alberta Constance (Dee) Wells (1925–2003), with whom he had one son. Ayer's marriage to Wells was dissolved in 1983 and that same year he married Vanessa Salmon, former wife of politician Nigel Lawson. She died in 1985 and in 1989 he remarried Dee Wells, who survived him. Ayer also had a daughter with Hollywood columnist Sheilah Graham Westbrook.
Near-death experience.
In 1988, shortly before his death, Ayer wrote an article entitled, "What I saw when I was dead", describing an unusual near-death experience. Of the experience, Ayer first said that it "slightly weakened my conviction that my genuine death ... will be the end of me, though I continue to hope that it will be." However, a few days later he revised this, saying "what I should have said is that my experiences have weakened, not my belief that there is no life after death, but my inflexible attitude towards that belief".
In 2001 Dr Jeremy George, the attending physician, claimed that Ayer had confided to him: "I saw a Divine Being. I'm afraid I'm going to have to revise all my books and opinions." Ayer's son Nick, however, said that he had never mentioned this to him though he did find his father's words to be extraordinary, and said he had long felt there was something possibly suspect about his father's version of his near death experience.
Works.
Ayer is best known for popularising the verification principle, in particular through his presentation of it in "Language, Truth, and Logic" (1936). The principle was at the time at the heart of the debates of the so-called Vienna Circle which Ayer visited as a young guest. Others, including the leading light of the circle, Moritz Schlick, were already offering their own papers on the issue. Ayer's own formulation was that a sentence can only be meaningful if it has verifiable empirical import, otherwise it is either "analytical" if tautologous, or "metaphysical" (i.e. meaningless, or "literally senseless"). He started to work on the book at the age of 23 and it was published when he was 26. Ayer's philosophical ideas were deeply influenced by those of the Vienna Circle and David Hume. His clear, vibrant and polemical exposition of them makes "Language, Truth and Logic" essential reading on the tenets of logical empiricism– the book is regarded as a classic of 20th century analytic philosophy, and is widely read in philosophy courses around the world. In it, Ayer also proposed that the distinction between a conscious man and an unconscious machine resolves itself into a distinction between 'different types of perceptible behaviour', an argument which anticipates the Turing test published in 1950 to test a machine's capability to demonstrate intelligence.
Ayer wrote two books on the philosopher Bertrand Russell, "Russell and Moore: The Analytic Heritage" (1971) and "Russell" (1972). He also wrote an introductory book on the philosophy of David Hume and a short biography of Voltaire.
Ayer was strong critic of the German philosopher Martin Heidegger. As a logical positivist Ayer was in conflict with Heidegger's proposed vast, overarching theories regarding existence. These he felt were completely unverifiable through empirical demonstration and logical analysis. This sort of philosophy was an unfortunate strain in modern thought. He considered Heidegger to be the worst example of such philosophy, which Ayer believed to be entirely useless.
In 1972–1973 Ayer gave the Gifford Lectures at University of St Andrews, later published as "The Central Questions of Philosophy". In the preface to the book, he defends his selection to hold the lectureship on the basis that Lord Gifford wished to promote '"Natural Theology", in the widest sense of that term', and that non-believers are allowed to give the lectures if they are "able reverent men, true thinkers, sincere lovers of and earnest inquirers after truth". He still believed in the viewpoint he shared with the logical positivists: that large parts of what was traditionally called "philosophy"– including the whole of metaphysics, theology and aesthetics– were not matters that could be judged as being true or false and that it was thus meaningless to discuss them.
In "The Concept of a Person and Other Essays" (1963), Ayer heavily criticized Wittgenstein's private language argument.
Ayer's sense-data theory in "Foundations of Empirical Knowledge" was famously criticised by fellow Oxonian J. L. Austin in "Sense and Sensibilia", a landmark 1950s work of common language philosophy. Ayer responded to this in the essay "Has Austin Refuted the Sense-data Theory?", which can be found in his "Metaphysics and Common Sense" (1969).
Awards.
He was awarded a Knighthood as Knight Bachelor in the London Gazette on 1 January 1970.

</doc>
<doc id="2019" url="https://en.wikipedia.org/wiki?curid=2019" title="André Weil">
André Weil

André Weil (; ; 6 May 1906 – 6 August 1998) was an influential French mathematician of the 20th century, known for his foundational work in number theory and algebraic geometry. He was a founding member and the "de facto" early leader of the Bourbaki group. The philosopher Simone Weil was his sister.
Life.
André Weil was born in Paris to Alsatian agnostic Jewish parents who fled the annexation of Alsace-Lorraine by the German Empire after the Franco-Prussian War in 1870–71. The famous philosopher Simone Weil was Weil's only sibling. He studied in Paris, Rome and Göttingen and received his doctorate in 1928. While in Germany, Weil befriended Carl Ludwig Siegel. Starting in 1930, he spent two academic years at Aligarh Muslim University. Aside from mathematics, Weil held lifelong interests in Hinduism and Sanskrit literature. After teaching for one year in Aix-Marseille University, he taught for six years in Strasbourg. He married Éveline in 1937.
Weil was in Finland when World War II broke out; he had been traveling in Scandinavia since April 1939. His wife Éveline returned to France without him. Weil was mistakenly arrested in Finland at the outbreak of the Winter War on suspicion of spying; however, accounts of his life having been in danger were shown to be exaggerated. Weil returned to France via Sweden and the United Kingdom, and was detained at Le Havre in January 1940. He was charged with failure to report for duty, and was imprisoned in Le Havre and then Rouen. It was in the military prison in Bonne-Nouvelle, a district of Rouen, from February to May, that Weil completed the work that made his reputation. He was tried on 3 May 1940. Sentenced to five years, he requested to be attached to a military unit instead, and was given the chance to join a regiment in Cherbourg. After the fall of France, he met up with his family in Marseille, where he arrived by sea. He then went to Clermont-Ferrand, where he managed to join his wife Éveline, who had been living in German-occupied France.
In January 1941, Weil and his family sailed from Marseille to New York. He spent the remainder of the war in the United States, where he was supported by the Rockefeller Foundation and the Guggenheim Foundation. For two years, he taught undergraduate mathematics at Lehigh University. But, he hated Lehigh very much for their heavy teaching workload and he swore that he would never talk about "Lehigh" any more. He quit the job at Lehigh, and then he moved to Brazil and taught at the Universidade de São Paulo from 1945 to 1947, where he worked with Oscar Zariski. He then returned to the United States and taught at the University of Chicago from 1947 to 1958, before moving to the Institute for Advanced Study, where he would spend the remainder of his career. In 1979, Weil shared the second Wolf Prize in Mathematics with Jean Leray.
Work.
Weil made substantial contributions in a number of areas, the most important being his discovery of profound connections between algebraic geometry and number theory. This began in his doctoral work leading to the Mordell–Weil theorem (1928, and shortly applied in Siegel's theorem on integral points). Mordell's theorem had an "ad hoc" proof; Weil began the separation of the infinite descent argument into two types of structural approach, by means of height functions for sizing rational points, and by means of Galois cohomology, which would not be categorized as such for another two decades. Both aspects of Weil's work have steadily developed into substantial theories.
Among his major accomplishments were the 1940s proof of the Riemann hypothesis for zeta-functions of curves over finite fields, and his subsequent laying of proper foundations for algebraic geometry to support that result (from 1942 to 1946, most intensively). The so-called Weil conjectures were hugely influential from around 1950; these statements were later proved by Bernard Dwork, Alexander Grothendieck, Michael Artin, and finally by Pierre Deligne, who completed the most difficult step in 1973.
Weil introduced the adele ring in the late 1930s, following Claude Chevalley's lead with the ideles, and gave a proof of the Riemann–Roch theorem with them (a version appeared in his "Basic Number Theory" in 1967). His 'matrix divisor' (vector bundle "avant la lettre") Riemann–Roch theorem from 1938 was a very early anticipation of later ideas such as moduli spaces of bundles. The Weil conjecture on Tamagawa numbers proved resistant for many years. Eventually the adelic approach became basic in automorphic representation theory. He picked up another credited "Weil conjecture", around 1967, which later under pressure from Serge Lang (resp. of Serre) became known as the Taniyama–Shimura conjecture (resp. Taniyama–Weil conjecture) based on a roughly formulated question of Taniyama at the 1955 Nikkō conference. His attitude towards conjectures was that one should not dignify a guess as a conjecture lightly, and in the Taniyama case, the evidence was only there after extensive computational work carried out from the late 1960s.
Other significant results were on Pontryagin duality and differential geometry. He introduced the concept of a uniform space in general topology, as a by-product of his collaboration with Nicolas Bourbaki (of which he was a Founding Father). His work on sheaf theory hardly appears in his published papers, but correspondence with Henri Cartan in the late 1940s, and reprinted in his collected papers, proved most influential.
He discovered that the so-called Weil representation, previously introduced in quantum mechanics by Irving Segal and Shale, gave a contemporary framework for understanding the classical theory of quadratic forms. This was also a beginning of a substantial development by others, connecting representation theory and theta functions.
He also wrote several books on the history of Number Theory. Weil was elected Foreign Member of the Royal Society (ForMemRS) in 1966.
As expositor.
Weil's ideas made an important contribution to the writings and seminars of Bourbaki, before and after World War II.
He says on page 114 of his autobiography that he was responsible for the null set symbol (Ø) and that it came from the Norwegian alphabet, which he alone among the Bourbaki group was familiar with.
Beliefs.
Indian (Hindu) thought had great influence on Weil. In his autobiography, he says that the only religious ideas that appealed to him were those to be found in Hindu philosophical thought. Although he was an agnostic, he respected religions.
Books.
Mathematical works:
Collected papers:
Autobiography:
Memoir by his daughter:

</doc>
<doc id="2020" url="https://en.wikipedia.org/wiki?curid=2020" title="Achaeans (Homer)">
Achaeans (Homer)

The Achaeans (; "Akhaioí") constitute one of the collective names for the Greeks in Homer's "Iliad" (used 598 times) and "Odyssey". The other common names are Danaans (; "Danaoi"; used 138 times in the "Iliad") and Argives (; ; used 182 times in the "Iliad") while Panhellenes ( "Panhellenes") and Hellenes (; "Hellenes") both appear only once; all of the aforementioned terms were used synonymously to denote a common Greek civilizational identity. In the historical period, the Achaeans were the inhabitants of the region of Achaea, a region in the north-central part of the Peloponnese. The city-states of this region later formed a confederation known as the Achaean League, which was influential during the 3rd and 2nd centuries BC.
Homeric versus later use.
The Homeric "long-haired Achaeans" would have been a part of the Mycenaean civilization that dominated Greece from circa 1600 BC until 1100 BC. Later, by the Archaic and Classical periods, the term "Achaeans" referred to inhabitants of the much smaller region of Achaea. Herodotus identified the Achaeans of the northern Peloponnese as descendants of the earlier, Homeric Achaeans. According to Pausanias, writing in the 2nd century CE, the term "Achaean" was originally given to those Greeks inhabiting the Argolis and Laconia.
Pausanias and Herodotus both recount the legend that the Achaeans were forced from their homelands by the Dorians, during the legendary Dorian invasion of the Peloponnese. They then moved into the region later called Achaea.
A scholarly consensus has not yet been reached on the origin of the historic Achaeans relative to the Homeric Achaeans and is still hotly debated. Former emphasis on presumed race, such as John A. Scott's article about the blond locks of the Achaeans as compared to the dark locks of "Mediterranean" Poseidon, on the basis of hints in Homer, has been rejected by some. The contrasting belief that "Achaeans", as understood through Homer, is "a name without a country", an "ethnos" created in the Epic tradition, has modern supporters among those who conclude that "Achaeans" were redefined in the 5th century BC, as contemporary speakers of Aeolic Greek.
Karl Beloch has suggested there was no Dorian invasion, but rather that the Peloponnesian Dorians were the Achaeans. Eduard Meyer, disagreeing with Beloch, has instead put forth the suggestion that the real-life Achaeans were mainland pre-Dorian Greeks. His conclusion is based on his research on the similarity between the languages of the Achaeans and pre-historic Arcadians. William Prentice disagrees with both, noting archeological evidence suggests the Achaeans instead migrated from "southern Asia Minor to Greece, probably settling first in lower Thessaly" probably prior to 2000 BC.
Hittite documents.
Emil Forrer, a Swiss Hittitologist who worked on the Boghazköy tablets in Berlin, said the Achaeans of pre-Homeric Greece were directly associated with the term "Land of Ahhiyawa" mentioned in the Hittite texts. His conclusions at the time were challenged by other Hittitologists (i.e. Johannes Friedrich in 1927 and Albrecht Götze in 1930), as well as by Ferdinand Sommer, who published his "Die Ahhijava-Urkunden" ("The Ahhiyawa Documents") in 1932.
Some Hittite texts mention a nation lying to the west called Ahhiyawa. In the earliest reference to this land, a letter outlining the treaty violations of the Hittite vassal Madduwatta, it is called "Ahhiya". Another important example is the "Tawagalawa Letter" written by an unnamed Hittite king (most probably Hattusili III) of the empire period (14th–13th century BC) to the king of "Ahhiyawa", treating him as an equal and suggesting Miletus ("Millawanda") was under his control. It also refers to an earlier ""Wilusa" episode" involving hostility on the part of "Ahhiyawa". Ahhiya(wa) has been identified with the Achaeans of the Trojan War and the city of Wilusa with the legendary city of Troy (note the similarity with early Greek "Wilion", later "Ilion", the name of the acropolis of Troy). The exact relationship of the term "Ahhiyawa" to the Achaeans beyond a similarity in pronunciation was hotly debated by scholars, even following the discovery that Mycenaean Linear B is an early form of Greek; the earlier debate was summed up in 1984 by Hans G. Güterbock of the Oriental Institute. More recent research based on new readings and interpretations of the Hittite texts, as well as of the material evidence for Mycenaean contacts with the Anatolian mainland, came to the conclusion that "Ahhiyawa" referred to the Mycenaean world, or at least to a part of it.
Egyptian sources.
It has been proposed that "Ekwesh" of the Egyptian records may relate to "Achaea" (compared to Hittite "Ahhiyawa"), whereas "Denyen" and "Tanaju" may relate to Classical Greek "Danaoi". The earliest textual reference to the Mycenaean world is in the Annals of Thutmosis III (ca. 1479–1425 BC), which refers to messengers from the king of the Tanaju, circa 1437 BC, offering greeting gifts to the Egyptian king, in order to initiate diplomatic relations, when the latter campaigned in Syria. Tanaju is also listed in an inscription at the Mortuary Temple of Amenhotep III. The latter ruled Egypt in circa 1382–1344 BC. Moreover, a list of the cities and regions of the Tanaju is also mentioned in this inscription; among the cities listed are Mycenae, Nauplion, Kythera, Messenia and the Thebaid (region of Thebes).
During the 5th year of Pharaoh Merneptah, a confederation of Libyan and northern peoples is supposed to have attacked the western delta. Included amongst the ethnic names of the repulsed invaders is the Ekwesh or Eqwesh, whom some have seen as Achaeans, although Egyptian texts specifically mention these Ekwesh to be circumcised (which does not seem to have been a general practice in the Aegean at the time). Homer mentions an Achaean attack upon the delta, and Menelaus speaks of the same in Book IV of the "Odyssey" to Telemachus when he recounts his own return home from the Trojan War. Later Greek myths also say Helen had spent the time of the Trojan War in Egypt, and not at Troy, and that after Troy the Greeks went there to recover her.
Greek mythology.
In Greek mythology, the perceived cultural divisions among the Hellenes were represented as legendary lines of descent that identified kinship groups, with each line being derived from an eponymous ancestor. Each of the Greek "ethne" were said to be named in honor of their respective ancestors: Achaeus of the Achaeans, Danaus of the Danaans, Cadmus of the Cadmeans (the Thebans), Hellen of the Hellenes (not to be confused with Helen of Troy), Aeolus of the Aeolians, Ion of the Ionians, and Dorus of the Dorians.
Cadmus from Phoenicia, Danaus from Egypt, and Pelops from Anatolia each gained a foothold in mainland Greece and were assimilated and Hellenized. Hellen, Graikos, Magnes, and Macedon were sons of Deucalion and Pyrrha, the only people who survived the Great Flood; the "ethne" were said to have originally been named "Graikoi" after the elder son but later renamed "Hellenes" after Hellen who was proved to be the strongest. Sons of Hellen and the nymph Orseis were Dorus, Xuthos, and Aeolus. Sons of Xuthos and Kreousa, daughter of Erechthea, were Ion and Achaeus.
According to Hyginus, 22 Achaeans killed 362 Trojans during their ten years at Troy.
Etymology.
For the etymology of the name "Akhaioi", see Achaeans (tribe). The etymology of "Danaoi" is uncertain. According to R. S. P. Beekes "the name is certainly Pre-Greek".

</doc>
<doc id="2021" url="https://en.wikipedia.org/wiki?curid=2021" title="Atle Selberg">
Atle Selberg

Atle Selberg (14 June 1917 – 6 August 2007) was a Norwegian mathematician known for his work in analytic number theory, and in the theory of automorphic forms, in particular bringing them into relation with spectral theory. He was awarded the Fields Medal in 1950.
Early years.
Selberg was born in Langesund, Norway, the son of teacher Anna Kristina Selberg and mathematician Ole Michael Ludvigsen Selberg. Two of his brothers also went on to become mathematicians as well, and the remaining one became a professor of engineering. 
While he was still at school he was influenced by the work of Srinivasa Ramanujan and he found an exact analytical formula for the partition function as suggested by the works of Ramanujan; however, this result was first published by Hans Rademacher. During the war he fought against the German invasion of Norway, and was imprisoned several times. 
He studied at the University of Oslo and completed his Ph.D. in 1943.
World War II.
During World War II, Selberg worked in isolation due to the German occupation of Norway. After the war his accomplishments became known, including a proof that a positive proportion of the zeros of the Riemann zeta function lie on the line formula_1. 
After the war, he turned to sieve theory, a previously neglected topic which Selberg's work brought into prominence. In a 1947 paper he introduced the Selberg sieve, a method well adapted in particular to providing auxiliary upper bounds, and which contributed to Chen's theorem, among other important results. 
In March 1948, Selberg established, by elementary means, the asymptotic formula
where
for primes formula_4. By July of that year, Selberg and Paul Erdős had each obtained elementary proofs of the prime number theorem, both using Selberg's then unpublished asymptotic formula as a starting point. Circumstances leading up to the proofs, as well as publication disagreements, led to a bitter dispute between the two mathematicians. 
For his fundamental accomplishments during the 1940s, Selberg received the 1950 Fields Medal.
Institute for Advanced Study.
Selberg moved to the United States and settled at the Institute for Advanced Study in Princeton, New Jersey in the 1950s where he remained until his death. During the 1950s he worked on introducing spectral theory into number theory, culminating in his development of the Selberg trace formula, the most famous and influential of his results. In its simplest form, this establishes a duality between the lengths of closed geodesics on a compact Riemann surface and the eigenvalues of the Laplacian, which is analogous to the duality between the prime numbers and the zeros of the zeta function.
He was awarded the 1986 Wolf Prize in Mathematics. He was also awarded an honorary Abel Prize in 2002, its founding year, before the awarding of the regular prizes began.
Selberg received many distinctions for his work in addition to the Fields Medal, the Wolf Prize and the Gunnerus Medal. He was elected to the Norwegian Academy of Science and Letters, the Royal Danish Academy of Sciences and Letters and the American Academy of Arts and Sciences.
In 1972 he was awarded an honorary degree, doctor philos. honoris causa, at the Norwegian Institute of Technology, later part of Norwegian University of Science and Technology.
Selberg had two children, Ingrid Selberg and Lars Selberg. Ingrid Selberg is married to playwright Mustapha Matura.
He died at home in Princeton on 6 August 2007 of heart failure.

</doc>
<doc id="2023" url="https://en.wikipedia.org/wiki?curid=2023" title="Aeschylus">
Aeschylus

Aeschylus ( or ; "Aiskhulos"; ; c. 525/524 – c. 456/455 BC) was an ancient Greek tragedian. He is also the first whose plays still survive; the others are Sophocles and Euripides. He is often described as the father of tragedy: critics and scholars' knowledge of the genre begins with his work, and understanding of earlier tragedies is largely based on inferences from his surviving plays. According to Aristotle, he expanded the number of characters in theater to allow conflict among them, whereas characters previously had interacted only with the chorus.
Only seven of his estimated seventy to ninety plays have survived, and there is a longstanding debate regarding his authorship of one of these plays, "Prometheus Bound", which some believe his son Euphorion actually wrote. Fragments of some other plays have survived in quotes and more continue to be discovered on Egyptian papyrus, often giving us surprising insights into his work. He was probably the first dramatist to present plays as a trilogy; his "Oresteia" is the only ancient example of the form to have survived. At least one of his plays was influenced by the Persians' second invasion of Greece (480-479 BC). This work, "The Persians", is the only surviving classical Greek tragedy concerned with contemporary events (very few of that kind were ever written), and a useful source of information about its period. The significance of war in Ancient Greek culture was so great that Aeschylus' epitaph commemorates his participation in the Greek victory at Marathon while making no mention of his success as a playwright. Despite this, Aeschylus' work – particularly the "Oresteia" "–" is acclaimed by today's literary academics.
Life.
There are no reliable sources for the life of Aeschylus.
Aeschylus was born in c. 525 BC in Eleusis, a small town about 27 kilometers northwest of Athens, which is nestled in the fertile valleys of western Attica, though the date is most likely based on counting back forty years from his first victory in the Great Dionysia. His family was wealthy and well established; his father, Euphorion, was a member of the Eupatridae, the ancient nobility of Attica, though this might be a fiction that the ancients invented to account for the grandeur of his plays.
As a youth, he worked at a vineyard until, according to the 2nd-century AD geographer Pausanias, the god Dionysus visited him in his sleep and commanded him to turn his attention to the nascent art of tragedy. As soon as he woke from the dream, the young Aeschylus began to write a tragedy, and his first performance took place in 499 BC, when he was only 26 years old. He would win his first victory at the City Dionysia in 484 BC.
In 510 BC, when Aeschylus was 15 years old, Cleomenes I expelled the sons of Peisistratus from Athens, and Cleisthenes came to power. Cleisthenes' reforms included a system of registration that emphasized the importance of the deme over family tradition. In the last decade of the 6th century, Aeschylus and his family were living in the deme of Eleusis.
The Persian Wars would play a large role in the playwright's life and career. In 490 BC, Aeschylus and his brother Cynegeirus fought to defend Athens against Darius I's invading Persian army at the Battle of Marathon. The Athenians emerged triumphant, a victory celebrated across the city-states of Greece. Cynegeirus, however, died in the battle, receiving a mortal wound while trying to prevent a Persian ship retreating from the shore, for which his countrymen extolled him as a hero.
In 480, Aeschylus was called into military service again, this time against Xerxes I's invading forces at the Battle of Salamis, and perhaps, too, at the Battle of Plataea in 479. Ion of Chios was a witness for Aeschylus's war record and his contribution in Salamis. Salamis holds a prominent place in "The Persians", his oldest surviving play, which was performed in 472 BC and won first prize at the Dionysia.
Aeschylus was one of many Greeks who had been initiated into the Eleusinian Mysteries, a cult to Demeter based in his hometown of Eleusis. As the name implies, members of the cult were supposed to have gained some secret knowledge. Firm details of specific rites are sparse, as members were sworn under the penalty of death not to reveal anything about the Mysteries to non-initiates. Nevertheless, according to Aristotle some thought that Aeschylus had revealed some of the cult's secrets on stage.
Other sources claim that an angry mob tried to kill Aeschylus on the spot, but he fled the scene. Heracleides of Pontus asserts that the audience tried to stone Aeschylus. He then took refuge at the altar in the orchestra of the Theater of Dionysus. At his trial, he pleaded ignorance. He was acquitted, with the jury sympathetic to the wounds that Aeschylus and Cynegeirus had suffered at Marathon. According to the 2nd-century AD author Aelian, Aeschylus's younger brother Ameinias helped to acquit Aeschylus by showing the jury the stump of the hand that he lost at Salamis, where he was voted bravest warrior. The truth is that the award for bravery at Salamis went not to Aeschylus' brother but to Ameinias of Pallene.
Aeschylus travelled to Sicily once or twice in the 470s BC, having been invited by Hiero I of Syracuse, a major Greek city on the eastern side of the island; and during one of these trips he produced "The Women of Aetna" (in honor of the city founded by Hieron) and restaged his "Persians". By 473 BC, after the death of Phrynichus, one of his chief rivals, Aeschylus was the yearly favorite in the Dionysia, winning first prize in nearly every competition. In 472 BC, Aeschylus staged the production that included the "Persians", with Pericles serving as "choregos".
In 458 BC, he returned to Sicily for the last time, visiting the city of Gela where he died in 456 or 455 BC. Valerius Maximus wrote that he was killed outside the city by a tortoise dropped by an eagle which had mistaken his head for a rock suitable for shattering the shell of the reptile. Pliny, in his "Naturalis Historiæ", adds that Aeschylus had been staying outdoors to avoid a prophecy that he would be killed by a falling object. Aeschylus's work was so respected by the Athenians that after his death, his were the only tragedies allowed to be restaged in subsequent competitions. His sons Euphorion and Euæon and his nephew Philocles also became playwrights.
The inscription on Aeschylus's gravestone makes no mention of his theatrical renown, commemorating only his military achievements:
According to Castoriadis, the inscription on his graveyard signifies the primary importance of "belonging to the City" (polis), of the solidarity that existed within the collective body of citizen-soldiers.
Personal life.
Aeschylus married and had two sons, Euphorion and Euaeon, both of whom became tragic poets. Euphorion won first prize in 431 in competition against both Sophocles and Euripides. His nephew, Philocles (his sister's son), was also a tragic poet, and won first prize in the competition against Sophocles' "Oedipus Rex".
A scholiast has noted that Philocles' "Tereus" was part of his "Pandionis" tetralogy. Aeschylus had at least two brothers, Cynegeirus and Ameinias.
Works.
The roots of Greek drama are in religious festivals for the gods, chiefly Dionysus, the god of wine. During Aeschylus's lifetime, dramatic competitions became part of the City Dionysia in the spring. The festival opened with a procession, followed with a competition of boys singing dithyrambs and culminated in a pair of dramatic competitions. The first competition Aeschylus would have participated in, consisted of three playwrights each presenting three tragic plays followed by a shorter comedic satyr play. A second competition of five comedic playwrights followed, and the winners of both competitions were chosen by a panel of judges.
Aeschylus entered many of these competitions in his lifetime, and various ancient sources attribute between seventy and ninety plays to him. Only seven tragedies have survived intact: "The Persians", "Seven against Thebes", "The Suppliants", the trilogy known as "The Oresteia", consisting of the three tragedies "Agamemnon", "The Libation Bearers" and "The Eumenides", together with "Prometheus Bound" (whose authorship is disputed). With the exception of this last play – the success of which is uncertain – all of Aeschylus's extant tragedies are known to have won first prize at the City Dionysia.
The Alexandrian "Life of Aeschylus" claims that he won the first prize at the City Dionysia thirteen times. This compares favorably with Sophocles' reported eighteen victories (with a substantially larger catalogue, at an estimated 120 plays), and dwarfs the five victories of Euripides, who is thought to have written roughly 90 plays.
Trilogies.
One hallmark of Aeschylean dramaturgy appears to have been his tendency to write connected trilogies, in which each play serves as a chapter in a continuous dramatic narrative. "The Oresteia" is the only extant example of this type of connected trilogy, but there is evidence that Aeschylus often wrote such trilogies. The comic satyr plays that follow his trilogies also drew upon stories derived from myths.
For example, the "Oresteia"'s satyr play "Proteus" treated the story of Menelaus' detour in Egypt on his way home from the Trojan War. Based on the evidence provided by a catalogue of Aeschylean play titles, scholia, and play fragments recorded by later authors, it is assumed that three other of his extant plays were components of connected trilogies: "Seven against Thebes" being the final play in an Oedipus trilogy, and "The Suppliants" and "Prometheus Bound" each being the first play in a Danaid trilogy and Prometheus trilogy, respectively (see below). Scholars have moreover suggested several completely lost trilogies derived from known play titles. A number of these trilogies treated myths surrounding the Trojan War. One, collectively called the "Achilleis", comprised the titles "Myrmidons", "Nereids" and "Phrygians" (alternately, "The Ransoming of Hector").
Another trilogy apparently recounts the entry of the Trojan ally Memnon into the war, and his death at the hands of Achilles ("Memnon" and "The Weighing of Souls" being two components of the trilogy); "The Award of the Arms", "The Phrygian Women", and "The Salaminian Women" suggest a trilogy about the madness and subsequent suicide of the Greek hero Ajax; Aeschylus also seems to have written about Odysseus' return to Ithaca after the war (including his killing of his wife Penelope's suitors and its consequences) in a trilogy consisting of "The Soul-raisers", "Penelope" and "The Bone-gatherers". Other suggested trilogies touched on the myth of Jason and the Argonauts ("Argô", "Lemnian Women", "Hypsipylê"); the life of Perseus ("The Net-draggers", "Polydektês", "Phorkides"); the birth and exploits of Dionysus ("Semele", "Bacchae", "Pentheus"); and the aftermath of the war portrayed in "Seven against Thebes" ("Eleusinians", "Argives" (or "Argive Women"), "Sons of the Seven").
Surviving plays.
"The Persians".
The earliest of his plays to survive is "The Persians" ("Persai"), performed in 472 BC and based on experiences in Aeschylus's own life, specifically the Battle of Salamis. It is unique among surviving Greek tragedies in that it describes a recent historical event. "The Persians" focuses on the popular Greek theme of "hubris" by blaming Persia's loss on the pride of its king.
It opens with the arrival of a messenger in Susa, the Persian capital, bearing news of the catastrophic Persian defeat at Salamis to Atossa, the mother of the Persian King Xerxes. Atossa then travels to the tomb of Darius, her husband, where his ghost appears to explain the cause of the defeat. It is, he says, the result of Xerxes' hubris in building a bridge across the Hellespont, an action which angered the gods. Xerxes appears at the end of the play, not realizing the cause of his defeat, and the play closes to lamentations by Xerxes and the chorus.
"Seven against Thebes".
"Seven against Thebes" ("Hepta epi Thebas"), which was performed in 467 BC, has the contrasting theme of the interference of the gods in human affairs. It also marks the first known appearance in Aeschylus's work of a theme which would continue through his plays, that of the polis (the city) being a key development of human civilization.
The play tells the story of Eteocles and Polynices, the sons of the shamed King of Thebes, Oedipus. The sons agree to alternate in the throne of the city, but after the first year Eteocles refuses to step down, and Polynices wages war to claim his crown. The brothers kill each other in single combat, and the original ending of the play consisted of lamentations for the dead brothers.
A new ending was added to the play some fifty years later: Antigone and Ismene mourn their dead brothers, a messenger enters announcing an edict prohibiting the burial of Polynices; and finally, Antigone declares her intention to defy this edict. The play was the third in a connected Oedipus trilogy; the first two plays were "Laius" and "Oedipus". The concluding satyr play was "The Sphinx".
"The Suppliants".
Aeschylus continued his emphasis on the polis with "The Suppliants" in 463 BC ("Hiketides"), which pays tribute to the democratic undercurrents running through Athens in advance of the establishment of a democratic government in 461. In the play, the Danaids, the fifty daughters of Danaus, founder of Argos, flee a forced marriage to their cousins in Egypt. They turn to King Pelasgus of Argos for protection, but Pelasgus refuses until the people of Argos weigh in on the decision, a distinctly democratic move on the part of the king. The people decide that the Danaids deserve protection, and they are allowed within the walls of Argos despite Egyptian protests.
The 1952 publication of Oxyrhynchus Papyrus 2256 fr. 3 confirmed a long-assumed (because of "The Suppliants"' cliffhanger ending) Danaid trilogy, whose constituent plays are generally agreed to be "The Suppliants", "The Egyptians" and "The Danaids". A plausible reconstruction of the trilogy's last two-thirds runs thus: In "The Egyptians", the Argive-Egyptian war threatened in the first play has transpired. During the course of the war, King Pelasgus has been killed, and Danaus rules Argos. He negotiates a peace settlement with Aegyptus, as a condition of which, his fifty daughters will marry the fifty sons of Aegyptus. Danaus secretly informs his daughters of an oracle predicting that one of his sons-in-law would kill him; he therefore orders the Danaids to murder their husbands on their wedding night. His daughters agree. "The Danaids" would open the day after the wedding.
In short order, it is revealed that forty-nine of the Danaids killed their husbands as ordered; Hypermnestra, however, loved her husband Lynceus, and thus spared his life and helped him to escape. Angered by his daughter's disobedience, Danaus orders her imprisonment and, possibly, her execution. In the trilogy's climax and dénouement, Lynceus reveals himself to Danaus, and kills him (thus fulfilling the oracle). He and Hypermnestra will establish a ruling dynasty in Argos. The other forty-nine Danaids are absolved of their murderous crime, and married off to unspecified Argive men. The satyr play following this trilogy was titled "Amymone", after one of the Danaids.
"The Oresteia".
The only complete (save a few missing lines in several spots) trilogy of Greek plays by any playwright still extant is the "Oresteia" (458 BC); although the satyr play that originally followed it, "Proteus", is lost except for some fragments. The trilogy consists of "Agamemnon", "The Libation Bearers" ("Choephoroi"), and "The Eumenides". Together, these plays tell the bloody story of the family of Agamemnon, King of Argos.
"Agamemnon".
Aeschylus begins in Greece describing the return of King Agamemnon from his victory in the Trojan War, from the perspective of the towns people (the Chorus) and his wife, Clytemnestra. However, dark foreshadowings build to the death of the king at the hands of his wife, who was angry at his sacrifice of their daughter Iphigenia, killed so the Gods would stop a storm hindering the Greek fleet in the war. She was also unhappy at his keeping of the Trojan prophetess Cassandra as a concubine. Cassandra foretells of the murder of Agamemnon, and of herself, to the assembled townsfolk, who are horrified. She then enters the palace knowing that she cannot avoid her fate. The ending of the play includes a prediction of the return of Orestes, son of Agamemnon, who will seek to avenge his father."
"The Libation Bearers".
"The Libation Bearers" continues the tale, opening with Orestes's arrival at Agamemnon's tomb. At the tomb, Electra meets Orestes, who has returned from exile in Phocis, and they plan revenge upon Clytemnestra and her lover Aegisthus. Clytemnestra's account of a nightmare in which she gives birth to a snake is recounted by the chorus; and this leads her to order Electra, her daughter, to pour libations on Agamemnon's tomb (with the assistance of libation bearers) in hope of making amends. Orestes enters the palace pretending to bear news of his own death, and when Clytemnestra calls in Aegisthus to share in the news, Orestes kills them both. Orestes is then beset by the Furies, who avenge the murders of kin in Greek mythology.
"The Eumenides".
The final play of "The Oresteia" addresses the question of Orestes' guilt. The Furies drive Orestes from Argos and into the wilderness. He makes his way to the temple of Apollo and begs him to drive the Furies away. Apollo had encouraged Orestes to kill Clytemnestra, and so bears some of the guilt for the murder. The Furies are a more ancient race of the gods, and Apollo sends Orestes to the temple of Athena, with Hermes as a guide.
The Furies track him down, and the goddess Athena, patron of Athens, steps in and declares that a trial is necessary. Apollo argues Orestes' case and, after the judges, including Athena deliver a tie vote, Athena announces that Orestes is acquitted. She renames the Furies "The Eumenides" (The Good-spirited, or Kindly Ones), and extols the importance of reason in the development of laws, and, as in "The Suppliants", the ideals of a democratic Athens are praised.
"Prometheus Bound".
In addition to these six works, a seventh tragedy, "Prometheus Bound", is attributed to Aeschylus by ancient authorities. Since the late 19th century, however, scholars have increasingly doubted this ascription, largely on stylistic grounds. Its production date is also in dispute, with theories ranging from the 480s BC to as late as the 410s.
The play consists mostly of static dialogue, as throughout the play the Titan Prometheus is bound to a rock as punishment from the Olympian Zeus for providing fire to humans. The god Hephaestus, the Titan Oceanus, and the chorus of Oceanids all express sympathy for Prometheus' plight. Prometheus meets Io, a fellow victim of Zeus' cruelty; and prophesies her future travels, revealing that one of her descendants will free Prometheus. The play closes with Zeus sending Prometheus into the abyss because Prometheus refuses to divulge the secret of a potential marriage that could prove Zeus' downfall.
The "Prometheus Bound" appears to have been the first play in a trilogy called the "Prometheia". In the second play, "Prometheus Unbound", Heracles frees Prometheus from his chains and kills the eagle that had been sent daily to eat Prometheus' perpetually regenerating liver. Perhaps foreshadowing his eventual reconciliation with Prometheus, we learn that Zeus has released the other Titans whom he imprisoned at the conclusion of the Titanomachy.
In the trilogy's conclusion, "Prometheus the Fire-Bringer", it appears that the Titan finally warns Zeus not to sleep with the sea nymph Thetis, for she is fated to give birth to a son greater than the father. Not wishing to be overthrown, Zeus marries Thetis off to the mortal Peleus; the product of that union is Achilles, Greek hero of the Trojan War. After reconciling with Prometheus, Zeus probably inaugurates a festival in his honor at Athens.
Lost plays.
Only the titles and assorted fragments of Aeschylus's other plays have come down to us. We have enough fragments of some plays (along with comments made by later authors and scholiasts) to produce rough synopses of their plots.
"Myrmidons".
This play was based on books 9 and 16 in Homer's "Iliad". Achilles sits in silent indignation over his humiliation at Agamemnon's hands for most of the play. Envoys from the Greek army attempt to reconcile him to Agamemnon, but he yields only to his friend Patroclus, who then battles the Trojans in Achilles' armour. The bravery and death of Patroclus are reported in a messenger's speech, which is followed by mourning.
"Nereids".
This play was based on books 18, 19, and 22 of the "Iliad"; it follows the Daughters of Nereus, the sea god, who lament Patroclus' death. In the play, a messenger tells how Achilles, perhaps reconciled to Agamemnon and the Greeks, slew Hector.
"Phrygians", or "Hector's Ransom".
In this play, Achilles sits in silent mourning over Patroclus, after a brief discussion with Hermes. Hermes then brings in King Priam of Troy, who wins over Achilles and ransoms his son's body in a spectacular coup de théâtre. A scale is brought on stage and Hector's body is placed in one scale and gold in the other. The dynamic dancing of the chorus of Trojans when they enter with Priam is reported by Aristophanes.
"Niobe".
The children of Niobe, the heroine, have been slain by Apollo and Artemis because Niobe had gloated that she had more children than their mother, Leto. Niobe sits in silent mourning on stage during most of the play. In the "Republic", Plato quotes the line "God plants a fault in mortals when he wills to destroy a house utterly."
These are the remaining 71 plays ascribed to Aeschylus which are known to us:
Influence.
Influence on Greek drama and culture.
When Aeschylus first began writing, the theatre had only just begun to evolve, although earlier playwrights like Thespis had already expanded the cast to include an actor who was able to interact with the chorus. Aeschylus added a second actor, allowing for greater dramatic variety, while the chorus played a less important role. He is sometimes credited with introducing "skenographia", or scene-decoration, though Aristotle gives this distinction to Sophocles. Aeschylus is also said to have made the costumes more elaborate and dramatic, and having his actors wear platform boots ("cothurni") to make them more visible to the audience. According to a later account of Aeschylus's life, as they walked on stage in the first performance of the "Eumenides", the chorus of Furies were so frightening in appearance that they caused young children to faint, patriarchs to urinate, and pregnant women to go into labour.
His plays were written in verse, no violence is performed on stage, and the plays have a remoteness from daily life in Athens, either by relating stories about the gods or by being set, like "The Persians", in far-away locales. Aeschylus's work has a strong moral and religious emphasis. The "Oresteia" trilogy concentrated on man's position in the cosmos in relation to the gods, divine law, and divine punishment.
Aeschylus's popularity is evident in the praise the comic playwright Aristophanes gives him in "The Frogs", produced some half-century after Aeschylus's death. Appearing as a character in the play, Aeschylus claims at line 1022 that his "Seven against Thebes" "made everyone watching it to love being warlike"; with his "Persians", Aeschylus claims at lines 1026–7 that he "taught the Athenians to desire always to defeat their enemies." Aeschylus goes on to say at lines 1039ff. that his plays inspired the Athenians to be brave and virtuous.
Influence outside of Greek culture.
Aeschylus's works were influential beyond his own time. Hugh Lloyd-Jones (Regius Professor of Greek Emeritus at Oxford University) draws attention to Richard Wagner's reverence of Aeschylus. Michael Ewans argues in his "Wagner and Aeschylus. The Ring and the Oresteia" (London: Faber. 1982) that the influence was so great as to merit a direct character by character comparison between Wagner's "Ring" and Aeschylus's "Oresteia". A critic of his book however, while not denying that Wagner read and respected Aeschylus, has described his arguments as unreasonable and forced.
Sir J. T. Sheppard argues in the second half of his "Aeschylus and Sophocles: Their Work and Influence" that Aeschylus, along with Sophocles, have played a major part in the formation of dramatic literature from the Renaissance to the present, specifically in French and Elizabethan drama. He also claims that their influence went beyond just drama and applies to literature in general, citing Milton and the Romantics.
During his presidential campaign in 1968, Senator Robert F. Kennedy quoted the Edith Hamilton translation of Aeschylus on the night of the assassination of Martin Luther King, Jr. Kennedy was notified of King's murder before a campaign stop in Indianapolis, Indiana and was warned not to attend the event due to fears of rioting from the mostly African-American crowd. Kennedy insisted on attending and delivered an impromptu speech that delivered news of King's death to the crowd.
Acknowledging the audience's emotions, Kennedy referred to his own grief at the murder of his brother, President John F. Kennedy and, quoting a passage from the play "Agamemnon" (in translation), said: "My favorite poet was Aeschylus. And he once wrote: 'Even in our sleep, pain which cannot forget falls drop by drop upon the heart, until in our own despair, against our will, comes wisdom through the awful grace of God.' What we need in the United States is not division; what we need in the United States is not hatred; what we need in the United States is not violence and lawlessness; but is love and wisdom, and compassion toward one another, and a feeling of justice toward those who still suffer within our country, whether they be white or whether they be black... Let us dedicate ourselves to what the Greeks wrote so many years ago: to tame the savageness of man and make gentle the life of this world." The quotation from Aeschylus was later inscribed on a memorial at the gravesite of Robert Kennedy following his own assassination.

</doc>
<doc id="2024" url="https://en.wikipedia.org/wiki?curid=2024" title="Amber Road">
Amber Road

The Amber Road was an ancient trade route for the transfer of amber from coastal areas of the North Sea and the Baltic Sea to the Mediterranean Sea. Prehistoric trade routes between Northern and Southern Europe were defined by the amber trade. As an important raw material, sometimes dubbed "the gold of the north", amber was transported from the North Sea and Baltic Sea coasts overland by way of the Vistula and Dnieper rivers to Italy, Greece, the Black Sea, Syria and Egypt thousands of years ago, and long after.
Antiquity.
From at least the sixteenth century BC amber was moved from Northern Europe to the Mediterranean area. The breast ornament of the Egyptian pharaoh Tutankhamen (ca. 1333-1324 BC) contains large Baltic amber beads Heinrich Schliemann found Baltic amber beads at Mycenae, as shown by spectroscopic investigation. The quantity of amber in the Royal Tomb of Qatna, Syria, is unparalleled for known second millennium BC sites in the Levant and the Ancient Near East. Amber was sent from the North Sea to the temple of Apollo at Delphi as an offering. From the Black Sea, trade could continue to Asia along the Silk Road, another ancient trade route. In Roman times, a main route ran south from the Baltic coast through the land of the Boii (modern Czech Republic and Slovakia) to the head of the Adriatic Sea (modern Gulf of Venice).
The Old Prussian towns of Kaup and Truso on the Baltic were the starting points of the route to the south. In Scandinavia the amber road probably gave rise to the thriving Nordic Bronze Age culture, bringing influences from the Mediterranean Sea to the northernmost countries of Europe.
Sometimes the Kaliningrad Oblast is called the Янтарный край, which means "the amber area".
Overview of known amber finding places in Europe.
Amber roads connect amber finding locations to customer sites in Europe, in the Middle East regions and in the Far East.
Overview of known amber roads by country.
Central Europe.
The shortest (and possibly oldest) road avoids alpine areas and led from the Baltic coastline (nowadays Lithuania and Poland), through Biskupin and what is now Wrocław, passed the Moravian Gate, followed the river Morava, crossed the Danube near Carnuntum in the Noricum Province, headed southwest past Poetovio, Celeia, Emona, Nauportus, and reached Aquileia at the Adriatic coast. One of the oldest directions of the last stage of the Amber Road to the south of the Danube, noted in the myth about the Argonauts, used the Sava and Kupa rivers, ending with a short continental road from Nauportus to "Tarsatica" (Trsat, Rijeka) on the coast of the Adriatic.
Germany.
Several roads connected the North Sea and Baltic Sea, especially the city of Hamburg to the Brenner Pass, proceeding southwards to Brindisi (nowadays Italy) and Ambracia (nowadays Greece).
Switzerland.
The Swiss region indicates a number of alpine roads, concentrating around the capital city Bern and probably originating from the borders of the Rhône River and the Rhine.
The Netherlands.
A small section, including Baarn, Barneveld, Amersfoort and Amerongen, connected the North Sea with the Lower Rhine.
Belgium.
A small section led southwards from Antwerp and Bruges to the towns Braine-l’Alleud and Braine-le-Comte, both originally named "Brennia-Brenna". The route continued by following the Meuse River towards Bern in Switzerland.
France.
Three routes may be identified leading from an amber finding region or delta at the mouth of the River Openia towards Bresse and Bern, crossing the Alps to Switzerland and Italy.
Southern France and Spain.
Routes connecting amber finding locations at Ambares (near Bordeaux), leading to Béarn and the Pyrenees. Routes connecting the amber finding locations in northern Spain and in the Pyrenees were a trading route to the Mediterranean Sea.
Modern use of "Amber Road".
There is a tourist route stretching along the Baltic coast from Kaliningrad to Latvia called „Amber Road“.
„Amber Road“ objects are:
In Poland a north-south motorway A1 is officially named Amber Highway.

</doc>
<doc id="2025" url="https://en.wikipedia.org/wiki?curid=2025" title="Crandall University">
Crandall University

Crandall University is a small Christian Liberal Arts university located in Moncton, New Brunswick, Canada. Crandall is operated by the Convention of Atlantic Baptist Churches.
Charity.
Crandall University 106736150RR0001 was registered as a charitable organization in Canada on January 1, 1967. The primary areas in which the charity is now carrying on programs to achieve its charitable purposes, ranked according to the percentage of time and resources devoted to each program area follow:
The charity carried on charitable programs to further its charitable purpose(s) (as defined in its governing documents) this fiscal period:
Library and archives.
Crandall University houses the Baptist Heritage Center whose 300 artifacts preserve the material history of Atlantic Baptists, the Convention of Atlantic Baptist Churches, and its predecessor organizations. The collection and archives includes objects used in worship services, furniture, musical instruments, church building architecture pictures and printed material.
History.
The school was founded in 1949 under the name United Baptist Bible Training School (UBBTS), and served as both a secondary school and a Bible school. Over two decades, the focus of the school gradually shifted toward post-secondary programs. In 1968, UBBTS became a Bible and junior Christian liberal arts college, and in 1970 the name was changed to Atlantic Baptist College (ABC). A sustained campaign to expand the school's faculty and improve the level of education resulted in ABC being able to grant full Bachelor of Arts degrees in 1983. Its campus at this time was located along the Salisbury Road, west of Moncton's central business district.
The institution moved to a new campus constructed on the Gorge Road, north of the central business district, in 1996. The name was also changed to Atlantic Baptist University, a reflection of expanded student enrollment and academic accreditation. In 2003, the ABU sports teams adopted the name "The Blue Tide". The institution was the first, and thus far only, English university in Moncton. The "Atlantic Baptist University Act" was passed by the Legislative Assembly of New Brunswick in 2008.
On August 21, 2009 it was announced that the institution had changed its name to Crandall University in honour of Rev. Joseph Crandall, a pioneering Baptist minister in the maritime region. In conjunction with the University name change, Crandall Athletics took on a new identity as "The Crandall Chargers."
In 2012, Crandall University came under public scrutiny for receiving municipal funds regardless of having an anti-gay hiring policy.
Controversy.
The University has been criticized for accepting public money (municipal, provincial and federal) to fund programs and expansions to the campus but maintaining a hiring policy which would prohibit gay faculty. A year after the controversy erupted, the University opted to not apply for $150,000 in public funding that it had received annually in order to avoid changing its hiring policy.

</doc>
<doc id="2027" url="https://en.wikipedia.org/wiki?curid=2027" title="Andrew Wiles">
Andrew Wiles

Sir Andrew John Wiles (born 11 April 1953) is a British mathematician and a Royal Society Research Professor at the University of Oxford, specialising in number theory. He is most notable for proving Fermat's Last Theorem.
Early life.
Wiles was born in 1953 in Cambridge, England, the son of Maurice Frank Wiles (1923–2005), the Regius Professor of Divinity at the University of Oxford, and Patricia Wiles (née Mowll). His father worked as the Chaplain at Ridley Hall, Cambridge, for the years 1952–55. Wiles attended King's College School, Cambridge, and The Leys School, Cambridge.
Wiles states that he came across Fermat's Last Theorem on his way home from school when he was 10 years old. He stopped by his local library where he found a book about the theorem. Fascinated by the existence of a theorem that was so easy to state that he, a ten-year-old, could understand it, but nobody had proven it, he decided to be the first person to prove it. However, he soon realised that his knowledge was too limited, so he abandoned his childhood dream, until it was brought back to his attention at the age of 33 by Ken Ribet's 1986 proof of the epsilon conjecture, which Gerhard Frey had previously linked to Fermat's famous equation.
Mathematical career.
Wiles earned his bachelor's degree in mathematics in 1974 at Merton College, Oxford, and a PhD in 1980 at Clare College, Cambridge. After a stay at the Institute for Advanced Study in New Jersey in 1981, Wiles became a professor at Princeton University. In 1985–86, Wiles was a Guggenheim Fellow at the Institut des Hautes Études Scientifiques near Paris and at the École Normale Supérieure. From 1988 to 1990, Wiles was a Royal Society Research Professor at the University of Oxford, and then he returned to Princeton. He rejoined Oxford in 2011 as Royal Society Research Professor.
Wiles's graduate research was guided by John Coates beginning in the summer of 1975. Together these colleagues worked on the arithmetic of elliptic curves with complex multiplication by the methods of Iwasawa theory. He further worked with Barry Mazur on the main conjecture of Iwasawa theory over the rational numbers, and soon afterward, he generalised this result to totally real fields.
The proof of Fermat's Last Theorem.
Starting in the mid 1986, based on successive progress of the previous few years of Gerhard Frey, Jean-Pierre Serre and Ken Ribet, it became clear that Fermat's Last Theorem could be proven as a corollary of a limited form of the modularity theorem (unproven at the time and then known as the "Taniyama–Shimura-Weil conjecture"). The modularity theorem involved elliptic curves, which was also Wiles' own specialist area.
The conjecture was seen by contemporary mathematicians as important, but extraordinarily difficult or perhaps impossible to prove. For example, Wiles' ex-supervisor John Coates states that it seemed "impossible to actually prove", and Ken Ribet considered himself "one of the vast majority of people who believed was completely inaccessible", adding that "Andrew Wiles was probably one of the few people on earth who had the audacity to dream that you can actually go and prove ."
Despite this, Wiles, who had a childhood fascination with Fermat's Last Theorem, decided to undertake the challenge of proving the conjecture at least to the extent needed for Frey's curve. He dedicated all of his research time to this problem for over 6 years in near-total secrecy, covering up his efforts by releasing prior work in small segments as separate papers and confiding only in his wife. In 1993, he presented his proof to the public for the first time at a conference in Cambridge. In August 1993 it was discovered that the proof contained a flaw in one area. Wiles tried and failed for over a year to repair his proof. According to Wiles, the crucial idea for circumventing, rather than closing this area, came to him on 19 September 1994 when he was on the verge of giving up. Together with his former student Richard Taylor, he published a second paper which circumvented the problem and thus completed the proof. Both papers were published in 1995 in a special volume of the "Annals of Mathematics".
Recognition by the media.
His proof of Fermat's Last Theorem has stood up to the scrutiny of the world's other mathematical experts. Wiles was interviewed for an episode of the BBC documentary series "Horizon" that focused on Fermat's Last Theorem. This was renamed "The Proof", and it was made an episode of the Public Broadcasting Service's science television series "Nova". His work and life are also mentioned in great detail in Simon Singh's popular book, Fermat's Last Theorem. He has been a foreign member of the US National Academy of Sciences since 1996.
Awards and honours.
Wiles has been awarded a number of major prizes in mathematics and science:
Wiles nomination for election to the Royal Society reads:

</doc>
<doc id="2028" url="https://en.wikipedia.org/wiki?curid=2028" title="Ambient">
Ambient

Ambient or Ambiance may refer to:

</doc>
<doc id="2029" url="https://en.wikipedia.org/wiki?curid=2029" title="Anne Brontë">
Anne Brontë

Anne Brontë (, "commonly" ; 17 January 1820 – 28 May 1849) was an English novelist and poet, the youngest member of the Brontë literary family.
The daughter of Patrick Brontë, a poor Irish clergyman in the Church of England, Anne Brontë lived most of her life with her family at the parish of Haworth on the Yorkshire moors. She also attended a boarding school in Mirfield between 1836 and 1837. At 19 she left Haworth and worked as a governess between 1839 and 1845. After leaving her teaching position, she fulfilled her literary ambitions. She published a volume of poetry with her sisters ("Poems by Currer, Ellis, and Acton Bell", 1846) and two novels. "Agnes Grey", based upon her experiences as a governess, was published in 1847. Her second and last novel, "The Tenant of Wildfell Hall", which is considered to be one of the first sustained feminist novels, appeared in 1848. Like her poems, both her novels were first published under the masculine penname of Acton Bell. Anne's life was cut short when she died of what is now suspected to be pulmonary tuberculosis at the age of 29.
Partly because the re-publication of "The Tenant of Wildfell Hall" was prevented by Charlotte Brontë after Anne's death, she is not as well known as her sisters. Charlotte wrote four novels including "Jane Eyre" and Emily wrote "Wuthering Heights".
However, her novels, like those of her sisters, have become classics of English literature.
Family background.
Anne's father, Patrick Brontë (1777–1861), was born in a two-room cottage in Emdale, Loughbrickland, County Down, Ireland. He was the oldest of ten children born to Hugh Brunty and Eleanor McCrory, poor Irish peasant farmers. The family surname "mac Aedh Ó Proinntigh" was Anglicised as Prunty or Brunty. Struggling against poverty, Patrick learned to read and write and from 1798 taught others. In 1802, at 25, he won a place to study theology at St. John's College, Cambridge where he changed his name, Brunty, to the more distinguished sounding Brontë. In 1807 he was ordained in the priesthood in the Church of England. He served as a curate first in Essex and latterly in Wellington, Shropshire. In 1810, he published his first poem "Winter Evening Thoughts" in a local newspaper, followed in 1811 by a collection of moral verse, "Cottage Poems". In 1811, he became vicar of St. Peter's Church in Hartshead in Yorkshire. The following year he was appointed an examiner in Classics at Woodhouse Grove School, near Bradford a Wesleyan academy where, aged 35, he met his future wife, Maria Branwell, the headmaster's niece.
Anne's mother, Maria Branwell (1783–1821), was the daughter of Thomas Branwell, a successful, property-owning grocer and tea merchant in Penzance and Anne Carne, the daughter of a silversmith. The eleventh of twelve children, Maria enjoyed the benefits of belonging to a prosperous family in a small town. After the death of her parents within a year of each other, Maria went to help her aunt administer the housekeeping functions of the school. A tiny, neat woman aged 30, she was well read and intelligent. Her strong Methodist faith attracted Patrick Brontë because his own leanings were similar.
Though from considerably different backgrounds, within three months Patrick Brontë and Maria Branwell were married on 29 December 1812. Their first child, Maria (1814–1825), was born after they moved to Hartshead. In 1815, Patrick was appointed curate of the chapel in Thornton, near Bradford; a second daughter, Elizabeth (1815–1825), was born shortly after. Four more children followed: Charlotte, (1816–1855), Patrick Branwell (1817–1848), Emily, (1818–1848) and Anne (1820–1849).
Early life.
Anne, the youngest member of the Brontë children, was born on 17 January 1820, at 74 Market Street in Thornton where her father was curate and she was baptised there on 25 March 1820. Anne's father was appointed to the perpetual curacy in Haworth, a small town seven miles (11 km) away. In April 1820, the Brontës moved into the five-roomed Haworth Parsonage which became their home for the rest of their lives.
Anne was barely a year old when her mother became ill of what is believed to have been uterine cancer. Maria Branwell died on 15 September 1821. In order to provide a mother for his children, Patrick tried to remarry, but without success. Maria's sister, Elizabeth Branwell (1776–1842), moved to the parsonage, initially to nurse her dying sister, but she spent the rest of her life there raising the children. She did it from a sense of duty, but she was a stern woman who expected respect, rather than love. There was little affection between her and the older children, but Anne, according to tradition, was her favourite.
In Elizabeth Gaskell's biography, Anne's father remembered her as precocious, reporting that once, when she was four years old, in reply to his question about what a child most wanted, she answered: "age and experience".
In summer 1824, Patrick sent Maria, Elizabeth, Charlotte and Emily to Crofton Hall in Crofton, West Yorkshire, and subsequently to the Clergy Daughter's School at Cowan Bridge in Lancashire. When his eldest daughters died of consumption in 1825, Maria on 6 May and Elizabeth on 15 June, Charlotte and Emily were immediately brought home. The unexpected deaths distressed the family so much that Patrick could not face sending them away again. For the next five years, they were educated at home, largely by their father and aunt. The children made little attempt to mix with others outside the parsonage, but relied on each other for friendship and companionship. The bleak moors surrounding Haworth became their playground. Anne shared a room with her aunt, they were close which may have influenced Anne's personality and religious beliefs. Like her sister Charlotte, Anne grew up devoted to the Christian faith.
Education.
Anne's studies at home included music and drawing. Anne, Emily and Branwell had piano lessons from the Keighley church organist. They had art lessons from John Bradley of Keighley and all drew with some skill. Their aunt tried to teach the girls how to run a household, but their minds were more inclined to literature. Their father's well-stocked library was a source of knowledge. They read the Bible, Homer, Virgil, Shakespeare, Milton, Byron, Scott, and many others, they examined articles from Blackwood's Edinburgh Magazine, Fraser's Magazine, and The Edinburgh Review and read history, geography and biographies.
Reading fed the children's imagination. Their creativity soared after their father presented Branwell with a set of toy soldiers in June 1826. They gave the soldiers names and developed their characters, which they called the "Twelves". This led to the creation of an imaginary world: the African kingdom of "Angria" which was illustrated with maps and watercolour renderings. The children devised plots about the inhabitants of Angria and its capital city, "Glass Town", later called Verreopolis or Verdopolis.
The fantasy worlds and kingdoms gradually acquired the characteristics of real world—sovereigns, armies, heroes, outlaws, fugitives, inns, schools and publishers. The characters and lands created by the children had newspapers, magazines and chronicles which were written in extremely tiny books, with writing so small it was difficult to read without a magnifying glass. These creations and writings were an apprenticeship for their later, literary talents.
Juvenilia.
Around 1831, when Anne was eleven, she and Emily broke away from Charlotte and Branwell to create and develop their own fantasy world, "Gondal". Anne was particularly close to Emily especially after Charlotte's departure for Roe Head School, in January 1831. When Charlotte's friend Ellen Nussey visited Haworth in 1833, she reported that Emily and Anne were "like twins", "inseparable companions". She described Anne:
"Anne, dear gentle Anne was quite different in appearance from the others, and she was her aunt's favourite. Her hair was a very pretty light brown, and fell on her neck in graceful curls. She had lovely violet-blue eyes; fine pencilled eyebrows and a clear almost transparent complexion. She still pursued her studies and especially her sewing, under the surveillance of her aunt."
Anne took lessons from Charlotte, after she returned from Roe Head. Charlotte returned to Roe Head as a teacher on 29 July 1835 accompanied by Emily as a pupil; her tuition largely financed by Charlotte's teaching. Within a few months, Emily unable to adapt to life at school, was physically ill from homesickness. She was withdrawn from school by October, and replaced by Anne.
Aged 15, it was Anne's first time away from home, and she made few friends at Roe Head. She was quiet and hard working, and determined to stay and get the education she needed to support herself. She stayed for two years, winning a good-conduct medal in December 1836, and returning home only during Christmas and summer holidays. Anne and Charlotte do not appear to have been close while at Roe Head (Charlotte's letters almost never mention her) but Charlotte was concerned about her sister's health. Sometime before December 1837, Anne became seriously ill with gastritis and underwent a religious crisis. A Moravian minister was called to see her several times during her illness, suggesting her distress was caused, in part, by conflict with the local Anglican clergy. Charlotte wrote to her father who took Anne home where she remained while she recovered.
Employment at Blake Hall.
In 1839, a year after leaving the school and aged 19, she was seeking a teaching position. As the daughter of a poor clergyman, she needed to earn a living. Her father had no private income and the parsonage would revert to the church on his death. Teaching or working as governess for a family were among the few options available to poor but educated women. In April 1839, Anne started work as a governess for the Ingham family at Blake Hall, near Mirfield.
The children in her charge were spoilt and wild, persistently disobedient and tormented her. She had great difficulty controlling them, and little success in instilling any education. She was not empowered to inflict punishment, and when she complained about their behaviour received no support, but was criticised for not being capable. The Inghams, dissatisfied with their children's progress, dismissed Anne. She returned home at Christmas, 1839, joining Charlotte and Emily, who had left their positions, and Branwell. The episode at Blake Hall was so traumatic that she reproduced it in almost perfect detail in her novel, "Agnes Grey".
William Weightman.
On her return to Haworth, she met William Weightman (1814–1842), her father's new curate, who started work in the parish in August 1839. Aged 25, he had obtained a two-year licentiate in theology from the University of Durham. He was welcome at the parsonage. Her acquaintance with him parallels her writing a number of poems, which may suggest she fell in love with him although there is disagreement over this possibility. Little evidence exists beyond a teasing anecdote of Charlotte's to Ellen Nussey in January 1842.
The source of "Agnes Grey"'s renewed interest in poetry is, however, the curate to whom she is attracted. William Weightman aroused much curiosity. It seems clear he was a good-looking, engaging young man, whose easy humour and kindness towards the sisters made a considerable impression. It is such a character that she portrays in Edward Weston, and that her heroine Agnes Grey finds deeply appealing.
If Anne formed an attachment to Weightman it does not imply that he was attracted to her. It is possible that Weightman was no more aware of her, her sisters or their friend Ellen Nussey. Nor does it imply that Anne believed him to be interested in her. If anything, her poems suggest the opposite–they speak of quietly experienced but intensely felt emotions, hidden from others, without any indication of being requited. It is possible that an initially mild attraction to Weightman assumed increasing importance to Anne over time, in the absence of other opportunities for love, marriage and children.
Anne would have seen Weightman on her holidays at home, particularly during the summer of 1842 when her sisters were away. Weightman died of cholera in the same year. Anne expressed her grief for his death in her poem "I will not mourn thee, lovely one", in which she called him "our darling".
Governess.
Anne obtained a second post as governess to the children of the Reverend Edmund Robinson and his wife Lydia, at Thorp Green Hall, a comfortable country house near York. Anne was employed at Thorp Green Hall from 1840 to 1845. The house appeared as Horton Lodge in her novel "Agnes Grey". Anne had four pupils: Lydia, aged 15, Elizabeth, aged 13, Mary, aged 12, and Edmund, aged 8. Initially, she encountered similar problems as she had experienced at Blake Hall. Anne missed her home and family, commenting in a diary paper in 1841 that she did not like her situation and wished to leave it. Her quiet, gentle disposition did not help. However, despite her outwardly placid appearance, Anne was determined and with experience, made a success of her position, becoming well liked by her employers. Her charges, the Robinson girls, became lifelong friends.
For the next five years, Anne spent no more than five or six weeks a year with her family, during holidays at Christmas and in June. The rest of her time was spent with the Robinsons at Thorp Green. She was obliged to accompany them on annual holidays to Scarborough. Between 1840 and 1844, Anne spent around five weeks each summer at the coastal town and loved the place. A number of locations in Scarborough were the setting for "Agnes Grey"'s final scenes and for Linden-Car village in "The Tenant of Wildfell Hall".
Whilst working for the Robinsons, Anne and her sisters considered the possibility of setting up a school. Various locations including the parsonage were considered. The project never materialised and Anne chose to return to Thorp Green. She came home on the death of her aunt in early November 1842 while her sisters were in Brussels. Elizabeth Branwell left a £350 legacy (£ as of 2016) for each of her nieces.
It was at the Long Plantation at Thorp Green in 1842 that Anne wrote her three-verse poem "Lines Composed in a Wood on a Windy Day", which was published in 1846 under her pen-name of Acton Bell.
Anne returned to Thorp Green in January 1843 where she secured a position for Branwell. He was to take over as tutor to the Robinsons' son, Edmund, who was growing too old to be in Anne's care. Branwell did not live in the house as Anne did. Anne's vaunted calm appears to have been the result of hard-fought battles, balancing deeply felt emotions with careful thought, a sense of responsibility and resolute determination. All three Brontë sisters worked as governesses or teachers, and all experienced problems controlling their charges, gaining support from their employers, and coping with homesickness—but Anne was the only one who persevered and made a success of her work.
Back at the parsonage.
Anne and Branwell taught at Thorp Green for the next three years. Branwell entered into a secret relationship with his employer's wife, Lydia Robinson. When Anne and her brother returned home for the holidays in June 1846, she resigned her position. While Anne gave no reason for leaving Thorp Green, it is thought she wanted to leave on becoming aware of the relationship between her brother and Mrs Robinson. Branwell was dismissed when his employer found out about the relationship. Anne retained close ties to Elizabeth and Mary Robinson, exchanging letters even after Branwell's disgrace. The Robinson sisters came to visit Anne in December 1848.
Anne took Emily to visit some of the places she had come to know and love in the five years spent with the Robinsons. A plan to visit Scarborough fell through and instead the sisters went to York where Anne showed her sister York Minster.
A book of poems.
During the summer of 1845, the Brontës were at home with their father. None had any immediate prospect of employment. Charlotte came across Emily's poems which had been shared only with Anne, her partner in the world of Gondal. Charlotte proposed that they be published. Anne revealed her own poems but Charlotte's reaction was characteristically patronising: "I thought that these verses too had a sweet sincere pathos of their own". Eventually the sisters reached an agreement. They told neither Branwell, nor their father, nor their friends about what they were doing. Anne and Emily each contributed 21 poems and Charlotte contributed 19 and with Aunt Branwell's money, they paid to have the collection published.
Afraid their work would be judged differently if they revealed they were women, the book appeared using male pen names, the initials of which were the same as their own. Charlotte, Emily, and Anne respectively became Currer, Ellis, and Acton Bell. "Poems by Currer, Ellis, and Acton Bell" was available for sale in May 1846. The cost of publication was about three-quarters of Anne's salary at Thorp Green. On 7 May 1846, the first three copies were delivered to Haworth Parsonage. It achieved three somewhat favourable reviews, but was a dismal failure, with only two copies being sold in the first year. Anne, however, found a market for her more recent poetry. The "Leeds Intelligencer" and "Fraser's Magazine" published her poem "The Narrow Way" under her pseudonym, Acton Bell in December 1848. Four months earlier, in August, Fraser's Magazine had published her poem "The Three Guides".
Novels.
"Agnes Grey".
Even before the fate of the book of poems became apparent, the sisters began work on their first novels. Charlotte wrote "The Professor", Emily "Wuthering Heights", and Anne "Agnes Grey". By July 1846, a package with the three manuscripts was making the rounds of London publishers.
After a number of rejections, Emily's "Wuthering Heights" and Anne's "Agnes Grey" were accepted by a publisher, but Charlotte's novel was rejected by every publisher to whom it was sent. Charlotte was not long in completing her second novel, "Jane Eyre", and it was immediately accepted by Smith, Elder & Co. and was the first to appear in print. While Anne and Emily's novels 'lingered in the press', Charlotte's second novel was an immediate and resounding success. Anne and Emily were obliged to pay fifty pounds to help meet their publishing costs. Their publisher, urged on by the success of "Jane Eyre", published Anne and Emily's novels in December 1847. They sold well, but "Agnes Grey" was outshone by Emily's more dramatic "Wuthering Heights".
"The Tenant of Wildfell Hall".
Anne's second novel, "The Tenant of Wildfell Hall", was published in the last week of June 1848. It was an instant, phenomenal success; within six weeks it was sold out.
"The Tenant of Wildfell Hall" is perhaps amongst the most shocking of contemporary Victorian novels. In seeking to present the truth in literature, Anne's depiction of alcoholism and debauchery was profoundly disturbing to 19th-century sensibilities. Helen Graham, the tenant of the title, intrigues Gilbert Markham and gradually she reveals her past as an artist and wife of the dissipated Arthur Huntingdon. The book's brilliance lies in its revelation of the position of women at the time, and its multi-layered plot.
It is easy today to underestimate the extent to which the novel challenged existing social and legal structures. May Sinclair, in 1913, said that the slamming of Helen Huntingdon's bedroom door against her husband reverberated throughout Victorian England. Anne's heroine eventually left her husband to protect their young son from his influence. She supported herself and her son by painting while living in hiding, fearful of discovery. In doing so, she violated not only social conventions, but English law. Until 1870, when the Married Women's Property Act was passed, a married woman had no independent legal existence apart from her husband; could not own property, sue for divorce, or control custody of her children. If she attempted to live apart, her husband had the right to reclaim her. If she took their child, she was liable for kidnapping. By living on her own income she was held to be stealing her husband's property, since any property she held or income she made was legally his.
London visit.
In July 1848, to dispel the rumour that the "Bell brothers" were all the same person, Charlotte and Anne went to London to reveal their identities to the publisher George Smith. The women spent several days in his company. Many years after Anne's death, he wrote in the Cornhill Magazine his impressions of her, describing her as:
"...a gentle, quiet, rather subdued person, by no means pretty, yet of a pleasing appearance. Her manner was curiously expressive of a wish for protection and encouragement, a kind of constant appeal which invited sympathy."
In the second edition of "The Tenant of Wildfell Hall", which appeared in August 1848, Anne clearly stated her intentions in writing it. She presented a forceful rebuttal to critics (Charlotte was among them) who considered her portrayal of Huntingdon overly graphic and disturbing.
Anne sharply castigated reviewers who speculated on the sex of the authors, and the appropriateness of their writing, in words that do little to reinforce the stereotype of Anne as meek and gentle.
The increasing popularity of the Bells' work led to renewed interest in the "Poems by Currer, Ellis, and Acton Bell", originally published by Aylott and Jones. The remaining print run was bought by Smith and Elder, and reissued under new covers in November 1848. It still sold poorly.
Family tragedies.
Although Anne and her sisters were only in their late twenties, a highly successful literary career appeared a certainty for them. However, an impending tragedy was to engulf the family. Within the next ten months, three of the siblings, including Anne, would be dead.
Branwell's health had deteriorated over two years, but its seriousness was disguised by his persistent drunkenness. He died on the morning of 24 September 1848. His sudden death came as a shock to the family. He was aged 31. The cause was recorded as chronic bronchitis – marasmus; though it is now believed he was suffering from tuberculosis.
The family had suffered from coughs and colds during the winter of 1848, and Emily next became severely ill. She deteriorated rapidly over two months, persistently refusing all medical aid until the morning of 19 December, when, being very weak, she declared: "if you will send for a doctor, I will see him now". It was, however, far too late. At about two o'clock that afternoon, after a hard, short conflict in which she struggled desperately to hang on to life, she died, aged 30.
Emily's death deeply affected Anne, and her grief undermined her physical health. Over Christmas, Anne caught influenza. Her symptoms intensified, and her father sent for a Leeds physician in early January. The doctor diagnosed her condition as consumption (tuberculosis) and intimated that it was quite advanced, leaving little hope of recovery. Anne met the news with characteristic determination and self-control.
Unlike Emily, Anne took all the recommended medicines and followed the advice she was given. That same month she wrote her last poem, "A dreadful darkness closes in", in which she deals with being terminally ill. Her health fluctuated as the months passed, but she progressively grew thinner and weaker.
Death.
In February 1849, Anne seemed somewhat better. She decided to make a return visit to Scarborough in the hope that the change of location and fresh sea air might initiate a recovery. On 24 May 1849, Anne said her goodbyes to her father and the servants at Haworth, and set off for Scarborough with Charlotte and Ellen Nussey. En route, they spent a day and a night in York, where, escorting Anne around in a wheelchair, they did some shopping, and at Anne's request, visited York Minster. However, it was clear that Anne had little strength left.
On Sunday, 27 May, Anne asked Charlotte whether it would be easier if she returned home to die instead of remaining in Scarborough. A doctor, consulted the next day, indicated that death was close. Anne received the news quietly. She expressed her love and concern for Ellen and Charlotte, and seeing Charlotte's distress, whispered to her to "take courage". Conscious and calm, Anne died at about two o'clock in the afternoon, Monday, 28 May 1849.
Over the following days, Charlotte made the decision to "lay the flower where it had fallen". Anne was buried, not in Haworth with the rest of her family, but in Scarborough. The funeral was held on Wednesday 30 May which did not allow time for Patrick Brontë to make the journey, had he wished to do so. The former schoolmistress at Roe Head, Miss Wooler, was in Scarborough and she was the only other mourner at Anne's funeral. She was buried in St Mary's churchyard, beneath the castle walls, overlooking the bay. Charlotte commissioned a stone to be placed over her grave, with the simple inscription "Here lie the remains of Anne Brontë, daughter of the Revd P. Brontë, Incumbent of Haworth, Yorkshire. She died Aged 28 May 28th 1849". When Charlotte visited the grave three years later, she discovered multiple errors on the headstone, and thus it was refaced. However, Anne's age at death was still written as 28 when, in fact, she was 29 when she died.
In 2011, the Brontë Society installed a new plaque at Anne Brontë's grave. Due to weathering and erosion, the original gravestone had become illegible at places and could not be restored. The original stone was left undisturbed, while the new plaque, laid horizontally, interpreted the fading words of the original, and also added a correction to remaining error on the headstone (Anne's age at death). In April 2013, the Brontë Society held a dedication and blessing service at the gravesite to mark the installation of the new plaque.
Reputation.
A year after Anne's death, further editions of her novels were reprinted but Charlotte prevented re-publication of "The Tenant of Wildfell Hall". In 1850, Charlotte wrote ""Wildfell Hall" it hardly appears to me desirable to preserve. The choice of subject in that work is a mistake, it was too little consonant with the character, tastes and ideas of the gentle, retiring inexperienced writer." Subsequent critics paid less attention to Anne's work, and those such as Lane dismissed her as a "a Brontë without genius" and gave her output little consideration. However, in recent years, with increasing critical interest in female authors, her life has been re-examined and her work re-evaluated. Biographies by Winifred Gérin (1959), Elizabeth Langland (1989) and Edward Chitham (1991) as well as Juliet Barker's group biography, "The Brontës" (1994; revised edition 2000) and work by critics such as Inga-Stina Ewbank, Marianne Thormählen, Laura C Berry, Jan B Gordon and Juliet McMaster, has led to her acceptance, not as a minor Brontë, but as a major literary figure in her own right. Sally McDonald of the Brontë Society said in 2013, "In some ways though she is now viewed as the most radical of the sisters, writing about tough subjects such as women's need to maintain independence and how alcoholism can tear a family apart."

</doc>
<doc id="2030" url="https://en.wikipedia.org/wiki?curid=2030" title="Augustine of Hippo">
Augustine of Hippo

Augustine of Hippo ( or ; ; 13 November 354 – 28 August 430), also known as Saint Augustine, Saint Austin, or Blessed Augustine, was an early Christian theologian and philosopher whose writings influenced the development of Western Christianity and Western philosophy. He was the bishop of Hippo Regius (modern-day Annaba, Algeria), located in Numidia (Roman province of Africa). He is viewed as one of the most important Church Fathers in Western Christianity for his writings in the Patristic Era. Among his most important works are "The City of God" and "Confessions."
According to his contemporary, Jerome, Augustine "established anew the ancient Faith." In his early years, he was heavily influenced by Manichaeism and afterward by the neo-Platonism of Plotinus. After his baptism and conversion to Christianity in 387, Augustine developed his own approach to philosophy and theology, accommodating a variety of methods and perspectives. Believing that the grace of Christ was indispensable to human freedom, he helped formulate the doctrine of original sin and made seminal contributions to the development of just war theory.
When the Western Roman Empire began to disintegrate, Augustine developed the concept of the pre-Schism Catholic Church as a spiritual City of God, distinct from the material Earthly City. His thoughts profoundly influenced the medieval worldview. The segment of the Church that adhered to the concept of the Trinity as defined by the Council of Nicaea and the Council of Constantinople closely identified with Augustine's "City of God."
In the Catholic Church and the Anglican Communion, he is a saint, a preeminent Doctor of the Church, and the patron of the Augustinians. His memorial is celebrated on 28 August, the day of his death. He is the patron saint of brewers, printers, theologians, the alleviation of sore eyes, and a number of cities and dioceses. Many Protestants, especially Calvinists, consider him to be one of the theological fathers of the Protestant Reformation due to his teachings on salvation and divine grace.
In the East, some of his teachings are disputed and have in the 20th century in particular come under attack by such theologians as Father John Romanides. But other theologians and figures of the Orthodox Church have shown significant appropriation of his writings, chiefly Father Georges Florovsky. The most controversial doctrine surrounding his name is the filioque, which has been rejected by the Orthodox Church. Other disputed teachings include his views on original sin, the doctrine of grace, and predestination. Nevertheless, though considered to be mistaken on some points, he is still considered a saint, and has even had influence on some Eastern Church Fathers, most notably Saint Gregory Palamas. In the Orthodox Church his feast day is celebrated on 28 August and carries the title of "Blessed".
Life.
Childhood and education.
Augustine was born in 354 in the municipium of Thagaste (now Souk Ahras, Algeria) in Roman Africa. His mother, Monica, was a devout Christian; his father Patricius was a Pagan who converted to Christianity on his deathbed. Scholars believe that Augustine's ancestors included Latins and Phoenicians. He considered himself to be Punic.
Augustine's family name, Aurelius, suggests that his father's ancestors were freedmen of the "gens Aurelia" given full Roman citizenship by the Edict of Caracalla in 212. Augustine's family had been Roman, from a legal standpoint, for at least a century when he was born. It is assumed that his mother, Monica, was of Berber origin, on the basis of her name, but as his family were "honestiores", an upper class of citizens known as honorable men, Augustine's first language is likely to have been Latin.
At the age of 11, Augustine was sent to school at Madaurus (now M'Daourouch), a small Numidian city about south of Thagaste. There he became familiar with Latin literature, as well as pagan beliefs and practices. His first insight into the nature of sin occurred when he and a number of friends stole fruit they did not want from a neighborhood garden. He tells this story in his autobiography, "The Confessions". He remembers that he did not steal the fruit because he was hungry, but because "it was not permitted." His very nature, he says, was flawed. 'It was foul, and I loved it. I loved my own error—not that for which I erred, but the error itself." From this incident he concluded the human person is naturally inclined to sin, and in need of the grace of Christ.
At the age of 17, through the generosity of his fellow citizen Romanianus, Augustine went to Carthage to continue his education in rhetoric. It was while he was a student in Carthage that he read Cicero's dialogue "Hortensius" (now lost), which he described as leaving a lasting impression and sparking his interest in philosophy. Although raised as a Christian, Augustine left the church to follow the Manichaean religion, much to his mother's despair. As a youth Augustine lived a hedonistic lifestyle for a time, associating with young men who boasted of their sexual exploits. The need to gain their acceptance forced inexperienced boys like Augustine to seek or make up stories about sexual experiences. It was during this period that he uttered his famous prayer, "Grant me chastity and continence, but not yet."
At about the age of 19, Augustine began an affair with a young woman in Carthage. Though his mother wanted him to marry a person of his class, the woman remained his lover for over fifteen years and gave birth to his son Adeodatus, who was viewed as extremely intelligent by his contemporaries. In 385, Augustine ended his relationship with his lover in order to prepare himself to marry a ten year old heiress. (He had to wait for two years because the legal age of marriage was twelve. By the time he was able to marry her, however, he instead decided to become a celibate priest.)
Augustine was from the beginning a brilliant student, with an eager intellectual curiosity, but he never mastered Greek —he tells us that his first Greek teacher was a brutal man who constantly beat his students, and Augustine rebelled and refused to study. By the time he realized that he needed to know Greek, it was too late; and although he acquired a smattering of the language, he was never eloquent with it. However, his mastery of Latin was another matter. He became an expert both in the eloquent use of the language and in the use of clever arguments to make his points.
Teaching rhetoric.
Augustine taught grammar at Thagaste during 373 and 374. The following year he moved to Carthage to conduct a school of rhetoric and would remain there for the next nine years. Disturbed by unruly students in Carthage, he moved to establish a school in Rome, where he believed the best and brightest rhetoricians practised, in 383. However, Augustine was disappointed with the apathetic reception. It was the custom for students to pay their fees to the professor on the last day of the term, and many students attended faithfully all term, and then did not pay. Manichaean friends introduced him to the prefect of the City of Rome, Symmachus, who had been asked by the imperial court at Milan to provide a rhetoric professor.
Augustine won the job and headed north to take his position in late 384. Thirty years old, he had won the most visible academic position in the Latin world at a time when such posts gave ready access to political careers. Although Augustine showed some fervour for Manichaeism, he was never an initiate or "elect", but an "auditor", the lowest level in the sect's hierarchy.
While still at Carthage a disappointing meeting with the Manichaean Bishop, Faustus of Mileve, a key exponent of Manichaean theology, started Augustine's scepticism of Manichaeanism. In Rome, he reportedly turned away from Manichaeanism, embracing the scepticism of the New Academy movement. Because of his education, Augustine had great rhetorical prowess and was very knowledgeable of the philosophies behind many faiths. At Milan, his mother's religiosity, Augustine's own studies in Neoplatonism, and his friend Simplicianus all urged him towards Christianity. Initially Augustine was not strongly influenced by Christianity and its ideologies, but after coming in contact with Ambrose of Milan, Augustine reevaluated himself and was forever changed.
Like Augustine, Ambrose was a master of rhetoric, but older and more experienced. Augustine was very much influenced by Ambrose, even more than by his own mother and others he admired. Augustine arrived in Milan and was immediately taken under the wing by Ambrose. Within his "Confessions", Augustine states, "That man of God received me as a father would, and welcomed my coming as a good bishop should." Soon, their relationship grew, as Augustine wrote, "And I began to love him, of course, not at the first as a teacher of the truth, for I had entirely despaired of finding that in thy Church—but as a friendly man." Augustine visited Ambrose in order to see if Ambrose was one of the greatest speakers and rhetoricians in the world. More interested in his speaking skills than the topic of speech, Augustine quickly discovered that Ambrose was a spectacular orator. Eventually, Augustine says that through the unconscious, he was led into the faith of Christianity.
Augustine's mother had followed him to Milan and arranged a marriage for which he abandoned his concubine. Although Augustine accepted this marriage, Augustine was deeply hurt by the loss of his lover. He said, "My mistress being torn from my side as an impediment to my marriage, my heart, which clave to her, was racked, and wounded, and bleeding." Augustine confessed that he was not a lover of wedlock so much as a slave of lust, so he procured another concubine since he had to wait two years until his fiancée came of age. However, his wound was not healed, even began to fester.
There is evidence that Augustine may have considered this former relationship to be equivalent to marriage. In his "Confessions", he admitted that the experience eventually produced a decreased sensitivity to pain. Augustine eventually broke off his engagement to his eleven-year-old fiancée, but never renewed his relationship with either of his concubines. Alypius of Thagaste steered Augustine away from marriage, saying that they could not live a life together in the love of wisdom if he married. Augustine looked back years later on the life at Cassiciacum, a villa outside of Milan where he gathered with his followers, and described it as "Christianae vitae otium" – the Christian life of leisure.
Christian conversion and priesthood.
In the summer of 386, at the age of 31, after having heard and been inspired and moved by the story of Ponticianus's and his friends' first reading of the life of Saint Anthony of the Desert, Augustine converted to Christianity. As Augustine later told it, his conversion was prompted by a childlike voice he heard telling him to "take up and read" (), which he took as a divine command to open the Bible and read the first thing he saw. Augustine read from Paul's Epistle to the Romans – the so-called "" section, consisting of chapters 12 through 15 – wherein Paul outlines how the Gospel transforms believers, and the believers' resulting behaviour. The specific part to which Augustine opened his Bible was Romans chapter 13, verses 13 and 14, to wit:
He later wrote an account of his conversion – his very transformation, as Paul described – in his "Confessions" (), which has since become a classic of Christian theology and a key text in the history of autobiography. This work is an outpouring of thanksgiving and penitence. Although it is written as an account of his life, the "Confessions" also talks about the nature of time, causality, free will, and other important philosophical topics. The following is taken from that work:
Ambrose baptized Augustine, along with his son Adeodatus, on Easter Vigil in 387 in Milan. A year later, in 388, Augustine completed his apology "On the Holiness of the Catholic Church". That year, also, Adeodatus and Augustine returned home to Africa. Augustine's mother Monica died at Ostia, Italy, as they prepared to embark for Africa. Upon their arrival, they began a life of aristocratic leisure at Augustine's family's property. Soon after, Adeodatus, too, died. Augustine then sold his patrimony and gave the money to the poor. The only thing he kept was the family house, which he converted into a monastic foundation for himself and a group of friends.
In 391 Augustine was ordained a priest in Hippo Regius (now Annaba), in Algeria. He became a famous preacher (more than 350 preserved sermons are believed to be authentic), and was noted for combating the Manichaean religion, to which he had formerly adhered.
In 395 he was made coadjutor Bishop of Hippo, and became full Bishop shortly thereafter, hence the name "Augustine of Hippo"; and he gave his property to the church of Thagaste. He remained in that position until his death in 430. He wrote his autobiographical "Confessions" in 397-398. His work "The City of God" was written to console his fellow Christians shortly after the Visigoths had sacked Rome in 410.
Augustine worked tirelessly in trying to convince the people of Hippo to convert to Christianity. Though he had left his monastery, he continued to lead a monastic life in the episcopal residence. He left a "regula" for his monastery that led to his designation as the "patron saint of regular clergy."
Much of Augustine's later life was recorded by his friend Possidius, bishop of Calama (present-day Guelma, Algeria), in his "Sancti Augustini Vita". Possidius admired Augustine as a man of powerful intellect and a stirring orator who took every opportunity to defend Christianity against its detractors. Possidius also described Augustine's personal traits in detail, drawing a portrait of a man who ate sparingly, worked tirelessly, despised gossip, shunned the temptations of the flesh, and exercised prudence in the financial stewardship of his see.
Death and veneration.
Shortly before Augustine's death the Vandals, a Germanic tribe that had converted to Arianism, invaded Roman Africa. The Vandals besieged Hippo in the spring of 430, when Augustine entered his final illness. According to Possidius, one of the few miracles attributed to Augustine, the healing of an ill man, took place during the siege. According to Possidius, Augustine spent his final days in prayer and repentance, requesting that the penitential Psalms of David be hung on his walls so that he could read them. He directed that the library of the church in Hippo and all the books therein should be carefully preserved. He died on 28 August 430. Shortly after his death, the Vandals lifted the siege of Hippo, but they returned not long thereafter and burned the city. They destroyed all of it but Augustine's cathedral and library, which they left untouched.
Augustine was canonized by popular acclaim, and later recognized as a Doctor of the Church in 1298 by Pope Boniface VIII. His feast day is 28 August, the day on which he died. He is considered the patron saint of brewers, printers, theologians, sore eyes, and a number of cities and dioceses.
Relics.
According to Bede's "True Martyrology", Augustine's body was later translated or moved to Cagliari, Sardinia, by the Catholic bishops expelled from North Africa by Huneric. Around 720, his remains were transported again by Peter, bishop of Pavia and uncle of the Lombard king Liutprand, to the church of San Pietro in Ciel d'Oro in Pavia, in order to save them from frequent coastal raids by Muslims. In January 1327, Pope John XXII issued the papal bull "Veneranda Santorum Patrum", in which he appointed the Augustinians guardians of the tomb of Augustine (called "Arca"), which was remade in 1362 and elaborately carved with bas-reliefs of scenes from Augustine's life.
In October 1695, some workmen in the Church of San Pietro in Ciel d'Oro in Pavia discovered a marble box containing some human bones (including part of a skull). A dispute arose between the Augustinian hermits (Order of Saint Augustine) and the regular canons (Canons Regular of Saint Augustine) as to whether these were the bones of Augustine. The hermits did not believe so; the canons affirmed that they were. Eventually Pope Benedict XIII (1724–1730) directed the Bishop of Pavia, Monsignor Pertusati, to make a determination. The bishop declared that, in his opinion, the bones were those of Saint Augustine.
The Augustinians were expelled from Pavia in 1700, taking refuge in Milan with the relics of Augustine, and the disassembled "Arca", which were removed to the cathedral there. San Pietro fell into disrepair, but was finally rebuilt in the 1870s, under the urging of Agostino Gaetano Riboldi, and reconsecrated in 1896 when the relics of Augustine and the shrine were once again reinstalled.
Thought.
Christian anthropology.
Augustine was one of the first Christian ancient Latin authors with a very clear vision of theological anthropology. He saw the human being as a perfect unity of two substances: soul and body. In his late treatise "" (420 AD) he exhorted to respect the body on the grounds that it belonged to the very nature of the human person. Augustine's favourite figure to describe "body-soul" unity is marriage: "caro tua, coniunx tua — your body is your wife". Initially, the two elements were in perfect harmony. After the fall of humanity they are now experiencing dramatic combat between one another. They are two categorically different things. The body is a three-dimensional object composed of the four elements, whereas the soul has no spatial dimensions. Soul is a kind of substance, participating in reason, fit for ruling the body. Augustine was not preoccupied, as Plato and Descartes were, with going too much into details in efforts to explain the metaphysics of the soul-body union. It sufficed for him to admit that they are metaphysically distinct: to be a human is to be a composite of soul and body, and the soul is superior to the body. The latter statement is grounded in his hierarchical classification of things into those that merely exist, those that exist and live, and those that exist, live, and have intelligence or reason.
Like other Church Fathers such as Athenagoras, Augustine "vigorously condemned the practice of induced abortion", and although he disapproved of an abortion during any stage of pregnancy, he made a distinction between early abortions and later ones. Nevertheless, he accepted the distinction between "formed" and "unformed" fetuses mentioned in the Septuagint translation of , a text that, he observed, did not classify as murder the abortion of an "unformed" fetus, since it could not be said with certainty that it had already received a soul (see, e.g., "De Origine Animae" 4.4).
Slavery.
Augustine led many clergy under his authority at Hippo to free their slaves "as an act of piety." He boldly wrote a letter urging the emperor to set up a new law against slave traders and was very much concerned about the sale of children. Christian emperors of his time for 25 years had permitted sale of children, not because they approved of the practice, but as a way of preventing infanticide when parents were unable to care for a child. Augustine noted that the tenant farmers in particular were driven to hire out or to sell their children as a means of survival. In his famous book, "The City of God", he presents the development of slavery as a product of sin and as contrary to God's divine plan. He wrote that God "did not intend that this rational creature, who was made in his image, should have dominion over anything but the irrational creation--not man over man, but man over the beasts." Thus he wrote that righteous men in primitive times were made shepherds of cattle, not kings over men. "The condition of slavery is the result of sin," he declared.
Astrology.
Augustine's contemporaries often believed astrology to be an exact and genuine science. Its practitioners were regarded as true men of learning and called "mathemathici". Astrology played a prominent part in Manichaean doctrine, and Augustine himself was attracted by their books in his youth, being particularly fascinated by those who claimed to foretell the future. Later, as a bishop, he used to warn that one should avoid astrologers who combine science and horoscopes. (Augustine's term "mathematici", meaning "astrologers", is sometimes mistranslated as "mathematicians".) According to Augustine, they were not genuine students of Hipparchus or Eratosthenes but "common swindlers".
Creation.
In "City of God", Augustine rejected both the immortality of the human race proposed by pagans, and contemporary ideas of ages (such as those of certain Greeks and Egyptians) that differed from the Church's sacred writings. In "The Literal Interpretation of Genesis", Augustine took the view that everything in the universe was created simultaneously by God, and not in seven calendar days like a literal account of Genesis would require. He argued that the six-day structure of creation presented in the book of Genesis represents a logical framework, rather than the passage of time in a physical way — it would bear a spiritual, rather than physical, meaning, which is no less literal. One reason for this interpretation is the passage in Sirach 18:1, "creavit omnia simul" ("He created all things at once"), which Augustine took as proof that the days of Genesis 1 had to be taken non-literally. Augustine also does not envision original sin as causing structural changes in the universe, and even suggests that the bodies of Adam and Eve were already created mortal before the Fall. Apart from his specific views, Augustine recognizes that the interpretation of the creation story is difficult, and remarks that we should be willing to change our mind about it as new information comes up.
Ecclesiology.
Augustine developed his doctrine of the Church principally in reaction to the Donatist sect. He taught that there is one Church, but that within this Church there are two realities, namely, the visible aspect (the institutional hierarchy, the Catholic sacraments, and the laity) and the invisible (the souls of those in the Church, who are either dead, sinful members or elect predestined for Heaven). The former is the institutional body established by Christ on earth which proclaims salvation and administers the sacraments, while the latter is the invisible body of the elect, made up of genuine believers from all ages, and who are known only to God. The Church, which is visible and societal, will be made up of "wheat" and "tares", that is, good and wicked people (as per Mat. 13:30), until the end of time. This concept countered the Donatist claim that only those in a state of grace were the "true" or "pure" church on earth, and that priests and bishops who were not in a state of grace had no authority or ability to confect the sacraments.
Augustine's ecclesiology was more fully developed in "City of God". There he conceives of the church as a heavenly city or kingdom, ruled by love, which will ultimately triumph over all earthly empires which are self-indulgent and ruled by pride. Augustine followed Cyprian in teaching that the bishops and priests of the Church are the successors of the Apostles, and that their authority in the Church is God-given.
Eschatology.
Augustine originally believed in premillennialism, namely that Christ would establish a literal 1,000-year kingdom prior to the general resurrection, but later rejected the belief, viewing it as carnal. He was the first theologian to expound a systematic doctrine of amillennialism, although some theologians and Christian historians believe his position was closer to that of modern postmillennialists. The mediaeval Catholic church built its system of eschatology on Augustinian amillennialism, where Christ rules the earth spiritually through his triumphant church. At the Reformation, theologians such as John Calvin accepted amillennialism. Augustine taught that the eternal fate of the soul is determined at death, and that purgatorial fires of the intermediate state purify only those that died in communion with the Church. His teaching provided fuel for later theology.
Epistemological views.
Epistemological concerns shaped Augustine's intellectual development. His early dialogues 'Contra academicos" (386) and "De Magistro" (389, both written shortly after his conversion to Christianity, reflect his engagement with sceptical arguments and show the development of his doctrine of inner illumination. The doctrine of illumination claims that God plays an active and regular part in human perception (as opposed to God designing the human mind to be reliable consistently, as in, for example, Descartes' idea of clear and distinct perceptions) and understanding by illuminating the mind so that human beings can recognize intelligible realities that God presents. According to Augustine, illumination is obtainable to all rational minds, and is different from other forms of sense perception. It is meant to be an explanation of the conditions required for the mind to have a connection with intelligible entities. Augustine also posed the problem of other minds throughout different works, most famously perhaps in "On the Trinity" (VIII.6.9), and developed what has come to be a standard solution: the argument from analogy to other minds. In contrast to Plato and other earlier philosophers, Augustine recognized the centrality of testimony to human knowledge and argued that what others tell us can provide knowledge even if we don't have independent reasons to believe their testimonial reports.
Just war.
Augustine asserted that Christians should be pacifists as a personal, philosophical stance. However, peacefulness in the face of a grave wrong that could only be stopped by violence would be a sin. Defence of one's self or others could be a necessity, especially when authorized by a legitimate authority. While not breaking down the conditions necessary for war to be just, Augustine coined the phrase in his work "The City of God". In essence, the pursuit of peace must include the option of fighting for its long-term preservation. Such a war could not be pre-emptive, but defensive, to restore peace. Thomas Aquinas, centuries later, used the authority of Augustine's arguments in an attempt to define the conditions under which a war could be just.
Mariology.
Although Augustine did not develop an independent Mariology, his statements on Mary surpass in number and depth those of other early writers. Even before the Council of Ephesus, he defended the ever Virgin Mary as the Mother of God, who, because of her virginity, is full of grace. Likewise, he affirmed that the Virgin Mary "conceived as virgin, gave birth as virgin and stayed virgin forever."
Natural knowledge and biblical interpretation.
Augustine took the view that, if a literal interpretation contradicts science and our God-given reason, the Biblical text should be interpreted metaphorically. While each passage of Scripture has a literal sense, this "literal sense" does not always mean that the Scriptures are mere history; at times they are rather an extended metaphor.
Original sin.
Augustine taught that Original sin of Adam and Eve was either an act of foolishness ("insipientia") followed by pride and disobedience to God or that pride came first. The first couple disobeyed God, who had told them not to eat of the Tree of the knowledge of good and evil (Gen 2:17). The tree was a symbol of the order of creation. Self-centeredness made Adam and Eve eat of it, thus failing to acknowledge and respect the world as it was created by God, with its hierarchy of beings and values. They would not have fallen into pride and lack of wisdom, if Satan hadn't sown into their senses "the root of evil" ("radix Mali"). Their nature was wounded by concupiscence or libido, which affected human intelligence and will, as well as affections and desires, including sexual desire. In terms of metaphysics, concupiscence is not a being but bad quality, the privation of good or a wound.
Augustine's understanding of the consequences of the original sin and of necessity of the redeeming grace was developed in the struggle against Pelagius and his Pelagian disciples, Caelestius and Julian of Eclanum, who had been inspired by Rufinus of Syria, a disciple of Theodore of Mopsuestia. They refused to agree that libido wounded human will and mind, insisting that the human nature was given the power to act, to speak, and to think when God created it. Human nature cannot lose its moral capacity for doing good, but a person is free to act or not to act in a righteous way. Pelagius gave an example of eyes: they have capacity for seeing, but a person can make either good or bad use of it. Like Jovinian, Pelagians insisted that human affections and desires were not touched by the fall either. Immorality, "e.g." fornication, is exclusively a matter of will, "i.e." a person does not use natural desires in a proper way. In opposition to that, Augustine pointed out the apparent disobedience of the flesh to the spirit, and explained it as one of the results of original sin, punishment of Adam and Eve's disobedience to God.
Augustine had served as a "Hearer" for the Manichaeans for about nine years, who taught that the original sin was carnal knowledge. But his struggle to understand the cause of evil in the world started before that, at the age of nineteen. By "malum" (evil) he understood most of all concupiscence, which he interpreted as a vice dominating person and causing in men and women moral disorder. A. Trapè insists that Augustine's personal experience cannot be credited for his doctrine about concupiscence. His marriage experience, though Christian marriage celebration was missing, was exemplary, very normal and by no means specifically sad. As J. Brachtendorf showed, Augustine used Ciceronian Stoic concept of passions, to interpret Paul's doctrine of universal sin and redemption.
The view that not only human soul but also senses were influenced by the fall of Adam and Eve was prevalent in Augustine's time among the Fathers of the Church. It is clear that the reason for Augustine's distancing from the affairs of the flesh was different from that of Plotinus, a neo-Platonist who taught that only through disdain for fleshly desire could one reach the ultimate state of mankind. Augustine taught the redemption, i.e. transformation and purification, of the body in the resurrection.
Some authors perceive Augustine's doctrine as directed against human sexuality and attribute his insistence on continence and devotion to God as coming from Augustine's need to reject his own highly sensual nature as described in the "Confessions". But in view of his writings it is apparently a misunderstanding. Augustine taught that human sexuality has been wounded, together with the whole of human nature, and requires redemption of Christ. That healing is a process realized in conjugal acts. The virtue of continence is achieved thanks to the grace of the sacrament of Christian marriage, which becomes therefore a "remedium concupiscentiae" – remedy of concupiscence. The redemption of human sexuality will be, however, fully accomplished only in the resurrection of the body.
The sin of Adam is inherited by all human beings. Already in his pre-Pelagian writings, Augustine taught that Original Sin is transmitted to his descendants by concupiscence, which he regarded as the passion of both, soul and body, making humanity a "massa damnata" (mass of perdition, condemned crowd) and much enfeebling, though not destroying, the freedom of the will.
Augustine's formulation of the doctrine of original sin was confirmed at numerous councils, "i.e." Carthage (418), Ephesus (431), Orange (529), Trent (1546) and by popes, "i.e." Pope Innocent I (401–417) and Pope Zosimus (417–418). Anselm of Canterbury established in his "Cur Deus Homo" the definition that was followed by the great 13th century Schoolmen, namely that Original Sin is the "privation of the righteousness which every man ought to possess", thus separating it from "concupiscence", with which some of Augustine's disciples had defined it as later did Luther and Calvin. In 1567, Pope Pius V condemned the identification of Original Sin with concupiscence.
Augustine taught that some people are predestined by God to salvation by an eternal, sovereign decree which is not based on man's merit or will. The saving grace which God bestows is irresistible and unfailingly results in conversion. God also grants those whom he saves with the gift of perseverance so that none of those whom God has chosen may conceivably fall away.
In "On Rebuke and Grace" ("De correptione et gratia"), Augustine wrote: "And what is written, that He wills all men to be saved, while yet all men are not saved, may be understood in many ways, some of which I have mentioned in other writings of mine; but here I will say one thing: He wills all men to be saved, is so said that all the predestinated may be understood by it, because every kind of men is among them."
Free will.
Included in Augustine's theodicy is the claim that God created humans and angels as rational beings possessing free will. Free will was not intended for sin, meaning it is not equally predisposed to both good and evil. A will defiled by sin is not considered as "free" as it once was because it is bound by material things, which could be lost or be difficult to part with, resulting in unhappiness. Sin impairs free will, while grace restores it. Only a will that was once free can be subjected to sin's corruption.
The Catholic Church considers Augustine's teaching to be consistent with free will. He often said that any can be saved if they wish. While God knows who will and won't be saved, with no possibility for the latter to be saved in their lives, this knowledge represents God's perfect knowledge of how humans will freely choose their destinies.
Sacramental theology.
Also in reaction against the Donatists, Augustine developed a distinction between the "regularity" and "validity" of the sacraments. Regular sacraments are performed by clergy of the Catholic Church, while sacraments performed by schismatics are considered irregular. Nevertheless, the validity of the sacraments do not depend upon the holiness of the priests who perform them ("ex opere operato"); therefore, irregular sacraments are still accepted as valid provided they are done in the name of Christ and in the manner prescribed by the Church. On this point Augustine departs from the earlier teaching of Cyprian, who taught that converts from schismatic movements must be re-baptised. Augustine taught that sacraments administered outside the Catholic Church, though true sacraments, avail nothing. However, he also stated that baptism, while it does not confer any grace when done outside the Church, does confer grace as soon as one is received into the Catholic Church.
Augustine upheld the early Christian understanding of the real presence of Christ in the Eucharist, saying that Christ's statement, "This is my body" referred to the bread he carried in his hands, and that Christians must have faith that the bread and wine are in fact the body and blood of Christ, despite what they see with their eyes.
Against the Pelagians, Augustine strongly stressed the importance of infant baptism. About the question whether baptism is an absolute necessity for salvation, however, Augustine appears to have refined his beliefs during his lifetime, causing some confusion among later theologians about his position. He said in one of his sermons that only the baptized are saved. This belief was shared by many early Christians. However, a passage from his "City of God", concerning the Apocalypse, may indicate that Augustine did believe in an exception for children born to Christian parents.
Statements on Jews.
Against certain Christian movements, some of which rejected the use of Hebrew Scripture, Augustine countered that God had chosen the Jews as a special people, and he considered the scattering of Jewish people by the Roman Empire to be a fulfillment of prophecy. He rejected homicidal attitudes, quoting part of the same prophecy, namely "Slay them not, lest they should at last forget Thy law" (Psalm 59:11). Augustine, who believed Jewish people would be converted to Christianity at "the end of time," argued that God had allowed them to survive their dispersion as a warning to Christians; as such, he argued, they should be permitted to dwell in Christian lands. The sentiment sometimes attributed to Augustine that Christians should let the Jews "survive but not thrive" (it is repeated by author James Carroll in his book "Constantine's Sword", for example) is apocryphal and is not found in any of his writings.
Views on sexuality.
For Augustine, the evil of sexual immorality was not in the sexual act itself, but rather in the emotions that typically accompany it. In "On Christian Doctrine" Augustine contrasts love, which is enjoyment on account of God, and lust, which is not on account of God. For Augustine, proper love exercises a denial of selfish pleasure and the subjugation of corporeal desire to God. He wrote that the pious virgins raped during the sack of Rome were innocent because they did not intend to sin.
He believed that the serpent approached Eve because she was less rational and lacked self-control, while Adam's choice to eat was viewed as an act of kindness so that Eve would not be left alone. Augustine believed sin entered the world because man (the spirit) did not exercise control over woman (the flesh). Augustine does, however, praise women and their role in society and in the Church. In his "Tractates on the Gospel of John", Augustine, commenting on the Samaritan woman from John 4:1–42, uses the woman as a figure of the church.
According to Raming, the authority of the "Decretum Gratiani", a collection of Roman Catholic canon law which prohibits women from leading, teaching, or being a witness, rests largely on the views of the early church fathers—one of the most influential being Augustine, the Bishop of Hippo. The laws and traditions founded upon Augustine's views of sexuality and women continue to exercise considerable influence over church doctrinal positions regarding the role of women in the church.
Philosophy of teaching.
Augustine is considered an influential figure in the history of education. A work early in Augustine's writings is "De Magistro" (On the Teacher), which contains insights about education. However, his ideas changed as he found better directions or better ways of expressing his ideas. In the last years of his life Saint Augustine wrote his "Retractationes", reviewing his writings and improving specific texts. Henry Chadwick believes an accurate translation of "retractationes" may be "reconsiderations". Reconsiderations can be seen as an overarching theme of the way Saint Augustine learned. Augustine's understanding of the search for understanding/meaning/truth as a restless journey leaves room for doubt, development and change.
Augustine was a strong advocate of critical thinking skills. Because written works were still rather limited during this time, spoken communication of knowledge was very important. His emphasis on the importance of community as a means of learning distinguishes his pedagogy from some others. Augustine believed that dialogue/dialectic/discussion is the best means for learning, and this method should serve as a model for learning encounters between teachers and students. Saint Augustine's dialogue writings model the need for lively interactive dialogue among learners.
He recommended adapting educational practices to fit the students' educational backgrounds:
If a student has been well educated in a wide variety of subjects, the teacher must be careful not to repeat what they have already learned, but to challenge the student with material which they do not yet know thoroughly. With the student who has had no education, the teacher must be patient, willing to repeat things until the student understands, and sympathetic. Perhaps the most difficult student, however, is the one with an inferior education who believes he understands something when he does not. Augustine stressed the importance of showing this type of student the difference between "having words and having understanding," and of helping the student to remain humble with his acquisition of knowledge.
A "restrained" style of teaching ensures the students' full understanding of a concept because the teacher does not bombard the student with too much material; focuses on one topic at a time; helps them discover what they don't understand, rather than moving on too quickly; anticipates questions; and helps them learn to solve difficulties and find solutions to problems.
Augustine believed that students should be given an opportunity to apply learned theories to practical experience. Yet another of Augustine's major contributions to education is his study on the styles of teaching. He claimed there are two basic styles a teacher uses when speaking to the students. The "mixed style" includes complex and sometimes showy language to help students see the beautiful artistry of the subject they are studying. The "grand style" is not quite as elegant as the mixed style, but is exciting and heartfelt, with the purpose of igniting the same passion in the students' hearts. Augustine balanced his teaching philosophy with the traditional Bible-based practice of strict discipline.
Works.
Augustine was one of the most prolific Latin authors in terms of surviving works, and the list of his works consists of more than one hundred separate titles. They include apologetic works against the heresies of the Arians, Donatists, Manichaeans and Pelagians; texts on Christian doctrine, notably "De Doctrina Christiana" ("On Christian Doctrine"); exegetical works such as commentaries on Genesis, the Psalms and Paul's Letter to the Romans; many sermons and letters; and the "Retractationes", a review of his earlier works which he wrote near the end of his life. Apart from those, Augustine is probably best known for his "Confessions", which is a personal account of his earlier life, and for "De civitate Dei" ("The City of God", consisting of 22 books), which he wrote to restore the confidence of his fellow Christians, which was badly shaken by the sack of Rome by the Visigoths in 410. His "On the Trinity", in which he developed what has become known as the 'psychological analogy' of the Trinity, is also among his masterpieces, and arguably one of the greatest theological works of all time. He also wrote "On Free Choice of the Will" ("De libero arbitrio"), addressing why God gives humans free will that can be used for evil.
Influence.
In both his philosophical and theological reasoning, Augustine was greatly influenced by Stoicism, Platonism and Neoplatonism, particularly by the work of Plotinus, author of the "Enneads", probably through the mediation of Porphyry and Victorinus (as Pierre Hadot has argued). Although he later abandoned Neoplatonism, some ideas are still visible in his early writings. His early and influential writing on the human will, a central topic in ethics, would become a focus for later philosophers such as Schopenhauer, Kierkegaard, and Nietzsche. He was also influenced by the works of Virgil (known for his teaching on language), and Cicero (known for his teaching on argument).
In philosophy.
Philosopher Bertrand Russell was impressed by Augustine's meditation on the nature of time in the "Confessions", comparing it favourably to Kant's version of the view that time is subjective. Catholic theologians generally subscribe to Augustine's belief that God exists outside of time in the "eternal present"; that time only exists within the created universe because only in space is time discernible through motion and change. His meditations on the nature of time are closely linked to his consideration of the human ability of memory. Frances Yates in her 1966 study "The Art of Memory" argues that a brief passage of the "Confessions", 10.8.12, in which Augustine writes of walking up a flight of stairs and entering the vast fields of memory clearly indicates that the ancient Romans were aware of how to use explicit spatial and architectural metaphors as a mnemonic technique for organizing large amounts of information.
Augustine's philosophical method, especially demonstrated in his "Confessions", had continuing influence on Continental philosophy throughout the 20th century. His descriptive approach to intentionality, memory, and language as these phenomena are experienced within consciousness and time anticipated and inspired the insights of modern phenomenology and hermeneutics. Edmund Husserl writes: "The analysis of time-consciousness is an age-old crux of descriptive psychology and theory of knowledge. The first thinker to be deeply sensitive to the immense difficulties to be found here was Augustine, who laboured almost to despair over this problem." Martin Heidegger refers to Augustine's descriptive philosophy at several junctures in his influential work "Being and Time". Hannah Arendt began her philosophical writing with a dissertation on Augustine's concept of love, "Der Liebesbegriff bei Augustin" (1929): "The young Arendt attempted to show that the philosophical basis for "vita socialis" in Augustine can be understood as residing in neighbourly love, grounded in his understanding of the common origin of humanity." Jean Bethke Elshtain in "Augustine and the Limits of Politics" : "Augustine did not see evil as glamorously demonic but rather as absence of good, something which paradoxically is really nothing. Arendt ... envisioned even the extreme evil which produced the Holocaust as merely banal n "[[Eichmann in Jerusalem]]'." Augustine's philosophical legacy continues to influence contemporary critical theory through the contributions and inheritors of these 20th-century figures. Seen from a historical perspective, there are three main perspectives on the political thought of Augustine: first, political Augustinianism; second, Augustinian political theology; and third, Augustinian political theory.
In theology.
Thomas Aquinas was influenced heavily by Augustine. On the topic of original sin, Aquinas proposed a more optimistic view of man than that of Augustine in that his conception leaves to the reason, will, and passions of fallen man their natural powers even after the Fall. Augustine's doctrine of efficacious grace found eloquent expression in the works of Bernard of Clairvaux; also Reformation theologians such as Martin Luther and John Calvin would look back to him as their inspiration. While in his pre-Pelagian writings Augustine taught that Adam's guilt as transmitted to his descendants much enfeebles, though does not destroy, the freedom of their will, Protestant reformers Martin Luther and John Calvin affirmed that Original Sin completely destroyed liberty (see total depravity).
According to Leo Ruickbie, Augustine's arguments against magic, differentiating it from miracle, were crucial in the early Church's fight against paganism and became a central thesis in the later denunciation of witches and witchcraft. According to Professor Deepak Lal, Augustine's vision of the heavenly city has influenced the secular projects and traditions of the Enlightenment, Marxism, Freudianism and eco-fundamentalism. Post-Marxist philosophers Antonio Negri and Michael Hardt rely heavily on Augustine's thought, particularly "The City of God", in their book of political philosophy "Empire".
Augustine has influenced many modern-day theologians and authors such as John Piper. Hannah Arendt, an influential 20th-century political theorist, wrote her doctoral dissertation in philosophy on Augustine, and continued to rely on his thought throughout her career. Ludwig Wittgenstein extensively quotes Augustine in "Philosophical Investigations" for his approach to language, both admiringly, and as a sparring partner to develop his own ideas, including an extensive opening passage from the "Confessions". Contemporary linguists have argued that Augustine has significantly influenced the thought of Ferdinand de Saussure, who did not 'invent' the modern discipline of semiotics, but rather built upon Aristotelian and Neoplatonist knowledge from the Middle Ages, via an Augustinian connection: "as for the constitution of Saussurian semiotic theory, the importance of the Augustinian thought contribution (correlated to the Stoic one) has also been recognized. Saussure did not do anything but reform an ancient theory in Europe, according to the modern conceptual exigencies."
In his autobiographical book "Milestones", Pope Benedict XVI claims Augustine as one of the deepest influences in his thought.
In popular culture.
Augustine was played by Dary Berkani in the 1972 television movie "Augustine of Hippo". He was played by Franco Nero in the 2010 mini-series ' and the 2012 feature film '. The modern-day name links to the Agostinelli family.
Jostein Gaarder's philosophical novel "Vita Brevis" is presented as a translation of a manuscript written by Augustine's concubine after he became the Bishop of Hippo. Augustine also appears in the novel "The Dalkey Archive" by Flann O'Brien (the pen name of Irish author Brian O'Nolan). He is summoned to an underwater cavern by an absurd scientist called De Selby; together they discuss life in Heaven and the characters of other saints. Walter M. Miller, Jr.'s novel "A Canticle for Leibowitz" cites Augustine as possibly positing the first version of a theory of evolution.
Bob Dylan recorded a song entitled "I Dreamed I Saw St. Augustine" on his album "John Wesley Harding". Pop artist Sting pays an homage of sorts to Augustine's struggles with lust with the song "Saint Augustine in Hell" which appears on the singer's 1993 album "Ten Summoner's Tales". Christian rock band Disciple named their fourth track on their 2010 release "Horseshoes and Handgrenades" after Augustine, called "The Ballad of St. Augustine". The song "St. Augustine" appears on Girlyman's album, "Supernova". American rock band Moe named and referenced Augustine of Hippo in their song entitled "St. Augustine".

</doc>
<doc id="2032" url="https://en.wikipedia.org/wiki?curid=2032" title="Acting">
Acting

Acting is the work of an actor or actress, which is a person in theatre, television, film, or any other storytelling medium who tells the story by portraying a character and, usually, speaking or singing the written text or play.
Most early sources in the West that examine the art of acting (, "hypokrisis") discuss it as part of rhetoric.
Definition and history.
One of the first actors is believed to be an ancient Greek called Thespis of Icaria. An apocryphal story says that Thespis stepped out of the dithyrambic chorus and spoke to them as a separate character. Before Thespis, the chorus narrated (for example, "Dionysus did this, Dionysus said that"). When Thespis stepped out from the chorus (year 12 BC), he spoke as if he was the character (for example, "I am Dionysus. I did this"). From Thespis' name derives the word "thespian".
Acting requires a wide range of skills, including vocal projection, clarity of speech, physical expressivity, emotional facility, a well-developed imagination, and the ability to interpret drama. Acting also often demands an ability to employ dialects, accents and body language, improvisation, observation and emulation, mime, and stage combat. Many actors train at length in special programs or colleges to develop these skills, and today the vast majority of professional actors have undergone extensive training. Even though one actor may have years of training, they always strive for more lessons; the cinematic and theatrical world is always changing and because of this, the actor must stay as up to date as possible. Actors and actresses will often have many instructors and teachers for a full range of training involving, but not limited to, singing, scene-work, monologue techniques, audition techniques, and partner work.
Professional actors.
A professional actor is someone who gets paid for acting. Not all people working as actors in film, television or theater are professionally trained. For example, Bob Hoskins did not have any training before taking up acting.
Training.
Conservatories typically offer two- to four-year training on all aspects of acting. Universities will offer three- to four-year programs, where a student is often able to choose to focus on drama, while still learning about other aspects of theatre. Schools will vary in their approach, but in North America the most popular method taught derives from the "system" of Constantin Stanislavski, which was developed and popularised in America by Lee Strasberg, Stella Adler, and others. The ambiguously termed method acting came about through iterations of Stanislavski's system by Strasberg. Part of this style of training includes actors memorizing lines to be able to work off-book, a term that means being able to work without a script. Other approaches may include a more physical approach, following the teachings of Jerzy Grotowski and others, or may be based on the training developed by other theatre practitioners including Sanford Meisner. Other classes may include mask work, improvisation, and acting for the camera. Regardless of a school's approach, students should expect intensive training in textual interpretation, voice and movement. Although there are some teachers who will encourage the improvisation as technique in order to free the actor of limitations in rehearsal. Harold Guskin's approach or "taking it off the page" as he calls it is steeped in this philosophy. Applications to drama programs and conservatories are through auditions in the United States. Anybody over the age of 18 can usually apply to drama school.
Training may also start at a very young age. Acting classes and professional schools targeted at the under-18 crowd are offered in many locations. These classes introduce young actors to different aspects of acting and theatre, including scene study.
Amateur actors.
Amateur actors are actors who do not require payment for performances. Although there are some paid professional actors who do amateur work for multiple reasons. Some may be for educational purposes or even charity events.
Improvisation.
Improvisation was created by Viola Spolin after working with Neva Boyd at a Hull House in Chicago, Illinois. She was Boyds student from 1924 to 1927. Improv was created on the realization that adults do not play games. Spolin felt that playing games were good exercises and can benefit in future acting. With improv, people can find true expressive freedom since they don't ever know how the situation is going to turn out. When one continues to operate with an open mind they will have a real sense of spontaneity rather than pre-planning a response. You perform a character of your own making, and with that character and the others working with you, you create a new and spontaneous piece. Improv is also used to cover up if an actor or actress makes a mistake.
Semiotics of Acting.
Semiotics of Acting is the actor’s ability to transform into a convincing character in front of the audience. The audience no longer sees the actor as a performer, but sees a character as a completely different being. Once this shift occurs, the actor becomes a semiotic device communicating a set of signs to the audience. A character’s signification can represent a multitude of different meanings to the audience. This may or may not be intended by the actor, who has limited control over how the audience will “read” the character. For example, if the actor is playing a character diagnosed with cancer, the audience may not just see a cancer patient, but may instead see a character similar to other cancer victims or survivors they have known. The actor’s performance, like any text, must be read by the audience. 
However, the actor is judged by giving a convincing and believable performance. The actor’s performance is mediated by particular semiotic signs including facial expression, emotion, and vocabulary. All these examples are known as performance signs. Performance signs are simple codes that the audience must decode during the actor's performance. It is the actor’s job to deliver those codes effectively to the audience. If the audience does not find the character believable, then the actor has failed in their performance. Like other forms of communication, non-verbal or visual clues are tremendously important. Acting teacher Sanford Meisner once said, “An ounce of emotion is worth a pound of words.” Great actors master performance signs in order to win over an audience. 
Acting involves two forms of communication: intrascenic (communication between characters) and extrascenic (communication between the characters and the audience). Both intrascenic and extrascenic communication must work in order for the audience to read the semiotic signs of the actor’s performance. The characters must have intrascenic skills – “good chemistry” – in a scene in order for the audience to understand the performance. 
The actor represents the text of the script as performance signs. Actors bring the text to life through performance and through the personal qualities they may contribute to the narrative of script. Actors represent the ideas of the text, but also create a new visually dimensioned reality through their performance.
Becoming an actor representing semiotic signs can be a very difficult process. One must understand the performance signs, the audience, and human emotion.
Rehearsing.
Rehearsing is the opportunity to practice and create confrontation between one character and another. This is a process created by consistently repeating your lines until they are learned. Rehearsal is also a process in which an actor is able to try new tactics in the scene and find to convey a message. Many actors continually rehearse a scene throughout the run of a show to keep the scene fresh in their minds and exciting for the audience. 
·Claudia Springer and Julie Levinson, eds., Acting (Behind the Silver Screen series), Rutgers University Press, 2015. ISBN 978-0813564326

</doc>
