<doc id="8632" url="https://en.wikipedia.org/wiki?curid=8632" title="Druze">
Druze

The Druze (; ' or ', plural '; ' plural , "") are an Arabic-speaking esoteric ethnoreligious group, originating in Southwestern Asia, who self-identify as unitarians ("Al-Muwaḥḥidūn/Muwahhideen"). Jethro of Midian is considered an ancestor of all Druze and revered as their spiritual founder as well as chief prophet.
The Druze faith is a monotheistic and Abrahamic religion based on the teachings of Plato, Aristotle, Socrates, Akhenaten, Hamza, and Al Hakim. The Epistles of Wisdom is the foundational text of the Druze faith, alongside supplemental texts such as the Epistles of India. The Druze faith incorporates elements of Gnosticism, Neoplatonism, Pythagoreanism, Ismailism, Judaism, Christianity, Hinduism, and other philosophies and beliefs, creating a distinct and secretive theology known to esoterically interpret religious scriptures and to highlight the role of the mind and truthfulness. The Druze follow theophany, and believe in reincarnation or the transmigration of the soul. At the end of the cycle of rebirth, which is achieved through successive reincarnations, the soul is united with the Cosmic Mind ("Al Aaqal Al Kulli").
Although dwarfed by other, larger communities, the Druze community played an important role in shaping the history of the Levant, and continues to play a large political role there. As an ethnic and religious minority in every country in which they live, they have frequently experienced persecution. Druze are not considered Muslims and are thus often labeled as infidels ("Murtadun"). Ali az-Zahir of the Fatimid Caliphate campaigned to exterminate the faith, ethnically cleansing Druze communities in Antioch, Aleppo, and northern Syria. Further attempts were made by the Mamluks and Ottomans. Most recently, Druze were targeted by the Islamic State of Iraq and the Levant and Al-Qaeda in order to cleanse Syria and neighboring countries of non-Islamic influence.
The Druze faith is one of the major religious groups in the Levant, with about 1.5 million adherents. They are found primarily in Syria, Lebanon and Israel, with small communities in Jordan and outside Southwestern Asia. The oldest and most densely-populated Druze communities exist in Mount Lebanon and in the south of Syria around Jabal al-Druze (literally the "Mountain of the Druzes"). The Druze's social customs differ markedly from those of Muslims or Christians, and they are known to form a close-knit, cohesive community but also integrate fully in their adopted homelands.
Location.
The Druze people reside primarily in Syria, Lebanon, Israel and Jordan. The Institute of Druze Studies estimates that forty to fifty percent of Druze live in Syria, thirty to forty percent in Lebanon, six to seven percent in Israel, and one or two percent in Jordan. About two percent of the Druze population are also scattered within other countries in the Middle East.
Large communities of Druze also live outside the Middle East, in Australia, Canada, Europe, Latin America, the United States, and West Africa. They use the Arabic language and follow a social pattern very similar to those of the other peoples of the Levant (eastern Mediterranean).
The number of Druze people worldwide exceeds one million, with the vast majority residing in the Levant.
History.
Origin of the name.
The name Druze is derived from the name of Muhammad bin Ismail Nashtakin ad-Darazī (from Persian "", "seamster") who was an early preacher. Although the Druze consider ad-Darazī a heretic, the name has been used to identify them.
Before becoming public, the movement was secretive and held closed meetings in what was known as Sessions of Wisdom. During this stage a dispute occurred between ad-Darazi and Hamza bin Ali mainly concerning ad-Darazi's ' ("exaggeration"), which refers to the belief that God was incarnated in human beings (especially 'Ali and his descendants, including Al-Hakim bi-Amr Allah, who was the caliph at the time) and to ad-Darazi naming himself "The Sword of the Faith", which led Hamza to write an epistle refuting the need for the sword to spread the faith and several epistles refuting the beliefs of the '.
In 1016 ad-Darazi and his followers openly proclaimed their beliefs and called people to join them, causing riots in Cairo against the Unitarian movement including Hamza bin Ali and his followers. This led to the suspension of the movement for one year and the expulsion of ad-Darazi and his supporters.
Although the Druze religious books describe ad-Darazi as the "insolent one" and as the "calf" who is narrow-minded and hasty, the name "Druze" is still used for identification and for historical reasons. In 1018 ad-Darazi was assassinated for his teachings; some sources claim that he was executed by Al-Hakim bi-Amr Allah.
Some authorities see in the name "Druze" a descriptive epithet, derived from Arabic ' ("those who study"). Others have speculated that the word comes from the Persian word ' ( "bliss") or from Shaykh Hussayn ad-Darazī, who was one of the early converts to the faith. In the early stages of the movement, the word "Druze" is rarely mentioned by historians, and in Druze religious texts only the word "Muwaḥḥidūn" ("Unitarian") appears. The only early Arab historian who mentions the Druze is the eleventh century Christian scholar Yahya of Antioch, who clearly refers to the heretical group created by ad-Darazī rather than the followers of Hamza ibn 'Alī. As for Western sources, Benjamin of Tudela, the Jewish traveler who passed through Lebanon in or around 1165, was one of the first European writers to refer to the Druzes by name. The word "" ("Druzes") occurs in an early Hebrew edition of his travels, but it is clear that this is a scribal error. Be that as it may, he described the Druze as "mountain dwellers, monotheists, who believe in 'soul eternity' and reincarnation." He also stated that "they loved the Jews."
Early history.
The Druze faith began as a movement in Ismailism that was heavily influenced by Greek philosophy and Gnosticism and opposed certain religious and philosophical ideologies that were present during that epoch.
The faith was preached by Hamza ibn 'Alī ibn Ahmad, an Ismaili mystic and scholar. He came to Egypt in 1014 and assembled a group of scholars and leaders from across the world to establish the Unitarian movement. The order's meetings were held in the Raydan Mosque, near the Al-Hakim Mosque.
In 1017, Hamza officially revealed the Druze faith and began to preach the Unitarian doctrine. Hamza gained the support of the Fātimid caliph al-Hakim, who issued a decree promoting religious freedom prior to the declaration of the divine call.
Al-Hakim became a central figure in the Druze faith even though his own religious position was disputed among scholars. John Esposito states that al-Hakim believed that "he was not only the divinely appointed religio-political leader but also the cosmic intellect linking God with creation", while others like Nissim Dana and Mordechai Nisan state that he is perceived as the manifestation and the reincarnation of God or presumably the image of God.
Some Druze and non-Druze scholars like Samy Swayd and Sami Makarem state that this confusion is due to confusion about the role of the early preacher ad-Darazi, whose teachings the Druze rejected as heretical. These sources assert that al-Hakim rejected ad-Darazi's claims of divinity, and ordered the elimination of his movement while supporting that of Hamza ibn Ali.
Al-Hakim disappeared one night while out on his evening ride – presumably assassinated, perhaps at the behest of his formidable elder sister Sitt al-Mulk. The Druze believe he went into Occultation with Hamza ibn Ali and three other prominent preachers, leaving the care of the "Unitarian missionary movement" to a new leader, Al-Muqtana Baha'uddin (also spelled Baha' ad-Din).
Closing of the faith.
Al-Hakim was replaced by his underage son, 'Alī az-Zahir. The Unitarian Druze movement, which existed in the Fatimid Caliphate, acknowledged az-Zahir as the caliph, but followed Hamzah as its Imam. The young caliph's regent, Sitt al-Mulk, ordered the army to destroy the movement in 1021. At the same time, Bahā'a ad-Dīn as-Samuki was assigned the leadership of the Unitarian Movement by Hamza Bin Ali.
For the next seven years, the Druze faced extreme persecution by the new caliph, al-Zahir, who wanted to eradicate the faith. This was the result of a power struggle inside of the Fatimid empire in which the Druze were viewed with suspicion because of their refusal to recognize the new caliph, Ali az-Zahir, as their Imam. Many spies, mainly the followers of Ad-Darazi, joined the Unitarian movement in order to infiltrate the Druze community. The spies set about agitating trouble and soiling the reputation of the Druze. This resulted in friction with the new caliph who clashed militarily with the Druze community. The clashes ranged from Antioch to Alexandria, where tens of thousands of Druze were slaughtered by the Fatimid army. The largest massacre was at Antioch, where 5,000 Druze religious leaders were killed, followed by that of Aleppo. As a result, the faith went underground in hope of survival, as those captured were either forced to renounce their faith or be killed. Druze survivors "were found principally in southern Lebanon and Syria." In 1038, two years after the death of al-Zahir, the Druze movement was able to resume because the new leadership that replaced him had friendly political ties with at least one prominent Druze leader.
In 1043 Baha' ad-Din declared that the sect would no longer accept new pledges, and since that time proselytization has been prohibited.
During the Crusades.
It was during the period of Crusader rule in Syria (1099–1291) that the Druze first emerged into the full light of history in the Gharb region of the Chouf Mountains. As powerful warriors serving the Muslim rulers of Damascus against the Crusades, the Druze were given the task of keeping watch over the crusaders in the seaport of Beirut, with the aim of preventing them from making any encroachments inland. Subsequently, the Druze chiefs of the Gharb placed their considerable military experience at the disposal of the Mamluk rulers of Egypt (1250–1516); first, to assist them in putting an end to what remained of Crusader rule in coastal Syria, and later to help them safeguard the Syrian coast against Crusader retaliation by sea.
In the early period of the Crusader era, the Druze feudal power was in the hands of two families, the Tanukhs and the Arslans. From their fortresses in the Gharb area (now in Aley District) of southern Mount Lebanon Governorate, the Tanukhs led their incursions into the Phoenician coast and finally succeeded in holding Beirut and the marine plain against the Franks. Because of their fierce battles with the Crusaders, the Druzes earned the respect of the Sunni Muslim caliphs and thus gained important political powers. After the middle of the twelfth century, the Ma'an family superseded the Tanukhs in Druze leadership. The origin of the family goes back to a Prince Ma'an who made his appearance in the Lebanon in the days of the 'Abbasid caliph al-Mustarshid (1118–35 AD). The Ma'ans chose for their abode the Chouf District in south-western Lebanon (southern Mount Lebanon Governorate), overlooking the maritime plain between Beirut and Sidon, and made their headquarters in Baaqlin, which is still a leading Druze village. They were invested with feudal authority by Sultan Nur ad-Din and furnished respectable contingents to the Muslim ranks in their struggle against the Crusaders.
Persecution during the Mamluk and Ottoman period.
Having cleared Syria of the Franks, the Mamluk sultans of Egypt turned their attention to the schismatic Muslims of Syria. In 1305, after the issuing of a fatwa by the scholar Ibn Taymiyyah calling for jihad against all non-Sunni Muslims like the Druze, Alawites, Ismaili, and Twelver Shia Muslims, al-Malik al-Nasir inflicted a disastrous defeat on the Druze at Keserwan and forced outward compliance on their part to orthodox Sunni Islam. Later, under the Ottoman, they were severely attacked at Saoufar in 1585 after the Ottomans claimed that they assaulted their caravans near Tripoli. As a result of the Ottoman experience with the rebellious Druze, the word "Durzi" in Turkish came, and continues, to mean someone who is the ultimate thug.
Consequently, the 16th and 17th centuries were to witness a succession of armed Druze rebellions against the Ottomans, countered by repeated Ottoman punitive expeditions against the Chouf, in which the Druze population of the area was severely depleted and many villages destroyed. These military measures, severe as they were, did not succeed in reducing the local Druze to the required degree of subordination. This led the Ottoman government to agree to an arrangement whereby the different nahiyes (districts) of the Chouf would be granted in "iltizam" ("fiscal concession") to one of the region's amirs, or leading chiefs, leaving the maintenance of law and order and the collection of its taxes in the area in the hands of the appointed amir. This arrangement was to provide the cornerstone for the privileged status which ultimately came to be enjoyed by the whole of Mount Lebanon, Druze and Christian areas alike.
Ma'an dynasty.
With the advent of the Ottoman Turks and the conquest of Syria by Sultan Selim I in 1516, the Ma'ans were acknowledged by the new rulers as the feudal lords of southern Lebanon. Druze villages spread and prospered in that region, which under Ma'an leadership so flourished that it acquired the generic term of "Jabal Bayt-Ma'an" (the mountain of the Ma'an family) or "Jabal al-Druze". The latter title has since been usurped by the Hawran region, which since the middle of the 19th century has proven a haven of refuge to Druze emigrants from Lebanon and has become the headquarters of Druze power.
Under Fakhr-al-Dīn II (Fakhreddin II), the Druze dominion increased until it included almost all Syria, extending from the edge of the Antioch plain in the north to Safad in the south, with a part of the Syrian desert dominated by Fakhr-al-Din's castle at Tadmur (Palmyra), the ancient capital of Zenobia. The ruins of this castle still stand on a steep hill overlooking the town. Fakhr-al-Din became too strong for his Turkish sovereign in Constantinople. He went so far in 1608 as to sign a commercial treaty with Duke Ferdinand I of Tuscany containing secret military clauses. The Sultan then sent a force against him, and he was compelled to flee the land and seek refuge in the courts of Tuscany and Naples in 1613 and 1615 respectively.
In 1618 political changes in the Ottoman sultanate had resulted in the removal of many enemies of Fakhr-al-Din from power, signaling the prince's triumphant return to Lebanon soon afterwards. Through a clever policy of bribery and warfare, he extended his domains to cover all of modern Lebanon, some of Syria and northern Galilee.
In 1632 Küçük Ahmet Pasha was named Lord of Damascus. Küçük Ahmet Pasha was a rival of Fakhr-al-Din and a friend of the sultan Murad IV, who ordered the pasha and the sultanate's navy to attack Lebanon and depose Fakhr-al-Din.
This time the prince decided to remain in Lebanon and resist the offensive, but the death of his son Ali in Wadi al-Taym was the beginning of his defeat. He later took refuge in Jezzine's grotto, closely followed by Küçük Ahmet Pasha who eventually caught up with him and his family.
Fakhr-al-Din was captured, taken to Istanbul, and imprisoned with two of his sons in the infamous Yedi Kule prison. The Sultan had Fakhr-al-Din and his sons killed on 13 April 1635 in Istanbul, bringing an end to an era in the history of Lebanon, which would not regain its current boundaries until it was proclaimed a mandate state and republic in 1920. One version recounts that the younger son was spared, raised in the harem and went on to become Ottoman Ambassador to India.
Fakhr-al-Din II was the first ruler in modern Lebanon to open the doors of his country to foreign Western influences. Under his auspices the French established a khān (hostel) in Sidon, the Florentines a consulate, and Christian missionaries were admitted into the country. Beirut and Sidon, which Fakhr-al-Din II beautified, still bear traces of his benign rule. See the new biography of this Prince, based on original sources, by TJ Gorton: "Renaissance Emir: a Druze Warlord at the Court of the Medici" (London, Quartet Books, 2013), for an updated view of his life.
Fakhr ad Din II was succeeded in 1635 by his nephew Ahmed Ma'an, who ruled through his death in 1658. (Fakhr ad Din's only surviving son, Husayn, lived the rest of his life as a court official in Constantinople.) Emir Mulhim exercised Iltizam taxation rights in the Shuf, Gharb, Jurd, Matn, and Kisrawan districts of Lebanon. Mulhim's forces battled and defeated those of Mustafa Pasha, Beylerbey of Damascus, in 1642, but he is reported by historians to have been otherwise loyal to Ottoman rule.
Following Mulhim's death, his sons Ahmad and Korkmaz entered into a power struggle with other Ottoman-backed Druze leaders. In 1660, the Ottoman Empire moved to reorganize the region, placing the sanjaks (districts) of Sidon-Beirut and Safed in a newly formed province of Sidon, a move seen by local Druze as an attempt to assert control. Contemporary historian Istifan al-Duwayhi reports that Korkmaz was killed in act of treachery by the Beylerbey of Damascus in 1662. Ahmad however emerged victorious in the power struggle among the Druze in 1667, but the Maʿnīs lost control of Safad and retreated to controlling the iltizam of the Shuf mountains and Kisrawan. Ahmad continued as local ruler through his death from natural causes, without heir, in 1697.
During the Ottoman-Habsburg war of 1683 to 1699, Ahmad Ma'n collaborated in a rebellion against the Ottomans which extended beyond his death. Iltizam rights in Shuf and Kisrawan passed to the rising Shihab family through female-line inheritance.
Shihab Dynasty.
As early as the days of Saladin, and while the Ma'ans were still in complete control over southern Lebanon, the Shihab tribe, originally Hijaz Arabs but later settled in Ḥawran, advanced from Ḥawran, in 1172, and settled in Wadi al-Taym at the foot of mount Hermon. They soon made an alliance with the Ma'ans and were acknowledged as the Druze chiefs in "Wadi al-Taym". At the end of the 17th century (1697) the Shihabs succeeded the Ma'ans in the feudal leadership of Druze southern Lebanon, although they reportedly professed Sunni Islam, they showed sympathy with Druzism, the religion of the majority of their subjects.
The Shihab leadership continued until the middle of the 19th century and culminated in the illustrious governorship of Amir Bashir Shihab II (1788–1840) who, after Fakhr-al-Din, was the most powerful feudal lord Lebanon produced. Though governor of the Druze Mountain, Bashir was a crypto-Christian, and it was he whose aid Napoleon solicited in 1799 during his campaign against Syria.
Having consolidated his conquests in Syria (1831–1838), Ibrahim Pasha, son of the viceroy of Egypt, Muhammad Ali Pasha, made the fatal mistake of trying to disarm the Christians and Druzes of the Lebanon and to draft the latter into his army. This was contrary to the principles of the life of independence which these mountaineers had always lived, and resulted in a general uprising against Egyptian rule. The uprising was encouraged, for political reasons, by the British. The Druzes of Wadi al-Taym and Ḥawran, under the leadership of Shibli al-Aryan, distinguished themselves in their stubborn resistance at their inaccessible headquarters, "al-Laja", lying southeast of Damascus.
Qaysites and the Yemenites.
The conquest of Syria by the Muslim Arabs in the middle of the seventh century introduced into the land two political factions later called the Qaysites and the Yemenites. The Qaysite party represented the Bedouin Arabs who were regarded as inferior by the Yemenites who were earlier and more cultured emigrants into Syria from southern Arabia. Druzes and Christians grouped in political rather than religious parties so the party lines in Lebanon obliterated racial and religious lines and the people grouped themselves regardless of their religious affiliations, into one or the other of these two parties. The sanguinary feuds between these two factions depleted, in course of time, the manhood of the Lebanon and ended in the decisive battle of Ain Dara in 1711, which resulted in the utter defeat of the Yemenite party. Many Yemenite Druzes thereupon immigrated to the Hawran region and thus laid the foundation of Druze power there.
Civil War of 1860.
The Druzes and their Christian Maronite neighbors, who had thus far lived as religious communities on friendly terms, entered a period of social disturbance in the year 1840, which culminated in the civil war of 1860.
After the Shehab dynasty converted to Christianity, the Druze community and feudal leaders came under attack from the regime with the collaboration of the Catholic Church, and the Druze lost most of their political and feudal powers. Also, the Druze formed an alliance with Britain and allowed Protestant missionaries to enter Mount Lebanon, creating tension between them and the Catholic Maronites.
The Maronite-Druze conflict in 1840–60 was an outgrowth of the Maronite Christian independence movement, directed against the Druze, Druze feudalism, and the Ottoman-Turks. The civil war was not therefore a religious war, except in Damascus, where it spread and where the vastly non-Druze population was anti-Christian. The movement culminated with the 1859–60 massacre and defeat of the Christians by the Druzes. The civil war of 1860 cost the Christians some ten thousand lives in Damascus, Zahlé, Deir al-Qamar, Hasbaya, and other towns of Lebanon.
The European powers then determined to intervene, and authorized the landing in Beirut of a body of French troops under General Beaufort d'Hautpoul, whose inscription can still be seen on the historic rock at the mouth of Nahr al-Kalb. French intervention on behalf of the Maronites did not help the Maronite national movement, since France was restricted in 1860 by Britain, which did not want the Ottoman Empire dismembered. But European intervention pressured the Turks to treat the Maronites more justly. Following the recommendations of the powers, the Ottoman Porte granted Lebanon local autonomy, guaranteed by the powers, under a Christian governor. This autonomy was maintained until World War I.
Rebellion in Hauran.
The Hauran rebellion was a violent Druze uprising against Ottoman authority in the Syrian province, which erupted in May 1909. The rebellion was led by al-Atrash family, originated in local disputes and Druze unwillingness to pay taxes and conscript into the Ottoman Army. The rebellion ended in brutal suppression of the Druze by General Sami Pasha al-Farouqi, significant depopulation of the Hauran region and execution of the Druze leaders in 1910. In the outcome of the revolt, 2,000 Druze were killed, a similar number wounded and hundreds of Druze fighters imprisoned. Al-Farouqi also disarmed the population, extracted significant taxes and launched a census of the region.
Modern history.
In Lebanon, Syria, Israel and Jordan, the Druze have official recognition as a separate religious community with its own religious court system. Druze are known for their loyalty to the countries they reside in, though they have a strong community feeling, in which they identify themselves as related even across borders of countries.
Despite their practice of blending with dominant groups to avoid persecution, and because the Druze religion does not endorse separatist sentiments but urges blending with the communities they reside in, the Druze have had a history of resistance to occupying powers, and they have at times enjoyed more freedom than most other groups living in the Levant.
In Syria.
In Syria, most Druze live in the Jebel al-Druze, a rugged and mountainous region in the southwest of the country, which is more than 90 percent Druze inhabited; some 120 villages are exclusively so. Other notable communities live in the Harim Mountains, the Damascus suburb of Jaramana, and on the southeast slopes of Mount Hermon. A large Syrian Druze community historically lived in the Golan Heights, but following wars with Israel in 1967 and 1973, many of these Druze fled to other parts of Syria; most of those who remained live in a handful of villages in the disputed zone, while only a few live in the narrow remnant of Quneitra Governorate that is still under effective Syrian control.
The Druze always played a far more important role in Syrian politics than its comparatively small population would suggest. With a community of little more than 100,000 in 1949, or roughly three percent of the Syrian population, the Druze of Syria's southwestern mountains constituted a potent force in Syrian politics and played a leading role in the nationalist struggle against the French. Under the military leadership of Sultan Pasha al-Atrash, the Druze provided much of the military force behind the Syrian Revolution of 1925–27. In 1945, Amir Hasan al-Atrash, the paramount political leader of the Jebel al-Druze, led the Druze military units in a successful revolt against the French, making the Jebel al-Druze the first and only region in Syria to liberate itself from French rule without British assistance. At independence the Druze, made confident by their successes, expected that Damascus would reward them for their many sacrifices on the battlefield. They demanded to keep their autonomous administration and many political privileges accorded them by the French and sought generous economic assistance from the newly independent government.
When a local paper in 1945 reported that President Shukri al-Quwatli (1943–49) had called the Druzes a "dangerous minority", Sultan Pasha al-Atrash flew into a rage and demanded a public retraction. If it were not forthcoming, he announced, the Druzes would indeed become "dangerous" and a force of 4,000 Druze warriors would "occupy the city of Damascus." Quwwatli could not dismiss Sultan Pasha's threat. The military balance of power in Syria was tilted in favor of the Druzes, at least until the military build up during the 1948 War in Palestine. One advisor to the Syrian Defense Department warned in 1946 that the Syrian army was "useless", and that the Druzes could "take Damascus and capture the present leaders in a breeze."
During the four years of Adib Shishakli's rule in Syria (December 1949 to February 1954) (on 25 August 1952: Adib al-Shishakli created the Arab Liberation Movement (ALM), a progressive party with pan-Arabist and socialist views), the Druze community was subjected to a heavy attack by the Syrian government. Shishakli believed that among his many opponents in Syria, the Druzes were the most potentially dangerous, and he was determined to crush them. He frequently proclaimed: "My enemies are like a serpent: the head is the Jebel al-Druze, the stomach Homs, and the tail Aleppo. If I crush the head the serpent will die." Shishakli dispatched 10,000 regular troops to occupy the Jebel al-Druze. Several towns were bombarded with heavy weapons, killing scores of civilians and destroying many houses. According to Druze accounts, Shishakli encouraged neighboring bedouin tribes to plunder the defenseless population and allowed his own troops to run amok.
Shishakli launched a brutal campaign to defame the Druzes for their religion and politics. He accused the entire community of treason, at times claiming they were agents of the British and Hashimites, at others that they were fighting for Israel against the Arabs. He even produced a cache of Israeli weapons allegedly discovered in the Jabal. Even more painful for the Druze community was his publication of "falsified Druze religious texts" and false testimonials ascribed to leading Druze sheikhs designed to stir up sectarian hatred. This propaganda also was broadcast in the Arab world, mainly Egypt. Shishakli was assassinated in Brazil on 27 September 1964 by a Druze seeking revenge for Shishakli's bombardment of the Jebel al-Druze.
He forcibly integrated minorities into the national Syrian social structure, his "Syrianization" of Alawite and Druze territories had to be accomplished in part using violence, he declared: "My enemies are like serpent. The head is the Jabal Druze, if I crush the head the serpent will die" (Seale 1963:132). To this end, al-Shishakli encouraged the stigmatization of minorities. He saw minority demands as tantamount to treason. His increasingly chauvinistic notions of Arab nationalism were predicated on the denial that "minorities" existed in Syria.
After the Shishakli's military campaign, the Druze community lost much of its political influence, but many Druze military officers played important roles in the Ba'ath government currently ruling Syria.
In 1967, a community of Druze in the Golan Heights came under Israeli control, today about 20,000 strong.
In Lebanon.
The Druze community in Lebanon played an important role in the formation of the modern state of Lebanon, and even though they are a minority they play an important role in the Lebanese political scene. Before and during the Lebanese Civil War (1975–90), the Druze were in favor of Pan-Arabism and Palestinian resistance represented by the PLO. Most of the community supported the Progressive Socialist Party formed by their leader Kamal Jumblatt and they fought alongside other leftist and Palestinian parties against the Lebanese Front that was mainly constituted of Christians. After the assassination of Kamal Jumblatt on 16 March 1977, his son Walid Jumblatt took the leadership of the party and played an important role in preserving his father's legacy after winning the Mountain War and sustained the existence of the Druze community during the sectarian bloodshed that lasted until 1990.
In August 2001, Maronite Catholic Patriarch Nasrallah Boutros Sfeir toured the predominantly Druze Chouf region of Mount Lebanon and visited Mukhtara, the ancestral stronghold of Druze leader Walid Jumblatt. The tumultuous reception that Sfeir received not only signified a historic reconciliation between Maronites and Druze, who fought a bloody war in 1983–84, but underscored the fact that the banner of Lebanese sovereignty had broad multi-confessional appeal and was a cornerstone for the Cedar Revolution in 2005. Jumblatt's post-2005 position diverged sharply from the tradition of his family. He also accused Damascus of being behind the 1977 assassination of his father, Kamal Jumblatt, expressing for the first time what many knew he privately suspected. The BBC describes Jumblatt as "the smartest leader of Lebanon's most powerful Druze clan and heir to a leftist political dynasty". The second largest political party supported by Druze is the Lebanese Democratic Party led by Prince Talal Arslan, the son of Lebanese independence hero Emir Majid Arslan.
In Israel.
The Druze form a religious minority in Israel of more than 100,000, mostly residing in the north of the country. In 2004, there were 102,000 Druze living in the country. In 2010, the population of Israeli Druze citizens grew to over 125,000. At the end of 2014 there were 140,000.
In 1957, the Israeli government designated the Druze a distinct ethnic community at the request of its communal leaders. The Druze are Arabic-speaking citizens of Israel and serve in the Israel Defense Forces just as most citizens do in Israel. Members of the community have attained top positions in Israeli politics and public service. The number of Druze parliament members usually exceeds their proportion in the Israeli population, and they are integrated within several political parties.
In Jordan.
The Druze form a religious minority in Jordan of around 32,000, mostly residing in the northwestern part of the country.
Beliefs.
God.
The Druze conception of the deity is declared by them to be one of strict and uncompromising unity. The main Druze doctrine states that God is both transcendent and immanent, in which he is above all attributes but at the same time he is present.
In their desire to maintain a rigid confession of unity, they stripped from God all attributes ("tanzīh"). In God, there are no attributes distinct from his essence. He is wise, mighty, and just, not by wisdom, might and justice, but by his own essence. God is "the whole of existence", rather than "above existence" or on his throne, which would make him "limited". There is neither "how", "when", nor "where" about him; he is incomprehensible.
In this dogma, they are similar to the semi-philosophical, semi-religious body which flourished under Al-Ma'mun and was known by the name of Mu'tazila and the fraternal order of the Brethren of Purity ("Ikhwan al-Ṣafa").
Unlike the "Mu'tazila", however, and similar to some branches of Sufism, the Druze believe in the concept of "Tajalli" (meaning "theophany"). "Tajalli" is often misunderstood by scholars and writers and is usually confused with the concept of incarnation.
Scriptures.
Druze Sacred texts include the Kitab Al Hikma (Epistles of Wisdom). Other ancient Druze writings include the "Rasa'il al-Hind (Epistles of India)" and the previously lost (or hidden) manuscripts such as "al-Munfarid bi-Dhatihi" and "al-Sharia al-Ruhaniyya" as well as others including didactic and polemic treatises.
Reincarnation.
Reincarnation is a paramount principle in the Druze faith. Reincarnations occur instantly at one's death because there is an eternal duality of the body and the soul and it is impossible for the soul to exist without the body. A human soul will only transfer to a human body in contrast to the Hindu and Buddhist belief system where souls can transfer to any living creature. Furthermore, a male Druze can only be reincarnated as another male Druze and a female Druze can only be reincarnated as another female Druze. A Druze cannot be reincarnated in the body of a non-Druze. Additionally, souls cannot be divided and the number of souls existing in the universe is finite. The cycle of rebirth is continuous and the only way to escape is through successive reincarnations. When this occurs, the soul is united with the Cosmic Mind and achieves the ultimate happiness.
Pact of Time Custodian.
The Pact of Time Custodian ("") is considered the entrance to the Druze religion, and they believe that all Druze in their past lives have signed this Charter, and Druze believe that this Charter embodies with human souls after death.
I rely on our Moula Al-Hakim the lonely God, the individual, the eternal, who is out of couples and numbers, (someone) the son of (someone) has approved recognition enjoined on himself and on his soul, in a healthy of his mind and his body, permissibility aversive is obedient and not forced, to repudiate from all creeds, articles and all religions and beliefs on the differences varieties, and he does not know something except obedience of almighty Moulana Al-Hakim, and obedience is worship and that it does not engage in worship anyone ever attended or wait, and that he had handed his soul and his body and his money and all he owns to almighty Maulana Al-Hakim.
The Druze also use a similar formula, called al-'ahd, when one is initiated into the ʻUqqāl.
Sanctuaries.
The prayer-houses of the Druze are called "khalwa" or "khalwat". The primary sanctuary of the Druze is at Khalwat al-Bayada.
Esotericism.
The Druze believe that many teachings given by prophets, religious leaders and holy books have esoteric meanings preserved for those of intellect, in which some teachings are symbolic and allegorical in nature, and divide the understanding of holy books and teachings into three layers.
These layers, according to the Druze, are as follows:
Druze do not believe that the esoteric meaning abrogates or necessarily abolishes the exoteric one. Hamza bin Ali refutes such claims by stating that if the esoteric interpretation of "taharah" (purity) is purity of the heart and soul, it doesn't mean that a person can discard his physical purity, as "salat" (prayer) is useless if a person is untruthful in his speech and that the esoteric and exoteric meanings complement each other.
Seven Druze precepts.
The Druze follow seven moral precepts or duties that are considered the core of the faith.
The Seven Druze precepts are:
Taqiyya.
Complicating their identity is the custom of ""—concealing or disguising their beliefs when necessary—that they adopted from Ismailism and the esoteric nature of the faith, in which many teachings are kept secretive. This is done in order to keep the religion from those who are not yet prepared to accept the teachings and therefore could misunderstand it, as well as to protect the community when it is in danger. Druzes tend to follow the dominant religion of the country where they reside. Some claim to be Muslim or Christian in order to avoid persecution; some do not. Druze in different states can have radically different lifestyles.
Other beliefs.
The Druze forbid divorce; circumcision is not necessary; those who purify and perfect their soul ascend to the stars upon death; when al-Hakim returns, all faithful Druze will join him in his march from China and on to conquer the world; apostasy is forbidden; religious services usually take place on Thursday evenings; they follow Sunni Hanafi law on issues which their own faith has no particular ruling; other influential figures of the religion include Plato, Aristotle, Socrates, Alexander the Great and Akhenaten.
Religious symbol.
The Druze strictly avoid iconography but use five colors (“Five Limits” ') as a religious symbol: green, red, yellow, blue, and white. Each color pertains to a metaphysical power called ', literally 'a limit', as in the boundaries that separate humans from animals, or the powers that makes the animal body human. Each " is color-coded in the following manner:
The mind generates qualia and gives consciousness. The soul embodies the mind and is responsible for transmigration and the character of oneself. The word which is the atom of language communicates qualia between humans and represent the platonic forms in the sensible world. The ' and ' is the ability to perceive and learn from the past and plan for the future and predict it.
The colors can be arranged in vertically descending stripes (as a flag) or a five-pointed star. The stripes are a diagrammatic cut of the spheres in neoplatonic philosophy, while the five-pointed star embodies the golden ratio, phi, as a symbol of temperance and a life of moderation.
Prayer houses and holy places.
Holy places of the Druze are archaeological sites important to the community and associated with religious holidays – the most notable example being Nabi Shu'ayb, dedicated to Jethro, who is a central figure of the Druze religion. Druze make pilgrimages to this site on the holiday of Ziyarat al-Nabi Shu'ayb.
One of the most important features of the Druze village having a central role in social life is the '—a house of prayer, retreat and religious unity. The ' may be known as "" in local languages.
The second type of religious shrine is one associated with the anniversary of a historic event or death of a prophet. If it is a mausoleum the Druze call it ' and if it is a shrine they call it '. The holy places become more important to the community in times of adversity and calamity. The holy places and shrines of the Druze are scattered in various villages, in places where they are protected and cared for. They are found in Syria, Lebanon and Israel.
ʻUqqāl and Juhhāl.
The Druzes do not recognize any religious hierarchy. As such, there is no "Druze clergy".
Given the strict religious, intellectual and spiritual requirements, most of the Druzes are not initiated and might be referred to as ' (), literally "the Ignorant", but in practice referring to the non-initiated Druzes; however, that term is seldom used by the Druzes. Those are not granted access to the Druze holy literature or allowed to attend the initiated religious meetings of the '. The cohesiveness and frequent inter-community social interaction however makes it in sort that that most Druzes have an idea about their broad ethical requirements and have some sense of what their theology consists of (albeit often flawed).
The initiated religious group, which includes both men and women (less than 10% of the population), is called ' ( "the Knowledgeable Initiates"). They might or might not dress differently, although most wear a costume that was characteristic of mountain people in previous centuries. Women can opt to wear ', a loose white veil, especially in the presence of other people. They wear ' on their heads to cover their hair and wrap it around their mouths. They wear black shirts and long skirts covering their legs to their ankles. Male ' often grow mustaches, and wear dark Levantine-Turkish traditional dresses, called the ', with white turbans that vary according to the seniority of the '. It is important to note that traditionally the Druze women have played an important role both socially and religiously inside the community.
' have equal rights to ', but establish a hierarchy of respect based on religious service. The most influential of ' become ', recognized religious leaders, and from this group the spiritual leaders of the Druze are assigned. While the ', which is an official position in Syria, Lebanon, and Israel, is elected by the local community and serves as the head of the Druze religious council, judges from the Druze religious courts are usually elected for this position. Unlike the spiritual leaders, the authority of the ' is limited to the country he is elected in, though in some instances spiritual leaders are elected to this position.
The Druze believe in the unity of God, and are often known as the "People of Monotheism" or simply "Monotheists". Their theology has a Neo-Platonic view about how God interacts with the world through emanations and is similar to some gnostic and other esoteric sects. Druze philosophy also shows Sufi influences.
Druze principles focus on honesty, loyalty, filial piety, altruism, patriotic sacrifice, and monotheism. They reject nicotine, alcohol, and other drugs, and often the consumption of pork (to those Uqqāl and not necessarily to be required by the Juhhāl). Druze reject polygamy, believe in reincarnation, and are not obliged to observe most of the religious rituals. The Druze believe that rituals are symbolic and have an individualistic effect on the person, for which reason Druze are free to perform them, or not. The community does celebrate Eid al-Adha, however, considered their most significant holiday.
Culture.
Cuisine.
Mate is a popular drink consumed by the Druze brought to the Levant from Lebanese migrants from Argentina in the 19th century. Mate is made by utilizing steeping dried leaves of yerba mate in hot water and is served with a metal straw ("bambija" or "masassa") from a gourd ("finjan"). Mate is often the first item served when entering a Druze home. It is a social drink and can be shared between multiple participants. After each drinker, the metal straw is cleaned with a lemon rind. Traditional snacks eaten with mate include raisins, nuts, dried figs, biscuits, and chips.
Origins.
Ethnic origins.
Arabian hypothesis.
The Druze faith extended to many areas in the Middle East, but most of the modern Druze can trace their origin to the "Wadi al-Taym" in South Lebanon, which is named after an Arab tribe "Taymour-Allah (formerly Taymour-Allat) " which, according to Islamic historian, al-Tabari, first came from Arabia into the valley of the Euphrates where they had been Christianized prior to their migration into the Lebanon. Many of the Druze feudal families whose genealogies have been preserved by the two modern Syrian chroniclers Haydar al-Shihabi and al-Shidyaq seem also to point in the direction of this origin. Arabian tribes emigrated via the Persian Gulf and stopped in Iraq on the route that was later to lead them to Syria. The first feudal Druze family, the Tanukh family, which made for itself a name in fighting the Crusaders, was, according to Haydar al-Shihabi, an Arab tribe from Mesopotamia where it occupied the position of a ruling family and apparently was Christianized.
Travelers like Niebuhr, and scholars like Von Oppenheim, undoubtedly echoing the popular Druze belief regarding their own origin, have classified them as Arabs. The prevailing idea among the Druzes themselves today is that they are of Arab stock.
Druze as a mixture of Middle Eastern tribes.
The 1911 edition of Encyclopædia Britannica states that the Druzes are "a mixture of refugee stocks, in which the Arab largely predominates, grafted on to an original mountain population of Aramaic blood."
The Tanukhs must have left Arabia as early as the second or third century AD. The Ma'an tribe, which superseded the Tanukhs and produced the greatest Druze hero, Fakhr-al-Din, had the same traditional origin. The "Talhuq" family and " 'Abd-al-Malik", who supplied the later Druze leadership, have the same record as the Tanukhs. The Imad family is named for "al-Imadiyyah" — the Kurdish town of Amadiya, northeast of Mosul inside Kurdistan, and, like the Jumblatts, is thought to be of Kurdish origin. The Arsalan family claims descent from the Hirah Arab kings, but the name "Arsalan" (Persian and Turkish for lion) suggests Persian influence, if not origin.
During the 18th century, there were two branches of Druze living in Lebanon: the Yemeni Druze, headed by the Harmouche and Alamuddine families; and the Kaysi Druze, headed by the Jumblatt and Arslan families. The Harmouche family was banished from Mount Lebanon following the battle of Ain Dara in 1711. The battle was fought between two Druze factions: the Yemeni and the Kaysi. Following their dramatic defeat, the Yemeni faction migrated to Syria in the Jebel-Druze region and its capital, Soueida. However, it has been argued that these two factions were of political nature rather than ethnic, and had both Christian and Druze supporters.
Iturean hypothesis.
According to Jewish contemporary literature, the Druze, who were visited and described in 1165 by Benjamin of Tudela, were pictured as descendants of the Itureans, an Ismaelite Arab tribe, which used to reside in the northern parts of the Golan plateau through Hellenistic and Roman periods. The word Druzes, in an early Hebrew edition of his travels, occurs as "Dogziyin", but it is clear that this is a scribal error.
Archaeological assessments of the Druze region have also proposed the possibility of Druze descending from Itureans, who had inhabited Mount Lebanon and Golan Heights in late classic antiquity, but their traces fade in the Middle Ages.
Genetics.
In a 2005 study of ASPM gene variants, Mekel-Bobrov et al. found that the Israeli Druze people of the Carmel region have among the highest rate of the newly evolved ASPM-haplogroup "D", at 52.2% occurrence of the approximately 6,000-year-old allele. While it is not yet known exactly what selective advantage is provided by this gene variant, the haplogroup D allele is thought to be positively selected in populations and to confer some substantial advantage that has caused its frequency to rapidly increase.
One small DNA study has shown that Israeli Druze are remarkable for the high frequency (35%) of males who carry the Y-chromosomal haplogroup L (though some Afshar village and the Ar-Raqqah Syrians have even more), which is otherwise uncommon in the Mideast (Shen et al. 2004). This haplogroup originates from prehistoric South Asia and has spread from Pakistan into southern Iran. However, studies done on larger samples showed that L-M20 averages 5% in Israeli Druze, 8% in Lebanese Druze, and it was not found in a sample of 59 Syrian Druze.
Cruciani in 2007 found E1b1b1a2 (E-V13) subclade of E1b1b1a (E-M78 in high levels (>10% of the male population) in Turkish Cypriot and Druze Arab lineages. Recent genetic clustering analyses of ethnic groups are consistent with the close ancestral relationship between the Druze and Cypriots, and also identified similarity to the general Syrian and Lebanese populations, as well as a variety of Jewish groups (Ashkenazi, Sephardi, Iraqi, and Moroccan) (Behar et al. 2010).
Also, a new study concluded that the Druze harbor a remarkable diversity of mitochondrial DNA lineages that appear to have separated from each other thousands of years ago. But instead of dispersing throughout the world after their separation, the full range of lineages can still be found within the Druze population.
The researchers noted that the Druze villages contained a striking range of high frequency and high diversity of the X haplogroup, suggesting that this population provides a glimpse into the past genetic landscape of the Near East at a time when the X haplogroup was more prevalent.
These findings are consistent with the Druze oral tradition, that claims that the adherents of the faith came from diverse ancestral lineages stretching back tens of thousands of years.
A 2008 study published on the genetic background of Druze communities in Israel showed highly heterogeneous parental origins. A total of 311 Israeli Druze were sampled: 37 from the Golan Heights, 183 from the Galilee, and 35 from Mount Carmel, as well as 27 Druze immigrants from Syria and 29 from Lebanon. The researchers found the following frequencies of Y-chromosomal haplogroups:

</doc>
<doc id="8633" url="https://en.wikipedia.org/wiki?curid=8633" title="December 12">
December 12


</doc>
<doc id="8637" url="https://en.wikipedia.org/wiki?curid=8637" title="Door">
Door

A door is a moving structure used to block off, and allow access to, an entrance to or within an enclosed space, such as a building or vehicle. Similar exterior structures are called gates. Typically, doors have an interior side that faces the inside of a space and an exterior side that faces the outside of that space. While in some cases the interior side of a door may match its exterior side, in other cases there are sharp contrasts between the two sides, such as in the case of the vehicle door. Doors normally consist of a panel that swings on hinges or that slides or spins inside of a space.
When open, doors admit people, animals, ventilation or light. The door is used to control the physical atmosphere within a space by enclosing the air drafts, so that interiors may be more effectively heated or cooled. Doors are significant in preventing the spread of fire. They also act as a barrier to noise. Many doors are equipped with locking mechanisms to allow entrance to certain people and keep out others. As a form of courtesy and civility, people often knock before opening a door and entering a room.
Doors are used to screen areas of a building for aesthetics, keeping formal and utility areas separate. Doors also have an aesthetic role in creating an impression of what lies beyond. Doors are often symbolically endowed with ritual purposes, and the guarding or receiving of the keys to a door, or being granted access to a door can have special significance. Similarly, doors and doorways frequently appear in metaphorical or allegorical situations, literature and the arts, often as a portent of change.
History.
The earliest in records are those represented in the paintings of the Egyptian tombs, in which they are shown as single or double doors, each in a single piece of wood. In Egypt, where the climate is intensely dry, there would be no fear of their warping, but in other countries it would be necessary to frame them, which according to Vitruvius (iv. 6.) was done with stiles (sea/si) and rails "(see: Frame and panel)": the spaces enclosed being filled with panels (tympana) let into grooves made in the stiles and rails. The stiles were the vertical boards, one of which, tenoned or hinged, is known as the hanging stile, the other as the middle or meeting stile. The horizontal cross pieces are the top rail, bottom rail, and middle or intermediate rails. The most ancient doors were in timber, those made for King Solomon's temple being in olive wood (I Kings vi. 31-35), which were carved and overlaid with gold. The doors dwelt upon in Homer would appear to have been cased in silver or brass. Besides olive wood, elm, cedar, oak and cypress were used. A 5,000-year-old door has been found by archaeologists in Switzerland.
All ancient doors were hung by pivots at the top and bottom of the hanging stile which worked in sockets in the lintel and sill, the latter being always in some hard stone such as basalt or granite. Those found at Nippur by Dr. Hilprecht dating from 2000 B.C. were in dolerite. The tenons of the gates at Balawat were sheathed with bronze (now in the British Museum). These doors or gates were hung in two leaves, each about wide and . high; they were encased with bronze bands or strips, 10 in. high, covered with repouss decoration of figures, etc. The wood doors would seem to have been about 3 in. thick, but the hanging stile was over diameter. Other sheathings of various sizes in bronze have been found, which proves this to have been the universal method adopted to protect the wood pivots. In the Hauran in Syria, where timber is scarce the doors were made in stone, and one measuring by is in the British Museum; the band on the meeting stile shows that it was one of the leaves of a double door. At Kuffeir near Bostra in Syria, Burckhardt found stone doors, 9 to . high, being the entrance doors of the town. In Etruria many stone doors are referred to by Dennis.
The ancient Greek and Roman doors were either single doors, double doors, triple doors, sliding doors or folding doors, in the last case the leaves were hinged and folded back. In Eumachia, is a painting of a door with three leaves. In the tomb of Theron at Agrigentum there is a single four-panel door carved in stone. In the Blundell collection is a bas-relief of a temple with double doors, each leaf with five panels. Among existing examples, the bronze doors in the church of SS. Cosmas and Damiano, in Rome, are important examples of Roman metal work of the best period; they are in two leaves, each with two panels, and are framed in bronze. Those of the Pantheon are similar in design, with narrow horizontal panels in addition, at the top, bottom and middle. Two other bronze doors of the Roman period are in the Lateran Basilica.
The Greek scholar Heron of Alexandria created the earliest known automatic door in the 1st century AD during the era of Roman Egypt. The first foot-sensor-activated automatic door was made in China during the reign of Emperor Yang of Sui (r. 604–618), who had one installed for his royal library. The first automatic gate operators were later created in 1206 by the Arabic inventor, Al-Jazari.
Copper and its alloys were integral in medieval architecture. The doors of the church of the Nativity at Bethlehem (6th century) are covered with plates of bronze, cut out in patterns.
Those of Hagia Sophia at Constantinople, of the 8th and 9th century, are wrought in bronze, and the west doors of the cathedral of Aix-la-Chapelle (9th century), of similar manufacture, were probably brought from Constantinople, as also some of those in St. Marks, Venice. The bronze doors on the Aachen Cathedral in Germany date back to about AD 800. Bronze baptistery doors at the Cathedral of Florence were completed in 1423 by Ghiberti. "(For more information, see: Copper in architecture)."
Of the 11th and 12th centuries there are numerous examples of bronze doors, the earliest being one at Hildesheim, Germany (1015). The Hildesheim design affected the concept of Gniezno door in Poland. Of others in South Italy and Sicily, the following are the finest: in Sant Andrea, Amalfi (1060); Salerno (1099); Canosa (1111); Troia, two doors (1119 and 1124); Ravello (1179), by Barisano of Trani, who also made doors for Trani cathedral; and in Monreale and Pisa cathedrals, by Bonano of Pisa. In all these cases the hanging stile had pivots at the top and bottom. The exact period when the hinge was substituted is not quite known, but the change apparently brought about another method of strengthening and decorating doors, viz, with wrought-iron bands of infinite varieties of design. As a rule three bands from which the ornamental work springs constitute the hinges, which have rings outside the hanging stiles fitting on to vertical tenons run into the masonry or wooden frame. There is an early example of the 12th century in Lincoln; in France the metal work of the doors of Notre Dame at Paris is perhaps the most beautiful in execution, but examples are endless throughout France and England.
Returning to Italy, the most celebrated doors are those of the Battistero di San Giovanni (Florence), which together with the door frames are all in bronze, the borders of the latter being perhaps the most remarkable: the modeling of the figures, birds and foliage of the south doorway, by Andrea Pisano (1330), and of the east doorway by Ghiberti (1425–1452), are of great beauty; in the north door (1402–1424) Ghiberti adopted the same scheme of design for the paneling and figure subjects in them as Andrea Pisano, but in the east door the rectangular panels are all filled, with bas-reliefs, in which Scripture subjects are illustrated with innumerable figures, these being probably the gates of Paradise of which Michelangelo speaks.
The doors of the mosques in Cairo were of two kinds; those which, externally, were cased with sheets of bronze or iron, cut out in decorative patterns, and incised or inlaid, with bosses in relief; and those in wood, which were framed with interlaced designs of the square and diamond, this latter description of work being Coptic in its origin. The doors of the palace at Palermo, which were made by Saracenic workmen for the Normans, are fine examples and in good preservation. A somewhat similar decorative class of door to these latter is found in Verona, where the edges of the stiles and rails are beveled and notched.
In the Renaissance period the Italian doors are quite simple, their architects trusting more to the doorways for effect; but in France and Germany the contrary is the case, the doors being elaborately carved, especially in the Louis XIV and Louis XV periods, and sometimes with architectural features such as columns and entablatures with pediment and niches, the doorway being in plain masonry. While in Italy the tendency was to give scale by increasing the number of panels, in France the contrary seems to have been the rule; and one of the great doors at Fontainebleau, which is in two leaves, is entirely carried out as if consisting of one great panel only.
The earliest Renaissance doors in France are those of the cathedral of St. Sauveur at Aix (1503). In the lower panels there are figures . high in Gothic niches, and in the upper panels a double range of niches with figures about . high with canopies over them, all carved in cedar. The south door of Beauvais Cathedral is in some respects the finest in France; the upper panels are carved in high relief with figure subjects and canopies over them. The doors of the church at Gisors (1575) are carved with figures in niches subdivided by classic pilasters superimposed. In St. Maclou at Rouen are three magnificently carved doors; those by Jean Goujon have figures in niches on each side, and others in a group of great beauty in the center. The other doors, probably about forty to fifty years later, are enriched with bas-reliefs, landscapes, figures and elaborate interlaced borders.
When it comes to the world's largest door, there is not just one, in fact there are four and they all belong to NASA's Vehicle Assembly Building at the Kennedy Space Center. The Vehicle Assembly Building was originally built to assembly of Apollo and Saturn vehicles and was then used to support Space Shuttle operations. Each of the four doors are 139 meters or 456 feet high, in comparison the Statue of Liberty is only 93 meters or 305 feet high.
The oldest door in England can be found in Westminster Abbey and dates from 1050. In England in the 17th century the door panels were raised with bolection or projecting moldings, sometimes richly carved, round them; in the 18th century the moldings worked on the stiles and rails were carved with the egg and tongue ornament.
Design and styles.
There are many kinds of doors, with different purposes. The most common type is the single-leaf door, which consists of a single rigid panel that fills the doorway. There are many variations on this basic design, such as the double-leaf door or double door and French windows, which have two adjacent independent panels hinged on each side of the doorway.
Types of mechanism.
Hinged doors.
Most doors are hinged along one side to allow the door to pivot away from the doorway in one direction, but not the other. The axis of rotation is usually vertical. In some cases, such as hinged garage doors, the axis may be horizontal, above the door opening.
Doors can be hinged so that the axis of rotation is not in the plane of the door to reduce the space required on the side to which the door opens. This requires a mechanism so that the axis of rotation is on the side other than that in which the door opens. This is sometimes the case in trains or airplanes, such as for the door to the toilet, which opens inward.
A swing door has special double-action hinges that allow it to open either outwards or inwards, and is usually sprung to keep it closed.
A selfbolting door has special hinges that permit the panel leaf to move laterally, so that the door itself becomes a giant bolt for better security. The selfbolting door principle can be used both for hinged doors as for rotating doors, as well as up-and-over doors (in the latter case, the bolts are then placed at top and bottom rather than at the sides).
French doors are derived from an original French design called the casement door. It is a door with lites where all or some panels would be in a casement door. A French door traditionally has a moulded panel at the bottom of the door. It is called a French window when used in a pair as double-leaved doors with large glass panels in each door leaf, and in which the doors may swing out (typically) as well as in.
A Mead door, developed by S Mead of Leicester, swings both ways. It is susceptible to forced entry due to its design.
A Dutch door or stable door consists of two halves. The top half operates independently from the bottom half. A variant exists in which opening the top part separately is possible, but because the lower part has a lip on the inside, closing the top part, while leaving the lower part open, is not.
A garden door resembles a French window (with lites), but is more secure because only one door is operable. The hinge of the operating door is next to the adjacent fixed door and the latch is located at the wall opening jamb rather than between the two doors or with the use of an espagnolette bolt.
Sliding doors.
It is often useful to have doors which slide along tracks, often for space or aesthetic considerations.
A bypass door is a door unit that has two or more sections. The doors can slide in either direction along one axis on parallel overhead tracks, sliding past each other. They are most commonly used in closets, in order to access one side of the closet at a time. The doors in a bypass unit will overlap slightly when viewed from the front, in order not to have a visible gap between them.
Doors which slide between two wall panels are called pocket doors.
Sliding glass doors are common in many houses, particularly as an entrance to the backyard. Such doors are also popular for use for the entrances to commercial structures, although they are not counted as fire exit doors. The door that moves is called the "active leaf", while the door that remains fixed is called the "inactive leaf".
Rotating doors.
A revolving door has several wings or leaves, generally four, radiating from a central shaft, forming compartments that rotate about a vertical axis. A revolving door allows people to pass in both directions without colliding, and forms an airlock maintaining a seal between inside and out.
A pivot door, instead of hinges, is supported on a bearing some distance away from the edge, so that there is more or less of a gap on the pivot side as well as the opening side. In some cases the pivot is central, creating two equal openings.
High-speed door.
A high-speed door is a very fast door some with opening speeds of up to 4 m/s, mainly used in the industrial sector where the speed of a door has an effect on production logistics, temperature and pressure control. High Speed Clean Room Doors are used in Pharmaceutical industries for the special curtain and stainless steel frames. They guarantee the tightness of all accesses. The powerful high-speed doors have a smooth surface structure and no protruding edges. Therefore, they can be easily cleaned and depositing of particles is largely excluded. High-speed doors are made to handle a high number of openings, generally more than 200000 a year. They need to be built with heavy duty parts and counterbalance systems for speed enhancement and emergency opening function. The door curtain was originally made of PVC, but was later also developed in aluminium and acrylic glass sections. High Speed refrigeration and cold room doors with excellent insulation values was also introduced with the Green and Energy saving requirements.
In North America, the Door and Access Systems Manufacturing Association (DASMA) defines high-performance doors as non-residential, powered doors, characterized by rolling, folding, sliding or swinging action, that are either high-cycle (minimum 100 cycles/day) or high-speed (minimum 20 inches(508 mm)/second), and two out of three of the following: made-to-order for exact size and custom features, designed to be able to withstand equipment impact (break-away if accidentally hit by vehicle) or designed to sustain heavy usage with minimal maintenance.
Automatic.
Automatically opening doors are powered open and closed either by electricity, spring, or both. There are several methods by which an automatically opening door is activated:
In addition to activation sensors automatically opening doors are generally fitted with safety sensors. These are usually an infrared curtain or beam, but can be a pressure mat fitted on the swing side of the door. The purpose of the safety sensor is to prevent the door from colliding with an object in its path by stopping or slowing its motion.A mechanism is set in modern automatic doors to ensure that door will be in open state in case of power failure.
Others.
Up-and-over or overhead doors are often used in garages. Instead of hinges it has a mechanism, often counterbalanced or sprung, that allows it to be lifted so that it rests horizontally above the opening. A roller shutter or sectional overhead door is one variant of this type.
A tambour door or roller door is an up-and-over door made of narrow horizontal slats and "rolls" up and down by sliding along vertical tracks and is typically found in entertainment centres and cabinets.
Inward opening doors are doors that can only be opened (or forced open) from outside a building. Such doors pose a substantial fire risk to occupants of occupied buildings when they are locked. As such doors can only be forced open from the outside, building occupants would be prevented from escaping. In commercial and retail situations manufacturers have included in the design a mechanism that allows an inward opening door to be pushed open outwards in the event of an emergency (which is often a regulatory requirement). This is known as a 'breakaway' feature. Pushing the door outward at its closed position, through a switch mechanism, disconnects power to the latch and allows the door to swing outward. Upon returning the door to the closed position, power is restored.
Rebated doors, a term chiefly used in Britain, are double doors having a lip or overlap (i.e. a Rabbet) on the vertical edge(s) where they meet. Fire-rating can be achieved with an applied edge-guard or astragal molding on the meeting stile, in accordance with the American Fire door.
Evolution Door is a trackless door that moves in the same closure level as a sliding door. The system is an invention of the Austrian artist Klemens Torggler. From the technical point of view the Evolution Door is a further development (evolution) of the or flip panel door that normally consists of two rotatable, connected panels which move to each other when opening.
Applications.
Architectural doors have numerous general and specialized uses. Doors are generally used to separate interior spaces (rooms, closets, etc.) for privacy, convenience, security, and safety reasons. Doors are also used to secure passages into a building from the exterior for reasons of safety and climate control.
Doors also are applied in more specialized cases:
Construction and components.
Panel doors.
Panel doors, also called stile and rail doors, are built with frame and panel construction. EN 12519 is describing the terms which are officially used in European Member States. The main parts are listed below:
Plank and batten doors.
Plank and batten doors are an older design consisting primarily of vertical slats:
Ledged and braced doors.
This type consists of vertical tongue and grooved boards held together with battens and diagonal braces.
Impact-resistant doors.
Impact-resistant doors have rounded stile edges to dissipate energy and minimize edge chipping, scratching and denting. The formed edges are often made of an engineered material such as Acrovyn. Impact-resistant doors excel in high traffic areas such as hospitals, schools, and hotels.
Frame and filled doors.
This type consists of a solid timber frame, filled on one face, face with Tongue and Grooved boards. Quite often used externally with the boards on the weather face.
Flush doors.
Many modern doors, including most interior doors, are flush doors:
Swing direction.
Door swings
For most of the world, door swings, or handing, are determined while standing on the outside or less secure side of the door while facing the door (i.e., standing on the side you use the key on, going from outside to inside, or from public to private).
It is especially important to get the hand and swing right on exterior doors, as the transom is usually sloped and sealed to resist water entry, and to properly drain. In some custom millwork (or with some master carpenters), the manufacture or installer will bevel the leading edge (the first edge to meet the jamb as the door is closing) so that the door fits tight without binding. Specifying an incorrect hand or swing will cause the door to bind, not close properly or leak (for exterior doors). Fixing this specification error will be expensive or time consuming. See note below for Australia where a different orientation is used.
In North America, many doors now come with factory-installed hinges, pre-hung on the jamb and sills.
To determine hand, stand on the outside (or less secure) side of the door. While facing the door, if the hinge is on the right side of the door, the door is "Right handed"; or if the hinge is on the left, it is "Left handed".
If the door swings toward you, it is "Reverse swing"; or if the door swings away from you, it is "Normal swing".
In other words:
Note: In Australia, this is different. The refrigerator rule applies (you can't stand in a fridge, the door always opens towards you) – If the hinges are on the left then it is a left hand (or left hung) door. If the hinges are on the right then it is a right hand (or right hung) door.
See the Australian Standards for Installation of Timber Doorsets, AS 1909–1984 pg 6.
In Europe one of the oldest DIN standard applies: DIN 107 "Building construction; identification of right and left side" (first 1922-05, current 1974-04) defines that doors are categorized from the side where the door hinges can be seen. If the hinges are on the left, it's a DIN Left door (DIN links, DIN gauche), if the hinges are on the right, it's a DIN Right door (DIN rechts, DIN droite). The DIN Right and DIN Left marking are also used to categorize matching installation material such as mortise locks (referenced in DIN 107). The European Standard DIN EN 12519 "Windows and pedestrian doors. Terminology" includes these definitions of orientation.
In public buildings, exterior doors should open to the outside in order to comply with any fire codes that may be in force in that jurisdiction. If the door opens inward and there is a fire, there can be a crush of people who run for the door and they will not be able to open it.
Types.
New exterior doors are largely defined by the type of materials they are made from: wood, steel, fiberglass, uPVC/Vinyl, aluminum, composite, glass (patio doors)...
Wooden doors – including solid wood doors – are a top choice for many homeowners, largely because of the aesthetic qualities of wood. Many wood doors are custom-made, but they have several downsides: their price, their maintenance requirements (regular painting and staining) and their limited insulating value (R-5 to R-6, not including the effects of the glass elements of the doors).
Steel doors are another major type of residential front doors; most of them come with a polyurethane or other type of foam insulation core – a critical factor in a building's overall comfort and efficiency.
Most modern exterior walls are designed to provide thermal insulation and energy efficiency, which can be indicated by the Energy Star label or the Passive House standards. And that’s where premium composite (including steel doors with a thick core of polyurethane or other foam), fiberglass and vinyl doors become interesting. From a thermal standpoint, these doors can benefit from the materials they are made from.
Insulation and weatherstrips.
But there are very few door models with an R-value close to 10 (which is far less than the R-40 walls or the R-50 ceilings of super-insulated buildings – Passive Solar and Zero Energy Buildings). Typical doors are not thick enough to provide very high levels of energy efficiency.
Many doors may have good R-values at their center, but their overall energy efficiency is reduced because of the presence of glass and reinforcing elements, or because of poor weatherstripping and the way the door is manufactured.
Door weatherstripping is particularly important for energy efficiency. German-made Passive House doors use multiple weatherstrips, including magnetic strips, to meet higher standards. These weatherstrips are critical to reduce to a minimum energy losses due to air leakage.
Dimensions.
United States.
The standard door sizes in the US run along 2" increments. Customary sizes have a height of 78" (1981 mm) or 80" (2032 mm) and a width of 18" (472 mm), 24" (610 mm), 26" (660 mm), 28" (711 mm), 30" (762 mm) or 36" (914 mm). Most residential passage (room to room) doors are 30" x 80" (762 mm x 2032 mm).
A standard US residential (exterior) door size is 36" x 80" (91 x 203 cm). Interior doors for wheelchair access must also have a minimum width of 3'-0" (91 cm). Residential interior doors are often somewhat smaller being 6'-8" high, as are many small stores, offices, and other light commercial buildings. Larger commercial, public buildings and grand homes often use doors of greater height. Older buildings often have smaller doors.
Thickness: Most pre-fabricated doors are 1 3/8" thick (for interior doors) or 1 3/4" (exterior).
Closets: small spaces such as closets, dressing rooms, half-baths, storage rooms, cellars, etc. often are accessed through doors smaller than passage doors in one or both dimensions but similar in design.
Garages: Garage doors are generally 7'-0" or 8'-0" wide for a single-car opening. Two car garage doors (sometimes called double car doors) are a single door 16'-0". Because of size and weight these doors are usually sectional. That is split into four or five horizontal sections so that they can be raised more easily and don't require a lot of additional space above the door when opening and closing. There are single piece double garage doors found in some older homes. circa 1950's
Europe.
The standard DIN doors are defined in DIN 18101 (published 1955-07, 1985-01, 2014-08). The door sizes are also given in the construction standard for wooden door panels (DIN 68706-1). The DIN commission was also responsible for the harmonized European standard DIN EN 14351-1 for exterior doors and DIN EN 14351-2 for interior doors (published 2006-07, 2010-08), which defines the requirements for the CE marking giving standard sizes by examples in the appendix.
The DIN 18101 standard has a normative size (Nennmaß) slightly larger than the panel size (Türblatt) as the standard derives the panel sizes from the normative size being different single door vs double door and molded vs unmolded doors. The DIN 18101/1985 defines interior single molded doors to have a common panel height of 1985 mm (normativ height 2010 mm) at panel widths of 610 mm, 735 mm, 860 mm, 985 mm, 1110 mm, plus a larger door panel size of 1110 mm x 2110 mm. The newer DIN 18101/2014 drops the definition of just five standard door sizes in favor of a basic raster running along 125 mm increments where the height and width are independent. The panel width may be in the range 485 mm to 1360 mmm, and the height may be in the range of 1610 mm to 2735 mm. The most common interior door is 860 mm x 1985 mm (33.8" x 78.1").
Doorway components.
When framed in wood for snug fitting of a door, the doorway consists of two vertical "jambs" on either side, a "lintel" or "head jamb" at the top, and perhaps a "threshold" at the bottom. When a door has more than one movable section, one of the sections may be called a "leaf". See door furniture for a discussion of attachments to doors such as door handles, doorknobs, and door knockers.
Related hardware.
Door furniture or hardware refers to any of the items that are attached to a door or a drawer to enhance its functionality or appearance. This includes items such as hinges, handles, door stops, etc.
Related accidents.
Door safety relates to prevention of door-related accidents. Such accidents take place in various forms, and in a number of locations; ranging from car doors to garage doors. Accidents vary in severity and frequency. According to the National Safety Council in the United States, 300,000 injuries are caused by doors every year.
The types of accidents vary from relatively minor cases where doors cause damage to other objects, such as walls, to serious cases resulting in human injury, particularly to fingers. A closing door can exert up to 40 tons per square inch of pressure between the hinges. Because of the number of accidents taking place, there has been a surge in the number of lawsuits. Thus organisations may be at risk when car doors or doors within buildings are unprotected.
According to the US General Services Administration:
Outward and inward opening.
Whenever a door is opened outwards there is a risk that it could strike another person. In many cases this can be avoided by architectural design which favors doors which open inwards into rooms (from the perspective of a common area such as a corridor, the door opens outwards). In cases where this is infeasible, it may be possible to avoid an accident by placing windows in the door.
However, inward-hinged doors can also escalate an accident by preventing people from escaping the building: people inside the building may press against the doors, and thus prevent the doors from opening. This was partly the case in the Grue Church fire in Norway in 1822.
Today, the exterior doors of most large (especially public) buildings open outward, while interior doors such as doors to individual rooms, offices, suites, etc. open inward, as do many exterior doors of houses, particularly in North America.
Doorstops.
Doorstops are simple devices used to prevent a door from coming into contact with another object (typically a wall). Without the door stop damage might be done to the wall. They may either absorb the force of a moving door, or hold the door in place to prevent unintended motion.
Door guards.
The purpose of door guards (also known as hinge guards, anti-finger trapping devices, or finger guards) is to reduce the number of finger trapping accidents in doors, as doors pose a risk to children especially when closing. Door guards protect fingers in door hinges by covering the gap that is created by opening doors by covering the hinges of doors with a piece of rubber or plastic that wraps from the door frame to the door. There are also door safety products which eject the fingers from the push side of the door as it is being closed.
There are various levels of door protection. Front door protection a front anti-finger trapping device but leaves the rear hinge pin side of the door unprotected. Full door protection uses front and rear anti-finger trapping devices and ensures the hinge side of a door is fully protected.
Which level of protection is appropriate should be determined by a risk assessment of the door.
There is also handle-side door protection, which prevents the door from slamming shut on the frame, which can cause injury to fingers/hands.
Safety doors.
A safety door prevents finger injuries near the hinges without the use of door guards. Rather than cover the danger area, the approach is to change the shape of the door so that an accessible gap does not form in the first place. This is achieved by adding a perfectly circular ("bull-nose" shaped) extension to the door, which moves in and out of a cavity as the door opens and closes. This prevents any part of a hand being crushed near the hinges – either inside or outside. These doors have an operating range of slightly over 90 degrees, so their use is limited to where they come into contact with a side wall when fully open (or where they can be prevented from opening too far by a doorstop).
Glass doors.
Glass doors pose the risk of unintentional collision if a person believes the door to be open when it is closed, or is unaware there is a door at all. This risk may be particularly pronounced with sliding glass doors because they often have large single panes which are hard to see. To prevent injury from glass doors, stickers or other types of warnings are sometimes placed on the glass surface to make it more visible. For instance, in the UK, Regulation 14 of the Workplace (Health and Safety Regulations) 1992 requires the marking of windows and glass doors to make them conspicuous. Australian Standards: AS1288 and AS2208 require glass doors to be made from laminated or toughened glass.
Fire.
Special purpose fire doors are often employed in buildings to reduce the overall risk of fire, particularly by preventing the spread of fire and smoke. In cases where they are improperly installed, employed, or tampered with, the risk of fire can be increased. Door closers are sometimes used to ensure fire doors remain closed.
An additional risk in a fire is that doors may prevent access to emergency services personnel in order to fight the fire, rescue occupants, etc. Door breaching techniques may be required in these situations to gain access.
Panic bars are often used in buildings so that a door locked from the exterior can quickly and easily be opened from the inside in the event of a fire or other emergency.
Vehicle doors.
There may be an increased risk of trapping hands or fingers in car doors compared to other types of doors, due to the proximity in which the occupant sits. In some car accidents, injury to occupants from the movement of car doors may occur.
Bicyclists often fear collision with an opening car door in case the car's occupant does not look carefully to check that it is safe to open the door. Because cyclists often ride near parked cars along the side of the road (see door zone) they are particularly vulnerable.
Aircraft doors.
Doors which lead from interior, pressurized, sections of an aircraft to exterior or unpressurized areas can pose extreme risk if they are inadvertently opened during flight. This can be mitigated by having doors that open inwardly and are designed to be forced into their door frames by the internal cabin pressure – most cabin doors are of this type. However, an outward opening door is often advantageous for cargo doors to maximise available space, and these need to be secured by hefty locking mechanisms to overcome internal pressure.
A number of accidents have occurred where outward-opening aircraft doors were opened in flight, often accidentally:

</doc>
<doc id="8640" url="https://en.wikipedia.org/wiki?curid=8640" title="Database normalization">
Database normalization

Database normalization (or normalisation) is the process of organizing the columns (attributes) and tables (relations) of a relational database to minimize data redundancy.
Normalization involves decomposing a table into less redundant (and smaller) tables without losing information; defining foreign keys in the old table referencing the primary keys of the new ones. The objective is to isolate data so that additions, deletions, and modifications of an attribute can be made in just one table and then propagated through the rest of the database using the defined foreign keys.
Edgar F. Codd, the inventor of the relational model (RM), introduced the concept of normalization and what we now know as the First normal form (1NF) in 1970. Codd went on to define the Second normal form (2NF) and Third normal form (3NF) in 1971, and Codd and Raymond F. Boyce defined the Boyce-Codd Normal Form (BCNF) in 1974. Informally, a relational database table is often described as "normalized" if it meets Third Normal Form. Most 3NF tables are free of insertion, update, and deletion anomalies.
The relational model separates the logical design from the physical design: DBMS performance is a matter of physical design using indexes, view materialization, big buffers, etc. It is not a matter of changing the logical design.
A typical example of normalization is that an entity's unique ID is stored everywhere in the system but its name is held in only one table. The name can be updated more easily in one row of one table. A typical update in such an example would be the RIM company changing its name to BlackBerry. That update would be done in one place and immediately the correct "BlackBerry" name would be displayed throughout the system.
Objectives.
Normalization, in general, is the process of applying a set of rules with the goal of organizing something.
A basic objective of the first normal form defined by Codd in 1970 was to permit data to be queried and manipulated using a "universal data sub-language" grounded in first-order logic. (SQL is an example of such a data sub-language, albeit one that Codd regarded as seriously flawed.)
The objectives of normalization beyond 1NF (First Normal Form) were stated as follows by Codd:
The sections below give details of each of these objectives.
Free the database of modification anomalies.
When an attempt is made to modify (update, insert into, or delete from) a table, undesired side-effects may arise in tables that have not been sufficiently normalized. An insufficiently normalized table might have one or more of the following characteristics:
Minimize redesign when extending the database structure.
When a fully normalized database structure is extended to allow it to accommodate new types of data, the pre-existing aspects of the database structure can remain largely or entirely unchanged. As a result, applications interacting with the database are minimally affected.
Normalized tables, and the relationship between one normalized table and another, mirror real-world concepts and their interrelationships.
Example.
Querying and manipulating the data within a data structure that is not normalized, such as the following non-1NF representation of customers' credit card transactions, involves more complexity than is really necessary:
<br>
To each customer corresponds a "repeating group" of transactions. The automated evaluation of any query relating to customers' transactions therefore would broadly involve two stages:
For example, in order to find out the monetary sum of all transactions that occurred in October 2003 for all customers, the system would have to know that it must first unpack the "Transactions" group of each customer, then sum the "Amounts" of all transactions thus obtained where the "Date" of the transaction falls in October 2003.
One of Codd's important insights was that this structural complexity could always be removed completely, leading to much greater power and flexibility in the way queries could be formulated (by users and applications) and evaluated (by the DBMS). The normalized equivalent of the structure above would look like this:
Now each row represents an individual credit card transaction, and the DBMS can obtain the answer of interest, simply by finding all rows with a Date falling in October, and summing their Amounts. The data structure places all of the values on an equal footing, exposing each to the DBMS directly, so each can potentially participate directly in queries; whereas in the previous situation some values were embedded in lower-level structures that had to be handled specially. Accordingly, the normalized design lends itself to general-purpose query processing, whereas the unnormalized design does not. The normalized version also allows the user to change the customer name in one place and guards against errors that arise if the customer name is misspelled on some records.

</doc>
<doc id="8641" url="https://en.wikipedia.org/wiki?curid=8641" title="Desmothoracid">
Desmothoracid

Order Desmothoracida, the desmothoracids, are a group of heliozoan protists, usually sessile and found in freshwater environments. The adult is a spherical cell around 10-20 μm in diameter surrounded by a perforated organic lorica, or shell, with many radial pseudopods projecting through the holes to capture food. These are supported by small bundles of microtubules that arise near a point on the nuclear membrane. Unlike other heliozoans, the microtubules are not in any regular geometric array, there does not appear to be a microtubule organizing center, and there is no distinction between the outer and inner cytoplasm.
Reproduction takes place by the budding off of small motile cells, usually with two flagella. Later these are lost, and the pseudopods and lorica are formed. Typically, a single lengthened pseudopod will secrete a hollow stalk that attaches the cell to the substrate. The form of the flagella, the tubular cristae within the mitochondria, and other characters have led to the suggestion that the desmothoracids belong among what is now the Cercozoa. This was later confirmed by genetic studies.
As of the year 2000, the order Desmothoracida contained five genera with a total of 10 species.

</doc>
<doc id="8642" url="https://en.wikipedia.org/wiki?curid=8642" title="Dalhousie University">
Dalhousie University

Dalhousie University (commonly known as Dal) is a public research university in Nova Scotia, Canada, with three campuses in Halifax, and a fourth in Bible Hill. Dalhousie offers more than 4,000 courses and 180 degree programs in twelve undergraduate, graduate, and professional faculties. The university is a member of the U15, a group of research-intensive universities in Canada.
Dalhousie was established as a nonsectarian college in 1818 by the eponymous Lieutenant Governor of Nova Scotia, George Ramsay, 9th Earl of Dalhousie. The college did not hold its first class until 1838, until then operating sporadically due to financial difficulties. It reopened for a third time in 1863 following a reorganization that brought a change of name to "The Governors of Dalhousie College and University". The university formally changed its name to "Dalhousie University" in 1997 through provincial legislation, the same legislation that merged the institution with the Technical University of Nova Scotia.
The university's notable alumni include a Nobel Prize winner, two Canadian Prime Ministers, two Herzberg Prize winners, a NASA astronaut who was the first American woman to walk in space, 89 Rhodes Scholars, and a range of other top government officials, academics, and business leaders. The university ranked 235th in the 2014 QS World University Rankings, 226-250th in the 2014-2015 Times Higher Education World University Rankings, and 201–300th in the 2014 Academic Ranking of World Universities. Dalhousie is a centre for marine research, and is host to the headquarters of the Ocean Tracking Network.
The Dalhousie library system operates the largest library in Atlantic Canada, and holds the largest collection of agricultural resource material in the region. The university operates a total of fourteen residences. There are currently two student unions that represent student interests at the university: the Dalhousie Student Union and the Dalhousie Association for Graduate Students. Dalhousie's varsity teams, the Tigers, compete in the Atlantic University Sport conference of Canadian Interuniversity Sport. Dalhousie's Faculty of Agriculture varsity teams are called the Dalhousie Rams, and compete in the ACAA and CCAA. Dalhousie is a coeducational university with more than 18,000 students and 110,000 alumni.
History.
Dalhousie was founded as the Lieutenant Governor of Nova Scotia George Ramsay, 9th Earl of Dalhousie desired a non-denominational college in Halifax. Financing largely came from customs duties collected by a previous Lieutenant Governor, John Coape Sherbrooke, during the War of 1812 occupation of Castine, Maine; Sherbrooke invested GBP£7,000 as an initial endowment and reserved £3,000 for the physical construction of the college. The college was established in 1818, though it faltered shortly after as Ramsay left Halifax to serve as the Governor General of British North America. The school was structured upon the principles of the University of Edinburgh, where lectures were open to all, regardless of religion or nationality. The University of Edinburgh was located near Ramsay's home in Scotland.
In 1821 Dalhousie College was officially incorporated by the Nova Scotia House of Assembly under the 1821 Act of Incorporation. The college did not hold its first class until 1838; operation of the college was intermittent and no degrees were awarded. In 1841 an Act of the Nova Scotia House of Assembly conferred university powers on Dalhousie.
In 1863 the college opened for a third time and was reorganized by another legislative act, which added "University" to the school's name: "The Governors of Dalhousie College and University". Dalhousie reopened with six professors and one tutor. When it awarded its first degrees in 1866 the student body consisted of 28 students working toward degrees and 28 occasional students. The first female graduate was Margaret Florence Newcome from Grafton, Nova Scotia, who earned her degree in 1885. Despite the reorganization and an increase in students, money continued to be a problem for the institution. In 1879, amid talks of closure due to the university's dire financial situation, a wealthy New York publisher with Nova Scotian roots, George Munro, began to donate to the university; Munro was brother-in-law to Dalhousie's Board of Governors member John Forrest. Munro is credited with rescuing Dalhousie from closure, and in honour of his contributions Dalhousie observes a university holiday called George Munro Day on the first Friday of each February.
Originally located at the space now occupied by Halifax City Hall, the college moved in 1886 to Carleton Campus and spread gradually to Studley Campus. Dalhousie grew steadily during the 20th century. From 1889 to 1962 the Halifax Conservatory was affiliated with and awarded degrees through Dalhousie. In 1920 several buildings were destroyed by fire on the campus of the University of King's College in Windsor, Nova Scotia. Through a grant from the Carnegie Foundation, King's College relocated to Halifax and entered into a partnership with Dalhousie that continues to this day.
Dalhousie expanded on 1 April 1997 when provincial legislation mandated an amalgamation with the nearby Technical University of Nova Scotia. This merger saw reorganization of faculties and departments to create the Faculty of Engineering, Faculty of Computer Science and the Faculty of Architecture and Planning. From 1997 to 2000, the Technical University of Nova Scotia operated as a constituent college of Dalhousie called Dalhousie Polytechnic of Nova Scotia (DalTech) until the collegiate system was dissolved. The legislation that merged the two schools also formally changed the name of the institution to its present form, Dalhousie University. On 1 September 2012 the Nova Scotia Agricultural College merged into Dalhousie to form a new Faculty of Agriculture, located in Bible Hill, Nova Scotia.
Campuses.
Dalhousie has three campuses within the Halifax Peninsula and a fourth, the Agricultural Campus, in Bible Hill, Nova Scotia. The largest, Studley Campus, serves as the primary campus; it houses the majority of the university's academic buildings such as faculties, athletic facilities, and the university's Student Union Building. The campus is largely surrounded by residential neighbourhoods. Robie Street divides it from the adjacent Carleton Campus, which houses the faculties of dentistry, medicine, and other health profession departments. The campus is adjacent to two large teaching hospitals affiliated with the school: the IWK Health Centre and the Queen Elizabeth II Health Sciences Centre.
Sexton Campus in Downtown Halifax hosts the engineering, architecture and planning faculties. Sexton Campus served as the campus of the Technical University of Nova Scotia prior to amalgamation. The Agricultural Campus in Bible Hill, a suburban community of Truro, Nova Scotia, served as the campus for the Nova Scotia Agricultural College prior to its merger with Dalhousie in 2011.
The buildings at Dalhousie vary in age from Hart House, which was completed in 1864, to the Collaborative Health Education Building, completed in 2015. The original building of Dalhousie University was completed in 1824 on Halifax's Grand Parade. It was demolished in 1885 when the university outgrew the premises, and the City of Halifax sought possession of the entire Grand Parade. Halifax City Hall presently occupies the site of the original Dalhousie College.
Libraries and museums.
The university has five libraries. The largest, Killam Memorial Library, opened in 1971. It is the largest academic library in Atlantic Canada with over one million books and 40,000 journals. The library's collection largely serves the faculties of arts and social sciences, sciences, management, and computer science. The W. K. Kellogg Health Science Library provides services largely for the faculties of dentistry, medicine, and other health professions. The Sexton Design & Technology Library is located within Sexton Campus. Its collection largely serves those in the faculties of engineering, architecture and planning, and houses the university's rare books collection. The Sir James Dunn Law Library holds the university's collection of common law materials, legal periodicals, as well as books on international law, health law, and environmental law. MacRae Library is located at the university's Agricultural Campus, and has the largest collection of agricultural resource material in Atlantic Canada. The Dalhousie University Archives houses official records of, or relating to, or people/activities connected with Dalhousie University and its founding institutions. The archives also houses material related to theatre, business and labour in Nova Scotia. The collection consists of manuscripts, texts, photographs, audio-visual material, microfilm, music, and artifacts.
The biology department operates the Thomas McCulloch Museum. The most notable of the museum's exhibits is its preserved birds collection. Other exhibits include its collection of lorenzen ceramic mushrooms, its coral and shell collection, and its butterfly and insect collection. The museum's namesake Thomas McCulloch was a Scottish Presbyterian minister who served as Dalhousie's first president and created the Audubon mounted bird collection which is now housed at the museum.
The Dalhousie Art Gallery is both a public gallery and an academic support unit housed since 1971 on the lowest level of the Dalhousie Arts Centre. Admission is free of charge. It is host to a permanent collection of over 1000 works. Some of the outdoor sculptures around the campus are part of this collection, such as the distinctive "Marine Venus" which has sat in the median of University Avenue since 1969.
Housing and student facilities.
The university has eleven student residences throughout its Halifax campuses: Eliza Ritchie Hall, Gerard Hall, Howe Hall, LeMarchant Place, Mini Rez, O'Brien Hall, Residence Houses, Risley Hall, Shirreff Hall, Glengary Apartments, and Graduate House. The largest, Howe Hall in Studley Campus, houses 716 students during the academic year. Howe Hall's most recent addition to the residence is called Fountain. It is the only residence in Howe Hall to have a sink in every room. The university also operates three residences in its Agricultural Campus: Chapman House, Fraser House, and Truman House. The largest residence in the Agricultural Campus is Chapman House, housing 125 students during the academic year. The residences are represented by a Residence Council responsible for resident concerns, providing entertainment services, organizing events, and upholding rules and regulations.
The Student Union Building serves as the main student activity centre. Completed in 1968, it is located in the Studley Campus. The Student Union building hosts a number of student societies and organization offices, most notably the Dalhousie Student Union. The building houses five restaurants, both independently owned and international franchises such as Tim Hortons.
Sustainability.
Dalhousie University is actively involved in sustainability issues and has received a number of sustainability awards and recognition for academic programs, university operations, and research. In 2015, Dalhousie received a GOLD rating from AASHE STARS (Version 2.0). In 2009, the university signed the University and College Presidents’ Climate Change Statement of Action for Canada to reduce emissions of greenhouse gases. Dalhousie is also a signatory of UNEP's International Declaration on Cleaner Production. In 1999, the university signed the Talloires Declaration, which committed Dalhousie and other higher education institutions to developing, creating, supporting, and maintaining sustainability.
In 2008, the College of Sustainability, the Office of Sustainability, and the Dalhousie Student Union Sustainability Office were formed. During 2008, the President’s Advisory Council on Sustainability was also created. The council meets quarterly to discuss pan-university sustainability issues. Dalhousie’s international award-winning College of Sustainability offers an undergraduate Major in Environment, Sustainability and Society (ESS) integrating with seven bachelor's degrees and forty subjects across five faculties. The College of Sustainability offers a virtual and a physical space for the intersection of interdisciplinary collaboration, conversation, and teaching with a core of cross-appointed Dalhousie faculty members joined by visiting fellows, distinguished guest lecturers, community leaders, and environmental advocates. In addition, the Sustainability Leadership Certificate program offers students the opportunity to participate in an engaged exploration of personal and group leadership and empowers their sense of personal agency to address environmental and social change.
The Office of Sustainability spearheads a number of campus sustainability plans and policies including the Climate Change Plan, Natural Environment Plan, and Green Building policy. A number of initiatives have been developed and implemented with campus partners including numerous energy and water retrofits, residence Eco-Olympics competition, an Employee Sustainability Leadership Program, and an Employee Bus Pass. The Dalhousie Student Union Sustainability Office promotes awareness and behaviour change. DSUSO hosts “Green Week,” the “Green Gala” and the “Greenie Awards” to celebrate campus accomplishments on sustainability. A number of student societies are also active in sustainability issues from on-campus gardening and food security to environmental law.
Administration.
University governance is conducted through the Board of Governors and the Senate, both of which were given much of their present power in the Unofficial Consolidation of an Act for the Regulation and Support of Dalhousie College in Chapter 24 of the Acts of 1863. This statute replaced ones from 1820, 1823, 1838, 1841 and 1848, and has since been supplemented 11 times, most recently in 1995. The Board is responsible for conduct, management, and control of the university and of its property, revenues, business, and affairs. Board members, known as Governors of the Board, include the university's chancellor, president, and 25 other members. Members include people from within the university community such as four approved representatives from Dalhousie Student Union, and those in the surrounding community, such as the Mayor of Halifax. The Senate is responsible for the university's academics, including standards for admission and qualifications for degrees, diplomas, and certificates. The Senate consists of 73 positions granted to the various faculty representatives, academic administrators, and student representatives.
The president acts as the chief executive officer and is responsible to the Board of Governors and to the Senate for the supervision of administrative and academic works.Richard Florizone is the 11th president of the university, and has served since 2013. Thomas McCulloch served as the first president when the office was created in 1838. John Forrest was the longest-serving president, holding the office from 1885 to 1911.
Affiliated institutions.
The University of King's College is a post-secondary institution in Halifax affiliated with Dalhousie. Established in 1789, it was the first post-secondary institution in English Canada and the oldest English-speaking Commonwealth university outside the United Kingdom. The University of King's College was formerly an independent institution located in Windsor, Nova Scotia, until 1920, when a fire ravaged its campus. To continue operation, the University of King's College accepted a generous grant from the Carnegie Foundation, although the terms of the grant required that it move to Halifax and enter into association with Dalhousie. Under the agreement, King's agreed to pay the salaries of a number of Dalhousie professors, who in turn were to help in the management and academic life of the college. Students at King's were to have access to all of the amenities Dalhousie, and the academic programs at King's would fold into the College of Arts and Sciences at Dalhousie. Presently, students of both institutions are allowed to switch between the two throughout their enrolment. In spite of the shared academic programs and facilities, the University of King's College maintains its own scholarships, bursaries, athletics programs, and student residences.
Finances.
The university completed the 2011–12 year with revenues of $573.597 million and expenses of $536.451 million, yielding a surplus of $37.146 million. The largest source of revenue for the university was provincial operational grants, which made up 32 percent of revenue. Tuition fees generated $123.2 million in the 2011–12 fiscal year, making up 21 percent of revenue. , Dalhousie's endowment was valued at $400.6 million.
Academics.
Dalhousie is a publicly funded research university, and a member of the Association of Universities and Colleges of Canada, as well as the U15, a group of Canadian research-intensive universities. , there were 18,220 students enrolled at the university and 3,700 courses in over 190 degree programs. Dalhousie offers more than 3,700 courses and 190 degree programs in twelve undergraduate, graduate, and professional faculties. The requirements for admission differ between students from Nova Scotia, students from other provinces in Canada, and international students due to lack of uniformity in marking schemes. The requirements for admission also differ depending on the program. In 2011, the secondary school average for incoming first-year undergraduate students was 85 percent.
Canadian students may apply for financial aid such as the Nova Scotia Student Assistance Program and Canada Student Loans and Grants through the federal and provincial governments. Financial aid may also be provided in the form of loans, grants, bursaries, scholarships, fellowships, debt reduction, interest relief, and work programs.
Reputation.
The 2015-2016 Times Higher Education World University Rankings placed Dalhousie 201–250th in the world. The 2014 QS World University Rankings ranked the university 235th. According to the 2014 Academic Ranking of World Universities rankings, the university ranked 201–300 in the world and 8–17th in Canada. In terms of national rankings, "Maclean's" ranked Dalhousie 7th in their 2014 Medical Doctoral university rankings. Dalhousie was ranked in spite of having opted out—along with several other universities in Canada—of participating in Maclean's graduate survey since 2006.
A number of Dalhousie's individual programs and faculties have gained accolades nationally and internationally. In Maclean's 2013 common law school rankings, the Schulich School of Law placed 6th in Canada. In the QS rankings of law programs, the university placed 51–100 in the world. The Rowe School of Business was named the most innovative business school in Canada by European CEO magazine on 17 November 2010.
Research.
Dalhousie University is a member of the U15, a group that represents 15 of Canada’s most research-intensive universities. Out of 50 universities in Canada, Research Infosource ranked Dalhousie University the 16th most research-intensive for 2011, with a sponsored research income of $125.147 million, averaging $124,500 per faculty member. In 2003 and 2004, "The Scientist" placed Dalhousie among the top five places in the world outside the United States for postdoctoral work and conducting scientific research. In 2007 Dalhousie topped the list of "The Scientist"’s “Best Places to Work in Academia”. The annual list divides research and academic institutions into American and international lists; Dalhousie University ranked first in the international category. According to a survey conducted by "The Scientist", Dalhousie was the best non-commercial scientific institute in which to work in Canada.
In terms of research performance, High Impact Universities 2010 ranked Dalhousie 239th out of 500 universities, and 12th in Canada. The university was ranked 194th out of 500 universities and 12th in the country for research performance in the fields of medicine, dentistry, pharmacology, and health sciences. The Higher Education Evaluation and Accreditation Council of Taiwan (HEEACT) ranked Dalhousie 279th in the world and 12th in Canada for its 2011 scientific paper's performances. HEEACT had also ranked Dalhousie 86th in the world and fourth nationally for research performance in geoscience in its 2010 rankings.
Marine research at Dalhousie has become a large focus of the university, with many of the university's faculty members involved in some form of marine research. Notably, Dalhousie is the headquarters of the Ocean Tracking Network, a research effort using implanted acoustic transmitters to study fish migration patterns. Dalhousie houses a number of marine research pools, a wet laboratory, and a benthic flume, which are collectively known as the Aquatron laboratory. Dalhousie is one of the founding members of the Halifax Marine Research Institute, founded on 2 June 2011. The institute, which is a partnership between a number of private industries, government, and post-secondary institutions, was designed to help increase the scale, quality, internationalization and impact of marine research in the region. In 2011, the university, along with WWF-Canada, created the Conservation Legacy For Oceans, which aimed at providing scholarships, funding, curriculum development, and work placements for students and academics dedicated to marine research, law, management, and policy making.
Many of Dalhousie's faculties and departments focus on marine research. The Faculty of Engineering operates the Ocean Research Centre Atlantic, which is dedicated to research and tests in naval and off-shore engineering. Schulich School of Law also operates the Marine & Environmental Law Institute, which carries out research and conducts consultancy activities for governmental and non-governmental organizations. The school's Department of Political Science similarly operates the Centre for Foreign Policy Studies, which is primarily concerned with the fields of Canadian and American foreign, security, and defence policy, including maritime security policy.
Student life.
The student body of Dalhousie is currently represented by two student unions; the Dalhousie Student Union, which represents the general student population, and the Dalhousie Association for Graduate Students, which represents the interests of graduate students specifically. Dalhousie Student Union began as the Dalhousie Student Government in 1863, and was renamed the University Student Council before taking its present name. The student union recognizes more than 100 student organizations and societies. The organizations and clubs accredited at Dalhousie cover a wide range of interests including academics, culture, religion, social issues, and recreation. Accredited extracurricular organizations at the university fall under the jurisdiction of the Dalhousie Student Union, and must conform to its by-laws. As of 2011, there were three sororities (Omega Pi, Iota Beta Chi, and Alpha Gamma Delta) and three fraternities (Phi Delta Theta, Sigma Chi, and Phi Kappa Pi). They operate as non-accredited organizations and are not recognized by the Dalhousie Student Union.
The university's student population operates a number of media outlets. The main student newspaper, "The Dalhousie Gazette", claims to be the oldest student-run newspaper in North America. It is published Thursdays, and is distributed to over 100 locations around the Halifax area. The newspaper's offices are in the Student Union building. Dalhousie's student population runs a radio station which began as a radio club in 1964, and began to broadcast and operate as CKDU in 1975; it began FM frequency broadcasting in 1985. CKDU acquired its present frequency 88.1 in 2006 alongside an upgrading of its transmitting power.
Clubs and societies.
In addition to the efforts made by the Dalhousie Student Union (DSU) Council, Dalhousie students have created and participated in over 320 clubs/societies. Dalhousie offers a website named "Tiger Society" which lists all current clubs and societies that are available for students to join. Through this website, students can request to join a society. Dalhousie also holds a Society Fair at the beginning of each fall and winter semester, in which all societies are given the opportunity to display their purpose/efforts and recruit new members. Student societies partake in a range of activities from simple gatherings, study groups, bake sales, intramural sports teams, to organizing larger scale fundraising events.
Athletics.
Dalhousie's sports teams are called the Tigers. The Tigers varsity teams participate in the Atlantic University Sport (AUS) of the Canadian Interuniversity Sport (CIS). There are teams for basketball, hockey, soccer, swimming, track and field, cross country running, and volleyball. The Tigers garnered a number of championships in the first decade of the 20th century, winning 63 AUS championships and 2 CIS championships. More than 2,500 students participate in competitive clubs, intramural sport leagues, and tournaments. Opportunities are offered at multiple skill levels across a variety of sports. Dalhousie has six competitive sports clubs and 17 recreational clubs. Dalhousie's Agricultural Campus operates its own varsity team, called the Dalhousie Rams. The Rams varsity team participates in the Atlantic Collegiate Athletic Association, a member of the Canadian Collegiate Athletic Association. The Rams varsity teams include badminton, basketball, rugby, soccer, volleyball, and woodsmen.
Dalhousie has a number of athletic facilities open to varsity teams and students. Dalplex is the largest main fitness and recreational facility. It houses a large fieldhouse, an Olympic-sized swimming pool, an indoor running track, weight rooms, courts and other facilities. Wickwire Field, with a seating capacity of up to 1,200, is the university's main outdoor field and is host to the varsity football, soccer, field hockey, lacrosse and rugby teams. Other sporting facilities include the Studley Gymnasium, and the Sexton Gymnasium and field. The Memorial Arena, home to the varsity hockey team, was demolished in 2012. The school is working to build a new arena jointly with nearby Saint Mary's University, whose facility is also aging. The Agricultural Campus has one athletic facility, the Langille Athletic Centre.
As of 2010, through the efforts of alumni and devoted volunteers, the Dalhousie Football Club was reinstated. Playing in the AFL (Atlantic Football League), the team operates on donations and registration from its players. The team plays its home games at Wickwire Field. Additionally, the university boasts the first quidditch team in Atlantic Canada. As of 2014, the Dalhousie Tigers Quidditch varsity club is the top-ranked team in the area and, though still developing, is showing great promise for regional and national bids in the future.
Insignia and other representations.
Seal.
The Dalhousie seal is based on the heraldic achievement of the Clan Ramsay of Scotland, of which founder George Ramsay was clan head. The heraldic achievement consists of five parts: shield, coronet, crest, supporters, and motto. One major difference between the Ramsay coat of arms and the university seal is that the Ramsay seal features a griffin and greyhound, and the Dalhousie seal has two dragons supporting the eagle-adorned shield. Initially, the Ramsay coat of arms was used to identify Dalhousie, but the seal has evolved with the amalgamations the university has undergone. The seal was originally silver-coloured, but in 1950, the university's Board of Governors changed it to gold to match the university's colours, gold and black. These colours were adopted in 1887, after the rugby team led the debate about college colours for football jerseys. The shield and eagle of Dalhousie's seal have been used as the logo since 1987, with the present incarnation in use since 2003, which includes the tagline "inspiring minds".
Motto and song.
The university motto translates from Latin as "pray and work"; it adopted in 1870 from the Earl of Dalhousie's motto to replace the university's original one, which the administration believed did not convey confidence. The original motto was , which tranlsates as "Perhaps", and first appeared in the first "Dalhousie Gazette" of 1869. It was from Virgil's epic poem "Aeneid", Book 1, line 203, , which translates as "Perhaps the time may come when these difficulties will be sweet to remember".
A number of songs are commonly played and sung at various events such as commencement, convocation, and athletic contests, including "Carmina Dalhousiana", written in Halifax in 1882. The Dalhousie University songbook was compiled by Charles B. Weikel in 1904.
Notable alumni.
Dalhousie graduates have found success in a variety of fields, serving as heads of a diverse array of public and private institutions. Dalhousie University has over 110,000 alumni. Throughout Dalhousie's history, faculty, alumni, and former students have played prominent roles in many fields, and include 89 Rhodes Scholars.
Dalhousie has also educated Nobel laureates. Astrophysicist and Dalhousie alumni Arthur B. McDonald (BSc’64, MSc’65) received the 2015 Nobel Prize in Physics for identifying neutrino change identities and mass. McDonald is one of only four Canadians awarded the Nobel Prize in Physics. McDonald was also previously awarded the Herzberg Prize and the Benjamin Franklin Prize in physics.
Notable politicians who have graduated from Dalhousie include two Prime Ministers of Canada, R. B. Bennett and Joe Clark. Canadian Prime Minister Brian Mulroney attended Dalhousie Law School, though he failed after his first year. Eight graduates have served as Lieutenant Governors: John Crosbie, Myra Freeman, Clarence Gosse, John Keiller MacKay, Henry Poole MacKeen, John Robert Nicholson, Fabian O'Dea, and Albert Walsh. Twelve graduates have served as provincial premiers: Allan Blakeney, John Buchanan, Alex Campbell, Amor De Cosmos, Darrell Dexter, Joe Ghiz, John Hamm, Angus Lewis Macdonald, Russell MacLellan, Gerald Regan, Robert Stanfield, Clyde Wells, and Danny Williams. The first woman appointed to the Supreme Court of Canada, Bertha Wilson, was a graduate from Dalhousie Law School.
Prominent business leaders who studied at Dalhousie include Jamie Baillie, former CEO of Credit Union Atlantic, Graham Day, former CEO of British Shipbuilders, Sean Durfy, former CEO of WestJet, and Charles Peter McColough, former president and CEO of Xerox. Other notable graduates of Dalhousie includes Donald O. Hebb, who helped advance the field of neuropsychology, and Kathryn D. Sullivan, the first American woman to walk in space.
References.
Bibliography

</doc>
<doc id="8643" url="https://en.wikipedia.org/wiki?curid=8643" title="Molecular diffusion">
Molecular diffusion

Molecular diffusion, often simply called diffusion, is the thermal motion of all (liquid or gas) particles at temperatures above absolute zero. The rate of this movement is a function of temperature, viscosity of the fluid and the size (mass) of the particles. Diffusion explains the net flux of molecules from a region of higher concentration to one of lower concentration. Once the concentrations are equal the molecules continue to move, but since there is no concentration gradient the process of molecular diffusion has ceased and is instead governed by the process of self-diffusion, originating from the random motion of the molecules. The result of diffusion is a gradual mixing of material such that the distribution of molecules is uniform. Since the molecules are still in motion, but an equilibrium has been established, the end result of molecular diffusion is called a "dynamic equilibrium". In a phase with uniform temperature, absent external net forces acting on the particles, the diffusion process will eventually result in complete mixing.
Consider two systems; S and S at the same temperature and capable of exchanging particles. If there is a change in the potential energy of a system; for example μ>μ (μ is Chemical potential) an energy flow will occur from S to S, because nature always prefers low energy and maximum entropy.
Molecular diffusion is typically described mathematically using Fick's laws of diffusion.
Applications.
Diffusion is of fundamental importance in many disciplines of physics, chemistry, and biology. Some example applications of diffusion:
Significance.
Diffusion is part of the transport phenomena. Of mass transport mechanisms, molecular diffusion is known as a slower one.
Biology.
In cell biology, diffusion is a main form of transport for necessary materials such as amino acids within cells. Diffusion of water (HO) through a partially permeable membrane is classified as osmosis.
Metabolism and respiration rely in part upon diffusion in addition to bulk or active processes. For example, in the alveoli of mammalian lungs, due to differences in partial pressures across the alveolar-capillary membrane, oxygen diffuses into the blood and carbon dioxide diffuses out. Lungs contain a large surface area to facilitate this gas exchange process.
Tracer, self- and chemical diffusion.
Fundamentally, two types of diffusion are distinguished:
The diffusion coefficients for these two types of diffusion are generally different because the diffusion coefficient for chemical diffusion is binary and it includes the effects due to the correlation of the movement of the different diffusing species.
Non-equilibrium system.
Because chemical diffusion is a net transport process, the system in which it takes place is not an equilibrium system (i.e. it is not at rest yet). Many results in classical thermodynamics are not easily applied to non-equilibrium systems. However, there sometimes occur so-called quasi-steady states, where the diffusion process does not change in time, where classical results may locally apply. As the name suggests, this process is a not a true equilibrium since the system is still evolving.
Non-equilibrium fluid systems can be successfully modeled with Landau-Lifshitz fluctuating hydrodynamics. In this theoretical framework, diffusion is due to fluctuations whose dimensions range from the molecular scale to the macroscopic scale.
Chemical diffusion increases the entropy of a system, i.e. diffusion is a spontaneous and irreversible process. Particles can spread out by diffusion, but will not spontaneously re-order themselves (absent changes to the system, assuming no creation of new chemical bonds, and absent external forces acting on the particle).
Concentration dependent "collective" diffusion.
"Collective diffusion" is the diffusion of a large number of particles, most often within a solvent.
Contrary to brownian motion, which is the diffusion of a single particle, interactions between particles may have to be considered, unless the particles form an ideal mix with their solvent (ideal mix conditions correspond to the case where the interactions between the solvent and particles are identical to the interactions between particles and the interactions between solvent molecules; in this case, the particles do not interact when inside the solvent).
In case of an ideal mix, the particle diffusion equation holds true and the diffusion coefficient "D" the speed of diffusion in the particle diffusion equation is independent of particle concentration. In other cases, resulting interactions between particles within the solvent will account for the following effects:
Molecular Diffusion of Gases.
Transport of material in stagnant fluid or across streamlines of a fluid in a laminar flow occurs by molecular diffusion. Two adjacent compartments separated by a partition, containing pure gases A or B may be envisaged. Random movement of all molecules occurs so that after a period molecules are found remote from their original positions. If the partition is removed, some molecules of A move towards the region occupied by B, their number depends on the number of molecules at the point considered. Concurrently, molecules of B diffuse toward regimens formerly occupied by pure A.
Finally, complete mixing occurs. Before this point in time, a gradual variation in the concentration of A occurs along an axis, designated x, which joins the original compartments. This variation, expressed mathematically as -dC/dx, where C is the concentration of A. The negative sign arises because the concentration of A decreases as the distance x increases. Similarly, the variation in the concentration of gas B is -dC/dx. The rate of diffusion of A, N, depend on concentration gradient and the average velocity with which the molecules of A moves in the x direction. This relationship is expressed by Fick's Law
where D is the Diffusivity of A through B, proportional to the average (squared?) molecular velocity and, therefore dependent on the temperature and pressure of gases. The rate of Diffusion N,is usually expressed as the number of moles diffusing across unit area in unit time. As with the basic equation of heat transfer, this indicates that the rate of force is directly proportional to the driving force, which is the concentration gradient.
This basic equation applies to a number of situations. Restricting discussion exclusively to steady state conditions, in which neither dC/dx or dC/dx change with time, equimolecular counterdiffusion is considered first.
Equimolecular Counterdiffusion.
If no bulk flow occurs in an element of length dx, the rates of diffusion of two gases A and B must be equal and opposite, that is formula_2.
The partial pressure of A changes by dP over the distance dx. Similarly, the partial pressure of B changes dP. As there is no difference in total pressure across the element (no bulk flow), we have
For an ideal gas the partial pressure is related to the molar concentration by the relation
where n is the number of moles of gas "A" in a volume "V". As the molar concentration "C" is equal to "n/ V" therefore
Consequently, for gas A,
where D is the diffusivity of A in B. Similarly,
Considering that dP/dx=-dP/dx, it therefore proves that D=D=D. If the partial pressure of A at x is P and x is P, integration of above equation,
A similar equation may be derived for the counterdiffusion of gas B.

</doc>
<doc id="8645" url="https://en.wikipedia.org/wiki?curid=8645" title="Declension">
Declension

In linguistics, declension is the inflection of nouns, pronouns, adjectives, and articles to indicate number (at least singular and plural), case (nominative or subjective, genitive or possessive, etc.), and gender. A declension is also a group of nouns that follow a particular pattern of inflection.
Declension occurs in many of the world's languages, and features very prominently in many European languages. Old English was a highly inflected language, as befits its Indo-European and especially its Germanic linguistic ancestry, but its declensions greatly simplified as it evolved into Modern English.
Modern English.
In Modern English, nouns have distinct singular and plural forms; that is, they "decline" to reflect their grammatical number; consider the difference between "book" and "books". In addition, a few English pronouns have distinct nominative (also called subjective) and oblique (or objective) forms; that is, they decline to reflect their relationship to a verb or preposition, or case. Consider the difference between "he" (subjective) and "him" (objective), as in "He saw it" and "It saw him"; similarly, consider "who", which is subjective, and the objective "whom". Further, these pronouns and a few others have distinct possessive forms, such as "his" and "whose". By contrast, nouns have no distinct nominative and objective forms, the two being merged into a single "plain case". For example, "chair" does not change form between "the chair is here" (subject) and "I saw the chair" (direct object). Possession is shown by the clitic "-'s" attached to a possessive noun phrase, rather than by declension of the noun itself.
Gender is at best only weakly grammaticalized in Modern English. While masculine, feminine, and neuter genders are recognized, nouns do not normally decline for gender, though some nouns, especially Latin words and personal names, exist in multiple forms corresponding to different genders: alumnus (masculine singular) and alumna (feminine singular); Andrew and Andrea, Paul and Paula, etc. Suffixes such as "-ess", "-ette", and "-er" can also derive overtly gendered versions of nouns, with marking for feminine being much more common than marking for masculine. Many nouns can actually function as members of two genders or even all three, and the gender classes of English nouns are usually determined by their agreement with pronouns, rather than marking on the nouns themselves.
Adjectives are rarely declined for any purpose. They can be declined for number when they are used as substitutes for nouns (as in, "I'll take the reds", meaning "I'll take the red ones" or as shorthand for "I'll take the red wines", for example). Also the demonstrative determiners "this" and "that" are declined for number, as "these" and "those". Some adjectives borrowed from other languages are, or can be, declined for gender, at least in writing: "blond" (male) and "blonde" (female) or a "bonie" lad as compared to a "bonnie" lass. Adjectives are not declined for case in Modern English, though they were in Old English. The article is never regarded as declined in Modern English, although formally, the words "that" and possibly "she" correspond to forms of the predecessor of "the" ("se" m., "þæt" n., "sēo" f.) as it was declined in Old English.
Basic declension theory.
Core examples.
The following hypothetical examples illustrate how languages with declension work. If English were a language with declension, one might add a suffix -by before the subjects of sentences, and a suffix -em before the objects. Sentences would appear as follows:
The above sentences sound unnatural to English speakers. However, it is common in many other languages for affixes to be added at the start (or at the end) of subjects and of objects, such as in Japanese, Russian and Basque. These languages have a freer word order than English does, because English depends on word order to identify the subject and object:
However, in a language that uses em- and by- to identify subject and object, there is no longer any need of placing the subject always before the verb, because the subject will always be whatever has the 'by' attached to it, regardless of word-order, so the sentence remains meaning the same however we shuffle the parts:
In such a language, the word order is not so important to understand who did that to whom; however, the em- and by- must be added to all objects and subjects or confusion will result. Technically speaking, "declension is not optional and nouns must be marked". The following are hypothetical cases and suffixes that would be used in this declined English.
Other possible cases.
Now assume that: going to/in direction of takes the suffix -mo, doing something with an object or person takes -wot, and addressing someone by their name takes the suffix -hey.
The following sentences in this theoretical English will demonstrate how this would seem to us if there were declensions.
Note that these sentences could be written in any order and the meaning would stay the same:
This word order is not possible in modern English as there are no cases or declension as in some other languages. This means that in English, word order is essential to constructing coherent sentences, otherwise most sentences would be confusing.
This theoretical system of declension is relatively simple and is more or less how declension works in languages such as Hungarian, Russian, Bengali, Greek, Basque, Japanese or Sanskrit. Some of these have a far more complicated set of declensions where the suffixes (or prefixes or infixes) change depending on the gender of the noun, the quantity of the noun and many other possible factors. Many of them lack articles. There may also be irregular nouns where the declensions are unique for each word. In many languages, articles, demonstratives and adjectives are also declined. The following example demonstrates such declension in our theoretical English.
Cases applied to adjectives and particles.
In the examples above we have made sentences like: 
putting the "by-" and the "em-" after the noun in each phrase. But many declined languages do not use articles (for example "a" or "the") at all, such as Japanese and Basque. In such a language we would say something like:
Now, in some languages, like Russian and Sanskrit, you would not place "by" after a whole phrase like "big man", but would place "by" after every word of that phrase, like this:
and instead of 
that would be:
Some languages decline many different parts of speech including adverbs and demonstratives:
Cases exotic to Indo-European languages.
Finally, assume that: an object that is located inside another object takes the suffix -boo. If an person/object turns around and goes back to where they came from it takes -yoo. If the object belongs to someone who is not present it takes -foo.
These might look like the suffixes in earlier sections, but the big difference is that the earlier ones are related to the cases found in Indo-European languages, while no Indo-European language has any suffixes like the above three.
Latin.
An example of a Latin noun declension is given below, using the singular forms of the word "homo" ("man"), which belongs to Latin's third declension.
There are two further noun cases in Latin, the vocative and the locative:
Sanskrit.
Sanskrit has eight cases: nominative, vocative, accusative, genitive, dative, ablative, locative and instrumental. Some count vocative not as a separate case, despite it having a distinctive ending in the singular, but consider it as a different use of the nominative.
Sanskrit grammatical case was analyzed extensively. The grammarian Pāṇini identified seven semantic roles or "karaka", which correspond closely to the eight cases:
For example, consider the following sentence:
Here "leaf" is the agent, "tree" is the source, and "ground" is the locus. The endings "-aṁ", "-at", "-āu" mark the cases associated with these meanings.

</doc>
<doc id="8648" url="https://en.wikipedia.org/wiki?curid=8648" title="Daffynition">
Daffynition

A daffynition (derived from "daffy" and "definition") is a pun format involving the reinterpretation of an existing word, on the basis that it sounds like another word (or group of words). They are similar to transpositional puns, but often much less complex and easier to create. 
Some daffynitions may be puns. For example, "a hangover is the wrath of grapes" is a play on the title of the book "The Grapes of Wrath". A subclass of daffynition is the goofinition which relies strictly on literal associations and correct spellings, such as "lobster = a weak tennis player".
Under the name Uxbridge English Dictionary making up daffynitions is a popular game on the BBC Radio 4 comedy quiz show "I'm Sorry I Haven't a Clue.

</doc>
<doc id="8649" url="https://en.wikipedia.org/wiki?curid=8649" title="List of football clubs in the Netherlands">
List of football clubs in the Netherlands

The Dutch Football League is organized by the Royal Dutch Football Association (KNVB, Koninklijke Nederlandse Voetbalbond).The most successful teams are Ajax (33), PSV (21) and Feyenoord (14). Important teams of the past are HVV (10 titles), Sparta Rotterdam (6 titles) and Willem II (3 titles).
The annual match that marks the beginning of the season is called the Johan Cruijff Schaal (Johan Cruyff Shield). Contenders are the champions and the cup winners of the previous season.
In women's football, the KNVB and its Belgian counterpart, the Royal Belgian Football Association (KBVB/URBSFA), jointly organize the top-level BeNe League. Each federation organizes its own system of lower-level leagues.

</doc>
<doc id="8650" url="https://en.wikipedia.org/wiki?curid=8650" title="Dragon 32/64">
Dragon 32/64

The Dragon 32 and Dragon 64 are home computers that were built in the 1980s. The Dragons are very similar to the TRS-80 Color Computer, and were produced for the European market by Dragon Data, Ltd., in Port Talbot, Wales, and for the US market by Tano of New Orleans, Louisiana. The model numbers reflect the primary difference between the two machines, which have 32 and 64 kilobytes of RAM, respectively.
Product history.
In the early 1980s, the British home computer market was booming. New machines were released almost monthly. In August 1982, Dragon Data joined the fray with the Dragon 32; the Dragon 64 followed a year later. The computers sold quite well initially and attracted the interest of several independent software developers, most notably Microdeal. A magazine, "Dragon User", also began publication shortly after the machine's launch.
In the private home computer market, where games were a significant driver, the Dragon suffered because its graphical capabilities were inferior to contemporary machines such as the Sinclair ZX Spectrum and BBC Micro.
The Dragon was also unable to display lower-case letters easily. Some more sophisticated applications would synthesise them using high-resolution graphics modes (in the same way that user-defined characters would be designed for purely graphical applications such as games). Simpler programs just managed without lower case. This effectively locked it out of the then-blooming educational market.
As a result of these limitations, the Dragon was not a commercial success, and Dragon Data collapsed in June 1984.
Technical notes.
Hardware and peripherals.
The Dragon is built around the Motorola MC6809E processor running at 0.89 MHz. This was an advanced 8-bit CPU design, having, among other things, limited 16-bit capabilities. In terms of raw computational power, the Dragon beat most of its contemporary rivals based on the older MOS Technology 6502, but this made little difference in a market where graphical capabilities and games were much more important to consumers.
It was possible to increase the speed of the computer by using POKE 65495,0 which accelerates the ROM-resident BASIC interpreter, but temporarily disables correct functioning of the cassette/printer ports. Manufacturing variances means that not all Dragons are able to function at this higher speed, and use of this POKE can cause some units to crash or be unstable, though with no permanent damage. POKE 65494,0 returns the speed to normal. POKE 65497,0 pushes the speed yet higher but the display is lost until a slower speed is restored.
The Dragon also used the SN74LS783/MC6883 Synchronous Address Multiplexer (SAM) and the MC6847 Video Display Generator (VDG). I/O was provided by two MC6821 Peripheral Interface Adapters (PIAs). Many Dragon 32s were upgraded by their owners to 64 kB of memory. A few were further expanded to 128 kB, 256 kB, or 512 kB, with home-built memory controllers/memory management units (MMUs).
A broad range of peripherals exist for the Dragon 32/64, and on top of this there are add-ons such as the Dragon's Claw which give the Dragons access to the BBC Micro's large range of accessories (a particularly important factor in the UK home market). Although neither machine has a built-in disk operating system (cassette tapes being the default data-storage mechanism in the home computer market at the time), DragonDOS was supplied as part of the disk controller interface from Dragon Data Ltd. The numerous external ports (by the standards of the time), including the standard RS-232 on the 64, also allows hobbyists to attach a diverse range of equipment.
An unusual feature was a monitor port for connection of a computer monitor, as an alternative to the TV output. This was rarely used due to the cost of dedicated monitors at that time. The port is actually a Composite Video port and can be used to connect the Dragon 32 to most modern TVs to deliver a much better picture.
The Dragon uses analogue joysticks, unlike most systems of the time which used less versatile but cheaper digital systems. Other uses for the joystick ports include light pens.
Tony Clarke and Richard Wadman laid out the specifications for the Dragon.
The units had a robust motherboard in a spacious case, reminiscent of the BBC Micro, and so were more tolerant of home-modification than some of their contemporaries, which often had their components crammed into the smallest possible space.
Video modes.
The Dragon's main display mode is 'black on green' text (actually the black was a deeper, muddier green). The only graphics possible in this mode are quarter-tile block based.
It also has a selection of five high resolution modes, named PMODEs 0-4, which alternate monochrome and four-colour in successively higher resolutions, culminating in the black and white 256×192 PMODE 4. Each mode has two possible colour palettes. Unfortunately, these are rather garish and cause the system to fare poorly in visual comparisons with other home computers of the time. It is also impossible to use standard printing commands to print text on the graphical modes, causing software development difficulties.
Full colour scanline based 64×192 "semi-graphics" modes are also possible, though their imbalanced resolution and programming difficulty (they are not accessible via BASIC) meant they were not often utilised.
Disk systems.
A complete Disk Operating System was produced for the Dragon by a third-party supplier, Premier Microsystems located near Croydon, South London. The system was sold as the "Delta" disk operating system. Although Premier offered the Delta system to be marketed by Dragon themselves, Dragon were not happy that a third party were hijacking the standards for their computer, and produced their own rival DragonDOS system making it clear that the third party Delta was not compatible with the 'standard' Dragon Disk system.
Inevitably, with Delta's head start, software was marketed in either system, but rarely both. The result was the inevitable confusion with customers upset that a particular piece of software was not available for the Disk system that they had. Although this was far from the principal driver for the Dragon's demise, it was nevertheless a factor, and had Dragon adopted the established Delta system, the machine may well have had a greater following and a longer life.
System software.
The Dragon comes with a Microsoft BASIC interpreter in 16 KB of ROM.
The BASIC appears to be identical to Tandy Color Computer's Extended Basic.
Unlike a modern PC with the operating system on disk, a Dragon starts instantly when powered up. Some software providers also produced compilers for BASIC, and other languages, to produce binary (or "machine") code which would run many times faster and make better use of the small system RAM. Towards the end of its life, Dragon Data produced an assembler/disassembler/editor suite called "Dream".
In addition to the DragonDOS disk operating system, the Dragon 32/64 is capable of running several others, including FLEX, and even OS-9 which brought UNIX-like multitasking to the platform. Memory-expanded and MMU-equipped Dragons are able to run OS-9 Level 2.
Games.
Initially, the Dragon was reasonably well supported by the major UK software companies with versions of popular games from other systems being ported to the Dragon. Examples of top selling games available for the Dragon include "Arcadia" (Imagine), "Chuckie Egg" (A&F), "Manic Miner" and sequel "Jet Set Willy" (Software Projects), "Hunchback" (Ocean) and "Football Manager" (Addictive). There were also companies that concentrated on the Dragon such as Microdeal. Their character Cuthbert appeared in several games on the Dragon with "Cuthbert Goes Walkabout" also being converted for Atari 8-bit and Commodore 64 systems.
Due to the limited graphics modes of the Dragon, converted games had a distinctive appearance with colour games being usually played on a green or white background (rather than the more common black on other systems) or games with high definition graphics having to run in black and white.
When the system was discontinued, support from software companies also effectively ended. However, Microdeal continued supporting the Dragon until January 1988. Some of their final games developed for the Dragon in 1987 such as "Tanglewood" and "Airball" were also converted for 16-bit machines such as the Atari ST and Amiga.
Differences from the TRS-80.
Both the Dragon and the TRS-80 Color Computer are based on a Motorola data sheet design for the MC6883 SAM (MMU) chip for memory management and peripheral control.
The systems are sufficiently similar that a significant fraction of the compiled software produced for one machine will run on the other. Software running via the built-in Basic interpreters also has a high level of compatibility, but only after they are re-tokenized, which can be achieved fairly easily by transferring via cassette tape with appropriate options.
It is possible to permanently convert a Color Computer into a Dragon by swapping the original Color Computer ROM and rewiring the keyboard cable.
The Dragon has additional circuitry to make the MC6847 VDG compatible with European 625-line television standards, rather than the US 525-line NTSC standard, and a Centronics parallel printer port not present on the TRS-80. Some models were manufactured with NTSC video for the US market.
Dragon 32 vs. Dragon 64.
Aside from the amount of RAM, the 64 also has a functional RS-232 serial port which was not included on the 32. A minor difference between the two Dragon models is the outer case colour; the Dragon 32 is beige and the 64 is light grey. Besides the case, branding and the Dragon 64's serial port, the two machines look the same. The Dragon 32 is upgradable to Dragon 64.
Reception.
"BYTE" wrote in January 1983 that the Dragon 32 "offers more feature for the money than most of its ritis competitors", but "there's nothing exceptional about it". The review described it as a redesigned, less-expensive Color Computer with 32K RAM and better keyboard.

</doc>
<doc id="8651" url="https://en.wikipedia.org/wiki?curid=8651" title="Dark matter">
Dark matter

Dark matter is a hypothetical substance that is believed by most astronomers to account for around five-sixths of the matter in the universe. Although it has not been directly observed, its existence and properties are inferred from its various gravitational effects: on the motions of visible matter; via gravitational lensing; its influence on the universe's large-scale structure, and its effects in the cosmic microwave background. Dark matter is transparent to electromagnetic radiation and/or is so dense and small that it fails to absorb or emit enough radiation to be detectable with imaging technology.
Estimates of masses for galaxies and larger structures via dynamical and general relativistic means are much greater than those based on the mass of the visible "luminous" matter.
The standard model of cosmology indicates that the total mass–energy of the universe contains 4.9% ordinary matter, 26.8% dark matter and 68.3% dark energy. Thus, dark matter constitutes 84.5% of total mass, while dark energy plus dark matter constitute 95.1% of total mass–energy content. As a matter of fact, the great majority of ordinary matter in the universe is also unseen, since visible stars and gas inside galaxies and clusters account for less than 10 per cent of the ordinary matter contribution to the mass-energy density of the universe.
The dark matter hypothesis plays a central role in state-of-the-art modeling of cosmic structure formation and galaxy formation and evolution and on explanations of the anisotropies observed in the cosmic microwave background (CMB). All these lines of evidence suggest that galaxies, clusters of galaxies and the universe as a whole contain far more matter than that which is observable via electromagnetic signals.
The most widely accepted form for dark matter is that it is composed of weakly interacting massive particles (WIMPs) that interact only through gravity and the weak force.
Although the existence of dark matter is generally accepted by most of the astronomical community, a minority of astronomers 
MOND and TeVeS, that attempt to account for the observations without invoking additional matter.
Many experiments to detect proposed dark matter particles through non-gravitational means are under way.
History.
The first to suggest the existence of dark matter (using stellar velocities) was Dutch astronomer Jacobus Kapteyn in 1922.
Fellow Dutchman and radio astronomy pioneer Jan Oort also hypothesized the existence of dark matter in 1932. Oort was studying stellar motions in the local galactic neighborhood and found that the mass in the galactic plane must be greater than what was observed, but this measurement was later determined to be erroneous.
In 1933, Swiss astrophysicist Fritz Zwicky, who studied galactic clusters while working at the California Institute of Technology, made a similar inference. Zwicky applied the virial theorem to the Coma cluster and obtained evidence of unseen mass that he called "dunkle Materie" 'dark matter'. Zwicky estimated its mass based on the motions of galaxies near its edge and compared that to an estimate based on its brightness and number of galaxies. He estimated that the cluster had about 400 times more mass than was visually observable. The gravity effect of the visible galaxies was far too small for such fast orbits, thus mass must be hidden from view. Based on these conclusions, Zwicky inferred that some unseen matter provided the mass and associated gravitation attraction to hold the cluster together. This was the first formal inference about the existence of dark matter. Zwicky's estimates were off by more than an order of magnitude, mainly due to an obsolete value of the Hubble constant; 
the same calculation today shows a smaller fraction, using greater values for luminous mass. However, Zwicky did correctly infer that the bulk of the matter was dark.
The first robust indications that the mass to light ratio was anything other than unity came from measurements of galaxy rotation curves. In 1939, Horace W. Babcock reported the rotation curve for the Andromeda nebula, which suggested that the mass-to-luminosity ratio increases radially. He attributed it to either light absorption within the galaxy or modified dynamics in the outer portions of the spiral and not to missing matter.
Vera Rubin and Kent Ford in the 1960s–1970s were the first to postulate "dark matter" based upon robust evidence, using galaxy rotation curves. Rubin worked with a new spectrograph to measure the velocity curve of edge-on spiral galaxies with greater accuracy. This result was independently confirmed in 1978. An influential paper presented Rubin's results in 1980. Rubin found that most galaxies must contain about six times as much dark as visible mass; thus, by around 1980 the apparent need for dark matter was widely recognized as a major unsolved problem in astronomy.
A stream of independent observations in the 1980s indicated its presence, including gravitational lensing of background objects by galaxy clusters, the temperature distribution of hot gas in galaxies and clusters, and the pattern of anisotropies in the cosmic microwave background. According to consensus among cosmologists, dark matter is composed primarily of a not yet characterized type of subatomic particle. The search for this particle, by a variety of means, is one of the major efforts in particle physics.
Cosmic microwave background radiation.
In cosmology, the CMB is explained as relic radiation which has travelled freely since the era of recombination, around 375,000 years after the Big Bang. The CMB's anisotropies are explained as the result of small primordial density fluctuations, and subsequent acoustic oscillations in the photon-baryon plasma whose restoring force is gravity.
The NASA Cosmic Background Explorer (COBE) found the CMB spectrum to be a very precise blackbody spectrum with a temperature of 2.726 K. In 1992, COBE detected CMB fluctuations (anisotropies) at a level of about one part in 10.
In the following decade, CMB anisotropies were investigated by ground-based and balloon experiments. Their primary goal was to measure the angular scale of the first acoustic peak of the anisotropies' power spectrum, for which COBE had insufficient resolution. During the 1990s, the first peak was measured with increasing sensitivity, and in 2000 the BOOMERanG experiment reported that the highest power fluctuations occur at scales of approximately one degree, showing that the Universe is close to flat. These measurements were able to rule out cosmic strings as the leading theory of cosmic structure formation, and suggested cosmic inflation was the correct theory.
Ground-based interferometers provided fluctuation measurements with higher accuracy, including the Very Small Array, the Degree Angular Scale Interferometer (DASI) and the Cosmic Background Imager (CBI). DASI first detected the CMB polarization, and CBI provided the first E-mode polarization spectrum with compelling evidence that it is out of phase with the T-mode spectrum. COBE's successor, the Wilkinson Microwave Anisotropy Probe (WMAP) provided the most detailed measurements of (large-scale) anisotropies in the CMB in 2003 - 2010. ESA's Planck spacecraft returned more detailed results in 2013-2015.
WMAP's measurements played the key role in establishing the Standard Model of Cosmology, namely the Lambda-CDM model, which posits a dark energy-dominated flat universe, supplemented by dark matter and atoms with density fluctuations seeded by a Gaussian, adiabatic, nearly scale invariant process. Its basic properties are determined by six adjustable parameters: dark matter density, baryon (atom) density, the universe's age (or equivalently, the Hubble constant), the initial fluctuation amplitude and their scale dependence.
Observational evidence.
Much of the evidence comes from the motions of galaxies. Many of these appear to be fairly uniform, so by the virial theorem, the total kinetic energy should be half the galaxies' total gravitational binding energy. Observationally, the total kinetic energy is much greater. In particular, assuming the gravitational mass is due to only visible matter, stars far from the center of galaxies have much higher velocities than predicted by the virial theorem. Galactic rotation curves, which illustrate the velocity of rotation versus the distance from the galactic center, show the "excess" velocity. Dark matter is the most straightforward way of accounting for this discrepancy.
The distribution of dark matter in galaxies required to explain the motion of the observed matter suggests the presence of a roughly spherically symmetric, centrally concentrated halo of dark matter with the visible matter concentrated in a central disc.
Low surface brightness dwarf galaxies are important sources of information for studying dark matter. They have an uncommonly low ratio of visible to dark matter, and have few bright stars at the center that would otherwise impair observations of the rotation curve of outlying stars.
Gravitational lensing observations of galaxy clusters allow direct estimates of the gravitational mass based on its effect on light coming from background galaxies, since large collections of matter (dark or otherwise) gravitationally deflect light. In clusters such as Abell 1689, lensing observations confirm the presence of considerably more mass than is indicated by the clusters' light. In the Bullet Cluster, lensing observations show that much of the lensing mass is separated from the X-ray-emitting baryonic mass. In July 2012, lensing observations were used to identify a "filament" of dark matter between two clusters of galaxies, as cosmological simulations predicted.
Galaxy rotation curves.
A galaxy rotation curve is a plot of the orbital velocities (i.e., the speeds) of visible stars or gas in that galaxy versus their radial distance from that galaxy's center. The rotational/orbital speeds of galaxies/stars does not decline with distance, unlike other orbital systems such as stars/planets and planets/moons that also have most of their mass at the centre. In the latter cases, this reflects the mass distributions within those systems. The mass observations for galaxies based on the light that they emit are far too low to explain the velocity observations.
The dark matter hypothesis supplies the missing mass, resolving the anomaly.
A universal rotation curve can be expressed as the sum of an exponential distribution of visible matter that tapers to zero with distance from the center, and a spherical dark matter halo with a flat core of radius r and density ρ = 4.5 × 10(r/kpc) pc.
Low-surface-brightness (LSB) galaxies have a much larger visible mass deficit than others. This property simplifies the disentanglement of the dark and visible matter contributions to the rotation curves.
Rotation curves for some elliptical galaxies do display low velocities for outlying stars (tracked for example by the motion of embedded planetary nebulae). A dark-matter compliant hypothesis proposes that some stars may have been torn by tidal forces from disk-galaxy mergers from their original galaxies during the first close passage and put on outgoing trajectories, explaining the low velocities of the remaining stars even in the presence of a halo.
Velocity dispersions of galaxies.
Velocity dispersion estimates of elliptical galaxies, with some exceptions, generally indicate a relatively high dark matter content.
Diffuse interstellar gas measurements of galactic edges indicate missing ordinary matter beyond the visible boundary, but that galaxies are virialized (i.e., gravitationally bound and orbiting each other with velocities that correspond to predicted orbital velocities of general relativity) up to ten times their visible radii. This has the effect of pushing up the dark matter as a fraction of the total matter from 50% as measured by Rubin to the now accepted value of nearly 95%.
Dark matter seems to be a small component or absent in some places. Globular clusters show little evidence of dark matter, except that their orbital interactions with galaxies do support galactic dark matter. Star velocity profiles seemed to indicate a concentration of dark matter in the disk of the Milky Way. It now appears, however, that the high concentration of baryonic matter in the disk (especially in the interstellar medium) can account for this motion. Galaxy mass and light profiles appear to not match. The typical model for dark matter galaxies is a smooth, spherical distribution in virialized halos. This avoids small-scale (stellar) dynamical effects. A 2006 study explained the warp in the Milky Way's disk by the interaction of the Large and Small Magellanic Clouds and the 20-fold increase in predicted mass from dark matter.
In 2005, astronomers claimed to have discovered a galaxy made almost entirely of dark matter, 50 million light years away in the Virgo Cluster, which was named VIRGOHI21. Unusually, VIRGOHI21 does not appear to contain visible stars: it was discovered with radio frequency observations of hydrogen. Based on rotation profiles, the scientists estimate that this object contains approximately 1000 times more dark matter than hydrogen and has a mass of about 1/10 that of the Milky Way. The Milky Way is estimated to have roughly 10 times as much dark matter as ordinary matter. Models of the Big Bang and structure formation suggested that such dark galaxies should be very common, but VIRGOHI21 was the first to be detected.
The velocity profiles of some galaxies such as NGC 3379 indicate an absence of dark matter.
Galaxy clusters and gravitational lensing.
Clusters of galaxies are particularly important for dark matter studies since their masses can be estimated in three independent ways:
Generally these three methods are in reasonable agreement, that dark matter outweighs visible matter by around 5 to 1.
Gravity acts as a lens to bend the light from a more distant source (such as a quasar) around a massive object (such as a cluster of galaxies) lying between the source and the observer in accordance with general relativity.
Strong lensing is the observed distortion of background galaxies into arcs when their light passes through such a gravitational lens. It has been observed around many distant clusters including Abell 1689. By measuring the distortion geometry, the mass of the intervening cluster can be obtained. In the dozens of cases where this has been done, the mass-to-light ratios obtained correspond to the dynamical dark matter measurements of clusters.
Weak gravitational lensing investigates minute distortions of galaxies, using statistical analyses from vast galaxy surveys. By examining the apparent shear deformation of the adjacent background galaxies, astrophysicists can characterize the mean distribution of dark matter. The mass-to-light ratios correspond to dark matter densities predicted by other large-scale structure measurements.
Galaxy cluster Abell 2029 comprises thousands of galaxies enveloped in a cloud of hot gas and dark matter equivalent to more than . At the center of this cluster is an enormous elliptical galaxy likely formed from many smaller galaxies.
The most direct observational evidence comes from the Bullet Cluster. In most regions dark and visible matter are found together, due to their gravitational attraction. In the Bullet Cluster however, the two matter types split apart, due to a past collision between two smaller clusters. Electromagnetic interactions between gas particles has caused the gas to slow and concentrate near the point of impact. The galaxies, stars and dark matter continued through with negligible collisions. Lensing observations show two dark matter peaks near the galaxy peaks, as expected in dark matter theory. Since the gas peaks contain more ordinary matter than the stars, modified-gravity theories should show the lensing peaks near the gas peaks, contrary to the observations.
X-ray observations show that much of the luminous matter (in the form of 10–10 Kelvin gas or plasma) is concentrated in the cluster's center. Weak gravitational lensing observations show that much dark matter resides outside the central region. Unlike galactic rotation curves, this evidence is independent of the details of Newtonian gravity, directly supporting dark matter.
Dark matter's observed behavior constrains whether and how much it scatters off other dark matter particles, quantified as its self-interaction cross section. If dark matter has no pressure, it can be described as a perfect fluid that has no damping. The distribution of mass in galaxy clusters has been used to argue both for and against the significance of self-interaction.
An ongoing survey using the Subaru telescope uses weak lensing to analyze background light, bent by dark matter, to determine the statistical distribution of dark matter in the foreground. The survey studies galaxies more than a billion light-years distant, across an area greater than a thousand square degrees (about one fortieth of the entire sky).
Cosmic microwave background.
Angular CMB fluctuations provide evidence for dark matter. The typical angular scales of CMB oscillations, measured as the power spectrum of the CMB anisotropies, reveal the different effects of baryonic and dark matter. Ordinary matter interacts strongly via radiation whereas dark matter particles (WIMPs) do not; both affect the oscillations by way of their gravity, so the two forms of matter have different effects.
The spectrum shows a large first peak and smaller successive peaks. The first peak tells mostly about the density of baryonic matter, while the third peak relates mostly to the density of dark matter, measuring the density of matter and the density of atoms.
Sky surveys and baryon acoustic oscillations.
 The early universe's acoustic oscillations in the photon-baryon fluid are observed as the prominent acoustic peaks in the CMB spectrum. This set up a preferred length scale for baryons in the early universe which is determined as 147 Megaparsec (comoving) by the Planck spacecraft. As the dark matter and baryons clumped together after recombination, the effect is much weaker in the galaxy distribution in the nearby universe, but is detectable as a subtle (~ 1 percent) preference for pairs of galaxies to be separated by 147 Mpc, rather than 130 or 160 Mpc, called the BAO feature. This feature was predicted theoretically in the 1990s and then discovered in 2005, in two large galaxy redshift surveys, the Sloan Digital Sky Survey and the 2dF Galaxy Redshift Survey. Combining the CMB observations with BAO measurements from galaxy redshift surveys provides a precise estimate of the Hubble constant and the average matter density in the Universe.
Type Ia supernova distance measurements.
Type Ia supernovae can be used as "standard candles" to measure extragalactic distances. Extensive data sets of these supernovae can be used to constrain cosmological models. They constrain the dark energy density Ω = ~0.713 for a flat, Lambda CDM universe and the parameter formula_1 for a quintessence model. The results are roughly consistent with those derived from the WMAP observations and further constrain the Lambda CDM model and (indirectly) dark matter.
Lyman-alpha forest.
In astronomical spectroscopy, the Lyman-alpha forest is the sum of the absorption lines arising from the Lyman-alpha transition of neutral hydrogen in the spectra of distant galaxies and quasars. Lyman-alpha forest observations can also constrain cosmological models. These constraints agree with those obtained from WMAP data.
Structure formation.
Structure formation refers to the serial transformations of the universe following the Big Bang. Prior to structure formation, e.g., Friedmann cosmology solutions to general relativity describe a homogeneous universe. Later, small anisotropies gradually grew and condensed the homogeneous universe into stars, galaxies and larger structures.
Observations suggest that structure formation proceeds hierarchically, with the smallest structures collapsing first, followed by galaxies and then galaxy clusters. As the structures collapse in the evolving universe, they begin to "light up" as baryonic matter heats up through gravitational contraction and approaches hydrostatic pressure balance.
CMB anisotropy measurements fix models in which most matter is dark. Dark matter also closes gaps in models of large-scale structure. The dark matter hypothesis corresponds with statistical surveys of the visible structure and precisely to CMB predictions.
Initially, baryonic matter's post-Big Bang temperature and pressure were too high to collapse and form smaller structures, such as stars, via the Jeans instability. The gravity from dark matter increase the compaction force, allowing the creation of these structures.
Computer simulations of billions of dark matter particles confirmed that the "cold" dark matter model of structure formation is consistent with the structures observed through galaxy surveys, such as the Sloan Digital Sky Survey and 2dF Galaxy Redshift Survey, as well as observations of the Lyman-alpha forest.
Tensions separate observations and simulations. Observations have turned up 90-99% fewer small galaxies than permitted by dark matter-based predictions. In addition, simulations predict dark matter distributions with a dense cusp near galactic centers, but the observed halos are smoother than predicted.
Composition.
The composition of dark matter remains uncertain. Possibilities include dense baryonic (interacts with electromagnetic force) matter and non-baryonic matter (interacts with its surroundings only through gravity).
Baryonic vs nonbaryonic matter.
Baryonic matter.
Baryonic matter is made of baryons (protons and neutrons), that make up stars and planets. It also encompasses less common black holes, neutron stars, faint old white dwarfs and brown dwarfs, collectively known as massive compact halo objects or MACHOs.
Multiple lines of evidence suggest the majority of dark matter is not made of baryons:
Non-baryonic matter.
Candidates for nonbaryonic dark matter are hypothetical particles such as axions or supersymmetric particles; neutrinos can only supply a small fraction of dark matter, due to limits derived from large-scale structure and high-redshift galaxies.
Unlike baryonic matter, nonbaryonic matter did not contribute to the formation of the elements in the early universe ("Big Bang nucleosynthesis") and so its presence is revealed only via its gravitational effects. In addition, if the particles of which it is composed are supersymmetric, they can undergo annihilation interactions with themselves, possibly resulting in observable by-products such as gamma rays and neutrinos ("indirect detection").
Classification: cold/warm/hot.
Dark matter can be divided into "cold", "warm" and "hot" categories. These categories refer to velocity rather than an actual temperature, indicating how far corresponding objects moved due to random motions in the early universe, before they slowed due to cosmic expansion – this is an important distance called the "free streaming length" (FSL). Primordial density fluctuations smaller than this length get washed out as particles spread from overdense to underdense regions, while larger fluctuations are unaffected; therefore this length sets a minimum scale for later structure formation. The categories are set with respect to the size of a protogalaxy (an object that later evolves into a dwarf galaxy): dark matter particles are classified as cold, warm, or hot according as their FSLs are respectively much smaller (cold), similar (warm), or much larger (hot) than a protogalaxy.
Mixtures of the above are also possible: a theory of mixed dark matter was popular in the mid-1990s, but was rejected following the discovery of dark energy.
Cold dark matter leads to a "bottom-up" formation of structure while hot dark matter would result in a "top-down" formation scenario; the latter is excluded by high-redshift galaxy observations.
Alternative definitions.
These categories also correspond to fluctuation spectrum effects and the interval following the Big Bang at which each type became non-relativistic.
Davis "et al." wrote in 1985:
Another approximate dividing line is that warm dark matter became non-relativistic when the universe was approximately 1 year old and 1 millionth of its present size and in the radiation-dominated era (photons and neutrinos), with a photon temperature 2.7 million K. Standard physical cosmology gives the particle horizon size as 2 ct in the radiation-dominated era, thus 2 light-years. A region of this size would expand to 2 million light years today (absent structure formation). The actual FSL is roughly 5x the above length, since it continues to grow slowly as particle velocities decrease inversely with the scale factor after they become non-relativistic. In this example the FSL would correspond to 10 million light-years or 3 Mpc today, around the size containing an average large galaxy.
The 2.7 million K photon temperature gives a typical photon energy of 250 electron-volts, thereby setting a typical mass scale for "warm" dark matter: particles much more massive than this, such as GeV – TeV mass WIMPs, would become non-relativistic much earlier than 1 year after the Big Bang and thus have FSL's much smaller than a proto-galaxy, making them cold. Conversely, much lighter particles, such as neutrinos with masses of only a few eV, have FSL's much larger than a proto-galaxy, thus qualifying them as hot.
Cold dark matter.
Cold dark matter offers the simplest explanation for most cosmological observations. It is dark matter composed of constituents with an FSL much smaller than a protogalaxy. This is the focus for dark matter research, as hot dark matter does not seem to be capable of supporting galaxy or galaxy cluster formation, and most particle candidates slowed early.
The constituents of cold dark matter are unknown. Possibilities range from large objects like MACHOs (such as black holes) or RAMBOs (such as clusters of brown dwarfs), to new particles such as WIMPs and axions.
Studies of Big Bang nucleosynthesis and gravitational lensing convinced most cosmologists that MACHOs cannot make up more than a small fraction of dark matter. According to A. Peter: "... the only "really plausible" dark-matter candidates are new particles."
The DAMA/NaI experiment and its successor DAMA/LIBRA claimed to directly detect dark matter particles passing through the Earth, but many researchers remain skeptical, as negative results from similar experiments seem incompatible with the DAMA results.
Many supersymmetric models offer dark matter candidates in the form of the WIMPy Lightest Supersymmetric Particle (LSP). Separately, heavy sterile neutrinos exist in non-supersymmetric extensions to the standard model that explain the small neutrino mass through the seesaw mechanism.
Warm dark matter.
Warm dark matter refers to particles with an FSL comparable to the size of a protogalaxy. Predictions based on warm dark matter are similar to those for cold dark matter on large scales, but with less small-scale density perturbations. This reduces the predicted abundance of dwarf galaxies and may lead to lower density of dark matter in the central parts of large galaxies; some researchers consider this to be a better fit to observations. A challenge for this model is the lack of particle candidates with the required mass ~ 300 eV to 3000 eV.
No known particles can be categorized as warm dark matter. A postulated candidate is the sterile neutrino: a heavier, slower form of neutrino that does not interact through the weak force (unlike other neutrinos). Some modified gravity theories, such as scalar-tensor-vector gravity, require warm dark matter to make their equations work.
Hot dark matter.
Hot dark matter consists of particles whose FSL is much larger than the size of a protogalaxy. The neutrino qualifies. They were discovered independently, long before the hunt for dark matter: they were postulated in 1930, and detected in 1956. Neutrinos' mass is less than 10 that of an electron. Neutrinos interact with normal matter only via gravity and the weak force, making them difficult to detect (the weak force only works over a small distance, thus a neutrino triggers a weak force event only if it hits a nucleus head-on). This makes them 'weakly interacting light particles' (WILPs), as opposed to WIMPs.
The three known flavors of neutrinos are the "electron", "muon" and "tau." Their masses are slightly different. Neutrinos oscillate among the flavors as they move. It is hard to determine an exact upper bound on the collective average mass of the three neutrinos (or for any of the three individually). For example, if the average neutrino mass were over 50 "eV/c" (less than 10 of the mass of an electron), the universe would collapse. CMB data and other methods indicate that their average mass probably does not exceed 0.3 "eV/c". Thus, observed neutrinos cannot explain dark matter.
Because galaxy-size density fluctuations get washed out by free-streaming, hot dark matter implies that the first objects that can form are huge supercluster-size pancakes, which then fragment into galaxies. Deep-field observations show instead that galaxies formed first, followed by clusters and superclusters as galaxies clump together.
Detection.
If dark matter is made up of WIMPs, then millions, possibly billions, of WIMPs must pass through every square centimeter of the Earth each second. Many experiments aim to test this hypothesis. Although WIMPs are popular search candidates, the Axion Dark Matter eXperiment (ADMX) searches for axions. Another candidate is heavy hidden sector particles that only interact with ordinary matter via gravity.
These experiments can be divided into two classes: direct detection experiments, which search for the scattering of dark matter particles off atomic nuclei within a detector; and indirect detection, which look for the products of WIMP annihilations.
Direct detection.
Direct detection experiments operate deep underground to reduce the interference from cosmic rays. Detectors include the Stawell mine, the Soudan mine, the SNOLAB underground laboratory at Sudbury, Ontario, the Gran Sasso National Laboratory, the Canfranc Underground Laboratory, the Boulby Underground Laboratory, the Deep Underground Science and Engineering Laboratory and the Particle and Astrophysical Xenon Detector.
These experiments mostly use either cryogenic or noble liquid detector technologies. Cryogenic detectors operating at temperatures below 100mK, detect the heat produced when a particle hits an atom in a crystal absorber such as germanium. Noble liquid detectors detect scintillation produced by a particle collision in liquid xenon or argon. Cryogenic detector experiments include: CDMS, CRESST, EDELWEISS, EURECA. Noble liquid experiments include ZEPLIN, XENON, DEAP, ArDM, WARP, DarkSide, PandaX, and LUX, the Large Underground Xenon experiment. Both of these techniques distinguish background particles (that scatter off electrons) from dark matter particles (that scatter off nuclei). Other experiments include SIMPLE and PICASSO.
The DAMA/NaI, DAMA/LIBRA experiments detected an annual modulation in the event rate that they claim is due to dark matter. (As the Earth orbits the Sun, the velocity of the detector relative to the dark matter halo will vary by a small amount). This claim is so far unconfirmed and unreconciled with negative results of other experiments.
Directional detection is a search strategy based on the motion of the Solar System around the Galactic Center.
A low pressure time projection chamber makes it possible to access information on recoiling tracks and constrain WIMP-nucleus kinematics. WIMPs coming from the direction in which the Sun is travelling (roughly towards Cygnus) may then be separated from background, which should be isotropic. Directional dark matter experiments include DMTPC, DRIFT, Newage and MIMAC.
Results.
In 2009, CDMS researchers reported two possible WIMP candidate events. They estimate that the probability that these events are due to background (neutrons or misidentified beta or gamma events) is 23%, and conclude "this analysis cannot be interpreted as significant evidence for WIMP interactions, but we cannot reject either event as signal."
In 2011, researchers using the CRESST detectors presented evidence of 67 collisions occurring in detector crystals from subatomic particles. They calculated the probability that all were caused by known sources of interference/contamination was 1 in 10.
Indirect detection.
Indirect detection experiments search for the products of WIMP annihilation/decay. If WIMPs are Majorana particles (their own antiparticle) then two WIMPs could annihilate to produce gamma rays or Standard Model particle-antiparticle pairs. If the WIMP is unstable, WIMPs could decay into standard model (or other) particles. These processes could be detected indirectly through an excess of gamma rays, antiprotons or positrons emanating from high density regions. The detection of such a signal is not conclusive evidence, as the sources of gamma ray production are not fully understood.
A few of the WIMPs passing through the Sun or Earth may scatter off atoms and lose energy. Thus WIMPs may accumulate at the center of these bodies, increasing the chance of collision/annihilation. This could produce a distinctive signal in the form of high-energy neutrinos. Such a signal would be strong indirect proof of WIMP dark matter. High-energy neutrino telescopes such as AMANDA, IceCube and ANTARES are searching for this signal.
WIMP annihilation from the Milky Way Galaxy as a whole may also be detected in the form of various annihilation products. The Galactic Center is a particularly good place to look because the density of dark matter may be higher there.
The recent detection by LIGO, the Laser Inferometer Gravitational Observatory, in February 2016, of gravity waves, opens the possibility of observing Dark Matter in a new way. Dark Matter seems to have no effects except gravitational and so the actual observation of Gravitational Waves provides scientists with a new way of observing the phenomena.
Results.
The EGRET gamma ray telescope observed more gamma rays than expected from the Milky Way, but scientists concluded that this was most likely due to incorrect estimation of the telescope's sensitivity.
The Fermi Gamma-ray Space Telescope is searching for similar gamma rays. In April 2012, an analysis of previously available data from its Large Area Telescope instrument produced statistical evidence of a 130 GeV signal in the gamma radiation coming from the center of the Milky Way. WIMP annihilation was seen as the most probable explanation.
At higher energies, ground-based gamma-ray telescopes have set limits on the annihilation of dark matter in dwarf spheroidal galaxies and in clusters of galaxies.
The PAMELA experiment (launched 2006) detected excess positrons. They could be from dark matter annihilation or from pulsars. No excess anti-protons were observed.
In 2013 results from the Alpha Magnetic Spectrometer on the International Space Station indicated excess high-energy cosmic rays that could be due to dark matter annihilation.
Synthesis.
An alternative approach to the detection of WIMPs in nature is to produce them in the laboratory. Experiments with the Large Hadron Collider (LHC) may be able to detect WIMPs produced in collisions of the LHC proton beams. Because a WIMP has negligible interaction with matter, it may be detected indirectly as (large amounts of) missing energy and momentum that escape the detectors, provided other (non-negligible) collision products are detected. These experiments could show that WIMPs can be created, but a direct detection experiment must still show that they exist in sufficient numbers to account for dark matter.
Alternative theories.
Mass in extra dimensions.
In some multidimensional theories, the force of gravity is the only force with effect across all dimensions. This explains the relative weakness of gravity compared to the other forces of nature that cannot cross into extra dimensions. In that case, dark matter could exist in a "Hidden Valley" in other dimensions that only interact with the matter in our dimensions through gravity. That dark matter could potentially aggregate in the same way as ordinary matter, forming other-dimensional galaxies.
Topological defects.
Dark matter could consist of primordial defects ("birth defects") in the topology of quantum fields, which would contain energy and therefore gravitate. This possibility may be investigated by the use of an orbital network of atomic clocks that would register the passage of topological defects by changes to clock synchronization. The Global Positioning System may be able to operate as such a network.
Modified gravity.
Some theories modify the laws of gravity.
The earliest was Mordehai Milgrom's Modified Newtonian Dynamics (MOND) in 1983, which adjusts Newton's laws to increase gravitational field strength where gravitational acceleration becomes tiny (such as near the rim of a galaxy). It had some success explaining rotational velocity curves of elliptical and dwarf elliptical galaxies, but not galaxy cluster gravitational lensing. MOND was not relativistic: it was an adjustment of the Newtonian account. Attempts were made to bring MOND into conformity with general relativity; this spawned competing MOND-based hypotheses—including TeVeS, MOG or STV gravity and the phenomenological covariant approach.
In 2007, Moffat proposed a modified gravity hypothesis based on nonsymmetric gravitational theory (NGT) that claims to account for the behavior of colliding galaxies. This model requires the presence of non-relativistic neutrinos or other cold dark matter, to work.
Another proposal uses a gravitational backreaction from a theory that explains gravitational force between objects as an action, a reaction and then a back-reaction. Thus, an object A affects an object B, and the object B then re-affects object A, and so on: creating a feedback loop that strengthens gravity.
In 2008, another group proposed "dark fluid", a modification of large-scale gravity. It hypothesized that attractive gravitational effects are instead a side-effect of dark energy. Dark fluid combines dark matter and dark energy in a single energy field that produces different effects at different scales. This treatment is a simplification of a previous fluid-like model called the generalized Chaplygin gas model in which the whole of spacetime is a compressible gas. Dark fluid can be compared to an atmospheric system. Atmospheric pressure causes air to expand and air regions can collapse to form clouds. In the same way, the dark fluid might generally disperse, while collecting around galaxies.
Spacetime fractality.
Applying relativity to fractal, non-differentiable spacetime, Nottale suggests that potential energy may arise due to the fractality of spacetime, which would account for the missing mass-energy observed at cosmological scales.
Popular culture.
Mention of dark matter is made in some video games and other works of fiction. In such cases, it is usually attributed extraordinary physical or magical properties. Such descriptions are often inconsistent with the hypothesized properties of dark matter in physics and cosmology.

</doc>
<doc id="8653" url="https://en.wikipedia.org/wiki?curid=8653" title="Ducati">
Ducati

Ducati Motor Holding S.p.A. is an Italian company that designs and manufactures motorcycles. Headquartered in Bologna, Italy, Ducati is owned by German automotive manufacturer Audi through its Italian subsidiary Lamborghini, which is all owned by the Volkswagen Group.
History.
 In 1926 Antonio Cavalieri Ducati and his three sons, Adriano, Marcello, and Bruno Cavalieri Ducati; founded "Società Scientifica Radio Brevetti Ducati" in Bologna to produce vacuum tubes, condensers and other radio components. In 1935 they had become successful enough to enable construction of a new factory in the Borgo Panigale area of the city. Production was maintained during World War II, despite the Ducati factory being a repeated target of Allied bombing.
Meanwhile, at the small Turinese firm SIATA ("Societa Italiana per Applicazioni Tecniche Auto-Aviatorie"), Aldo Farinelli began developing a small pushrod engine for mounting on bicycles. Barely a month after the official liberation of Italy in 1944, SIATA announced its intention to sell this engine, called the "Cucciolo" (Italian for "puppy," in reference to the distinctive exhaust sound) to the public. The first Cucciolos were available alone, to be mounted on standard bicycles, by the buyer; however, businessmen soon bought the little engines in quantity, and offered complete motorized-bicycle units for sale.
In 1950, after more than 200,000 Cucciolos had been sold, in collaboration with SIATA, the Ducati firm finally offered its own Cucciolo-based motorcycle. This first Ducati motorcycle was a 48 cc bike weighing with a top speed of had a 15 mm carburetor giving just under . Ducati soon dropped the Cucciolo name in favor of "55M" and "65TL".
When the market moved toward larger motorcycles, Ducati management decided to respond, making an impression at an early-1952 Milan show, introducing their 65TS cycle and Cruiser (a four-stroke motor scooter). Despite being described as the most interesting new machine at the 1952 show, the Cruiser was not a great success, and only a few thousand were made over a two-year period before the model ceased production.
In 1953, management split the company into two separate entities, Ducati Meccanica SpA and Ducati Elettronica, in acknowledgment of its diverging motorcycle and electronics product lines. Ducati Elettronica became Ducati Energia SpA in the eighties. Dr. Giuseppe Montano took over as head of Ducati Meccanica SpA and the Borgo Panigale factory was modernized with government assistance. By 1954, Ducati Meccanica SpA had increased production to 120 bikes a day.
In the 1960s, Ducati earned its place in motorcycling history by producing the fastest 250 cc road bike then available, the Mach 1. In the 1970s Ducati began producing large-displacement V-twin motorcycles and in 1973, released a V-twin with the trademarked desmodromic valve design. In 1985, Cagiva bought Ducati and planned to rebadge Ducati motorcycles with the Cagiva name. By the time the purchase was completed, Cagiva kept the "Ducati" name on its motorcycles. Eleven years later, in 1996, Cagiva accepted the offer from Texas Pacific Group and sold a 51% stake in the company for US$325 million; then, in 1998, Texas Pacific Group bought most of the remaining 49% to become the sole owner of Ducati. In 1999, TPG issued an initial public offering of Ducati stock and renamed the company "Ducati Motor Holding SpA". TPG sold over 65% of its shares in Ducati, leaving TPG the majority shareholder. In December 2005, Ducati returned to Italian ownership with the sale of Texas Pacific's stake (minus one share) to Investindustrial Holdings, the investment fund of Carlo and Andrea Bonomi.
In April 2012, Volkswagen Group's Audi subsidiary announced its intention to buy Ducati for € (US$). Volkswagen chairman Ferdinand Piëch, a motorcycle enthusiast, had long coveted Ducati, and had regretted that he passed up an opportunity to buy the company from the Italian government in 1984. Analysts doubted a tiny motorcycle maker would have a meaningful effect on a company the size of Volkswagen, commenting that the acquisition has "a trophy feel to it," and, "is driven by VW's passion for nameplates rather than industrial or financial logic". Italian luxury car brand Lamborghini was strengthened under VW ownership. AUDI AG's Automobili Lamborghini S.p.A. subsidiary acquired 100 percent of the shares of Ducati Motor Holding S.p.A. on July 19, 2012 for € (US$).
Ownership.
Since 1926, Ducati has been owned by a number of groups and companies.
 From the 1960s to the 1990s, the Spanish company MotoTrans licensed Ducati engines and produced motorcycles that, although they incorporated subtle differences, were clearly Ducati-derived. MotoTrans's most notable machine was the 250 cc "24 Horas" (Spanish for "24 hours").
Motorcycle designs.
Ducati is best known for high performance motorcycles characterized by large capacity four-stroke, 90° V-twin engines, with a desmodromic valve design. Ducati refers to this configuration as L-twin because one cylinder is vertical while the other is horizontal, making it look like a letter "L". Modern Ducatis remain among the dominant performance motorcycles available today partly because of the desmodromic valve design, which is nearing its 50th year of use. Desmodromic valves are closed with a separate, dedicated cam lobe and lifter instead of the conventional valve springs used in most internal combustion engines in consumer vehicles. This allows the cams to have a more radical profile, thus opening and closing the valves more quickly without the risk of valve-float, which causes a loss of power that is likely when using a "passive" closing mechanism under the same conditions.
While most other manufacturers use wet clutches (with the spinning parts bathed in oil) Ducati previously used multiplate dry clutches in many of their motorcycles. The dry clutch eliminates the power loss from oil viscosity drag on the engine, even though the engagement may not be as smooth as the oil-bath versions, but the clutch plates can wear more rapidly. Ducati has converted to wet clutches across their current product lines.
Ducati also extensively uses a trellis frame, although Ducati's MotoGP project broke with this tradition by introducing a revolutionary carbon fibre frame for the Ducati Desmosedici GP9.
Product history.
The chief designer of most Ducati motorcycles in the 1950s was Fabio Taglioni (1920–2001). His designs ranged from the small single-cylinder machines that were successful in the Italian 'street races' to the large-capacity twins of the 1980s. Ducati introduced the Pantah in 1979; its engine was updated in the 1990s in the Ducati SuperSport (SS) series. All modern Ducati engines are derivatives of the Pantah, which uses a toothed belt to actuate the engine's valves. Taglioni used the Cavallino Rampante (identified with the Ferrari brand) on his Ducati motorbikes, Taglioni chose this emblem of courage and daring as a sign of respect and admiration for Francesco Baracca, a heroic World War I fighter pilot who died during an air raid in 1918.
1970s.
In 1973, Ducati commemorated its 1972 win at the Imola 200 with the production model green frame Ducati 750 SuperSport.
Ducati also targeted the offroad market with the two-stroke Regolarità 125, building 3,486 models from 1975 to 1979, but the bike was not successful.
In 1975, the company introduced the 860 GT, designed by noted car stylist Giorgetto Giugiaro. Its angular lines were unique, but raised handlebars made for an uncomfortable seating position at high speeds and also caused steering issues.
1980s.
Ducati's liquid-cooled multi-valve V-twins made from 1985 on are known as "Desmoquattro" ("desmodromic valve four"). These include the 851, 916 and 996, 999 and a few predecessors and derivatives.
The Ducati Paso was introduced in 1986 with the Paso 750, followed in 1989 with the Paso 906. The final version came in 1991 with the 907IE (Iniezione Elettronica), now without the name "Paso". The design was from the hand of Massimo Tamburini, who also designed the Ducati 916 and MV Agusta F4. The Paso was a typical "you love it, you hate it" bike. However, at that time it looked like that all-enclosed bodywork would be the future for all motorcycles. The Paso design was copied for the Moto Morini Dart 400 and 125. Together with Tamburini's Bimota DB1, they were enormously influential in terms of styling.
1990s.
In 1993, Miguel Angel Galluzzi introduced the Ducati Monster, a naked bike with exposed trellis and engine. Today the Monster accounts for almost half of the company's worldwide sales. The Monster has undergone the most changes of any motorcycle that Ducati has ever produced.
In 1993, Pierre Terblanche, Massimo Bordi and Claudio Domenicali designed the Ducati Supermono. A 550 cc single-cylinder lightweight "Catalog Racer". Only 67 were built between 1993 and 1997.
In 1994, the company introduced the Ducati 916 model designed by Massimo Tamburini, a water-cooled version that allowed for higher output levels and a striking new bodywork that had aggressive lines, an underseat exhaust, and a single-sided swingarm. Ducati has since ceased production of the 916, supplanting it (and its progeny, the 748, 996 and 998) with the 749 and 999.
2000s.
In 2006, the retro-styled Ducati PaulSmart 1000 LE was released, which shared styling cues with the 1973 750 SuperSport (itself a production replica of Paul Smart's 1972 race winning 750 Imola Desmo), as one of a SportClassic series representing the 750 GT, 750 Sport, and 750 SuperSport Ducati motorcycles.
Motorcycle design history.
Ducati (in its various incarnations) has produced several styles of motorcycle engines, including varying the number of cylinders, type of valve actuation and fuel delivery. Ducati is best known for its V-twin engine, called a L-twin by the company, which is the powerplant in the majority of Ducati-marqued motorcycles.
Ducati has also manufactured engines with one, two, three or four cylinders; operated by pull rod valves and push rod valves; single, double and triple overhead camshafts; two-stroke and even at one stage manufactured small diesel engines, many of which were used to power boats, generators, garden machinery and emergency pumps (for example, for fire fighting). The engines were the IS series from air-cooled and the larger twin DM series water- and air-cooled. The engines have been found in all parts of the globe. Wisconsin Diesel even assembled and "badge engineered" the engines in the USA. They have also produced outboard motors for marine use. Currently, Ducati makes no other engines except for its motorcycles.
On current Ducati motors, except for the Desmosedici and 1199 Panigale, the valves are actuated by a standard valve cam shaft which is rotated by a timing belt driven by the motor directly. The teeth on the belt keep the camshaft drive pulleys indexed. On older Ducati motors, prior to 1986, drive was by solid shaft that transferred to the camshaft through bevel-cut gears. This method of valve actuation was used on many of Ducati's older single-cylinder motorcycles — the shaft tube is visible on the outside of the cylinder.
Ducati is also famous for using the desmodromic valve system championed by engineer and designer Fabio Taglioni, though the firm has also used engines that use valve springs to close their valves. In the early days, Ducati reserved the desmodromic valve heads for its higher performance bikes and its race bikes. These valves do not suffer from valve float at high engine speeds, thus a desmodromic engine is capable of far higher revolutions than a similarly configured engine with traditional spring-valve heads.
In the 1960s and 1970s, Ducati produced a wide range of small two-stroke bikes, mainly sub-100 cc capacities. Large quantities of some models were exported to the United States.
Ducati has produced the following motorcycle engine types:
Enthusiasts groups.
A key part of Ducati's marketing strategy since the 1990s has been fostering a distinct community identity in connection with branding efforts including online communities and local, regional and national Ducati enthusiast clubs. There are more than 400 Ducati clubs worldwide and 20,000 registered users of the Ducati Owners Club web site and 17,000 subscribers to the racing web site. Enthusiasts and riders are informally referred to in the motorcycling community as Ducatista (singular) or Ducatisti (plural).
In North America there are several Ducati enthusiasts organizations with varying degrees of factory sponsorship. DESMO, the Ducati Enthusiast Sport Motorcycle Organization, is a North American group affiliated with the factory Desmo Owners Club. Some groups are focused on vintage Ducatis while several are based primarily or entirely on email discussion lists or web forums such as Ducati.net.
Ducati products other than motorcycles.
Ducati Meccanica (as the company was previously known) has its marque on non-motorcycle products as well. In the 1930s and 1940s, Ducati manufactured radios, cameras, and electrical products such as a razor. Ducati made a marine binocular called the BIMAR for the Kriegsmarine during World War II, some of which were sold on the civilian market after the war. The Ducati Sogno was a half-frame Leica-like camera which is now a collector's item. Ducati and Bianchi have developed and launched a new line of racing bicycles.
Currently, there are four Ducati companies: Ducati Motor Holding (the subject of this article), Ducati Corse (which runs the Ducati racing program and is wholly owned by Ducati Motor Holding), Ducati Energia, a designer and manufacturer of electrical and electronic components and systems and Ducati Sistemi, a subsidiary of Ducati Energia. All are located in Borgo Panigale in Bologna, Italy.
Ducati Motor Holding often uses electrical components and subsystems from Ducati Energia.
Merchandising.
Ducati has a wide range of accessories, lifestyle products and co-branded merchandise bearing their logos and designs. The company has a licensing agreement with Tumi Inc., launching a collection of eight co-branded luggage pieces in 2006 sold through both of the brands' retail outlets.
Racing history.
Ducati's history with motorsport began with speed records on Cucciolo motorized bicycle factory racers in 1951, followed in 1954 with bringing in Fabio Taglioni to found a road racing program with the 100 Gran Sport. , Ducati was still pursuing the "win on Sunday, sell on Monday" business model and spending 10% of company revenues, €, on its racing business.
MotoGP World Championship.
Ducati rejoined Grand Prix motorcycle racing in , after a 30-year absence. On September 23, 2007, Casey Stoner clinched his and Ducati's first Grand Prix World Championship.
When Ducati re-joined MotoGP in , MotoGP had changed its rules to allow four-stroke 990 cc engines to race. At the time Ducati was the fastest bike. In , MotoGP reduced the engine size to "", and Ducati continued to be the fastest with a bike that was markedly quicker than its rivals as was displayed by Casey Stoner on tracks with long straights.
For , Ducati Marlboro Team campaigned their Desmosedici GP9 with former World Champions Casey Stoner and Nicky Hayden.
Ducati also supplies customer bikes to the Alice Team, with Mika Kallio and Niccolò Canepa riding for the team in 2009.
Nine-time world champion Valentino Rossi rode for Ducati Corse for the and seasons.
Rossi returned to the Yamaha team for the 2013 season.
For , Ducati Marlboro Team raced with Nicky Hayden and the Italian rider Andrea Dovizioso. In 2014 Cal Crutchlow teamed up with Dovizioso for the season, and he left at the end of the year.
In , Ducati Marlboro Team, under the control of the new race team director Gigi Dall'Igna and the new Desmosedici GP15, raced with two Italian riders: Andrea Dovizioso and Andrea Iannone.
For , Ducati Marlboro Team has confirmed the Team of 2015, Andrea Dovizioso and Andrea Iannone as official riders, Michele Pirro as official tester.
They also put under contract Casey Stoner, the only rider able to win a World Championship with the Italian Bike. He will be the World Superbike and Moto GP tester, occasionally.
Superbike World Championship (SBK).
The company has won 14 riders world championships and 17 manufacturers world championships, competing since the series' inception in 1988. At the end of 2015, Ducati has amassed 318 wins, more than any other manufacturer involved in the championship.
Ducati has also won the manufacturer world championship for years 1991–1996, 1998–2004, 2006, 2008–2009 and 2011.
British Superbike Championship.
The British Superbike Championship has been won by Ducati riders on eight occasions and entered since 1988:
AMA Superbike Championship.
In the AMA Superbike Championship, Ducati has had its share of success, with Doug Polen winning the title in 1993 and Troy Corser the following year in 1994. Ducati has entered a bike in every AMA Superbike season since 1986, but withdrew from the series after the 2006 season.
Ducati had an important place in early Superbike racing history in the United States and vice versa: In 1977, "Cycle" magazine editors Cook Neilson and Phil Schilling took a Ducati 750SS to first place at Daytona in the second-ever season of AMA Superbike racing. "Neilson retired from racing at the end of the year, but the bike he and Schilling built — nicknamed Old Blue for its blue livery — became a legend," says Richard Backus from Motorcycle Classics: "How big a legend? Big enough for Ducati to team with Italian specialty builder NCR to craft a limited-edition update, New Blue, based on the 2007 Sport 1000S, and big enough to inspire the crew at the Barber Vintage Motorsports Museum (see Barber Motorsports Park), arguably one of the most important motorcycle museums in the world, to commission Ducati specialist Rich Lambrechts to craft a bolt-by-bolt replica for its collection. The finished bike's name? Deja Blue."
Formula TT.
Ducati's first ever world title was the 1978 TT Formula 1 World Championship, achieved thanks to Mike Hailwood's victory at the Isle of Man TT. Between 1981 and 1984 Tony Rutter won four TT Formula 2 World Championships riding Ducati bikes.

</doc>
<doc id="8654" url="https://en.wikipedia.org/wiki?curid=8654" title="Data General Nova">
Data General Nova

The Data General Nova was a popular 16-bit minicomputer built by the American company Data General starting in 1969. The Nova was packaged into a single rack mount case and had enough power to do most simple computing tasks. The Nova became popular in science laboratories around the world, and eventually 50,000 units were sold. It was succeeded by the Data General Eclipse, which was similar in most ways but added virtual memory support and other features required by modern operating systems.
History.
de Castro and the Nova’s origin.
Edson de Castro was the Product Manager at Digital Equipment Corporation (DEC) of their pioneering PDP-8, a 12-bit computer generally considered by most to be the first true minicomputer. De Castro was convinced, however, that it was possible to improve upon the PDP-8 by building a 16-bit minicomputer on a single board. Ken Olsen was not supportive of this project, so de Castro left DEC along with another hardware engineer, Richard Sogge, and a software engineer, Henry Burkhardt III, to found Data General (DG) in 1968. The fourth founder, Herbert Richman, had been a salesman for Fairchild Semiconductor and knew the others through his contacts with Digital Equipment.
In keeping with the original concept, the Nova was based on two printed circuit boards, one for the CPU and another for various support systems. The boards were designed so they could be connected together using a printed circuit backplane, with minimal manual wiring, allowing all the boards to be built in an automated fashion. This greatly reduced costs over the rival PDP-8 and PDP-8/I, which consisted of many smaller boards that had to be wired together at the backplane. The larger-board construction also made the Nova more reliable, which made it especially attractive for industrial or lab settings. Fairchild Semiconductor provided the medium-scale integration (MSI) chips used throughout the system. The Nova was one of the first 16-bit minicomputers and was a leader in moving to word lengths that were multiples of the 8-bit byte in that market. 
DG released the Nova in 1969 at a base price of US$3,995, advertising it as "the best small computer in the world." The basic model was not very useful out of the box, and adding RAM in the form of core memory typically brought the price up to $7,995. Starting in 1969, Data General shipped a total of 50,000 Novas at $8000 each. The Nova’s biggest competition was from the new DEC PDP-11 computer series, and to a lesser extent the older DEC PDP-8 systems. It has been said that the Nova was crude compared to its competitors; but it was quite effective and very fast for its day, at least at its low-cost end of the market.
SuperNOVA.
A further improvement on the Nova design followed the next year, the SuperNOVA. The SuperNOVA included a number of improvements that dramatically improved performance over the original model. This included the use of ROM for library software that could be run much faster than the same code in the normal core memory, due to the latter’s need to be written immediately after being read. Additionally the system included a new set of core with an 800 ns cycle time, faster than the original’s 1200 ns version. Finally the SuperNOVA also replaced the earlier model’s 4-bits-at-a-time math unit with a new 16-bit parallel version, speeding math by up to four times.
Soon after the introduction of the SuperNOVA, the SuperNOVA SC was introduced, featuring semiconductor (SC) memory in place of core. The much higher performance memory allowed the CPU, which was synchronous with memory, to be further increased in speed to run at a 300 ns cycle time (3.3 MHz), which made it the fastest minicomputer for over a decade following its introduction.
Later versions.
Further improvements in the line followed in 1970/1 with a pair of machines that replaced the Nova/SuperNOVA, the Nova 1200 and Nova 800 series. The 1200 used 1200 ns core while the 800 featured the SuperNOVA’s 800 ns core, explaining the somewhat confusing naming where the lower number represents the faster machine. Like the earlier models, the 1200 used a 4-bit math unit while the 800 used the SuperNOVA’s 16-bit unit. Both models were offered in a variety of cases, the 1200 with seven slots, the 1210 with four and the 1220 with fourteen. The 840, first offered in 1973, also included a new paged memory system allowing for addresses of up to 17-bits. An index offset the base address into the larger 128 kWord memory. Actually installing this much memory required considerable space; the 840 shipped in a large 14-slot case.
The next version was the Nova 2, with the first versions shipping in 1973. The Nova 2 was essentially a simplified version of the earlier machines as increasing chip densities allowed the CPU to be reduced in size. While the SuperNOVA used three 15×15" boards to implement the CPU and its memory, the Nova 2 fitted all of this onto a single board. ROM was used to store the boot code, which was then copied into core when the “program load” switch was flipped. Versions were available with four, seven and ten slots.
The Nova 3 of 1975 added two more registers, used to control access to a built-in stack. The processor was also re-implemented using TTL components, further increasing the performance of the system. The Nova 3 was offered in four-slot (the Nova 3/4) and twelve-slot (the Nova 3/12) versions.
It appears that Data General originally intended the Nova 3 to be the last of its line, planning to replace the Nova with the later Eclipse machines. However, continued demand led to a Nova 4 machine, this time based on four AMD 2901 bit-slice ALUs. This machine was designed from the start to be both the Nova 4 and the Eclipse S/140, with different microcode for each. A floating-point co-processor was also available, taking up a separate slot. An additional option allowed for memory mapping, allowing programs to access up to 128 kWords of memory using bank switching. Unlike the earlier machines, the Nova 4 did not include a front panel console and instead relied on the terminal to emulate a console when needed.
There were three different versions of the Nova 4, the Nova 4/C, the Nova 4/S and the Nova 4/X. The Nova 4/C was a single-board implementation that included all of the memory (16 or 32 kWords). The Nova 4/S and 4/X used separate memory boards. The Nova 4/X had the on-board memory management unit (MMU) enabled to allow up to 128 kWords of memory to be used (the MMU was also installed in the Nova 4/S, but was disabled by firmware). Both the 4/S and the 4/X included a “prefetcher” to increase performance by fetching up to two instructions from memory before they were needed.
microNOVA.
Data General also produced a series of single-chip implementations of the Nova processor as the microNOVA. Changes to the bus architecture limited speed dramatically, to the point where it was about one-half the speed of the original Nova. The original microNOVA with the “mN601” processor shipped in 1977. It was followed by the microNOVA MP/100 in 1979, which reduced the CPU to a single VLSI chip, the mN602. A larger version was also offered as the microNOVA MP/200, shipping the same year.
The microNOVA was later re-packaged in a PC-style case with two floppy disks as the Enterprise. Enterprise shipped in 1981, running RDOS, but the introduction of the IBM PC the same year made most other machines disappear under the radar.
Nova’s legacy.
The Nova influenced the design of both the Xerox Alto (1973) and Apple I (1976) computers, and its architecture was the basis for the Computervision CGP (Computervision Graphics Processor) series. Its external design has been reported to be the direct inspiration for the front panel of the MITS Altair (1975) microcomputer.
Data General followed up on the success of the original Nova with a series of faster designs. The Eclipse family of systems was later introduced with an extended upwardly compatible instruction set, and the MV-series further extended the Eclipse into a 32-bit architecture to compete with the DEC VAX. The development of the MV-series was documented in Tracy Kidder’s popular 1981 book, "The Soul of a New Machine". Data General itself would later evolve into a vendor of Intel processor-based servers and storage arrays, eventually being purchased by EMC.
Technical description.
Processor design.
The Nova, unlike the PDP-8, was a load-store architecture. It had four 16-bit accumulator registers, of which two (2 and 3) could be used as index registers. There was a 15-bit program counter and a single-bit carry register. As with the PDP-8, current + zero page addressing was central. There was no stack register, but later Eclipse designs would utilize a dedicated hardware memory address for this function.
The earliest models of the Nova processed math serially in 4-bit packets, using a single 74181 bitslice ALU. A year after its introduction this design was improved to include a full 16-bit parallel math unit using four 74181s, this design being referred to as the SuperNova. Future versions of the system added a stack unit and hardware multiply/divide.
The Nova 4 / Eclipse S/140 was based on four AMD 2901 bit-slice ALUs, with microcode in read-only memory, and was the first Nova designed for DRAM main memory only, without provision for magnetic core memory.
Memory and I/O.
The first models were available with 8K words of magnetic core memory as an option, one that practically everyone had to buy, bringing the system cost up to $7,995. 
This core memory board, Part Number 50823 D8 7504-14166, and with layout artwork copyrighted 1971 by DGC, was organized in planar fashion as four groups of four banks, each bank carrying two sets of core in a 64 by 64 matrix; thus there were 64 x 64 = 4096 bits per set, x 2 sets giving 8,192 bits, x 4 banks giving 32,768 bits, x 4 groups giving a total of 131,072 bits, and this divided by the machine word size of 16 bits gave 8,192 Words of memory. 
The core on this 8K Word memory board occupied a centrally located 'board-on-a-board' 5.25" wide by 6.125" high and was covered by a protective plate. It was surrounded by the necessary support driver read-write-rewrite circuitry and epitomized the state of the art of core memory, soon to be replaced by solid state memory. Even here DG managed to innovate, packing this very small core and the corresponding support electronics onto a single standard 15 x board. Up to 32K of such core RAM could be supported in one external expansion box. Semiconductor ROM was already available at the time, and RAM-less systems (i.e. with ROM only) became popular in many industrial settings. The original Nova machines ran at approximately 200 kHz, but its SuperNova was designed to run at up to 3 MHz when used with special semiconductor main memory.
The standardized backplane and I/O signals created a simple, efficient I/O design that made interfacing programmed I/O and Data Channel devices to the Nova simple compared to competing machines. In addition to its dedicated I/O bus structure, the Nova backplane had wire wrap pins that could be used for non-standard connectors or other special purposes.
Programming model.
The instruction format could be broadly categorized into one of three functions: 1) register-to-register manipulation, 2) memory reference, and 3) input/output. Each instruction was contained in one word. The register-to-register manipulation was almost RISC-like in its bit-efficiency; and an instruction that manipulated register data could also perform tests, shifts and even elect to discard the result. Hardware options included an integer multiply and divide unit, a floating-point unit (single and double precision), and memory management.
The earliest Nova came with a BASIC interpreter on paper tape. As the product grew, Data General developed many languages for the Nova computers, running under a range of consistent operating systems. FORTRAN IV, ALGOL, Extended BASIC, Data General Business Basic, Interactive COBOL, and several assemblers were available from Data General. Third party vendors and the user community expanded the offerings with Forth, Lisp, BCPL, C, Algol, and other proprietary versions of COBOL and BASIC.
Instruction set.
The machine instructions implemented below are the common set implemented by all of the Nova series processors. Specific models often implemented additional instructions, and some instructions were provided by optional hardware.
Arithmetic instructions.
All arithmetic instructions operated between accumulators. For operations requiring two operands, one was taken from the source accumulator, and one from the destination accumulator, and the result was deposited in the destination accumulator. For single-operand operations, the operand was taken from the source register and the result replaced the destination register. For all single-operand opcodes, it was permissible for the source and destination accumulators to be the same, and the operation functioned as expected.
All arithmetic instructions included a "no-load" bit which, when set, suppressed the transfer of the result to the destination register; this was used in conjunction with the test options to perform a test without losing the existing contents of the destination register. In assembly language, adding a '#' to the opcode set the no-load bit. 
The CPU contained a single-bit register called the carry bit, which after an arithmetic operation would contain the carry out of the most significant bit. The carry bit could be set to a desired value prior to performing the operation using a two-bit field in the instruction. The bit could be set, cleared, or complemented prior to performing the instruction. In assembly language, these options were specified by adding a letter to the opcode: 'O' — set the carry bit; 'Z' — clear the carry bit, 'C' — complement the carry bit, nothing — leave the carry bit alone. If the no-load bit was also specified, the specified carry value would be used for the computation, but the actual carry register would remain unaltered.
All arithmetic instructions included a two-bit field which could be used to specify a shift option, which would be applied to the result before it was loaded into the destination register. A single-bit left or right shift could be specified, or the two bytes of the result could be swapped. Shifts were 17-bit circular, with the carry bit "to the left" of the most significant bit. In other words, when a left shift was performed, the most significant bit of the result was shifted into the carry bit, and the previous contents of the carry bit were shifted into the least significant bit of the result. Byte swaps did not effect the carry bit. In assembly language, these options were specified by adding a letter to the opcode: 'L' — shift left; 'R' — shift right, 'S' — swap bytes; nothing — do not perform a shift or swap. 
All arithmetic instructions included a three-bit field that could specify a test which was to be applied to the result of the operation. If the test evaluated to true, the next instruction in line was skipped. In assembly language, the test option was specified as a third operand to the instruction. The available tests were:
The actual arithmetic instructions were:
An example arithmetic instructions, with all options utilized, is:
ADDZR# 0,2,SNC
This decoded as: clear the carry bit; add the contents of AC2 (accumulator 2) to AC0; circularly shift the result one bit to the right; test the result to see if the carry bit is set and skip the next instruction if so. Discard the result after performing the test. In effect, this adds two numbers and tests to see if the result is odd or even. 
Memory reference instructions.
The Nova instruction set contained a pair of instructions that transferred memory contents to accumulators and vice versa, two transfer-of-control instructions, and two instructions that tested the contents of a memory location. All memory reference instructions contained an eight-bit address field, and a two-bit field that specified the mode of memory addressing. The four modes were:
Obviously, mode 0 was only capable of addressing the first 256 memory words, given the eight-bit address field. This portion of memory was referred to as "page zero". Page zero memory words were considered precious to Nova assembly language programmers because of the small number available; only page zero locations could be addressed from anywhere in the program without resorting to indexed addressing, which required tying up accumulator 2 or 3 to use as an index register. In assembly language, a ".ZREL" directive caused the assembler to place the instructions and data words that followed it in page zero; an ".NREL" directive placed the following instructions and data words in "normal" memory. Later Nova models added instructions with extended addressing fields, which overcame this difficulty (at a performance penalty). 
The assembler computed relative offsets for mode 1 automatically, although it was also possible to write it explicitly in the source. If a memory reference instruction referenced a memory address in .NREL space but no mode specifier, mode 1 was assumed and the assembler calculated the offset between the current instruction and the referenced location, and placed this in the instruction's address field (provided that the resulting value fit into the 8-bit field). 
The two load and store instructions were:
Both of these instructions included an "indirect" bit. If this bit was set (done in assembly language by adding a '@' to the opcode), the contents of the target address were assumed to be a memory address itself, and that address would be referenced to do the load or store. 
The two transfer-of-control instructions were:
As in the case of the load and store instructions, the jump instructions contained an indirect bit, which likewise was specified in assembly using the '@' character. In the case of an indirect jump, the processor retrieved the contents of the target location, and used the value as the memory address to jump to. However, unlike the load and store instructions, if the indirect address had the most significant bit set, it would perform a further cycle of indirection. On the Nova series processors prior to the Nova 3, there was no limit on the number of indirection cycles; an indirect address that referenced itself would result in an infinite indirect addressing loop, with the instruction never completing. (This could be alarming to users, since when in this condition, pressing the STOP switch on the front panel did nothing. It was necessary to reset the machine to break the loop.)
The two memory test instructions were:
As in the case of the load and store instructions, there was an indirect bit that would perform a single level of indirect addressing. These instructions were odd in that, on the Novas with magnetic core memory, the instruction was executed within the memory board itself. As was common at the time, the memory boards contained a "write-back" circuit to solve the destructive-read problem inherent to magnetic core memory. But the write-back mechanism also contained a mini arithmetic unit, which the processor used for several purposes. For the ISZ and DSZ instructions, the increment or decrement occurred between the memory location being read and the write-back; the CPU simply waited to be told if the result was zero or nonzero. These instructions were useful because they allowed a memory location to be used as a loop counter without tying up an accumulator, but they were slower than performing the equivalent arithmetic instructions.
Some examples of memory reference instructions:
LDA 1,COUNT
Transfers the contents of the memory location labeled COUNT into accumulator 1. Assuming that COUNT is in .NREL space, this instruction is equivalent to: LDA 1,1,(COUNT-(.+1))
where '.' represents the location of the LDA instruction.
JSR@ 0,17
Jump indirect to the memory address specified by the contents of location 17, in page zero space, and deposit the return address in accumulator 3. This was the standard method for making an RDOS system call on early Nova models; the assembly language mnemonic ".SYSTM" translated to this.
JMP 3,0
Jump to the memory location whose address is contained in accumulator 3. This was a common means of returning from a function or subroutine call, since the JSR instruction left the return address in accumulator 3. 
STA 0,3,-1
Store the contents of accumulator 0 in the location that is one less than the address contained in accumulator 3.
DSZ COUNT
Decrement the value in the location labeled COUNT, and skip the next instruction if the result is zero. As in the case above, if COUNT is assumed to be in .NREL space, this is equivalent to: DSZ 1,(COUNT-(.+1))
I/O Instructions.
The Novas implemented a channelized model for interfacing to I/O devices. In the model, each I/O device was expected to implement two flags, referred to as "Busy" and "Done", and three data and control registers, referred to as A, B, and C. I/O instructions were available to read and write the registers, and to send one of three signals to the device, referred to as "start", "clear", and "pulse". In general, sending a start signal initiated an I/O operation that had been set up by loading values into the A/B/C registers. The clear signal halted an I/O operation and cleared any resulting interrupt. The pulse signal was used to initiate ancillary operations on complex subsystems, such as seek operations on disk drives. Polled devices usually moved data directly between the device and the A register. DMA devices generally used the A register to specify the memory address, the B register to specify the number of words to be transferred, and the C register for control flags. Channel 63 referred to the CPU itself and was used for various special functions. 
Each I/O instruction contained a six-bit channel number field, a four-bit to specify which register to read or write, and a two-bit field to specify which signal was to be sent. In assembly language, the signal was specified by adding a letter to the opcode: 'S' for start, 'C' for clear, 'P' for pulse, and nothing for no signal. The opcodes were:
In addition, four instructions were available to test the status of a device:
Starting a device caused it to set its busy flag. When the requested operation was completed, conventionally the device cleared its busy flag and set its done flag; most devices had their interrupt request mechanism wired to the done flag, so setting the done flag caused an interrupt (if interrupts were enabled and the device wasn't masked). 
Special Instructions.
These instructions performed various CPU control and status functions. All of them were actually shorthand mnemonics for I/O instructions on channel 63, the CPU's self-referential I/O channel.
Front panel layout.
As was the convention of the day, most Nova models provided a front panel console to control and monitor CPU functions. Models prior to the Nova 3 all relied on a canonical front panel layout, as shown in the Nova 840 panel photo above. The layout contained a keyed power switch, two rows of address and data display lamps, a row of data entry switches, and a row of function switches that activated various CPU functions when pressed. The address lamps always displayed the current value of the program counter, in binary. The data lamps displayed various values depending on which CPU function was active at the moment. To the left of the leftmost data lamp, an additional lamp displayed the current value of the carry bit. On most models the lamps were incandescent lamps which were soldered to the panel board; replacing burned-out lamps was a bane of existence for Data General field service engineers. 
Each of the data switches controlled the value of one bit in a 16-bit value, and per Data General convention, they were numbered 0-15 from left to right. The data switches provided input to the CPU for various functions, and could also be read by a running program using the READS assembly language instruction. To reduce panel clutter and save money, the function switches were implemented as two-way momentary switches. When a function switch lever was lifted, it triggered the function whose name was printed above the switch on the panel; when the lever was pressed down, it activated the function whose name appeared below the switch. The switch lever returned to a neutral position when released.
Referencing the Nova 840 photo, the first four switches from the left performed the EXAMINE and DEPOSIT functions for the four accumulators. Pressing EXAMINE on one of these caused the current value of the accumulator to be displayed in binary by the data lamps. Pressing DEPOSIT transferred the binary value represented by the current settings of the data switches to the accumulator. 
Going to the right, the next switch was the RESET/STOP switch. Pressing STOP caused the CPU to halt after completing the current instruction. Pressing RESET caused the CPU to halt immediately, cleared a number of CPU internal registers, and sent an I/O reset signal to all connected devices. The switch to the right of that was the START/CONTINUE switch. Pressing CONTINUE caused the CPU to resume executing at the instruction currently pointed at by the program counter. Pressing START transferred the value currently set in data switches 1-15 to the program counter, and then began executing from there.
The next two switches provided read and write access to memory from the front panel. Pressing EXAMINE transferred the value set in data switches 1-15 to the program counter, fetched the value in the corresponding memory location, and displayed its value in the data lamps. Pressing EXAMINE NEXT incremented the program counter and then performed an examine operation on that memory location, allowing the user to step through a series of memory locations. Pressing DEPOSIT wrote the value contained in the data switches to the memory location pointed at by the program counter. Pressing DEPOSIT NEXT first incremented the program counter and then deposited to the pointed-to memory location. 
The INST STEP function caused the CPU to execute one instruction, at the current program counter location, and then halt. Since the program counter would be incremented as part of the instruction execution, this allowed the user to single-step through a program. MEMORY STEP, a misnomer, caused the CPU to run through a single clock cycle and halt. This was of little use to users and was generally only used by field service personnel for diagnostics. 
PROGRAM LOAD was the mechanism usually used to boot a Nova. When this switch was triggered, it caused the 32-word boot ROM to be mapped over the first 32 words of memory, set the program counter to 0, and started the CPU. The boot ROM contained code that would read 256 words (512) bytes of code from a selected I/O device into memory and then transfer control to the read-in code. The data switches 8-15 were used to tell the boot ROM which I/O channel to boot from. If switch 0 was off, the boot ROM would assume the device was a polled device (e.g., the paper tape reader) and run a polled input loop until 512 bytes had been read. If switch 0 was on, the boot room assumed the device was a DMA-capable device and it initiated a DMA data transfer. The boot ROM was not smart enough to position the device prior to initiating the transfer. This was a problem when rebooting after a crash; if the boot device was a disk drive, its heads had likely been left on a random cylinder. They had to be repositioned to cylinder 0, where RDOS wrote the first-level boot block, in order for the boot sequence to work. Conventionally this was done by cycling the drive through its load sequence, but users who got frustrated with the wait time (up to 5 minutes depending on the drive model) learned how to input from the front panel a drive "recalibrate" I/O code and single-step the CPU through it, an operation that took an experienced user only a few seconds. 
The power switch was a 3-way keyed switch with positions marked OFF, ON, and LOCK. In the OFF position all power was removed from the CPU. Turning the key to ON applied power to the CPU. However, unlike current CPUs, the CPU did not start automatically when power was applied; the user had to use PROGRAM LOAD or some other method to start the CPU and initiate the boot sequence. Turning the switch to LOCK disabled the front panel function switches; by turning the switch to LOCK and removing the key, the user could render the CPU resistant to tampering. On systems with magnetic core memory, the LOCK position also enabled the auto power failure recovery function. The key could be removed in the OFF or LOCK positions.
Performance.
The Nova 1200 executed core memory access instructions (LDA and STA) in 2.55 microseconds (μs). Use of read only memory saved 0.4 μs. Accumulator instructions (ADD, SUB, COM, NEG, etc.) took 1.55 μs, MUL 2.55 μs, DIV 3.75 μs, ISZ 3.15-4.5 μs. On the later Eclipse MV/6000, LDA and STA took 0.44 μs, ADD, etc. took 0.33 μs, MUL 2.2 μs, DIV 3.19 μs, ISZ 1.32 μs, FAD 5.17 μs, FMMD 11.66 μs.
Assembly language examples.
Hello world program.
This is a minimal programming example in Nova assembly language. It is designed to run under RDOS and prints the string “Hello, world.” on the console.
16-bit multiplication.
Basic models of the Nova came without built-in hardware multiply and divide capability, to keep prices competitive. The following routine multiplies two 16-bit words to produce a 16-bit word result (overflow is ignored). It demonstrates combined use of ALU op, shift, and test (skip). Note that when this routine is called by jsr, AC3 holds the return address. This is used by the return instruction jmp 0,3. An idiomatic way to clear an accumulator is sub 0,0. Other single instructions can be arranged to load a specific set of useful constants (e.g. -2, -1, or +1).
Binary print accumulator.
The following routine prints the value of AC1 as a 16 digit binary number, on the RDOS console. It reveals further quirks of the Nova instruction set. For instance, there is no instruction to load an arbitrary “immediate” value into an accumulator (although memory reference instructions do encode such a value to form an effective address). Accumulators must generally be loaded from initialized memory locations (e.g. n16). Other contemporary machines such as the PDP-11, and practically all modern architectures, allow for immediate loads, although many such as ARM restrict the range of values that can be loaded immediately.
Because the RDOS .systm call macro implements a jsr, AC3 is overwritten by the return address for the .pchar function. Therefore a temporary location is needed to preserve the return address of the caller of this function. For a recursive or otherwise re-entrant routine, a stack, hardware if available, software if not, must be used instead. The return instruction becomes jmp @ retrn which exploits the Nova's indirect addressing mode to load the return PC. 
The constant definitions at the end show two assembler features: the assembler radix is octal by default (20 = sixteen), and character constants could be encoded as e.g. "0.
Emulating a Data General Nova.
Nova assembly language programs can be run under Bob Supnik’s SIMH emulator, in RDOS. Of the above examples, only "Hello, world" is a complete program. It includes the necessary directives for a successful assembly and generation of a runnable program. 
Stepwise instructions
Start the Nova emulation and boot RDOS following the instructions under “Nova and Eclipse RDOS” in the file src/simh_swre.txt of the simh distribution. After booting, RDOS’ command prompt, R, should appear on the screen. 
Before going further with serious experimentation, it can be convenient to check one’s programs at the PC using a suitable cross-assembler, such as the portable PDP-8/DG Nova cross-assembler listed in the External links section, before attempting execution in the RDOS environment.
RDOS hints
Facts.
The Canadian Broadcasting Corporation in Montreal used the Nova 1200 for channel play-out automation up until the late 80's. It was then replaced with refurbished Nova 4 units and these were in use until the mid 90's.

</doc>
<doc id="8659" url="https://en.wikipedia.org/wiki?curid=8659" title="Protestant Church in the Netherlands">
Protestant Church in the Netherlands

The Protestant Church in the Netherlands (, abbreviated PKN) is the largest Protestant Christian denomination in the Netherlands, being both Reformed (Calvinist) and Lutheran. With a membership of some 1.721.000 people (or 10.2 % of the Dutch population, 2012), it is the second largest church in the Netherlands after the Catholic Church. Historically the various Protestant churches had collectively formed the largest Christian denomination in the country, with about 60% of the population being Protestant in the early 20th century, but religiosity drastically declined after the 1960s. It is the traditional faith of the Dutch Royal Family – a remnant of the church's historical dominance.
The PKN was founded 1 May 2004 as the merger of the Dutch Reformed Church, the Reformed Churches in the Netherlands and the Evangelical Lutheran Church in the Kingdom of the Netherlands. The merger was the culmination of an organizational process started in 1961. 
Doctrine and practice.
The doctrine of the Protestant Church in the Netherlands is expressed in its creeds. In addition to holding the Apostles', the Nicene and the Athanasian Creeds of the universal church, it also holds to the confessions of its predecessor bodies. From the Lutheran tradition are the unaltered Augsburg Confession and Luther's Catechism. From the Reformed, the Heidelberg and Genevan Catechisms along with the Belgic Confession with the Canons of Dordt. The Church also acknowledges the Theological Declaration of Barmen and the Leuenberg Agreement. Ordination of women and blessings of same-sex marriages are allowed.
The PKN contains both liberal and conservative movements; although the liberal Remonstrants left talks when they could not agree with the unaltered adoption of the Canons of Dordt. Local congregations have far-reaching powers concerning "controversial" matters (such as admittance to holy communion or whether women are admitted as members of the congregation's consistory).
Organization.
The polity of the Protestant Church in the Netherlands is a hybrid of presbyterian and congregationalist church governance. Church governance is organised along local, regional, and national lines. At the local level is the congregation. An individual congregation is led by a church council made of the minister along with elders and deacons elected by the congregation. At the regional level are the 57 classical assemblies whose members are chosen by the church councils. At the national level is the General Synod which directs areas of common interest, such as theological education, ministry training and ecumenical co-operation.
The PKN has four different types of congregations:
Lutherans are a minority (about 1 percent) of the PKN's membership. To ensure that Lutherans are represented in the Church, the Lutheran congregations have their own synod. The Lutheran Synod also has representatives in the General Synod.
Secularization.
Secularization, or the decline in religiosity, first became noticeable after 1960 in the Protestant rural areas of Friesland and Groningen. Then, it spread to Amsterdam, Rotterdam and the other large cities in the west. Finally the Catholic southern areas showed religious declines. A countervailing trend is produced by a religious revival in the Protestant Bible Belt, and the growth of Muslims and Hindu communities resulting from immigration and high birth rates.
Research in 2007 concludes that 42% of the members of the PKN is a non-theist Furthermore, in the Protestant Church in the Netherlands (PKN) and several other smaller denominations of the Netherlands, 1 in 6 clergy are either agnostic or atheist.A minister of the PKN, Klaas Hendrikse has described God as "a word for experience, or human experience" and said that Jesus may have never existed.
Separations.
Only those congregations belonging to the former Reformed Churches in the Netherlands have the legal right to secede from the PKN without losing its property and church during a transition period of 10 years. Seven congregations have so far decided to form the Continued Reformed Churches in the Netherlands. Two congregations have joined one of the other smaller Reformed churches in the Netherlands. Some minorities within congregations that joined the PKN decided to leave the church and associated themselves individually with one of the other Reformed churches.
Some congregations and members in the Dutch Reformed Church did not agree with the merger and have separated. They have organized themselves in the Restored Reformed Church. Estimations of their membership vary from 35,000 up to 70,000 people in about 120 local congregations. They disagree with the pluralism of the merged church which maintains, as they see it, contradicting Reformed and Lutheran confessions. This group also considers same-sex marriages and female clergy unbiblical.
Involvement in the Middle East.
A PKN supported organization, Kerk in Actie, employs an individual to represent them in Israel who works at the Palestinian Christian non-profit Sabeel in Jerusalem, which promotes the Kairos Palestine document and the Boycott, Divestment and Sanctions campaign against Israel. In addition, Kerk in Actie works together with Interchurch Organisation for Development Cooperation also involved in controversial activities critical of Israel.
In a meeting of eight Jewish and eight Protestant Dutch leaders in Israel in May 2011, a statement of cooperation was issued, indicating, for the most part, that the Protestant Church recognizes the issues involved with the Palestinian Christians and that this is sometimes at odds with support for the State of Israel, but standing up for the rights of the Palestinians does not detract from the emphasis on the safety of the State of Israel and vice versa.

</doc>
<doc id="8660" url="https://en.wikipedia.org/wiki?curid=8660" title="Christian Church (Disciples of Christ)">
Christian Church (Disciples of Christ)

The Christian Church (Disciples of Christ) is a mainline Protestant Christian denomination in the United States in the Reformed tradition. It is often referred to as The Christian Church, The Disciples of Christ, or as The Disciples. The Christian Church was a charter participant in the formation of the World Council of Churches and of the Federal Council of Churches (now the National Council of Churches), and it continues to be engaged in ecumenical conversations.
The Disciples' local churches are congregationally governed. In 2008 there were 679,563 members in 3,714 congregations in North America. For 2012, their Yearbook & Directory claimed 625,252 members in 3,627 congregations. In 2015, Pew Research estimated .3% of the U.S population, or roughly 936,000 adherents, self-identify with the mainline tradition which is significantly higher than registered membership.
History.
The name, Disciples of Christ, is shared by two groups, The Churches of Christ and the independent Christian churches and churches of Christ. They emerged from the same roots. The Stone-Campbell movement began as two separate threads, each without knowledge of the other, during the Second Great Awakening in the early 19th century. The first of these two groups, led by Barton W. Stone began at Cane Ridge, Bourbon County, Kentucky. The group called themselves simply "Christians". The second, began in western Pennsylvania and Virginia (now West Virginia), led by Thomas Campbell and his son, Alexander Campbell. Because the founders wanted to abandon all denominational labels, they used the biblical names for the followers of Jesus that they found in the Bible.
Stone.
In 1801, the Cane Ridge Revival in Kentucky planted the seed for a movement in Kentucky and the Ohio River Valley to disassociate from denominationalism. In 1803 Stone and others withdrew from the Kentucky Presbytery and formed the Springfield Presbytery. The defining event of the Stone wing of the movement was the publication of the "Last Will and Testament of the Springfield Presbytery", at Cane Ridge, Kentucky, in 1804. "The Last Will" is a brief document in which Stone and five others announced their withdrawal from Presbyterianism and their intention to be solely part of the body of Christ. The writers appealed for the unity of all who follow Jesus, suggested the value of congregational self-governance, and lifted the Bible as the source for understanding the will of God. They denounced the divisive use of the Westminster Confession of Faith.
Soon, they adopted the name "Christian" to identify their group. Thus, the remnants of the Springfield Presbytery became the Christian Church. It is estimated that the Christian Church numbered about 12,000 by 1830.
Campbells.
Independently of Stone, the Campbell wing of the movement was launched when Thomas Campbell published the "Declaration and Address of the Christian Association of Washington," (Pennsylvania) in 1809. The Presbyterian Synod had suspended his ministerial credentials. In "The Declaration and Address" he set forth some of his convictions about the church of Jesus Christ, as he organized the Christian Association of Washington, not as a church but as an association of persons seeking to grow in faith. On May 4, 1811, however, the Christian Association constituted itself as a congregationally governed church. With the building it then constructed at Brush Run, it became known as Brush Run Church.
When their study of the New Testament led the reformers to begin to practice baptism by immersion, the nearby Redstone Baptist Association invited Brush Run Church to join with them for the purpose of fellowship. The reformers agreed provided that they would be "allowed to preach and to teach whatever they learned from the Scriptures."
Thus began a sojourn for the reformers among the Baptists within the Redstone Baptist Association (1815–1824). While the reformers and the Baptists shared the same beliefs in baptism by immersion and congregational polity, it was soon clear that the reformers were not traditional Baptists. Within the Redstone Association, the differences became intolerable to some of the Baptist leaders, when Alexander Campbell began publishing a journal, "The Christian Baptist," promoting reform. Campbell anticipated the conflict and moved his membership to a congregation of the Mahoning Baptist Association in 1824.
In 1827, the Mahoning Association appointed reformer Walter Scott as an Evangelist. Through Scott's efforts, the Mahoning Association grew rapidly. In 1828, Thomas Campbell visited several of the congregations formed by Scott and heard him preach. The elder Campbell realized that Scott was bringing an important new dimension to the movement with his approach to evangelism.
Several Baptist associations began disassociating congregations that refused to subscribe to the Philadelphia Confession. The Mahoning Association came under attack. In 1830, the Mahoning Baptist Association disbanded. Alexander ceased publication of "The Christian Baptist". In January 1831, he began publication of the "Millennial Harbinger".
Merging.
The two groups united at High Street Meeting House, Lexington, Kentucky, with a handshake between Barton W. Stone and "Raccoon" John Smith, on Saturday, December 31, 1831. Smith had been chosen, by those present, to speak on behalf of the followers of the Campbells. While contemporaneous accounts are clear that the handshake took place on Saturday, some historians have changed the date of the merger to Sunday, January 1, 1832. The 1832 date has become generally accepted. The actual difference is about 20 hours.
Two representatives of those assembled were appointed to carry the news of the union to all the churches: John Rogers, for the Christians and "Raccoon" John Smith for the reformers. Despite some challenges, the merger succeeded.
Naming.
With the merger, there was the challenge of what to call the new movement. Clearly, finding a Biblical, non-sectarian name was important. Stone wanted to continue to use the name "Christians." Alexander Campbell insisted upon "Disciples of Christ". Walter Scott and Thomas Campbell sided with Stone, but the younger Campbell had strong reasons and would not yield. As a result, both names were used. The confusion over names has been present ever since. Prior to the 1906 separation, congregations would typically be named "Disciples of Christ," "Christian Church," and "Church of Christ." However, there are different practices by each. More than the name separates each church example:"Independent Christian Church" will not accept a woman as a minister when some of the "Disciples of Christ" congregation will. These different congregations (Disciples of Christ, Church of Christ, and Independent Church) share many of the same beliefs and practices but there are, in fact, differences.
First national convention and missionary movement.
In 1849, the first National Convention was held at Cincinnati, Ohio. Alexander Campbell had concerns that holding conventions would lead the movement into divisive denominationalism. He did not attend the gathering. Among its actions, the convention elected Alexander Campbell its President and created the American Christian Missionary Society (ACMS).
The formation of a missionary society set the stage for further "co-operative" efforts. By the end of the century, the Foreign Christian Missionary Society and the Christian Women's Board of Missions were also engaged in missionary activities. Forming the ACMS did not reflect a consensus of the entire movement. Sponsorship of missionary activities became a divisive issue. In the succeeding decades, for some congregations and their leaders, co-operative work through missionary societies and the adoption of instrumental music in church worship was straying too far from their conception of the early church. After the American Civil War, the schism grew. While there was no disagreement over the need for evangelism, many believed that missionary societies were not authorized by scripture and would compromise the autonomy of local congregations. This became one important factor leading to the separation of the Churches of Christ from the Christian Church (Disciples of Christ).
Journals.
From the beginning of the movement, the free exchange of ideas among the people was fostered by the journals published by its leaders. Alexander Campbell published "The Christian Baptist" and "The Millennial Harbinger". Barton W. Stone published "The Christian Messenger". In a respectful way, both men routinely published the contributions of others whose positions were radically different from their own.
Following Campbell's death in 1866, journals continued to keep the discussion and conversation alive. Between 1870 and 1900, two journals emerged as the most prominent. The "Christian Standard" was edited and published by Isaac Errett of Cincinnati. "The Christian Evangelist" was edited and published by J. H. Garrison from St. Louis. The two men enjoyed a friendly rivalry, and kept the dialog going within the movement. A third journal became part of the conversation with the publication in 1884 of "The Christian Oracle", later to become "The Christian Century", with an interdenominational appeal. In 1914, Garrison's Christian Publishing company was purchased by R. A. Long, who then established a non-profit corporation, "The Christian Board of Publication" as the Brotherhood publishing house.
Division.
In 1906, the U.S. Religious Census listed Churches of Christ for the first time as a group which was separate and distinct from the Disciples of Christ. However, the division had been growing for years, with published reports as early as 1883. The most obvious distinction between the two groups was the Churches of Christ rejecting the use of musical instruments in worship. The controversy over musical instruments began in 1860, when some congregations introduced organs, traditionally associated with wealthier, denominational churches. More basic were the underlying approaches to Biblical interpretation. The Churches of Christ permitted only those practices found in accounts of New Testament worship. They could find no New Testament documentation of the use of instrumental music in worship. The Disciples, by contrast, considered permissible any practices that the New Testament did not expressly forbid.
After the division, Disciples churches used "Christian Church" as the dominant designation for congregations. While music and the approach to missionary work were the most visible issues, there were also some deeper ones. The process that led to the separation had begun prior to the American Civil War.
Following the 1906 separation by the Churches of Christ, additional controversies arose. Should missionary efforts be cooperative or should they be independently sponsored by congregations? Should new methods of Biblical analysis, developed in the late 19th century, be embraced in the study and interpretation of the Bible? The "cooperative" churches were generally more likely to adopt the new biblical study methods.
During the first half of the 20th century, these opposing factions among the Christian Churches coexisted but with growing discomfort and tension. Among the cooperative churches, the three Missionary Societies merged into the United Christian Missionary Society in 1920. Human service ministries grew through the National Benevolent Association and provided assistance to orphans, the elderly and the disabled.
By mid century, the cooperative Christian Churches and the independent Christian Churches were following different paths.
Restructure.
Following World War II, it became obvious that the organizations that had been developed in previous decades no longer effectively met the needs of the postwar era. After a number of discussions throughout the 1950s, the 1960 International Convention of Christian Churches adopted a process to "restructure" the entire organization. The Commission on Restructure, chaired by Granville T. Walker, held its first meeting on October 30 & November 1, 1962. In 1968, the International Convention of Christian Churches (Disciples of Christ) adopted the commission's proposed "Provisional Design of the Christian Church (Disciples of Christ)." Soon the Provisional Design became "The Design."
Under the design, all churches in the 1968 yearbook of Christian Churches (Disciples of Christ) were automatically recognized as part of the Christian Church (Disciples of Christ). In the years that followed, many of the Independent Christian Church Congregations requested formal withdrawal from the yearbook. Many of those congregations became part of the Christian churches and churches of Christ.
The modern disciples have been described as "a Reformed North American Mainstream Moderate Denomination."
Beliefs and practices.
As an integral part of worship in most Christian Church (Disciples of Christ) congregations members celebrate the Lord's Supper. Most congregations also sing hymns, read from the Old and New Testaments of Christian Scripture, hear the word of God proclaimed through sermon or other medium and extend an invitation to become Christ's Disciples. As a congregational church, each congregation determines the nature of its worship, study, Christian service, and witness to the world. Through the observance of communion, individuals are invited to acknowledge their faults and sins, to remember the death and resurrection of Jesus Christ, to remember their baptism, and to give thanks for God's redeeming love. The Christian Church (Disciples of Christ) believes that it is in the local congregations where people come, find, and know God as they gather in Christ's name. Because Disciples believe that the invitation to the table comes from Jesus Christ, communion is open to all who confess that Jesus Christ is Lord, regardless of their denominational affiliation.
For most Disciples, communion is understood as the symbolic presence of Jesus within the gathered community. Most Disciple congregations practice believer's baptism in the form of immersion, believing it to be the form used in the New Testament. The experiences of yielding to Christ in being buried with him in the waters of baptism and rising to a new life, have profound meaning for the church.
"In essentials, Unity; In non-essentials, Liberty; and in all things, Charity."
For modern Disciples the one essential is the acceptance of Jesus Christ as Lord and Savior, and obedience to him in baptism. There is no requirement to give assent to any other statement of belief or creed. Nor is there any "official" interpretation of the Bible. Hierarchical doctrine was traditionally rejected by Disciples as human-made and divisive, and subsequently, freedom of belief and scriptural interpretation allows many Disciples to question or even deny beliefs common in doctrinal churches such as the Incarnation, the Trinity, and the Atonement. Beyond the essential commitment to follow Jesus there is a tremendous freedom of belief and interpretation. As the basic teachings of Jesus are studied and applied to life, there is the freedom to interpret Jesus' teaching in different ways. As would be expected from such an approach, there is a wide diversity among Disciples in what individuals and congregations believe. It is not uncommon to find individuals who seemingly hold diametrically opposed beliefs within the same congregation affirming one another's journeys of faith as sisters and brothers in Christ.
Members and seekers are encouraged to take being disciples seriously, meaning that they are student followers of Jesus. Often the best teaching comes in the form, "I'll tell you what I think, but read the Bible for yourself, and then study and pray about it. Decide in what ways God is calling you to be a follower of Jesus."
Modern Disciples reject the use of creeds as "tests of faith," that is, as required beliefs, necessary to be accepted as a follower of Jesus. Although Disciples respect the great creeds of the church as informative affirmations of faith, they are never seen as binding. Since the adoption of The Design of the Christian Church (Disciples of Christ), in 1968, Disciples have celebrated a sense of unity in reading the preamble to the Design publicly. It is as a meaningful affirmation of faith, not binding upon any member. It was originally intended to remind readers that this Church seeks God through Jesus Christ, even when it adopts a design for its business affairs.
"...the church of Christ upon earth is essentially, intentionally, and constitutionally one;consisting of all those in every place that profess their faith in Christ and obedience to him in all things..."
The Disciples celebrate their oneness with all who seek God through Jesus Christ, throughout time and regardless of location. That oneness is symbolized in the open invitation to communion for all who have professed faith in Christ without regard to church affiliation.
In local communities, congregations share with churches of other denominations in joint worship and in community Christian service. Ecumenical cooperation and collaboration with other Christian Communions has long been practiced, by the Regions.
At the General Church level, the Council on Christian Unity coordinates the ecumenical activities of the church. The Disciples continues to relate to the National Council of Churches, of which it was a founding member. It shares in the dialog and in the theological endeavors of the World Council of Churches. The Disciples has been a full participant in the Consultation on Church Union since it began in the sixties. It continues to support those ongoing conversations which have taken on the title Churches Uniting in Christ. The goal of these endeavors is not the merger into some "Super Church", but rather to discover ways to celebrate and proclaim the unity and oneness that is Christ's gift to his church.
Congregations.
Congregations of the Christian Church are self-governing in the tradition of congregational polity. They select their own leadership, own their own property, and manage their own affairs.
In Disciples congregations, the priesthood of all believers finds its expression in worship and Christian service. Typically, lay persons who have been elected and ordained as Elders preside with called and installed ordained pastors in the celebration of the sacrament of Holy Communion. The Elders and called Pastors provide spiritual oversight and care for members in partnership with one another.
Regional ministries.
The Regional Churches of the Christian Church provide resources for leadership development and opportunities for Christian fellowship beyond the local congregation. They have taken responsibility for the nurture and support of those individuals seeking to discern God's call to service as ordained or licensed ministers. Typically, they organize summer camping experiences for children and youth.
Regional churches assist congregations who are seeking ministers and ministers who are seeking congregations. Regional leadership is available on request to assist congregations that face conflict. Though they have no authority to direct the life of any congregation, the Regional Churches are analogous to the middle judicatories of other denominations.
General Ministries.
The Christian Church (Disciples of Christ) at the "General Church" level consists of a number of self-governing agencies, which focus upon specific Christian witnesses to the world that have emerged in the dialog within the movement since before the first convention in 1849. Typically, these ministries have a scope that is larger than Regional Ministries, and often have a global perspective. The church agencies report to the General Assembly, which meets biennially in odd numbered years. The General Minister and President (GMP) is the designated leader for the General Church, but does not have the administrative authority to direct any of the general church agencies other than "The Office of General Minister and President." The GMP has influence that derives from the respect of the church much as the pastor of a local church leads a local congregation.
The General Ministries are:
One highly popular and respected General Agency program is the "Week of Compassion," named for the special offering to fund the program when it began in the 1950s. The Week of Compassion is the disaster relief and Third World development agency. It works closely with Church World Service and church related organizations in countries around the world where disasters strike, providing emergency aid.
The General Church has challenged the entire denomination to work for a 2020 Vision for the first two decades of the 21st Century. Together the denomination is well on the way to achieving its four foci:
The relationship between the congregations, regions and the general church are detailed in "The Design of the Christian Church (Disciples of Christ)".
At the 2005 General Assembly, over 3000 delegates voted nearly unanimously to elect the Sharon E. Watkins as General Minister and President of the denomination. Watkins was the first woman to be elected as the presiding minister of a mainline Protestant denomination.
Chalice.
The logo of the Christian Church (Disciples of Christ) is a red chalice with a white St. Andrew's Cross. The chalice represents the centrality of Communion to the life of the church. The cross of Saint Andrew is a reminder of the ministry of each person and the importance of evangelism, and recalls the denomination's Scottish Presbyterian ancestry.
After the 1968 General Assembly, the Administrative Committee charged a sub-committee with the task of proposing a symbol for the church. Hundreds of designs were submitted, but none seemed right. By November the Deputy General Minister and President, William Howland, suggested that the committee's staff consultant and chairperson agree on a specific proposal and bring it back to the committee: that meant Robert L. Friedly of the Office of Interpretation and Ronald E. Osborn.
On January 20, 1970, the two men sat down for lunch. With a red felt-tip pen, Osborn began to scrawl a Saint Andrew's cross circumscribed inside a chalice on his placemat.
Immediately, Friedly dispatched the crude drawing to Bruce Tilsley, a commercial artist and member of Central Christian Church of Denver, with the plea that he prepare an artistic version of the ideas. Tilsley responded with two or three sketches, from which was selected the now-familiar red chalice. Use of the proposed symbol became so prevalent that there was little debate when official adoption was considered at the 1971 General Assembly.
The chalice is a registered trademark of the Christian Church (Disciples of Christ). Congregations and ministries of the Christian Church (Disciples of Christ) are free to use the chalice in publications, web sites and other media. Organizations not affiliated with the Christian Church (Disciples of Christ) are asked to obtain permission.
Because most congregations call themselves "Christian Churches," the chalice has become a simple way to identify Disciples of Christ Churches through signage, letterhead, and other forms of publicity.
Membership trends.
The Christian Church (Disciples of Christ) has experienced a significant loss of membership since the middle of the 20th century. Membership peaked in 1958 at just under 2 million. In 1993, membership dropped below 1 million. In 2009, the denomination reported 658,869 members in 3,691 congregations. As of 2010, the five states with the highest adherence rates were Kansas, Missouri, Iowa, Kentucky and Oklahoma. The states with the largest absolute number of adherents were Missouri, Texas, Indiana, Kentucky and Ohio.
Affiliated academic institutions.
From the very beginnings of the movement, Disciples have founded institutions of higher learning. Alexander Campbell taught young leaders and founded Bethany College. The movement established similar schools, especially in the years following the American Civil War.
Because intellectual and religious freedom are important values for the Disciples of Christ, the colleges, universities, and seminaries founded by its congregations do not seek to indoctrinate students or faculty with a sectarian point of view.
In the 21st century, the relationship between the Christian Church (Disciples of Christ) and its affiliated universities is the purview of Higher Education and Leadership Ministries (HELM), an agency of the General Church.
Ecumenical relations.
The Disciples of Christ maintains ecumenical relations with the Pontifical Council for Promoting Christian Unity. It is also affiliated with other ecumenical organizations such as Churches Uniting in Christ, Christian Churches Together, the National Council of Churches and the World Council of Churches. It maintains Ordained Ministerial Partner Standing with the United Church of Christ, which means that clergy ordained in the Disciples of Christ may also serve in the United Church of Christ.

</doc>
<doc id="8662" url="https://en.wikipedia.org/wiki?curid=8662" title="David Rice Atchison">
David Rice Atchison

David Rice Atchison (August 11, 1807January 26, 1886) was a mid-19th century Democratic United States Senator from Missouri. He served as President pro tempore of the United States Senate for six years. He is best known for the claim that for one day (March 4, 1849) he may have been Acting President of the United States. This belief, however, is dismissed by nearly all historians, scholars, and biographers.
Atchison, owner of many slaves and a plantation, was a prominent pro-slavery activist and Border Ruffian leader, deeply involved with violence against abolitionists and other free-staters during the "Bleeding Kansas" events.
Early life.
Atchison was born to William Atchison in Frogtown (later Kirklevington), which is now part of Lexington, Kentucky. He was educated at Transylvania University in Lexington, where his classmates included five future Democratic senators (Solomon Downs of Louisiana, Jesse Bright of Indiana, George W. Jones of Iowa, Edward Hannegan of Indiana, and Jefferson Davis of Mississippi). Atchison was admitted to the Kentucky bar in 1829.
Missouri lawyer and politician.
In 1830 he moved to Liberty in Clay County in western Missouri, and set up practice there, where he also farmed. Atchison's law practice flourished, and his best-known client was Latter Day Saint Movement founder Joseph Smith. Atchison represented Smith in land disputes with non-Mormon settlers in Caldwell County and Daviess County.
Alexander William Doniphan joined Atchison's law practice in Liberty in May 1833. The two became fast friends and spent many leisure time hours playing cards, going to the horse races, hunting, fishing, attending social functions and political events. Atchison, already a member of the Liberty Blues, a volunteer militia in Missouri, got Doniphan to join.
Atchison was elected to the Missouri House of Representatives in 1834. He worked hard for the Platte Purchase, which extended the northwestern boundary of Missouri to the Missouri River in 1837.
When the earlier disputes broke out into the so-called Mormon War of 1838, Atchison was appointed a major general in the state militia and took part in suppression of the violence by both sides.
In 1838 he was re-elected to the Missouri State House of Representatives. Three years later, he appointed a circuit court judge for the six-county area of the Platte Purchase. In 1843 he was named a county commissioner in Platte County, where he then lived.
Senate career.
In October 1843, Atchison was appointed to the U.S. Senate to fill the vacancy left by the death of Lewis F. Linn. He thus became the first senator from western Missouri. At age 36, he was the youngest senator from Missouri up to that time. Later in 1843, Atchison was appointed to serve the remainder of Linn's term and was re-elected in 1849.
Atchison was very popular with his fellow Senate Democrats. When the Democrats took control of the Senate in December 1845, they chose Atchison as President pro tempore, placing him third in succession for the Presidency, and also giving him the duty of presiding over the Senate when the Vice President was absent. He was then only 38 years old and had served in the Senate just two years. In 1849 Atchison stepped down as President pro tempore in favor of William R. King. King in turn yielded the office back to Atchison in December 1852, since King had been elected Vice President of the United States. Atchison continued as President pro tempore until December 1854.
As a Senator, Atchison was a fervent advocate of slavery and territorial expansion. He supported the annexation of Texas and the U.S.-Mexican War. Atchison and Missouri's other Senator, the venerable Thomas Hart Benton, became rivals and finally enemies, though both were Democrats. Benton declared himself to be against slavery in 1849, and in 1851 Atchison allied with the Whigs to defeat Benton for re-election.
Benton, intending to challenge Atchison in 1854, began to agitate for territorial organization of the area west of Missouri (now the states of Kansas and Nebraska) so it could be opened to settlement. To counter this, Atchison proposed that the area be organized "and" that the section of the Missouri Compromise banning slavery there be repealed in favor of popular sovereignty, under which the settlers in each territory would decide themselves whether slavery would be allowed.
At Atchison's request, Senator Stephen Douglas of Illinois introduced the Kansas-Nebraska Act, which embodied this idea, in November 1853. The Act became law in May 1854, establishing the Territories of Kansas and Nebraska.
Border Ruffians.
Douglas (and Atchison) had assumed that Nebraska would be settled by Free-State men from Iowa and Illinois, and Kansas by pro-slavery Missourians and other Southerners, thus preserving the numerical balance between free states and slave states. In 1854 Atchison helped found the town of Atchison, Kansas, as a pro-slavery settlement. The town (and county) were named for him.
In fact, while Southerners welcomed the opportunity to settle Kansas, very few actually chose to do so. Instead, most free-soilers preferred Kansas. Furthermore, anti-slavery activists throughout the North came to view Kansas as a battleground and formed societies to encourage free-soil settlers to go to Kansas and ensure that both Kansas and Nebraska would become free states.
It appeared as if the Kansas Territorial legislature to be elected in March 1855 would be controlled by free-soilers and ban slavery. This was viewed as a breach of faith by Atchison and his supporters. An angry Atchison called on pro-slavery Missourians to uphold slavery by force and "to kill every God-damned abolitionist in the district" if necessary. He recruited an immense mob of heavily armed Missourians, the infamous "Border Ruffians". On the election day, March 30, 1855, Atchison led 5,000 Border Ruffians into Kansas. They seized control of all polling places at gunpoint, cast tens of thousands of fraudulent votes for pro-slavery candidates, and elected a pro-slavery legislature.
The outrage was nonetheless accepted by the Federal government. When Territorial Governor Andrew Reeder objected, he was fired by President Pierce.
Despite this show of force, far more free-soilers than pro-slavery settlers migrated to Kansas. There were continual raids and ambushes by both sides in "Bleeding Kansas". But in spite of the best efforts of Atchison and the Ruffians, Kansas did reject slavery and finally became a free state in 1861.
Defeated for re-election.
Atchison's Senate term expired March 4, 1855. He sought election to another term, but the Democrats in the Missouri legislature were split between him and Benton, while the Whig minority put forward their own man. No Senator was elected until January 1857, when James S. Green was chosen.
Railroad proposal.
When the First Transcontinental Railroad was proposed in the 1850s, Atchison called for it to be built along the central route (from St. Louis through Missouri, Kansas, and Utah), rather than the southern route (from New Orleans through Texas and New Mexico). Naturally, his suggested route went through Atchison.
Civil War General.
Atchison and A. W. Doniphan would fall out over the politics preceding the Civil War and on which direction Missouri should proceed. Atchison favored secession, while Doniphan was torn and would remain for the most part non-committal. Privately Doniphan favored the Union, but found it hard to go against his friends and associates.
During the secession crisis in Missouri at the beginning of the American Civil War, Atchison sided with Missouri's pro-Confederate governor, Claiborne Jackson. He accepted an appointment as a general in the Missouri State Guard. Atchison actively recruited State Guardsmen in northern Missouri and served with Missouri State Guard commander General Sterling Price in the summer campaign of 1861. In September 1861, Atchison led 3,500 State Guard recruits across the Missouri River to reinforce Price, and defeated Union troops that tried to block his force in the Battle of Liberty.
Atchison continued to serve through the end of 1861. In March 1862, Union forces in the Trans-Mississippi theater won a decisive victory at Pea Ridge in Arkansas and secured Union control of Missouri. Atchison then resigned from the army over reported strategy arguments with General Price and moved to Texas for the duration of the Civil War. After the war he retired to his farm near Gower, and was noted to deny many of his pro-slavery public statements made prior to the Civil War. In addition, his retirement cottage outside of Plattsburg, burned to the ground before his death in 1886. This included the complete loss of his library containing books, documents, and letters which documented his role in the Mormon War, Indian affairs, pro-slavery activities, Civil War activities, and other legislation covering his career as a lawyer, senator, and soldier.
"President for One Day".
Atchison himself never claimed that he was technically President of the United States for one day—Sunday, March 4, 1849. Outgoing President James K. Polk's term ended at noon on March 4, which was a Sunday. His successor, Zachary Taylor, refused to be sworn into office on Sunday. As President "pro tempore", and therefore Acting Vice President, under the presidential succession law in place at the time, Atchison was believed by some to be Acting President.
In an interview with the "St. Louis Globe-Democrat", Atchison revealed that he slept through most of the day of his alleged presidency: "There had been three or four busy nights finishing up the work of the Senate, and I slept most of that Sunday."
Despite this, a museum exhibit opened in his honor, in which its owner claims it to be the country's smallest Presidential Library. Although it is not recognized as such by the U.S Government, it opened in February 2006 as the Atchison County Historical Museum in Atchison, Kansas.
Historians, constitutional scholars and biographers all dismiss the claim. They point out that Atchison's Senate term had ended on March 4 as well, and he also was not sworn in for another term, or re-elected President "pro tempore", until March 5. Furthermore, the Constitution doesn't require the President-elect to take the oath of office to hold the office, just to execute the powers. As Atchison never swore the oath either, that did not make him Acting President. Most historians and scholars assert that as soon as the outgoing President's term expires, the President-elect automatically assumes the office. Some claim instead that the office is vacant until the taking of the oath. 
Atchison discussed the claim in a September 1872 issue of the "Plattsburg Lever":
Atchison was 41 years and 6 months old at the alleged time of the One-Day Presidency, younger than any official President. Theodore Roosevelt, the youngest to serve, was 42 years and 11 months old when he was sworn in following the death of William McKinley in 1901, and John F. Kennedy, the youngest to be elected, was 43 years and 7 months old when he was inaugurated in 1961.
Memorials.
Atchison died on January 26, 1886, at the age of 78. He was buried at Greenlawn Cemetery in Plattsburg, Missouri, where a statue honors him in front of the Clinton County Courthouse. His grave marker reads "President of the United States for One Day."
Both Atchison and Atchison County, Kansas, are named for him. The town subsequently gave its name to the Atchison, Topeka, and Santa Fe Railroad. Atchison County, Missouri, is also named for him.
In 1991, Atchison was inducted into the Hall of Famous Missourians, and a bronze bust depicting him is on permanent display in the rotunda of the Missouri State Capitol.
There is also a memorial for Atchison located in the Landsdowne area of Lexington, Kentucky.
See also.
Places named for David Atchison
External links.
 Retrieved on 2008-02-13
 

</doc>
<doc id="8663" url="https://en.wikipedia.org/wiki?curid=8663" title="Daniel Gabriel Fahrenheit">
Daniel Gabriel Fahrenheit

Daniel Gabriel Fahrenheit (; ; 24 May 1686 – 16 September 1736) was a German physicist, engineer, and glass blower who is best known for inventing the mercury-in-glass thermometer (1714), and for developing a temperature scale now named after him.
Biography.
Fahrenheit was born in Danzig (Gdańsk), in the Polish-Lithuanian Commonwealth, but lived most of his life in the Dutch Republic. The Fahrenheits were a German Hanse merchant family who had lived in several Hanseatic cities. Fahrenheit's great-grandfather had lived in Rostock, and research suggests that the Fahrenheit family originated in Hildesheim. Daniel's grandfather moved from Kneiphof in Königsberg to Danzig and settled there as a merchant in 1650. His son, Daniel Fahrenheit (the father of the subject of this article), married Concordia Schumann, daughter of a well-known Danzig business family. Daniel was the eldest of the five Fahrenheit children (two sons, three daughters) who survived childhood. His sister, Virginia Elizabeth Fahrenheit, married Benjamin Ephraim Krueger of an aristocratic family from Danzig.
Daniel Gabriel began training as a merchant in Amsterdam after his parents died on 14 August 1701 from eating poisonous mushrooms. However, Fahrenheit's interest in natural science led him to begin studies and experimentation in that field. From 1717, he traveled to Berlin, Halle, Leipzig, Dresden, Copenhagen, and also to his hometown, where his brother still lived. During that time, Fahrenheit met or was in contact with Ole Rømer, Christian Wolff, and Gottfried Leibniz. In 1717, Fahrenheit settled in The Hague as a glassblower, making barometers, altimeters, and thermometers. From 1718 onwards, he lectured in chemistry in Amsterdam. He visited England in 1724 and was the same year elected a Fellow of the Royal Society. Fahrenheit died in The Hague and was buried there at the Kloosterkerk (Cloister Church).
In 2012, scientists made a computer image of his face using photos of his relatives.
Fahrenheit scale.
According to Fahrenheit's 1724 article, he determined his scale by reference to three fixed points of temperature. The lowest temperature was achieved by preparing a frigorific mixture of ice, water, and ammonium chloride (a salt), and waiting for it to reach equilibrium. The thermometer then was placed into the mixture and the liquid in the thermometer allowed to descend to its lowest point. The thermometer's reading there was taken as 0 °F. The second reference point was selected as the reading of the thermometer when it was placed in still water when ice was just forming on the surface. This was assigned as 32 °F. The third calibration point, taken as 96 °F, was selected as the thermometer's reading when the instrument was placed under the arm or in the mouth.
Fahrenheit came up with the idea that mercury boils around 300 degrees on this temperature scale. Work by others showed that water boils about 180 degrees above its freezing point. The Fahrenheit scale later was redefined to make the freezing-to-boiling interval exactly 180 degrees, a convenient value as 180 is a highly composite number, meaning that it is evenly divisible into many fractions. It is because of the scale's redefinition that normal body temperature today is taken as 98.2 degrees, whereas it was 96 degrees on Fahrenheit's original scale.
The Fahrenheit scale was the primary temperature standard for climatic, industrial and medical purposes in English-speaking countries until the 1960s.

</doc>
<doc id="8664" url="https://en.wikipedia.org/wiki?curid=8664" title="Freescale DragonBall">
Freescale DragonBall

Motorola/Freescale Semiconductor's DragonBall, or MC68328, is a microcontroller design based on the famous 68000 core, but implemented as an all-in-one low-power system for handheld computer use. It is supported by μClinux. It was designed by Motorola in Hong Kong and released in 1995.
The DragonBall's major design win was in earlier versions of the Palm Computing platform; however, from Palm OS 5 onwards it has been superseded by ARM-based processors from Texas Instruments and Intel. The processor is also used in some of the AlphaSmart line of portable word processors. Examples include the Dana and Dana Wireless.
The processor is capable of speeds of up to 16.58 MHz and can run up to 2.7 MIPS (million instructions per second), for the base 68328 and DragonBall EZ (MC68EZ328) model. It was extended to 33 MHz, 5.4 MIPS for the DragonBall VZ (MC68VZ328) model, and 66 MHz, 10.8 MIPS for the DragonBall Super VZ (MC68SZ328).
It is a 32-bit processor with 32-bit internal and external address bus (24-bit external address bus for EZ and VZ variants) and 32-bit data bus. It has many built-in functions, like a color and grayscale display controller, PC speaker sound, serial port with UART and IRDA support, UART bootstrap, real time clock, is able to directly access DRAM, Flash ROM, and mask ROM, and has built-in support for touch screens.
It is an all-in-one computer on a chip; before the DragonBall EZ, Palm handhelds had twice as many ICs.
The more recent DragonBall MX series microcontrollers, later renamed the Freescale i.MX (MC9328MX/MCIMX) series, are intended for similar application to the earlier DragonBall devices but are based on an ARM9 or ARM11 processor core instead of a 68000 core.

</doc>
<doc id="8667" url="https://en.wikipedia.org/wiki?curid=8667" title="Double-slit experiment">
Double-slit experiment

The modern double-slit experiment is a demonstration that light and matter can display characteristics of both classically defined waves and particles; moreover, it displays the fundamentally probabilistic nature of quantum mechanical phenomena. A simpler form of the double-slit experiment was performed originally by Thomas Young in 1801 (well before quantum mechanics). He believed it demonstrated that the wave theory of light was correct and his experiment is sometimes referred to as Young's experiment or "Young's slits". The experiment belongs to a general class of "double path" experiments, in which a wave is split into two separate waves that later combine into a single wave. Changes in the path lengths of both waves result in a phase shift, creating an interference pattern. Another version is the Mach–Zehnder interferometer, which splits the beam with a mirror.In the basic version of this experiment, a coherent light source such as a laser beam illuminates a plate pierced by two parallel slits, and the light passing through the slits is observed on a screen behind the plate. The wave nature of light causes the light waves passing through the two slits to interfere, producing bright and dark bands on the screen—a result that would not be expected if light consisted of classical particles. However, the light is always found to be absorbed at the screen at discrete points, as individual particles (not waves), the interference pattern appearing via the varying density of these particle hits on the screen. Furthermore, versions of the experiment that include detectors at the slits find that each detected photon passes through one slit (as would a classical particle), and not through both slits (as would a wave). These results demonstrate the principle of wave–particle duality.
Other atomic-scale entities such as electrons are found to exhibit the same behavior when fired towards a double slit. Additionally, the detection of individual discrete impacts is observed to be inherently probabilistic, which is inexplicable using classical mechanics.
The experiment can be done with entities much larger than electrons and photons, although it becomes more difficult as size increases. The largest entities for which the double-slit experiment has been performed were molecules that each comprised 810 atoms (whose total mass was over 10,000 atomic mass units).
Overview.
If light consisted strictly of ordinary or classical particles, and these particles were fired in a straight line through a slit and allowed to strike a screen on the other side, we would expect to see a pattern corresponding to the size and shape of the slit. However, when this "single-slit experiment" is actually performed, the pattern on the screen is a diffraction pattern in which the light is spread out. The smaller the slit, the greater the angle of spread. The top portion of the image shows the central portion of the pattern formed when a red laser illuminates a slit and, if one looks carefully, two faint side bands. More bands can be seen with a more highly refined apparatus. Diffraction explains the pattern as being the result of the interference of light waves from the slit.
If one illuminates two parallel slits with a more intense red laser, the light from the two slits again interferes. Here the interference is a more pronounced pattern with a series of light and dark bands. The width of the bands is a property of the frequency of the illuminating light. (See the bottom photograph to the right.) When Thomas Young (1773–1829) first demonstrated this phenomenon, it indicated that light consists of waves, as the distribution of brightness can be explained by the alternately additive and subtractive interference of wavefronts. Young's experiment, performed in the early 1800s, played a vital part in the acceptance of the wave theory of light, vanquishing the corpuscular theory of light proposed by Isaac Newton, which had been the accepted model of light propagation in the 17th and 18th centuries. However, the later discovery of the photoelectric effect demonstrated that under different circumstances, light can behave as if it is composed of discrete particles. These seemingly contradictory discoveries made it necessary to go beyond classical physics and take the quantum nature of light into account.
The double-slit experiment (and its variations) has become a classic thought experiment, for its clarity in expressing the central puzzles of quantum mechanics. Because it demonstrates the fundamental limitation of the ability of the observer to predict experimental results, Richard Feynman called it "a phenomenon which is impossible to explain in any classical way, and which has in it the heart of quantum mechanics. In reality, it contains the "only" mystery f quantum mechanic." Feynman was fond of saying that all of quantum mechanics can be gleaned from carefully thinking through the implications of this single experiment. Richard Feynman also proposed (as a thought experiment) that if detectors were placed before each slit, the interference pattern would disappear.
The Englert–Greenberger duality relation provides a detailed treatment of the mathematics of double-slit interference in the context of quantum mechanics.
A low-intensity double-slit experiment was first performed by G. I. Taylor in 1909, by reducing the level of incident light until photon emission/absorption events were mostly nonoverlapping.
A double-slit experiment was not performed with anything other than light until 1961, when Claus Jönsson of the University of Tübingen performed it with electrons. In 1974 the Italian physicists Pier Giorgio Merli, Gian Franco Missiroli, and Giulio Pozzi repeated the experiment using single electrons, showing that each electron interferes with itself as predicted by quantum theory. In 2002, the single-electron version of the experiment was voted "the most beautiful experiment" by readers of "Physics World."
Variations of the experiment.
Interference of individual particles.
An important version of this experiment involves single particles (or waves—for consistency, they are called particles here). Sending particles through a double-slit apparatus one at a time results in single particles appearing on the screen, as expected. Remarkably, however, an interference pattern emerges when these particles are allowed to build up one by one (see the image to the right). This demonstrates the wave-particle duality, which states that all matter exhibits both wave and particle properties: the particle is measured as a single pulse at a single position, while the wave describes the probability of absorbing the particle at a specific place of the detector. This phenomenon has been shown to occur with photons, electrons, atoms and even some molecules, including buckyballs. So experiments with electrons add confirmatory evidence to the view that electrons, protons, neutrons, and even larger entities that are ordinarily called particles nevertheless have their own wave nature and even their own specific frequencies.
The probability of detection is the square of the amplitude of the wave and can be calculated with classical waves (see below). The particles do not arrive at the screen in a predictable order, so knowing where all the previous particles appeared on the screen and in what order tells nothing about where a future particle will be detected. If there is a cancellation of waves at some point, that does not mean that a particle disappears; it will appear somewhere else. Ever since the origination of quantum mechanics, some theorists have searched for ways to incorporate additional determinants or "hidden variables" that, were they to become known, would account for the location of each individual impact with the target.
More complicated systems that involve two or more particles in superposition are not amenable to the above explanation.
"Which-way" experiments and the principle of complementarity.
A well-known thought experiment predicts that if particle detectors are positioned at the slits, showing through which slit a photon goes, the interference pattern will disappear. This "which-way" experiment illustrates the complementarity principle that photons can behave as either particles or waves, but cannot be observed as both at the same time.
Despite the importance of this "gedanken" in the history of quantum mechanics (for example, see the discussion on ), technically feasible realizations of this experiment were not proposed until the 1970s. (Naive implementations of the textbook "gedanken" are not possible because photons cannot be detected without absorbing the photon.) Currently, multiple experiments have been performed illustrating various aspects of complementarity.
An experiment performed in 1987 produced results that demonstrated that information could be obtained regarding which path a particle had taken without destroying the interference altogether. This showed the effect of measurements that disturbed the particles in transit to a lesser degree and thereby influenced the interference pattern only to a comparable extent. In other words, if one does not insist that the method used to determine which slit each photon passes through be completely reliable, one can still detect a (degraded) interference pattern.
Delayed choice and quantum eraser variations.
Wheeler's delayed choice experiments demonstrate that extracting "which path" information "after" a particle passes through the slits can seem to retroactively alter its previous behavior at the slits.
Quantum eraser experiments demonstrate that wave behavior can be restored by erasing or otherwise making permanently unavailable the "which path" information.
A simple do-it-at-home demonstration of the quantum eraser phenomenon was given in an article in "Scientific American". If one sets polarizers before each slit with their axes orthogonal to each other, the interference pattern will be eliminated. The polarizers can be considered as introducing which-path information to each beam. Introducing a third polarizer in front of the detector with an axis of 45° relative to the other polarizers "erases" this information, allowing the interference pattern to reappear. This can also be accounted for by considering the light to be a classical wave, and also when using circular polarizers and single photons. Implementations of the polarizers using entangled photon pairs have no classical explanation.
Weak measurement.
In a highly publicized experiment in 2012, researchers claimed to have identified the path each particle had taken without any adverse effects at all on the interference pattern generated by the particles. In order to do this, they used a setup such that particles coming to the screen were not from a point-like source, but from a source with two intensity maxima. However, commentators such as Motl and Svensson have pointed out that there is in fact no conflict between the weak measurements performed in this variant of the double-slit experiment and the Heisenberg uncertainty principle. Weak measurement followed by post-selection did not allow simultaneous position and momentum measurements for each individual particle, but rather allowed measurement of the average trajectory of the particles that arrived at different positions. In other words, the experimenters were creating a statistical map of the full trajectory landscape.
Other variations.
In 1967, Pfleegor and Mandel demonstrated two-source interference using two separate lasers as light sources.
It was shown experimentally in 1972 that in a double-slit system where only one slit was open at any time, interference was nonetheless observed provided the path difference was such that the detected photon could have come from either slit. The experimental conditions were such that the photon density in the system was much less than unity.
In 1999, the double-slit experiment was successfully performed with buckyball molecules (each of which comprises 60 carbon atoms). A buckyball is large enough (diameter about 0.7 nm, nearly half a million times larger than a proton) to be seen under an electron microscope.
In 2005, E. R. Eliel presented an experimental and theoretical study of the optical transmission of a thin metal screen perforated by two subwavelength slits, separated by many optical wavelengths. The total intensity of the far-field double-slit pattern is shown to be reduced or enhanced as a function of the wavelength of the incident light beam.
In 2012, researchers at the University of Nebraska–Lincoln performed the double-slit experiment with electrons as described by Richard Feynman, using new instruments that allowed control of the transmission of the two slits and the monitoring of single-electron detection events. Electrons were fired by an electron gun and passed through one or two slits of 62 nm wide × 4 μm tall.
In 2013, the double-slit experiment was successfully performed with molecules that each comprised 810 atoms (whose total mass was over 10,000 atomic mass units).
Hydrodynamic pilot wave analogs.
Hydrodynamic analogs have been developed that can recreate various aspects of quantum mechanical systems, including single-particle interference through a double-slit. A silicone oil droplet, bouncing along the surface of a liquid, self-propels via resonant interactions with its own wave field. The droplet gently sloshes the liquid with every bounce. At the same time, ripples from past bounces affect its course. The droplet’s interaction with its own ripples, which form what is known as a pilot wave, causes it to exhibit behaviors previously thought to be peculiar to elementary particles — including behaviors customarily taken as evidence that elementary particles are spread through space like waves, without any specific location, until they are measured.
Behaviors mimicked via this hydrodynamic pilot-wave system include quantum single particle diffraction, tunneling, quantized orbits, orbital level splitting, spin, and multimodal statistics. It is also possible to infer uncertainty relations and exclusion principles. On the other hand, no hydrodynamic analog of entanglement has yet been developed. Videos are available illustrating various features of this system. (See the External links.)
Classical wave-optics formulation.
Much of the behaviour of light can be modelled using classical wave theory. The Huygens–Fresnel principle is one such model; it states that each point on a wavefront generates a secondary wavelet, and that the disturbance at any subsequent point can be found by summing the contributions of the individual wavelets at that point. This summation needs to take into account the phase as well as the amplitude of the individual wavelets. It should be noted that only the intensity of a light field can be measured—this is proportional to the square of the amplitude.
In the double-slit experiment, the two slits are illuminated by a single laser beam. If the width of the slits is small enough (less than the wavelength of the laser light), the slits diffract the light into cylindrical waves. These two cylindrical wavefronts are superimposed, and the amplitude, and therefore the intensity, at any point in the combined wavefronts depends on both the magnitude and the phase of the two wavefronts. The difference in phase between the two waves is determined by the difference in the distance travelled by the two waves.
If the viewing distance is large compared with the separation of the slits (the far field), the phase difference can be found using the geometry shown in the figure below right. The path difference between two waves travelling at an angle is given by:
Where d is the distance between the two slits. When the two waves are in phase, i.e. the path difference is equal to an integral number of wavelengths, the summed amplitude, and therefore the summed intensity is maximum, and when they are in anti-phase, i.e. the path difference is equal to half a wavelength, one and a half wavelengths, etc., then the two waves cancel and the summed intensity is zero. This effect is known as interference. The interference fringe maxima occur at angles
where λ is the wavelength of the light. The angular spacing of the fringes, , is given by
The spacing of the fringes at a distance from the slits is given by
For example, if two slits are separated by 0.5 mm (), and are illuminated with a 0.6μm wavelength laser (), then at a distance of 1m (), the spacing of the fringes will be 1.2 mm.
If the width of the slits is greater than the wavelength, the Fraunhofer diffraction equation gives the intensity of the diffracted light as:
Where the sinc function is defined as sinc("x") = sin("x")/("x") for "x" ≠ 0, and sinc(0) = 1.
This is illustrated in the figure above, where the first pattern is the diffraction pattern of a single slit, given by the function in this equation, and the second figure shows the combined intensity of the light diffracted from the two slits, where the function represent the fine structure, and the coarser structure represents diffraction by the individual slits as described by the function.
Similar calculations for the near field can be done using the Fresnel diffraction equation. As the plane of observation gets closer to the plane in which the slits are located, the diffraction patterns associated with each slit decrease in size, so that the area in which interference occurs is reduced, and may vanish altogether when there is no overlap in the two diffracted patterns.
Interpretations of the experiment.
Like the Schrödinger's cat thought experiment, the double-slit experiment is often used to highlight the differences and similarities between the various interpretations of quantum mechanics.
Copenhagen interpretation.
The Copenhagen interpretation, put forth by some of the pioneers in the field of quantum mechanics, asserts that it is undesirable to posit anything that goes beyond the mathematical formulae and the kinds of physical apparatus and reactions that enable us to gain some knowledge of what goes on at the atomic scale. One of the mathematical constructs that enables experimenters to predict very accurately certain experimental results is sometimes called a probability wave. In its mathematical form it is analogous to the description of a physical wave, but its "crests" and "troughs" indicate levels of probability for the occurrence of certain phenomena (e.g., a spark of light at a certain point on a detector screen) that can be observed in the macro world of ordinary human experience.
The probability "wave" can be said to "pass through space" because the probability values that one can compute from its mathematical representation are dependent on time. One cannot speak of the location of any particle such as a photon between the time it is emitted and the time it is detected simply because in order to say that something is located somewhere at a certain time one has to detect it. The requirement for the eventual appearance of an interference pattern is that particles be emitted, and that there be a screen with at least two distinct paths for the particle to take from the emitter to the detection screen. Experiments observe nothing whatsoever between the time of emission of the particle and its arrival at the detection screen. If a ray tracing is next made as if a light wave (as understood in classical physics) is wide enough to take both paths, then that ray tracing will accurately predict the appearance of maxima and minima on the detector screen when many particles pass through the apparatus and gradually "paint" the expected interference pattern.
Path-integral formulation.
The Copenhagen interpretation is similar to the path integral formulation of quantum mechanics provided by Feynman. The path integral formulation replaces the classical notion of a single, unique trajectory for a system, with a sum over all possible trajectories. The trajectories are added together by using functional integration.
Each path is considered equally likely, and thus contributes the same amount. However, the phase of this contribution at any given point along the path is determined by the action along the path:
All these contributions are then added together, and the magnitude of the final result is squared, to get the probability distribution for the position of a particle:
As is always the case when calculating probability, the results must then be normalized by imposing:
To summarize, the probability distribution of the outcome is the normalized square of the norm of the superposition, over all paths from the point of origin to the final point, of waves propagating proportionally to the action along each path. The differences in the cumulative action along the different paths (and thus the relative phases of the contributions) produces the interference pattern observed by the double-slit experiment. Feynman stressed that his formulation is merely a mathematical description, not an attempt to describe a real process that we can measure.
Relational interpretation.
According to the relational interpretation of quantum mechanics, first proposed by Carlo Rovelli, observations such as those in the double-slit experiment result specifically from the interaction between the observer (measuring device) and the object being observed (physically interacted with), not any absolute property possessed by the object. In the case of an electron, if it is initially "observed" at a particular slit, then the observer–particle (photon–electron) interaction includes information about the electron's position. This partially constrains the particle's eventual location at the screen. If it is "observed" (measured with a photon) not at a particular slit but rather at the screen, then there is no "which path" information as part of the interaction, so the electron's "observed" position on the screen is determined strictly by its probability function. This makes the resulting pattern on the screen the same as if each individual electron had passed through both slits. It has also been suggested that space and distance themselves are relational, and that an electron can appear to be in "two places at once"—for example, at both slits—because its spatial relations to particular points on the screen remain identical from both slit locations.
Many-worlds interpretation.
Physicist David Deutsch argues in his book "The Fabric of Reality" that the double-slit experiment is evidence for the many-worlds interpretation.

</doc>
<doc id="8668" url="https://en.wikipedia.org/wiki?curid=8668" title="Dan Bricklin">
Dan Bricklin

Daniel Singer "Dan" Bricklin (born 16 July 1951), often referred to as “The Father of the Spreadsheet”, is the American co-creator, with Bob Frankston, of the VisiCalc spreadsheet program. He also founded Software Garden, Inc., of which he is currently president, and Trellix Corporation, which is currently owned by Web.com. He currently serves as the Chief Technology Officer of Alpha Software.
His book, "Bricklin on Technology", was published by Wiley in May 2009.
Early life.
Bricklin was born in a Jewish family in Philadelphia, where he attended Akiba Hebrew Academy during his high school years. He earned a Bachelor of Science in electrical engineering and computer science from the Massachusetts Institute of Technology in 1973, where he was a resident of Bexley Hall. He began his college career as a mathematics major, but soon switched to computer science.
Upon graduating from MIT, Bricklin worked for Digital Equipment Corporation (DEC) until 1976, when he began working for FasFax, a cash register manufacturer. In 1977, he decided to return to school, and he earned a Master of Business Administration from Harvard University in 1979.
While a student at Harvard Business School, Bricklin co-developed VisiCalc in 1979, making it the first electronic spreadsheet readily available for home and office use. It ran on an Apple II computer, and was considered a fourth generation software program. VisiCalc is widely credited for fueling the rapid growth of the personal computer industry. Instead of doing financial projections with manually calculated spreadsheets, and having to recalculate with every single cell in the sheet, VisiCalc allowed the user to change any cell, and have the entire sheet automatically recalculated. This turned 20 hours of work into 15 minutes and allowed for more creativity.
Professional career.
Software Arts.
In 1979, Bricklin and Frankston founded Software Arts, Inc., and began selling VisiCalc. Along with co-founder Bob Frankston, he started writing versions of the program for the Tandy TRS-80, Commodore PET and the Atari 800. Soon after its launch, VisiCalc became a fast seller at $100.
Bricklin was awarded the [[Grace Murray Hopper Award]] in 1981 for VisiCalc. Bricklin never received a patent for VisiCalc, since software programs weren't made eligible for patents by the Supreme Court until after 1981.
Bricklin was chairman of Software Arts until 1985, when he left to found Software Garden.
Software Garden.
Dan Bricklin founded Software Garden, a small consulting firm and developer of software applications, in 1985. The company's focus was to produce and market “Dan Bricklin's Demo Program”. The program allowed users to create demonstrations of their programs before they were even written, and was also used to create tutorials for Windows-based programs. Other versions released soon after included demo-it!. He remained the president of the company until he co-founded Slate Corporation in 1990. In 1992 he became the vice president of Slate corporation. When Slate closed in 1994, Bricklin returned to Software Garden.
Trellix Corporation.
In 1995 Bricklin founded Trellix Corporation. Trellix was bought by Interland (now [[Web.com]]) in 2003, and Bricklin became Interland's [[chief technology officer]] until early 2004.
He introduced the term "[[friend-to-friend]] networking" on August 11, 2000.
He also introduced the term [[inverse commons|cornucopia of the commons]] about the same time.
Current work.
Bricklin continues to serve as president of Software Garden, a small company which develops and markets software tools he creates, as well as providing speaking and consulting services.
He has released Note Taker HD, an app that integrates hand written notes on the [[iPad]].
He is also developing [[wikiCalc]], a collaborative, basic spreadsheet running on the Web.
He is currently the Chief Technology Officer of [[Alpha Software]] in Burlington, MA, a company that creates tools to easily develop cross-platform mobile business apps.
Affiliations.
In 1994, Bricklin was inducted as a Fellow of the [[Association for Computing Machinery]]. He is a founding trustee of the Massachusetts Technology Leadership Council and has served on the boards of the Software Publishers Association and the [[Boston Computer Society]]. He was also elected to be a member of the National Academy of Engineering.
Awards and accomplishments.
In 1981, Dan Bricklin was given a [[Grace Murray Hopper Award]] for VisiCalc.
In 1996, Bricklin was awarded by the IEEE Computer Society with the Computer Entrepreneur Award for pioneering the development and commercialization of the spreadsheet and the profound changes it fostered in business and industry.
In 2003, Bricklin was given the [[Wharton Infosys Business Transformation Award]] for being a technology change leader. He was recognized for having used information technology in an industry-transforming way. He has received an Honorary Doctor of Humane Letters from Newbury College.
In 2004, he was made a Fellow of the [[Computer History Museum]] "for advancing the utility of personal computers by developing the VisiCalc electronic spreadsheet."
Bricklin appeared in the 1996 documentary [[Triumph of the Nerds]], as well as the 2005 documentary [[Aardvark'd: 12 Weeks with Geeks]], in both cases discussing the development of VisiCalc.
[[Category:1951 births]]
External links.
[[Category:Living people]]
[[Category:American Jews]]
[[Category:American electrical engineers]]
[[Category:American computer businesspeople]]
[[Category:Computer programmers]]
[[Category:People from Philadelphia, Pennsylvania]]
[[Category:Grace Murray Hopper Award laureates]]
[[Category:Fellows of the Association for Computing Machinery]]
[[Category:Harvard Business School alumni]]
[[Category:Massachusetts Institute of Technology alumni]]
[[Category:Akiba Hebrew Academy (Merion, Pennsylvania) alumni]]
[[Category:Members of the United States National Academy of Engineering]]
[[Category:Chief technology officers]]

</doc>
<doc id="8674" url="https://en.wikipedia.org/wiki?curid=8674" title="Digital Enhanced Cordless Telecommunications">
Digital Enhanced Cordless Telecommunications

Digital Enhanced Cordless Telecommunications (Digital European Cordless Telecommunications), usually known by the acronym DECT, is a standard primarily used for creating cordless telephone systems. It originated in Europe, where it is the universal standard, replacing earlier cordless phone standards, such as 900 MHz CT1 and CT2.
Beyond Europe, it has been adopted by Australia, and most countries in Asia and South America. North American adoption was delayed by United States radio frequency regulations. This forced development of a variation of DECT, called DECT 6.0, using a slightly different frequency range. The technology is nearly identical, but the frequency difference makes the technology incompatible with systems in other areas, even from the same manufacturer. DECT has almost universally replaced other standards in most countries where it is used, with the exception of North America.
DECT is used primarily in home and small office systems, but is also available in many private branch exchange (PBX) systems for medium and large businesses. DECT can also be used for purposes other than cordless phones. Voice applications, such as baby monitors, are becoming common. Data applications also exist, but have been eclipsed by Wi-Fi. 3G and 4G cellular also competes with both DECT and Wi-Fi for both voice and data. DECT is also used in special applications, such as remote controls for industrial applications.
In 2011, development of a low power variant (DECT ULE — ultra low energy) was initiated to take advantage of the existing data channels for DECT to address markets such as home automation, security and climate control. The low power variant enables this standard to be used in battery powered devices and for many devices in the home to be connected through a single control unit.
The DECT standard includes a standardized interoperability profile for simple telephone capabilities, called GAP, which most manufacturers implement. GAP-conformance enables DECT handsets and bases from different manufacturers to interoperate at the most basic level of functionality, that of making and receiving calls. The standard also contains several other interoperability profiles, for data and for radio local-loop services.
Application.
The DECT standard fully specifies a means for a portable unit, such as a cordless telephone, to access a fixed telecoms network via radio. But, unlike the GSM standards, does not specify any internal aspects of the fixed network. Connectivity to the fixed network (that may be of many different kinds) is done through a base station or "Radio Fixed Part" to terminate the radio link, and a gateway to connect calls to the fixed network. In most cases the gateway connection is to the public switched telephone network or telephone jack, although connectivity with newer technologies such as Voice over IP has become available. There are also other devices such as some baby monitors utilizing DECT, and in these devices there is no gateway functionality.
The DECT standard originally envisaged three major areas of application:
Of these, the domestic application (cordless home telephones) has been extremely successful. The enterprise PABX market had some success, and all the major PABX vendors have offered DECT access options. The public access application did not succeed, since public cellular networks rapidly out-competed DECT by coupling their ubiquitous coverage with large increases in capacity and continuously falling costs. There has been only one major installation of DECT for public access: in early 1998 Telecom Italia launched a DECT network known as "Fido" after much regulatory delay, covering major cities in Italy. The service was promoted for only a few months and, having peaked at 142,000 subscribers, was shut down in 2001.
DECT has also been used for Fixed Wireless Access as a substitute for copper pairs in the "last mile" in countries such as India and South Africa. By using directional antennas and sacrificing some traffic capacity, cell coverage could extend to over . In Europe the power limit laid down for use of the DECT spectrum (250 mW peak) was expressed as effective radiated power (ERP), rather than the more commonly used equivalent isotropically radiated power (EIRP), permitting the use of high-gain directional antennas to produce much higher EIRP and hence long ranges.
The standard is also used in electronic cash terminals, traffic lights, and remote door openers.
DECT 6.0.
The "6.0" in DECT 6.0 does not equate to a spectrum band, but is a marketing term coined by Rick Krupka while at Siemens for DECT devices manufactured for use in the United States and Canada. Although DECT 6.0 operates at 1.9 GHz, it was decided the term DECT 1.9 might have confused customers who equate larger numbers (such as the 2.4 and 5.8 in existing 2.4 GHz and 5.8 GHz cordless telephones) with later products. The DECT and DECT 6.0 technologies are essentially identical, except for operating frequency.
DECT 6.0 products do not support "GAP"; therefore, phones of differing brands are not guaranteed to work together. For example, a GE 28105EE1 DECT 6.0 headset will not receive calls if registered to a Uniden base unit.
The term DECT 6.0 is also sometimes inappropriately advertised in Australia, even though Australia uses the same allocated spectrum frequencies as Europe.
VoIP/IP-DECT.
To facilitate migrations from traditional PBXs to VoIP, manufacturers such as Gigaset (formerly Siemens), Lantiq, Ascom, Aastra, Philips, snom, Grandstream, Yealink and Spectralink developed IP-DECT solutions where the backhaul from the base station is via VoIP (H.323 or SIP) over Ethernet, while communications between base and handsets is via DECT.
DECT was originally intended for use with traditional analog telephone networks, but DECT bases have higher bit-rates at their disposal than traditional analog telephone networks could provide. To take advantage of these higher-speed networks, ETSI released the CAT-iq standard in 2005, which, among other features, standardizes wideband audio over DECT. CAT-iq is backwards compatible with GAP.
For enterprises, DECT with (wired) VoIP backhaul has several significant advantages and disadvantages in comparison to VoIP-over-WiFi, where, typically, the handsets are directly WiFi+VoIP-enabled (instead of having the DECT handset communicate via an intermediate VoIP-enabled base). On the one hand, VoIP-over-WiFi has a range advantage given sufficient access-points, while a DECT phone must remain in proximity to its own base (or repeaters thereof). On the other hand, for large networks VoIP-over-WiFi imposes significant design and maintenance complexity to ensure roaming facilities and high quality-of-service. In North America, DECT suffers from major deficits, especially in comparison to DECT elsewhere, since the UPCS band (1920–1930 MHz) used for DECT in North America is not free from heavy interference and only half as wide as that used in Europe (1880–1900 MHz), the 4 mW average transmission power limits the range to far less than the 10 mW permitted in Europe, and the commonplace lack of GAP compatibility among US vendors binds companies to a single vendor. Other alternatives to DECT-plus-VoIP include local microcells (e.g. with OpenBTS), which then enables use of standard cellular phones for local "in-house" telephony. In the Far East, in particular in Japan and Taiwan, the Personal Handy-phone System renders the "in-house" (and PBX) concept superfluous by making the entire "last-mile" wireless.
DECT ULE.
The latest DECT variant is DECT ULE, or DECT Ultra low energy. The standard was first discussed in January 2011 and the first commercial products were launched later that year by Dialog Semiconductor. Like DECT, DECT ULE standard uses the 1.9 GHz band, and so suffers less interference than Zigbee, Bluetooth, Wi-Fi, from microwave ovens, which all operate in the ISM unlicensed 2.4 GHz band.
The standard has been created to enable home automation, security, healthcare and energy monitoring applications that are battery powered and can easily connect to the Web using the large number of existing DECT enabled modems and can be managed using a smartphone.
Standards history.
The DECT standard was developed by ETSI in several phases, the first of that took place between 1988 and 1992 when the first round of standards were published. These were the ETS 300-175 series in 9 parts defining the air interface, and ETS 300-176 defining how the units should be type approved. A technical report, ETR-178, was also published to explain the standard. Subsequent standards were developed and published by ETSI to cover interoperability profiles and standards for testing.
Initially named "Digital European Cordless Telephone" at its launch by CEPT in November 1987, following a suggestion by Enrico Tosato of Italy, its name was soon changed to "Digital European Cordless Telecommunications" to reflect its broader range of application, including data services. In 1995, due to its more global usage, the name was changed from "European" to "Enhanced". It is an ETSI standard for digital portable phones (cordless home telephones), commonly used for domestic or corporate purposes. It is recognized by the ITU as fulfilling the IMT-2000 requirements and thus qualifies as a 3G system. Within the IMT-2000 group of technologies, DECT is referred to as 'IMT-2000 Frequency Time' (IMT-FT).
DECT was developed by ETSI but has since been adopted by many countries all over the World. The original DECT frequency band (1880 MHz–1900 MHz) is used in all countries in Europe. Outside Europe, it is used in most of Asia, Australia and South America. In the United States, the Federal Communications Commission in 2005 changed channelization and licensing costs in a nearby band (1920 MHz–1930 MHz, or 1.9 GHz), known as Unlicensed Personal Communications Services (UPCS), allowing DECT devices to be sold in the U.S. with only minimal changes. These channels are reserved exclusively for voice communication applications and therefore are less likely to experience interference from other wireless devices such as baby monitors and wireless networks.
Technical features.
Typical abilities of a domestic DECT Generic Access Profile (GAP) system includes:
Technical properties.
ETSI standards documentation (ETSI EN 300 175-1/2/3/4/5/6/7/8, REN/DECT-000268-1/2/3/4/5/6/7/8) prescribes the following technical properties for DECT.
The DECT physical layer uses:
This means that the radio spectrum is divided into physical channels in two dimensions: frequency and time.
The maximum allowed power for portable equipment as well as base stations is 250 mW. A portable device radiates an average of about 10 mW during a call as it is only using one of 24 time slots to transmit.
The DECT media access control layer controls the physical layer and provides connection oriented, connectionless and broadcast services to the higher layers.
The DECT data link layer uses Link Access Protocol Control (LAPC), a specially designed variant of the ISDN data link protocol called LAPD. They are based on HDLC.
The DECT network layer always contains the following protocol entities:
Optionally it may also contain others:
All these communicate through a Link Control Entity (LCE).
The call control protocol is derived from ISDN DSS1, which is a Q.931-derived protocol. Many DECT-specific changes have been made. The mobility management protocol includes many elements similar to the GSM protocol, but also includes elements unique to DECT.
Unlike the GSM protocol, the DECT network specifications do not define cross-linkages between the operation of the entities (for example, Mobility Management and Call Control). The architecture presumes that such linkages will be designed into the interworking unit that connects the DECT access network to whatever mobility-enabled fixed network is involved. By keeping the entities separate, the handset is capable of responding to any combination of entity traffic, and this creates great flexibility in fixed network design without breaking full interoperability.
DECT GAP is an interoperability profile for DECT. The intent is that two different products from different manufacturers that both conform not only to the DECT standard, but also to the GAP profile defined within the DECT standard, are able to interoperate for basic calling. The DECT standard includes full testing suites for GAP, and GAP products on the market from different manufacturers are in practice interoperable for the basic functions.
Security.
The DECT media access control layer also provides encryption services with the DECT Standard Cipher (DSC). The encryption is fairly weak, using a 35-bit initialization vector and encrypting the voice stream with 64-bit encryption.
The security algorithm has been decoded. Another attack involves impersonating a DECT base station, which allows calls to be listened to, recorded, and re-routed to a different destination.
While most of the DECT standard is publicly available, the part describing the DECT Standard Cipher was only available under a non-disclosure agreement to the phones' manufacturers from ETSI.
On June 8, 2002, a posting was made to the "alt.anonymous.messages" newsgroup containing what was claimed to be the reverse engineered source code of the implementation of the DECT Standard Cipher for the Samsung SP-R6150 telephone. This claim has since been refuted.
In 2008, members of the deDECTed.org project actually did reverse engineer the DECT Standard Cipher, and as of 2010 there has been a viable attack on it that can recover the key.
DECT for data networks.
Other interoperability profiles exist in the DECT suite of standards, and in particular the DPRS (DECT Packet Radio Services) bring together a number of prior interoperability profiles for the use of DECT as a wireless LAN and wireless internet access service. With good range (up to indoors and using directional antennae outdoors), dedicated spectrum, high interference immunity, open interoperability and data speeds of around 500 kbit/s, DECT appeared at one time to be a superior alternative to Wi-Fi. The protocol capabilities built into the DECT networking protocol standards were particularly good at supporting fast roaming in the public space, between hotspots operated by competing but connected providers. The first DECT product to reach the market, Olivetti's Net, was a wireless LAN, and German firms Dosch & Amand and Hoeft & Wessel built niche businesses on the supply of data transmission systems based on DECT.
However, the timing of the availability of DECT, in the mid-1990s, was too early to find wide application for wireless data outside niche industrial applications. Whilst contemporary providers of Wi-Fi struggled with the same issues, providers of DECT retreated to the more immediately lucrative market for cordless telephones. A key weakness was also the inaccessibility of the U.S. market, due to FCC spectrum restrictions at that time. By the time mass applications for wireless Internet had emerged, and the U.S. had opened up to DECT, well into the new century, the industry had moved far ahead in terms of performance and DECT's time as a technically competitive wireless data transport had passed.
Radio links.
DECT operates in the 1880–1900 MHz band and defines ten channels from 1881.792 MHz to 1897.344 MHz with a band gap of 1728 kHz. Each base station frame provides 12 duplex speech channels, with each time slot occupying any channel. DECT operates in multicarrier/TDMA/TDD structure. DECT also provides Frequency-hopping spread spectrum over TDMA/TDD structure. If frequency-hopping is avoided, each base station can provide up to 120 channels in the DECT spectrum before frequency reuse. Each timeslot can be assigned to a different channel in order to exploit advantages of frequency hopping and to avoid interference from other users in asynchronous fashion.
XDECT R.
XDECT R is a marketing term used by Uniden for its products for extending the range of DECT phones (apparently indefinitely) by using multiple repeater stations. The company has demonstrated the technology to a range in Australia.
Batteries.
Typically DECT handsets use replaceable rechargeable batteries (e.g. a pair of AAA NiMH). Charge control sophistication varies but can cause limited battery endurance. Replacements should not be fully/over charged before fitting because some phones (e.g. Gigaset S79H) detect the voltage and will not work if mistaken for primary cells.
Health and safety.
DECT is a UHF technology, with science similar to mobile phones, baby monitors, Wi-Fi, and other cordless telephone technologies, though the UK Health Protection Agency (HPA) claims that due to a mobile phone's adaptive power ability, a DECT cordless phone's radiation could actually exceed the radiation of a mobile phone (a DECT cordless phone's radiation has an average output power of 10 mW but is in the form of 100 bursts per second of 250 mW, a strength comparable to some mobile phones). As with all such wireless technologies, consensus is that there are negligible health effects from very low levels of non-ionizing radiation. Most studies have been unable to demonstrate any link to health effects, or have been inconclusive. Nevertheless, there has been persistent controversy over their health safety, and some national and international agencies have made specific recommendations about exposure.

</doc>
<doc id="8676" url="https://en.wikipedia.org/wiki?curid=8676" title="Dhyāna">
Dhyāna

Dhyāna may refer to:

</doc>
<doc id="8677" url="https://en.wikipedia.org/wiki?curid=8677" title="December 30">
December 30


</doc>
<doc id="8678" url="https://en.wikipedia.org/wiki?curid=8678" title="Donn">
Donn

According to Irish mythology, Donn, or the Dark One, is the Lord of the Dead and father of Diarmuid Ua Duibhne, whom he gave to Aengus Óg to be nurtured. Donn is regarded as the father of the Irish race; a position similar to that of Dis Pater and the Gauls, as noted by Julius Caesar.
Originally, Donn was the chief of the Sons of Mil, a mythological race who invaded Ireland, ousting the Tuatha Dé Danann. Donn slighted Ériu, one of the eponymous goddesses of Ireland, and he was drowned off the south-west coast of the island. A place near this spot, on a small rocky island named 'Tech nDuinn' ('the House of Donn'), became Donn's dwelling place as god of the dead. This house was the assembly place for the dead before they began the journey to the Otherworld. He is similar in some regards to the Hindu deity Yama.
Knockfierna, County Limerick was Donn Fírinne's residence. Cnoc Fírinne (meaning 'Hill of Truth') takes its name from Donn, who is said to forewarn the local people of bad weather by gathering up rain clouds around him on the hill.
Donn is also associated with Doonbeg, County Clare, specifically the sand dunes located near the modern-day golf course.
In modern Irish, the word for the colour brown is "donn".

</doc>
<doc id="8681" url="https://en.wikipedia.org/wiki?curid=8681" title="Data compression ratio">
Data compression ratio

Data compression ratio, also known as compression power, is a computer science term used to quantify the reduction in data-representation size produced by a data compression algorithm. The data compression ratio is analogous to the physical compression ratio used to measure physical compression of substances.
Definitions.
Data compression ratio is defined as the ratio between the "uncompressed size" and "compressed size":
Thus a representation that compresses a 10MB file to 2MB has a compression ratio of 10/2 = 5, often notated as an explicit ratio, 5:1 (read "five" to "one"), or as an implicit ratio, 5/1. Note that this formulation applies equally for compression, where the uncompressed size is that of the original; and for decompression, where the uncompressed size is that of the reproduction. 
Sometimes the space savings is given instead, which is defined as the reduction in size relative to the uncompressed size: 
Thus a representation that compresses a 10MB file to 2MB would yield a space savings of 1 - 2/10 = 0.8, often notated as a percentage, 80%.
For signals of indefinite size, such as streaming audio and video, the compression ratio is defined in terms of uncompressed and compressed data rates instead of data sizes: 
and instead of space savings, one speaks of data-rate savings, which is defined as the data-rate reduction relative to the uncompressed data rate: 
For example, uncompressed songs in CD format have a data rate of 16 bits/channel x 2 channels x 44.1 kHz ≅ 1.4 Mbit/s, whereas AAC files on an iPod are typically compressed to 128 kbit/s, yielding a compression ratio of 10.9, for a data-rate savings of 0.91, or 91%. 
When the uncompressed data rate is known, the compression ratio can be inferred from the compressed data rate.
Lossless vs. lossy.
Lossless compression of digitized data such as video, digitized film, and audio preserves all the information, but it does not generally achieve compression ratio much better than 2:1 because of the intrinsic entropy of the data. Compression algorithms which provide higher ratios either incur very large overheads or work only for specific data sequences (e.g. compressing a file with mostly zeros). In contrast, lossy compression (e.g. JPEG for images, or MP3 and Opus for audio) can achieve much higher compression ratios at the cost of a decrease in quality, such as Bluetooth audio streaming, as visual or audio compression artifacts from loss of important information are introduced. A compression ratio of at least 50:1 is needed to get 1080i video into a 20 Mbit/s MPEG transport stream.
Uses.
The data compression ratio can serve as a measure of the complexity of a data set or signal, in particular it is used to approximate the algorithmic complexity.

</doc>
<doc id="8683" url="https://en.wikipedia.org/wiki?curid=8683" title="Disc jockey">
Disc jockey

A disc jockey (abbreviated DJ, D.J. or deejay) is a person who plays a mix of recorded music for an audience, either a radio, television or Internet audience if the mix is broadcast over the airwaves or via another medium (e.g., cable television or online) or the audience in a venue such as a bar, nightclub or dance club. In venues such as a club or dance events such as a rave or Electronic Dance Music festival, this is typically an audience which dances to the music, which is played through a powerful sound reinforcement system. In some expensive lounges and restaurants, there may be a DJ, but the audience listens to the music rather than dancing. Club DJs developed the skill to seamlessly transition from one recording of a song to another by using turntable skills that involve the simultaneous manipulation of a two record turntables and a DJ mixer. Key skills include matching the beats of two records, cueing up the next song to the desired start point with headphones and using their knowledge of music genres and artists to select songs that will keep the audience dancing.
Originally, "disc" (uncommonly spelled "disque" in French or commonly "disk" in American English) referred to phonograph records, not the later compact discs. In the 2010s, the term includes all forms of music playback, no matter which medium is used (e.g. vinyl records, CDs, or digital audio players such as MP3 players).
The title "DJ" is also commonly used by DJs in front of their real names or adopted pseudonyms or stage names as a title to denote their profession and the music they play.
Types.
There are several types of disc jockey. Radio DJs or radio personalities introduce and play music that is broadcast on AM, FM, digital or Internet radio stations. Club DJs select and play music in bars, nightclubs or discothèques, or at parties or raves, or even in stadiums. Mobile DJs travel with portable sound systems and play recorded music at a variety of events. Some mobile DJs also serve as the master of ceremonies (MC) at weddings or other events, directing the attention of attendees, and maintaining a room-wide focus on what is included in the event's agenda. There are also many competitions for DJs that specialise in different turntablism techniques, such as mixing, hip hop music-style "scratching" or other kinds of techniques.
Other types of DJ use musical performance techniques that allow them to be categorized as performing musicians, depending on the situation. Hip hop DJs not only select and play music using multiple turntables (or other sound sources) to back one or more MCs or rappers, but they also perform turntable "scratching" to create rhythmic and percussive sounds. Hip hop DJs and are also often songwriters or music producers who use turntablism and sampling to create backing instrumentals for new tracks.
In reggae, the DJ (deejay) is a vocalist who raps, "toasts", or chats over pre-recorded rhythm tracks while the individual who helps the DJ by selecting tracks to be played is called the selector.
Many electronica artists and producers who also work as DJs often perform music by combining turntablism with keyboards or live electronics. Electronica, hip-hop or reggae DJs also often collaborate and play live music with bands and musicians from several musical genres (rock, heavy metal, jazz or even classical music), using turntables and electronics as musical instruments. According to a 2012 study, there are approximately 1¼ million professional disc jockeys in the world.
The tunes a DJ picks to play and the style in which they mix them defines a DJ's style. DJs are often connoisseurs of various music genres, and they often spend time in used record stores searching for rare or obscure tracks to use in their club sets. DJs also use DJ mixers to transition from song to song in different ways. One key technique used by DJs for seamlessly transitioning from one song to another is beatmatching. The DJ's style can and should be pliable, depending on what club he or she is playing in and what kind of music is expected of the DJ (e.g. a house music dance requires a different set list than a rave or a techno event). The DJ also has to "read" the mood of the dancers, and pick songs or styles of music that will keep the dancers on the dance floor.
Equipment and technique.
The minimum equipment that a club DJ needs includes:
Other equipment could or can be added to the basic DJ setup (above), providing unique sound manipulations. Such devices include, but are not limited to:
Several techniques are used by DJs as a means to better mix and blend recorded music. These techniques primarily include the cueing, equalization and audio mixing of two or more sound sources. The complexity and frequency of special techniques depends largely on the setting in which a DJ is working. Radio DJs are less likely to focus on advanced music-mixing procedures than club DJs, who rely on a smooth transition between songs using a range of techniques. However, some radio DJs are experienced club DJs, so they use the same sophisticated mixing techniques.
Club DJ turntable techniques include beatmatching, phrasing and slip-cueing to preserve energy on a dancefloor. Turntablism embodies the art of cutting, beat juggling, scratching, needle drops, phase shifting, back spinning and more to perform the transitions and overdubs of samples in a more creative manner (although turntablism is often considered a use of the turntable as a musical instrument rather than a tool for blending recorded music). Professional DJs may use harmonic mixing to choose songs that are in compatible musical keys.
Recent advances in technology in both DJ hardware and software can provide assisted or automatic completion of some traditional DJ techniques and skills. Examples include phrasing and beatmatching, which can be partially or completely automated by utilizing DJ software that performs automatic synchronization of sound recordings, a feature commonly labelled "sync". Most DJ mixers now include a beat-counter which analyzes the tempo of an incoming sound source and displays its tempo in beats per minute (BPM), which may assist with beatmatching analog sound sources.
In the past, being a DJ has largely been a self-taught craft but with the complexities of new technologies and the convergence with music production methods, there are a growing number of schools and organizations that offer instruction on the techniques.
History.
19th century to 1920s.
In 1892, Emile Berliner began commercial production of his gramophone records, the first disc records to be offered to the public. In 1906, Reginald Fessenden transmitted the first audio radio broadcast in history also playing the first record, that of a contralto singing Handel's "Largo" from "Xerxes".
The world's first radio disc jockey was Ray Newby, of Stockton, California. In 1909, at 16 years of age, Newby began regularly playing records on a small transmitter while he was a student at Herrold College of Engineering and Wireless, located in San Jose, California, under the authority of radio pioneer Charles "Doc" Herrold.
By 1910, regular radio broadcasting had started to use "live" as well as prerecorded sound. In the early radio age, content typically included comedy, drama, news, music, and sports reporting. The on-air announcers and programmers would later be known as disc jockeys. In the 1920s, juke joints became popular as places for dancing and drinking to recorded jukebox music. In 1927, Christopher Stone became the first radio announcer and programmer in the United Kingdom, on the BBC radio station.
1930s–1950s.
In 1935, American radio commentator Walter Winchell coined the term "disc jockey" (the combination of "disc", referring to the disc records, and "jockey", which is an operator of a machine) as a description of radio announcer Martin Block, the first announcer to become a star. While his audience was awaiting developments in the Lindbergh kidnapping, Block played records and created the illusion that he was broadcasting from a ballroom, with the nation’s top dance bands performing live. The show, which he called "Make Believe Ballroom", was an instant hit. The term "disc jockey" appeared in print in "Variety" in 1941.
Prior to this, most music heard on radio was live; most radio stations had an orchestra or band on the payroll. The Federal Communications Commission also clearly favored live music, providing accelerated license approval to stations promising not to use any recordings for their first three years on the air. Many noted recording artists tried to keep their recorded works off the air by having their records labeled as not being legal for airplay. It took a Federal court ruling in 1940 to establish that a recording artist had no legal right to control the use of a record after it was sold.
On 4 February 1943, Columbia Pictures released Reveille with Beverly, a musical review starring Ann Miller as a switchboard operator for a hospital who becomes a local disc jockey popular with servicemen in the hospital. Out of compassion for the sick, she insists on playing jive instead of the classics that have been programmed.
In 1943, Jimmy Savile launched the world's first DJ dance party by playing jazz records in the upstairs function room of the Loyal Order of Ancient Shepherds in Otley, England. In 1947, he claims to have become the first DJ to use twin turntables for continuous play, and in 1958 became a radio DJ at Radio Luxembourg. Also in 1947, the Whiskey à Go-Go nightclub opened in Paris, France, considered to be the world's first commercial discothèque, or disco (deriving its name from the French word meaning a nightclub where the featured entertainment is recorded music rather than an on-stage band). Regine began playing on twin turntables there in 1953. Discos began appearing across Europe and the United States.
The postwar period coincided with the rise of the radio disc jockey as a celebrity separate from the radio station, also known as a "radio personality". In the days before station-controlled playlists, the DJ often followed their personal tastes in music selection. DJs also played a role in exposing rock and roll artists to large, national audiences. While at WERE (1300 AM) in Cleveland, Ohio, DJ Bill Randle was one of the first to introduce Elvis Presley to radio audiences in the northeastern U.S.
Notable U.S. radio disc jockeys of the period include Alan Freed, Wolfman Jack, Casey Kasem, and their British counterparts such as the BBC's Brian Matthew and Alan Freeman, and in the '60s Radio London's John Peel, and Radio Caroline's Tony Blackburn.
Freed is commonly referred to as the "father of rock and roll" due to his promotion of the music and his introduction of the phrase "rock and roll" on radio in the early 1950s. Freed also made a practice of presenting music by African-American artists rather than cover versions by white artists on his radio program. Freed's career ended when it was shown that he had accepted payola, a practice that was highly controversial at the time, resulting in his being fired from his job at WABC.
In the 1950s, American radio DJs would appear live at "sock hops" and "platter parties" and assume the role of a human jukebox. They would usually play 45-rpm records, featuring hit singles on one turntable while talking between songs. In some cases, a live drummer was hired to play beats between songs to maintain the dance floor. In 1955, Bob Casey, a well-known "sock hop" DJ, brought the two-turntable system to the U.S. Throughout the 1950s, payola continued to be a problem and one result of the payola scandal was tighter control of the music by station management. The Top 40 format emerged, where popular songs are played repeatedly.
In the late 1950s, sound systems, a new form of public entertainment, were developed in the ghettos of Kingston, Jamaica. Promoters, who called themselves DJs, would throw large parties in the streets that centered on the disc jockey, called the "selector," who played dance music from large, loud PA systems and bantered over the music with a boastful, rhythmic chanting style called "toasting". These parties quickly became profitable for the promoters, who would sell admission, food, and alcohol, leading to fierce competition between DJs for the biggest sound systems and newest records.
1960s and 1970s.
In the mid-1960s, nightclubs and discothèques continued to grow in Europe and the United States. Specialized DJ equipment, such as Rudy Bozak's classic CMA-10-2DL mixer, began to appear on the market. In 1969, American club DJ Francis Grasso popularized beatmatching at New York's Sanctuary nightclub. Beatmatching is the technique of creating seamless transitions between records with "matching" "beats", or tempos. Grasso also developed slip-cuing, the technique of holding a record still while the turntable is revolving underneath, releasing it at the desired moment to create a sudden transition from the previous record. (This technique had long been used in radio.)
By 1968, the number of dance clubs started to decline; most American clubs either closed or were transformed into clubs featuring live bands. Neighborhood block parties that were modelled after Jamaican sound systems gained popularity in Europe and in the boroughs of New York City.
In 1973, Jamaican-born DJ Kool Herc, widely regarded as the "father of hip-hop culture," performed at block parties in his Bronx neighborhood and developed a technique of mixing back and forth between two identical records to extend the rhythmic instrumental segment, or "break". Turntablism, the art of using turntables not only to play music but to manipulate sound and create original music, began to develop.
In 1974, Technics released the first SL-1200 turntable, which evolved into the SL-1200 MK2 in 1979—which, as of the early-2010s, remains an industry standard for DJing. In 1974, German electronic music band Kraftwerk released the 22-minute song "Autobahn," which takes up the entire first side of that LP. Years later, Kraftwerk would become a significant influence on hip-hop artists such as Afrika Bambaataa and house music pioneer Frankie Knuckles. During the mid-1970s, Hip-hop music and culture began to emerge, originating among urban African Americans and Latinos in New York City. The four main elements of Hip Hop culture are graffiti, DJing, breakdancing, and MCing (rapping).
In the mid-1970s, the soul-funk blend of dance pop known as disco took off in the mainstream pop charts in the United States and Europe, causing discothèques to experience a rebirth. Unlike many late-1960s clubs, which featured live bands, discothèques used the DJ's selection and mixing of records as the entertainment. In 1975, record pools began, providing disc jockeys access to newer music from the industry in an efficient method.
In 1975, hip-hop DJ Grand Wizard Theodore invented the scratching technique by accident. In 1976, American DJ, editor, and producer Walter Gibbons remixed "Ten Percent" by Double Exposure, one of the earliest commercially released 12″ singles (a.k.a. "maxi-single"). In 1979, the Sugar Hill Gang released "Rapper's Delight", the first hip-hop record to become a hit.
In 1977, Saratoga Springs, NY disc jockey Tom L. Lewis introduced the Disco Bible (later renamed Disco Beats), which published hit disco songs listed by beats per minute (tempo), as well as by either artist or song title. Billboard ran an article on the new publication, and it went national relatively quickly. The list made it easier for beginning DJs to learn how to create seamless transitions between songs without dancers having to change their rhythm on the dance floor. Today, DJs can find the beats per minute of songs in the BPM List.
1980s.
In 1981, the cable television network MTV was launched, originally devoted to music videos, especially popular rock music. The term "video jockey", or VJ, was used to describe the fresh-faced youth who introduced the music videos. In 1982, the demise of disco in the mainstream by the summer of 1982 forced many nightclubs to either close or change entertainment styles, such as by providing MTV-style video dancing or live bands. Released in 1982, the song "Planet Rock" by DJ Afrika Bambaataa was the first hip-hop song to feature synthesizers. The song melded electro hip-hop beats influenced by Yellow Magic Orchestra with the melody from Kraftwerk's "Trans-Europe Express." In 1982, the Compact Disc reached the public market in Asia, and early the following year in other markets. This event is often seen as the "Big Bang" of the digital audio revolution.
In the early 1980s, NYC disco DJ Larry Levan, known for his electric mixes, gained a cult following, and the Paradise Garage, the nightclub at which he spun, became the prototype for the modern dance club where the music and the DJ were showcased. Around the same time, the disco-influenced electronic style of dance music called house music emerged in Chicago. The name was derived from the Warehouse Club in Chicago, where resident DJ Frankie Knuckles mixed old disco classics and Eurosynth pop. House music is essentially disco music with electronic drum machine beats. The common element of most house music is a 4/4 beat generated by a drum machine or other electronic means (such as a sampler), together with a solid (usually also electronically generated) synth bassline. In 1983, Jesse Saunders released what some consider the first house music track, "On & On." The mid-1980s also saw the emergence of New York Garage, a house music hybrid that was inspired by Levan's style and sometimes eschewed the accentuated high-hats of the Chicago house sound.
During the mid-1980s, techno music emerged from the Detroit club scene. Being geographically located between Chicago and New York, Detroit techno artists combined elements of Chicago house and New York garage along with European imports. Techno distanced itself from disco's roots by becoming almost purely electronic with synthesized beats. In 1985, the Winter Music Conference started in Fort Lauderdale Florida and became the premier electronic music conference for dance music disc jockeys.
In 1985, TRAX Dance Music Guide was launched by American Record Pool in Beverly Hills. It was the first national DJ-published music magazine, created on the Macintosh computer using extensive music market research and early desktop publishing tools. In 1986, "Walk This Way", a rap/rock collaboration by Run DMC and Aerosmith, became the first hip-hop song to reach the Top 10 on the "Billboard" Hot 100. This song was the first exposure of hip-hop music, as well as the concept of the disc jockey as band member and artist, to many mainstream audiences. In 1988, "DJ Times" magazine was first published. It was the first US-based magazine specifically geared toward the professional mobile and club DJ.
1990s.
During the early 1990s, the rave scene built on the acid house scene. The rave scene changed dance music, the image of DJs, and the nature of promoting. The innovative marketing surrounding the rave scene created the first superstar DJs who established marketable "brands" around their names and sound. Some of these celebrity DJs toured around the world and were able to branch out into other music-related activities. During the early 1990s, the Compact Disc surpassed the gramophone record in popularity, but gramophone records continued to be made (although in very limited quantities) into the 21st century—particularly for club DJs and for local acts recording on small regional labels.
In 1991, "Mobile Beat" magazine, geared specifically toward mobile DJs, began publishing. In 1992, the Moving Picture Experts Group released the MPEG-1 standard, designed to produce reasonable sound at low bit rates. The lossy compression scheme MPEG-1 Layer-3, popularly known as MP3, later revolutionized the digital music domain. In 1993, the first internet "radio station", Internet Talk Radio, was developed by Carl Malamud. Because the audio was relayed over the internet, it was possible to access internet radio stations from anywhere in the world. This made it a popular service for both amateur and professional disc jockeys operating from a personal computer.
In 1998, the first MP3 digital audio player was released, the Eiger Labs MPMan F10. Final Scratch debuted at the BE Developer Conference, marking the first digital DJ system to allow DJs control of MP3 files through special time-coded vinyl records or CDs. While it would take sometime for this novel concept to catch on with the "die hard Vinyl DJs", This would soon become the first step in the new Digital DJ revolution. Manufacturers joined with computer DJing pioneers to offer professional endorsements, the first being Professor Jam (a.k.a. William P. Rader), who went on to develop the industry's first dedicated computer DJ convention and learning program, the "CPS (Computerized Performance System) DJ Summit", to help spread the word about the advantages of this emerging technology.
In 1999, Shawn Fanning released Napster, the first of the massively popular peer-to-peer file sharing systems. During this period, the AVLA (Audio Video Licensing Agency) of Canada announced an MP3 DJing license, administered by the Canadian Recording Industry Association. This meant that DJs could apply for a license giving them the right to perform publicly using music stored on a hard drive, instead of having to cart their whole CD collections around to their gigs.
2000s.
At the start of the new century, the introduction of advances in technology made it possible for new sounds to be developed. The introduction of the Pioneer SVM-1000 Audio and Video Mixer and other high tech digital sound mixers made a whole new culture of disco DJ integration. The proliferation of Internet technologies have also created a culture of disc jockey enthusiast groups. DJ battles imitating the events on the game gave the DJ industry a more competitive phase. The DJ industry has become increasingly about the atmosphere that goes along with a performance. Now not only does the DJ show deal with music and mixing but also lights and effect go along with it.

</doc>
<doc id="8687" url="https://en.wikipedia.org/wiki?curid=8687" title="Detroit">
Detroit

Detroit () is the most populous city in the U.S. state of Michigan, the fourth-largest city in the Midwest and the largest city on the United States–Canada border. It is the seat of Wayne County, the most populous county in the state. Detroit's metropolitan area, known as Metro Detroit, is home to 5.3 million people, making it the fourteenth-most populous metropolitan area in the United States and the second-largest in the Midwestern United States (behind Chicago). It is a major port on the Detroit River, a strait that connects the Great Lakes system to the Saint Lawrence Seaway. 
The City of Detroit anchors the second-largest economic region in the Midwest, behind Chicago, and the thirteenth-largest in the United States.
Detroit is the center of a three-county urban area (population 3,734,090, area of , a 2010 United States Census) six-county metropolitan statistical area (2010 Census population of 4,296,250, area of ), and a nine-county Combined Statistical Area (2010 Census population of 5,218,852, area of ). The Detroit–Windsor area, a commercial link straddling the Canada–U.S. border, has a total population of about 5,700,000. The Detroit metropolitan region holds roughly one-half of Michigan's population.
Detroit was founded on July 24, 1701, by the French explorer and adventurer Antoine de la Mothe Cadillac and a party of settlers. 
With expansion of the automobile industry, the Detroit area emerged as a significant metropolitan region within the United States in the early 20th century, when the city became the fourth-largest in the country for a period. In the 1950s and 1960s, expansion continued with construction of a regional freeway system.
Due to industrial restructuring and loss of jobs in the auto industry, Detroit lost considerable population from the late 20th century to present. Between 2000 and 2010 the city's population fell by 25 percent, changing its ranking from the nation's 10th-largest city to 18th. In 2010, the city had a population of 713,777, more than a 60 percent drop from a peak population of over 1.8 million at the 1950 census. This resulted from suburbanization, industrial restructuring, and the decline of Detroit's auto industry. Following the shift of population and jobs to its suburbs or other states or nations, the city has focused on becoming the metropolitan region's employment and economic center. Downtown Detroit has held an increased role as an entertainment destination in the 21st century, with the restoration of several historic theatres, several new sports stadiums, and a riverfront revitalization project. More recently, the population of Downtown Detroit, Midtown Detroit, and a handful of other neighborhoods has increased. Many other neighborhoods remain distressed, with extensive abandonment of properties.
The Governor of Michigan, Rick Snyder, declared a financial emergency for the city in March 2013, appointing an emergency manager. On July 18, 2013, Detroit filed the largest municipal bankruptcy case in U.S. history. It was declared bankrupt by Judge Steven W. Rhodes of the Bankruptcy Court for the Eastern District of Michigan on December 3, 2013; he cited its $18.5 billion debt and declared that negotiations with its thousands of creditors were unfeasible. On November 7, 2014, Judge Rhodes approved the city's bankruptcy plan, allowing the city to begin the process of exiting bankruptcy. The City of Detroit successfully exited Chapter 9 municipal bankruptcy with all finances handed back to the city at midnight on December 11, 2014.
History.
European settlement.
The city was named by French colonists, referring to the Detroit River (, meaning "the strait of Lake Erie"), linking Lake Huron and Lake Erie; in the historical context, the strait included the St. Clair River, Lake St. Clair and the Detroit River.
On the shores of the strait, in 1701, the French officer Antoine de la Mothe Cadillac, along with fifty-one French people and French Canadians, founded a settlement called Fort Pontchartrain du Détroit, naming it after Louis Phélypeaux, comte de Pontchartrain, Minister of Marine under Louis XIV. France offered free land to colonists to attract families to Detroit; when it reached a total population of 800 in 1765, it was the largest city between Montreal and New Orleans, both also French settlements. By 1773, the population of Detroit was 1,400. By 1778, its population was up to 2,144 and it was the third-largest city in the Province of Quebec.
The region grew based on the lucrative fur trade, in which numerous Native American people had important roles. Detroit's city flag reflects its French colonial heritage. (See Flag of Detroit). Descendants of the earliest French and French Canadian settlers formed a cohesive community who gradually were replaced as the dominant population after more Anglo-American settlers came to the area in the early 19th century. Living along the shores of Lakes St. Clair, and south to Monroe and downriver suburbs, the French Canadians of Detroit, also known as Muskrat French, remain a subculture in the region today.
During the French and Indian War (1754–63), the North American front of the Seven Years' War between Britain and France, British troops gained control of the settlement in 1760. They shortened the name to "Detroit". Several Native American tribes launched Pontiac's Rebellion (1763), and conducted a siege of Fort Detroit, but failed to capture it. In defeat, France ceded its territory in North America east of the Mississippi to Britain following the war.
Following the American Revolutionary War and United States independence, Britain ceded Detroit along with other territory in the area under the Jay Treaty (1796), which established the northern border with Canada. In 1805, fire destroyed most of the Detroit settlement, which consisted mostly of wooden buildings. A river warehouse and brick chimneys of the former wooden homes were the sole structures to survive.
19th century.
From 1805 to 1847, Detroit was the capital of Michigan (first the territory, then the state). Detroit surrendered without a fight to British troops during the War of 1812 in the Siege of Detroit. The Battle of Frenchtown (January 18–23, 1813) was part of a United States effort to retake the city, and American troops suffered their highest fatalities of any battle in the war. This battle is commemorated at River Raisin National Battlefield Park south of Detroit in Monroe County. Detroit was finally recaptured by the United States later that year.
It was incorporated as a city in 1815. As the city expanded, a geometric street plan developed by Augustus B. Woodward was followed, featuring grand boulevards as in Paris.
Prior to the American Civil War, the city's access to the Canadian border made it a key stop for refugee slaves gaining freedom in the North along the Underground Railroad. Many went across the Detroit River to Canada to escape pursuit by slave catchers. There were estimated to be 20,000 to 30,000 African-American refugees who settled in Canada.
Numerous men from Detroit volunteered to fight for the Union during the American Civil War, including the 24th Michigan Infantry Regiment (part of the legendary Iron Brigade), which fought with distinction and suffered 82% casualties at the Battle of Gettysburg in 1863. When the First Volunteer Infantry Regiment arrived to fortify Washington, DC, President Abraham Lincoln is quoted as saying "Thank God for Michigan!" George Armstrong Custer led the Michigan Brigade during the Civil War and called them the "Wolverines".
During the late 19th century, several Gilded Age mansions reflecting the wealth of industry and shipping magnates were built east and west of the current downtown, along the major avenues of the Woodward plan. Most notable among them was the David Whitney House located at 4421 Woodward Avenue, which became a prime location for mansions. During this period some referred to Detroit as the "Paris of the West" for its architecture, grand avenues in the Paris style, and for Washington Boulevard, recently electrified by Thomas Edison. The city had grown steadily from the 1830s with the rise of shipping, shipbuilding, and manufacturing industries. Strategically located along the Great Lakes waterway, Detroit emerged as a major port and transportation hub.
In 1896, a thriving carriage trade prompted Henry Ford to build his first automobile in a rented workshop on Mack Avenue. During this growth period, Detroit expanded its borders by annexing all or part of several surrounding villages and townships.
20th century.
In 1903, Henry Ford founded the Ford Motor Company. Ford's manufacturing—and those of automotive pioneers William C. Durant, the Dodge Brothers, Packard, and Walter Chrysler—established Detroit's status in the early 20th century as the world's automotive capital. The growth of the auto industry was reflected by changes in businesses throughout the Midwest and nation, with the development of garages to service vehicles and gas stations, as well as factories for parts and tires.
With the rapid growth of industrial workers in the auto factories, labor unions such as the American Federation of Labor and the United Auto Workers fought to organize workers to gain them better working conditions and wages. They initiated strikes and other tactics in support of improvements such as the 8-hour day/40-hour work week, increased wages, greater benefits and improved working conditions. The labor activism during those years increased influence of union leaders in the city such as Jimmy Hoffa of the Teamsters and Walter Reuther of the Autoworkers.
The city became the 4th-largest in the nation in 1920, after only New York City, Chicago and Philadelphia, with the influence of the booming auto industry.
The prohibition of alcohol from 1920 to 1933 resulted in the Detroit River becoming a major conduit for smuggling of illegal Canadian spirits.
Detroit, like many places in the United States, developed racial conflict and discrimination in the 20th century following rapid demographic changes as hundreds of thousands of new workers were attracted to the industrial city; in a short period it became the 4th-largest city in the nation. The Great Migration brought rural blacks from the South; they were outnumbered by southern whites who also migrated to the city. Immigration brought southern and eastern Europeans of Catholic and Jewish faith; these new groups competed with native-born whites for jobs and housing in the booming city. Detroit was one of the major Midwest cities that was a site for the dramatic urban revival of the Ku Klux Klan beginning in 1915. "By the 1920s the city had become a stronghold of the KKK," whose members opposed Catholic and Jewish immigrants, as well as black Americans. 
The Black Legion, a secret vigilante group, was active in the Detroit area in the 1930s, when one-third of its estimated 20,000 to 30,000 members in Michigan were based in the city. It was defeated after numerous prosecutions following the kidnapping and murder in 1936 of Charles Poole, a Catholic Works Progress Administration organizer. A total of 49 men of the Black Legion were convicted of numerous crimes, with many sentenced to life in prison for murder.
In the 1940s the world's "first urban depressed freeway" ever built, the Davison, was constructed in Detroit. During World War II, the government encouraged retooling of the American automobile industry in support of the Allied powers, leading to Detroit's key role in the American Arsenal of Democracy.
Jobs expanded so rapidly that 400,000 people were attracted to the city from 1941 to 1943, including 50,000 blacks in the second wave of the Great Migration, and 350,000 whites, many of them from the South. Some European immigrants and their descendants feared black competition for jobs and housing. The federal government prohibited discrimination in defense work but when in June 1943, Packard promoted three blacks to work next to whites on its assembly lines, 25,000 whites walked off the job. The Detroit race riot of 1943 took place three weeks after the Packard plant protest. Over the course of three days, 34 people were killed, of whom 25 were African American, and approximately 600 were injured, 75% black people.
Postwar era.
Industrial mergers in the 1950s, especially in the automobile sector, increased oligopoly in the American auto industry. Detroit manufacturers such as Packard and Hudson merged into other companies and eventually disappeared. At its peak population of 1,849,568, in the 1950 Census, the city was the 5th-largest in the United States, after New York City, Chicago, Philadelphia and Los Angeles.
As in other major American cities in the postwar era, construction of an extensive highway and freeway system around Detroit and pent-up demand for new housing stimulated suburbanization; highways made commuting by car easier. In 1956, Detroit's last heavily used electric streetcar line along the length of Woodward Avenue was removed and replaced with gas-powered buses. It was the last line of what had once been a 534-mile network of electric streetcars. In 1941 at peak times, a streetcar ran on Woodward Avenue every 60 seconds.
All of these changes in the area's transportation system favored low-density, auto-oriented development rather than high-density urban development, and industry also moved to the suburbs. The metro Detroit area developed as one of the most sprawling job markets in the United States by the 21st century, and combined with poor public transport, resulted in many jobs beyond the reach of urban low-income workers.
In 1950, the city held about one-third of the state's population, anchored by its industries and workers. Over the next sixty years, the city's population declined to less than 10 percent of the state's population. During the same time period, the sprawling Detroit metropolitan area, which surrounds and includes the city, grew to contain more than half of Michigan's population. The shift of population and jobs eroded Detroit's tax base.
In June 1963, Rev. Martin Luther King, Jr. gave a major speech in Detroit that foreshadowed his "I Have a Dream" speech in Washington, D.C. two months later. While the African-American Civil Rights Movement gained significant federal civil rights laws in 1964 and 1965, longstanding inequities resulted in confrontations between the police and inner city black youth wanting change. Longstanding tensions in Detroit culminated in the Twelfth Street riot in July 1967. Governor George W. Romney ordered the Michigan National Guard into Detroit, and President Johnson sent in U.S. Army troops. The result was 43 dead, 467 injured, over 7,200 arrests, and more than 2,000 buildings destroyed, mostly in black residential and business areas. Thousands of small businesses closed permanently or relocated to safer neighborhoods. The affected district lay in ruins for decades. It was the most costly riot in the United States.
On August 18, 1970, the NAACP filed suit against Michigan state officials, including Governor William Milliken, charging "de facto" public school segregation. The NAACP argued that although schools were not legally segregated, the city of Detroit and its surrounding counties had enacted policies to maintain racial segregation in public schools. The NAACP also suggested a direct relationship between unfair housing practices and educational segregation, which followed segregated neighborhoods. The District Court held all levels of government accountable for the segregation in its ruling. The Sixth Circuit Court affirmed some of the decision, holding that it was the state's responsibility to integrate across the segregated metropolitan area.
The U.S. Supreme Court took up the case February 27, 1974. The subsequent "Milliken v. Bradley" decision had wide national influence. In a narrow decision, the Court found that schools were a subject of local control and that suburbs could not be forced to solve problems in the city's school district.
"Milliken was perhaps the greatest missed opportunity of that period," said Myron Orfield, professor of law at the University of Minnesota. "Had that gone the other way, it would have opened the door to fixing nearly all of Detroit's current problems." 
John Mogk, a professor of law and an expert in urban planning at Wayne State University in Detroit, says, "Everybody thinks that it was the riots n 196 that caused the white families to leave. Some people were leaving at that time but, really, it was after Milliken that you saw mass flight to the suburbs. If the case had gone the other way, it is likely that Detroit would not have experienced the steep decline in its tax base that has occurred since then."
1970s and decline.
In November 1973, the city elected Coleman Young as its first black mayor. After taking office, Young emphasized increasing racial diversity in the police department.
Young also worked to improve Detroit's transportation system, but tension between Young and his suburban counterparts over regional matters was problematic throughout his mayoral term. In 1976, the federal government offered $600 million for building a regional rapid transit system, under a single regional authority. But the inability of Detroit and its suburban neighbors to solve conflicts over transit planning resulted in the region losing the majority of funding for rapid transit. Following the failure to reach an agreement over the larger system, the City moved forward with construction of the elevated downtown circulator portion of the system, which became known as the Detroit People Mover.
The gasoline crises of 1973 and 1979 also affected Detroit and the U.S. auto industry. Buyers chose smaller, more fuel-efficient cars made by foreign makers as the price of gas rose. Efforts to revive the city were stymied by the struggles of the auto industry, as their sales and market share declined. Automakers laid off thousands of employees and closed plants in the city, further eroding the tax base. To counteract this, the city used eminent domain to build two large new auto assembly plants in the city.
As mayor, Young sought to revive the city by seeking to increase investment in the city's declining downtown. The Renaissance Center, a mixed-use office and retail complex, opened in 1977. This group of skyscrapers was an attempt to keep businesses in downtown.
Young also gave city support to other large developments to attract middle and upper-class residents back to the city. Despite the Renaissance Center and other projects, the downtown area continued to lose businesses to the suburbs. Major stores and hotels closed and many large office buildings went vacant. Young was criticized for being too focused on downtown development and not doing enough to lower the city's high crime rate and improve city services.
Long a major population center and site of worldwide automobile manufacturing, Detroit has suffered a long economic decline produced by numerous factors. Like many industrial American cities, Detroit reached its population peak in the 1950 census. The peak population was 1.8 million people. Following suburbanization, industrial restructuring, and loss of jobs (as described above), by the 2010 census, the city had less than 40 percent of that number, with just over 700,000 residents. The city has declined in population in each census since 1950.
High unemployment was compounded by middle-class flight to the suburbs, and some residents leaving the state to find work. The city was left with a higher proportion of poor in its population, reduced tax base, depressed property values, abandoned buildings, abandoned neighborhoods, high crime rates and a pronounced demographic imbalance.
1990s–2000s.
In 1993 Young retired as Detroit's longest serving mayor, deciding not to seek a sixth term. That year the city elected Dennis Archer, a former Michigan Supreme Court justice. Archer prioritized downtown development and easing tensions with Detroit's suburban neighbors. A referendum to allow casino gambling in the city passed in 1996; several temporary casino facilities opened in 1999, and permanent downtown casinos with hotels opened in 2007–08.
Campus Martius, a reconfiguration of downtown's main intersection as a new park was opened in 2004. The park has been cited as one of the best public spaces in the United States. 
The city's riverfront has been the focus of redevelopment, following successful examples of other older industrial cities. In 2001, the first portion of the International Riverfront was completed as a part of the city's 300th anniversary celebration, with miles of parks and associated landscaping completed in succeeding years. In 2011, the Port Authority Passenger Terminal opened with the river walk connecting Hart Plaza to the Renaissance Center.
Since 2006, $9 billion has been invested in downtown and surrounding neighborhoods; $5.2 billion of that in has come in 2013 and 2014. Construction activity, particularly rehabilitation of historic downtown buildings, has increased markedly. The number of vacant downtown buildings has dropped from nearly 50 to around 13. Among the most notable redevelopment projects are the Book Cadillac Hotel and the Fort Shelby Hotel; the David Broderick Tower; and the David Whitney Building. Meanwhile, work is underway or set to begin on the historic, vacant Wurlitzer Building and Strathmore Hotel.
Financial crisis and bankruptcy.
Detroit's protracted decline has resulted in severe urban decay and thousands of empty buildings around the city. Some parts of Detroit are so sparsely populated that the city has difficulty providing municipal services. The city has considered various solutions, such as demolishing abandoned homes and buildings; removing street lighting from large portions of the city; and encouraging the small population in certain areas to move to more populated locations. While some have estimated 20,000 stray dogs roam the city, studies have shown the true number to be around 1,000-3,000
Roughly half of the owners of Detroit's 305,000 properties failed to pay their 2011 tax bills, resulting in about $246 million in taxes and fees going uncollected, nearly half of which was due to Detroit; the rest of the money would have been earmarked for Wayne County, Detroit Public Schools, and the library system.
The city's financial crisis resulted in the state of Michigan taking over administrative control of its government. The state governor declared a financial emergency in March 2013, appointing Kevyn Orr as emergency manager. On July 18, 2013, Detroit became the largest U.S. city to file for bankruptcy. It was declared bankrupt by U.S. District Court on December 3, 2013, in light of the city's $18.5 billion debt and its inability to fully repay its thousands of creditors.
On August 11, 2014, historic flooding occurred after a storm brought about five inches of rain in a period of several hours.
A new arena for the Detroit Red Wings, with attached residential, hotel, and retail use is under construction set to open in fall 2017. The plans for the project call for mixed-use residential on the blocks surrounding the arena and the renovation the vacant 14-story Eddystone Hotel.
Geography.
Topography.
According to the U.S. Census Bureau, the city has a total area of , of which is land and is water. Detroit is the principal city in Metro Detroit and Southeast Michigan situated in the Midwestern United States and the Great Lakes region.
The Detroit River International Wildlife Refuge is the only international wildlife preserve in North America, uniquely located in the heart of a major metropolitan area. The Refuge includes islands, coastal wetlands, marshes, shoals, and waterfront lands along of the Detroit River and Western Lake Erie shoreline.
The city slopes gently from the northwest to southeast on a till plain composed largely of glacial and lake clay. The most notable topographical feature in the city is the Detroit Moraine, a broad clay ridge on which the older portions of Detroit and Windsor sit atop, rising approximately above the river at its highest point. The highest elevation in the city is located directly north of Gorham Playground on the northwest side approximately three blocks south of 8 Mile Road, at a height of . Detroit's lowest elevation is along the Detroit River, at a surface height of .
Belle Isle Park is a island park in the Detroit River, between Detroit and Windsor, Ontario. It is connected to the mainland by the MacArthur Bridge in Detroit. Belle Isle Park contains such attractions as the James Scott Memorial Fountain, the Belle Isle Conservatory, the Detroit Yacht Club on an adjacent island, a half-mile (800 m) beach, a golf course, a nature center, monuments, and gardens. The city skyline may be viewed from the island.
Three road systems cross the city: the original French template, with avenues radiating from the waterfront; and true north–south roads based on the Northwest Ordinance township system. The city is north of Windsor, Ontario. Detroit is the only major city along the U.S.–Canadian border in which one travels south in order to cross into Canada.
Detroit has four border crossings: the Ambassador Bridge and the Detroit–Windsor Tunnel provide motor vehicle thoroughfares, with the Michigan Central Railway Tunnel providing railroad access to and from Canada. The fourth border crossing is the Detroit–Windsor Truck Ferry, located near the Windsor Salt Mine and Zug Island. Near Zug Island, the southwest part of the city was developed over a salt mine that is below the surface. The Detroit Salt Company mine has over of roads within.
Climate.
Detroit and the rest of southeastern Michigan have a humid continental climate (Köppen "Dfa") which is influenced by the Great Lakes; the city and close-in suburbs are part of USDA Hardiness zone 6b, with farther-out northern and western suburbs generally falling in zone 6a. Winters are cold, with moderate snowfall and temperatures not rising above freezing on an average 44 days annually, while dropping to or below on an average 4.4 days a year; summers are warm to hot with temperatures exceeding on 12 days. The warm season runs from May to September. The monthly daily mean temperature ranges from in January to in July. Official temperature extremes range from on July 24, 1934 down to on January 21, 1984; the record low maximum is on January 19, 1994, while, conversely the record high minimum is on August 1, 2006, the most recent of five occurrences. A decade or two may pass between readings of or higher, which last occurred July 17, 2012. The average window for freezing temperatures is October 20 thru April 22, allowing a growing season of 180 days.
Precipitation is moderate and somewhat evenly distributed throughout the year, although the warmer months such as May and June average more, averaging annually, but historically ranging from in 1963 to in 2011. Snowfall, which typically falls in measurable amounts between November 15 through April 4 (occasionally in October and very rarely in May), averages per season, although historically ranging from in 1881−82 to in 2013−14. A thick snowpack is not often seen, with an average of only 27.5 days with or more of snow cover. Thunderstorms are frequent in the Detroit area. These usually occur during spring and summer.
Cityscape.
Architecture.
Seen in panorama, Detroit's waterfront shows a variety of architectural styles. The post modern Neo-Gothic spires of the One Detroit Center (1993) were designed to blend with the city's Art Deco skyscrapers. Together with the Renaissance Center, they form a distinctive and recognizable skyline. Examples of the Art Deco style include the Guardian Building and Penobscot Building downtown, as well as the Fisher Building and Cadillac Place in the New Center area near Wayne State University. Among the city's prominent structures are United States' largest Fox Theatre, the Detroit Opera House, and the Detroit Institute of Arts.
While the Downtown and New Center areas contain high-rise buildings, the majority of the surrounding city consists of low-rise structures and single-family homes. Outside of the city's core, residential high-rises are found in upper-class neighborhoods such as the East Riverfront extending toward Grosse Pointe and the Palmer Park neighborhood just west of Woodward. The University Commons-Palmer Park district in northwest Detroit, near the University of Detroit Mercy and Marygrove College, anchors historic neighborhoods including Palmer Woods, Sherwood Forest, and the University District.
The National Register of Historic Places lists several area neighborhoods and districts. Neighborhoods constructed prior to World War II feature the architecture of the times, with wood-frame and brick houses in the working-class neighborhoods, larger brick homes in middle-class neighborhoods, and ornate mansions in upper-class neighborhoods such as Brush Park, Woodbridge, Indian Village, Palmer Woods, Boston-Edison, and others.
Some of the oldest neighborhoods are along the Woodward and East Jefferson corridors. Some newer residential construction may also be found along the Woodward corridor, the far west, and northeast. Some of the oldest extant neighborhoods include West Canfield and Brush Park, which have both seen multimillion-dollar restorations and construction of new homes and condominiums.
Many of the city's architecturally significant buildings have been listed on the National Register of Historic Places; the city has one of United States' largest surviving collections of late 19th- and early 20th-century buildings. Architecturally significant churches and cathedrals in the city include St. Joseph's, Old St. Mary's, the Sweetest Heart of Mary, and the Cathedral of the Most Blessed Sacrament.
The city has substantial activity in urban design, historic preservation, and architecture. A number of downtown redevelopment projects—of which Campus Martius Park is one of the most notable—have revitalized parts of the city. Grand Circus Park stands near the city's theater district, Ford Field, home of the Detroit Lions, and Comerica Park, home of the Detroit Tigers. Other projects include the demolition of the Ford Auditorium off of Jefferson St.
The Detroit International Riverfront includes a partially completed three-and-one-half mile riverfront promenade with a combination of parks, residential buildings, and commercial areas. It extends from Hart Plaza to the MacArthur Bridge accessing Belle Isle Park (the largest island park in a U.S. city). The riverfront includes Tri-Centennial State Park and Harbor, Michigan's first urban state park. The second phase is a two-mile (3 km) extension from Hart Plaza to the Ambassador Bridge for a total of five miles (8 km) of parkway from bridge to bridge. Civic planners envision that the pedestrian parks will stimulate residential redevelopment of riverfront properties condemned under eminent domain.
Other major parks include River Rouge (in the southwest side), the largest park in Detroit; Palmer (north of Highland Park) and Chene Park (on the east river downtown).
Neighborhoods.
Detroit has a variety of neighborhood types. The revitalized Downtown, Midtown, and New Center areas feature many historic buildings and are high density, while further out, particularly in the northeast and on the fringes, high vacancy levels are problematic, for which a number of solutions have been proposed. In 2007, Downtown Detroit was recognized as a best city neighborhood in which to retire among the United States' largest metro areas by CNN Money Magazine editors.
Lafayette Park is a revitalized neighborhood on the city's east side, part of the Ludwig Mies van der Rohe residential district. The development was originally called the Gratiot Park. Planned by Mies van der Rohe, Ludwig Hilberseimer and Alfred Caldwell it includes a landscaped, park with no through traffic, in which these and other low-rise apartment buildings are situated. Immigrants have contributed to the city's neighborhood revitalization, especially in southwest Detroit. Southwest Detroit has experienced a thriving economy in recent years, as evidenced by new housing, increased business openings and the recently opened Mexicantown International Welcome Center.
The city has numerous neighborhoods consisting of vacant properties resulting in low inhabited density in those areas, stretching city services and infrastructure. These neighborhoods are concentrated in the northeast and on the city's fringes. A 2009 parcel survey found about a quarter of residential lots in the city to be undeveloped or vacant, and about 10% of the city's housing to be unoccupied. The survey also reported that most (86%) of the city's homes are in good condition with a minority (9%) in fair condition needing only minor repairs.
To deal with vacancy issues, the city has begun demolishing the derelict houses, razing 3,000 of the total 10,000 in 2010, but the resulting low density creates a strain on the city's infrastructure. To remedy this, a number of solutions have been proposed including resident relocation from more sparsely populated neighborhoods and converting unused space to urban agricultural use, including Hantz Woodlands, though the city expects to be in the planning stages for up to another two years.
Public funding and private investment have also been made with promises to rehabilitate neighborhoods. In April 2008, the city announced a $300-million stimulus plan to create jobs and revitalize neighborhoods, financed by city bonds and paid for by earmarking about 15% of the wagering tax. The city's working plans for neighborhood revitalizations include 7-Mile/Livernois, Brightmoor, East English Village, Grand River/Greenfield, North End, and Osborn. Private organizations have pledged substantial funding to the efforts. Additionally, the city has cleared a section of land for large-scale neighborhood construction, which the city is calling the "Far Eastside Plan". In 2011, Mayor Bing announced a plan to categorize neighborhoods by their needs and prioritize the most needed services for those neighborhoods.
Demographics.
In the 2010 United States Census, the city had 713,777 residents, ranking it the 18th most populous city in the United States.
Of the large shrinking cities of the United States, Detroit has had the most dramatic decline in population of the past 60 years (down 1,135,971) and the second largest percentage decline (down 61.4%, second only to St. Louis, Missouri's 62.7%). While the decline in Detroit's population has been ongoing since 1950, the most dramatic period was the significant 25% decline between the 2000 and 2010 Census.
The population collapse has resulted in large numbers of abandoned homes and commercial buildings, and areas of the city hit hard by urban decay.
Detroit's 713,777 residents represent 269,445 households, and 162,924 families residing in the city. The population density was 5,144.3 people per square mile (1,895/km²). There were 349,170 housing units at an average density of 2,516.5 units per square mile (971.6/km²). Housing density has declined. The city has demolished thousands of Detroit's abandoned houses, planting some areas and in others allowing the growth of urban prairie.
Of the 269,445 households, 34.4% had children under the age of 18 living with them, 21.5% were married couples living together, 31.4% had a female householder with no husband present, 39.5% were non-families, 34.0% were made up of individuals, and 3.9% had someone living alone who is 65 years of age or older. Average household size was 2.59, and average family size was 3.36.
There is a wide distribution of age in the city, with 31.1% under the age of 18, 9.7% from 18 to 24, 29.5% from 25 to 44, 19.3% from 45 to 64, and 10.4% 65 years of age or older. The median age was 31 years. For every 100 females there were 89.1 males. For every 100 females age 18 and over, there were 83.5 males.
According to a 2014 study, 67% of the population of the city identified themselves as Christians, with 49% professing attendance Protestant churches, and 16% professing Roman Catholic beliefs, while 24% claim no religious affiliation. Other religions collectively make up about 8% of the population
Income and employment.
The loss of industrial and working-class jobs in the city has resulted in high rates of poverty and associated problems. From 2000 to 2009, the city's estimated median household income fell from $29,526 to $26,098. As of 2010 the mean income of Detroit is below the overall U.S. average by several thousand dollars. Of every three Detroit residents, one lives in poverty. Luke Bergmann, author of "Getting Ghost: Two Young Lives and the Struggle for the Soul of an American City", said in 2010, "Detroit is now one of the poorest big cities in the country."
In the 2010 American Community Survey, median household income in the city was $25,787, and the median income for a family was $31,011. The per capita income for the city was $14,118. 32.3% of families had income at or below the federally defined poverty level. Out of the total population, 53.6% of those under the age of 18 and 19.8% of those 65 and older had income at or below the federally defined poverty line.
Oakland County in Metro Detroit, once rated amongst the wealthiest US counties per household, is no longer shown in the top 25 listing of "Forbes" magazine. But internal county statistical methods – based on measuring per capita income for counties with more than one million residents – show that Oakland is still within the top 12, slipping from the 4th-most affluent such county in the U.S. in 2004 to 11th-most affluent in 2009. Detroit dominates Wayne County, which has an average household income of about $38,000, compared to Oakland County's $62,000.
Race and ethnicity.
The city's population increased more than sixfold during the first half of the 20th century, fed largely by an influx of European, Middle Eastern (Lebanese, Assyrian/Chaldean), and Southern migrants to work in the burgeoning automobile industry. In 1940, Whites were 90.4% of the city's population. Since 1950 the city has seen a major shift in its population to the suburbs. In 1910, fewer than 6,000 blacks called the city home; in 1930 more than 120,000 blacks lived in Detroit. The thousands of African Americans who came to Detroit were part of the Great Migration of the 20th century.
Detroit remains one of the most racially segregated cities in the United States. From the 1940s to the 1970s a second wave of Blacks moved to Detroit to escape Jim Crow laws in the south and find jobs. However, they soon found themselves excluded from white areas of the city—through violence, laws, and economic discrimination (e.g., redlining). White residents attacked black homes: breaking windows, starting fires, and exploding bombs. The pattern of segregation was later magnified by white migration to the suburbs. One of the implications of racial segregation, which correlates with class segregation, may be overall worse health for some populations.
While Blacks/African-Americans comprised only 13 percent of Michigan's population in 2010, they made up nearly 82 percent of Detroit's population. The next largest population groups were Whites, at 10 percent, and Hispanics, at 6 percent. According to the 2010 Census, segregation in Detroit has decreased in absolute and in relative terms. In the first decade of the 21st century, about two-thirds of the total black population in metropolitan area resided within the city limits of Detroit. The number of integrated neighborhoods has increased from 100 in 2000 to 204 in 2010. The city has also moved down the ranking, from number one most segregated to number four. A 2011 op-ed in "The New York Times" attributed the decreased segregation rating to the overall exodus from the city, cautioning that these areas may soon become more segregated. This pattern already happened in the 1970s, when apparent integration was actually a precursor to white flight and resegregation. Over a 60-year period, white flight occurred in the city. According to an estimate of the Michigan Metropolitan Information Center, from 2008 to 2009 the percentage of non-Hispanic White residents increased from 8.4% to 13.3%. Some empty nesters and many younger White people moved into the city while many African Americans moved to the suburbs.
Detroit has a Mexican-American population. In the early 20th century thousands of Mexicans came to Detroit to work in agricultural, automotive, and steel jobs. During the Mexican Repatriation of the 1930s many Mexicans in Detroit were willingly repatriated or forced to repatriate. By the 1940s the Mexican community began to settle what is now Mexicantown. The population significantly increased in the 1990s due to immigration from Jalisco. In 2010 Detroit had 48,679 Hispanics, including 36,452 Mexicans. The number of Hispanics was a 70% increase from the number in 1990.
After World War II, many people from Appalachia settled in Detroit. Appalachians formed communities and their children acquired southern accents. Many Lithuanians settled in Detroit during the World War II era, especially on the city's Southwest side in the West Vernor area, where the renovated Lithuanian Hall reopened in 2006.
In 2001, 103,000 Jews, or about 1.9% of the population, were living in the Detroit area, in both Detroit and Ann Arbor.
Asians and Asian Americans.
As of 2002, of all of the municipalities in the Wayne County-Oakland County-Macomb County area, Detroit had the second largest Asian population. As of that year Detroit's percentage of Asians was 1%, far lower than the 13.3% of Troy. By 2000 Troy had the largest Asian American population in the tricounty area, surpassing Detroit.
As of 2002 there are four areas in Detroit with significant Asian and Asian American populations. Northeast Detroit has population of Hmong with a smaller group of Lao people. A portion of Detroit next to eastern Hamtramck includes Bangladeshi Americans, Indian Americans, and Pakistani Americans; nearly all of the Bangladeshi population in Detroit lives in that area. Many of those residents own small businesses or work in blue collar jobs, and the population in that area is mostly Muslim. The area north of Downtown Detroit; including the region around the Henry Ford Hospital, the Detroit Medical Center, and Wayne State University; has transient Asian national origin residents who are university students or hospital workers. Few of them have permanent residency after schooling ends. They are mostly Chinese and Indian but the population also includes Filipinos, Koreans, and Pakistanis. In Southwest Detroit and western Detroit there are smaller, scattered Asian communities including an area in the westside adjacent to Dearborn and Redford Township that has a mostly Indian Asian population, and a community of Vietnamese and Laotians in Southwest Detroit.
As of 2006 the city has one of the U.S.'s largest concentrations of Hmong Americans. In 2006, the city had about 4,000 Hmong and other Asian immigrant families. Most Hmong live east of Coleman Young Airport near Osborn High School. Hmong immigrant families generally have lower incomes than those of suburban Asian families.
Economy.
Several major corporations are based in the city, including three Fortune 500 companies. The most heavily represented sectors are manufacturing (particularly automotive), finance, technology, and health care. The most significant companies based in Detroit include: General Motors, Quicken Loans, Ally Financial, Compuware, Shinola, American Axle, Little Caesars, DTE Energy, Lowe Campbell Ewald, Blue Cross Blue Shield of Michigan, and Rossetti Architects.
About 80,500 people work in downtown Detroit, comprising one-fifth of the city's employment base. Aside from the numerous Detroit-based companies listed above, downtown contains large offices for Comerica, Chrysler, HP Enterprise, Deloitte, PricewaterhouseCoopers, KPMG, and Ernst & Young. Ford Motor Company is located in the adjacent city of Dearborn.
Thousands more employees work in Midtown, north of the central business district. Midtown's anchors are the city's largest single employer Detroit Medical Center, Wayne State University, and the Henry Ford Health System in New Center. Midtown is also home to watchmaker Shinola and an array of small and/or startup companies. New Center bases TechTown, a research and business incubator hub that’s part of the WSU system. Like downtown and Corktown, Midtown also has a fast-growing retailing and restaurant scene.
A number of the city's downtown employers are relatively new, as there has been a marked trend of companies moving from satellite suburbs around Metropolitan Detroit into the downtown core. Compuware completed its world headquarters in downtown in 2003. OnStar, Blue Cross Blue Shield, and HP Enterprise Services are located at the Renaissance Center. PricewaterhouseCoopers Plaza offices are adjacent to Ford Field, and Ernst & Young completed its office building at One Kennedy Square in 2006. Perhaps most prominently, in 2010, Quicken Loans, one of the largest mortgage lenders, relocated its world headquarters and 4,000 employees to downtown Detroit, consolidating its suburban offices. In July 2012, the U.S. Patent and Trademark Office opened its Elijah J. McCoy Satellite Office in the Rivertown/Warehouse District as its first location outside Washington, D.C.'s metropolitan area.
In April 2014, the Department of Labor reported the city's unemployment rate at 14.5%.
The city of Detroit and other private-public partnerships have attempted to catalyze the region's growth by facilitating the building and historical rehabilitation of residential high-rises in the downtown, creating a zone that offers many business tax incentives, creating recreational spaces such as the Detroit RiverWalk, Campus Martius Park, Dequindre Cut Greenway, and Green Alleys in Midtown. The city itself has cleared sections of land while retaining a number of historically significant vacant buildings in order to spur redevelopment; though it has struggled with finances, the city issued bonds in 2008 to provide funding for ongoing work to demolish blighted properties. Two years earlier, downtown reported $1.3 billion in restorations and new developments which increased the number of construction jobs in the city. In the decade prior to 2006, downtown gained more than $15 billion in new investment from private and public sectors.
Despite the city's recent financial issues, many developers remain unfazed by Detroit's problems. Midtown is one of the most successful areas within Detroit to have a residential occupancy rate of 96%. Numerous developments have been recently completely or are in various stages of construction. These include the $82 million reconstruction of downtown's David Whitney Building (now an Aloft Hotel and luxury residences), the Woodward Garden Block Development in Midtown, the residential conversion of the David Broderick Tower in downtown, the rehabilitation of the Book Cadillac Hotel (now a Westin and luxury condos) and Fort Shelby Hotel (now Doubletree) also in downtown, and various smaller projects.
Downtown's population of young professionals is growing and retail is expanding. A study in 2007 found out that Downtown's new residents are predominantly young professionals (57% are ages 25 to 34, 45% have bachelor's degrees, and 34% have a master's or professional degree), a trend which has hastened over the last decade. John Varvatos is set to open a downtown store in 2015, and Restoration Hardware is rumored to be opening a store nearby.
On July 25, 2013, Meijer, a midwestern retail chain, opened its first supercenter store in Detroit,; this was a 20 million dollar, 190,000-square-foot store in the northern portion of the city and it also is the centerpiece of a new 72 million dollar shopping center named Gateway Marketplace. On June 11, 2015, Meijer opened its second supercenter store in the city.
On May 21, 2014, JPMorgan Chase announced that it was injecting $100 million over five years into Detroit's economy, providing development funding for a variety of projects that would increase employment. It is the largest commitment made to any one city by the nation's biggest bank. Of the $100 million, $50 million will go toward development projects, $25 million will go toward city blight removal, $12.5 million will go for job training, $7 million will go for small businesses in the city, and $5.5 million will go toward the M-1 light rail project. On May 19, 2015, JPMorgan Chase announced that it has invested $32 million for two redevelopment projects in the city's Capitol Park district, the Capitol Park Lofts (the former Capitol Park Building) and the Detroit Savings Bank building at 1212 Griswold. Those investments are separate from Chase's five-year, $100-million commitment.
Culture and contemporary life.
In the central portions of Detroit, the population of young professionals, artists, and other transplants is growing and retail is expanding. This dynamic is luring additional new residents, and former residents returning from other cities, to the city's Downtown along with the revitalized Midtown and New Center areas.
A desire to be closer to the urban scene has also attracted some young professionals to reside in inner ring suburbs such as Grosse Pointe and Royal Oak, Detroit. Detroit's proximity to Windsor, Ontario, provides for views and nightlife, along with Ontario's minimum drinking age of 19. A 2011 study by Walk Score recognized Detroit for its above average walkability among large U.S. cities. About two-thirds of suburban residents occasionally dine and attend cultural events or take in professional games in the city of Detroit.
Nicknames.
Known as the world's automotive center, "Detroit" is a metonym for that industry. Detroit's auto industry, some of which was converted to wartime defense production, was an important element of the American "Arsenal of Democracy" supporting the Allied powers during World War II. It is an important source of popular music legacies celebrated by the city's two familiar nicknames, the "Motor City" and "Motown". Other nicknames arose in the 20th century, including "City of Champions," beginning in the 1930s for its successes in individual and team sport; "The D"; "Hockeytown" (a trademark owned by the city's NHL club, the Red Wings); "Rock City" (after the Kiss song "Detroit Rock City"); and "The 313" (its telephone area code).
Music.
Live music has been a prominent feature of Detroit's nightlife since the late 1940s, bringing the city recognition under the nickname 'Motown'. The metropolitan area has many nationally prominent live music venues. Concerts hosted by Live Nation perform throughout the Detroit area. Large concerts are held at DTE Energy Music Theatre and The Palace of Auburn Hills. The city's theatre venue circuit is the United States' second largest and hosts Broadway performances.
The city of Detroit has a rich musical heritage and has contributed to a number of different genres over the decades leading into the new millennium. Important music events in the city include: the Detroit International Jazz Festival, the Detroit Electronic Music Festival, the Motor City Music Conference (MC2), the Urban Organic Music Conference, the Concert of Colors, and the hip-hop Summer Jamz festival.
In the 1940s, Detroit blues artist John Lee Hooker became a long-term resident in the city's southwest Delray neighborhood. Hooker, among other important blues musicians migrated from his home in Mississippi bringing the Delta blues to northern cities like Detroit. Hooker recorded for Fortune Records, the biggest pre-Motown blues/soul label. During the 1950s, the city became a center for jazz, with stars performing in the Black Bottom neighborhood. Prominent emerging Jazz musicians of the 1960s included: trumpet player Donald Byrd who attended Cass Tech and performed with Art Blakey and the Jazz Messengers early in his career and Saxophonist Pepper Adams who enjoyed a solo career and accompanied Byrd on several albums. The Graystone International Jazz Museum documents jazz in Detroit.
Other, prominent Motor City R&B stars in the 1950s and early 1960s was Nolan Strong, Andre Williams and Nathaniel Mayer – who all scored local and national hits on the Fortune Records label. According to Smokey Robinson, Strong was a primary influence on his voice as a teenager. The Fortune label was a family-operated label located on Third Avenue in Detroit, and was owned by the husband and wife team of Jack Brown and Devora Brown. Fortune, which also released country, gospel and rockabilly LPs and 45s, laid the groundwork for Motown, which became Detroit's most legendary record label.
Berry Gordy, Jr. founded Motown Records which rose to prominence during the 1960s and early 1970s with acts such as Stevie Wonder, The Temptations, The Four Tops, Smokey Robinson & The Miracles, Diana Ross & The Supremes, the Jackson 5, Martha and the Vandellas, The Spinners, Gladys Knight & the Pips, The Marvelettes, The Elgins, The Monitors, The Velvelettes and Marvin Gaye. Artists were backed by in-house vocalists The Andantes and The Funk Brothers, the Motown house band that was featured in Paul Justman's 2002 documentary film Standing in the Shadows of Motown, based on Allan Slutsky's book of the same name.
The Motown Sound played an important role in the crossover appeal with popular music, since it was the first African American owned record label to primarily feature African-American artists. Gordy moved Motown to Los Angeles in 1972 to pursue film production, but the company has since returned to Detroit. Aretha Franklin, another Detroit R&B star, carried the Motown Sound; however, she did not record with Berry's Motown Label.
Local artists and bands rose to prominence in the 1960s and 70s including: the MC5, The Stooges, Bob Seger, Amboy Dukes featuring Ted Nugent, Mitch Ryder and The Detroit Wheels, Rare Earth, Alice Cooper, and Suzi Quatro. The group Kiss emphasized the city's connection with rock in the song "Detroit Rock City" and the movie produced in 1999. In the 1980s, Detroit was an important center of the hardcore punk rock underground with many nationally known bands coming out of the city and its suburbs, such as The Necros, The Meatmen, and Negative Approach.
In the 1990s and the new millennium, the city has produced a number of influential hip hop artists, including Eminem, the hip-hop artist with the highest cumulative sales, hip-hop producer J Dilla, rapper and producer Esham and hip hop duo Insane Clown Posse. The city is also home to rappers Big Sean and Danny Brown. The band Sponge toured and produced music, with artists such as Kid Rock and Uncle Kracker. The city also has an active garage rock genre that has generated national attention with acts such as: The White Stripes, The Von Bondies, The Detroit Cobras, The Dirtbombs, Electric Six, and The Hard Lessons.
Detroit is cited as the birthplace of techno music in the early 1980s. The city also lends its name to an early and pioneering genre of electronic dance music, "Detroit techno". Featuring science fiction imagery and robotic themes, its futuristic style was greatly influenced by the geography of Detroit's urban decline and its industrial past. Prominent Detroit techno artists include Juan Atkins, Derrick May, and Kevin Saunderson. The Detroit Electronic Music Festival, now known as "Movement", occurs annually in late May on Memorial Day Weekend, and takes place in Hart Plaza. In the early years (2000-2002), this was a landmark event, boasting over a million estimated attendees annually, coming from all over the world to celebrate Techno music in the city of its birth.
Entertainment and performing arts.
Major theaters in Detroit include the Fox Theatre (5,174 seats), Music Hall (1,770 seats), the Gem Theatre (451 seats), Masonic Temple Theatre (4,404 seats), the Detroit Opera House (2,765 seats), the Fisher Theatre (2,089 seats), The Fillmore Detroit (2,200 seats), Saint Andrew's Hall, the Majestic Theater, and Orchestra Hall (2,286 seats) which hosts the renowned Detroit Symphony Orchestra. The Nederlander Organization, the largest controller of Broadway productions in New York City, originated with the purchase of the Detroit Opera House in 1922 by the Nederlander family.
Motown Motion Picture Studios with produces movies in Detroit and the surrounding area based at the Pontiac Centerpoint Business Campus for a film industry expected to employ over 4,000 people in the metro area.
Tourism.
Many of the area's prominent museums are located in the historic cultural center neighborhood around Wayne State University and the College for Creative Studies. These museums include the Detroit Institute of Arts, the Detroit Historical Museum, Charles H. Wright Museum of African American History, the Detroit Science Center, as well as the main branch of the Detroit Public Library. Other cultural highlights include Motown Historical Museum, the Ford Piquette Avenue Plant museum (birthplace of the Ford Model T and the world's oldest car factory building open to the public), the Pewabic Pottery studio and school, the Tuskegee Airmen Museum, Fort Wayne, the Dossin Great Lakes Museum, the Museum of Contemporary Art Detroit (MOCAD), the Contemporary Art Institute of Detroit (CAID), and the Belle Isle Conservatory.
In 2010, the G.R. N'Namdi Gallery opened in a complex in Midtown. Important history of America and the Detroit area are exhibited at The Henry Ford in Dearborn, the United States' largest indoor-outdoor museum complex. The Detroit Historical Society provides information about tours of area churches, skyscrapers, and mansions. Inside Detroit, meanwhile, hosts tours, educational programming, and a downtown welcome center. Other sites of interest are the Detroit Zoo in Royal Oak, the Cranbrook Art Museum in Bloomfield Hills, the Anna Scripps Whitcomb Conservatory on Belle Isle, and Walter P. Chrysler Museum in Auburn Hills.
The city's Greektown and three downtown casino resort hotels serve as part of an entertainment hub. The Eastern Market farmer's distribution center is the largest open-air flowerbed market in the United States and has more than 150 foods and specialty businesses. On Saturdays, about 45,000 people shop the city's historic Eastern Market. The Midtown and the New Center area are centered on Wayne State University and Henry Ford Hospital. Midtown has about 50,000 residents and attracts millions of visitors each year to its museums and cultural centers; for example, the Detroit Festival of the Arts in Midtown draws about 350,000 people.
Annual summer events include the Electronic Music Festival, International Jazz Festival, the Woodward Dream Cruise, the African World Festival, the country music Hoedown, Noel Night, and Dally in the Alley. Within downtown, Campus Martius Park hosts large events, including the annual Motown Winter Blast. As the world's traditional automotive center, the city hosts the North American International Auto Show. Held since 1924, America's Thanksgiving Parade is one of the nation's largest. River Days, a five-day summer festival on the International Riverfront lead up to the Windsor–Detroit International Freedom Festival fireworks, which draw super sized-crowds ranging from hundreds of thousands to over three million people.
An important civic sculpture in Detroit is "The Spirit of Detroit" by Marshall Fredericks at the Coleman Young Municipal Center. The image is often used as a symbol of Detroit and the statue itself is occasionally dressed in sports jerseys to celebrate when a Detroit team is doing well. A memorial to Joe Louis at the intersection of Jefferson and Woodward Avenues was dedicated on October 16, 1986. The sculpture, commissioned by "Sports Illustrated" and executed by Robert Graham, is a long arm with a fisted hand suspended by a pyramidal framework.
Artist Tyree Guyton created the controversial street art exhibit known as the Heidelberg Project in 1986, using found objects including cars, clothing and shoes found in the neighborhood near and on Heidelberg Street on the near East Side of Detroit. Guyton continues to work with neighborhood residents and tourists in constantly evolving the neighborhood-wide art installation.
Sports.
Detroit is one of 12 American metropolitan areas that are home to professional teams representing the four major sports in North America. All these teams but one play within the city of Detroit itself (the NBA's Detroit Pistons play in suburban Auburn Hills at The Palace of Auburn Hills). There are three active major sports venues within the city: Comerica Park (home of the Major League Baseball team Detroit Tigers), Ford Field (home of the NFL's Detroit Lions), and Joe Louis Arena (home of the NHL's Detroit Red Wings). A 1996 marketing campaign promoted the nickname "Hockeytown".
The Detroit Tigers have won four World Series titles. The Detroit Red Wings have won 11 Stanley Cups (the most by an American NHL franchise). The Detroit Pistons have won three NBA titles. With the Pistons' first of three NBA titles in 1989, the city of Detroit has won titles in all four of the major professional sports leagues. Two new downtown stadiums for the Detroit Tigers and Detroit Lions opened in 2000 and 2002, respectively, returning the Lions to the city proper.
In college sports, Detroit's central location within the Mid-American Conference has made it a frequent site for the league's championship events. While the MAC Basketball Tournament moved permanently to Cleveland starting in 2000, the MAC Football Championship Game has been played at Ford Field in Detroit since 2004, and annually attracts 25,000 to 30,000 fans. The University of Detroit Mercy has a NCAA Division I program, and Wayne State University has both NCAA Division I and II programs. The NCAA football Little Caesars Pizza Bowl is held at Ford Field each December.
The local soccer team is called the Detroit City Football Club and was founded in 2012. The team plays in the National Premier Soccer League, and its nickname is "Le Rouge".
The city hosted the 2005 MLB All-Star Game, 2006 Super Bowl XL, 2006 and 2012 World Series, WrestleMania 23 in 2007, and the NCAA Final Four in April 2009. 
The city hosted the Detroit Indy Grand Prix on Belle Isle Park from 1989 to 2001, 2007 to 2008, and 2012 and beyond. In 2007, open-wheel racing returned to Belle Isle with both Indy Racing League and American Le Mans Series Racing.
In the years following the mid-1930s, Detroit was referred to as the "City of Champions" after the Tigers, Lions, and Red Wings captured all three major professional sports championships in a seven-month period of time (the Tigers won the World Series in October 1935; the Lions won the NFL championship in December 1935; the Red Wings won the Stanley Cup in April 1936). In 1932, Eddie "The Midnight Express" Tolan from Detroit won the 100- and 200-meter races and two gold medals at the 1932 Summer Olympics. Joe Louis won the heavyweight championship of the world in 1937.
Detroit has made the most bids to host the Summer Olympics without ever being awarded the games: seven unsuccessful bids for the 1944, 1952, 1956, 1960, 1964, 1968 and 1972 games.
Law and government.
The city is governed pursuant to the "Home Rule Charter of the City of Detroit". The city government is run by a mayor and a nine-member city council and clerk elected on an at-large nonpartisan ballot. Since voters approved the city's charter in 1974, Detroit has had a "strong mayoral" system, with the mayor approving departmental appointments. The council approves budgets but the mayor is not obligated to adhere to any earmarking. City ordinances and substantially large contracts must be approved by the council. The "Detroit City Code" is the codification of Detroit's local ordinances.
The city clerk supervises elections and is formally charged with the maintenance of municipal records. Municipal elections for mayor, city council and city clerk are held at four-year intervals, in the year after presidential elections. Following a November 2009 referendum, seven council members will be elected from districts beginning in 2013 while two will continue to be elected at-large.
Detroit's courts are state-administered and elections are nonpartisan. The Probate Court for Wayne County is located in the Coleman A. Young Municipal Center in downtown Detroit. The Circuit Court is located across Gratiot Ave. in the Frank Murphy Hall of Justice, in downtown Detroit. The city is home to the Thirty-Sixth District Court, as well as the First District of the Michigan Court of Appeals and the United States District Court for the Eastern District of Michigan. The city provides law enforcement through the Detroit Police Department and emergency services through the Detroit Fire Department.
Crime.
Detroit has struggled with high crime for decades. Detroit held the title of murder capital between 1985-1987 with a murder rate around 58 per 100,000. Crime has since decreased and, in 2014, the murder rate was 43.4 per 100,000, lower than in St Louis, Missouri. Although the murder rate increased by 6% during the first half of 2015, it was surpassed by St Louis and Baltimore which saw much greater spikes in violence. At year-end 2015, Detroit had 295 criminal homicides, down slightly from 299 in 2014.
Nearly two-thirds of all murders in Michigan in 2011 occurred in Detroit. Although the rate of violent crime dropped 11 percent in 2008, violent crime in Detroit has not declined as much as the national average from 2007 to 2011. The violent crime rate is one of the highest in the United States. Neighborhoodscout.com reported a crime rate of 62.18 per 1,000 residents for property crimes, and 16.73 per 1,000 for violent crimes (compared to national figures of 32 per 1,000 for property crimes and 5 per 1,000 for violent crime in 2008).
The city's downtown typically has lower crime than national and state averages. According to a 2007 analysis, Detroit officials note that about 65 to 70 percent of homicides in the city were drug related, with the rate of unsolved murders roughly 70%.
Areas of the city closer to the Detroit River are also patrolled by the United States Border Patrol.
In 2012, crime in the city was among the reasons for more expensive car insurance.
Politics.
Beginning with its incorporation in 1802, Detroit has had a total of 74 mayors. Detroit's last mayor from the Republican Party was Louis Miriani, who served from 1957 to 1962. In 1973, the city elected its first black mayor, Coleman Young. Despite development efforts, his combative style during his five terms in office was not well received by many suburban residents. Mayor Dennis Archer, a former Michigan Supreme Court Justice, refocused the city's attention on redevelopment with a plan to permit three casinos downtown. By 2008, three major casino resort hotels established operations in the city.
In 2000, the City requested an investigation by the United States Justice Department into the Detroit Police Department which was concluded in 2003 over allegations regarding its use of force and civil rights violations. The city proceeded with a major reorganization of the Detroit Police Department.
Public finances.
In March 2013, Governor Rick Snyder declared a financial emergency in the city, stating that the city has a $327 million budget deficit and faces more than $14 billion in long-term debt. It has been making ends meet on a month-to-month basis with the help of bond money held in a state escrow account and has instituted mandatory unpaid days off for many city workers. Those troubles, along with underfunded city services, such as police and fire departments, and ineffective turnaround plans from Bing and the City Council led the state of Michigan to appoint an emergency manager for Detroit on March 14, 2013. On June 14, 2013 Detroit defaulted on $2.5 billion of debt by withholding $39.7 million in interest payments, while Emergency Manager Kevyn Orr met with bondholders and other creditors in an attempt to restructure the city's $18.5 billion debt and avoid bankruptcy. On July 18, 2013, the City of Detroit filed for Chapter 9 bankruptcy protection. It was declared bankrupt by U.S. judge Stephen Rhodes on December 3, with its $18.5 billion debt he said in accepting the city's contention that it is broke and that negotiations with its thousands of creditors were infeasible.
Education.
Colleges and universities.
Detroit is home to several institutions of higher learning including Wayne State University, a national research university with medical and law schools in the Midtown area offering hundreds of academic degrees and programs. The University of Detroit Mercy, located in Northwest Detroit in the University District, is a prominent Roman Catholic co-educational university affiliated with the Society of Jesus (the Jesuits) and the Sisters of Mercy. The University of Detroit Mercy offers more than a hundred academic degrees and programs of study including business, dentistry, law, engineering, architecture, nursing and allied health professions. The University of Detroit Mercy School of Law is located Downtown across from the Renaissance Center.
Sacred Heart Major Seminary, originally founded in 1919, is affiliated with Pontifical University of Saint Thomas Aquinas, "Angelicum" in Rome and offers pontifical degrees as well as civil undergraduate and graduate degrees. Sacred Heart Major Seminary offers a variety of academic programs for both clerical and lay students. Other institutions in the city include the College for Creative Studies, Lewis College of Business, Marygrove College and Wayne County Community College. In June 2009, the Michigan State University College of Osteopathic Medicine which is based in East Lansing opened a satellite campus located at the Detroit Medical Center. The University of Michigan was established in 1817 in Detroit and later moved to Ann Arbor in 1837. In 1959, University of Michigan–Dearborn was established in neighboring Dearborn.
Primary and secondary schools.
Public schools and charter schools.
With about 66,000 public school students (2011–12), the Detroit Public Schools (DPS) district is the largest school district in Michigan. Detroit has an additional 56,000 charter school students for a combined enrollment of about 122,000 students. As of 2009 there are about as many students in charter schools as there are in district schools.
In 1999, the Michigan Legislature removed the locally elected board of education amid allegations of mismanagement and replaced it with a reform board appointed by the mayor and governor. The elected board of education was re-established following a city referendum in 2005. The first election of the new 11-member board of education occurred on November 8, 2005.
Due to growing Detroit charter schools enrollment as well as a continued exodus of population, the city planned to close many public schools. State officials report a 68% graduation rate for Detroit's public schools adjusted for those who change schools.
Public and charter school students in the city have performed poorly on standardized tests. While Detroit public schools scored a record low on national tests, the publicly funded charter schools did even worse than the public schools.
Private schools.
Detroit is served by various private schools, as well as parochial Roman Catholic schools operated by the Archdiocese of Detroit. As of 2013 there are four Catholic grade schools and three Catholic high schools in the City of Detroit, with all of them in the city's west side. The Archdiocese of Detroit lists a number of primary and secondary schools in the metro area as Catholic education has emigrated to the suburbs. Of the three Catholic high schools in the city, two are operated by the Society of Jesus and the third is co-sponsored by the Sisters, Servants of the Immaculate Heart of Mary and the Congregation of St. Basil.
In the 1964-1965 school year there were about 110 Catholic grade schools in Detroit, Hamtramck, and Highland Park and 55 Catholic high schools in those three cities. The Catholic school population in Detroit has decreased due to the increase of charter schools, increasing tuition at Catholic schools, the small number of African-American Catholics, White Catholics moving to suburbs, and the decreased number of teaching nuns.
Media.
The "Detroit Free Press" and "The Detroit News" are the major daily newspapers, both broadsheet publications published together under a joint operating agreement called the Detroit Newspaper Partnership. Media philanthropy includes the "Detroit Free Press" high school journalism program and the Old Newsboys' Goodfellow Fund of Detroit. In March 2009, the two newspapers reduced home delivery to three days a week, print reduced newsstand issues of the papers on non-delivery days and focus resources on Internet-based news delivery. The "Metro Times", founded in 1980, is a weekly publication, covering news, arts & entertainment.
Also founded in 1935 and based in Detroit the Michigan Chronicle is one of the oldest and most respected African-American weekly newspapers in America. Covering politics, entertainment, sports and community events. The Detroit television market is the 11th largest in the United States; according to estimates that do not include audiences located in large areas of Ontario, Canada (Windsor and its surrounding area on broadcast and cable TV, as well as several other cable markets in Ontario, such as the city of Ottawa) which receive and watch Detroit television stations.
Detroit has the 11th largest radio market in the United States, though this ranking does not take into account Canadian audiences. Nearby Canadian stations such as Windsor's CKLW (whose jingles formerly proclaimed "CKLW-the Motor City") are popular in Detroit.
Hardcore Pawn, an American documentary reality television series produced for truTV, features the day-to-day operations of American Jewelry and Loan, a family-owned pawn shop on Greenfield Road.
Infrastructure.
Health systems.
Within the city of Detroit, there are over a dozen major hospitals which include the Detroit Medical Center (DMC), Henry Ford Health System, St. John Health System, and the John D. Dingell VA Medical Center. The DMC, a regional Level I trauma center, consists of Detroit Receiving Hospital and University Health Center, Children's Hospital of Michigan, Harper University Hospital, Hutzel Women's Hospital, Kresge Eye Institute, Rehabilitation Institute of Michigan, Sinai-Grace Hospital, and the Karmanos Cancer Institute. The DMC has more than 2,000 licensed beds and 3,000 affiliated physicians. It is the largest private employer in the City of Detroit. The center is staffed by physicians from the Wayne State University School of Medicine, the largest single-campus medical school in the United States, and the United States' fourth largest medical school overall.
Detroit Medical Center formally became a part of Vanguard Health Systems on December 30, 2010, as a for profit corporation. Vanguard has agreed to invest nearly $1.5 B in the Detroit Medical Center complex which will include $417 M to retire debts, at least $350 M in capital expenditures and an additional $500 M for new capital investment. Vanguard has agreed to assume all debts and pension obligations. The metro area has many other hospitals including William Beaumont Hospital, St. Joseph's, and University of Michigan Medical Center.
In 2011, Detroit Medical Center and Henry Ford Health System substantially increased investments in medical research facilities and hospitals in the city's Midtown and New Center.
In 2012, two major construction projects were begun in New Center, the Henry Ford Health System started the first phase of a $500 million, 300-acre revitalization project, with the construction of a new $30 million, 275,000-square-foot, "Medical Distribution Center" for Cardinal Health, Inc. and Wayne State University started construction on a new $93 million, 207,000-square-foot, Integrative Biosciences Center (IBio). As many as 500 researchers, and staff will work out of the IBio Center.
Transportation.
With its proximity to Canada and its facilities, ports, major highways, rail connections and international airports, Detroit is an important transportation hub. The city has three international border crossings, the Ambassador Bridge, Detroit–Windsor Tunnel and Michigan Central Railway Tunnel, linking Detroit to Windsor, Ontario. The Ambassador Bridge is the single busiest border crossing in North America, carrying 27% of the total trade between the U.S. and Canada.
On February 18, 2015, Canadian Transport Minister Lisa Raitt announced that Canada has agreed to pay the entire cost to build a $250 million U.S. Customs plaza adjacent to the planned new Detroit–Windsor bridge, now the Gordie Howe International Bridge. Canada had already planned to pay for 95 per cent of the bridge, which will cost $2.1 billion, and is expected to open in 2020. "This allows Canada and Michigan to move the project forward immediately to its next steps which include further design work and property acquisition on the U.S. side of the border," Raitt said in a statement issued after she spoke in the House of Commons.
Airports.
Detroit Metropolitan Wayne County Airport (DTW), the principal airport serving Detroit, is located in nearby Romulus. DTW is a primary hub for Delta Air Lines (following its acquisition of Northwest Airlines), and a secondary hub for Spirit Airlines.
Coleman A. Young International Airport (DET), previously called Detroit City Airport, is on Detroit's northeast side; the airport now maintains only charter service and general aviation. Willow Run Airport, in far-western Wayne County near Ypsilanti, is a general aviation and cargo airport.
Transit systems.
Mass transit in the region is provided by bus services. The Detroit Department of Transportation (DDOT) provides service to the outer edges of the city. From there, the Suburban Mobility Authority for Regional Transportation (SMART) provides service to the suburbs. Cross border service between the downtown areas of Windsor and Detroit is provided by Transit Windsor via the Tunnel Bus.
An elevated rail system known as the People Mover, completed in 1987, provides daily service around a loop downtown. The under construction (to open in 2016) M-1 Rail Line (see below) will serve as a link between the Detroit People Mover and SEMCOG Commuter Rail/Detroit Amtrak station. The SEMCOG Commuter Rail line will extend from Detroit's New Center area to The Henry Ford, Dearborn, Detroit Metropolitan Airport, Ypsilanti, and Ann Arbor when it is opened.
The Regional Transit Authority (RTA) was established by an act of the Michigan legislature in December 2012 to oversee and coordinate all existing regional mass transit operations, and to develop new transit services in the region. The M-1 Rail Line is expected to open in mid-2017, running along Woodward Avenue from downtown to the New Center area.
Amtrak provides service to Detroit, operating its "Wolverine" service between Chicago and Pontiac. The Amtrak station is located in the New Center area north of downtown. The "J. W. Westcott II", which delivers mail to lake freighters on the Detroit River, is the world's only floating post office.
Freeways.
Metro Detroit has an extensive toll-free network of freeways administered by the Michigan Department of Transportation. Four major Interstate Highways surround the city. Detroit is connected via Interstate 75 (I-75) and I-96 to Kings Highway 401 and to major Southern Ontario cities such as London, Ontario and the Greater Toronto Area. I-75 (Chrysler and Fisher freeways) is the region's main north–south route, serving Flint, Pontiac, Troy, and Detroit, before continuing south (as the Detroit–Toledo and Seaway Freeways) to serve many of the communities along the shore of Lake Erie.
I-94 (Edsel Ford Freeway) runs east–west through Detroit and serves Ann Arbor to the west (where it continues to Chicago) and Port Huron to the northeast. The stretch of the current I-94 freeway from Ypsilanti to Detroit was one of America's earlier limited-access highways. Henry Ford built it to link the factories at Willow Run and Dearborn during World War II. A portion was known as the Willow Run Expressway. The I-96 freeway runs northwest–southeast through Livingston, Oakland and Wayne counties and (as the Jeffries Freeway through Wayne County) has its eastern terminus in downtown Detroit.
I-275 runs north–south from I-75 in the south to the junction of I-96 and I-696 in the north, providing a bypass through the western suburbs of Detroit. I-375 is a short spur route in downtown Detroit, an extension of the Chrysler Freeway. I-696 (Reuther Freeway) runs east–west from the junction of I-96 and I-275, providing a route through the northern suburbs of Detroit. Taken together, I-275 and I-696 form a semicircle around Detroit. Michigan state highways designated with the letter M serve to connect major freeways.

</doc>
<doc id="8688" url="https://en.wikipedia.org/wiki?curid=8688" title="Deccan Traps">
Deccan Traps

The Deccan Traps are a large igneous province located on the Deccan Plateau of west-central India (between 17°–24°N, 73°–74°E) and one of the largest volcanic features on Earth. They consist of multiple layers of solidified flood basalt that together are more than 2,000 m (6,562 ft) thick, cover an area of 500,000 km (193,051 sq mi) and have a volume of 512,000 km (123,000 cu mi).
Etymology.
The term "trap" has been used in geology since 1785–95 for such rock formations. It is derived from the Scandinavian word for stairs ("trapp") and refers to the step-like hills forming the landscape of the region.
History.
The Deccan Traps began forming 66.250 million years ago, at the end of the Cretaceous period. The bulk of the volcanic eruption occurred at the Western Ghats (near Mumbai) some 66 million years ago. This series of eruptions may have lasted less than 30,000 years in total.
The original area covered by the lava flows is estimated to have been as large as 1.5 million km², approximately half the size of modern India. The Deccan Traps region was reduced to its current size by erosion and plate tectonics; the present area of directly observable lava flows is around .
Effect on mass extinctions and climate.
The release of volcanic gases, particularly sulfur dioxide, during the formation of the traps contributed to contemporary climate change. Data points to an average drop in temperature of 2 °C in this period.
Because of its magnitude, scientists formerly speculated that the gases released during the formation of the Deccan Traps played a role in the Cretaceous–Paleogene extinction event (also known as the K–Pg extinction). It was theorized that sudden cooling due to sulfurous volcanic gases released by the formation of the traps and localised gas concentrations may have contributed significantly to the K-Pg, as well as other mass extinctions. However, the current consensus among the scientific community is that the extinction was triggered by the Chicxulub impact event in Central America (which would have produced a sunlight-blocking dust cloud that killed much of the plant life and reduced global temperature, called an impact winter).
Work published in 2014 by geologist Gerta Keller and others on the timing of the Deccan volcanism suggests the extinction may have been caused by both the volcanism and the impact event. This was followed by a similar study in 2015.
Chemical composition.
Within the Deccan Traps at least 95% of the lavas are tholeiitic basalts, however other rock types do occur:
Mantle xenoliths have been described from Kachchh (northwestern India) and elsewhere in the western Deccan.
Fossils.
The Deccan Traps are famous for the beds of fossils that have been found between layers of lava. Particularly well known species include the frog "Oxyglossus pusillus" (Owen) of the Eocene of India and the toothed frog "Indobatrachus", an early lineage of modern frogs, which is now placed in the Australian family Myobatrachidae. The infratrappean and intertrappean beds also contain fossil freshwater mollusks.
Theories of formation.
It is postulated that the Deccan Traps eruption was associated with a deep mantle plume. The area of long-term eruption (the hotspot), known as the Réunion hotspot, is suspected of both causing the Deccan Traps eruption and opening the rift that once separated the Seychelles plateau from India. Seafloor spreading at the boundary between the Indian and African Plates subsequently pushed India north over the plume, which now lies under Réunion island in the Indian Ocean, southwest of India. The mantle plume model has, however, been challenged.
Data continues to emerge which supports the plume model. The motion of the Indian tectonic plate and the eruptive history of the Deccan traps show strong correlations. Based on data from marine magnetic profiles, a pulse of unusually rapid plate motion begins at the same time as the first pulse of Deccan flood basalts, which is dated at 67 million years ago. The spreading rate rapidly increased and reached a maximum at the same time as the peak basaltic eruptions. The spreading rate then dropped off, with the decrease occurring around 63 million years ago, by which time the main phase of Deccan volcanism ended. This correlation is seen as driven by plume dynamics.
The Indian and African plates' motions have also been shown to be coupled, with the common element being the position of these plates relative to the location of the Réunion plume head. The onset of accelerated motion of India coincides with a large slowing of the rate of counterclockwise rotation of Africa. The close correlations between the plate motions suggest that they were both driven by the force of the Réunion plume.
There is some evidence to link the Deccan Traps eruption to the asteroid impact which created the Chicxulub crater in the Mexican state of Yucatán. The combination of the asteroid impact and the resulting eruption may have been responsible for the mass extinctions that occurred at the time that separates the Cretaceous and Paleogene periods, known as the K–Pg boundary.
Suggested link to Shiva Crater.
A geological structure exists in the sea floor off the west coast of India that has been suggested as a possible impact crater, in this context called the Shiva crater. It has also been dated at approximately 66 million years ago, potentially matching the Deccan traps. The researchers claiming that this feature is an impact crater suggest that the impact may have been the triggering event for the Deccan Traps as well as contributing to the acceleration of the Indian plate in the early Paleogene. However, the current consensus in the Earth science community is that this feature is unlikely to be an actual impact crater.

</doc>
<doc id="8690" url="https://en.wikipedia.org/wiki?curid=8690" title="Don't ask, don't tell">
Don't ask, don't tell

"Don't ask, don't tell" (DADT) was the official United States policy on service by gays, bisexuals, and lesbians in the military instituted by the Clinton Administration on February 28, 1994, when Department of Defense Directive 1304.26 issued on December 21, 1993, took effect, lasting until September 20, 2011. The policy prohibited military personnel from discriminating against or harassing closeted homosexual or bisexual service members or applicants, while barring openly gay, lesbian, or bisexual persons from military service. This relaxation of legal restrictions on service by gays and lesbians in the armed forces was mandated by United States federal law (), which was signed November 30, 1993. The policy prohibited people who "demonstrate a propensity or intent to engage in homosexual acts" from serving in the armed forces of the United States, because their presence "would create an unacceptable risk to the high standards of morale, good order and discipline, and unit cohesion that are the essence of military capability".
The act prohibited any homosexual or bisexual person from disclosing his or her sexual orientation or from speaking about any homosexual relationships, including marriages or other familial attributes, while serving in the United States armed forces. The act specified that service members who disclose that they are homosexual or engage in homosexual conduct should be separated (discharged) except when a service member's conduct was "for the purpose of avoiding or terminating military service" or when it "would not be in the best interest of the armed forces". Since DADT ended in 2011, persons who are openly homosexual and bisexual have been able to serve, but those who exhibit "transvestism" are psychiatrically disqualified and those who have "major abnormalities or defects of the genitalia" are still medically disqualified.
The "don't ask" part of the DADT policy specified that superiors should not initiate investigation of a servicemember's orientation without witnessing disallowed behaviors, though credible evidence of homosexual behavior could be used to initiate an investigation. Unauthorized investigations and harassment of suspected servicemen and women led to an expansion of the policy to "don't ask, don't tell, don't pursue, don't harass".
Legislation to repeal DADT was enacted in December 2010, specifying that the policy would remain in place until the President, the Secretary of Defense, and the Chairman of the Joint Chiefs of Staff certified that repeal would not harm military readiness, followed by a 60-day waiting period. A July 6, 2011, ruling from a federal appeals court barred further enforcement of the U.S. military's ban on openly gay service members. President Barack Obama, Secretary of Defense Leon Panetta, and Chairman of the Joint Chiefs of Staff Admiral Mike Mullen sent that certification to Congress on July 22, 2011, which set the end of DADT to September 20, 2011.
Background.
Engaging in homosexual activity has been grounds for discharge from the American military since the Revolutionary War. Policies based on sexual orientation appeared as the United States prepared to enter World War II. When the military added psychiatric screening to its induction process, it included homosexuality as a disqualifying trait, then seen as a form of psychopathology. When the army issued revised mobilization regulations in 1942, it distinguished "homosexual" recruits from "normal" recruits for the first time. Before the buildup to the war, gay servicemembers were court-martialed, imprisoned, and dishonorably discharged; but in wartime, commanding officers found it difficult to convene court-martial boards of commissioned officers and the administrative blue discharge became the military's standard method for handling gay and lesbian personnel. In 1944, a new policy directive decreed that homosexuals were to be committed to military hospitals, examined by psychiatrists and discharged under Regulation 615-360, section 8.
In 1947, blue discharges were discontinued and two new classifications were created: "general" and "undesirable". Under such a system, a serviceman or woman found to be gay but who had not committed any sexual acts while in service would tend to receive an undesirable discharge. Those found guilty of engaging in sexual conduct were usually dishonorably discharged. A 1957 U.S. Navy study known as the Crittenden Report dismissed the charge that homosexuals constitute a security risk, but advocated stringent anti-homosexual policies because "Homosexuality is wrong, it is evil, and it is to be branded as such." It remained secret until 1976. Fannie Mae Clackum was the first service member to successfully appeal such a discharge, winning eight years of back pay from the US Court of Claims in 1960.
From the 1940s through the Vietnam War, some notable gay servicemembers avoided discharges despite pre-screening efforts, and when personnel shortages occurred, homosexuals were allowed to serve.
The gay and lesbian rights movement in the 1970s and 1980s raised the issue by publicizing several noteworthy dismissals of gay servicemembers. Sgt. Leonard Matlovich appeared on the cover of "Time" in 1975. In 1982 the Department of Defense issued a policy stating that "Homosexuality is incompatible with military service." It cited the military's need "to maintain discipline, good order, and morale" and "to prevent breaches of security". In 1988, in response to a campaign against lesbians at the Marines' Parris Island Depot, activists launched the Gay and Lesbian Military Freedom Project (MFP) to advocate for an end to the exclusion of gays and lesbians from the armed forces. In 1989, reports commissioned by the Personnel Security Research and Education Center (PERSEREC), an arm of the Pentagon, were discovered in the process of Joseph Steffan's lawsuit fighting his forced resignation from the U.S. Naval Academy. One report said that "having a same-gender or an opposite-gender orientation is unrelated to job performance in the same way as is being left- or right-handed." Other lawsuits fighting discharges highlighted the service record of servicemembers like Tracey Thorne and Margarethe (Grethe) Cammermeyer. The MFP began lobbying Congress in 1990, and in 1991 Senator Brock Adams (D-Washington) and Rep. Barbara Boxer introduced the Military Freedom Act, legislation to end the ban completely. Adams and Rep. Pat Schroeder (D-Colorado) re-introduced it the next year. In July 1991, Secretary of Defense Dick Cheney, in the context of the outing of his press aide Pete Williams, dismissed the idea that gays posed a security risk as "a bit of an old chestnut" in testimony before the House Budget Committee. In response to his comment, several major newspapers endorsed ending the ban, including "USA Today", the "Los Angeles Times", and the "Detroit Free Press". In June 1992, the General Accounting Office released a report that members of Congress had requested two years earlier estimating the costs associated with the ban on gays and lesbians in the military at $27 million annually.
During the 1992 U.S. presidential election campaign, the civil rights of gays and lesbians, particularly their open service in the military, attracted some press attention, and all candidates for the Democratic presidential nomination supported ending the ban on military service by gays and lesbians, but the Republicans did not make a political issue of that position. In an August cover letter to all his senior officers, Gen. Carl Mundy, Jr., Commandant of the Marine Corps, praised a position paper authored by a Marine Corps chaplain that said that "In the unique, intensely close environment of the military, homosexual conduct can threaten the lives, including the physical (e.g. AIDS) and psychological well-being of others". Mundy called it "extremely insightful" and said it offered "a sound basis for discussion of the issue". The murder of gay U.S. Navy petty officer Allen R. Schindler, Jr. on October 27, 1992, brought calls from advocates of allowing open service by gays and lesbians for prompt action from the incoming Clinton administration.
Origin.
The policy was introduced as a compromise measure in 1993 by President Bill Clinton who campaigned in 1992 on the promise to allow all citizens to serve in the military regardless of sexual orientation. Commander Craig Quigley, a Navy spokesman, expressed the opposition of many in the military at the time when he said, "Homosexuals are notoriously promiscuous" and that in shared shower situations, heterosexuals would have an "uncomfortable feeling of someone watching".
During the 1993 policy debate, the National Defense Research Institute prepared a study for the Office of the Secretary of Defense published as "Sexual Orientation and U.S. Military Personnel Policy: Options and Assessment". It concluded that "circumstances could exist under which the ban on homosexuals could be lifted with little or no adverse consequences for recruitment and retention" if the policy were implemented with care, principally because many factors contribute to individual enlistment and re-enlistment decisions. On May 5, 1993, Gregory M. Herek, associate research psychologist at the University of California at Davis and an authority on public attitudes toward lesbians and gay men, testified before the House Armed Services Committee on behalf of several professional associations. He stated: "The research data show that there is nothing about lesbians and gay men that makes them inherently unfit for military service, and there is nothing about heterosexuals that makes them inherently unable to work and live with gay people in close quarters." Herek added: "The assumption that heterosexuals cannot overcome their prejudices toward gay people is a mistaken one."
In Congress, Democratic Senator Sam Nunn of Georgia led the contingent that favored maintaining the absolute ban on gays. Reformers were led by Democratic Congressman Barney Frank of Massachusetts, who favored modification (but ultimately voted for the defense authorization bill with the gay ban language), and Barry Goldwater, a former Republican Senator and a retired Major General, who argued on behalf of allowing service by open gays and lesbians. In a June 1993 "Washington Post" opinion piece, Goldwater wrote: "You don't have to be straight to shoot straight," after Congressional phone lines were flooded by organized anti-gay opposition, indicating substantial public opposition to Clinton's open service proposal.
Congress rushed to enact the existing gay ban policy into federal law, outflanking Clinton's planned repeal effort. Clinton called for legislation to overturn the ban, but encountered intense opposition from the Joint Chiefs of Staff, members of Congress, and portions of the public. DADT emerged as a compromise policy. Congress included text in the National Defense Authorization Act for Fiscal Year 1994 (passed in 1993) requiring the military to abide by regulations essentially identical to the 1982 absolute ban policy. The Clinton Administration on December 21, 1993, issued Defense Directive 1304.26, which directed that military applicants were not to be asked about their sexual orientation. This is the policy now known as "Don't Ask, Don't Tell". The phrase was coined by Charles Moskos, a military sociologist.
In accordance with the December 21, 1993, Department of Defense Directive 1332.14, it was legal policy (10 U.S.C. § 654) that homosexuality was incompatible with military service and that persons who engaged in homosexual acts or stated that they are homosexual or bisexual were to be discharged. The Uniform Code of Military Justice, passed by Congress in 1950 and signed by President Harry S Truman, established the policies and procedures for discharging service members.
The full name of the policy at the time was "Don't Ask, Don't Tell, Don't Pursue". The "Don't Ask" provision mandated that military or appointed officials will not ask about or require members to reveal their sexual orientation. The "Don't Tell" stated that a member may be discharged for claiming to be a homosexual or bisexual or making a statement indicating a tendency towards or intent to engage in homosexual activities. The "Don’t Pursue" established what was minimally required for an investigation to be initiated. A "Don’t Harass" provision was added to the policy later. It ensured that the military would not allow harassment or violence against service members for any reason.
The Servicemembers Legal Defense Network was founded in 1993 to advocate an end to discrimination on the basis of sexual orientation in the U.S. Armed Forces.
Court challenges.
DADT was upheld by five federal Courts of Appeal. The Supreme Court, in "Rumsfeld v. Forum for Academic and Institutional Rights, Inc." (2006), unanimously held that the federal government could constitutionally withhold funding from universities, no matter what their nondiscrimination policies might be, for refusing to give military recruiters access to school resources. An association of law schools had argued that allowing military recruiting at their institutions compromised their ability to exercise their free speech rights in opposition to discrimination based on sexual orientation as represented by DADT.
"McVeigh v. Cohen".
In January 1998, Senior Chief Petty Officer Timothy R. McVeigh (not to be confused with convicted Oklahoma City bomber, Timothy J. McVeigh) won a preliminary injunction from a U.S. district court that prevented his discharge from the U.S. Navy for "homosexual conduct" after 17 years of service. His lawsuit did not challenge the DADT policy, but asked the court to hold the military accountable for adhering to the policy's particulars. The Navy had investigated McVeigh's sexual orientation based on his AOL email account name and user profile. District Judge Stanley Sporkin ruled in "McVeigh v. Cohen" that the Navy had violated its own DADT guidelines: "Suggestions of sexual orientation in a private, anonymous email account did not give the Navy a sufficient reason to investigate to determine whether to commence discharge proceedings." He called the Navy's investigation "a search and destroy mission" against McVeigh. The case also attracted attention because a navy paralegal had misrepresented himself when querying AOL for information about McVeigh's account. Frank Rich linked the two issues: "McVeigh is as clear-cut a victim of a witch hunt as could be imagined, and that witch hunt could expand exponentially if the military wants to add on-line fishing to its invasion of service members' privacy." AOL apologized to McVeigh and paid him damages. McVeigh reached a settlement with the Navy that paid his legal expenses and allowed him to retire with full benefits in July. The "New York Times" called Sporkin's ruling "a victory for gay rights, with implications for the millions of people who use computer on-line services".
"Witt v. Department of the Air Force".
In April 2006, Margaret Witt, a major in the United States Air Force who was being investigated for homosexuality filed suit in the United States District Court for the Western District of Washington seeking declaratory and injunctive relief on the grounds that DADT violates substantive due process, the Equal Protection Clause, and procedural due process. In July 2007 the Secretary of the Air Force ordered her honorable discharge. Dismissed by the district court, the case was heard on appeal and the Ninth Circuit issued its ruling on May 21, 2008. Its decision in "Witt v. Department of the Air Force" reinstated Witt's substantive due process and procedural due process claims and affirmed the dismissal of her Equal Protection claim. The Ninth Circuit, analyzing the Supreme Court decision in "Lawrence v. Texas" (2003), determined that DADT had to be subjected to heightened scrutiny, meaning that there must be an "important" governmental interest at issue, that DADT must "significantly" further the governmental interest, and that there can be no less intrusive way for the government to advance that interest.
The Obama administration declined to appeal, allowing a May 3, 2009, deadline to pass, leaving "Witt" as binding on the entire Ninth Circuit, and returning the case to the District Court. On September 24, 2010, District Judge Ronald B. Leighton ruled that Witt's constitutional rights had been violated by her discharge and that she must be reinstated to the Air Force.
The government filed an appeal with the Ninth Circuit on November 23, but made no attempt to have the trial court's ruling stayed pending the outcome. In a settlement announced on May 10, 2011, the Air Force agreed to drop its appeal and remove Witt's discharge from her military record. She will retire with full benefits.
"Log Cabin Republicans v. United States of America".
In 2010, a lawsuit filed in 2004 by the Log Cabin Republicans (LCR), the nation's largest Republican gay organization, went to trial. Challenging the constitutionality of DADT, the plaintiffs stated that the policy violates the rights of gay military members to free speech, due process and open association. The government argued that DADT was necessary to advance a legitimate governmental interest. Plaintiffs introduced statements by President Barack Obama, from prepared remarks, that DADT "doesn't contribute to our national security", "weakens our national security", and that reversal is "essential for our national security". According to plaintiffs, these statements alone satisfied their burden of proof on the due process claims.
On September 9, 2010, Judge Virginia A. Phillips ruled in "Log Cabin Republicans v. United States of America" that the ban on service by openly gay servicemembers was an unconstitutional violation of the First and Fifth Amendments. On October 12, 2010, she granted an immediate worldwide injunction prohibiting the Department of Defense from enforcing the "Don't Ask Don't Tell" policy and ordered the military to suspend and discontinue any investigation or discharge, separation, or other proceedings based on it. The Department of Justice appealed her decision and requested a stay of her injunction, which Phillips denied but which the Ninth Circuit Court of Appeals granted on October 20
and stayed pending appeal on November 1. The U.S. Supreme Court refused to overrule the stay.
District Court neither anticipated questions of
constitutional law nor formulated a rule broader than is required by the facts. The
constitutional issues regarding DADT are well-defined, and the District Court
focused specifically on the relevant inquiry of whether the statute impermissibly
infringed upon substantive due process rights with regard to a protected area of
individual liberty. Engaging in a careful and detailed review of the facts presented
to it at trial, the District Court properly concluded that the Government put forward
no persuasive evidence to demonstrate that the statute is a valid exercise of
congressional authority to legislate in the realm of protected liberty interests. See
Log Cabin, 716 F. Supp. 2d at 923. Hypothetical questions were neither presented
nor answered in reaching this decision. 
On October 19, 2010, military recruiters were told they could accept openly gay applicants. On October 20, 2010, Lt. Daniel Choi, an openly gay man honorably discharged under DADT, re-enlisted in the U.S. Army.
Following passage of the Don't Ask, Don't Tell Repeal Act of 2010, the Justice Department asked the Ninth Circuit to suspend LCR's suit in light of the legislative repeal. LCR opposed the request, noting that gay personnel were still subject to discharge. On January 28, 2011, the Court denied the Justice Department's request. The Obama administration responded by requesting that the policy be allowed to stay in place while they completed the process of assuring that its end would not impact combat readiness. On March 28, the LCR filed a brief asking that the court deny the administration's request.
In 2011, while waiting for certification, several service members were discharged under DADT at their own insistence, until July 6 when a three-judge panel of the Ninth Circuit Court of Appeals re-instated Judge Phillips' injunction barring further enforcement of the U.S. military's ban on openly gay service members. On July 11, the appeals court asked the DOJ to inform the court if it intended to proceed with its appeal. On July 14, the Justice Department filed a motion "to avoid short-circuiting the repeal process established by Congress during the final stages of the implementation of the repeal". and warning of "significant immediate harms on the government". On July 15, the Ninth Circuit restored most of the DADT policy, but continued to prohibit the government from discharging or investigating openly gay personnel. Following the implementation of DADT's repeal, a panel of three judges of the Ninth Circuit Court of Appeals vacated the Phillips ruling.
Debate.
Following the July 1999 murder of Army Pfc. Barry Winchell, apparently motivated by anti-gay bias, President Clinton issued an executive order modifying the Uniform Code of Military Justice to permit evidence of a hate crime to be admitted during the sentencing phase of a trial. In December, Secretary of Defense William Cohen ordered a review of DADT to determine if the policy's anti-gay harassment component was being observed. When that review found anti-gay sentiments were widely expressed and tolerated in the military, the DOD adopted a new anti-harassment policy in July 2000, though its effectiveness was disputed. On December 7, 1999, Hillary Clinton told an audience of gay supporters that "Gays and lesbians already serve with distinction in our nation's armed forces and should not face discrimination. Fitness to serve should be based on an individual's conduct, not their sexual orientation." Later that month, retired Gen. Carl E. Mundy defended the implementation of DADT against what he called the "politicization" of the issue by both Clintons. He cited discharge statistics for the Marines for the past 5 years that showed 75% were based on "voluntary admission of homosexuality" and 49% occurred during the first 6 months of service, when new recruits were most likely to reevaluate their decision to enlist. He also argued against any change in the policy, writing in the "New York Times": "Conduct that is widely rejected by a majority of Americans can undermine the trust that is essential to creating and maintaining the sense of unity that is critical to the success of a military organization operating under the very different and difficult demands of combat." The conviction of Winchell's murderer, according to the "New York Times", "galvanized opposition" to DADT, an issue that had "largely vanished from public debate". Opponents of the policy focused on punishing harassment in the military rather than the policy itself, which Sen. Chuck Hagel defended on December 25: "The U.S. armed forces aren't some social experiment."
The principal candidates for the Democratic presidential nomination in 2000, Al Gore and Bill Bradley, both endorsed military service by open gays and lesbians, provoking opposition from high-ranking retired military officers, notably the recently retired commandant of the Marine Corps, Gen. Charles C. Krulak. He and others objected to Gore's statement that he would use support for ending DADT as a "litmus test" when considering candidates for the Joint Chiefs of Staff. The 2000 Democratic Party platform was silent on the issue, while the Republican Party platform that year said: "We affirm that homosexuality is incompatible with military service." Following the election of George W. Bush in 2000, observers expected him to avoid any changes to DADT, since his nominee for Secretary of State Colin Powell had participated in its creation. Conservative critics of DADT were disappointed that he failed to withdraw the "don't ask" directives and return the military to its earlier complete ban on service by gays and lesbians.
In July 2004 the American Psychological Association issued a statement that DADT "discriminates on the basis of sexual orientation" and that "Empirical evidence fails to show that sexual orientation is germane to any aspect of military effectiveness including unit cohesion, morale, recruitment and retention." It said that the U.S. military's track record overcoming past racial and gender discrimination demonstrated its ability to integrate groups previously excluded. The Republican Party platform that year reiterated its support for the policy—"We affirm traditional military culture, and we affirm that homosexuality is incompatible with military service."—while the Democratic Party maintained its silence.
In February 2005, the Government Accountability Office released estimates of the cost of DADT. It reported at least $95.4 million in recruiting costs and at least $95.1 million for training replacements for the 9,488 troops discharged from 1994 through 2003, while noting that the true figures might be higher. In September, as part of its campaign to demonstrate that the military allowed open homosexuals to serve when its manpower requirements were greatest, the Center for the Study of Sexual Minorities in the Military (now the Palm Center) reported that army regulations allowed the active duty deployment of Army Reservists and National Guard troops who claim to be or who are accused of being gay. An U.S. Army Forces Command spokesperson said the regulation was intended to prevent Reservists and National Guard members from pretending to be gay to escape combat. Advocates of ending DADT repeatedly publicized discharges of highly trained gay and lesbian personnel, especially those in positions with critical shortages, including fifty-nine Arabic speakers and nine Persian speakers. Elaine Donnelly, president of the Center for Military Readiness, later argued that the military's failure to ask about sexual orientation at recruitment was the cause of the discharges: ou could reduce this number to zero
or near zero if the Department of Defense dropped Don't Ask, Don't Tell... We should not be training people who are not eligible to be in the Armed Forces."
In February 2006, a University of California Blue Ribbon Commission that included Lawrence Korb, a former assistant defense secretary during the Reagan administration, William Perry, Secretary of Defense in the Clinton administration, and professors from the United States Military Academy released their assessment of the GAO's analysis of the cost of DADT released a year earlier. The commission report stated that the GAO did not take into account the value the military lost from the departures. They said that that total cost was closer to $363 million, including $14.3 million for "separation travel" following a servicemember's discharge, $17.8 million for training officers, $252.4 million for training enlistees, and $79.3 million in recruiting costs.
In 2006, Soulforce, a national LGBT rights organization, organized its Right to Serve Campaign, in which gay men and lesbians in several cities attempted to enlist in the Armed Forces or National Guard.
In 2006, a speaking tour of gay former service members, organized by SLDN, Log Cabin Republicans, and Meehan, visited 18 colleges and universities. Patrick Guerriero, executive director of Log Cabin, thought the repeal movement was gaining "new traction" but "Ultimately", said, "we think it's going to take a Republican with strong military credentials to make a shift in the policy." Elaine Donnelly called such efforts "a big P.R. campaign" and said that "The law is there to protect good order and discipline in the military, and it's not going to change."
In December 2006, Zogby International released the results of a poll of military personnel conducted in October 2006 that found that 26% favored allowing gays and lesbians to serve openly in the military, 37% were opposed, while 37% expressed no preference or were unsure. Of respondents who had experience with gay people in their unit, 6% said their presence had a positive impact on their personal morale, 66% said no impact, and 28% said negative impact. Regarding overall unit morale, 3% said positive impact, 64% no impact, and 27% negative impact.
Retired Chairman of the Joint Chiefs of Staff General John Shalikashvili and former Senator and Secretary of Defense William Cohen opposed the policy in January 2007: "I now believe that if gay men and lesbians served openly in the United States military, they would not undermine the efficacy of the armed forces" Shalikashvili wrote. "Our military has been stretched thin by our deployments in the Middle East, and we must welcome the service of any American who is willing and able to do the job." Shalikashvili cited the recent "Zogby poll of more than 500 service members returning from Afghanistan and Iraq, three quarters of whom said they were comfortable interacting with gay people. The debate took a different turn in March when Gen. Peter Pace, Chairman of the Joint Chiefs of Staff, told the editorial board of the "Chicago Tribune" he supported DADT because "homosexual acts between two individuals are immoral and ... we should not condone immoral acts." His remarks became, according to the "Tribune", "a huge news story on radio, television and the Internet during the day and showed how sensitive the Pentagon's policy has become." Sen. John Warner, who backed DADT, said "I respectfully, but strongly, disagree with the chairman's view that homosexuality is immoral", and Pace expressed regret for expressing his personal views and said that DADT "does not make a judgment about the morality of individual acts." Massachusetts Governor Mitt Romney, then in the early stages of his campaign for the 2008 Republican presidential nomination, defended DADT:
That summer, after a U.S. senator was arrested for lewd conduct in a men's restroom, conservative commentator Michael Medved argued that any liberalization of DADT would "compromise restroom integrity and security". He wrote: "The national shudder of discomfort and queasiness associated with any introduction of homosexual eroticism into public men's rooms should make us more determined than ever to resist the injection of those lurid attitudes into the even more explosive situation of the U.S. military."
In November 2007, 28 retired generals and admirals urged Congress to repeal the policy, citing evidence that 65,000 gay men and women were serving in the armed forces and that there were over a million gay veterans. On November 17, 2008, 104 retired generals and admirals signed a similar statement. In December, SLDN arranged for "60 Minutes" to interview Darren Manzella, an Army medic who served in Iraq after coming out to his unit.
On May 4, 2008, while Chairman of the Joint Chiefs of Staff Admiral Mike Mullen addressed the graduating cadets at West Point, a cadet asked what would happen if the next administration were supportive of legislation allowing gays to serve openly. Mullen responded, "Congress, and not the military, is responsible for DADT." Previously, during his Senate confirmation hearing in 2007, Mullen told lawmakers, "I really think it is for the American people to come forward, really through this body, to both debate that policy and make changes, if that's appropriate." He went on to say, "I'd love to have Congress make its own decisions" with respect to considering repeal.
In May 2009, when a committee of military law experts at the Palm Center, an anti-DADT research institute, concluded that the President could issue an Executive Order to suspend homosexual conduct discharges, Obama rejected that option and said he wanted Congress to change the law.
On July 5, 2009, Colin Powell told CNN said that the policy was "correct for the time" but that "sixteen years have now gone by, and I think a lot has changed with respect to attitudes within our country, and therefore I think this is a policy and a law that should be reviewed." Interviewed for the same broadcast, Mullen said the policy would continue to be implemented until the law was repealed, and that his advice was to "move in a measured way... At a time when we're fighting two conflicts there is a great deal of pressure on our forces and their families." In September, "Joint Force Quarterly" published an article by an Air Force colonel that disputed the argument that unit cohesion is compromised by the presence of openly gay personnel.
In October 2009, the Commission on Military Justice, known as the Cox Commission, repeated its 2001 recommendation that Article 125 of the Uniform Code of Military Justice, which bans sodomy, be repealed, noting that "most acts of consensual sodomy committed by consenting military personnel are not prosecuted, creating a perception that prosecution of this sexual behavior is arbitrary."
In January 2010, the White House and congressional officials started work on repealing the ban by inserting language into the 2011 defense authorization bill. During Obama's State of the Union Address on January 27, 2010, he said that he would work with Congress and the military to enact a repeal of the gay ban law and for the first time set a timetable for repeal.
At a February 2, 2010, congressional hearing, Senator John McCain read from a letter signed by "over one thousand former general and flag officers". It said: "We firmly believe that this law, which Congress passed to protect good order, discipline and morale in the unique environment of the armed forces, deserves continued support." The signature campaign had been organized by Elaine Donnelly of the Center for Military Readiness, a longtime supporter of a traditional all-male and all-heterosexual military. Servicemembers United, a veterans group opposed to DADT, issued a report critical of the letter's legitimacy. They said that among those signing the letter were officers who had no knowledge of their inclusion or who had refused to be included, and even one instance of a general's widow who signed her husband's name to the letter though he had died before the survey was published. The average age of the officers whose names were listed as signing the letter was 74, the oldest was 98, and Servicemembers United noted that "only a small fraction of these officers have even served in the military during the 'Don't Ask, Don't Tell' period, much less in the 21st century military." That same month, the Human Rights Campaign launched its "Repeal DADT Now Campaign" to mobilize grassroots support and target swing states.
The Center for American Progress issued a report in March 2010 that said a smooth implementation of an end to DADT required eight specified changes to the military's internal regulations. On March 25, 2010, Defense Secretary Gates announced new rules mandating that only flag officers could initiate discharge proceedings and imposing more stringent rules of evidence on discharge proceedings.
Repeal.
The underlying justifications for DADT have been subjected to increasing suspicion and outright rejection by the early 21st century. Mounting evidence obtained from the integration efforts of foreign militaries, surveys of U.S. military personnel, and studies conducted by the DoD  gave credence to the view that the presence of open homosexuals within the military would not be detrimental at all to the armed forces. A DoD study conducted on the behest of Secretary of Defense Robert Gates in 2010 supports this most.
The DoD working group conducting the study considered the impact that lifting the ban would have on unit cohesion and effectiveness, good order and discipline, and military morale. The study included a survey that revealed significant differences between respondents who believed they had served with homosexual troops and those who did not believe they had.In analyzing such data, the DoD working group concluded that it was actually generalized perceptions of homosexual troops that led to the perceived unrest that would occur without DADT. Ultimately, the study deemed the overall risk to military effectiveness of lifting the ban to be low.Citing the ability of the armed forces to adjust to the previous integration of African-Americans and women, the DoD study asserted that the United States military could adjust as had it before in history without an impending serious effect.
In March 2005, Rep. Martin T. Meehan introduced the Military Readiness Enhancement Act in the House. It aimed "to amend title 10, United States Code, to enhance the readiness of the Armed Forces by replacing the current policy concerning homosexuality in the Armed Forces, referred to as 'Don't ask, don't tell,' with a policy of nondiscrimination on the basis of sexual orientation". As of 2006, it had 105 Democrats and 4 Republicans as co-sponsors. He introduced the bill again in 2007 and 2009.
During the 2008 presidential election campaign, Senator Barack Obama advocated a full repeal of the laws barring gays and lesbians from serving in the military. Nineteen days after his election, Obama's advisers announced that plans to repeal the policy might be delayed until 2010, because Obama "first wants to confer with the Joint Chiefs of Staff and his new political appointees at the Pentagon to reach a consensus, and then present legislation to Congress". As president he advocated a policy change to allow gay personnel to serve openly in the armed forces, stating that the U.S. government has spent millions of dollars replacing troops expelled from the military, including language experts fluent in Arabic, because of DADT. On the eve of the National Equality March in Washington, D.C., October 10, 2009, Obama stated in a speech before the Human Rights Campaign that he would end the ban, but he offered no timetable. Obama said in his 2010 State of the Union Address: "This year, I will work with Congress and our military to finally repeal the law that denies gay Americans the right to serve the country they love because of who they are." This statement was quickly followed up by Defense Secretary Robert Gates and Joint Chiefs Chairman Michael Mullen voicing their support for a repeal of DADT.
Don't Ask, Don't Tell Repeal Act of 2010.
Democrats in both houses of Congress first attempted to end DADT by amending the Defense Authorization Act. On May 27, 2010, on a 234–194 vote, the U.S. House of Representatives approved the Murphy amendment to the National Defense Authorization Act for Fiscal Year 2011. It provided for repeal of the DADT policy and created a process for lifting the policy, including a U.S. Department of Defense study and certification by key officials that the change in policy would not harm military readiness followed by a waiting period of 60 days. The amended defense bill passed the House on May 28, 2010. On September 21, 2010, John McCain led a successful filibuster against the debate on the Defense Authorization Act, in which 56 Senators voted to end debate, four short of the 60 votes required. Some advocates for repeal, including the Palm Center, OutServe, and Knights Out, opposed any attempt to block the passage of NDAA if it failed to include DADT repeal language. The Human Rights Campaign, the Center for American Progress, Servicemembers United and SLDN refused to concede that possibility.
The American Civil Liberties Union (ACLU) filed a lawsuit, "Collins v. United States", against the Department of Defense in November 2010 seeking full compensation for those discharged under the policy.
On November 30, 2010, the Joint Chiefs of Staff released the "Don't Ask, Don't Tell" Comprehensive Review Working Group (CRWG) report authored by Jeh C. Johnson, General Counsel of the Department of Defense, and Army General Carter F. Ham. It outlined a path to the implementation of repeal of DADT. The report indicated that there was a low risk of service disruptions due to repealing the ban, provided time was provided for proper implementation and training. It included the results of a survey of 115,000 active-duty and reserve service members. Across all service branches, 30 percent thought that integrating gays into the military would have negative consequences. In the Marine Corps and combat specialties, the percentage with that negative assessment ranged from 40 to 60 percent. The CRWG also said that 69 percent of all those surveyed believed they had already worked with a gay or lesbian and of those, 92 percent reported that the impact of that person's presence was positive or neutral. The same day, in response to the CRWG, 30 professors and scholars, most from military institutions, issued a joint statement saying that the CRWG "echoes more than 20 studies, including studies by military researchers, all of which reach the same conclusion: allowing gays and lesbians to serve openly will not harm the military ... We hope that our collective statement underscores that the debate about the evidence is now officially over..." The Family Research Council's president, Tony Perkins, interpreted the CRWG data differently, writing that it "reveals that 40 percent of Marines and 25 percent of the Army could leave".
Gates encouraged Congress to act quickly to repeal the law so that the military could carefully adjust rather than face a court decision requiring it to lift the policy immediately. The United States Senate held two days of hearings on December 2 and 3, 2010, to consider the CRWG report. Defense Secretary Robert Gates, Joint Chiefs Chairman Michael Mullen urged immediate repeal. The heads of the Marine Corps, Army, and Navy all advised against immediate repeal and expressed varied views on its eventual repeal. Oliver North, writing in "National Review" the next week, said that Gates' testimony showed "a deeply misguided commitment to political correctness". He interpreted the CRWG's data as indicating a high risk that large numbers of resignations would follow the repeal of DADT. Servicemembers, especially combat troops, he wrote, "deserve better than to be treated like lab rats in Mr. Obama's radical social experiment".
On December 9, 2010, another filibuster prevented debate on the Defense Authorization Act. In response to that vote, Senators Joe Lieberman and Susan Collins introduced a bill that included the policy-related portions of the Defense Authorization Act that they considered more likely to pass as a stand-alone bill. It passed the House on a vote of 250 to 175 on December 15, 2010. On December 18, 2010, the Senate voted to end debate on its version of the bill by a cloture vote of 63–33. The final Senate vote was held later that same day, with the measure passing by a vote of 65–31.
U.S. Secretary of Defense Robert Gates released a statement following the vote indicating that the planning for implementation of a policy repeal would begin right away and would continue until Gates certified that conditions were met for orderly repeal of the policy. President Obama signed the repeal into law on December 22, 2010.
Implementation of repeal.
The repeal act established a process for ending the DADT policy. The President, the Secretary of Defense and the Chairman of the Joint Chiefs of Staff were required to certify in writing that they had reviewed the Pentagon's report on the effects of DADT repeal, that the appropriate regulations had been reviewed and drafted, and that implementation of repeal regulations "is consistent with the standards of military readiness, military effectiveness, unit cohesion, and recruiting and retention of the Armed Forces". Once certification was given, DADT would be lifted after a 60-day waiting period.
Representative Duncan D. Hunter announced plans in January 2011 to introduce a bill designed to delay the end of DADT. His proposed legislation required all of the chiefs of the armed services to submit the certification at the time required only of the President, Defense Secretary and Joint Chiefs Chairman. In April, Perkins of the Family Research Council argued that the Pentagon was misrepresenting its own survey data and that hearings by the House Armed Services Committee, now under Republican control, could persuade Obama to withhold certification. Congressional efforts to prevent the change in policy from going into effect continued into May and June 2011.
On January 29, 2011, Pentagon officials stated that the training process to prepare troops for the end of DADT would begin in February and would proceed quickly, though they suggested that it might not be completed in 2011. On the same day, the DOD announced it would not offer any additional compensation to servicemembers who had been discharged under DADT, who received half of the separation pay other honorably discharged servicemembers received.
In May 2011, the U.S. Army reprimanded three colonels for performing a skit in March 2011 at a function at Yongsan Garrison, South Korea, that mocked the repeal.
In May 2011, revelations that an April Navy memo relating to its DADT training guidelines contemplated allowing same-sex weddings in base chapels and allowing chaplains to officiate if they so chose resulted in a letter of protest from 63 Republican congressman, citing the Defense of Marriage Act (DOMA) as controlling the use of federal property. Tony Perkins of the Family Research Council said the guidelines "make it even more uncomfortable for men and women of faith to perform their duties". A Pentagon spokesperson replied that DOMA "does not limit the type of religious ceremonies a chaplain may perform in a chapel on a military installation", and a Navy spokesperson said that "A chaplain can conduct a same-sex ceremony if it is in the tenets of his faith". A few days later the Navy rescinded its earlier instructions "pending additional legal and policy review and interdepartmental coordination".
While waiting for certification, several service members were discharged at their own insistence until a July 6 ruling from a federal appeals court barred further enforcement of the U.S. military's ban on openly gay service members, which the military promptly did.
Anticipating the lifting of DADT, some active duty servicemembers wearing civilian clothes marched in San Diego's gay pride parade on July 16. The DOD noted that participation "does not constitute a declaration of sexual orientation".
President Obama, Secretary of Defense Leon Panetta, and Admiral Mike Mullen, Chairman of the Joint Chiefs of Staff, sent the certification required by the Repeal Act to Congress on July 22, 2011, setting the end of DADT for September 20, 2011. A Pentagon spokesman said that servicemembers discharged under DADT would be able to re-apply to rejoin the military then.
At the end of August 2011, the DOD approved the distribution of the magazine produced by OutServe, an organization of gay and lesbian servicemembers, at Army and Air Force base exchanges beginning with the September 20 issue, coinciding with the end of DADT.
On September 20, Air force officials announced that 22 Air Force Instructions were "updated as a result of the repeal of DADT". On September 30, 2011, the Department of Defense modified regulations to reflect the repeal by deleting "homosexual conduct" as a ground for administrative separation.
Day of repeal and aftermath.
On the eve of repeal, US Air Force 1st Lt. Josh Seefried, one of the founders of OutServe, an organization of LGBT troops, revealed his identity after two years of hiding behind a pseudonym. Senior Airman Randy Phillips, after conducting a social media campaign seeking encouragement coming out and already out to his military co-workers, came out to his father on the evening of September 19. When the video of their conversation he posted on YouTube went viral, it made him, in one journalist's estimation, "the poster boy for the DADT repeal". The moment the repeal took effect at midnight on September 19, US Navy Lt. Gary C. Ross married his same-sex partner of eleven and a half years Dan Swezy, making them the first same-sex military couple to legally marry in the United States. Retired Rear Adm. Alan S. Steinman became the highest-ranking person to come out immediately following the end of DADT. HBO produced a World of Wonder documentary, "The Strange History of Don't Ask, Don't Tell", and premiered it on September 20. "Variety" called it "an unapologetic piece of liberal advocacy" and "a testament to what formidable opponents ignorance and prejudice can be". Discharge proceedings on the grounds of homosexuality, some begun years earlier, came to an end.
In the weeks that followed, a series of firsts attracted press attention to the impact of the repeal. Reservist Jeremy Johnson became the first person discharged under DADT to re-enlist. Jase Daniels became the first to return to active duty, re-joining the Navy as a third class petty officer. On December 2, Air Force intelligence officer Ginger Wallace became the first open LGBT servicemember to have a same-sex partner participate in the "pinning-on" ceremony that marked her promotion to colonel. On December 23, after 80 days at sea, US Navy Petty Officer 2nd Class Marissa Gaeta won the right to the traditional "first kiss" upon returning to port and shared it with her same-sex partner. Her ship's commanding officer, in one report, "said that the crew's reaction upon learning who was selected to have the first kiss after a raffle was positive". On January 20, 2012, U.S. servicemembers deployed to Bagram, Afghanistan, produced a video in support of the It Gets Better Project, which aims to support LGBT at-risk youth. Widespread news coverage continued even months after the repeal date, when a photograph of Marine Sgt. Brandon Morgan kissing his partner at a February 22, 2012, homecoming celebration on Marine Corps Base Hawaii went viral. When asked for her comment, a spokesperson for the Marine Corps said: "It's your typical homecoming photo."
On September 30, 2011, Under Secretary of Defense Clifford Stanley announced the DOD's policy that military chaplains are allowed to perform same-sex marriages "on or off a military installation" where local law permits them. His memo noted that "a chaplain is not required to participate in or officiate a private ceremony if doing so would be in variance with the tenets of his or her religion" and "a military chaplain's participation in a private ceremony does not constitute an endorsement of the ceremony by DoD". Some religious groups announced that their chaplains would not participate in such weddings, including an organization of evangelical Protestants, the Chaplain Alliance for Religious Liberty and Roman Catholics led by Archbishop Timothy Broglio of the Archdiocese for the Military Services, USA.
In late October 2011, speaking at the Air Force Academy, Col. Gary Packard, leader of the team that drafted the DOD's repeal implementation plan, said: "The best quote I've heard so far is, 'Well, some people's Facebook status changed, but that was about it.'" In late November, discussing the repeal of DADT and its implementation, Marine Gen. James F. Amos said "I'm very pleased with how it has gone" and called it a "non-event". He said his earlier public opposition was appropriate based on ongoing combat operations and the negative assessment of the policy given by 56% of combat troops under his command in the Department of Defense's November 2010 survey. A Defense Department spokesperson said implementation of repeal occurred without incident and added: "We attribute this success to our comprehensive pre-repeal training program, combined with the continued close monitoring and enforcement of standards by our military leaders at all levels."
In December 2011, Congress considered two DADT-related amendments in the course of work on the National Defense Authorization Act for 2012. The Senate approved 97-3, an amendment removing the prohibition on sodomy found in Article 125 of the Uniform Code of Military Justice as recommended by the Comprehensive Review Working Group (CRWG) a year earlier. The House approved an amendment banning same-sex marriages from being performed at military bases or by military employees, including chaplains and other employees of the military when "acting in an official capacity". Neither amendment appeared in the final legislation.
In July 2012 the Department of Defense granted permission for military personnel to wear their uniforms while participating in the San Diego Pride Parade. This was the first time that U.S. military personnel were permitted to wear their service uniforms in such a parade.
Marking the first anniversary of the passage of the Repeal Act, television news networks reported no incidents in the three months since DADT ended. One aired video of a social gathering for gay servicemembers at a base in Afghanistan. Another reported on the experience of lesbian and gay troops, including some rejection after coming out to colleagues.
The Palm Center, a think tank that studies issues of sexuality and the military, released a study in September 2012 that found no negative consequences, nor any effect on military effectiveness from DADT repeal. This study began six months following repeal and concluded at the one year mark. The study included surveys of 553 generals and admirals who had opposed repeal, experts who supported DADT, and more than 60 heterosexual, gay, lesbian and bisexual active duty service personnel.
On January 7, 2013, the ACLU reached a settlement with the federal government in "Collins v. United States". It provided for the payment of full separation pay to servicemembers discharged under DADT since November 10, 2004, who had previously been granted only half that.
2012 presidential campaign issue.
Several candidates for the 2012 Republican presidential nomination called for the restoration of DADT, including Michele Bachmann, Rick Perry, and Rick Santorum. Newt Gingrich called for an extensive review of DADT's repeal.
Ron Paul, having voted for the Repeal Act, maintained his support for allowing military service by open homosexuals. Herman Cain called the issue "a distraction" and opposed reinstating DADT. Mitt Romney said that the winding down of military operations in Iraq and Afghanistan obviated his opposition to the repeal and said he was not proposing any change to policy.
On September 22, 2011, the audience at a Republican candidates' debate booed a U.S. soldier posted in Iraq who asked a question via video about the repeal of DADT, and none of the candidates noticed or responded to the crowd's behavior. Two days later, Obama commented on the incident while addressing a dinner of the Human Rights Campaign: "You want to be commander in chief? You can start by standing up for the men and women who wear the uniform of the United States, even when it's not politically convenient".
In June 2012, Rep. Howard McKeon, Republican chair of the House Armed Services Committee, said he considered the repeal of DADT a settled issue and if Romney became president would not advocate its reinstatement, though others in his party might.
Views of the policy.
Public opinion.
According to a December 2010 Washington Post-ABC News poll 77% of Americans said gays and lesbians who publicly disclose their sexual orientation should be able to serve in the military. That number showed little change from polls over the previous two years, but represented the highest level of support in a Post-ABC poll. The support also cut across partisan and ideological lines, with majorities of Democrats (86%), Republicans (74%), independents (74%), liberals (92%), conservatives (67%), white evangelical Protestants (70%) and non-religious (84%) in favor of homosexuals serving openly.
A November 2010 survey by the Pew Research Center found that 58% of the U.S. public favored allowing gays and lesbians to serve openly in the military, while less than half as many (27%) were opposed. According to a November 2010 CNN/Opinion Research Corporation poll, 72% of adult Americans favored permitting people who are openly gay or lesbian to serve in the military, while 23% opposed it. "The main difference between the CNN poll and the Pew poll is in the number of respondents who told pollsters that they didn't have an opinion on this topic – 16 percent in the Pew poll compared to only five percent in the CNN survey", said CNN Polling Director Keating Holland. "The two polls report virtually the same number who say they oppose gays serving openly in the military, which suggests that there are some people who favor that change in policy but for some reason were reluctant to admit that to the Pew interviewers. That happens occasionally on topics where moral issues and equal-treatment issues intersect."
A February 2010 Quinnipiac University Polling Institute national poll showed 57% of American voters favored gays serving openly, compared to 36% opposed, while 66% said not allowing openly gay personnel to serve is discrimination, compared to 31% who did not see it as discrimination. A CBS News/"New York Times" national poll done at the same time showed 58% of Americans favored gays serving openly, compared to 28% opposed.
Chaplains and religious groups.
Chaplain groups and religious organizations took various positions on DADT. Some felt that the policy needed to be withdrawn to make the military more inclusive. The Southern Baptist Convention battled the repeal of DADT, warning that their endorsements for chaplains might be withdrawn if the repeal took place. They took the position that allowing gay men and women to serve in the military without restriction would have a negative impact on the ability of chaplains who think homosexuality is a sin to speak freely regarding their religious beliefs. The Roman Catholic Church called for the retention of the policy, but had no plans to withdraw its priests from serving as military chaplains. Sixty-five retired chaplains signed a letter opposing repeal, stating that repeal would make it impossible for chaplains whose faith teaches that same-sex behavior is immoral to minister to military servicemembers. Other religious organizations and agencies called the repeal of the policy a "non-event" or "non-issue" for chaplains, claiming that chaplains have always supported military service personnel, whether or not they agree with all their actions or beliefs.
Discharges under DADT.
After the policy was introduced in 1993, the military discharged over 13,000 troops from the military under DADT. The number of discharges per fiscal year under DADT dropped sharply after the September 11 attacks and remained comparatively low through to the repeal. Discharges exceeded 600 every year until 2009.

</doc>
<doc id="8691" url="https://en.wikipedia.org/wiki?curid=8691" title="Divination">
Divination

Divination (from Latin "divinare" "to foresee, to be inspired by a god", related to "divinus", divine) is the attempt to gain insight into a question or situation by way of an occultic, standardized process or ritual. Used in various forms throughout history, diviners ascertain their interpretations of how a querent should proceed by reading signs, events, or omens, or through alleged contact with a supernatural agency.
Divination can be seen as a systematic method with which to organize what appear to be disjointed, random facets of existence such that they provide insight into a problem at hand. If a distinction is to be made between divination and fortune-telling, divination has a more formal or ritualistic element and often contains a more social character, usually in a religious context, as seen in traditional African medicine. Fortune-telling, on the other hand, is a more everyday practice for personal purposes. Particular divination methods vary by culture and religion.
Divination is dismissed by the scientific community and skeptics as being superstition. In the 2nd century, Lucian devoted a witty essay to the career of a charlatan, "Alexander the false prophet", trained by "one of those who advertise enchantments, miraculous incantations, charms for your love-affairs, visitations for your enemies, disclosures of buried treasure, and successions to estates", even though most Romans believed in prophetic dreams and charms.
Categories.
Psychologist Julian Jaynes categorized divination into the following four types:
In addition to these four broad categories, there is palmistry, also called chiromancy, a practice common to many different places on the Eurasian landmass; it has been practised in the cultures of India, Tibet, China, Persia, Sumeria, Ancient Israel and Babylonia. In this practice, the diviner examines the hands of a person for whom they are divining for indications of their future.
Historical examples.
Ancient Egypt.
The Oracle of Amun at the Siwa Oasis was made famous when Alexander the Great visited it after delivering Egypt from Persian rule in 332 BC.
Hebrew Bible.
 or can be interpreted as categorically forbidding divination. However, some would claim that divination is indeed practiced in the Bible, such as in Exodus 28, when the Urim and Thummim are mentioned. Some would also say that Gideon also practiced divination, though when he uses a piece of fleece or wool in , he is not attempting to predict the outcome of an important battle; rather, he is communicating with God. Communicating with God through prayer is not the same as divination, though both are open, typically two-way conversations with God. In addition, the method of "casting lots" used in and to divide the conquered lands of Canaan between the twelve tribes is not seen by some as divination, but as done at the behest of God (Numbers 26:55).
Ancient Greece.
Both oracles and seers in ancient Greece practiced divination. Oracles were the conduits for the gods on earth; their prophecies were understood to be the will of the gods verbatim. Because of the high demand for oracle consultations and the oracles’ limited work schedule, they were not the main source of divination for the ancient Greeks. That role fell to the seers ("μάντεις" in Greek).
Seers were not in direct contact with the gods; instead, they were interpreters of signs provided by the gods. Seers used many methods to explicate the will of the gods including extispicy, bird signs, etc. They were more numerous than the oracles and did not keep a limited schedule; thus, they were highly valued by all Greeks, not just those with the capacity to travel to Delphi or other such distant sites.
The disadvantage to seers was that only direct yes-or-no questions could be answered. Oracles could answer more generalized questions, and seers often had to perform several sacrifices in order to get the most consistent answer. For example, if a general wanted to know if the omens were proper for him to advance on the enemy, he would ask his seer both that question and if it were better for him to remain on the defensive. If the seer gave consistent answers, the advice was considered valid.
At battle, generals would frequently ask seers at both the campground (a process called the "hiera") and at the battlefield (called the "sphagia"). The hiera entailed the seer slaughtering a sheep and examining its liver for answers regarding a more generic question; the sphagia involved killing a young female goat by slitting its throat and noting the animal’s last movements and blood flow. The battlefield sacrifice only occurred when two armies prepared for battle against each other. Neither force would advance until the seer revealed appropriate omens.
Because the seers had such power over influential individuals in ancient Greece, many were skeptical of the accuracy and honesty of the seers. Of course the degree to which seers were honest depends entirely on the individual seers. Despite the doubt surrounding individual seers, the craft as a whole was well regarded and trusted by the Greeks.
Christianity and Western society.
The divination method of casting lots (Cleromancy) was used by the remaining eleven disciples of Jesus in to select a replacement for Judas Iscariot. Therefore, divination was arguably an accepted practice in the early church. However, divination became viewed as a pagan practice by the institutional Christian church. The Catholic Church later passed canon laws forbidding the practice of divination. In 692 the Quinisext Council, also known as the "Council in Trullo" in the Eastern Orthodox Church, passed canons to eliminate pagan and divination practices. Fortune-telling and other forms of divination were widespread through the Middle Ages. In the constitution of 1572 and public regulations of 1661 of Kur-Saxony, capital punishment was used on those predicting the future. Laws forbidding divination practice continue to this day.
Mesoamerica.
Divination was a central component of ancient Mesoamerican religious life. Many Aztec gods, including central creator gods, were described as diviners and were closely associated with sorcery. Tezcatlipoca is the patron of sorcerers and practitioners of magic. His name means "smoking mirror", a reference to a device used for divinatory scrying. In the Mayan "Popol Vuh", the creator gods Xmucane and Xpiacoc perform divinatory hand casting during the creation of people.
Every civilization that developed in Ancient Mexico, from the Olmecs to the Aztecs, practiced divination in daily life, both public and private. Scrying through the use of reflective water surfaces, mirrors, or the casting of lots were among the most widespread forms of divinatory practice. Visions derived from hallucinogens were another important form of divination, and are still widely used among contemporary diviners of Mexico. Among the more common hallucinogenic plants used in divination are morning glory, jimson weed, and peyote.
Ethnographic examples.
Buddhism.
Buddhists in North Asia divine by different methods.
Serer religion.
Divination is one of the tenets of Serer religion. However, only those who have been initiated as Saltigues (the Serer high priests and priestesses) can divine the future. These are the "hereditary rain priests" whose role is both religious and medicinal.
Yoruba religion.
The Yoruba people of West Africa are internationally known for having developed the Ifá system, an intricate process of divination that is performed by an "Awo", an initiated priest or priestess of Orunmila, the spirit of the Yoruba oracle.
Sweden.
Småland is famous for Årsgång, a practice which occurred until the early 19th century in some parts of Småland. Generally occurring on Christmas and New Year's Eve, it is a practice in which one would fast and keep themselves away from light in a room until midnight to then complete a set of complex events to interpret symbols encountered throughout the journey to foresee the coming year.

</doc>
<doc id="8693" url="https://en.wikipedia.org/wiki?curid=8693" title="Diet of Nuremberg">
Diet of Nuremberg

The Diets of Nuremberg, also called the Imperial Diets of Nuremberg, took place at different times between the Middle Ages and the 17th century. 
The first Diet of Nuremberg, in 1211, elected the future emperor Frederick II of Hohenstaufen as German king.
At the Diet of 1356 the Emperor Charles IV issued the Golden Bull of 1356, which required each Holy Roman Emperor to summon the first Imperial Diet after his election at Nuremberg. Apart from that, a number of other diets were held there.
Important to Protestantism (and the Turks) were the Diets of 1522 ("First Diet of Nuremberg"), 1524 ("Second Diet of Nuremberg") and 1532 ("Third Diet of Nuremberg").
The 1522 Diet of Nuremberg.
This Diet has become known mostly for the reaction of the papacy to the decision made on Luther at the Diet of Worms the previous year. The new pope, Adrian VI, sent his nuncio Francesco Chieregati to the Diet, to insist both that the Edict of Worms be executed, and that action be taken promptly against Luther. This demand, however, was coupled with a promise of thorough reform in the Roman hierarchy, frankly admitting the partial guilt of the Vatican in the decline of the Church.
In the recess drafted on 9 February 1523, however, the German princes rejected this appeal. Using Adrian's admissions, they declared that they could not have it appear 'as though they wished to oppress evangelical truth and assist unchristian and evil abuses.'
The 1524 Diet of Nuremberg.
This Diet generally took the same line as the previous one. The Estates reiterated their decision from the previous Diet. The Cardinal-legate, Campeggio, who was present, showed his disgust at the behaviour of the Estates. On 18 April, the Estates decided to call 'a general gathering of the German nation', to meet at Speyer the following year and to decide what would be done until the meeting of the general council of the Church which they demanded.

</doc>
<doc id="8695" url="https://en.wikipedia.org/wiki?curid=8695" title="Dr. Strangelove">
Dr. Strangelove

Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb, more commonly known as Dr. Strangelove, is a 1964 political satire black comedy film that satirizes the Cold War fears of a nuclear conflict between the USSR and the US. The film was directed, produced, and co-written by Stanley Kubrick, stars Peter Sellers and George C. Scott, and features Sterling Hayden, Keenan Wynn, and Slim Pickens. Production took place in the United Kingdom. The film is loosely based on Peter George's thriller novel "Red Alert".
The story concerns an unhinged United States Air Force general who orders a first strike nuclear attack on the Soviet Union. It follows the President of the United States, his advisers, the Joint Chiefs of Staff, and a Royal Air Force (RAF) officer as they try to recall the bombers to prevent a nuclear apocalypse. It separately follows the crew of one B-52 bomber as they try to deliver their payload.
In 1989, the United States Library of Congress included it in the first group of films selected for preservation in the National Film Registry. It was listed as number three on AFI's 100 Years...100 Laughs list.
Plot.
United States Air Force Brigadier General Jack D. Ripper (Sterling Hayden) is commander of Burpelson Air Force Base, which houses the Strategic Air Command 843rd Bomb Wing, equipped with B-52 bombers. The 843rd is currently in-flight on airborne alert, a few hours from the Soviet border.
General Ripper orders his executive officer, Group Captain Lionel Mandrake (Peter Sellers) of the UK Royal Air Force, to put the base on alert. Ripper also issues "Wing Attack Plan R" to the patrolling aircraft, one of which is commanded by Major T. J. "King" Kong (Slim Pickens). All of the aircraft commence an attack flight on Russia and set their radios to allow communications only through the CRM 114 discriminator, which is programmed to accept only communications preceded by a secret three-letter code known only to General Ripper.
Mandrake discovers that no war order has been issued by the Pentagon and tries to stop Ripper, who locks them both in his office. Ripper tells Mandrake that he believes the Soviets have been using fluoridation of United States water supplies to pollute the "precious bodily fluids" of Americans; Mandrake now realizes that Ripper is insane.
At the Pentagon, General Buck Turgidson (George C. Scott) briefs President Merkin Muffley (also Peter Sellers) and other officers and aides about the attack in the "War Room". President Muffley is shocked to learn that such orders could be given without his authorization, but Turgidson reminds him that Plan R, enabling a senior officer to launch a strike against the Soviets if all superiors have been killed in a first strike on Washington, DC, allows such an action.
Turgidson reports that his men are trying every possible three-letter CRM code to issue the stand-down order, but that could take over two days and the planes are due to reach their targets in about an hour. Muffley orders the Army chief to storm the base and arrest General Ripper.
Turgidson attempts to convince Muffley to let the attack continue and use the element of surprise to annihilate the Soviet military altogether before they can strike back; Muffley, however, refuses to be party to a nuclear first strike. Instead, he brings Soviet ambassador Alexei de Sadeski (Peter Bull) into the War Room, to telephone Soviet premier Dimitri Kissov on the "hot line". Muffley warns the Premier of the impending attack and offers to reveal the planes' positions and targets so the Russians can protect themselves.
After a heated discussion in Russian with the Premier, the ambassador informs President Muffley that the Soviet Union has created a doomsday device, which consists of many buried bombs jacketed with "Cobalt-Thorium G" connected to a computer network set to detonate them automatically should any nuclear attack strike the country. Within two months after detonation, the Cobalt-Thorium G would encircle the earth in a radioactive cloud, wiping out all human and animal life, rendering the surface of the earth uninhabitable for 93 years. The device cannot be dismantled or "untriggered", as it is programmed to explode if any such attempt is made.
When the President's wheelchair-bound scientific advisor, former Nazi Dr. Strangelove (again, Peter Sellers), points out that such a doomsday device would only be an effective deterrent if everyone knew about it, de Sadeski replies that the Russian Premier had planned to reveal its existence to the world the following week (on the leader's birthday).
Meanwhile, United States Army forces arrive at Burpelson, which is still sealed by General Ripper's order. A bloody battle ensues, and the Army forces eventually take over the base. Ripper kills himself, fearing he will be tortured into revealing the recall code. A soldier named Colonel "Bat" Guano (Keenan Wynn) forces his way into Ripper's office, where Mandrake identifies Ripper's CRM code from his desk blotter ("OPE," a variant of both "Peace on Earth" and "Purity of Essence").
Mandrake relays this code to the Pentagon with difficulty via payphone, the only working method of communication. Using the recall code, SAC successfully recalls most of the aircraft; however, Muffley learns that a surface-to-air missile has ruptured the fuel tank of Major Kong's plane and destroyed its communications device, making it impossible to recall this particular plane even with the correct recall code.
Muffley tells the Soviets the plane's target to help them find it but does not realize that because of the shortened range of the crippled aircraft, Major Kong has selected a closer target. As the plane approaches the new target, the crew is unable to open the damaged bomb bay doors. Major Kong crimps broken electric wiring, whereupon the doors open. With Kong straddling it, the nuclear bomb falls and detonates, triggering the doomsday machine.
Dr. Strangelove recommends that the President gather several hundred thousand people to live in deep mineshafts where the radiation will not penetrate. He suggests a 10:1 female-to-male ratio for a breeding program to repopulate the Earth when the radiation has subsided. Turgidson, worried that the Soviets will do the same, warns about a "mineshaft gap" when the wheelchair-bound Strangelove suddenly stands and says, "Mein Führer, I can walk!" The film then cuts to a montage of nuclear detonations, accompanied by Vera Lynn's recording of "We'll Meet Again".
Cast and characters.
Peter Sellers's multiple roles.
Columbia Pictures agreed to finance the film if Peter Sellers played at least four major roles. The condition stemmed from the studio's opinion that much of the success of Kubrick's previous film "Lolita" (1962) was based on Sellers's performance in which his single character assumes a number of identities. Sellers had also played three roles in "The Mouse That Roared" (1959). Kubrick accepted the demand, later explaining that "such crass and grotesque stipulations are the "sine qua non" of the motion-picture business."
Sellers ended up playing three of the four roles written for him. He had been expected to play Air Force Major T. J. "King" Kong, the B-52 Stratofortress aircraft commander, but from the beginning, Sellers was reluctant. He felt his workload was too heavy and he worried he would not properly portray the character's Texas accent. Kubrick pleaded with him and asked screenwriter Terry Southern (who had been raised in Texas) to record a tape with Kong's lines spoken in the correct accent. Using Southern's tape, Sellers managed to get the accent right and started shooting the scenes in the airplane but then Sellers sprained an ankle and could not work in the cramped cockpit set.
Sellers is said to have improvised much of his dialogue, with Kubrick incorporating the ad-libs into the written screenplay so the improvised lines became part of the canonical screenplay, a practice known as retroscripting. 
According to film critic Alexander Walker, the author of biographies of both Sellers and Kubrick, the role of Group Captain Lionel Mandrake was the easiest of the three for Sellers to play, as he was aided by his experience of mimicking his superiors while serving in the RAF during World War II. There is also a heavy resemblance to Sellers's friend and occasional co-star Terry-Thomas and prosthetic-limbed RAF ace Douglas Bader.
For his performance as President Merkin Muffley, Sellers flattened his natural English accent to resemble an American Midwesterner. Sellers drew inspiration for the role from Adlai Stevenson, a former Illinois governor who was the Democratic candidate for the 1952 and 1956 presidential elections and the U.N. ambassador during the Cuban Missile Crisis.
In early takes, Sellers faked cold symptoms to emphasize the character's apparent weakness. That caused frequent laughter among the film crew, ruining several takes. Kubrick ultimately found this comic portrayal inappropriate, feeling that Muffley should be a serious character. In later takes Sellers played the role straight, though the President's cold is still evident in several scenes.
In keeping with Kubrick's satirical character names, a "merkin" is a pubic hair wig. The president is bald, and his last name is "Muffley"; both are additional homages to a merkin.
Dr. Strangelove is an ex-Nazi scientist, suggesting Operation Paperclip, the US effort to recruit top German technical talent at the end of World War II. He serves as President Muffley's scientific adviser in the War Room. When General Turgidson wonders aloud what kind of name "Strangelove" is, saying to Mr. Staines (Jack Creley) that it is not a "Kraut name," Staines responds that Strangelove's original German surname was "Merkwürdigliebe," without mentioning that "Merkwürdigliebe" translates to "Strangelove" in English. Twice in the film, Strangelove "accidentally" addresses the president as "Mein Führer". Dr. Strangelove did not appear in the book "Red Alert".
The character is an amalgamation of RAND Corporation strategist Herman Kahn, mathematician and Manhattan Project principal John von Neumann, rocket scientist Wernher von Braun (a central figure in Nazi Germany's rocket development program recruited to the US after the war), and Edward Teller, the "father of the hydrogen bomb." There is a common misconception that the character was based on Henry Kissinger, but Kubrick and Sellers denied this; Sellers said, "Strangelove was never modeled after Kissinger—that's a popular misconception. It was always Wernher Von Braun."
The wheelchair-bound Strangelove furthers a Kubrick trope of the menacing, seated antagonist, first depicted in "Lolita" through the character "Dr. Zaempf." Strangelove's accent was influenced by that of Austrian-American photographer Weegee, who worked for Kubrick as a special photographic effects consultant. Strangelove's appearance echoes the mad scientist archetype as seen in the character Rotwang in Fritz Lang's film "Metropolis" (1927). Sellers's Strangelove takes from Rotwang the single black gloved hand (which, in Rotwang's case is mechanical, because of a lab accident), the wild hair and, most importantly, his ability to avoid being controlled by political power. According to film critic Alexander Walker, Sellers improvised Dr. Strangelove's lapse into the Nazi salute, borrowing one of Kubrick's black leather gloves for the uncontrollable hand that makes the gesture. Dr. Strangelove apparently suffers from diagnostic apraxia (alien hand syndrome). Kubrick wore the gloves on the set to avoid being burned when handling hot lights, and Sellers, recognizing the potential connection to Lang's work, found them to be menacing.
Slim Pickens as Major T. J. "King" Kong.
Slim Pickens, an established character actor and veteran of many Western films, was eventually chosen to replace Sellers as Major Kong after Sellers's injury. Terry Southern's biographer, Lee Hill, said the part was originally written with John Wayne in mind, and that Wayne was offered the role after Sellers was injured but he immediately turned it down. Dan Blocker of the "Bonanza" western television series was approached to play the part, but according to Southern, Blocker's agent rejected the script as being "too pinko". Kubrick then recruited Pickens, whom he knew from Pickens's work in Marlon Brando's "One-Eyed Jacks".
Fellow actor James Earl Jones recalls, "He was Major Kong on and off the set—he didn't change a thing—his temperament, his language, his behavior." Pickens was not told that the movie was a comedy and was only given the script for scenes he was in, to get him to play it "straight".
Kubrick biographer John Baxter explains, in the documentary "Inside the Making of Dr. Strangelove":As it turns out, Slim Pickens had never left the United States. He had to hurry and get his first passport. He arrived on the set, and somebody said, "Gosh, he's arrived in costume!", not realizing that that's how he always dressed ... with the cowboy hat and the fringed jacket and the cowboy boots—and that he wasn't putting on the character—that's the way he talked.
Pickens, who had previously played only minor supporting and character roles, said his appearance as Maj. Kong greatly improved his career. He later commented, "After "Dr. Strangelove" the roles, the dressing rooms and the checks all started getting bigger."
George C. Scott as General Buck Turgidson.
Kubrick tricked Scott into playing the role of Gen. Turgidson far more ridiculously than Scott was comfortable doing. Kubrick talked Scott into doing over the top "practice" takes, which Kubrick told Scott would never be used, as a way to warm up for the "real" takes. Kubrick used these takes in the final film, causing Scott to swear never to work with Kubrick again.
During the filming, Kubrick and Scott had different opinions regarding certain scenes, but Kubrick got Scott to conform largely by repeatedly beating him at chess, which they played frequently on the set. Scott, a skilled player himself, later said that while he and Kubrick may not have always seen eye to eye, he respected Kubrick immensely for his skill at chess.
Production.
Novel and screenplay.
Stanley Kubrick started with nothing but a vague idea to make a thriller about a nuclear accident that built on the widespread Cold War fear for survival. While doing research, Kubrick gradually became aware of the subtle and paradoxical "balance of terror" between nuclear powers. At Kubrick's request, Alastair Buchan (the head of the Institute for Strategic Studies) recommended the thriller novel "Red Alert" by Peter George. Kubrick was impressed with the book, which had also been praised by game theorist and future Nobel Prize in Economics winner Thomas Schelling in an article written for the "Bulletin of the Atomic Scientists" and reprinted in "The Observer," and immediately bought the film rights. In 2006, Schelling wrote that conversations between Kubrick, Schelling, and George in late 1960 about a treatment of "Red Alert" updated with intercontinental missiles eventually led to the making of the film.
In collaboration with George, Kubrick started writing a screenplay based on the book. While writing the screenplay, they benefited from some brief consultations with Schelling and, later, Herman Kahn. In following the tone of the book, Kubrick originally intended to film the story as a serious drama. However, as he later explained during interviews, he began to see comedy inherent in the idea of mutual assured destruction as he wrote the first draft. Kubrick said:My idea of doing it as a nightmare comedy came in the early weeks of working on the screenplay. I found that in trying to put meat on the bones and to imagine the scenes fully, one had to keep leaving out of it things which were either absurd or paradoxical, in order to keep it from being funny; and these things seemed to be close to the heart of the scenes in question.
Among the titles that Kubrick considered for the film were "Dr. Doomsday or: How to Start World War III Without Even Trying", "Dr. Strangelove's Secret Uses of Uranus", and "Wonderful Bomb". After deciding to make the film a black comedy, Kubrick brought in Terry Southern as a co-writer. The choice was influenced by reading Southern's comic novel "The Magic Christian", which Kubrick had received as a gift from Peter Sellers, and which itself became a Sellers film in 1969.
Sets and filming.
"Dr. Strangelove" was filmed at Shepperton Studios, near London, as Sellers was in the middle of a divorce at the time and unable to leave England. The sets occupied three main sound stages: the Pentagon War Room, the B-52 Stratofortress bomber and the last one containing both the motel room and General Ripper's office and outside corridor. The studio's buildings were also used as the Air Force base exterior. The film's set design was done by Ken Adam, the production designer of several "James Bond" films (at the time he had already worked on "Dr. No"). The black and white cinematography was by Gilbert Taylor, and the film was edited by Anthony Harvey and Stanley Kubrick (uncredited). The original musical score for the film was composed by Laurie Johnson and the special effects were by Wally Veevers. The theme of the chorus from the bomb run scene is a modification of "When Johnny Comes Marching Home". Sellers and Kubrick got on famously during the film's production and shared a love of photography.
For the War Room, Ken Adam first designed a two-level set which Kubrick initially liked, only to decide later that it was not what he wanted. Adam next began work on the design that was used in the film, an expressionist set that was compared with "The Cabinet of Dr. Caligari" and Fritz Lang's "Metropolis". It was an enormous concrete room ( long and wide, with a -high ceiling) suggesting a bomb shelter, with a triangular shape (based on Kubrick's idea that this particular shape would prove the most resistant against an explosion). One side of the room was covered with gigantic strategic maps reflecting in a shiny black floor inspired by the dance scenes in old Fred Astaire films. In the middle of the room there was a large circular table lit from above by a circle of lamps, suggesting a poker table. Kubrick insisted that the table would be covered with green baize (although this could not be seen in the black and white film) to reinforce the actors' impression that they are playing 'a game of poker for the fate of the world.' Kubrick asked Adam to build the set ceiling in concrete to force the director of photography to use only the on-set lights from the circle of lamps. Moreover, each lamp in the circle of lights was carefully placed and tested until Kubrick was happy with the result.
Lacking cooperation from the Pentagon in the making of the film, the set designers reconstructed the aircraft cockpit to the best of their ability by comparing the cockpit of a B-29 Superfortress and a single photograph of the cockpit of a B-52 and relating this to the geometry of the B-52's fuselage. The B-52 was state-of-the-art in the 1960s, and its cockpit was off-limits to the film crew. When some United States Air Force personnel were invited to view the reconstructed B-52 cockpit, they said that "it was absolutely correct, even to the little black box which was the CRM." It was so accurate that Kubrick was concerned whether Ken Adam's production design team had done all of their research legally, fearing a possible investigation by the FBI.
In several shots of the B-52 flying over the polar ice en route to Russia, the shadow of the actual camera plane, a Boeing B-17 Flying Fortress, is visible on the snow below. The B-52 was a scale model composited into the Arctic footage, which was sped up to create a sense of jet speed. Home movie footage included in "Inside the Making of Dr. Strangelove" on the 2001 Special Edition DVD release of the film shows clips of the B-17 with a cursive "Dr. Strangelove" painted over the rear entry hatch on the right side of the fuselage.
"Fail Safe".
"Red Alert" author Peter George collaborated on the screenplay with Kubrick and satirist Terry Southern. "Red Alert" was more solemn than its film version and did not include the character Dr. Strangelove though the main plot and technical elements were quite similar. A novelization of the actual film, rather than a reprint of the original novel, was published by George, based on an early draft in which the narrative is bookended by the account of aliens, who, having arrived at a desolated Earth, try to piece together what has happened. It is to be reissued in December 2014 by Candy Jar Books, featuring never-before-published material on Strangelove's early career.
During the filming of "Dr. Strangelove", Stanley Kubrick learned that "Fail Safe", a film with a similar theme, was being produced. Although "Fail Safe" was to be an ultrarealistic thriller, Kubrick feared that its plot resemblance would damage his film's box office potential, especially if it were released first. Indeed, the novel "Fail-Safe" (on which the film is based) is so similar to "Red Alert" that Peter George sued on charges of plagiarism and settled out of court. What worried Kubrick most was that "Fail Safe" boasted acclaimed director Sidney Lumet and first-rate dramatic actors Henry Fonda as the American President and Walter Matthau as the advisor to the Pentagon, Professor Groeteschele. Kubrick decided to throw a legal wrench into "Fail Safe"'s production gears. Lumet recalled in the documentary, "Inside the Making of Dr. Strangelove": "We started casting. Fonda was already set ... which of course meant a big commitment in terms of money. I was set, Walter ernstein, the screenwrite was set ... And suddenly, this lawsuit arrived, filed by Stanley Kubrick and Columbia Pictures."
Kubrick argued that "Fail Safe"'s own 1960 source novel "Fail-Safe" had been plagiarized from Peter George's "Red Alert", to which Kubrick owned creative rights and pointed out unmistakable similarities in intentions between the characters Groeteschele and Strangelove. The plan worked, and "Fail Safe" opened eight months behind "Dr. Strangelove", to critical acclaim but mediocre ticket sales.
Ending.
The end of the film shows Dr. Strangelove exclaiming, ""Mein Führer," I can walk!" before cutting to footage of nuclear explosions, with Vera Lynn singing "We'll Meet Again." This footage comes from nuclear tests such as shot BAKER of Operation Crossroads at Bikini Atoll, the Trinity test, a test from Operation Sandstone and the hydrogen bomb tests from Operation Redwing and Operation Ivy. In some shots, old warships (such as the German heavy cruiser "Prinz Eugen"), which were used as targets, are plainly visible. In others, the smoke trails of rockets used to create a calibration backdrop can be seen.
Former "Goon Show" writer, and friend of Sellers, Spike Milligan, was credited with suggesting the Vera Lynn music for the ending. (Additionally, Sellers' ad-libbed dialogue as the President on the phone with the Russian premier is drawn from a skit between him and Milligan in the "Goon Show" episode "The Lost Emperor".)
Original ending: the pie fight.
It was originally planned for the film to end with a scene that was filmed, with everyone in the war room involved in a pie fight.
Accounts vary as to why the pie fight was cut. In a 1969 interview, Kubrick said, "I decided it was farce and not consistent with the satiric tone of the rest of the film." Critic Alexander Walker observed that "the cream pies were flying around so thickly that people lost definition, and you couldn't really say whom you were looking at." Nile Southern, son of screenwriter Terry Southern, suggested the fight was intended to be less jovial: "Since they were laughing, it was unusable, because instead of having that totally black, which would have been amazing, like, this blizzard, which in a sense is metaphorical for all of the missiles that are coming, as well, you just have these guys having a good old time. So, as Kubrick later said, 'it was a disaster of Homeric proportions.'"
Effects of the Kennedy assassination on the film.
A first test screening of the film was scheduled for November 22, 1963, the day of the John F. Kennedy assassination. The film was just weeks from its scheduled premiere but because of the assassination, the release was delayed until late January 1964, as it was felt that the public was in no mood for such a film any sooner.
One line by Slim Pickens, "a fella could have a pretty good weekend in Dallas with all that stuff", was dubbed to change "Dallas" to "Vegas" since Dallas was the city in which Kennedy was killed. The original reference to Dallas survives in the French-subtitled version of the film.
The assassination also serves as another possible reason that the pie-fight scene was cut. In the scene, after Muffley takes a pie in the face, General Turgidson exclaims: "Gentlemen! Our gallant young president has been struck down in his prime!" Editor Anthony Harvey stated that the scene "would have stayed, except that Columbia Pictures were horrified, and thought it would offend the president's family."
Kubrick and others have said that the scene had been cut earlier because it was not consistent with the rest of the film.
1994 re-release.
In 1994 the film was re-released. While the 1964 release used the 1.85:1 aspect ratio, the new print was in the slightly squarer 1.66:1 (5:3) ratio that Kubrick had originally intended.
Themes.
Satirizing the Cold War.
"Dr. Strangelove" takes passing shots at numerous Cold War attitudes, such as the "missile gap", but it primarily focuses its satire on the theory of mutual assured destruction (MAD), in which each side is supposed to be deterred from a nuclear war by the prospect of a universal cataclysmic disaster regardless of who "won". Military strategist and former physicist Herman Kahn, in his 1960 "On Thermonuclear War," used the theoretical example of a doomsday machine to illustrate the limitations of MAD, which was developed by John von Neumann. He merely meant that the MAD doctrine should not be pushed to extremes. It worried him that the military might like the idea of a doomsday machine and build one. Kahn, a leading critic of MAD, urged America to plan for a limited nuclear war and later became one of the architects of a doctrine that superficially resembled MAD, but allowed for limited nuclear warfare. He actually consulted with Kubrick on the doomsday machine concept for the film. Kahn came over as cold and calculating, for example, in his willingness to estimate how many human lives the United States could lose and still rebuild economically, but it was unfair, as he was not really advocating nuclear warfare. (He simply meant that if it came to nuclear war, there might, in fact, be a limited one, and options should be kept open.) Kahn's cold analytical attitude towards millions of deaths is reflected in Turgidson's remark to the president about the outcome of a preemptive nuclear war: "Mr. President, I'm not saying we wouldn't get our hair mussed. But I do say no more than ten to twenty million killed, tops, uh, depending on the breaks." Turgidson has a binder that is labelled "World Targets in Megadeaths", a term coined in 1953 by Kahn and popularized in his 1960 book "On Thermonuclear War".
The plan to regenerate the human race from the people sheltered in mineshafts is a parody of Nelson Rockefeller, Edward Teller, Herman Kahn, and Chet Holifield's 1961 plan to spend billions of dollars on a nationwide network of concrete-lined underground fallout shelters capable of holding millions of people.
The proposed fallout shelter network has similarities and contrasts to that of the very real and robust Swiss civil defense network. Switzerland has an overcapacity of nuclear fallout shelters for the country's population size, and by law, new homes must still be built with a fallout shelter. If the US did that, it would violate the spirit of MAD and destabilize the situation because the US could launch a first strike and be safe against a retaliatory second strike. See MAD--Theory section.
To refute early 1960s novels and Hollywood films like "Fail-Safe" and "Dr. Strangelove", which raised questions about US control over nuclear weapons, the Air Force produced a documentary film, "SAC Command Post", to demonstrate its responsiveness to presidential command and its tight control over nuclear weapons.
Sexual themes.
In the months following the film's release director Stanley Kubrick received a fan letter from Legrace G. Benson of the Department of History of Art at Cornell University interpreting the film as being sexually-layered. The director wrote back to Benson and confirmed the interpretation, "Seriously, you are the first one who seems to have noticed the sexual framework from intromission (the planes going in) to the last spasm (Kong's ride down and detonation at target)."
Sexual metaphors often popped up when the nuclear analysts that Kubrick consulted were discussing strategy, such as when Bernard Brodie compared his not attacking cities/withhold plan following belligerent escalation to coitus interruptus in an internally circulated memorandum at the RAND Corporation, while he described the SAC plan of massive retaliation as "going all the way". That led RAND scholar Herman Kahn, whom Kubrick consulted, to quip to an assembled group of "massive retaliation" SAC officers, "Gentlemen, you do not have a war plan. You have a Wargasm!".
Reception.
The film was a popular success, earning US$4,420,000 in rentals in North America during its initial theatrical release.
It was selected for preservation in the United States National Film Registry. In 2000, readers of "Total Film" magazine voted it the 24th greatest comedic film of all time. It holds a 99% rating on Rotten Tomatoes based on 68 reviews. It is ranked number 7 in the All-Time High Scores chart of Metacritic's Video/DVD section with an average score of 96. It is also listed as number 26 on "Empire's 500 Greatest Movies of All Time".
"Dr. Strangelove" is on Roger Ebert's list of The Great Movies, described as "arguably the best political satire of the century." It was also rated as the fifth greatest film in the 2002 "Sight & Sound's" directors' poll, the only comedy in the top ten.
Awards and honors.
The film was nominated for four Academy Awards and also seven BAFTA Awards, of which it won four.
In addition, the film won the best written American comedy award from the Writers Guild of America, a Hugo Award for Best Dramatic Presentation, and the Grand Prix of the Belgian Film Critics Association.
Kubrick won two awards for best director, from the New York Film Critics Circle and the Italian National Syndicate of Film Journalists, and was nominated for one by the Directors Guild of America.
The film ranked #32 on TV Guide's list of the 50 Greatest Movies on TV (and Video).
Potential sequel.
In 1995 Kubrick enlisted Terry Southern to script a sequel titled "Son of Strangelove". Kubrick had Terry Gilliam in mind to direct. The script was never completed, but index cards laying out the story's basic structure were found among Southern's papers after his October 1995 death; it was set largely in underground bunkers, where Dr. Strangelove had taken refuge with a group of women.
In 2013 Gilliam commented, "I was told after Kubrick died—by someone who had been dealing with him—that he had been interested in trying to do another "Strangelove" with me directing. I never knew about that until after he died but I would have loved to."

</doc>
<doc id="8697" url="https://en.wikipedia.org/wiki?curid=8697" title="DNA ligase">
DNA ligase

In molecular biology, DNA ligase is a specific type of enzyme, a ligase, () that facilitates the joining of DNA strands together by catalyzing the formation of a phosphodiester bond. It plays a role in repairing single-strand breaks in duplex DNA in living organisms, but some forms (such as DNA ligase IV) may specifically repair double-strand breaks (i.e. a break in both complementary strands of DNA). Single-strand breaks are repaired by DNA ligase using the complementary strand of the double helix as a template, with DNA ligase creating the final phosphodiester bond to fully repair the DNA.
DNA ligase is used in both DNA repair and DNA replication (see "Mammalian ligases"). In addition, DNA ligase has extensive use in molecular biology laboratories for recombinant DNA experiments (see "Applications in molecular biology research"). Purified DNA ligase is used in gene cloning to join DNA molecules together to form recombinant DNA.
Ligase mechanism.
The mechanism of DNA ligase is to form two covalent phosphodiester bonds between 3' hydroxyl ends of one nucleotide, ("acceptor") with the 5' phosphate end of another ("donor"). ATP is required for the ligase reaction, which proceeds in three steps: 
Ligase will also work with blunt ends, although higher enzyme concentrations and different reaction conditions are required.
Types of ligases.
"E. coli" DNA ligase.
The "E. coli" DNA ligase is encoded by the "lig" gene. DNA ligase in "E. coli", as well as most prokaryotes, uses energy gained by cleaving nicotinamide adenine dinucleotide (NAD) to create the phosphodiester bond. It does not ligate blunt-ended DNA except under conditions of molecular crowding with polyethylene glycol, and cannot join RNA to DNA efficiently.
T4 DNA ligase.
The DNA ligase from bacteriophage T4 is the ligase most-commonly used in laboratory research. It can ligate cohesive or "sticky" ends of DNA, oligonucleotides, as well as RNA and RNA-DNA hybrids, but not single-stranded nucleic acids. It can also ligate blunt-ended DNA with much greater efficiency than "E. coli" DNA ligase. Unlike "E. coli" DNA ligase, T4 DNA ligase cannot utilize NAD and it has an absolute requirement for ATP as a cofactor. Some engineering has been done to improve the "in vitro" activity of T4 DNA ligase; one successful approach, for example, tested T4 DNA ligase fused to several alternative DNA binding proteins and found that the constructs with either p50 or NF-kB as fusion partners were over 160% more active in blunt-end ligations for cloning purposes than wild type T4 DNA ligase.
Mammalian ligases.
In mammals, there are four specific types of ligase.
DNA ligase from eukaryotes and some microbes uses adenosine triphosphate (ATP) rather than NAD.
Thermostable ligases.
Ligases from various thermophilic bacteria have been cloned and sequenced, and are available commercially for use in ligase amplification reaction because of their thermostable properties.
Measurement of ligase activity.
There are at least three different units used to measure the activity of DNA ligase:
Applications in molecular biology research.
DNA ligases have become indispensable tools in modern molecular biology research for generating recombinant DNA sequences. For example, DNA ligases are used with restriction enzymes to insert DNA fragments, often genes, into plasmids.
Controlling the optimal temperature is a vital aspect of performing efficient recombination experiments involving the ligation of cohesive-ended fragments. Most experiments use T4 DNA Ligase (isolated from bacteriophage T4), which is most active at 37°C. However, for optimal ligation efficiency with cohesive-ended fragments ("sticky ends"), the optimal enzyme temperature needs to be balanced with the melting temperature T of the sticky ends being ligated, the homologous pairing of the sticky ends will not be stable because the high temperature disrupts hydrogen bonding. A ligation reaction is most efficient when the sticky ends are already stably annealed, and disruption of the annealing ends would therefore result in low ligation efficiency. The shorter the overhang, the lower the T.
Since blunt-ended DNA fragments have no cohesive ends to anneal, the melting temperature is not a factor to consider within the normal temperature range of the ligation reaction. However, the higher the temperature, the lower the chance that the ends to be joined will be aligned to allow for ligation (molecules move around the solution more at higher temperatures). The limiting factor in blunt end ligation is not the activity of the ligase but rather the number of alignments between DNA fragment ends that occur. The most efficient ligation temperature for blunt-ended DNA would therefore be the temperature at which the greatest number of alignments can occur. The majority of blunt-ended ligations are carried out at 14-25°C overnight. The absence of stably annealed ends also means that the ligation efficiency is lowered, requiring a higher ligase concentration to be used.
History.
The first DNA ligase was purified and characterized in 1967. The common commercially available DNA ligases were originally discovered in bacteriophage T4, "E. coli" and other bacteria.

</doc>
<doc id="8699" url="https://en.wikipedia.org/wiki?curid=8699" title="Dewey Decimal Classification">
Dewey Decimal Classification

The Dewey Decimal Classification (DDC), or Dewey Decimal System, is a proprietary library classification system first published in the United States by Melvil Dewey in 1876. It has been revised and expanded through 23 major editions, the latest issued in 2011, and has grown from a four-page pamphlet in 1876 with fewer than 1,000 classes to a four volume set. It is also available in an abridged version suitable for smaller libraries. It is currently maintained by the Online Computer Library Center (OCLC), a non-profit cooperative that serves libraries. OCLC licenses access to an online version for catalogers called "WebDewey".
The Decimal Classification introduced the concepts of "relative location" and "relative index" which allow new books to be added to a library in their appropriate location based on subject. Libraries previously had given books permanent shelf locations that were related to the order of acquisition rather than topic. The classification's notation makes use of three-digit Arabic numerals for main classes, with fractional decimals allowing expansion for further detail. A library assigns a classification number that unambiguously locates a particular volume in a position relative to other books in the library, on the basis of its subject. The number makes it possible to find any book and to return it to its proper place on the library shelves. The classification system is used in 200,000 libraries in at least 135 countries.
The major competing classification system to the Dewey Decimal system is the Library of Congress Classification system created by the U.S. Library of Congress.
History.
Early development (1873–1885).
Melvil Dewey (1851–1931) was an American librarian and self-declared reformer. He is best known for the Decimal System that he created, but he also was a founding member of the American Library Association and can be credited with the promotion of card systems in libraries and business. He developed the ideas for his library classification system in 1873 while working at Amherst College library. He applied the classification to the books in that library, until in 1876 he had a first version of the classification. In 1876, he published the classification in pamphlet form with the title "A Classification and Subject Index for Cataloguing and Arranging the Books and Pamphlets of a Library."
He used the pamphlet, published in more than one version during the year, to solicit comments from other librarians. It is not known who received copies or how many commented as only one copy with comments has survived, that of Ernest Cushing Richardson. His classification system was mentioned in an article in the first issue of the "Library Journal" and in an article by Dewey in the Department of Education publication "Public Libraries in America" in 1876. In March 1876, he applied for, and received copyright on the first edition of the index. The edition was 44 pages in length, with 2,000 index entries, and was printed in 200 copies.
Period of adoption (1885–1942).
The second edition of the Dewey Decimal system, published in 1885 with the title "Decimal Classification and Relativ Index for arranging, cataloging, and indexing public and private libraries and for pamflets, clippings, notes, scrap books, index rerums, etc.", comprised 314 pages, with 10,000 index entries. 500 copies were produced. Editions 3–14, published between 1888 and 1942, used a variant of this same title. Dewey modified and expanded his system considerably for the second edition. In an introduction to that edition Dewey states that "nearly 100 persons hav contributed criticisms and suggestions".
One of the innovations of the Dewey Decimal system was that of positioning books on the shelves in relation to other books on similar topics. When the system was first introduced, most libraries in the US used fixed positioning: each book was assigned a permanent shelf position based on the book's height and date of acquisition. Library stacks were generally closed to all but the most privileged patrons, so shelf browsing was not considered of importance. The use of the Dewey Decimal system increased during the early 20th century as librarians were convinced of the advantages of relative positioning and of open shelf access for patrons.
New editions were readied as supplies of previously published editions were exhausted, even though some editions provided little change from the previous, as they were primarily needed to fulfill demand. In the next decade, three editions followed closely on: the 3rd (1888), 4th (1891), and 5th (1894). Editions 6 through 11 were published from 1899 to 1922. The 6th edition was published in a record 7,600 copies, although subsequent editions were much lower. During this time, the size of the volume grew, and edition 12 swelled to 1243 pages, an increase of 25% over the previous edition.
In response to the needs of smaller libraries who were finding the expanded classification schedules difficult to use, in 1894, the first abridged edition of the Dewey Decimal system was produced. The abridged edition generally parallels the full edition, and has been developed for most full editions since that date. By popular request, in 1930, the Library of Congress began to print Dewey Classification numbers on nearly all of its cards, thus making the system immediately available to all libraries making use of the Library of Congress card sets.
Dewey's was not the only library classification available, although it was the most complete. Charles Ammi Cutter published the Expansive Classification in 1882, with initial encouragement from Melvil Dewey. Cutter's system was not adopted by many libraries, with one major exception: it was used as the basis for the Library of Congress Classification system.
In 1895, the International Institute of Bibliography, located in Belgium and led by Paul Otlet, contacted Dewey about the possibility of translating the classification into French, and using the classification system for bibliographies (as opposed to its use for books in libraries). This would have required some changes to the classification, which was under copyright. Dewey gave permission for the creation of a version intended for bibliographies, and also for its translation into French. Dewey did not agree, however, to allow the International Institute of Bibliography to later create an English version of the resulting classification, considering that a violation of their agreement, as well as a violation of Dewey's copyright. Shortly after Dewey's death in 1931, however, an agreement was reached between the committee overseeing the development of the Decimal Classification and the developers of the French "Classification Decimal". The English version was published as the Universal Decimal Classification and is still in use today.
According to a study done in 1927, the Dewey system was used in the US in approximately 96% of responding public libraries and 89% of the college libraries. After the death of Melvil Dewey in 1931, administration of the classification was under the Decimal Classification Committee of the Lake Placid Club Education Foundation, and the editorial body was the Decimal Classification Editorial Policy Committee with participation of the American Library Association (ALA), Library of Congress, and Forest Press. By the 14th edition in 1942, the Dewey Decimal Classification index was over 1,900 pages in length and was published in two volumes.
Forging an identity (1942–present).
The growth of the classification to date had led to significant criticism from medium and large libraries which were too large to use the abridged edition but found the full classification overwhelming. Dewey had intended issuing the classification in three editions: the library edition, which would be the fullest edition; the bibliographic edition, in English and French, which was to be used for the organization of bibliographies rather than of books on the shelf; and the abridged edition. In 1933, the bibliographic edition became the Universal Decimal Classification, which left the library and abridged versions as the formal Dewey Decimal Classification editions. The 15th edition, edited by Milton Ferguson, implemented the growing concept of the "standard edition", designed for the majority of general libraries but not attempting to satisfy the needs of the very largest or of special libraries. It also reduced the size of the Dewey system by over half, from 1,900 to 700 pages, a revision so radical that Ferguson was removed from the editorship for the next edition. The 16th and 17th editions, under the editorship of the Library of Congress, grew again to two volumes. However, by now, the Dewey Decimal system had established itself as a classification for general libraries, with the Library of Congress Classification having gained acceptance for large research libraries.
The first electronic version of "Dewey" was created in 1993. Hard-copy editions continue to be issued at intervals; the online WebDewey and Abridged WebDewey are updated quarterly.
Administration and publication.
Administratively, the very early editions were managed by Dewey and a small editorial staff. Beginning in 1922, administrative affairs were managed by the Lake Placid Club Educational Foundation, a not-for-profit organization founded by Melvil Dewey. The ALA created a Special Advisory Committee on the Decimal Classification as part of the Cataloging and Classification division of ALA, in 1952. The previous Decimal Classification Committee was changed to the Decimal Classification Editorial Policy Committee, with participation of the ALA Division of Cataloging and Classification, and the Library of Congress.
Melvil Dewey edited the first three editions of the classification system and oversaw the revisions of all editions until his death in 1931. May Seymour became editor in 1891, until her death in 1921. She was followed by Dorcas Fellows, who was editor until her death in 1938. Constantin J. Mazney edited the 14th edition. Milton Ferguson was editor from 1949 to 1951. The 16th edition in 1958 was edited under an agreement between the Library of Congress and Forest Press, with David Haykin as director. Editions 16-19 were edited by Benjamin A. Custer and the editor of edition 20 was John P. Comaromi. Joan Mitchell was editor until 2013, covering editions 21-23. The current Editor-in-Chief is Michael Panzer of OCLC.
Copyright in editions 1-6 (1876–1919) was held by Dewey himself. Copyright in editions 7–10 were held by the publisher, The Library Bureau. On the death of May Seymour, Dewey conveyed the "copyryts and control of all editions" to the Lake Placid Club Educational Foundation, a non-profit chartered in 1922. The Online Computer Library Center (OCLC) of Dublin, Ohio, US, acquired the trademark and copyrights associated with the Dewey Decimal Classification system when it bought Forest Press in 1988. In 2003, the Dewey Decimal Classification came to national attention when OCLC sued the Library Hotel for trademark infringement for using the classification system as the hotel theme. The case was settled shortly thereafter.
Since 1988, the classification has been maintained by the OCLC, which also publishes new editions of the system. The editorial staff responsible for updates is based partly at the Library of Congress and partly at OCLC. Their work is reviewed by the Decimal Classification Editorial Policy Committee, a ten-member international board which meets twice each year. The four-volume unabridged edition is published approximately every six years, the most recent edition (DDC 23) in mid-2011. The web edition is updated on an ongoing basis, with changes announced each month. An experimental version of Dewey in RDF is available at dewey.info. This includes access to the top three levels of the classification system in 14 languages.
In addition to the full version, a single volume abridged edition designed for libraries with 20,000 titles or fewer has been made available since 1895. "Abridged 15" was published in early 2012.
Design.
The Dewey Decimal Classification organizes library materials by discipline or field of study. Main divisions include philosophy, social sciences, science, technology, and history. The scheme is made up of ten classes, each divided into ten divisions, each having ten sections. The system's notation uses Arabic numbers, with three whole numbers making up the main classes and sub-classes and decimals creating further divisions. The classification structure is hierarchical and the notation follows the same hierarchy. Libraries not needing the full level of detail of the classification can trim right-most decimal digits from the class number to obtain a more general classification. For example:
The classification was originally enumerative, meaning that it listed all of the classes explicitly in the schedules. Over time it added some aspects of a faceted classification scheme, allowing classifiers to construct a number by combining a class number for a topic with an entry from a separate table. Tables cover commonly used elements such as geographical and temporal aspects, language, and bibliographic forms. For example, a class number could be constructed using 330 for economics + .9 for geographic treatment + .04 for Europe to create the class 330.94 European economy. Or one could combine the class 973 for United States + .05 for periodical publications on the topic to arrive at the number 973.05 for periodicals concerning the United States generally. The classification also makes use of mnemonics in some areas, such that the number 5 represents the country Italy in classification numbers like 945 (history of Italy), 450 (Italian language), 195 (Italian philosophy). The combination of faceting and mnemonics makes the classification "synthetic" in nature, with meaning built into parts of the classification number.
The Dewey Decimal Classification has a number for all subjects, including fiction, although many libraries create a separate fiction section shelved by alphabetical order of the author's surname. Each assigned number consists of two parts: a class number (from the Dewey system) and a book number, which "prevents confusion of different books on the same subject." A common form of the book number is called a Cutter number, which represents the author and distinguishes the book from other books on the same topic.
"Relativ Index".
The "Relativ Index" is an alphabetical index to the classification, for use both by classifiers but also by library users when seeking books by topic. The index was "relative" because the index entries pointed to the class numbers, not to the page numbers of the printed classification schedule. In this way, the Dewey Decimal Classification itself had the same relative positioning as the library shelf and could be used either as an entry point to the classification, by catalogers, or as an index to the Dewey-classed library itself.
Influence and criticism.
Dewey Decimal Classification numbers formed the basis of the Universal Decimal Classification (UDC), which combines the basic Dewey numbers with selected punctuation marks (comma, colon, parentheses, etc.). Adaptations of the system for specific regions outside the English-speaking world include the Korean Decimal Classification, the New Classification Scheme for Chinese Libraries, and the Nippon Decimal Classification (Japanese).
Despite its widespread usage, the classification has been criticized for its complexity and limited scope of scheme-adjustment. In particular, the arrangement of subheadings has been described as archaic and as being biased towards an Anglo-American world view. In 2007–08, the Maricopa County Library District in Arizona, abandoned the DDC in favor of the Book Industry Standards and Communications (BISAC) system, one that is commonly used by commercial bookstores, in an effort to make their libraries more accessible for patrons. Several other libraries across the United States, and other countries (including Canada and The Netherlands) followed suit. The classification has also been criticized as being a proprietary system licensed by a single entity (OCLC), making it expensive to adopt. However, book classification critic Justin Newlan stands by the Dewey Decimal System, stating newer, more advanced book classification systems "are too confusing to understand for newcomers".
It should be noted, however, that BISAC is also a proprietary classification system.

</doc>
<doc id="8702" url="https://en.wikipedia.org/wiki?curid=8702" title="Dukkha">
Dukkha

Dukkha (Pāli; Sanskrit: "duḥkha"; Tibetan: སྡུག་བསྔལ་ "sdug bsngal", pr. "duk-ngel") is a Buddhist term commonly translated as "suffering", "anxiety", "stress", or "unsatisfactoriness". The principle of dukkha is one of the most important concepts in the Buddhist tradition. The Buddha is reputed to have said: "I have taught one thing and one thing only, "dukkha" and the cessation of "dukkha"." The classic formulation of these teachings on "dukkha" is the doctrine of the Four Noble Truths, in which the Truth of Dukkha (Pali: "dukkha saccã"; Sanskrit: "duḥkha-satya") is identified as the first.
"Dukkha" is commonly explained according to three categories:
The Buddhist tradition emphasizes the importance of developing insight into the nature of "dukkha", the conditions that cause it, and how it can be overcome. This process is formulated in the teachings on the Four Noble Truths.
Etymology.
In ordinary usage, the Pali word "dukkha" (Sanskrit "duḥkha") means ‘suffering’, ‘pain’, ‘sorrow’ or ‘misery’, as opposed to the word "sukha" meaning ‘happiness’, ‘comfort’ or ‘ease’. Contemporary scholar Winthrop Sargeant explains the etymological roots of these terms as follows:
Joseph Goldstein explains the etymology as follows:
Nineteenth century translator Monier-Williams states that according to grammatical tradition, "dukkha" is derived from ' "uneasy"; but Monier-Williams asserts that the term is more likely a Prakritized form of ' "unsteady, disquieted".
Centrality to Buddhist thought.
The principle of "dukkha" is one of the most important concepts in the Buddhist tradition. The Buddha is reputed to have said: "I have taught one thing and one thing only, "dukkha" and the cessation of "dukkha"."
Piyadassi Thera states:
The classic formulation of these teachings on "dukkha" is the doctrine of the Four Noble Truths, in which the Truth of Dukkha (Pali: "dukkha saccã"; Sanskrit: "duḥkha-satya") is identified as the first of the four truths.
Neither pessimistic nor optimistic, but realistic.
The central importance of "dukkha" in Buddhist philosophy has caused some observers to consider Buddhism to be a pessimistic philosophy. However, the emphasis on "dukkha" is simply a basic fact of life: clinging to temporary things and states is inherently unsatisfying. Labelling this as "negative" is a refusal to accept the reality of "dukkha".
Some contemporary Buddhist teachers and translators emphasize that while the central message of Buddhism is optimistic, the Buddhist view of our situation in life (the conditions that we live in) is neither pessimistic nor optimistic, but realistic. Walpola Rahula explains the importance of this realistic point of view:
Lama Surya Das emphasizes the matter-of-fact nature of "dukkha":
The Buddha acknowledged that there is both happiness and sorrow in the world, but he taught that even when we have some kind of happiness, it is not permanent; it is subject to change. And due to this unstable, impermanent nature of all things, everything we experience is said to have the quality of "dukkha" or unsatisfactoriness. Therefore unless we can gain insight into that truth, and understand what is really able to provide lasting happiness, and what is unable to provide happiness, the experience of dissatisfaction will persist.
Three patterns.
Within the Buddhist tradition, dukkha is commonly explained according to three patterns or categories:
"Dukkha-dukkha" (the dukkha of painful experiences).
Translations.
The Pali term "dukkha-dukkha" (Sanskrit: "duhkha-duhkhata") is translated as follows:
Description.
This level of dukkha includes:
Joseph Goldstein states:
Geshe Tashi Tsering states:
"Viparinama-dukkha" (the dukkha of the changing nature of all things).
Translations.
The Pali term "viparinama-dukkha" (Sanskrit: "vipariṇāma-duhkhatta") is translated as:
Description.
This level of dukkha includes:
The Tibetan teacher Chogyam Trungpa expands this category to include ""not knowing" what you want."
Joseph Goldstein states:
Geshe Tashi Tsering states:
Relation to impermanence.
This level of dukkha is directly related to the Buddhist concept of impermanence. For example, Geshe Tashi Tsering states that in order to understand this level of dukkha:
Joseph Goldstein emphasizes the importance of reflecting on impermanence:
Goldstein presents five reflections that are practiced on a daily basis within many Buddhist traditions:
"Sankhara-dukkha" (the dukkha of conditioned experience).
Translations.
The Pali term "sankhara-dukkha" (Sanskrit: "samskara-duhkhatta") is referred to as:
Traleg Kyabgon referred to the third type of suffering as "samsara-duhkhatta" (Sanskrit), which he translates as "the suffering of conditioned existence."
Description.
This is the deepest, most subtle level of dukkha; it includes "a basic unsatisfactoriness pervading all existence, all forms of life, because all forms of life are changing, impermanent and without any inner core or substance." On this level, the term indicates a lack of satisfaction, a sense that things never measure up to our expectations or standards.
This subtle form of suffering arises as a reaction to qualities of conditioned things, including the skandhas, the factors constituting the human mind.
Pema Chodron describes this as the suffering of ego-clinging; the suffering of struggling with life as it is, as it presents itself to you; struggling against outer situations and yourself, your own emotions and thoughts, rather than just opening and allowing.
Phillip Moffitt states:
Phillip Moffit relates this level of dukkha with existential angst:
Geshe Tashi Tsering states:
Geshe Tashi Tsering asserts that we will not be free of this level of dukkha "until we are free from samsara, until we are buddhas."
This category ("sankhara-dukkha") is also identified as one of the "eight types of suffering".
Types.
Eight types.
Dukkha can also be categorized into eight types belonging to the three categories of inherited suffering, the suffering between the period of birth and death, and general misery. Chogyam Trunga explains these categories as follows:
Inherited suffering:
Suffering between the periods of birth and death:
General misery:
Six types.
Aung San Suu Kyi presented a list of "six great dukkha" at her Nobel Lecture, delivered on 16 June 2012. These are:
Developing insight into dukkha.
Four Noble Truths.
The Buddhist tradition emphasizes the importance of developing insight into the nature of "dukkha", the conditions that cause it, and how it can be overcome. This process is formulated in the teachings on the Four Noble Truths.
Meditation.
Meditation (Pali: "jhana") is considered to be an essential tool for developing insight into the nature of dukkha. Contemporary Buddhist teacher Ajahn Brahm emphasizes that without the experience of meditation, one's knowledge of the world is too limited to fully understand dukkha. In the following simile, Ajahn Brahm compares the experience of dukkha to being in prison, and compares meditation (Pali: "jhana") to a tunnel that leads out of the prison:
Contemporary scholar Michael Carrithers emphasizes the need to examine one's life. Carrithers asserts that insofar as it is dynamic, ever-changing, uncontrollable and not finally satisfactory, unexamined life is itself precisely dukkha. Carrithers also asserts that the question which underlay the Buddha's quest was "in what may I place lasting relevance?" He did not deny that there are satisfactions in experience: the exercise of vipassana assumes that the meditator sees instances of happiness clearly. Pain is to be seen as pain, and pleasure as pleasure. It is denied that happiness dependent on conditions will be secure and lasting.
Contemporary teacher Chogyam Trungpa presents a perspective on how meditation practice can help the practitioner relate with dukkha; he states:
Compassion.
Developing insight into "dukkha" is said to lead to greater compassion for other beings. For example, Joseph Goldstein states:
Within the discourses.
The Buddha taught on "dukkha" repeatedly throughout his lifetime. In the "Alagadduupama sutta", the Buddha states:
In the "Anuradha Sutta", the Buddha states:
The classic formulation of these teachings on "dukkha" is the doctrine of the Four Noble Truths, in which the Truth of Dukkha (Pali: "dukkha saccã"; Sanskrit: "duḥkha-satya") is identified as the first of the four truths. The Four Noble Truths are presented within the Buddha's first discourse, "Setting in Motion the Wheel of the Dharma (Dharmacakra Pravartana Sūtra)"; in this discourse, the Buddha defines "dukkha" as follows:
In the "Cula-Malunkyovada Sutta", the Buddha states:
In the sutra Samyutta Nikaya #35, the Buddha says:
The "Anapanasati Sutta" and "Mahāsatipaṭṭhāna Sutta" emphasize the importance of the practice of meditation (jnana) to purify the mind of the five hindrances before contemplating the nature "dukkha" in the context of the Four Noble Truths.
Relation to the five skandhas.
According to the Buddhist tradition, the "dukkha of conditioned states" ("saṃkhāra-dukkha") is related to clinging to the "skandhas". Oxford scholar Noa Ronkin presents her understanding of the relation between the "skandhas" (Sanskrit; Pali: "khandhas") and dukkha as follows:
Three marks of existence.
"Dukkha" is also listed among the three marks of existence. These are:
In this context, "dukkha" denotes the experience that all formations ("sankhara") are impermanent (anicca) - thus it explains the qualities which make the mind as fluctuating and impermanent entities. It is therefore also a gateway to anatta, not-self.
Translating the term "dukkha".
Contemporary translators of Buddhist texts use a variety of English words to convey the aspects of "dukkha".
Early Western translators of Buddhist texts (before the 1970s) typically translated the Pali term "dukkha" as "suffering", a translation that tended to convey the impression that Buddhism was a pessimistic or world-denying philosophy. Later translators, however, including Walpola Rahula ("What Buddha Taught", 1974) and nearly all contemporary translators, have emphasized that "suffering" is too limited a translation for the term dukkha, and have preferred to either leave the term untranslated or to clarify that translation with terms such as anxiety, stress, frustration, unease, unsatisfactoriness, etc.
For example, Piyadassi Thera states:
Contemporary scholar Rupert Gethin states:
Contemporary translator Bhikkhu Bodhi states:
Many contemporary teachers, scholars, and translators have used the term "unsatisfactoriness" to emphasize the subtlest aspects of dukkha. For example, contemporary scholar Damien Keown states that in the context of the subtle aspects of dukkha:
The writer Mark Epstein states: 'A more specific translation f the term "dukkha' would be something on the order of “pervasive unsatisfactoriness.” '
In the glossary for his text "Mindfulness: A Practical Guide to Awakening", Joseph Goldstein provides the following definition for "dukkha": "Suffering, unsatisfactoriness, stress".
Many translators prefer to leave the term untranslated. For example, scholar and translator Walpola Rahula states:
Alternate translations.
Contemporary translators have used a variety of English words to translate the term "dukkha"; translators commonly use different words to translate aspects of the term. For example, "dukkha" has been translated as follows in many contexts:
Translations into other languages.
In Chinese Buddhism, "dukkha" is translated as "kǔ" ( "bitterness; hardship; suffering; pain"), and this loanword is pronounced "ku" (苦) in Japanese Buddhism and "ko" (苦) in Korean Buddhism and "khổ" in Vietnamese Buddhism. The Tibetan (phonetic) is "dukngal". In Shan, it is and in Burmese, it is .
Within non-Buddhist literature.
Hinduism.
In Hindu literature, the earliest Upaniads — the and the — are believed to predate or coincide with the advent of Buddhism. In these texts' verses, the Sanskrit word "dukha" (translated below as "suffering" and "distress") occurs only twice. In the , it states (in English and Sanskrit):
In the , it is written:
Thus, as in Buddhism, these texts emphasize that one overcomes "dukha" through the development of a transcendent understanding.
Panetics.
In 1988, the Journal of Humanistic Psychology published an article by Ralph G.H. Siu entitled "Panetics—The Study of the Infliction of Suffering". In the abstract for the article, Sui proposed using the term dukkha as a quantitative measurement; he wrote:
After analyzing the unceasing mutual inflictions of suffering by practically everyone and the neglect of this pervasive and degenerating human deficiency by the academic community, I urge the immediate creation of a new and vigorous academic discipline, called panetics, to be devoted to the study of the infliction of suffering. The nature, scope, illustrative contents, and social value are outlined. The dukkha is proposed as a semiquantitative unit of suffering to assist in associated analytical operations.
Related publications include:
External links.
Online commentaries:
Other sources:

</doc>
<doc id="8703" url="https://en.wikipedia.org/wiki?curid=8703" title="Darwin Awards">
Darwin Awards

The Darwin Awards are a tongue-in-cheek honor, originating in Usenet newsgroup discussions c. 1985. They recognize individuals who have supposedly contributed to human evolution by selecting themselves out of the gene pool via death or sterilization by their own actions.
The project became more formalized with the creation of a website in 1993, and followed up by a series of books starting in 2000, authored by Wendy Northcutt. The criterion for the awards states, "In the spirit of Charles Darwin, the Darwin Awards commemorate individuals who protect our gene pool by making the ultimate sacrifice of their own lives. Darwin Award winners eliminate themselves in an extraordinarily idiotic manner, thereby improving our species' chances of long-term survival."
Accidental self-sterilization also qualifies; however, the site notes: "Of necessity, the award is usually bestowed posthumously." But the candidate is disqualified if "innocent bystanders", who might have contributed positively to the gene pool, are killed in the process.
The Darwin Awards books state that an attempt is made to disallow known urban legends from the awards, but some older "winners" have been "grandfathered" to keep their awards. The Darwin Awards site does try to verify all submitted stories, but many similar sites, and the vast number of circulating "Darwin awards" emails, are largely fictional.
History.
The origin of the Darwin Awards can be traced back to posts on Usenet group discussions as early as 1985. This early post, on August 7, 1985, describes the awards as being, "given posthumously to people who have made the supreme sacrifice to keep their genes out of our pool. Style counts, not everyone who dies from their own stupidity can win." This early post cites an example of a person who pulled a vending machine over their head and was crushed to death trying to break into it. Another widely distributed early story mentioning the Darwin Awards is the JATO Rocket Car, which describes a man who strapped a JATO (Jet-Assisted Take-Off) unit to his Chevrolet Impala in the Arizona desert and who died gloriously on the side of a cliff as his car achieved speeds of 250 to 300 miles per hour. This story was later confirmed to be an urban legend by the Arizona Department of Public Safety. The official Darwin Awards website run by Northcutt does its best to confirm all stories submitted, listing them as, "confirmed true by Darwin." Many of the viral emails circulating the Internet, however, are hoaxes and urban legends.
The website and collection of books were started in 1993 by Wendy Northcutt, who at the time was a graduate in molecular biology from the University of California, Berkeley. She went on to study neurobiology at Stanford University, doing research on cancer and telomerase. In her spare time, she organized chain letters from family members into the original Darwin Awards website hosted in her personal account space at Stanford. She eventually left the bench in 1998 and devoted herself full-time to her website and books in September 1999. By 2002, the website received 7 million page hits per month.
She encountered some difficulty in publishing the first book, since most publishers would only offer her a deal if she agreed to remove the stories from the Internet. Northcutt refused to do so, saying, “It was a community! I could not do that. Even though it might have cost me a lot of money, I kept saying no.” She eventually found a publisher who agreed to print the book containing only 10% of the material. The first book turned out to be a success, and was listed on "The New York Times" bestseller list for six months.
Not all of the feedback from the stories Northcutt published was positive, and she would occasionally receive email from people who knew the deceased. One such person wrote, "This is horrible. It has shocked our community to the core. You should remove this." Nevertheless, Northcutt would keep the stories on the website and in her books, citing them as a "funny-but-true safety guide", and mentioning that children who read the book are going to be a lot more careful around explosives.
The website also recognizes, with Honorable Mentions, individuals who survive their misadventures with their reproductive capacity intact. One example of this is Lawnchair Larry, who attached helium filled weather balloons to a lawn chair and floated far above Long Beach, California, in July 1982. He reached an altitude of but survived, to be later fined for crossing controlled airspace. Another notable honorable mention was given to the two men who attempted to burgle the home of footballer Duncan Ferguson (who had four convictions for assault and had served six months in Glasgow's Barlinnie prison) in 2001, with one burglar requiring three days' hospitalization after being confronted by the player.
A 2014 study published in the British Medical Journal found that between 1995–2014 males represented 88.7% of Darwin Award winners (see figure).
A 2006 comedy film, "The Darwin Awards", written and directed by Finn Taylor, was based on the website and many of the Darwin Awards stories.
Rules.
Northcutt has stated five requirements for a Darwin Award:
However, in 2011 the awards targeted a 16-year-old boy in Leeds who died stealing copper wiring (the standard minimum driving age in Great Britain being 17). In 2012, Northcutt made similar light of a 14-year-old girl in Brazil who was killed while leaning out of a school bus window, however "disqualified" the award itself because the likely public objection due to her age, which she asserts is based on "magical thinking."
In addition, later revisions to the qualification criteria add several requirements that have not been made into formalized 'rules': innocent bystanders cannot be in danger, and the qualifying event "must" be caused without deliberate intent (to prevent glory-seekers from purposely injuring themselves solely to win a Darwin).
Relationship to the theory of natural selection.
The Darwin Awards are named after Charles Darwin because of his theory of natural selection. However, there is no evidence that the stories depicted actually represent the removal of "judgement impairment genes" from the gene pool. While the stories depict a lack of intelligence in many of the individuals, no genetic differences between Darwin Award winners and the rest of humankind have been sought or demonstrated. Fatal stupidity could be culturally acquired during upbringing and socialization. The stories may actually represent excessive impulsiveness, which can result from a variety of factors.

</doc>
<doc id="8704" url="https://en.wikipedia.org/wiki?curid=8704" title="Outline of dance">
Outline of dance

The following outline is provided as an overview of and topical guide to dance:
Dance – human movement either used as a form of expression or presented in a social, spiritual or performance setting. Choreography is the art of making dances, and the person who does this is called a choreographer. Definitions of what constitutes dance are dependent on social, cultural, aesthetic, artistic and moral constraints and range from functional movement (such as Folk dance) to codified, virtuoso techniques such as ballet. A great many dances and dance styles are performed to dance music.
What "type" of thing is dance?
Dance (also called "dancing") can be described as all of the following:
Types of dance.
Type of dance – a particular dance or dance style. There are many varieties of dance. Dance categories are not mutually exclusive. For example, tango is traditionally a "partner dance". While it is mostly "social dance", its ballroom form may be "competitive dance", as in DanceSport. At the same time it is enjoyed as "performance dance", whereby it may well be a "solo dance".
History of dance.
History of dance
Dance science.
Dance science

</doc>
<doc id="8706" url="https://en.wikipedia.org/wiki?curid=8706" title="DCM">
DCM

DCM may refer to:

</doc>
<doc id="8707" url="https://en.wikipedia.org/wiki?curid=8707" title="DKW">
DKW

DKW (Dampf-Kraft-Wagen, ) is a defunct German car and motorcycle marque. The company and brand is one of the predecessor entities to the modern day Audi company.
In 1916, Danish engineer Jørgen Skafte Rasmussen founded a factory in Zschopau, Saxony, Germany, to produce steam fittings. That year he attempted to produce a steam-driven car, called the DKW. Although unsuccessful, he made a two-stroke toy engine in 1919, called "Des Knaben Wunsch" – "the boy's desire". He put a slightly modified version of this engine into a motorcycle and called it "Das Kleine Wunder" – "the little marvel". This was the beginning of the DKW brand: by the 1930s, DKW was the world's largest motorcycle manufacturer.
In 1932, DKW merged with Audi, Horch and Wanderer to form Auto Union. After World War II, DKW moved to West Germany, with the original factory becoming MZ. Auto Union came under Daimler-Benz ownership in 1957 and was purchased by the Volkswagen Group in 1964. The last German-built DKW car was the F102, which ceased production in 1966. Its successor, the four-stroke F103, was marketed under the Audi brand, another Auto Union marque.
DKW-badged cars continued to be built under license in Brazil and Argentina until 1967 and 1969 respectively. The DKW trademark is currently owned by Auto Union GmbH, a wholly owned subsidiary of Audi AG which also owns the rights to other historical trademarks and intellectual property of the Auto Union combine.
Automobiles made between 1928 and 1942.
DKW cars were made from 1928 until 1966, apart from an interruption caused by the Second World War. DKWs always used two-stroke engines, reflecting the company's position by the end of the 1920s as the world's largest producer of motorcycles. The first DKW car, the small and rather crude Typ P, emerged on 7 May 1928 and the model continued to be built at the company's Spandau (Berlin) plant, first as a roadster and later as a stylish if basic sports car, until 1931.
More significant was a series of inexpensive cars built 300 km (185 miles) to the south in Zwickau in the plant acquired by the company's owner in 1928 when he had become the majority owner in Audi Werke AG. Models F1 to F8 (F for Front) were built between 1931 and 1942, with successor models reappearing after the end of the war in 1945. They were the first volume production cars in Europe with front wheel drive, and were powered by transversely mounted two-cylinder two-stroke engines. Displacement was 584 or 692 cc: claimed maximum power was initially 15 PS, and from 1931 a choice between 18 or . These models had a generator that doubled as a starter, mounted directly on the crankshaft, known as a Dynastart. DKWs from Zwickau notched up approximately 218,000 units between 1931 and 1942. Most cars were sold on the home market and over 85% of DKWs produced in the 1930s were the little F series cars: DKW reached second place in German sales by 1934 and stayed there, accounting for 189,369 of the cars sold between 1931 and 1938, more than 16% of the market.
Between 1929 and 1940, DKW produced a less well remembered but technically intriguing series of rear-wheel drive cars called (among other names) "Schwebeklasse" and "Sonderklasse" with two-stroke V4 engines. Engine displacement was 1,000 cc, later 1,100 cc. The engines had two extra cylinders for forced induction, so they appeared like V6 engines but without spark plugs on the front cylinder pair.
In 1939, DKW made a prototype with the first three-cylinder engine, with a displacement of 900 cc and producing . With a streamlined body, the car could run at . It was put into production after World War II, first as an Industrieverband Fahrzeugbau (IFA) F9 (later Wartburg) in Zwickau, East Germany, and shortly afterwards in DKW-form from Düsseldorf as the 3=6 or F91.
DKW engines were used by Saab as a model for the Saab two-stroke in its Saab 92 car manufacturing venture, in 1947.
Automobiles made after 1945.
As Auto Union was based in Saxony in what became the German Democratic Republic, it took some time for it to regroup after the war. The company was registered in West Germany as Auto Union GmbH in 1949, first as a spare-part provider, but soon to take up production of the RT 125 motorcycle and a new delivery van, called a "Schnellaster" F800. Their first line of production took place in Düsseldorf. This van used the same engine as the last F8 made before the war.
Their first car was the F89 using the body from the prototype F9 made before the war and the two-cylinder two-stroke engine from the last F8. Production went on until it was replaced by the successful three-cylinder engine that came with the F91. The F91 was in production 1953–1955, and was replaced by the larger F93 in 1956. The F91 and F93 had 900 cc three-cylinder two-stroke engines, the first ones delivering , the last . The ignition system comprised three independent sets of points and coils, one for each cylinder, with the points mounted in a cluster around a single lobed cam at the front end of the crankshaft. The cooling system was of the free convection type assisted by a fan driven from a pulley mounted at the front end of the crankshaft.
The F93 was produced until 1959, and was replaced by the Auto-Union 1000. These models where produced with a 1,000 cc two-stroke engine, with a choice between or S versions until 1963. During this transition, production was moved from Düsseldorf to Ingolstadt, where Audi still has its production. From 1957, the cars could be fitted with a saxomat, an automatic clutch, the only small car then offering this feature. The last versions of the Auto-Union 1000S had disc brakes as option, an early development for this technology. A sporting 2+2 seater version was available as the Auto-Union 1000 SP from 1957 to 1964, the first years only as a coupé and from 1962 also as a convertible.
In 1956, the very rare DKW Monza was put into small-scale production on a private initiative, with a sporting two-seater body of glassfiber on a standard F93 frame. It was first called Solitude, but got its final name from the long-distance speed records it made on the Autodromo Nazionale Monza in Italy in November 1956. Running in Fédération Internationale de l'Automobile (FIA) class G, it set records including 48 hours at an average speed of , 10,000 km at and 72 hours at . The car was first produced by Dannenhauer & Strauss in Stuttgart, then by Massholder in Heidelberg and lastly by Robert Schenk in Stuttgart. The number produced is said to be around 230 and production finished by the end of 1958.
A more successful range of cars was sold from 1959, the Junior/F12 series based on a modern concept from the late 1950s. The range consist of Junior (basic model) made from 1959 to 1961, Junior de Luxe (a little enhanced) from 1961 to 1963, F11 (a little larger) and F12 (larger and bigger engine) from 1963 to 1965, and F12 Roadster from 1964 to 1965. The Junior/F12 series became quite popular, and many cars were produced. An assembly plant was licensed in Ireland between 1952 and c.1964 and roughly 4,000 vehicles were assembled, ranging from saloons, vans and motorbikes to commercial combine harvesters. This was the only DKW factory outside Germany in Europe.
All the three-cylinder two-stroke post-war cars had some sporting potential and formed the basis for many rally victories in the 1950s and early 1960s. This made DKW the most winning car brand in the European rally league for several years during the fifties.
In 1960, DKW developed a V6 engine by combining two three-cylinder two-stroke engines, with a capacity of 1,000 cc. The capacity was increased and the final V6 in 1966 had a capacity of 1,300 cc, which developed at 5,000 rpm using the standard configuration with two carburettors. A four-carburettor version produced , a six-carburettor one . It weighed only . The V6 was planned to be used in the DKW Munga and the F102. About 100 engines were built for testing purposes and 13 DKW F102 and some Mungas were fitted with the V6 engine in the 1960s.
The last DKW was the F102, coming into production in 1964 as a replacement for the old-looking AU1000. It was the direct forerunner of the first post-war Audi, the F103, with the main difference that the Audi used a conventional four-stroke engine. The transition to four-stroke engines marked the end of the DKW marque for cars.
From 1956 to 1961, Dutch importer Hart, Nibbrig & Greve assembled cars in an abandoned asphalt factory in Sassenheim, where they employed about 120 workers, two transporter, that collected SKD kits from Duesseldorf and build about 13.500 cars. When the DKW plant moved the import of SKD kits stopped, as it became too expensive.
DKW in South America.
From 1956 to 1967, DKW cars were made in Brazil by the local company Vemag ("Veículos e Máquinas Agrícolas S.A.", "Vehicles and Agricultural Machinery Inc."). Vemag was assembling Scania-Vabis trucks, but Scania Vabis became an independent company in July 1960. The original plans were to build the Candango off-roader (Munga), a utility vehicle and a four-door sedan, called Vemaguet and Belcar respectively. The first model built was the 900 cc F91 Universal but the Belcar and Vemaguet names were applied later.
In 1958, the F94 four-door sedan and station wagon were launched, in the early 1960s renamed Belcar and Vemaguet. The company also produced a luxury coupe (the DKW Fissore) and the off-road Munga (locally called Candango). In 1960 Vemag cars received the larger one-litre, engine from the Auto Union 1000.
Vemag had a successful official racing team, with the coupe GT Malzoni, with fiberglass body. This project was the foundation of the long-lasting Brazilian sports car brand Puma. The Brazilian F94 line has been improved with several cosmetic changes and became more and more different from the German and Argentine models. Vemag had no capital to invest in new products and came under governmental pressure to merge. In 1964-1965 Volkswagen gradually took over Auto Union, a minority holder in Vemag, and in 1967 Volkswagen bought the remainder of the stock. VW quickly began phasing out DKW-Vemag production and introduced the Volkswagen 1600 sedan to the old Vemag plant, after a total of 109,343 DKW-Vemag cars had been built.
DKW vehicles were made in Argentina from 1960 to 1969 by IASF S.A. (Industria Automotriz Santa Fe Sociedad Anónima) in Sauce Viejo, Santa Fe. The most beautiful were the Cupé Fissore, which had many famous owners (Julio Sosa, César Luis Menotti, and others). Other models are the Auto Union 1000 S Sedán (21,797 made until 1969) and the Auto Union 1000 Universal S (6,396 made until 1969). and the Auto Union Combi/Pick-up.
The last version of the Auto Union Combi/Pick-up (DKW F1000 L), launched in 1969, survived a few months and was bought out by IME, which continued production until 1979.
Vans and utility vehicles.
The DKW Munga was built by Auto Union in Ingolstadt. Production began in October 1956 and ended in December 1968, with 46,750 cars built.
From 1949 to 1962, DKW produced the "Schnellaster" with a trailing-arm rear suspension system with springs in the cross bar assembly. Spanish subsidiary IMOSA produced a modern successor introduced in 1963, the DKW F 1000 L. This van started with the three-cylinder 1,000 cc engine, but later received a Mercedes-Benz Diesel engine and was renamed a Mercedes-Benz in 1975.
Motorcycles.
During the late 1920s and 1930s, DKW was the world's largest motorcycle manufacturer. In 1931, Ing Zoller started building split-singles and this concept made DKW the dominant racing motorcycle in the Lightweight and Junior classes between the wars. This included off road events like the International Six Days Trial where the marque scored some considerable inter-war year successes alongside Bavarian Motor Works At the same time, the company also had some success with super-charged racing motorcycles which because of their light weight were particularly successful in the ISDT
The motorcycle branch produced famous models such as the RT 125 pre- and post-World War II, and after the war with production at the original factory in GDR becoming MZ it made 175, 250 and 350 models. As war reparations, the design drawings of the RT125 were given to Harley-Davidson in the US and BSA in the UK. The Harley-Davidson version was known as the Hummer, while BSA used them for the Bantam. IFA and later MZ models continued in production until the 1990s, when economics brought production of the two stroke to an end. Other manufacturers copied the DKW design, officially or otherwise. This can be seen in the similarity of many small two-stroke motorcycles from the 1950s, including from Yamaha, Voskhod, Maserati, and Polish WSK.

</doc>
<doc id="8708" url="https://en.wikipedia.org/wiki?curid=8708" title="Doctor Syn">
Doctor Syn

The Reverend Doctor Christopher Syn is the smuggler hero of a series of novels by Russell Thorndike. The first book, "Doctor Syn: A Tale of the Romney Marsh" was published in 1915. The story idea came from smuggling in the 18th century Romney Marsh, where brandy and tobacco were brought in at night by boat from France to avoid high tax. Minor battles were fought, sometimes at night, between gangs of smugglers, such as the Hawkhurst Gang and the Revenue, supported by the army and local militias in the South, Kent and the West, Sussex.
Character biography.
Christopher Syn, born 1729, is portrayed as a brilliant scholar from Queen's College, Oxford, possessing swashbuckling skills such as riding, fencing, and seamanship. He was content to live the quiet life of a country vicar in Dymchurch-under-the-Wall under the patronage of Sir Charles Cobtree, the father of his best friend Anthony Cobtree, until his beautiful young Spanish wife Imogene was seduced by and eloped with Nicholas Tappitt, whom Dr. Syn had considered a close friend.
Christopher Syn set out on a quest for revenge, always managing to reach the eloped pair's destinations ahead of them just in time to terrify them against landing and facing him in a deliberate campaign of terror. While sailing from Spain to America in pursuit, his ship was captured by the pirate ship "The Sulphur Pit", commanded by the negro Captain Satan. In a one-on-one fight, Syn defeated and killed Captain Satan to take command of his ship and crew; among them was Mr. Mipps, a former Royal Navy carpenter with whom Syn had become friends in England after rescuing him from the Customs men. Mipps swore loyalty to Syn from that time onward.
With Mipps at his side, Syn turned to piracy and became a great success. Later, when his crew refused to let Syn leave, Syn and Mipps slipped away in one of the ship's boats; unknown to Syn, Mipps had arranged a convenient "accident" in the ship's powder hold with an exploding barrel of gunpowder, eliminating witnesses of Syn's piratic acts.
Mipps then joined Syn in his quest for revenge, pursuing Tappitt and Imogene throughout the thirteen American colonies (supposedly preaching the gospel to the Indians) and around the world (as part of a whaling voyage) afterwards, and was with him in the Caribbean when Dr. Syn turned again to piracy, assuming the name of Captain Clegg (taking the name "Clegg" from a certain vicious biting fly he had encountered in America), hijacking his enemy Tappitt's own ship and crew and sailing off with them (renaming the ship the "Imogene") to become the most infamous pirate of the day.
However, a mulatto who escaped the destruction of Syn's previous ship stowed away in Clegg's ship and accused him before the crew; Clegg quelled the potential mutiny by having the mulatto's tongue cut out, marooning him on a coral reef and violently killing Yellow Pete, the ship's Chinese cook, who represented the crew in their wish to rescue the mulatto. Afterwards, realizing that Clegg had become too notorious, Syn decided to abandon his quest and return to England, and Mipps set up a second "accidental" explosion to destroy the "Imogene" and her crew.
Syn returned to England on the night of a storm (13 November 1775) that wrecked his brig off the English coast in sight of Dymchurch. That night he went to the house of his old friend (and now squire) Anthony Cobtree. When news came that the local vicar had drowned while trying to save victims of the shipwreck, Squire Cobtree offered the post to Christopher Syn. Syn accepted and settled down to a more respectable life as the vicar of Dymchurch and Dean of Peculiars in Romney Marsh, Kent, resuming his original name.
Mipps arrived in Dymchurch with the intent of settling down. Syn made him the village sexton upon condition that Mipps "remember to forget" (that Syn had been Clegg and that they had known each other before), and that Mipps never get involved with the local smugglers.
Syn soon became aware that his parishioners were smuggling goods from France to avoid the excessive customs duties the government charged. Learning from Mipps (who, contrary to Syn's orders, had become a leader of the smugglers) that certain townsfolk had been ambushed and captured during a smuggling run, Syn purchased the great black stallion Gehenna from gypsy horse-traders and raced to their rescue. A suit of clothing borrowed from a scarecrow made an improvised disguise, and Syn and Mipps were able to rescue the townsfolk from the Dragoons.
After this, Syn decided that he could only protect his people by becoming their leader. He created a more elaborate scarecrow costume, with eerie luminous paint. At night riding Gehenna, the respectable Dr. Syn became "The Scarecrow", the feared head of the smugglers. Together with Mipps, he organized the smugglers into a well-organized band of "Night Riders", also called "The Devil Riders" with macabre disguises and code-names.
Syn's cunning was so great that the smugglers outwitted the government forces for many years. A hidden stable watched over by Mother Handaway, the local "witch" (who believed the Scarecrow to be The Devil in living form), was the hiding place for the horses of the Scarecrow and his lieutenants, Mipps and the local highwayman Jimmie Bone (who, being as good a horseman as Syn and of similar build, was sometimes called upon to impersonate the Scarecrow when Syn either had to be elsewhere or seen in the same place.).
Shortly after the first appearances of the Scarecrow, Nicholas Tappitt (using the name "Colonel Delacourt") and the ailing Imogene returned to England, ending up in Dymchurch. Recognizing Syn as Clegg, Tappitt realized that Syn and the Scarecrow were the same and helped the authorities set a trap for him, hoping to both rid himself of his enemy and claim the reward for his capture. The trap was sprung, but Squire Cobtree's daughter Charlotte, who had fallen in love with Syn and also learned his secret identities as both Clegg and the Scarecrow, was the tragic victim when she dressed in the Scarecrow's disguise and was fatally wounded as a result. Tappitt was then suspected of being the Scarecrow, and a Customs officer and three constables came to arrest him. In the ensuing fight, Tappitt killed the Customs man and the constables subdued and arrested Tappitt for murdering the Customs officer.
After Imogene's death in Syn's arms (during which she revealed to him that he had a son by her who was missing somewhere in America), Syn fought a final duel with Tappitt in his jail cell, defeating him. Syn then struck a bargain with Tappitt: If Tappitt confessed to being the notorious pirate Clegg, then Syn would look after and care for Tappitt and Imogene's new-born infant daughter (also named Imogene). Tappitt agreed, and "Captain Clegg" was hanged and later "buried without benefit of clergy at a cross-roads hard by the Kent Ditch."
Many years later, Captain Collyer, a Royal Navy officer assigned to smash the local smuggling ring, uncovered the deception and Dr. Syn's true identity, thanks in part to the tongueless mulatto (who had been rescued by Collyer years before and who had been serving Collyer as a "ferret" seeking out hidden contraband) who recognized Syn as Clegg. Syn evaded capture while at the same time making sure that Imogene and Squire Cobtree's son Denis (who had fallen in love with Imogene) would have a happy life together (they were eventually married), but was murdered in revenge by the mulatto, who then mysteriously managed to escape, leaving Syn harpooned through the neck. As a last mark of respect, Collyer ordered that Syn be buried at sea, rather than have his body hung in chains.
Mipps escaped in the confusion of Syn's death and disappeared from England, but it is said that a little man very much like him is living out his days in a Buddhist Monastery somewhere in the Malay Peninsula, delighting the monks with recounting the adventures of Doctor Syn and the eerie stories of the Romney Marsh and the mysterious Scarecrow and his Night Riders.
Publication history.
The Dr. Syn books detail his adventures and attempts to help the people of Dymchurch and the surrounding area evade the Excise tax. There are:
Note that the "first" book, "Doctor Syn," is actually the final story chronologically; the others proceed in published sequence.
An expanded version of "Doctor Syn Returns" titled "The Scarecrow Rides" was published for the US market by The Dial Press in 1935.
In 1960 American author William Buchanan used the character in his novel "Christopher Syn". This is essentially a reworking of "Further Adventures of Doctor Syn" with a different conclusion and some conflation and renaming of the supporting characters. "Christopher Syn" became the basis for the 1962 Disney production (see below). There was also a novelization of the Disney theatrical version, titled "Doctor Syn, Alias the Scarecrow" and written by Vic Crume.
In other media.
Films.
Three film adaptations have been made of Dr. Syn's exploits.
"Doctor Syn" (1937).
The first, "Doctor Syn" (1937), featured noted actor George Arliss in the title role and was its star's last film.
"Captain Clegg" (1962).
"Captain Clegg" (1962), known as "Night Creatures" in the U. S., was produced by Hammer Film Productions with actor Peter Cushing in the lead role, directed by Peter Graham Scott. In the screenplay by Anthony Hinds, the main character's name was changed from Doctor Syn to Parson Blyss to avoid rights problems with Disney's forthcoming version, and "Captain Clegg"'s screenplay follows the novel "Doctor Syn" and the screenplay of the 1937 film closely with the exception of a tightening of the plot. In the Arliss movie "Doctor Syn", Syn escapes to sea with Mipps and the rest of the Dymchurch smugglers, whereas "Captain Clegg" ends more faithfully to the novel, with Parson Blyss being killed by the mulatto (who is then killed by Mipps) and then being carried to and buried in Captain Clegg's empty grave by Mipps. "Night Creatures" was never released on videotape in the United States, but is included in the two-disc DVD collection "The Hammer Horror Series".
"The Scarecrow of Romney Marsh" (1963).
Another version, "The Scarecrow of Romney Marsh" (1963), was produced for the "Walt Disney's Wonderful World of Color" TV series. It was shot on location in England and was directed by James Neilson. It stars Patrick McGoohan in the title role, with George Cole as Mipps and Sean Scully as John Banks, the younger son of Squire Banks (Michael Hordern) and Dr. Syn's second lieutenant. Part One dealt with the arrival of General Pugh (Geoffrey Keen), who had been ordered by the War Office to smash the smuggling ring and the efforts of The Scarecrow to rescue a Dymchurch man who had been captured by the Naval Press-gang and used by General Pugh as bait in a trap; Part Two centered around how The Scarecrow dealt with the traitorous Joe Ransley (Patrick Wymark); and Part Three showed how The Scarecrow rescued Harry Banks (David Buck) (Squire Banks' eldest son, a press-ganged man who had escaped from the Navy) and American Simon Bates (Tony Britton) from General Pugh's clutches in Dover Castle.
While originally conceived and edited for American television (and announced in an advertisement by NBC in the Tuesday, July 9, 1963 issue of "The Hollywood Reporter"), "The Scarecrow of Romney Marsh" was re-edited for a British theatrical run before the American television debut. Titled "Dr. Syn, Alias the Scarecrow", the British theatrical version was released on a double bill with "The Sword in the Stone", and ran during the 1963 Christmas season (advertised in the January 1964 issue of "Photoplay"). This version was shown in Europe as well as Central and South America through 1966.
In the 1970s, the production was re-edited again for its first American theatrical release, on double bills with both "Snow White and the Seven Dwarfs" and "Treasure Island". (The VHS version of the 1980s, sharing the removal of the Scarecrow's laugh from Terry Gilkyson's title song, was expanded to include the story material from all three TV episodes, while retaining feature film structure and credits; it was available for a relatively short amount of time.) Shortly after the US theatrical run, it was re-edited once more for a two-part presentation on Disney's television series in the 1970s, simply omitting the middle segment. The original three-part version was first shown as part of "Walt Disney's Wonderful World of Color" on February 9, 16 and 23, 1964. Later it was included in a late 1980s "Wonderful World of Disney" syndication rerun package, and cablecast in 1990s on the Disney Channel. This version generally followed the storyline of "The Further Adventures of Dr. Syn" and made it clear that Syn did not die or stage his own death: at film's end, he is having a cup of tea with the Squire, who admits to now owing a debt of gratitude to the Scarecrow.
On November 11, 2008 The Walt Disney Company released a limited pressing of 39,500 issues of "The Scarecrow of Romney Marsh" in DVD format for the first time as a part of the "" collection, and was now called "Dr. Syn: The Scarecrow of Romney Marsh". The issue sold out in three weeks, but as of February 17, 2009 the DVD was made available for members of the Disney movie club for $29.95. The two-disc set includes the American television version and the original British theatrical version "Dr. Syn, Alias the Scarecrow" in widescreen format. It also includes the original introductions by Walt Disney (in which he erroneously indicates that Dr. Syn was an actual historical figure) and a documentary on Disney's interest in the property.
St Clement's Church in Old Romney doubled as Dr Syn’s Dymchurch parish church in the production and Walt Disney funded the repair of the building in order to use it as a filming location.
Other adaptations.
Made in 1974, "Carry On Dick", of the celebrated "Carry On" series of films, followed the same premise of a country vicar (Sid James) who is secretly an outlaw, in this case the highwayman Dick Turpin.
Theatre.
In 2001 a stage adaptation titled "Doctor Syn" was performed at churches throughout the Romney Marsh, the final night being performed in Dymchurch itself. The cast featured Daniel Thorndike (the author's son), Michael Fields, Steven Povey and Ben Barton, along with various amateurs from the area.
Rufus Sewell read a 10-part audio adaptation combining and abridging "Doctor Syn on the High Seas" and "Doctor Syn Returns" for BBC Radio, broadcast on BBC Radio 7 in December 2006 and repeated in June 2007.
A 10-part audio adaptation of "The Further Adventures of Doctor Syn" (combining and abridging "The Further Adventures of Doctor Syn" and "The Shadow of Doctor Syn") read by Rufus Sewell was performed on BBC Radio 7 in December 2007.
In April 2009, a third series was announced for broadcast later in 2009. BBC Radio 7 broadcast the six-part series, an abridged reading by Rufus Sewell of the original "Doctor Syn" novel, from January 4, 2010 to January 11.
John Paul Jones of Led Zeppelin reinterpreted elements of the Doctor Syn story as his "No Quarter" fantasy sequence in Led Zeppelin's concert film "The Song Remains the Same".
Comic books.
A much abridged revision of the adventures of Dr. Syn appeared as a short comic serialized in the monthly publication Disney Adventures. The new story features the heroic Doctor and his young sidekick protecting innocent villagers from corrupt government officials and soldiers.
Doctor Syn appears in the "League of Extraordinary Gentlemen" series as a member of the league gathered by Lemuel Gulliver. His alter ego, Captain Clegg, also makes appearances, where he is mentioned to have had a brief romantic liaison with future teammate Fanny Hill. In the 2003 film adaptation of "League", Dr. Syn can be spotted in one of the portraits hanging on the wall in M's library.
Cultural legacy.
A "Days of Syn" festival is held even-numbered years by Dymchurch residents for fund-raising. The 2006 "Days of Syn" was on 26–28 August (UK August Bank Holiday weekend) and featured a talk on Dr. Syn at the Anglican church at 6:30 p.m. On Sunday at 3 p.m. there was a church service where Dr. Syn and the cast appeared in period costume. On Monday, starting at the Bowery Hall, scenes were reenacted from "Doctor Syn", and again during the day along the Dymchurch shoreline and in the Ocean pub.
Discussions are taking place to build a 100 ft high statue of "The Scarecrow" on a site (the Maize Maze site at Haguelands Farm) in the centre of Romney Marsh.
Doctor Syn is also the name given to one of the locomotives on the Romney, Hythe and Dymchurch Railway.
Doctor Syn also inspired novelist George Chittenden who captures smuggling on the Kent coast in his highly praised debut 'The Boy Who Led Them', which follows the rise and fall of a smuggling gang leader further down the coast in the notorious town of Deal.

</doc>
<doc id="8709" url="https://en.wikipedia.org/wiki?curid=8709" title="Dhrystone">
Dhrystone

Dhrystone is a synthetic computing benchmark program developed in 1984 by Reinhold P. Weicker intended to be representative of system (integer) programming. The Dhrystone grew to become representative of general processor (CPU) performance. The name "Dhrystone" is a pun on a different benchmark algorithm called Whetstone.
With Dhrystone, Weicker gathered meta-data from a broad range of software, including programs written in FORTRAN, PL/1, SAL, ALGOL 68, and Pascal. He then characterized these programs in terms of various common constructs: procedure calls, pointer indirections, assignments, etc. From this he wrote the Dhrystone benchmark to correspond to a representative mix. Dhrystone was published in Ada, with the C version for Unix developed by Rick Richardson ("version 1.1") greatly contributing to its popularity.
Dhrystone vs. Whetstone.
The Dhrystone benchmark contains no floating point operations, thus the name is a pun on the then-popular Whetstone benchmark for floating point operations. The output from the benchmark is the number of Dhrystones per second (the number of iterations of the main code loop per second).
Both Whetstone and Dhrystone are "synthetic" benchmarks, meaning that they are simple programs that are carefully designed to statistically mimic the processor usage of some common set of programs. Whetstone, developed in 1972, originally strove to mimic typical Algol 60 programs based on measurements from 1970, but eventually became most popular in its Fortran version, reflecting the highly numerical orientation of computing in the 1960s.
Issues addressed by Dhrystone.
Dhrystone's eventual importance as an indicator of general-purpose ("integer") performance of new computers made it a target for commercial compiler writers. Various modern compiler static code analysis techniques (such as elimination of dead code: for example, code which uses the processor but produces internal results which are not used or output) make the use and design of synthetic benchmarks more difficult. Version 2.0 of the benchmark, released by Weicker and Richardson in March 1988, had a number of changes intended to foil a range of compiler techniques. Yet it was carefully crafted so as not to change the underlying benchmark. This effort to foil compilers was only partly successful. Dhrystone 2.1, released in May of the same year, had some minor changes and remains the current definition of Dhrystone.
Other than issues related to compiler optimization, various other issues have been cited with the Dhrystone. Most of these, including the small code size and small data set size, were understood at the time of its publication in 1984. More subtle is the slight over-representation of string operations, which is largely language-related: both Ada and Pascal have strings as normal variables in the language, whereas C does not, so what was simple variable assignment in reference benchmarks became buffer copy operations in the C library. Another issue is that the score reported does not include information which is critical when comparing systems such as which compiler was used, and what optimizations.
Dhrystone remains remarkably resilient as a simple benchmark, but its continuing value in establishing true performance is questionable. It is easy to use, well documented, fully self-contained, well understood, and can be made to work on almost any system. In particular, it has remained in broad use in the embedded computing world, though the recently developed EEMBC benchmark suite, HINT, Stream, and even Bytemark are widely quoted and used, as well as more specific benchmarks for the memory subsystem (Cachebench), TCP/IP (TTCP), and many others.
Dhrystone remains in use 30 years after it was designed by Weicker, a longer life than most software.
Dhrystone vs. CoreMark.
CoreMark is a small benchmark released by the non-profit Embedded Microprocessor Benchmark Consortium (EEMBC) that targets the CPU core, similar to Dhrystone. Both benchmarks are available free of charge and are small enough to execute on any processor, including small micro-controllers. CoreMark avoids issues such as the compiler computing the work during compile time, and uses real algorithms rather than being completely synthetic. CoreMark also has established rules for running the benchmark and for reporting the results.
Results.
Dhrystone may represent a result more meaningfully than MIPS (million instructions per second) because instruction count comparisons between different instruction sets (e.g. RISC vs. CISC) can confound simple comparisons. For example, the same high-level task may require many more instructions on a RISC machine, but might execute faster than a single CISC instruction. Thus, the Dhrystone score counts only the number of program iteration completions per second, allowing individual machines to perform this calculation in a machine-specific way. Another common representation of the Dhrystone benchmark is the DMIPS (Dhrystone MIPS) obtained when the Dhrystone score is divided by 1757 (the number of Dhrystones per second obtained on the VAX 11/780, nominally a 1 MIPS machine).
Another way to represent results is in DMIPS/MHz, where DMIPS result is further divided by CPU frequency, to allow for easier comparison of CPUs running at different clock rates.
Shortcomings.
Using Dhrystone as a benchmark has pitfalls: 

</doc>
<doc id="8713" url="https://en.wikipedia.org/wiki?curid=8713" title="Dave Winer">
Dave Winer

Dave Winer (born May 2, 1955 in Brooklyn, New York City) is an American software developer, entrepreneur and writer in New York City. Winer is noted for his contributions to outliners, scripting, content management, and web services, as well as blogging and podcasting. He is the founder of the software companies Living Videotext, Userland Software and Small Picture Inc., a former contributing editor for the Web magazine HotWired, the author of the "Scripting News" weblog, a former research fellow at Harvard Law School, and current visiting scholar at New York University's Arthur L. Carter Journalism Institute.
Family background and education.
Winer was born on May 2, 1955, in Brooklyn, New York City, the son of Eve Winer, Ph.D., a school psychologist, and Leon Winer, Ph.D., a former professor of the Columbia University Graduate School of Business. Winer is also the grandnephew of German novelist Arno Schmidt and a relative of Hedy Lamarr. He graduated from the Bronx High School of Science in 1972. Winer received a BA in Mathematics from Tulane University in New Orleans in 1976. In 1978 he received an MS in Computer Science from the University of Wisconsin–Madison.
Career.
Early work in outliners.
In 1979 Dave Winer became an employee of Personal Software, where he worked on his own product idea named VisiText, which was his first attempt to build a commercial product around an "expand and collapse" outline display and which ultimately established outliners as a software product. In 1981 he left the company and founded Living Videotext to develop this still-unfinished product. The company was based in Mountain View, CA, and grew to more than 50 employees.
ThinkTank, which was based on VisiText, was released in 1983 for Apple II and was promoted as an "idea processor." It became the "first popular outline processor, the one that made the term generic." A ThinkTank release for the IBM PC followed in 1984, as well as releases for the Macintosh 128K and 512K. Ready, a RAM resident outliner for the IBM PC released in 1985, was commercially successful but soon succumbed to the competing Sidekick product by Borland. MORE, released for Apple's Macintosh in 1986, combined an outliner and a presentation program. It became "uncontested in the marketplace" and won the MacUser's Editor's Choice Award for "Best Product" in 1986.
In 1987, at the height of his company's success, Winer sold Living Videotext to Symantec for an undisclosed but substantial transfer of stock that "made his fortune." Winer continued to work at Symantec's Living Videotext division, but after six months he left the company in pursuit of other challenges.
Years at UserLand.
Winer founded UserLand Software in 1988 and served as the company's CEO until 2002.
UserLand's original flagship product, Frontier, was a system-level scripting environment for the Mac, Winer's pioneering weblog, "Scripting News", takes its name from this early interest. Frontier was an outliner-based scripting language, echoing Winer's longstanding interest in outliners and anticipating code-folding editors of the late 1990s.
Winer became interested in web publishing while helping automate the production process of the strikers' online newspaper during San Francisco's newspaper strike of November 1994, According to Newsweek, through this experience, he "revolutionized Net publishing." Winer subsequently shifted the company's focus to online publishing products, enthusiastically promoting and experimenting with these products while building his websites and developing new features. One of these products was Frontier's NewsPage Suite of 1997, which supported the publication of Winer's "Scripting News" and was adopted by a handful of users who "began playing around with their own sites in the Scripting News vein." These users included notably Chris Gulker and Jorn Barger, who envisaged blogging as a networked practice among users of the software.
In 1997 Winer was appointed advisor to Seybold Seminars due to his "pioneering work in web-based publishing systems." Keen to enter the "competitive arena of high-end Web development," Winer then came to collaborate with Microsoft and jointly developed the XML-RPC protocol. This led to the creation of SOAP, which he co-authored with Microsoft's Don Box, Bob Atkinson, and Mohsen Al-Ghosein.
In December 1997, acting on the desire to "offer much more timely information," Winer designed and implemented an XML syndication format for use on his "Scripting News" weblog, thus making an early contribution to the history of web syndication technology. By December 2000, competing dialects of RSS included several varieties of Netscape's RSS, Winer's RSS 0.92, and an RDF-based RSS 1.0. Winer continued to develop the branch of the RSS fork originating from RSS 0.92, releasing in 2002 a version called RSS 2.0. Winer's advocacy of web syndication in general and RSS 2.0 in particular convinced many news organizations to syndicate their news content in that format. For example, in early 2002 the New York Times entered an agreement with UserLand to syndicate many of their articles in RSS 2.0 format. Winer resisted calls by technologists to have the shortcomings of RSS 2.0 improved. Instead, he froze the format and turned its ownership over to Harvard University.
With products and services based on UserLand's Frontier system, Winer became a leader in blogging tools from 1999 onwards, as well as a "leading evangelist of weblogs." In 2000 Winer developed the Outline Processor Markup Language OPML, an XML format for outlines, which originally served as the native file format for Radio UserLand's outliner application and has since been adopted for other uses, the most common being to exchange lists of web feeds between web feed aggregators. UserLand was the first to add an "enclosure" tag in its RSS, modifying its blog software and its aggregator so that bloggers could easily link to an audio file (see podcasting and history of podcasting).
In February 2002 Winer was named one of the "Top Ten Technology Innovators" by InfoWorld.
In June 2002 Winer underwent life-saving bypass surgery to prevent a heart attack and as a consequence stepped down as CEO of UserLand shortly after. He remained the firm's majority shareholder, however, and claimed personal ownership of Weblogs.com.
Writer.
As "one of the most prolific content generators in Web history," Winer has enjoyed a long career as a writer and has come to be counted among Silicon Valley's "most influential web voices."
Winer started "DaveNet", "a stream-of-consciousness newsletter distributed by e-mail" in November 1994 and maintained Web archives of the "goofy and informative" 800-word essays since January 1995, which earned him a Cool Site of the Day award in March 1995. From the start, the "Internet newsletter" "DaveNet" was widely read among industry leaders and analysts, who experienced it as a "real community." Dissatisfied with the quality of the coverage that the Mac and, especially, his own Frontier software received in the trade press, Winer saw "DaveNet" as an opportunity to "bypass" the conventional news channels of the software business. Satisfied with his success, he "reveled in the new direct email line he had established with his colleagues and peers, and in his ability to circumvent the media." In the early years, Winer often used "DaveNet" to vent his grievances against Apple's management, and as a consequence of his strident criticism came to be seen as "the most notorious of the disgruntled Apple developers." Redacted "DaveNet" columns were published weekly by the web magazine "HotWired" between June 1995 and May 1996. "DaveNet" was discontinued in 2004.
Winer's "Scripting News", acclaimed as "one of the oldest blogs," launched in February 1997 and earned him titles such as "protoblogger" and "forefather of blogging." "Scripting News" started as "a home for links, offhand observations, and ephemera" and allowed Winer to mix "his roles as a widely read pundit and an ambitious entrepreneur." Offering an "as-it-happened portrait of the work of writing software for the Web in the 1990s," the site became an "established must-read for industry insiders." "Scripting News" continues to be updated regularly.
Berkman Fellow at Harvard.
Winer spent one year as a resident fellow at the Harvard Law School's Berkman Center for Internet & Society, where he worked on using weblogs in education. While there, he launched "Weblogs at Harvard Law School" using UserLand software, and held the first BloggerCon conferences. Winer's fellowship ended in June 2004.
Visiting Scholar at New York University.
In 2010 Winer was appointed Visiting Scholar at New York University's Arthur L. Carter Journalism Institute.
Return to Outliners.
On December 19, 2012, Winer co-founded (with Kyle Shank) Small Picture, Inc., a corporation which builds two outlining products, Little Outliner and Fargo. Little Outliner, an entry-level outliner designed to teach new users about outliners, launched on March 25, 2013. Fargo, the company's "primary product", launched less than a month later, on April 17, 2013. Fargo is a browser-based outliner which stores the user's outlines in their Dropbox account. It is a free product; in the future, Small Picture may offer paid-for services to Fargo users.
Projects and activities.
24 Hours of Democracy.
In February 1996, while working as a columnist for HotWired, Winer organized 24 Hours of Democracy, an online protest against the recently passed Communications Decency Act. As part of the protest, over 1,000 people, among them Microsoft chairman Bill Gates, posted essays to the Web on the subject of democracy, civil liberty and freedom of speech.
Edit This Page.
In December 1999, Winer became the "proprietor of a growing free blog service" at EditThisPage.com, hosting "approximately 20,000 sites" in February 2001. The service closed in December 2005.
Podcasting.
Winer has been given "credit for the invention of the podcasting model." Having received user requests for audioblogging features since October 2000, especially from Adam Curry, Winer decided to include new functionality in RSS 0.92 by defining a new element called "enclosure," which would pass the address of a media file to the RSS aggregator. He demonstrated the RSS enclosure feature on January 11, 2001 by enclosing a Grateful Dead song in his "Scripting News" weblog.
Winer's weblogging product, Radio Userland, the program favored by Curry, had a built-in aggregator and thus provided both the "send" and "receive" components of what was then called audioblogging. In July 2003 Winer challenged other aggregator developers to provide support for enclosures. In October 2003, Kevin Marks demonstrated a script to download RSS enclosures and pass them to iTunes for transfer to an iPod. Curry then offered an RSS-to-iPod script that moved MP3 files from Radio UserLand to iTunes. The term "podcasting" was suggested by Ben Hammersley in February 2004.
Winer also has an occasional podcast, Morning Coffee Notes, which has featured guests such as Doc Searls, Mike Kowalchik, Jason Calacanis, Steve Gillmor, Peter Rojas, Cecile Andrews, Adam Curry, Betsy Devine and others.
BloggerCon.
BloggerCon is a user-focused conference for the blogger community. BloggerCon I (October 2003) and II (April 2004), were organized by Dave Winer and friends at Harvard Law School's Berkman Center for the Internet and Society in Cambridge, Mass. BloggerCon III met at Stanford Law School on November 6, 2004.
Weblogs.com.
Weblogs.com provided a free ping-server used by many blogging applications, as well as free hosting to many bloggers. After leaving Userland, Winer claimed personal ownership of the site, and in mid-June 2004 he shut down its free blog-hosting service, citing lack of resources and personal problems. A swift and orderly migration off Winer's server was facilitated by Rogers Cadenhead, whom Winer then hired to port the server to a more stable platform. In October, 2005, VeriSign bought the Weblogs.com ping-server from Winer and promised that its free services would remain free. The podcasting-related web site audio.weblogs.com was also included in the $2.3 million deal.
Share your OPML.
Winer opened his self-described "commons for sharing outlines, feeds, and taxonomy" in May 2006. The site allowed users to publish and syndicate blogrolls and aggregator subscriptions using OPML. Winer suspended its service in January 2008.
Rebooting the News.
Since 2009, Winer has collaborated with New York University's associate professor of journalism Jay Rosen on "Rebooting the News", a weekly podcast on technology and innovation in journalism. It was announced on July 1, 2011 that the show would be on break, as NYU itself was, from June to September. However, no new episodes have been released since, making show #94 released on May 23, 2011 the last.

</doc>
<doc id="8714" url="https://en.wikipedia.org/wiki?curid=8714" title="December 10">
December 10


</doc>
<doc id="8715" url="https://en.wikipedia.org/wiki?curid=8715" title="Taiko">
Taiko

Taiko have a mythological origin in Japanese folklore, but historical records suggest that taiko were introduced to Japan through Korean and Chinese cultural influence as early as the 6th century CE. Some taiko are similar to instruments originating from India. Archaeological evidence also supports that taiko were present in Japan during the 6th century in the Kofun period. Their function has varied through history, ranging from communication, military action, theatrical accompaniment, and religious ceremony to both festival and concert performances. In modern times, taiko have also played a central role in social movements for minorities both within and outside Japan.
"Kumi-daiko" performance, characterized by an ensemble playing on different drums, was developed in 1951 through the work of Daihachi Oguchi and has continued with groups such as Kodo. Other performance styles, such as "hachijō-daiko", have also emerged from specific communities in Japan. "Kumi-daiko" performance groups are active not only in Japan, but also in the United States, Australia, Canada, and Brazil. Taiko performance consists of many components in technical rhythm, form, stick grip, clothing, and the particular instrumentation. Ensembles typically use different types of barrel-shaped "nagadō-daiko" as well as smaller "shime-daiko". Many groups accompany the drums with vocals, strings, and woodwind instruments.
History.
Origin.
The origin of the instruments is unclear, though there have been many suggestions. Historical accounts, of which the earliest date from 588 CE, note that young Japanese men traveled to Korea to study the kakko, a drum that originated in South China. This study and appropriation of Chinese instruments may have influenced the emergence of taiko. Certain court music styles, especially gigaku and gagaku, arrived in Japan through both Korea and China. In both traditions, dancers were accompanied by several instruments that included drums similar to taiko. Certain percussive patterns and terminology in togaku, an early dance and music style in Japan, in addition to physical features of the kakko, also reflect influence from both China and India on drum use in gagaku performance.
Archaeological evidence shows that taiko were used in Japan as early as the 6th century CE, during the latter part of the Kofun period, and were likely used for communication, in festivals, and in other rituals. This evidence was substantiated by the discovery of haniwa statues in the Sawa District of Gunma Prefecture. Two of these figures are depicted playing drums; one of them, wearing skins, is equipped with a barrel-shaped drum hung from his shoulder and uses a stick to play the drum at hip height. This statue is titled "Man Beating the Taiko" and is considered the oldest evidence of taiko performance in Japan. Similarities between the playing style demonstrated by this haniwa and known music traditions in Korea and China further suggest influences from these regions.
The "Nihon Shoki", the second oldest book of Japanese classical history, contains a mythological story describing the origin of taiko. The myth tells how Amaterasu, who had sealed herself inside a cave in anger, was beckoned out by an elder goddess Ame-no-Uzume when others had failed. Ame-no-Uzume accomplished this by emptying out a barrel of sake and dancing furiously on top of it. Historians regard her performance as the mythological creation of taiko music.
Use in warfare.
In feudal Japan, taiko were often used to motivate troops, call out orders or announcements, and set a marching pace; marches were usually set to six paces per beat of the drum. During the 16th-century Warring States period, specific drum calls were used to communicate orders for retreating and advancing. Other rhythms and techniques were detailed in period texts. According to the war chronicle "Gunji Yoshū", nine sets of five beats would summon an ally to battle, while nine sets of three beats, sped up three or four times, was the call to advance and pursue an enemy. Folklore from the 16th century on the legendary 6th-century Emperor Keitai offers a story that he obtained a large drum from China, which he named . The Emperor was thought to have used it to both encourage his own army and intimidate his enemies.
In traditional settings.
Taiko have been incorporated in Japanese theatre for rhythmic needs, general atmosphere, and in certain settings decoration. In the kabuki play "The Tale of Shiroishi and the Taihei Chronicles", scenes in the pleasure quarters are accompanied by taiko to create dramatic tension. Noh theatre also feature taiko where performance consists of highly specific rhythmic patterns. The school of drumming, for example, contains 65 basic patterns in addition to 25 special patterns; these patterns are categorized in several classes. Differences between these patterns include changes in tempo, accent, dynamics, pitch, and function in the theatrical performance. Patterns are also often connected together in progressions.
Taiko continue to be used in gagaku, a classical music tradition typically performed at the Tokyo Imperial Palace in addition to local temples and shrines. In gagaku, one component of the art form is traditional dance, which is guided in part by the rhythm set by the taiko. Taiko have played an important role in many local festivals across Japan. They are also used to accompany religious ritual music. In kagura, which generically describes music and dances stemming from Shinto practices, taiko frequently appear alongside other performers during local festivals. In Buddhist traditions, taiko are used for ritual dances that are a part of the Bon Festival. Taiko, along with other instruments, are featured atop towers that are adorned with red-and-white cloth and serve to provide rhythms for the dancers who are encircled around the performers.
Kumi-daiko.
In addition to the instruments, the term "taiko" also refers to the performance itself, and commonly to one style called "kumi-daiko", or ensemble-style playing (as opposed to festival performances, rituals, or theatrical use of the drums). "Kumi-daiko" was developed by Daihachi Oguchi in 1951. He is considered a master performer and helped transform taiko performance from its roots in traditional settings in festivals and shrines. Oguchi was trained as a jazz musician in Nagano, and at one point, a relative gave him an old piece of written taiko music. Unable to read the traditional and esoteric notation, Oguchi found help to transcribe the piece, and on his own added rhythms and transformed the work to accommodate multiple taiko players on different-sized instruments. Each instrument served a specific purpose that established present-day conventions in "kumi-daiko" performance.
Oguchi's ensemble, Osuwa Daiko, incorporated these alterations and other drums into their performances. They also devised novel pieces that were intended for non-religious performances. Several other groups emerged in Japan through the 1950s and 1960s. Oedo Sukeroku Daiko was formed in Tokyo in 1959 under Seidō Kobayashi, and has been referred to as the first taiko group who toured professionally. Globally, "kumi-daiko" performance became more visible during the 1964 Summer Olympics in Tokyo, when it was featured during the Festival of Arts event.
"Kumi-daiko" was also developed through the leadership of , who gathered young men who were willing to devote their entire lifestyle to taiko playing and took them to Sado Island for training where Den and his family had settled in 1968. Den chose the island based on a desire to reinvigorate the folk arts in Japan, particularly taiko; he became inspired by a drumming tradition unique to Sado called that required considerable strength to play well. Den called the group "Za Ondekoza" or Ondekoza for short, and implemented a rigorous set of exercises for its members including long-distance running. In 1975, Ondekoza was the first taiko group to tour in the United States. Their first performance occurred just after the group finished running the Boston Marathon while wearing their traditional uniforms. In 1981, some members of Ondekoza split from Den and formed another group called Kodo under the leadership of Eitetsu Hayashi. Kodo continued to use Sado Island for rigorous training and communal living, and went on to popularize taiko through frequent touring and collaborations with other musical performers. Kodo is one of the most recognized taiko groups both in Japan and worldwide.
Estimates of the number of taiko groups in Japan vary up to 5000 active in Japan, but more conservative assessments place the number closer to 800 based on membership in the Nippon Taiko Foundation, the largest national organization of taiko groups. Some pieces that have emerged from early "kumi-daiko" groups that continue to be performed include "Yatai-bayashi" from Ondekoza, from Osuwa Daiko, and from Kodo.
Categorization.
Taiko have been developed into a broad range of percussion instruments that are used in both Japanese folk and classical musical traditions. An early classification system based on shape and tension was advanced by Francis Taylor Piggott in 1909. Taiko are generally classified based on the construction process, or the specific context in which the drum is used, but some are not classified, such as the toy den-den daiko.
With few exceptions, taiko have a drum shell with heads on both sides of the body, and a sealed resonating cavity. The head may be fastened to the shell using a number of different systems, such as using ropes. Taiko may be either tunable or non-tunable depending on the system used.
Taiko are categorized into three types based on construction process. "Byō-uchi-daiko" are constructed with the drumhead nailed to the body. "Shime-daiko" are classically constructed with the skin placed over iron or steel rings, which are then tightened with ropes. Contemporary "shime-daiko" are tensioned using bolts or turnbuckles systems attached to the drum body. "Tsuzumi" are also rope-tensioned drums, but have a distinct hourglass shape and their skins are made using deerskin.
"Byō-uchi-daiko" were historically made only using a single piece of wood; they continue to be made in this manner, but are also constructed from staves of wood. Larger drums can be made using a single piece of wood, but at a much greater cost due to the difficulty in finding appropriate trees. The preferred wood is the Japanese zelkova or "keyaki", but a number of other woods, and even wine barrels, have been used to create taiko. "Byō-uchi-daiko" cannot be tuned.
The typical "byō-uchi-daiko" is the "nagadō-daiko", an elongated drum that is roughly shaped like a wine barrel. "Nagadō-daiko" are available in a variety of sizes, and their head diameter is traditionally measured in shaku (units of roughly 30 cm). Head diameters range from . are the smallest of these drums and are usually about in diameter. The "chū-daiko " is a medium-sized "nagadō-daiko" ranging from , and weighing about . vary in size, and are often as large as in diameter. Some "ō-daiko" are difficult to move due to their size, and therefore permanently remain inside the performance space, such as temple or shrine. "Ō-daiko" means "large drum" and for a given ensemble, the term refers to their largest drum. The other type of "byō-uchi-daiko" is called a and describes any drum constructed such that the head diameter is greater than the length of the body.
"Shime-daiko" are a set of smaller, roughly-snare drum sized instrument that are tunable. The tensioning system usually consists of hemp cords or rope, but bolt or turnbuckle systems have been used as well. , sometimes referred to as "taiko" in the context of theater, have thinner heads than other kinds of shime-daiko. The head includes a patch of deerskin is placed in the center, and in performance, drum strokes are generally restricted to this area. The is a heavier type of "shime-daiko". They are available in sizes 1–5, and are named according to their number: "namitsuke" (1), "nichō-gakke" (2), "sanchō-gakke" (3), "yonchō-gakke" (4), and "gochō-gakke" (5). The "namitsuke" has the thinnest skins and the shortest body in terms of height; thickness and tension of skins, as well as body height, increase toward the "gochō-gakke". The head diameters of all "shime-daiko" sizes are around .
"Okedō-daiko" or simply "okedō", are a type of "shime-daiko" that are stave-constructed using narrower strips of wood, have a tube-shaped frame. Like other "shime-daiko", drum heads are attached by metal hoops and fastened by rope or cords. "Okedō" can be played using the same drumsticks (called "bachi") as "shime-daiko", but can also be hand-played. "Okedō" come in short- and long-bodied types.
"Tsuzumi" are a class of hourglass-shaped drums. The drum body is shaped on a spool and the inner body carved by hand. Their skins can be made from cowhide, horsehide, or deerskin. While the "ō-tsuzumi" skins are made from cowhide, "ko-tsuzumi" are made from horsehide. While some classify "tsuzumi" as a type of taiko, others have described them as a drum entirely separate from taiko.
Taiko can also be categorized by the context in which they are used. The "miya-daiko", for instance, is constructed in the same manner as other "byō-uchi-daiko", but is distinguished by an ornamental stand and is used for ceremonial purposes at Buddhist temples. The (a "ko-daiko") and (a "nagadō-daiko" with a cigar-shaped body) are used in sumo and festivals respectively.
Several drums, categorized as "gagakki", are used in the Japanese theatrical form, gagaku. The lead instrument of the ensemble is the kakko, which is a smaller "shime-daiko" with heads made of deerskin, and is placed horizontally on a stand during performance. A "tsuzumi", called the "san-no-tsuzumi" is another small drum in gagaku that is placed horizontally and struck with a thin stick. are the largest drums of the ensemble, and have heads that are about in diameter. During performance, the drum is placed on a tall pedestals and surrounded by a rim decoratively painted with flames and adorned with mystical figures such as wyverns. "Dadaiko" are played while standing, and are usually only played on the downbeat of the music. The is a smaller drum that produces a lower sound, its head measuring about in diameter. It is used in ensembles that accompany bugaku, a traditional dance performed at the Tokyo Imperial Palace and in religious contexts. "Tsuri-daiko" are suspended on a small stand, and are played sitting down. "Tsuri-daiko" performers typically use shorter mallets covered in leather knobs instead of bachi. They can be played simultaneously by two performers; while one performer plays on the head, another performer uses bachi on the body of the drum.
The larger "ō-tsuzumi" and smaller "ko-tsuzumi" are used in the opening and dances of Noh theater. Both drums are struck using the fingers; players can also adjust pitch by manually applying pressure to the ropes on the drum. The color of the cords of these drums also indicates the skill of the musician: Orange and red for amateur players, light blue for performers with expertise, and lilac for masters of the instrument. "Nagauta-shime daiko" or "uta daiko" are also featured in Noh performance.
Many taiko in Noh are also featured in kabuki performance and are used in a similar manner. In addition to the "ō-tsuzumi", "ko-tsuzumi", and "nagauta-shime daiko", Kabuki performances make use of the larger "ō-daiko" offstage to help set the atmosphere for different scenes.
Construction.
Process.
Taiko construction has several stages, including making and shaping of the drum body (or shell), preparing the drum skin, and tuning the skin to the drumhead. Variations in the construction process often occur in the latter two parts of this process. Historically, "byō-uchi-daiko" were crafted from trunks of the Japanese zelkova tree that were dried out over years, using techniques to prevent splitting. A master carpenter then carved out the rough shape of the drum body with a chisel; the texture of the wood after carving softened the tone of the drum. In contemporary times, taiko are carved out on a large lathe using wood staves or logs that can be shaped to fit drum bodies of various sizes. Drumheads can be left to air-dry over a period of years, but some companies use large, smoke-filled warehouses to hasten the drying process. After drying is complete, the inside of the drum is worked with a deep-grooved chisel and sanded. Lastly, handles are placed onto the drum. These are used to carry smaller drums and they serve an ornamental purpose for larger drums.
The skins or heads of taiko are generally made from cowhide from Holstein cows aged about three or four years. Skins also come from horses, and bull skin is preferred for larger drums. Thinner skins are preferred for smaller taiko, and thicker skins are used for larger ones. On some drumheads, a patch of deer skin placed in the center serves as the target for many strokes during performance. Before fitting it to the drum body the hair is removed from the hide by soaking it in a river or stream for about a month; winter months are preferred as colder temperatures better facilitate hair removal. To stretch the skin over the drum properly, one process requires the body to be held on a platform with several hydraulic jacks underneath it. The edges of the cowhide are secured to an apparatus below the jacks, and the jacks stretch the skin incrementally to precisely apply tension across the drumhead. Other forms of stretching use rope or cords with wooden dowels or an iron wheel to create appropriate tension. Small tension adjustments can be made during this process using small pieces of bamboo that twist around the ropes. Particularly large drumheads are sometimes stretched by having several workers, clad in stockings, hop rhythmically atop it, forming a circle along the edge. After the skin has dried, tacks, called "byō", are added to the appropriate drums to secure it; "chū-daiko" require about 300 of them for each side. After the body and skin have been finished, excess hide is cut off and the drum can be stained as needed.
Drum makers.
Several companies specialize in the production of taiko. One such company that created drums exclusively for the Emperor of Japan, Miyamoto Unosuke Shoten in Tokyo, has been making taiko since 1861. The Asano Taiko Corporation is another major taiko-producing organization, and has been producing taiko for over 400 years. The family-owned business started in Mattō, Ishikawa, and, aside from military equipment, made taiko for Noh theater and later expanded to creating instruments for festivals during the Meiji period. Asano currently maintains an entire complex of large buildings referred to as Asano Taiko Village, and the company reports producing up to 8000 drums each year. As of 2012, there is approximately one major taiko production company in each prefecture of Japan, with some regions having several companies. Of the manufacturers in Naniwa, Taikoya Matabē is one of the most successful and is thought to have brought considerable recognition to the community and attracted many drum makers there. Umetsu Daiko, a company that operates in Hakata, has been producing taiko since 1821.
Performance.
Taiko performance styles vary widely across groups in terms of the number of performers, repertoire, instrument choices, and stage techniques. Nevertheless, a number of early groups have had broad influence on the tradition. For instance, many pieces developed by Ondekoza and Kodo are considered standard in many taiko groups.
Form.
Kata is a term used to describe the posture and movement associated with taiko performance. The term is used in martial arts in a similar way: for example, both traditions include the idea that the hara is the center of being. Author Sean Bender argues that kata is the primary feature that distinguishes different taiko groups from one another, and is a key factor in judging the quality of performance. For this reason, many practice rooms intended for taiko contain mirrors to provide visual feedback to players. An important part of kata in taiko is keeping the body stabilized while performing, and can be accomplished by keeping a wide, low stance with the legs, with the left knee bent over the toes and keeping the right leg straight. It is important for that the hips face the drum and the shoulders be relaxed. Some teachers note a tendency to rely on the upper body while playing, and emphasize the importance of the holistic use of the body during performance.
Some groups in Japan, particularly those active in Tokyo, also emphasize the importance of the lively and spirited "iki" aesthetic. In taiko, it refers to very specific kinds of movement while performing that evoke the sophistication stemming from the mercantile and artisan classes active during the Edo period (1603–1868).
The sticks for playing taiko are called "bachi", and are made in various sizes and from different kinds of wood such as white oak, bamboo, and Japanese magnolia. "Bachi" are also held in a number of different styles. In "kumi-daiko", it is common for a player to hold their sticks in a relaxed manner between the V-shape of the index finger and thumb, which points to the player. There are other grips that allow performers to play much more technically difficult rhythms, such as the "shime" grip, which is similar to a matched grip: the "bachi" are gripped at the back end, and the fulcrum rests between the performer's index finger and thumb, while the other fingers remain relaxed and slightly curled around the stick.
Performance in some groups is also guided by principles based on Zen Buddhism. For instance, among other concepts, the San Francisco Taiko Dojo is guided by emphasizing communication, respect, and harmony. The way the "bachi" are held can also be significant; for some groups, "bachi" represent a spiritual link between the body and the sky. Some physical parts of taiko, like the drum body, its skin, and the tacks also hold symbolic significance in Buddhism.
Instrumentation.
"Kumi-daiko" groups consist primarily of percussive instruments where each of the drums plays a specific role. Of the different kinds of taiko, the most common in groups is the "nagadō-daiko". "Chū-daiko" are common in taiko groups and represent the main rhythm of the group, whereas "shime-daiko" set and change tempo. "Ō-daiko" provide a steady, underlying pulse and serve as a counter-rhythm to the other parts. It is common for performances to begin with a single stroke roll called an "". The player starts slowly, leaving considerable space between strikes, gradually shortening the interval between hits, until the drummer is playing a rapid roll of hits. Oroshi are also played as a part of theatrical performance, such as in Noh theater.
Drums are not the only instruments played in the ensemble; other Japanese instruments are also used. Other kinds of percussion instruments include the , a hand-sized gong played with a small mallet. In kabuki, the shamisen, a plucked string instrument, often accompanies taiko during the theatrical performance. "Kumi-daiko" performances can also feature woodwinds such as the shakuhachi and the shinobue.
Voiced calls or shouts called kakegoe and kiai are also common in taiko performance. They are used as encouragement to other players or cues for transition or change in dynamics such as an increase in tempo. In contrast, the philosophical concept of ma, which superficially describes the space between drum strikes, is also important in shaping rhythmic phrases and creating appropriate contrast.
Clothing.
There is a wide variety of traditional clothing that players wear during taiko performance. Common in many "kumi-daiko" groups is the use of the happi, a decorative, thin-fabric coat, and traditional headbands called hachimaki. Tabi, , and are also typical. During his time with the group Ondekoza, Eitetsu Hayashi suggested that a loincloth called a fundoshi be worn when performing for French fashion designer Pierre Cardin, who saw Ondekoza perform for him in 1975. The Japanese group Kodo has sometimes worn fundoshi for its performances.
Education.
Taiko performance is generally taught orally and through demonstration. Historically, general patterns for taiko were written down, such as in the 1512 encyclopedia called the "Taigensho", but written scores for taiko pieces are generally unavailable. One reason for the adherence to an oral tradition is that, from group to group, the rhythmic patterns in a given piece are often performed differently. Furthermore, ethnomusicologist William P. Malm observed that Japanese players within a group could not usefully predict one another using written notation, and instead did so through listening. In Japan, printed parts are not used during lessons.
Orally, patterns of onomatopoeia called kuchi shōga are taught from teacher to student that convey the rhythm and timbre of drum strikes for a particular piece. For example, represents a single strike to the center of the drum, where as represents two successive strikes, first by the right and then the left, and lasts the same amount of time as one "don" strike. Some taiko pieces, such as "Yatai-bayashi", include patterns that are difficult to represent in Western musical notation. The exact words used can also differ from region to region.
More recently, Japanese publications have emerged in an attempt to standardize taiko performance. The Nippon Taiko Foundation was formed in 1979; its primary goals were to foster good relations among taiko groups in Japan and to both publicize and teach how to perform taiko. Daihachi Oguchi, the leader of the Foundation, wrote "Japan Taiko" with other teachers in 1994 out of concern that correct form in performance would degrade over time. The instructional publication described the different drums used in "kumi-daiko" performance, methods of gripping, correct form, and suggestions on instrumentation. The book also contains practice exercises and transcribed pieces from Oguchi's group, Osuwa Daiko. While there were similar textbooks published before 1994, this publication had much more visibility due to the Foundation's scope.
The system of fundamentals "Japan Taiko" put forward was not widely adopted because taiko performance varied substantially across Japan. An updated 2001 publication from the Foundation, called the , describes regional variations that depart from the main techniques taught in the textbook. The creators of the text maintained that mastering a set of prescribed basics should be compatible with learning local traditions.
Regional styles.
Aside from "kumi-daiko" performance, a number of folk traditions that use taiko have been recognized in different regions in Japan. Some of these include from Sado Island, ' from the town of Kokura, and ' from Iwate Prefecture.
Eisa.
A variety of folk dances originating from Okinawa, known collectively as eisa, often make use of the taiko. Some performers use drums while dancing, and generally speaking, perform in one of two styles: groups on the Yokatsu Peninsula and on Hamahiga Island use small, single-sided drums called whereas groups near the city of Okinawa generally use "shime-daiko". Use of "shime-daiko" over "pāranku" has spread throughout the island, and is considered the dominant style. Small "nagadō-daiko", referred to as "ō-daiko" within the tradition, are also used and are worn in front of the performer. These drum dances are not limited to Okinawa and have appeared in places containing Okinawan communities such as in São Paulo, Hawaii, and large cities on the Japanese mainland.
Hachijō-daiko.
 is a taiko tradition originating on the island of Hachijō-jima. Two styles of "Hachijō-daiko" emerged and have been popularized among residents: an older tradition based on a historical account, and a newer tradition influenced by mainland groups and practiced by the majority of the islanders.
The "Hachijō-daiko" tradition was documented as early as 1849 based on a journal kept by an exile named Kakuso Kizan. He mentioned some of its unique features, such as "a taiko is suspended from a tree while women and children gathered around", and observed that a player used either side of the drum while performing. Illustrations from Kizan's journal show features of "Hachijō-daiko". These illustrations also featured women performing, which is unusual as taiko performance elsewhere during this period was typically reserved for men. Teachers of the tradition have noted that the majority of its performers were women; one estimate asserts that female performers outnumbered males by three to one.
The first style of Hachijō-daiko is thought to descend directly from the style reported by Kizan. This style is called "Kumaoji-daiko", named after its creator Okuyama Kumaoji, a central performer of the style. "Kumaoji-daiko" has two players on a single drum, one of whom, called the , provides the underlying beat. The other player, called the , builds on this rhythmical foundation with unique and typically improvised rhythms. While there are specific types of underlying rhythms, the accompanying player is free to express an original musical beat. "Kumaoji-daiko" also features an unusual positioning for taiko: the drums are sometimes suspended from ropes, and historically, sometimes drums were suspended from trees.
The contemporary style of "Hachijō-daiko" is called , which differs from "Kumaoji-daiko" in multiple ways. For instance, while the lead and accompanying roles are still present, "shin-daiko" performances use larger drums exclusively on stands. "Shin-daiko" emphasizes a more powerful sound, and consequently, performers use larger bachi made out of stronger wood. Looser clothing is worn by "shin-daiko" performers compared to kimono worn by "Kumaoji-daiko" performers; the looser clothing in "shin-daiko" allow performers to adopt more open stances and larger movements with the legs and arms. Rhythms used for the accompanying "shita-byōshi" role can also differ. One type of rhythm, called "yūkichi", consists of the following: 
This rhythm is found in both styles, but is always played faster in "shin-daiko". Another type of rhythm, called "honbadaki", is unique to "shin-daiko" and also contains a song which is performed in standard Japanese.
Miyake-daiko.
 is a style that has spread amongst groups through Kodo, and is formally known as . The word "miyake" comes from Miyake-jima, part of the Izu Islands, and the word "Kamitsuki" refers to the village where the tradition came from. Miyake-style taiko came out of performances for — a traditional festival held annually in July on Miyake Island since 1820 honoring the deity Gozu Tennō. In this festival, players perform on taiko while portable shrines are carried around town. The style itself is characterized in a number of ways. A "nagadō-daiko" is typically set low to the ground and played by two performers, one on each side; instead of sitting, performers stand and hold a stance that is also very low to the ground, almost to the point of kneeling.
Outside Japan.
Australia.
Taiko groups in Australia began forming in the 1990s. The first group, called Ataru Taru Taiko, was formed in 1995 by Paulene Thomas, Harold Gent, and Kaomori Kamei. TaikOz was later formed by percussionist Ian Cleworth and Riley Lee, a former Ondekoza member, and has been performing in Australia since 1997. They are known for their work in generating interest in performing taiko among Australian audiences, such as by developing a complete education program with both formal and informal classes, and have a strong fan base. Cleworth and other members of the group have developed several original pieces.
Brazil.
The introduction of "kumi-daiko" performance in Brazil can be traced back to the 1970s and 1980s in São Paulo. Tangue Setsuko founded an eponymous taiko dojo and was Brazil's first taiko group; Setsuo Kinoshita later formed the group Wadaiko Sho. Brazilian groups have combined native and African drumming techniques with taiko performance. One such piece developed by Kinoshita is called "Taiko de Samba", which emphasizes both Brazilian and Japanese aesthetics in percussion traditions. Taiko was also popularized in Brazil from 2002 through the work of Yukihisa Oda, a Japanese native who visited Brazil several times through the Japan International Cooperation Agency.
The Brazilian Association of Taiko (ABT) suggests that there are about 150 taiko groups in Brazil and that about 10–15% of players are non-Japanese; Izumo Honda, coordinator of a large annual festival in São Paulo, estimated that about 60% of all taiko performers in Brazil are women.
North America.
Taiko emerged in the United States in the late 1960s. The first group, San Francisco Taiko Dojo, was formed in 1968 by Seiichi Tanaka, a postwar immigrant who studied taiko in Japan and brought the styles and teachings to the US. A year later, a few members of Senshin Buddhist Temple in Los Angeles led by its minister Masao Kodani initiated another group called Kinnara Taiko. San Jose Taiko later formed in 1973 in Japantown, San Jose, under Roy and PJ Hirabayashi. Taiko started to branch out to the eastern US in the late 1970s. This included formation of Denver Taiko in 1976 and Soh Daiko in New York City in 1979. Many of these early groups lacked the resources to equip each member with a drum and resorted to makeshift percussion materials such as rubber tires or creating taiko out of wine barrels.
Japanese-Canadian taiko began in 1979 with Katari Taiko, and was inspired by the San Jose Taiko group. Its early membership was predominantly female. Katari Taiko and future groups were thought to represent an opportunity for younger, third-generation Japanese Canadians to explore their roots, redevelop a sense of ethnic community, and expand taiko into other musical traditions.
There are no official counts or estimates of the number of active taiko groups in the United States or Canada, as there is no governing body for taiko groups in either country. Unofficial estimates have been made. In 1989, there were as many as 30 groups in the US and Canada, seven of which were in California. One estimate suggested that around 120 groups were active in the US and Canada as of 2001, many of which could be traced to the San Francisco Taiko Dojo; later estimates in 2005 and 2006 suggested there were about 200 groups in the United States alone.
The Cirque du Soleil shows "Mystère" in Las Vegas and "Dralion" have featured taiko performance. Taiko performance has also been featured in commercial productions such as the 2005 Mitsubishi Eclipse ad campaign, and in events such as the 2009 Academy Awards and 2011 Grammy Awards.
From 2005 to 2006, the Japanese American National Museum held an exhibition called "Big Drum: Taiko in the United States". The exhibition covered several topics related to taiko in the United States, such as the formation of performance groups, their construction using available materials, and social movements. Visitors were able to play smaller drums.
Related cultural and social movements.
Certain peoples have used taiko to advance social or cultural movements, both within Japan and elsewhere in the world.
Gender conventions.
Taiko performance has frequently been viewed as an art form dominated by men. Historians of taiko argue that its performance comes from masculine traditions. Those who developed ensemble-style taiko in Japan were men, and through the influence of Ondekoza, the ideal taiko player was epitomized in images of the masculine peasant class, particularly through the character Muhōmatsu in the 1958 film "Rickshaw Man". Masculine roots have also been attributed to perceived capacity for "spectacular bodily performance" where women's bodies are sometimes judged as unable to meet the physical demands of playing.
Before the 1980s, it was uncommon for Japanese women to perform on traditional instruments, including taiko, as their participation had been systematically restricted. In Ondekoza and in the early performances of Kodo, women performed only dance routines either during or between taiko performances. Thereafter, female participation in "kumi-daiko" started to rise dramatically, and by the 1990s, women equaled and possibly exceeded representation by men. While the proportion of women in taiko has become substantial, some have expressed concern that women still do not perform in the same roles as their male counterparts and that taiko performance continues to be a male-dominated profession. For instance, a member of Kodo was informed by the director of the group's apprentice program that women were permitted to play, but could only play "as women". Other women in the apprentice program recognized a gender disparity in performance roles, such as what pieces they were allowed to perform, or in physical terms based on a male standard.
Female taiko performance has also served as a response to gendered stereotypes of Japanese women as being quiet, subservient, or a femme fatale. Through performance, some groups believe they are helping to redefine not only the role of women in taiko, but how women are perceived more generally.
Burakumin.
Those involved in the construction of taiko are usually considered part of the burakumin, a marginalized minority class in Japanese society, particularly those working with leather or animal skins. Prejudice against this class dates back to the Tokugawa period in terms of legal discrimination and treatment as social outcasts. Although official discrimination ended with the Tokugawa era, the burakumin have continued to face social discrimination, such as scrutiny by employers or in marriage arrangements. Drum makers have used their trade and success as a means to advocate for an end to discriminatory practices against their class.
The , representing the contributions of burakumin, is found in Naniwa Ward in Osaka, home to a large proportion of burakumin. Among other features, the road contains taiko-shaped benches representing their traditions in taiko manufacturing and leatherworking, and their impact on nationalculture. The road ends at the Osaka Human Rights Museum, which exhibits the history of systematic discrimination against the burakumin. The road and museum were developed in part due an advocacy campaign led by the Buraku Liberation League and a taiko group of younger performers called .
North American "sansei".
Taiko performance was an important part of cultural development by third-generation Japanese residents of in North America, who are called "sansei". During World War II, second-generation Japanese residents, called "nisei" faced internment in the United States and in Canada on the basis of their race. During and after the war, Japanese residents were discouraged from activities such as speaking Japanese or forming ethnic communities. Subsequently, sansei could not engage in Japanese culture and instead were raised to assimilate into more normative activities. There were also prevailing stereotypes of Japanese people, which sansei sought to escape or subvert. The United States civil rights movement of the 1960s influenced sansei to reexamine their heritage by engaging in Japanese culture in their communities; one such approach was through taiko performance. Groups such as San Jose Taiko were organized to fulfill a need for solidarity and to have a medium to express their experiences as Japanese-Americans. Later generations have adopted taiko in programs or workshops established by sansei; social scientist Hideyo Konagaya remarks that this attraction to taiko among other Japanese art forms may be due to its accessibility and energetic nature. Konagaya has also argued that the resurgence of taiko in the United States and Japan are differently motivated: in Japan, performance was meant to represent the need to recapture sacred traditions, while in the United States it was meant to be an explicit representation of masculinity and power in Japanese-American men.
Notable performers and groups.
A number of performers and groups, including several early leaders, have been recognized for their contributions to taiko performance. Daihachi Oguchi was best known for developing "kumi-daiko" performance. Oguchi founded the first "kumi-daiko" group called Osuwa Daiko in 1951, and facilitated the popularization of taiko performance groups in Japan.
Seidō Kobayashi is the leader of the Tokyo-based taiko group Oedo Sukeroku Taiko as of December 2014. Kobayashi founded the group in 1959 and was the first group to tour professionally. Kobayashi is considered a master performer of taiko. He is also known for asserting intellectual control of the group's performance style, which has had an impact on performance for many groups, particularly in North America.
In 1969, founded Ondekoza, a group well known for making taiko performance internationally visible and for its artistic contributions to the tradition. Den was also known for developing a communal living and training facility for Ondekoza on Sado Island in Japan, which had a reputation for its intensity and broad education programs in folklore and music.
Performers and groups beyond the early practitioners have also been noted. Eitetsu Hayashi is best known for his solo performance work. Hayashi joined Ondekoza when he was 19, and after parting from the group helped found Kodo, one of the best known and most influential taiko performance groups in the world. Hayashi soon left the group to begin a solo career and has performed in venues such as Carnegie Hall in 1984, the first featured taiko performer there. He was awarded the 47th Education Minister's Art Encouragement Prize, a national award, in 1997 as well as the 8th Award for the Promotion of Traditional Japanese Culture from the Japan Arts Foundation in 2001.
Seiichi Tanaka is the founder of the San Francisco Taiko Dojo and is regarded as the primary developer of taiko performance in the United States. He was a recipient of a 2001 National Heritage Fellowship awarded by the National Endowment for the Arts.
Tokyo Dagekidan has performed worldwide, including at the National Theater of Japan, at the closing ceremonies of the 1998 FIFA World Cup in France, and the 2014 Festival Internacional Cervantino in Mexico. The group also appeared in "Kyoki no Sakura", a film produced by Toei.

</doc>
<doc id="8716" url="https://en.wikipedia.org/wiki?curid=8716" title="Dolly Parton">
Dolly Parton

Dolly Rebecca Parton Dean (born January 19, 1946) is an American singer-songwriter, actress, author, businesswoman, and humanitarian, known primarily for her work in country music. Her career began as a child performer, then recording a few singles from the age of 13. Relocating to Nashville at age 18 in 1964, her first commercial successes were as a songwriter. She rose to prominence in 1967 as a featured performer on singer Porter Wagoner's weekly syndicated TV program; their first duet single, "The Last Thing on My Mind", was a top-ten hit on the country singles chart and led to several successful albums before they ended their partnership in 1974. Moving towards mainstream pop music, her 1977 single "Here You Come Again" was a success on both the country and pop charts. A string of pop-country hits followed into the mid-1980s, the most successful being her 1980 hit "9 to 5" and her 1983 duet with Kenny Rogers "Islands in the Stream", both of which topped the U.S. pop and country singles charts. A pair of albums recorded with Linda Ronstadt and Emmylou Harris were among her later successes. In the late 1990s, she returned to classic country/bluegrass with a series of acclaimed recordings. Non-musical ventures include Dollywood, a theme park in Pigeon Forge in the Smoky Mountains of Tennessee, and her efforts on behalf of childhood literacy, particularly her Imagination Library, as well as Dolly Parton's Dixie Stampede and Pirates Voyage Dinner & Show.
Parton is the most honored female country performer of all time. Achieving 25 RIAA certified gold, platinum, and multi-platinum awards, she has had 25 songs reach No. 1 on the Billboard Country charts, a record for a female artist. She has 41 career top 10 country albums, a record for any artist, and she has 110 career charted singles over the past 40 years. All-inclusive sales of singles, albums, hits collections, and digital downloads during her career have topped 100 million worldwide. She has garnered eight Grammy Awards, two Academy Award nominations, ten Country Music Association Awards, seven Academy of Country Music Awards, three American Music Awards, and is one of only seven female artists to win the Country Music Association's Entertainer of the Year Award. Parton has received 46 Grammy nominations, tying her with Bruce Springsteen for the most Grammy nominations and placing her in tenth place overall.
In 1999, Parton was inducted into the Country Music Hall of Fame. She has composed over 3,000 songs, notably "I Will Always Love You" (a two-time U.S. country chart-topper for Parton, as well as an international pop hit for Whitney Houston). She is also one of the few to have received at least one nomination from the Academy Awards, Grammy Awards, Tony Awards, and Emmy Awards.
As an actress, she starred in films such as "9 to 5", "The Best Little Whorehouse in Texas", and "Rhinestone".
Early years.
Parton was born in Sevier County, Tennessee, the fourth of twelve children of Robert Lee Parton (1921–2000), a farmer and construction worker, and his wife Avie Lee (née Owens) (1923–2003). Parton's middle name comes from her maternal great-great grandmother, Rebecca (Dunn) Whitted (1861–1930). She has described her family as being "dirt poor". Parton's father paid with a bag of oatmeal the doctor who helped deliver her. She outlined her family's poverty in her early songs: "Coat of Many Colors" and "In the Good Old Days (When Times Were Bad)". They lived in a rustic, one-room cabin in Locust Ridge, just north of the Greenbrier Valley of the Great Smoky Mountains, a predominantly Pentecostal area.
Music played an important role in her early life. She was brought up in the Church of God, the church her grandfather, Jake Robert Owens (1899–1992) pastored. Her earliest public performances were in the church, beginning at age six. At seven, she started playing a homemade guitar. When she was eight years old, her uncle gave her her first real guitar.
Career discovery.
Parton began performing as a child, singing on local radio and television programs in the Eastern Tennessee area. By ten, she was appearing on "The Cas Walker Show" on both WIVK Radio and WBIR-TV in Knoxville, Tennessee. At thirteen, she was recording (the single "Puppy Love") on a small Louisiana label, Goldband Records, and appeared at the Grand Ole Opry where she first met Johnny Cash, who encouraged her to follow her own instincts regarding her career.
The day after she graduated from high school in 1964, she moved to Nashville. Her initial success came as a songwriter, having signed with Combine Publishing shortly after her arrival; with her frequent songwriting partner, her uncle Bill Owens, she wrote several charting singles during this time, including two top ten hits: Bill Phillips's 1966 record "Put It Off Until Tomorrow", and Skeeter Davis' 1967 hit "Fuel to the Flame". Her songs were recorded by many other artists during this period, including Kitty Wells and Hank Williams Jr.. She signed with Monument Records in 1965, at 19, where she was initially pitched as a bubblegum pop singer. She released a string of singles, but the only one that charted, "Happy, Happy Birthday Baby", did not crack the "Billboard" Hot 100. Although she expressed a desire to record country material, Monument resisted, thinking her unique voice with its strong vibrato was not suited to the genre.
It was only after her composition, "Put It Off Until Tomorrow", as recorded by Bill Phillips (and with Parton, uncredited, on harmony), went to No. 6 on the country chart in 1966, that the label relented and allowed her to record country. Her first country single, "Dumb Blonde" (one of the few songs during this era that she recorded but did not write), reached No. 24 on the country chart in 1967, followed by "Something Fishy", which went to No. 17. The two songs appeared on her first full-length album, "Hello, I'm Dolly".
Marriage.
On May 30, 1966, Parton and Carl Thomas Dean (born in Nashville, Tennessee) were married in Ringgold, Georgia. Although Parton does not use Dean's surname professionally, she has stated that her passport says "Dolly Parton Dean" and that she sometimes uses Dean when signing contracts.
Dean, who runs an asphalt road-paving business in Nashville, has always shunned publicity and rarely accompanies his wife to any events. According to Parton, he has only ever seen her perform once. However, she has also commented in interviews that, although it appears they spend little time together, it is simply that nobody sees him publicly. She has commented on Dean's romantic side, saying that he does spontaneous things to surprise her and sometimes even writes poems for her.
Parton and Dean helped raise several of Parton's younger siblings in Nashville, leading her nieces and nephews to refer to her as "Aunt Granny", a moniker that later lent its name to one of Parton's Dollywood restaurants. The couple have no children of their own but Parton is the godmother of performer Miley Cyrus.
In 2011, the couple celebrated their 45th anniversary. Later, Parton said, "We're really proud of our marriage. It's the first for both of us. And the last."
Music career.
1967–75: Country music success.
In 1967, musician and country music entertainer Porter Wagoner invited Parton to join his organization, offering her a regular spot on his weekly syndicated television program "The Porter Wagoner Show", as well as in his road show. As documented in her 1994 autobiography, initially, much of Wagoner's audience was unhappy that Norma Jean, the performer whom Parton had replaced, had left the show, and was reluctant to accept Parton (sometimes chanting loudly for Norma.. Jean from the audience). With Wagoner's assistance, however, Parton was eventually accepted. Wagoner convinced his label, RCA Victor, to sign her. RCA decided to protect their investment by releasing her first single as a duet with Wagoner. That song, a cover of Tom Paxton's "The Last Thing on My Mind", released in late 1967, reached the country top ten in January 1968, launching a six-year streak of virtually uninterrupted top ten singles for the pair.
Parton's first solo single for RCA Victor, "Just Because I'm a Woman", was released in the summer of 1968 and was a moderate chart hit, reaching No. 17. For the remainder of the decade, none of her solo efforts – even "In the Good Old Days (When Times Were Bad)", which later became a standard – were as successful as her duets with Wagoner. The duo was named "Vocal Group of the Year" in 1968 by the Country Music Association, but Parton's solo records were continually ignored. Wagoner had a significant financial stake in her future: as of 1969, he was her co-producer and owned nearly half of Owe-Par, the publishing company Parton had founded with Bill Owens. By 1970, both Parton and Wagoner had grown frustrated by her lack of solo chart success. Wagoner persuaded Parton to record Jimmie Rodgers's "Mule Skinner Blues", a gimmick that worked. The record shot to No. 3, followed closely, in February 1971, by her first number-one single, "Joshua". For the next two years, she had numerous solo hits – including her signature song "Coat of Many Colors" (#4, 1971) – in addition to her duets. Top twenty singles included "The Right Combination" and "Burning the Midnight Oil" (both duets with Porter Wagoner, 1971); "Lost Forever in Your Kiss" (with Wagoner) and "Touch Your Woman (1972); and "My Tennessee Mountain Home" and "Travelin' Man" (1973).
Although her solo singles and the Wagoner duets were successful, her biggest hit of this period was "Jolene". Released in late 1973, it topped the country chart in February 1974, and reached the lower regions of the Hot 100 (it eventually also charted in the UK, reaching No. 7 in 1976, representing Parton's first UK success). Parton, who'd always envisioned a solo career, made the decision to leave Wagoner's organization; the pair performed their last duet concert in April 1974, and she stopped appearing on his TV show in mid-1974, although they remained affiliated; he helped produce her records through 1975. The pair continued to release duet albums, their final release being 1975's "Say Forever You'll Be Mine".
In 1974, her song, "I Will Always Love You", written about her professional break from Wagoner, went to No. 1 on the country chart. Around the same time, Elvis Presley indicated that he wanted to cover the song. Parton was interested until Presley's wily manager, Colonel Tom Parker, told her that it was standard procedure for the songwriter to sign over half of the publishing rights to any song recorded by Presley. Parton refused. That decision has been credited with helping to make her many millions of dollars in royalties from the song over the years.
Parton had three solo singles reach No. 1 on the country chart in 1974 ("Jolene", "I Will Always Love You", and "Love Is Like a Butterfly"), as well as the duet with Porter Wagoner, "Please Don't Stop Loving Me"; she again topped the singles chart in 1975 with "The Bargain Store".
1976–86: Branching out into pop music.
From 1974 to 1980, she consistently charted in the country Top 10, with eight singles reaching No. 1. Parton had her own syndicated television variety show, "Dolly!" (1976–77). During this period, many performers, including Rose Maddox, Kitty Wells, Olivia Newton-John, Emmylou Harris, and Linda Ronstadt covered her songs. Her siblings Randy and Stella both received recording contracts of their own.
During this period, Parton began to embark on a high-profile crossover campaign, attempting to aim her music in a more mainstream direction and increase her visibility outside of the confines of country music. In 1976, she began working closely with Sandy Gallin, who would serve as her personal manager for the next 25 years. With her 1976 album "All I Can Do", which she co-produced with Porter Wagoner, Parton began taking more of an active role in production, and began specifically aiming her music in a more mainstream, pop direction. Her first entirely self-produced effort, "New Harvest ... First Gathering" (1977), highlighted her pop sensibilities, both in terms of choice of songs – the album contained covers of the pop and R&B classics "My Girl" and "Higher and Higher" – and production. Though the album was well received and topped the U.S. country albums chart, neither it, nor its single "Light of a Clear Blue Morning" made much of an impact on the pop charts.
After "New Harvest"'s disappointing chart performance, Parton turned to high profile pop producer Gary Klein for her next album. The result, 1977's "Here You Come Again," became her first million-seller, topping the country album chart and reaching No. 20 on the pop chart; the Barry Mann-Cynthia Weil-penned title track topped the country singles chart, and became Parton's first top-ten single on the pop chart (#3). A second single, the double A-sided "Two Doors Down"/"It's All Wrong, But It's All Right" topped the country chart and crossed over to the pop top twenty. For the remainder of the 1970s and into the early 1980s, many of her subsequent singles charted on both charts simultaneously. Her albums during this period were developed specifically for pop-crossover success.
In 1978, Parton won a Grammy Award for Best Female Country Vocal Performance for her "Here You Come Again" album. She continued to have hits with "Heartbreaker" (1978), "Baby I'm Burning" (1979), and "You're the Only One" (1979), all of which charted in the pop Top 40 and topped the country chart. "Sweet Summer Lovin'" (1979) became the first Parton single in two years to not top the country chart (though it did reach the Top 10). During this period, her visibility continued to increase, with multiple television appearances. A highly publicized candid interview on a "Barbara Walters Special" in 1977 (timed to coincide with "Here You Come Again"'s release) was followed by appearances in 1978 on Cher's ABC television special, and her own joint special with Carol Burnett on CBS, "Carol and Dolly in Nashville".
Parton served as one of three co-hosts (along with Roy Clark and Glen Campbell) on the CBS special "Fifty Years of Country Music". In 1979, Parton hosted the NBC special "The Seventies: An Explosion of Country Music", performed live at the Ford Theatre in Washington, D.C., and whose audience included President Jimmy Carter.
Her commercial success grew in 1980, with three consecutive No. 1 hits: the Donna Summer-written "Starting Over Again", "Old Flames Can't Hold a Candle to You", and "9 to 5", which topped the country and pop charts in early 1981. She had another Top 10 single that year with "Making Plans", a single released from a 1980 reunion album with Porter Wagoner.
"9 to 5", the theme song to the 1980 feature film "9 to 5" she starred in along with Jane Fonda and Lily Tomlin, not only reached No. 1 on the country chart, but also, in February 1981, reached No. 1 on the pop and the adult-contemporary charts, giving her a triple No. 1 hit. Parton became one of the few female country singers to have a No. 1 single on the country and pop charts simultaneously. It also received a nomination for an Academy Award for Best Original Song. Her singles continued to appear consistently in the country Top 10: between 1981 and 1985, she had 12 Top 10 hits; half of them hit No. 1. She continued to make inroads on the pop chart as well. A re-recorded version of "I Will Always Love You" from the feature film, "The Best Little Whorehouse in Texas" (1982) scraped the Top 50 that year and her duet with Kenny Rogers, "Islands in the Stream" (written by the Bee Gees and produced by Barry Gibb), spent two weeks at No. 1 in 1983.
Other chart hits during this period included her chart-topping cover of the 1969 First Edition hit, "But You Know I Love You" and "The House of the Rising Sun" (1981); "Single Women", "Heartbreak Express", and "Hard Candy Christmas" (1982); and "Potential New Boyfriend" (1983), which was accompanied by one of her first music videos and reached the U.S. dance chart. She continued to explore new business and entertainment ventures such as her Dollywood theme park that opened in 1986 in Pigeon Forge, Tennessee.
In the mid-1980s, her record sales were still relatively strong, with "Save the Last Dance for Me", "Downtown", "Tennessee Homesick Blues" (1984); "Real Love" (another duet with Kenny Rogers), "Don't Call It Love" (1985); and "Think About Love" (1986) all reaching the country Top 10. ("Tennessee Homesick Blues" and "Think About Love" reached No. 1; "Real Love" also reached No. 1 on the country chart and became a modest crossover hit). However, RCA Records did not renew her contract after it expired that year, and she signed with Columbia Records in 1987.
1987–94: Return to country roots.
Along with Emmylou Harris and Linda Ronstadt, she released "Trio" (1987) to critical acclaim. The album revitalized Parton's music career, spending five weeks at No. 1 on Billboard's Country Albums chart, and also reached the top-ten on Billboard's Top-200 Albums chart. It sold several million copies and producing four Top 10 country hits including Phil Spector's "To Know Him Is to Love Him", which went to No. 1. "Trio" won the Grammy Award for Best Country Performance by a Duo or Group with Vocal and was nominated for a Grammy Award for Album of the Year. After a further attempt at pop success with "Rainbow" (1987), including the single "The River Unbroken", Parton focused on recording country material. "White Limozeen" (1989) produced two No. 1 hits in "Why'd You Come in Here Lookin' Like That" and "Yellow Roses". Although it looked like Parton's career had been revived, it was actually just a brief revival before contemporary country music came on in the early 1990s and moved most veteran artists off the chart.
A duet with Ricky Van Shelton, "Rockin' Years" (1991), reached No. 1, though Parton's greatest commercial fortune of the decade came when Whitney Houston recorded "I Will Always Love You" for the soundtrack of the feature film "The Bodyguard" (1992); both the single and the album were massively successful. Parton's soundtrack album from the 1992 film, "Straight Talk", however, was less successful. But her 1993 album "Slow Dancing with the Moon" won critical acclaim, and did well on the charts, reaching No. 4 on the country albums chart, and No. 16 on the Billboard 200 album chart. She recorded "The Day I Fall in Love" as a duet with James Ingram for the feature film "Beethoven's 2nd" (1993). The songwriters (Sager, Ingram, and Clif Mangess) were nominated for an Academy Award for Best Original Song and Parton and Ingram performed the song on the awards telecast. Similar to her earlier collaborative album with Harris and Ronstadt, Parton released "Honky Tonk Angels" in the fall of 1993 with Loretta Lynn and Tammy Wynette. It was certified as a gold album by the Recording Industry Association of America and helped revive both Wynette's and Lynn's careers. Also in 1994, Parton contributed the song "You Gotta Be My Baby" to the AIDS benefit album "Red Hot + Country" produced by the Red Hot Organization. A live acoustic album, "", featuring stripped down versions of some of her hits, as well as some traditional songs, was released in late 1994.
1995–present.
Parton's recorded music during the mid-to late 1990s remained steady, though somewhat eclectic. Her 1995 re-recording of "I Will Always Love You" (performed as a duet with Vince Gill), from her album "Something Special" won the Country Music Association's Vocal Event of the Year Award. The following year, "Treasures", an album of covers of 1960s/70s hits was released, and featured a diverse collection of material, including songs by Mac Davis, Pete Seeger, Kris Kristofferson, Cat Stevens, and Neil Young. Her recording of Stevens' "Peace Train" was later remixed and released as a dance single, reaching Billboard's dance singles chart. Her 1998 country-rock album "Hungry Again" was made up entirely of her own compositions. Although neither of the album's two singles, "(Why Don't More Women Sing) Honky Tonk Songs" and "Salt in my Tears", charted, videos for both songs received significant airplay on CMT. A second and more contemporary collaboration with Harris and Ronstadt, "Trio II", was released in early 1999. Its cover of Neil Young's song "After the Gold Rush" won a Grammy Award for Best Country Collaboration with Vocals. Parton was also inducted into the Country Music Hall of Fame in 1999.
Parton recorded a series of bluegrass-inspired albums, beginning with "The Grass Is Blue" (1999), winning a Grammy Award for Best Bluegrass Album, and "Little Sparrow" (2001), with its cover of Collective Soul's "Shine" winning a Grammy Award for Best Female Country Vocal Performance. The third, "Halos & Horns" (2002) included a bluegrass version of the Led Zeppelin song "Stairway to Heaven". In 2005 she released "Those Were The Days" consisting of her interpretations of hits from the folk-rock era of the late 1960s and early 1970s, including "Imagine", "Where Do the Children Play?", "Crimson and Clover", and "Where Have All the Flowers Gone?"
Parton earned her second Academy Award nomination for Best Original Song for "Travelin' Thru", which she wrote specifically for the feature film "Transamerica" (2005). Due to the song's (and film's) uncritical acceptance of a transgender woman, Parton received death threats. She returned to No. 1 on the country chart later in 2005 by lending her distinctive harmonies to the Brad Paisley ballad, "When I Get Where I'm Goin'".
The music-competition reality-television show "American Idol" (since 2002) has weekly themes and the April 1–2, 2008, episodes' theme was "Dolly Parton Songs" with the nine then-remaining contestants each singing a Parton composition. Parton participated as a "guest mentor" to the contestants and also performed "Jesus and Gravity" (from "Backwoods Barbie" and released as a single in March 2008) receiving a standing ovation from the studio audience.
In September 2007, Parton released her first single from her own record company, Dolly Records, titled, "Better Get to Livin'", which eventually peaked at No. 48 on Billboard's Hot Country Songs chart. It was followed by the studio album "Backwoods Barbie", which was released on February 26, 2008, and reached No. 2 on the country chart. The album's debut at No. 17 on the all-genre "Billboard" 200 albums chart was the highest in her career. "Backwoods Barbie" produced four additional singles, including the title track, written as part of her score for "9 to 5: The Musical", an adaptation of her feature film "Nine to Five". After the sudden death of Michael Jackson, whom Parton knew personally, she released a video in which she somberly told of her feelings on Jackson and his death.
On October 27, 2009, Parton released a four-CD box set, "Dolly", which featured 99 songs and spanned most of her career. She released her second live DVD and album, "Live From London" in October 2009, which was filmed during her sold out 2008 concerts at London's The O2 Arena. In 2010, she was said to have been working on a dance-oriented album, "Dance with Dolly", but as of June 2015 the album had not been released. 
With longtime friend Billy Ray Cyrus, Parton released their album "Brother Clyde" on August 10, 2010. Parton is featured on "The Right Time", which she co-wrote with Cyrus and Morris Joseph Tancredi. On January 6, 2011, Parton announced that her new album would be titled "Better Day". In February 2011, she announced that she would embark on the Better Day World Tour on July 17, 2011, with shows in northern Europe and the U.S. The album's lead-off single, "Together You and I", was released on May 23, 2011, and "Better Day" was released on June 28, 2011. In 2011, Parton voiced the character Dolly Gnome in the animated film "Gnomeo & Juliet".
On February 11, 2012, after the sudden death of Whitney Houston, Dolly Parton stated, "Mine is only one of the millions of hearts broken over the death of Whitney Houston. I will always be grateful and in awe of the wonderful performance she did on my song, and I can truly say from the bottom of my heart, "Whitney, I will always love you. You will be missed." In 2013, Parton joined Lulu Roman for a recording of "I Will Always Love You" for Roman's album, "At Last".
In 2013, Parton and Kenny Rogers reunited for the title song of his album "You Can't Make Old Friends". For their performance, they were nominated at the 2014 Grammy Awards for Grammy Award for Best Country Duo/Group Performance.
In 2014, Parton embarked on the Blue Smoke World Tour in support of her forty-second studio album, "Blue Smoke". The album was first released in Australia & New Zealand on January 31 to coincide with tour dates there in February, and reached the top 10 in both countries. It was released in the US on May 13, and debuted at No. 6 on the Billboard 200 chart, making it her first top 10 album and her highest-charting solo album ever; it also reached the No. 2 position on the US country chart. The album was released in Europe on June 9, and reached No. 2 on the UK album chart. On June 29, 2014, Parton performed for the first time at the UK Glastonbury Festival performing songs such as "Jolene", "9 to 5"and "Coat of Many Colors" to a crowd of over 180,000.
In concert and on tour.
Parton toured extensively from the late 1960s until the early 1990s. In the 1960s and early 1970s, Parton toured as a member of Porter Wagoner's road show, as well as with other country musicians, including George Jones and Linda Ronstadt. Upon leaving Wagoner's organization in 1974, Parton formed her own "Travelin' Family Band", made up largely of siblings, cousins, and other family members, and touring with other acts, including Willie Nelson and Mac Davis.
In 1976, she disbanded the Travelin' Family Band, to form a new band, Gypsy Fever, composed of seasoned musicians who had more of a rock sensibility, to support her impending crossover. Parton toured as a headline act with Gypsy Fever from 1977 to 1979, to promote her albums, "New Harvest - First Gathering", "Here You Come Again",
"Heartbreaker", and "Great Balls of Fire". In the 1980s, movie roles and other ventures caused Parton to tour less than she had done during the previous decade. In 1982 and early 1983, she toured in support of her "Heartbreak Express" album, but health problems resulted in the cancellation of a several of that tour's dates. From 1984 to 1985, she toured alongside Kenny Rogers for the "Real Love Tour". She continued touring in 1986 with the "Think About Love Tour", and 1989 for the "White Limozeen Tour". Parton's only tour in the 1990s was between 1991–92 in support of her "Eagle When She Flies" album. 
Dollywood Foundation Shows.
From the early 1990s through 2001, her concert appearances were primarily limited to one weekend a year at Dollywood to benefit her Dollywood Foundation. In 2015, Dolly announced plans to debut her new show, "Pure and Simple," with four scheduled show dates for July and August.
Halos & Horns Tour.
After a decade-long absence from touring, Parton decided to return in 2002 with the Halos & Horns Tour, an 18-city, intimate club tour to promote "Halos & Horns" (2002). House of Blues Entertainment, Inc. produced the tour and sold out all its North American and European dates.
Hello, I'm Dolly Tour.
She returned to mid-sized stadium venues in 2004 with her 39-show, U.S. and Canadian Hello, I'm Dolly Tour, a glitzier, more elaborate stage show than two years earlier. With nearly 140,000 tickets sold, it was the tenth-biggest country tour of the year and grossed more than $6 million.
The Vintage Tour.
In late 2005, Parton completed a 40-city tour with The Vintage Tour promoting her new "Those Were the Days" (2005).
The October 2, 2005, Vintage show in San Francisco's Golden Gate Park was part of the free Hardly Strictly Bluegrass festival. The San Francisco Chronicle reported afterwards that an estimated 200,000 people had attended Dolly's performance.
An Evening with Dolly Parton.
Parton scheduled mini concerts in late 2006 throughout the U.S. and Canada as a gear-up to her 17-city, 21-date An Evening with Dolly Parton. Running from March 6 to April 3, 2007, this was her first world tour in many years and her first tour in the United Kingdom since 2002. The tour sold out in every European city and gained positive reviews. It grossed just over $16 million. 
Backwoods Barbie Tour.
In 2008, Parton went on the Backwoods Barbie Tour. It was set to begin in the U.S. (February–April 2008) to coincide with the release of "Backwoods Barbie" (2008), her first mainstream-country album in 17 years. However, due to health problems she postponed all U.S. dates. The tour started March 28, 2008, with 13 U.S. dates, followed by 17 European shows.
She returned to the U.S. with a concert at Humphrey's By The Bay in San Diego on August 1, 2008. She performed her Backwoods Barbie Tour on August 3, 2008, at the Greek Theatre in Los Angeles to a sold-out crowd and standing ovations. From August 1 to November 1, she scheduled 16 dates on both the east and west coasts of the U.S.
Her concerts at London's O2 Arena were the subject of a PBS Special
Better Day World Tour.
In 2011, Parton embarked on the Better Day World Tour to promote her 41st studio album, "Better Day" (2011). The tour began on July 17 and ended on December 1. With 49 shows, she visited the United States, Northern Europe, and Australia. The ticket sales were nearly 275,000 and the overall gross was $34 million, making the tour Parton's most successful. This tour was her first to visit Australia in 30 years.
Blue Smoke World Tour.
In January 2014, Parton embarked on the Blue Smoke World Tour to promote her 42nd studio album, "Blue Smoke" touring the United States, Europe, and Australia.
Songwriting.
Parton is a prolific songwriter, having begun by writing country-music songs with strong elements of folk music, based upon her upbringing in humble mountain surroundings, and reflecting her family's Christian background. Her songs "Coat of Many Colors", "I Will Always Love You", and "Jolene", among others, have become classics in the field. On November 4, 2003, Parton was honored as a BMI Icon at the 2003 BMI Country Awards.
Parton has earned over 35 BMI Pop and Country Awards throughout her songwriting career. In 2001, she was inducted into the Songwriters Hall of Fame. In a 2009 interview with CNN's "Larry King", Parton indicated she had written "at least 3,000" songs, having written seriously since the age of seven. Parton went on to say that she writes something every day, be it a song or an idea.
Compositions in films and television and covers.
Parton's songwriting has been featured prominently in several films. In addition to the title song for "Nine to Five" (1980), she also recorded a second version of "I Will Always Love You" for "The Best Little Whorehouse in Texas" (1982). The second version was a No. 1 country hit and also managed to reach the pop charts, going to No. 53.
"I Will Always Love You" has been covered by many country artists, including Ronstadt on "Prisoner In Disguise" (1975); Kenny Rogers on "Vote for Love" (1996); and LeAnn Rimes on ' (1997). Whitney Houston performed it on ' (1992) film soundtrack and her version became the best-selling hit both written and performed by a female vocalist, with worldwide sales of over twelve million copies. In addition, the song has been translated into Italian and performed by the Welsh opera singer Katherine Jenkins, a fact referred to by Parton herself at the Backwoods Barbie tour concert in Birmingham (UK).
As a songwriter, Parton has twice been nominated for an Academy Award for Best Original Song, for "9 to 5" (1980) and "Travelin' Thru" (2005) from the transgender themed film "Transamerica". "Travelin' Thru" won as Best Original Song award at the Phoenix Film Critics Society Awards (2005). The song was also nominated for both the Golden Globe Award for Best Original Song (2005) and the Broadcast Film Critics Association Award (also known as the Critics' Choice Awards) for Best Song (2005). A cover version of "Love Is Like A Butterfly", recorded by singer Clare Torry, was used as the theme music for the British TV show "Butterflies".
"9 to 5: The Musical".
Parton wrote the score (and Patricia Resnick wrote the book) for "", a musical-theatre adaptation of Parton's feature film "Nine to Five" (1980). The musical ran at the Ahmanson Theatre, Los Angeles in late 2008. It opened on Broadway at the Marquis Theatre in New York City, on April 30, 2009, to mixed reviews.
The title track of her 2008 "Backwoods Barbie" album was written for the musical's character Doralee. Although her score (as well as the musical debut of actress Allison Janney) was praised, the show struggled and closed on September 6, 2009 after 24 previews and 148 performances. Parton received nominations for Drama Desk Award for Outstanding Music and Drama Desk Award for Outstanding Lyrics, as well as a nomination for Tony Award for Best Original Score.
Developing the musical was not a quick process. According to a broadcast of the public-radio program "Studio 360" (October 29, 2005), in October 2005 Parton was in the midst of composing the songs for a Broadway musical theatre adaptation of the film. In late June 2007, "9 to 5: the Musical" was read for industry presentations. The readings starred Megan Hilty, Allison Janney, Stephanie J. Block, Bebe Neuwirth, and Marc Kudisch. Ambassador Theatre Group announced a 2012 UK tour for "Dolly Parton's ", commencing at Manchester Opera House, on October 12, 2012.
Acting career.
During the mid-1970s, Parton wanted to expand her audience base. Although her first attempt, the television variety show "Dolly!" (1976–77), had high ratings, it lasted only one season, with Parton requesting to be released from her contract because of the stress it was causing her vocal cords. (She later tried a second television variety show, also titled "Dolly" (1987–88), it lasted only one season).
Film.
In her first feature film, Parton portrayed a secretary in a leading role with Jane Fonda and Lily Tomlin in the comedy film "9 to 5" (1980). She received nominations for a Golden Globe Award for Best Actress – Motion Picture Musical or Comedy and a Golden Globe Award for New Star of the Year – Actress.
Parton wrote and recorded the film's title song. It received nominations for an Academy Award for Best Song and a Golden Globe Award for Best Original Song. Released as a single, the song won both the Grammy Award for Best Female Country Vocal Performance and the Grammy Award for Best Country Song. It also reached No. 1 on the Hot 100 chart and in was No. 78 on the "AFI's 100 Years...100 Songs" list released by the American Film Institute in 2004. "9 to 5" became a major box office success, grossing over $3.9 million its opening weekend, and over $103 million worldwide. Parton was named Top Female Box Office Star by the "Motion Picture Herald" in both 1981 and 1982 due to the film's success.
In late 1981, Parton began filming her second film, the musical film "The Best Little Whorehouse in Texas" (1982). The film earned her a second nomination for a Golden Globe Award for Best Actress – Motion Picture Musical or Comedy. The film was greeted with positive critical reviews and became a commercial success, earning over $69 million worldwide. After a two-year hiatus from films, Parton was teamed with Sylvester Stallone for "Rhinestone" (1984). A comedy film about a county music star's efforts to mould an unknown into a music sensation, the film was a critical and financial failure, making just over $21 million on a $28 million budget.
In 1989, she returned to film acting in "Steel Magnolias" (1989), based on the play of the same name by Robert Harling. The film was popular with critics and audiences, grossing over $95 million inside the U.S. Parton starred along with James Woods in "Straight Talk" (1992), which received mixed reviews, and grossed a mild $21 million at the box office. She launched a television series, "The Dolly Show", but it was not a success. Parton made a cameo appearance as herself in "The Beverly Hillbillies" (1993), an adaptation of the long-running TV sitcom of the same name (1962–71). She appeared as an overprotective mother in the comedy "Frank McKlusky, C.I." (2002). She made a cameo appearance in the comedy film "", starring Sandra Bullock. She was featured in "The Book Lady" (2008), a documentary film about her campaign for children's literacy.
Parton had expected to reprise her television role as Hannah's godmother in the musical comedy film "" (2009), but the character was omitted from the final screenplay. She had a voice role in the comedy family film "Gnomeo & Juliet" (2011), a computer-animated film with gnomes about William Shakespeare's "Romeo and Juliet".
She co-starred with Queen Latifah in the musical film "Joyful Noise" (2012), which finished filming in April 2011. She played a choir director's widow who joins forces with Latifah's character, a mother of two teens, to save a small Georgia town's gospel choir. The film was released in theaters on January 13, 2012.
Television.
In addition to her performing appearances on "The Porter Wagoner Show" in the 1960s and into the 1970s, her two self-titled television variety shows in the 1970s and 1980s, and on "American Idol" in 2008 and other guest appearances, Parton has had television roles. In 1979, she received an Emmy award nomination as "Outstanding Supporting Actress in a Variety Program" for her guest appearance in a Cher special.
During the 1980s, she starred in two popular television concert specials: 1983's "Dolly in London", filmed live in London's Dominion Theatre, and "Dolly & Kenny: Real Love", a 1985 concert special with Kenny Rogers, filmed during their joint concert tour. (Parton and Rogers also filmed a popular 1984 holiday special for CBS, and the two teamed up with Willie Nelson in 1989 for another concert special "Something Inside So Strong".) She starred in the television movies "A Smoky Mountain Christmas" (1986); "Wild Texas Wind" (1991); "Unlikely Angel" (1996), portraying an angel sent back to earth following a deadly car crash; and "Blue Valley Songbird" (1999), where her character lives through her music.
Parton has done voice work for animation for television series, playing herself in the "Alvin and the Chipmunks" (episode "Urban Chipmunk", 1983) and the character Katrina Eloise "Murph" Murphy (Ms. Frizzle's first cousin) in "The Magic School Bus" (episode "The Family Holiday Special", 1994). She also has guest-starred in several of sitcoms, including a 1990 episode of "Designing Women" (episode "The First Day of the Last Decade of the Entire Twentieth Century") as herself, the guardian movie star of Charlene's baby. She made a guest appearance on "Reba" (episode "Reba's Rules of Real Estate") portraying a real-estate agency owner and on "The Simpsons" (episode "Sunday, Cruddy Sunday", 1999). She appeared as herself in 2000 on the Halloween episode of Bette Midler's short-lived sitcom "Bette", and on episode 14 of "Babes" (which was produced by Sandollar Productions, Parton and Sandy Gallin's joint production company).
She made cameo appearances on the Disney Channel as "Aunt Dolly" visiting Hannah and her family in the fellow Tennessean Miley Cyrus' series "Hannah Montana" (episodes "Good Golly, Miss Dolly", 2006, "I Will Always Loathe You", 2007, and "Kiss It All Goodbye", 2010). She was nominated for an Outstanding Guest Actress in Comedy Series.
"Dolly Parton's Coat of Many Colors", a made-for-TV film based on Parton's song of the same name, and featuring narration by Parton, aired on NBC in December 2015.
Business ventures.
In 1998, Nashville Business ranked her to be the wealthiest country-music star.
The Dollywood Company.
Parton invested much of her earnings into business ventures in her native East Tennessee, notably Pigeon Forge. She is a co-owner of The Dollywood Company, which operates the theme park Dollywood (a former Silver Dollar City), a dinner theatre, Dolly Parton's Dixie Stampede, and the waterpark Dollywood's Splash Country, all in Pigeon Forge. Dollywood is ranked as the 24th-most-popular theme park in the United States, with some three million visitors per year.
The Dixie Stampede business has venues in Branson, Missouri, and Myrtle Beach, South Carolina. A former location in Orlando, Florida closed in January 2008 after the land and building were sold to a developer. Starting in June 2011, the Myrtle Beach location became Pirates Voyage Fun, Feast & Adventure; Parton appeared for the opening, and the South Carolina General Assembly declared June 3, 2011, Dolly Parton Day.
On January 19, 2012, Parton's 66th birthday, Gaylord Opryland and Dollywood announced plans to open a $50 million water and snow park, a fun and family-friendly travel destination that is open each month of the year in Nashville. However, on September 29, 2012, Parton officially withdrew her support of the Nashville park due to the restructuring of Gaylord Entertainment Company due to their merger with Marriott International.
On June 12, 2015, it was announced that The Dollywood Company had purchased the Lumberjack Feud Dinner Show in Pigeon Forge, Tennessee. The show, which opened originally in June 2011, was previously owned and operated by Rob Scheer until the close of the 2015 season. The new renovated show by The Dollywood Company opens in 2016.
Film and television production company.
Parton was a co-owner of Sandollar Productions, with Sandy Gallin, her former manager. A film-and-television-production company, it produced the "" (1989), which won an Academy Award for Best Documentary (Feature); the television series "Babes" (1990–91) and "Buffy the Vampire Slayer" (1997–2003); and the feature films "Father of the Bride" (1991), "Father of the Bride: Part II" (1995) "Straight Talk" (1992) (in which Parton starred), and "Sabrina" (1995), among other shows. In a 2009 interview singer Connie Francis revealed that Dolly had been contacting her for years in an attempt to film the singer's life story. Francis turned down Parton's offers as she was already in negotiations with singer Gloria Estefan to produce the film, a collaboration now ended. After the retirement of her partner, Sandy Gallin, Parton briefly operated Dolly Parton's Southern Light Productions and in 2015, announced her new production company would be called Dixie Pixie Productions and will produce the movies-of-week in development with NBC Television.
Philanthropic efforts.
Since the mid-1980s, Parton has supported many charitable efforts, particularly in the area of literacy, primarily through her Dollywood Foundation.
Dolly Parton's Imagination Library.
Her literacy program, Dolly Parton's Imagination Library, a part of the Dollywood Foundation, mails one book per month to each enrolled child from the time of their birth until they enter kindergarten. Currently over 1600 local communities provide the Imagination Library to almost 850,000 children each and every month across the U.S., Canada, the UK, and Australia. The program distributes more than 10 million free books to children annually.
In 2006, Parton published a cookbook, "Dolly's Dixie Fixin's: Love, Laughter and Lots of Good Food".
The Dollywood Foundation, funded from Parton's net profits, has been noted for bringing jobs and tax revenues to a previously depressed region. Parton has also worked to raise money on behalf of several other causes, including the American Red Cross and HIV/AIDS-related charities.
In December 2006, Parton pledged $500,000 toward a proposed $90-million hospital and cancer center to be constructed in Sevierville in the name of Dr. Robert F. Thomas, the physician who delivered her. She announced a benefit concert to raise additional funds for the project. The concert played to about 8,000 people. That same year, she and Emmylou Harris allowed their music to be used in a PETA ad campaign that encouraged pet owners to keep their dogs indoors rather than chained outside. In May 2009, Parton gave the commencement address at the University of Tennessee. Her speech was about her life lessons, and she encouraged the graduates to never stop dreaming.
Awards and honors.
Parton is one of the most-honored female country performers of all time. The Record Industry Association of America has certified 25 of her single or album releases as either Gold Record, Platinum Record or Multi-Platinum Record. She has had 26 songs reach No. 1 on the Billboard country charts, a record for a female artist. She has 42 career top-10 country albums, a record for any artist, and 110 career-charted singles over the past 40 years. All inclusive sales of singles, albums, collaboration records, compilation usage, and paid digital downloads during Parton's career have reportedly topped 100 million records around the world.
Parton has earned eight Grammy Awards (including her 2011 Lifetime Achievement Grammy) and a total of 46 Grammy Award nominations, the most nominations of any female artist in the history of the prestigious awards, a record tied by Beyoncé.
At the American Music Awards, she has won three awards out of 18 nominations. At the Country Music Association, she has won 10 awards out of 42 nominations. At the Academy of Country Music, she has won seven awards and 39 nominations. She is one of only six female artists (including Reba McEntire, Barbara Mandrell, Shania Twain, Loretta Lynn, and Taylor Swift), to win the Country Music Association's highest honor, Entertainer of the Year (1978). She has also been nominated for two Academy Awards and a Tony Award. She was awarded a star on the Hollywood Walk of Fame for her music in 1984, located at 6712 Hollywood Boulevard in Hollywood, California; a star on the Nashville Star Walk for Grammy winners; and a bronze sculpture on the courthouse lawn in Sevierville. She has called that statue of herself in her hometown "the greatest honor," because it came from the people who knew her. Parton was inducted into the Grand Ole Opry in 1969, and in 1986 was named one of "Ms. Magazine"'s Women of the Year. In 1986, she was inducted into the Nashville Songwriters Hall of Fame.
In 1999, Parton received country music's highest honor, an induction into the Country Music Hall of Fame. She received an honorary doctorate degree from Carson-Newman College (Jefferson City, Tennessee) in 1990. This was followed by induction into the National Academy of Popular Music/Songwriters Hall of Fame in 2001. In 2002, she ranked No. 4 in CMT's 40 Greatest Women of Country Music.
Parton was honored in 2003 with a tribute album called "Just Because I'm a Woman: Songs of Dolly Parton". The artists who recorded versions of Parton's songs included Melissa Etheridge ("I Will Always Love You"), Alison Krauss ("9 to 5"), Shania Twain ("Coat of Many Colors"), Me'Shell NdegéOcello ("Two Doors Down"), Norah Jones ("The Grass is Blue"), and Sinéad O'Connor ("Dagger Through the Heart"); Parton herself contributed a rerecording of the title song, originally the title song for her first RCA album in 1968. Parton was awarded the Living Legend Medal by the U.S. Library of Congress on April 14, 2004, for her contributions to the cultural heritage of the United States.
In 2005, she was honored with the National Medal of Arts, the highest honor given by the U.S. government for excellence in the arts and is presented by the U.S. President. On December 3, 2006, Parton received the Kennedy Center Honors from the John F. Kennedy Center for the Performing Arts for her lifetime of contributions to the arts. During the show, some of country music's biggest names came to show their admiration. Carrie Underwood performed "Islands in the Stream" with Rogers, Parton's original duet partner. Krauss performed "Jolene" and duetted "Coat of Many Colors" with Twain. McEntire and Reese Witherspoon also came to pay tribute. On November 16, 2010, Parton accepted the Liseberg Applause Award, the theme park industry's most prestigious honor, on behalf of Dollywood theme park during a ceremony held at IAAPA Attractions Expo 2010 in Orlando, Florida.
Hall of Fame Honors.
During her career, Parton has gained induction into numerous Halls of Fame. Those honors include:
Philanthropy-related honors.
In 2003, her efforts to preserve the bald eagle through the American Eagle Foundation's sanctuary at Dollywood earned her the Partnership Award from the U.S. Fish and Wildlife Service. Parton received the Woodrow Wilson Award for Public Service from the Woodrow Wilson International Center for Scholars of the Smithsonian Institution at a ceremony in Nashville on November 8, 2007.
For her work in literacy, Parton has received various awards including:
On May 8, 2009, Parton gave the commencement speech at the commencement ceremony for the University of Tennessee, Knoxville's College of Arts and Sciences. During the ceremony she received an honorary DHL from the university. It was only the second honorary degree given by the university, and in presenting the degree, the university's Chancellor, Jimmy G. Cheek, said, "Because of her career not just as a musician and entertainer, but for her role as a cultural ambassador, philanthropist and lifelong advocate for education, it is fitting that she be honored with an honorary degree from the flagship educational institution of her home state."
Image.
Parton has turned down several offers to pose for "Playboy" magazine, although she did appear on the cover of "Playboy"'s October 1978 issue wearing a Playboy bunny outfit, complete with ears (the October 1978 "Playboy" issue featured Lawrence Grobel's extensive and candid interview with Parton, representing one of her earliest high profile interviews with the mainstream press). The association of breasts with Parton's public image is illustrated in the naming of Dolly the sheep after her, since the sheep was cloned from a cell taken from an adult ewe's mammary gland. In Mobile, Alabama, the General W.K. Wilson Jr. Bridge is commonly referred to by a nickname, "the Dolly Parton Bridge," due to its arches resembling Parton's chest.
She is also known for having undergone considerable plastic surgery. On a 2003 broadcast of "The Oprah Winfrey Show", Winfrey asked what kind of cosmetic surgery Parton had undergone. Parton replied that cosmetic surgery was imperative in keeping with her famous image.
Parton has repeatedly joked about her physical image and surgeries, saying, "It takes a lot of money to look this cheap." Her breasts have garnered mention of her in several songs including: "Dolly Parton's Hits" by Bobby Braddock, "Marty Feldman Eyes" by Bruce Baum (a parody of "Bette Davis Eyes") and "Make Me Proud" by Drake ft. Nicki Minaj. When asked about future plastic surgeries, she famously said: "If I see something sagging, bagging or dragging, I'll get it nipped, tucked or sucked."

</doc>
<doc id="8717" url="https://en.wikipedia.org/wiki?curid=8717" title="Diprotodon">
Diprotodon

Diprotodon, meaning "two forward teeth", is the largest known marsupial ever to have lived. Along with many other members of a group of unusual species collectively called the "Australian megafauna", it existed from approximately 1.6 million years ago until extinction some 46,000 years ago (through most of the Pleistocene epoch).
"Diprotodon" species fossils have been found in sites across mainland Australia, including complete skulls and skeletons, as well as hair and foot impressions. Female skeletons have been found with babies located where the mother's pouch would have been. The largest specimens were hippopotamus-sized: about from nose to tail, standing tall at the shoulder and weighing about . Aboriginal rock art images in Quinkan traditional country (Queensland, Australia) have been claimed to depict diprotodonts.
They inhabited open forest, woodlands, and grasslands, possibly staying close to water, and eating leaves, shrubs, and some grasses.
The closest surviving relatives of "Diprotodon" are the wombats and the koala. It is suggested that diprotodonts may have been an inspiration for the legends of the bunyip, as some Aboriginal tribes identify "Diprotodon" bones as those of "bunyips".
Discovery.
The first recorded "Diprotodon" remains were discovered in a cave near Wellington in New South Wales in the early 1830s by Major Thomas Mitchell who sent them to England for study by Sir Richard Owen. In the 1840s Ludwig Leichhardt discovered many "Diprotodon" bones eroding from the banks of creeks in the Darling Downs of Queensland and when reporting the find to Owen commented that the remains were so well preserved he expected to find living examples in the then-unexplored central regions of Australia.
The majority of fossil finds are of demographic groups indicative of diprotodonts dying in drought conditions. For example, hundreds of individuals were found in Lake Callabonna with well-preserved lower bodies but crushed and distorted heads. It is theorised several family groups sank in mud while crossing the drying lake bed. Other finds consist of age groupings of young or old animals which are first to die during a drought.
In 2012, a significant group of about 40 was found at Eulo, South-West Queensland.
Taxonomy.
"Diprotodon" was named by Owen (1838). It was assigned to Diprotodontidae by McKenna and Bell (1997). The historical classification of "Diprotodon" consisted of eight species ("Diprotodon optatum" Owen, 1838; "Diprotodon australis" Owen, 1844; "D. annextans" McCoy, 1861; "D. minor" Huxley, 1862; "D. longiceps" McCoy 1865; "D. loderi" Krefft, 1873a; "D. bennettii" Krefft, 1873b (nec "D. bennettii" Owen, 1877); and "D. bennettii" Owen, 1877 (nec "D. bennettii" Krefft, 1873b); based on size or slight morphological differences of single specimens collected from isolated geographic regions. Bimodal dental sizes, rather than a continuum of tooth sizes, and identical male and female dental morphology, indicate sexual dimorphism instead of separate species, thus providing strong evidence that the eight species are synonyms for "D. optatum".
Description.
"Diprotodon" superficially resembled a rhinoceros without a horn. Its feet turned inwards like a wombat’s, giving it a pigeon-toed appearance. It had strong claws on the front feet and its pouch opening faced backwards. Footprints of its feet have been found showing a covering of hair which indicates it had a coat similar to a modern wombat.
Until recently it was unknown how many species of "Diprotodon" had existed. Eight species are described although many researchers believed these actually represented only three at most while some estimated there could be around twenty in total.
Paleobiology.
Recent research compared the variation between all of the described "Diprotodon" species with the variation in one of Australia’s largest living marsupials, the Eastern Grey Kangaroo, and found the range was comparable, with a near continent-wide distribution. This left only two possible "Diprotodon" species differing only in size with the smaller being around half the size of the larger. According to Gause’s "competitive exclusion principle" no two species with identical ecological requirements can coexist in a stable environment. However, both the small and large diprotodonts coexisted throughout the Pleistocene and the size difference is similar to other sexually dimorphic living marsupials. Further evidence is the battle damage common in competing males found on the larger specimens but absent from the smaller. Dental morphology also supports sexual dimorphism, with highly sexually dimorphic marsupials, such as the grey kangaroo, having different tooth sizes between males and females, but both sexes having the same dental morphology. An identical dental morphology occurs in the large and small "Diprotodon". The taxonomic implication is that Owen’s original "Diprotodon optatum" is the only valid species.
A single sexually dimorphic species allows behavioural interpretations. All sexually dimorphic species of over exhibit a polygynous breeding strategy. A modern example of this is the gender segregation of elephants where females and the young form family groups while lone males fight for the right to mate with all the females of the group. This behaviour is consistent with fossil finds where adult/juvenile fossil assemblages usually contain only female adult remains.
Extinction.
Most modern researchers including Richard Roberts and Tim Flannery argue that diprotodonts, along with a wide range of other Australian megafauna, became extinct shortly after humans arrived in Australia about 50,000 years ago.
Some older researchers including Richard Wright argue on the contrary that diprotodont remains from several sites, such as Tambar Springs and Trinkey and Lime Springs suggest that "Diprotodon" survived much longer, into the Holocene. Other more recent researchers, including Lesley Head and Judith Field, favour an extinction date of 28,000 - 30,000 years ago, which would mean that humans coexisted with "Diprotodon" for some 20,000 years. However, opponents of "late extinction" theories have interpreted such late dates based on indirect dating methods as artifacts resulting from redeposition of skeletal material into more recent strata, and recent direct dating results obtained with new technologies have tended to confirm this interpretation.
Three theories have been advanced to explain the mass extinction.
Climate change.
Australia has undergone a very long process of gradual aridification since it split off from Gondwanaland about 40 million years ago. From time to time the process reversed for a period, but overall the trend has been strongly toward lower rainfall. The recent ice ages produced no significant glaciation in mainland Australia but long periods of cold and very dry weather. This dry weather during the last ice age may have killed off all the large diprotodonts.
Critics point out a number of problems with this theory. First, large diprotodonts had already survived a long series of similar ice ages, and there does not seem to be any particular reason the most recent one should have achieved what all the previous ice ages had failed to do. Also, climate change apparently peaked 25,000 years "after" the extinctions. Finally, even during climatic extremes, some parts of the continent always remain relatively exempt: for example, the tropical north stays fairly warm and wet in all climatic circumstances; alpine valleys are less affected by drought, and so on.
Human hunting.
The overkill theory is that human hunters killed and ate the diprotodonts, causing their extinction. The extinctions appear to have coincided with the arrival of humans on the continent, and in broad terms, "Diprotodon" was the largest and least well-defended species that died out. Similar hunting-out happened with the megafauna of New Zealand, Madagascar and many smaller islands around the world (such as New Caledonia, Cyprus, Crete and Wrangel Island), and at least in part, in the Americas—probably within a thousand years or so. Recent finds of "Diprotodon" bones which appear to display butchering marks lend support to this theory. Critics of this theory regard it as simplistic, arguing that (unlike New Zealand and America) there is little direct evidence of hunting, and that the dates on which the theory rests are too uncertain to be relied on. However, the high-resolution chronology of the changes supports the hypothesis that human hunting alone eliminated the megafauna.
Human land management.
The third theory says that humans indirectly caused the extinction of diprotodonts, by destroying the ecosystem on which they depended. In particular, early Aborigines are thought to have been fire-stick farmers using fire regularly and persistently to drive game, open up dense thickets of vegetation, and create fresh green regrowth for both humans and game animals to eat. Evidence for the fire hypothesis is the sudden increase in widespread ash deposits at the time that people arrived in Australia, as well as land-management and hunting practices of modern Aboriginal people as recorded by the earliest European settlers before Aboriginal society was devastated by European contact and disease. Evidence against the hypothesis is the fact that humans appear to have eliminated the megafauna of Tasmania without using fire to modify the environment there.
Multiple causes.
The above hypotheses are not necessarily mutually exclusive. Each of proposed mechanisms can potentially support the other two. For example, while burning an area of fairly thick forest and thus turning it into a more open, grassy environment might reduce the viability of a large browser (an animal that eats leaves and shoots rather than grasses), the reverse could also be true: removing the browsing animals (by eating them, or by any other means) within a few years produces a very thick undergrowth which, when a fire eventually starts through natural causes (as fires tend to do every few hundred years), burns with greater than usual ferocity. The burnt-out area is then repopulated with a greater proportion of fire-loving plant species (notably eucalypts, some acacias, and most of the native grasses) which are unsuitable habitat for most browsing animals. Either way, the trend is toward the modern Australian environment of highly flammable open sclerophyllous forests, woodlands and grasslands, none of which are suitable for large, slow-moving browsing animals—and either way, the changed microclimate produces substantially less rainfall.
An examination of swamp sediment cores spanning the last 130,000 years from Lynch's Crater in Queensland suggests that hunting may have been the primary cause of the extinction. Analysis of "Sporormiella" fungal spores (which derive mainly from the dung of megaherbivores) in the cores shows that the megafauna of that region virtually disappeared about 41,000 years ago, at a time when climate changes were minimal; the change was accompanied by an increase in charcoal, and was followed by a transition from rainforest to fire-tolerant sclerophyll vegetation. The high-resolution chronology of the changes indicates that fire increased about a century after the disappearance of browsing megafauna, probably due to accumulation of fuel. Grass increased over the next several centuries; sclerophyll vegetation increased following a lag of another century, and a sclerophyll forest developed about a thousand years later. Earlier increases in sclerophyll vegetation during shifts to cooler, drier conditions about 120,000 and 75,000 years ago did not have any obvious impact on megafaunal abundance.

</doc>
<doc id="8718" url="https://en.wikipedia.org/wiki?curid=8718" title="Dirk Benedict">
Dirk Benedict

Dirk Benedict (born Dirk Niewoehner on March 1, 1945) is an American movie, television and stage actor who played the characters Lieutenant Templeton "Faceman" Peck in "The A-Team" television series and Lieutenant Starbuck in the original "Battlestar Galactica" film and television series. He is the author of "Confessions of a Kamikaze Cowboy" and "And Then We Went Fishing".
Early life.
Benedict was born Dirk Niewoehner in Helena, Montana, the son of Priscilla Mella (née Metzger), an accountant, and George Edward Niewoehner, a lawyer. He grew up in White Sulphur Springs, Montana. He graduated from Whitman College in 1967.
Benedict allegedly chose his stage name from a serving of Eggs Benedict he had enjoyed prior to his acting career.
Career.
Benedict's film debut was in the 1972 film "Georgia, Georgia". When the New York run for "Butterflies Are Free" ended, he received an offer to repeat his performance in Hawaii, opposite Barbara Rush. While there, he appeared as a guest lead on "Hawaii Five-O". The producers of a horror film called "Sssssss" (1973) saw Benedict's performance in "Hawaii Five-O" and promptly cast him as the lead in that movie. He next played the psychotic wife-beating husband of Twiggy in her American film debut, "W" (1974). Benedict starred in the television series "Chopper One" which aired for one season in 1974. He also made an appearance in "Charlie's Angels".
Benedict's career break came in 1978 when he appeared as Lieutenant Starbuck in the movie and television series "Battlestar Galactica". The same year Benedict starred in the TV movie "Cruise Into Terror", and appeared in the ensemble movie "Scavenger Hunt" the following year.
1980s and 1990s.
In 1981, Benedict starred alongside Linda Blair in an action-comedy Movie called Ruckus. In 1983, Dirk gained further popularity as con-man Lieutenant Templeton "Face" Peck in 1980s action television series, "The A-Team". He played "Faceman" from to , although the series didn't air until January 1983, and the final episode wasn't shown until 1987 rebroadcasts. The episode, "Steel," includes a scene at Universal Studios where Face is seen looking bemused as a Cylon walks by him as an in-joke to his previous role in "Battlestar Galactica". The clip is incorporated into the series' opening credit sequence from the third season onward.
In 1986, Benedict starred as a low-life band manager "Harry Smilac" in the movie "Body Slam" along with Lou Albano, Roddy Piper, and cameo appearances by Freddie Blassie, Ric Flair, and Bruno Sammartino. His character Smilac ends up managing the pro-wrestler "Quick Rick Roberts" (Piper) and faces opposition by Captain Lou and his wrestling tag-team "the Cannibals".
In 1987, Benedict took the title role of Shakespeare's Hamlet at the Abbey Theatre. Both his performance and the entire production were drubbed by critics. Benedict starred in the 1989 TV movie "Trenchcoat in Paradise".
In 1991, Benedict starred in "Blue Tornado," playing Alex, call sign Fireball, an Italian air force fighter pilot. Benedict published an autobiography, "Confessions of a Kamikaze Cowboy: A True Story of Discovery, Acting, Health, Illness, Recovery, and Life" (Avery Publishing ISBN 0895294796). In 1993, Benedict starred in "Shadow Force".
Benedict also appeared as Jake Barnes in the 1996 action-adventure film "Alaska".
2000s and 2010s.
In 2000, Benedict wrote and directed his first screenplay, "Cahoots".
In 2005, Benedict played an impostor, pretending to be an 80s movie star who wants to moderate a charity performance in "Goldene Zeiten" ("Golden Times") by Peter Thorwarth.
In 2006, he wrote an online essay criticizing the then-airing "Battlestar Galactica" re-imagined series, and especially its casting of a woman as his character, Starbuck, writing that "the war against masculinity has been won" and that "a television show based on hope, spiritual faith, and family is unimagined and regurgitated as a show of despair, sexual violence and family dysfunction".
He appeared as a contestant on the 2007 U.K. series of "Celebrity Big Brother". He arrived on launch night in a replica of the "A-Team" van, smoking a cigar and accompanied by Post & Carpenter's "A-Team" theme tune.
In 2010, Benedict starred in "Prescription: Murder" playing Lieutenant Columbo along with Patrick Ryecart and George Telfer. Benedict also made a cameo appearance in the 2010 film adaptation of "The A-Team" as Pensacola Prisoner Milt.
Personal life.
In the 1970s Benedict survived a prostate tumor believed to have been cancerous. Having rejected conventional medical treatment, he credited his survival to the adoption of a macrobiotic diet recommended to him by actress Gloria Swanson.
In 1986, he married Toni Hudson, an actress with whom he has two sons, George and Roland. Hudson had previously appeared as Dana in the fourth season "A-Team" episode titled "Blood, Sweat and Cheers". They divorced in 1995. In 1998, Benedict learned that he also has another son, John Talbert (born 1968), from an earlier relationship, who had been given up for adoption. With the help of his adoptive parents, Talbert discovered and contacted his birth parents.

</doc>
<doc id="8724" url="https://en.wikipedia.org/wiki?curid=8724" title="Doppler effect">
Doppler effect

The Doppler effect (or Doppler shift) is the change in frequency of a wave (or other periodic event) for an observer moving relative to its source. It is named after the Austrian physicist Christian Doppler, who proposed it in 1842 in Prague. It is commonly heard when a vehicle sounding a siren or horn approaches, passes, and recedes from an observer. Compared to the emitted frequency, the received frequency is higher during the approach, identical at the instant of passing by, and lower during the recession.
When the source of the waves is moving toward the observer, each successive wave crest is emitted from a position closer to the observer than the previous wave. Therefore, each wave takes slightly less time to reach the observer than the previous wave. Hence, the time between the arrival of successive wave crests at the observer is reduced, causing an increase in the frequency. While they are travelling, the distance between successive wave fronts is reduced, so the waves "bunch together". Conversely, if the source of waves is moving away from the observer, each wave is emitted from a position farther from the observer than the previous wave, so the arrival time between successive waves is increased, reducing the frequency. The distance between successive wave fronts is then increased, so the waves "spread out".
For waves that propagate in a medium, such as sound waves, the velocity of the observer and of the source are relative to the medium in which the waves are transmitted. The total Doppler effect may therefore result from motion of the source, motion of the observer, or motion of the medium. Each of these effects is analyzed separately. For waves which do not require a medium, such as light or gravity in general relativity, only the relative difference in velocity between the observer and the source needs to be considered.
Developments.
Doppler first proposed this effect in 1842 in his treatise "Über das farbige Licht der Doppelsterne und einiger anderer Gestirne des Himmels" (On the coloured light of the binary stars and some other stars of the heavens). The hypothesis was tested for sound waves by Buys Ballot in 1845. He confirmed that the sound's pitch was higher than the emitted frequency when the sound source approached him, and lower than the emitted frequency when the sound source receded from him. Hippolyte Fizeau discovered independently the same phenomenon on electromagnetic waves in 1848 (in France, the effect is sometimes called "effet Doppler-Fizeau" but that name was not adopted by the rest of the world as Fizeau's discovery was six years after Doppler's proposal). In Britain, John Scott Russell made an experimental study of the Doppler effect (1848).
General.
In classical physics, where the speeds of source and the receiver relative to the medium are lower than the velocity of waves in the medium, the relationship between observed frequency formula_1 and emitted frequency formula_2 is given by:
The frequency is decreased if either is moving away from the other.
The above formula assumes that the source is either directly approaching or receding from the observer. If the source approaches the observer at an angle (but still with a constant velocity), the observed frequency that is first heard is higher than the object's emitted frequency. Thereafter, there is a monotonic decrease in the observed frequency as it gets closer to the observer, through equality when it is coming from a direction perpendicular to the relative motion (and was emitted at the point of closest approach; but when the wave is received, the source and observer will no longer be at their closest), and a continued monotonic decrease as it recedes from the observer. When the observer is very close to the path of the object, the transition from high to low frequency is very abrupt. When the observer is far from the path of the object, the transition from high to low frequency is gradual.
If the speeds formula_6 and formula_5 are small compared to the speed of the wave, the relationship between observed frequency formula_1 and emitted frequency formula_2 is approximately
Given formula_3
we divide for formula_14
formula_15
Since formula_16 we can substitute the geometric expansion:
formula_17
Analysis.
To understand what happens, consider the following analogy. Someone throws one ball every second at a man. Assume that balls travel with constant velocity. If the thrower is stationary, the man will receive one ball every second. However, if the thrower is moving towards the man, he will receive balls more frequently because the balls will be less spaced out. The inverse is true if the thrower is moving away from the man. So it is actually the "wavelength" which is affected; as a consequence, the received frequency is also affected. It may also be said that the velocity of the wave remains constant whereas wavelength changes; hence frequency also changes.
With an observer stationary relative to the medium, if a moving source is emitting waves with an actual frequency formula_2 (in this case, the wavelength is changed, the transmission velocity of the wave keeps constant formula_19 note that the "transmission velocity" of the wave does not depend on the "velocity of the source"), then the observer detects waves with a frequency formula_1 given by
A similar analysis for a moving "observer" and a stationary source (in this case, the wavelength keeps constant, but due to the motion, the rate at which the observer receives waves formula_19 and hence the "transmission velocity" of the wave ith respect to the observe formula_19 is changed) yields the observed frequency:
These can be generalized into the equation that was presented in the previous section.
An interesting effect was predicted by Lord Rayleigh in his classic book on sound: if the source is moving at twice the speed of sound, a musical piece emitted by that source would be heard in correct time and tune, but "backwards". The Doppler effect with sound is only clearly heard with objects moving at high speed, as change in frequency of musical tone involves a speed of around 40 meters per second, and smaller changes in frequency can easily be confused by changes in the amplitude of the sounds from moving emitters. Neil A Downie has demonstrated how the Doppler effect can be made much more easily audible by using an ultrasonic (e.g. 40 kHz) emitter on the moving object. The observer then uses a heterodyne frequency converter, as used in many bat detectors, to listen to a band around 40 kHz. In this case, with the bat detector tuned to give frequency for the stationary emitter of 2000 Hz, the observer will perceive a frequency shift of a whole tone, 240 Hz, if the emitter travels at 2 meters per second.
Application.
Sirens.
The siren on a passing emergency vehicle will start out higher than its stationary pitch, slide down as it passes, and continue lower than its stationary pitch as it recedes from the observer. Astronomer John Dobson explained the effect thus:
In other words, if the siren approached the observer directly, the pitch would remain constant until the vehicle hit him, and then immediately jump to a new lower pitch. Because the vehicle passes by the observer, the radial velocity does not remain constant, but instead varies as a function of the angle between his line of sight and the siren's velocity:
where formula_27 is the angle between the object's forward velocity and the line of sight from the object to the observer.
Astronomy.
The Doppler effect for electromagnetic waves such as light is of great use in astronomy and results in either a so-called redshift or blueshift. It has been used to measure the speed at which stars and galaxies are approaching or receding from us; that is, their radial velocities. This may be used to detect if an apparently single star is, in reality, a close binary, to measure the rotational speed of stars and galaxies, or to detect exoplanets. (Note that redshift is also used to measure the expansion of space, but that this is not truly a Doppler effect.)
The use of the Doppler effect for light in astronomy depends on our knowledge that the spectra of stars are not homogeneous. They exhibit absorption lines at well defined frequencies that are correlated with the energies required to excite electrons in various elements from one level to another. The Doppler effect is recognizable in the fact that the absorption lines are not always at the frequencies that are obtained from the spectrum of a stationary light source. Since blue light has a higher frequency than red light, the spectral lines of an approaching astronomical light source exhibit a blueshift and those of a receding astronomical light source exhibit a redshift.
Among the nearby stars, the largest radial velocities with respect to the Sun are +308 km/s (BD-15°4041, also known as LHS 52, 81.7 light-years away) and -260 km/s (Woolley 9722, also known as Wolf 1106 and LHS 64, 78.2 light-years away). Positive radial velocity means the star is receding from the Sun, negative that it is approaching.
Radar.
The Doppler effect is used in some types of radar, to measure the velocity of detected objects. A radar beam is fired at a moving target — e.g. a motor car, as police use radar to detect speeding motorists — as it approaches or recedes from the radar source. Each successive radar wave has to travel farther to reach the car, before being reflected and re-detected near the source. As each wave has to move farther, the gap between each wave increases, increasing the wavelength. In some situations, the radar beam is fired at the moving car as it approaches, in which case each successive wave travels a lesser distance, decreasing the wavelength. In either situation, calculations from the Doppler effect accurately determine the car's velocity. Moreover, the proximity fuze, developed during World War II, relies upon Doppler radar to detonate explosives at the correct time, height, distance, etc.
Because the doppler shift affects the wave incident upon the target as well as the wave reflected back to the radar, the change in frequency observed by a radar due to a target moving at relative velocity formula_28 is twice that from the same target emitting a wave:
Medical imaging and blood flow measurement.
An echocardiogram can, within certain limits, produce an accurate assessment of the direction of blood flow and the velocity of blood and cardiac tissue at any arbitrary point using the Doppler effect. One of the limitations is that the ultrasound beam should be as parallel to the blood flow as possible. Velocity measurements allow assessment of cardiac valve areas and function, any abnormal communications between the left and right side of the heart, any leaking of blood through the valves (valvular regurgitation), and calculation of the cardiac output. Contrast-enhanced ultrasound using gas-filled microbubble contrast media can be used to improve velocity or other flow-related medical measurements.
Although "Doppler" has become synonymous with "velocity measurement" in medical imaging, in many cases it is not the frequency shift (Doppler shift) of the received signal that is measured, but the phase shift ("when" the received signal arrives).
Velocity measurements of blood flow are also used in other fields of medical ultrasonography, such as obstetric ultrasonography and neurology. Velocity measurement of blood flow in arteries and veins based on Doppler effect is an effective tool for diagnosis of vascular problems like stenosis.
Flow measurement.
Instruments such as the laser Doppler velocimeter (LDV), and acoustic Doppler velocimeter (ADV) have been developed to measure velocities in a fluid flow. The LDV emits a light beam and the ADV emits an ultrasonic acoustic burst, and measure the Doppler shift in wavelengths of reflections from particles moving with the flow. The actual flow is computed as a function of the water velocity and phase. This technique allows non-intrusive flow measurements, at high precision and high frequency.
Velocity profile measurement.
Developed originally for velocity measurements in medical applications (blood flow), Ultrasonic Doppler Velocimetry (UDV) can measure in real time complete velocity profile in almost any liquids containing particles in suspension such as dust, gas bubbles, emulsions. Flows can be pulsating, oscillating, laminar or turbulent, stationary or transient. This technique is fully non-invasive.
Satellite communication.
Fast moving satellites can have a Doppler shift of dozens of kilohertz relative to a ground station. The speed, thus magnitude of Doppler effect, changes due to earth curvature. Dynamic Doppler compensation, where the frequency of a signal is changed multiple times during transmission, is used so the satellite receives a constant frequency signal.
Audio.
The Leslie speaker, associated with and predominantly used with the Hammond B-3 organ, takes advantage of the Doppler effect by using an electric motor to rotate an acoustic horn around a loudspeaker, sending its sound in a circle. This results at the listener's ear in rapidly fluctuating frequencies of a keyboard note.
Vibration measurement.
A laser Doppler vibrometer (LDV) is a non-contact method for measuring vibration. The laser beam from the LDV is directed at the surface of interest, and the vibration amplitude and frequency are extracted from the Doppler shift of the laser beam frequency due to the motion of the surface.
Developmental biology.
During the segmentation of vertebrate embroys, waves of gene expression sweep across the presomitic mesoderm, the tissue from which the precursors of the vertebrae (somites) are formed. A new somite is formed upon arrival of a wave at the anterior end of the presomitic mesoderm. In zebrafish, it has been shown that the shortening of the presomitic mesoderm during segmentation leads to a Doppler effect as the anterior end of the tissue moves into the waves. This Doppler effect contributes to the period of segmentation.
Inverse Doppler effect.
Since 1968 scientists such as Victor Veselago have speculated about the possibility of an inverse Doppler effect. The experiment that claimed to have detected this effect was conducted by Nigel Seddon and Trevor Bearpark in Bristol, United Kingdom in 2003.
Researchers from Swinburne University of Technology and the University of Shanghai for Science and Technology showed that this effect can be observed in optical frequencies as well. This was made possible by growing a photonic crystal and projecting a laser beam into the crystal. This made the crystal act like a super prism and the inverse Doppler effect could be observed.

</doc>
<doc id="8727" url="https://en.wikipedia.org/wiki?curid=8727" title="ΔT">
ΔT

In precise timekeeping, ΔT (Delta T, delta-T, deltaT, or DT) is the time difference obtained by subtracting Universal Time (UT) from Terrestrial Time (TT): ΔT=TT−UT.
Universal Time is a time scale based on the Earth's rotation, which is somewhat irregular over short periods (days up to a century), thus any time based on it cannot have an accuracy better than 1 : 10. But the principal effect is over the long term: over many centuries tidal friction inexorably slows Earth's rate of rotation by about +2.3 ms/day/cy. However, there are other forces changing the rotation rate of the Earth. The most important one is believed to be a result of the melting of continental ice sheets at the end of the last glacial period. This removed their tremendous weight, allowing the land under them to begin to rebound upward in the polar regions, which has been continuing and will continue until isostatic equilibrium is reached. This "post-glacial rebound" brings mass closer to the rotation axis of the Earth, which makes the Earth spin faster (law of conservation of angular momentum): the rate derived from models is about −0.6 ms/day/cy. So the net acceleration (actually a deceleration) of the rotation of the Earth, or the change in the length of the mean solar day (LOD), is +1.7 ms/day/cy. This is indeed the average rate as observed over the past 27 centuries.
Terrestrial Time is a theoretical uniform time scale, defined to provide continuity with the former Ephemeris Time (ET). ET was an independent time-variable, proposed (and its adoption agreed) in the period 1948–52 with the intent of forming a gravitationally uniform time scale as far as was feasible at that time, and depending for its definition on Simon Newcomb's "Tables of the Sun" (1895), interpreted in a new way to accommodate certain observed discrepancies. Newcomb's tables formed the basis of all astronomical ephemerides of the Sun from 1900 through 1983: they were originally expressed (and published) in terms of Greenwich Mean Time and the mean solar day, but later, in respect of the period 1960–1983, they were treated as expressed in terms of ET, in accordance with the adopted ET proposal of 1948–52. ET, in turn, can now be seen (in light of modern results) as close to the average mean solar time between 1750 and 1890 (centered on 1820), because that was the period during which the observations on which Newcomb's tables were based were performed. While TT is strictly uniform (being based on the SI second, every second is the same as every other second), it is in practice realised by International Atomic Time (TAI) with an accuracy of about 1 : 10.
Earth's rate of rotation must be integrated to obtain time, which is Earth's angular position (specifically, the orientation of the meridian of Greenwich relative to the fictitious mean sun). Integrating +1.7 ms/d/cy and centering the resulting parabola on the year 1820 yields (to a first approximation) 31×((Year − 1820)/100) seconds for ΔT. Smoothed historical measurements of ΔT using total solar eclipses are about +16800 s at the year −500 (501 BC), +10600 s at 0 (1 BC), +5700 s at 500 (AD), +1600 s at 1000, and +180 s at 1500. After the invention of the telescope, measurements were made by observing occultations of stars by the Moon, which allowed the derivation of more closely spaced and more accurate values for ΔT. ΔT continued to decrease until it reached a plateau of +11±6 s between 1680 and 1866.
For about three decades immediately before 1902 it was negative, reaching −6.64 s. Then it increased to +63.83 s at 2000. It will continue to increase at an ever faster (quadratic) rate in the future. This will require the addition of an ever greater number of leap seconds to UTC as long as UTC is kept within one second of UT1. (The SI second as now used for UTC, when adopted, was already a little shorter than the current value of the second of mean solar time.) Physically, the meridian of Greenwich in Universal Time is almost always to the east of the meridian in Terrestrial Time, both in the past and in the future. +16800 s or h corresponds to 70°E. This means that at −500 (501 BC), Earth's faster rotation would cause a total solar eclipse to occur 70° to the east of the location calculated using the uniform TT.
All values of ΔT before 1955 depend on observations of the Moon, either via eclipses or occultations. Conservation of angular momentum in the Earth-Moon system requires that the angular momentum lost by the Earth due to tidal friction be transferred to the Moon, increasing its angular momentum, which means that its moment arm (its distance from the Earth) is increased (for the time being about +3.8 cm/year), which via Kepler's laws of planetary motion causes the Moon to revolve around the Earth at a slower rate. The cited values of ΔT assume that the lunar acceleration (actually a deceleration = a negative acceleration) due to this effect is dn/dt = −26"/cy, where n is the mean sidereal angular motion of the Moon. This is close to the best estimate for dn/dt as of 2002 of −25.858±0.003"/cy so ΔT need not be recalculated given the uncertainties and smoothing applied to its current values. Nowadays, UT is the observed orientation of the Earth relative to an inertial reference frame formed by extra-galactic radio sources, modified by an adopted ratio between sidereal time and solar time. Its measurement by several observatories is coordinated by the International Earth Rotation and Reference Systems Service (IERS).

</doc>
<doc id="8728" url="https://en.wikipedia.org/wiki?curid=8728" title="December 22">
December 22


</doc>
<doc id="8729" url="https://en.wikipedia.org/wiki?curid=8729" title="David Deutsch">
David Deutsch

David Elieser Deutsch, FRS (born 18 May 1953) is an Israeli-born British physicist at the University of Oxford. He is a non-stipendiary Visiting Professor in the Department of Atomic and Laser Physics at the Centre for Quantum Computation (CQC) in the Clarendon Laboratory of the University of Oxford. He pioneered the field of quantum computation by formulating a description for a quantum Turing machine, as well as specifying an algorithm designed to run on a quantum computer. He is a proponent of the many-worlds interpretation of quantum mechanics.
Early life and education.
Deutsch was born in Haifa in Israel on 18 May 1953, the son of Oskar and Tikva Deutsch. He attended William Ellis School in London (then a voluntary aided grammar school) before reading Natural Sciences at Clare College, Cambridge and taking Part III of the Mathematical Tripos. He went on to Wolfson College, Oxford for his doctorate in theoretical physics and wrote his thesis on quantum field theory in curved space-time.
Career.
In the Royal Society of London's announcement that Deutsch had become a Fellow of the Royal Society (FRS) in 2008, the Society described Deutsch's contributions thus:
David Deutsch laid the foundations of the quantum theory of computation, and has subsequently made or participated in many of the most important advances in the field, including the discovery of the first quantum algorithms, the theory of quantum logic gates and quantum computational networks, the first quantum error-correction scheme, and several fundamental quantum universality results. He has set the agenda for worldwide research efforts in this new, interdisciplinary field, made progress in understanding its philosophical implications (via a variant of the many-universes interpretation) and made it comprehensible to the general public, notably in his book The Fabric of Reality.
He is currently working on constructor theory, an attempt at generalizing the quantum theory of computation to cover not just computation but all physical processes.
Together with Chiara Marletto, he published a paper in December 2014 entitled "Constructor theory of information", that conjectures that information can be expressed solely in terms of which transformations of physical systems are possible and which are impossible.
Popular science books.
"The Fabric of Reality".
In his 1997 book "The Fabric of Reality", Deutsch details his "Theory of Everything." It aims not at the reduction of everything to particle physics, but rather mutual support among multiversal, computational, epistemological, and evolutionary principles. His theory of everything is (weakly) emergentist rather than reductive.
There are "four strands" to his theory:
"The Beginning of Infinity".
Deutsch’s second book, "The Beginning of Infinity: Explanations that Transform the World", was published on 31 March 2011. In this book Deutsch views the Enlightenment of the 18th century as near the beginning of an unending sequence of purposeful knowledge creation. He examines the nature of memes and how and why creativity evolved in humans.
Views.
Deutsch is an atheist. He is also a founding member of the parenting and educational method known as Taking Children Seriously.
He was awarded the Dirac Prize of the Institute of Physics in 1998, and the Edge of Computation Science Prize in 2005. The "Fabric of Reality" was shortlisted for the Rhone-Poulenc science book award in 1998.

</doc>
<doc id="8730" url="https://en.wikipedia.org/wiki?curid=8730" title="Volkssturm">
Volkssturm

The Volkssturm (, "people's storm") was a German national militia established during the last months of World War II. It was set up, not by the traditional German Army, but by the Nazi Party on the orders of Adolf Hitler and its official existence was not announced until October 18, 1944. It was staffed by conscripting males between the ages of 13 and 60 years who were not already serving in some military unit as part of a German Home Guard. Embodying a last-ditch effort and a call for a heroic defense, the "Volkssturm" comprised one of the final components of the Total War promulgated by Propaganda Minister Joseph Goebbels, part of a Nazi endeavor to overcome their enemies' military strength through force of will.
Origins and organization.
The new "Volkssturm" drew inspiration from the old Prussian "Landsturm" of 1813–1815 that fought in the liberation wars against Napoleon, mainly as guerrilla forces. Plans to form a "Landsturm" national militia in Eastern Germany as a last resort to boost fighting strength initially came from "Oberkommando des Heeres" Army Chief, General Heinz Guderian in 1944. Because the Wehrmacht lacked manpower to stop the Soviet advance, men in jobs not deemed necessary, those previously deemed unfit for military service, youth previously deemed too young, and injured soldiers recuperating from their wounds, were now called to arms. The "Volkssturm" had existed, on paper, since around 1925, but it was only after Hitler ordered Martin Bormann to recruit six million men for this militia that the group became a physical reality. The intended strength of six million was never attained.
Joseph Goebbels and other propagandists depicted the "Volkssturm" as an outburst of enthusiasm and will to resist. While it had some marginal affect on morale, it was undermined by the recruits visible lack of uniforms and weaponry. Nazi themes of death, transcendence, and commemoration were given full play to encourage the fight. Many realized that this was a desperate attempt to turn the course of the war. A popular joke and an occasional comment about the "Volkssturm" are telling in this regard. Sardonic old men would remark, ""We old monkeys are the Führer’s newest weapon"; whereas the joke went, "Why is the Volkssturm Germany's most precious resource?" to be answered by: "Because its members have silver in their hair, gold in their mouth, and lead in their bones.""
In order for these militia units to be effective, the Nazis counted not only on strength in numbers, but also on fanaticism. During the early stages of "Volkssturm" planning, it became apparent that if militia units lacked morale they would lack combat effectiveness. To achieve the envisaged fanaticism, "Volkssturm" units were placed under direct command of the local Nazi party, meaning local "Gau"- and "Kreisleiters". The new "Volkssturm" was also to become a nationwide organization, with Heinrich Himmler, as Replacement Army Commander, responsible for armament and training. Though normally under party control, "Volkssturm" units were placed under Wehrmacht command when engaged in action. Aware that a 'people's army' would not be able to withstand the onslaught of the modern army wielded by the Allies, Hitler issued the following order towards the end of 1944: 
With the Nazi Party in charge of organizing the "Volkssturm", each "Gauleiter", or Nazi Party District Leader, was charged with the leadership, enrollment, and organization of the "Volkssturm" in their district. The largest "Volkssturm" unit seems to have corresponded to the next smaller territorial subdivision of the Nazi Party organization—the Kreis. The basic unit was a battalion of 642 men. Units were mostly composed of members of the Hitler Youth, invalids, the elderly, or men who had previously been considered unfit for military service. Further desperation showed when on 12 February 1945, the Nazis conscripted German women and girls into the auxiliaries of the "Volkssturm". Correspondingly, girls as young as 14 years began receiving instructions on the use of small-arms, bazookas, machine guns, and hand grenades from December 1944 through May 1945. Like many aspects of the Nazi "Volksgemeinschaft" (people's community), the organization of Germany for war was all inclusive, reaching almost all levels of society as the establishment and expansion of the "Volkssturm" reveals.
Municipal organization:
Each Gauleiter and Kreisleiter, had a "Volkssturm" Chief of Staff to assist in handling militia problems.
From its inception until the very end of the Nazi regime, Himmler and Bormann engaged in a power-struggle over the jurisdictional control over the "Volkssturm" regarding security and police powers in Germany and the occupied territories; a contest which Himmler and his SS more or less won on one level (police and security) but lost to Bormann on another (mobilizing reserve forces). Historian David Yelton described the situation as two ranking officers at the helm of a sinking ship fighting over command.
Uniforms and insignia.
The "Volkssturm" "uniform" was only a black armband with words "Deutscher Volkssturm Wehrmacht" with a series of silver collar pips pinned to the wearer's collar. These were characteristically derived from the rank insignia of the various paramilitary organizations of the Nazi Party, which had control over them, and not of the regular Wehrmacht. Although the German government tried to issue as many of its members as possible with military uniforms of all sort, ranging from field gray to camouflage, these could not be provided to all its members, thus many members of the "Volkssturm" wore makeshift paramilitary uniforms or uniforms from their civilian jobs (such as train conductors of the Reichsbahn). The simple paramilitary insignia of the "Volkssturm" were as follows:
Training and impact.
Typically, members of the "Volkssturm" received only very basic military training. It included a brief indoctrination and training on the use of basic weapons such as the Karabiner 98k rifle and "Panzerfaust". Because of continuous fighting and weapon shortages, weapon training was often minimal. There was also a lack of instructors, meaning that weapons training was sometimes done by World War I veterans drafted into service themselves. Often "Volkssturm" members were only able to familiarize themselves with their weapons when in actual combat.
There was no standardization of any kind and units were issued only what equipment was available. This was true of every form of equipment—"Volkssturm" members were required to bring their own uniforms and culinary equipment etc. This resulted in the units looking very ragged and, instead of boosting civilian morale, it often reminded people of Germany's desperate state. Armament was equally haphazard: though some Karabiner 98ks were on hand, members were also issued older Gewehr 98s and 19th-century Gewehr 71s and Steyr-Mannlicher M1888s, as well as Dreyse M1907 pistols. In addition there was a plethora of Soviet, British, Belgian, French, Italian, and other weapons that had been captured by German forces during the war. The Germans had also developed cheap but reasonably effective "Volkssturm" weapons, like MP 3008 machine pistols, Volkssturmgewehr 1-5 rifles and VMG-27 light machine guns. These were completely stamped and machine-pressed constructions (in the 1940s, industrial processes were much cruder than today, so a firearm needed great amounts of semi-artisanal work to be actually reliable). The "Volkssturm" troops were nominally supplied when and where possible by both by the Wehrmacht and the SS, but oftentimes they had little to spare. Being armed with leftovers compounded the "Volkssturm's" ineffectiveness; the large number of different ammunition types also put a strain on an already burdened logistics system (for example, the Gewehr 71s used a different type of ammunition than the two 98 rifles). In the last few months of the war, the shortages of modern firearms led to the use of weapons such as shotguns, and even muskets and crossbows taken from museums.
When units had completed their training and received armament, members took a customary oath to Hitler and were then dispatched into combat. Unlike most English-speaking countries, Germany had universal military service for all young men for several generations, so many of the older members would have had at least basic military training from when they served in the German Army and many would have been veterans of the First World War. "Volkssturm" units were supposed to be used only in their own districts, but many were sent directly to the front lines. Ultimately, it was their charge to confront the overwhelming power of the British, American, Canadian, Polish, and Soviet armies alongside Wehrmacht forces to either turn the tide of the war or set a shining example for future generations of Germans and expunge the defeat of 1918 by fighting to the last, dying before surrendering. It was an apocalyptic goal which some of those assigned to the "Volkssturm" took to heart. Unremittingly fanatical members of the "Volkssturm" refused to abandon the Nazi ethos unto the dying days of the Third Reich and in a number of instances, took brutal police "actions" against German civilians deemed defeatists or cowards.
On some occasions, members of the "Volkssturm" showed tremendous courage and a determined will to resist, more so even than soldiers in the "Wehrmacht". The "Volkssturm" battalion 25/235 for instance, started out with 400 men but fought on until there were only 10 men remaining. Fighting at Küstrin between 30 January to 29 March 1945, militia units made up mostly of the "Volkssturm" resisted for nearly two months. Losses were upwards of 60 percent for the "Volkssturm" at Kolberg, roughly 1900 of them died at Breslau, and during the Battle of Königsberg (Kaliningrad), another 2400 members of the "Volkssturm" were killed. At other times along the western front particularly, "Volkssturm" troops would cast their arms aside and disappear into the chaos. Youthful ardor and fanaticism among Hitler Youth members fighting with the "Volkssturm" or an insatiable sense of duty from old men proved tragic sometimes. An example shared by historian Stephen Fritz is instructive in this case:
Not every "Volkssturm" unit was suicidal or apocalyptic in outlook as the war drew closer to its end. Many of them lost their enthusiasm for the fight when it became clear that the Allies had won, prompting them to lay down their weapons and surrender - they also feared being captured by Allied forces and tortured or executed as partisans. Duty to their communities and sparing their fellow Germans from atrocities like that described near Bad Windsheim also played a part in their capitulation as did self-preservation.
Their most extensive use was during the Battle of Berlin, during which "Volkssturm" units fought in many parts of the city. This battle was particularly devastating to its formations, however, since many members fought to the death out of fear of being captured by the Soviets, holding out to the very end which was in keeping with their covenant. Unfortunately for them, a force of over 2.5 million Soviet troops, equipped with 6,250 tanks and over 40,000 artillery pieces was headed their way as the diminished remnants of the Wehrmacht were no match for their enemy. Meanwhile, Hitler decried "betrayal" to everyone yet hunkered-down in the Berlin bunker. Not eager to die what was thought a pointless death above the bunker, many older members of the "Volkssturm" looked for places to hide from the approaching Soviet Army. Juxtaposed against the tragic image of Berlin holding out against all odds, was the frequent exodus and capitulation of Wehrmacht soldiers and members of the "Volkssturm" in southern and western Germany.
Battle for Berlin.
In the Battle for Berlin, members of the "Volkssturm" (mainly young boys from the ages of 13-18 and old men) were used by the German high command as a last-ditch attempt to defend Berlin. The "Volkssturm" had a strength of about 60,000 in the Berlin area formed into 92 battalions, of which about 30 battalions of "Volkssturm I" (those with some weapons) were sent to forward positions while those of "Volkssturm II" (those without weapons) remained in the inner city. One of the few substantive fighting units left to defend Berlin was the LVI Panzer Corps, which occupied the southeastern sector of the town, whereas the remaining parts of the city were being defended by what remained of the SS, the "Volkssturm", and the Hitler Youth formations.
One notable and unusual "Volkssturm" unit in the Battle for Berlin was the 3/115 Siemensstadt Battalion. It comprised 770 men, mainly First World War veterans in their 50s who were reasonably fit factory workers, with experienced officers. Unlike most "Volkssturm" units it was quite well equipped and trained. It was formed into three rifle companies, a support company (with two infantry support guns, four infantry mortars and heavy machine guns), and a heavy weapons company (with four Soviet M-20 howitzers and a French De Bange 220 mm mortar). The battalion first engaged Soviet troops at Friedrichsfelde on April 21 and saw the heaviest fighting over the following two days. It held out until May 2 by which time it was down to just 50 rifles and two light machine guns. The survivors fell back to join other "Volkssturm" units. 26 men from the battalion were awarded the Iron Cross. Already in rubble from Allied bombing, the final stand in Berlin dwindled down to street fighting between highly trained, battle-hardened Russian troops at the brink of final victory against the remnants of German police units, a handful of soldiers, the "Volkssturm", and flak helpers. The chances of the "Volkssturm" making a major difference were never realistic in the face of the overwhelming Allied numbers and material superiority.
While Iron Crosses were being handed out in places like Berlin, other cities and towns like Parchim and Mecklenburg witnessed old elites, acting as military commandants over the Hitler Youth and "Volkssturm", asserting themselves and demanding that the defensive fighting stop so as to spare lives and property. Despite their best efforts, the last 4 months of the war were an exercise in futility for the "Volkssturm" and the Nazi leadership's insistence to continue the fight to the bitter end contributed an additional 1.23 million (approximated) deaths, half of them German military personnel and the other half from the "Volkssturm".
In fiction.
Gregor Dorfmeister, under the pseudonym of Manfred Gregor, published in 1958 the novel "Die Brücke" based on his experiences in a Volkssturm unit.
The novel has been adapted to film in 1959 and 2008.
Other nations:

</doc>
<doc id="8731" url="https://en.wikipedia.org/wiki?curid=8731" title="Director's cut">
Director's cut

A director's cut is an ordinary edited version of a film, and less often TV series, music video, commercials, comic book or video games, that is supposed to represent the director's own approved edit. 'Cut' explicitly refers to the process of film editing: the director's cut is preceded by the rough editor's cut and followed by the final cut meant for the public film release.
Director's cuts of film are not generally released to the public: with most film studios the director does not have a final cut privilege. The studio (whose investment is at risk) can insist on changes that they think will make the film profit more at the box office. This sometimes means a happier ending or less ambiguity, or excluding scenes that would earn a more audience-restricting rating, but more often means that the film is simply shortened to provide more screenings per day. The most common form of director's cut is therefore to have extra scenes added, often making the director's cut considerably longer than the final cut.
Origin of the phrase.
Traditionally, the "director's cut" is not, by definition, the director's ideal or preferred cut. The editing process of a film is broken into three basic stages: First is the rough cut, which matches the script without any reductions. Second, the editor's cut, which is reduced from the rough cut, according to the editor's tastes. Third is the final cut, which actually gets released or broadcast. It is often the case that a director approves of the final cut, and even prefers it to the so-called earlier "director's cut." The director's cut may include unsatisfactory takes, a preliminary soundtrack, a lack of desired pick-up shots etc., which the director would not like to be shown.
For example, the director's cut of "Pat Garrett and Billy the Kid" was 122 minutes long. It was then trimmed to the final/released cut of 105 minutes. Although not complete or refined to his satisfaction, director Sam Peckinpah still preferred the director's cut, as it was more inclusive and thorough than the 105-minute cut. The restored cut, at 115 minutes, is thus not the traditional "director's cut," but is closest to the director's preferred version, as it was reconstructed based on Peckinpah's notes, and according to his style in general. In this case, the director's cut and the director's ideal preferred cut are distinctly separate versions. Considering this definition, "Alien: The Director's Cut", for example, is simply a misuse of the phrase. As Ridley Scott explains in the DVD insert, the 2003 cut of "Alien" was created at the request of 20th Century Fox, who wanted to re-release "Alien" in a form that was somehow altered or enhanced. Scott agreed, and settled on making an alternative cut of the film. He describes it simply as a second version that he is also satisfied with, even though the original released cut is still his preferred version. In contrast, the director's cut of Scott's "Kingdom of Heaven" (which was a commercial failure in its 2005 theatrical release) is the true version of the film Scott wanted, nearly an hour longer and has been met with more critical acclaim than the original version, with "Empire" magazine stating: "The added 45 minutes in the Director’s Cut are like pieces missing from a beautiful but incomplete puzzle." 
Inception.
The trend of releasing director's cuts was first introduced in the early 1980s alongside the rise of the home video industry. Video releases of director's cuts were originally created for the small but dedicated cult fan market. Two of the first films to be re-released as a director's cut were Michael Cimino's "Heaven's Gate" (first aired on the Los Angeles cable station Z Channel) and Ridley Scott's "Blade Runner".
Criticism.
When it was discovered that the market for alternative versions of films was substantial, the studios themselves began to promote "director's cuts" for a wide array of films, even some where the director already had final cut of the theatrical release. These were usually assembled with the addition of deleted scenes, sometimes adding as much as a half-hour to the length of the film without regard to pacing and storytelling. Such "commercial" director's cuts are seldom considered superior to the original film and in many cases, fans think the films are diminished by the director's own ego or the studios' desire for revenue.
The director's cut is often considered a mixed bag, with an equal share of supporters and detractors. Roger Ebert approves of the use of the label in unsuccessful films that had been tampered with by studio executives, such as Sergio Leone's original cut of "Once Upon a Time in America", and the moderately successful theatrical version of "Daredevil", which were altered by studio interference for their theatrical release. However, Ebert considers adding such material to a successful film a waste. Even Ridley Scott stated on the DVD commentary of "Alien" that the original theatrical release was his director's cut, and that the new version was released as a marketing ploy.
Extended cuts and special editions.
A related concept to the "Director's Cut" is that of an "extended" or "special edition". An example is Peter Jackson's "The Lord of the Rings" and "The Hobbit" trilogies. While Jackson considers the theatrical releases of those six films to be a final "director's cut" within the constraints of theatrical exhibition, the extended cuts were produced so that fans of the material could see nearly all of the scenes shot for the script to develop more of J. R. R. Tolkien's world, but which were originally cut for running time, or other reasons. New music and special effects were also added to the cuts.
In rare instances, such as Peter Weir's "Picnic at Hanging Rock", John Cassavetes's "The Killing of a Chinese Bookie", Ridley Scott's Alien, and Blake Edwards' "Darling Lili", scenes have been deleted instead of added, creating a shorter, more compact cut.
Special editions such as George Lucas's "Star Wars" films, and Steven Spielberg's "E.T. the Extra-Terrestrial", in which special effects are redone in addition to a new edit, have also caused controversy. ("See " List of changes in Star Wars re-releases and "E.T. the Extra-Terrestrial: The 20th Anniversary").
Extended or special editions can also apply to films that have been extended for television and video against the wishes of the director, such as the TV versions of "Dune" (1984), "The Warriors" (1979) and the "Harry Potter" films, and the DVD editions of Ridley Scott films "Gladiator", "Black Hawk Down", and "American Gangster".
More recently, a slightly different take on the re-cutting of films was seen in a 2006 revision of the 1980 film "Superman II". Most releases that contain the label "director's cut" or "extended edition" include minor changes and/or scene additions not seen in a film's theatrical release, but that do not tend to greatly affect or change the plot, story or overall product. However the new version of "Superman II" (known as ) restores as much of the original director's conception as possible, making it a considerably different picture. More than half of the footage filmed for "Superman II" by the originally credited director (Richard Lester) has been removed from the film and replaced with Donner footage shot during the original principal photography from 1977–1978. There are also several newly filmed shots and many new visual effects, and Richard Donner is credited as director of the film instead of Richard Lester. Another example of this is Brian Helgeland's "Payback". Possibly the most infamous collection of cuts, edits, reversions and modifications to a single film falls to "Caligula". The film exists in at least 10 different versions ranging from a sub-90 minute television edit version of TV-14 (later TVMA) for cable television to an unrated full XXX pornographic version exceeding 3.5 hours.
On February 22, 2016, it was announced their will be an "Ultimate Edition" for , with the director's cut included. It will be rated R.
Video game director's cuts.
In video games, the term "director's cut" is usually used as a colloquialism to refer to an expanded version of a previously released game. Often, these expanded versions, also referred as "complete editions", will have additions to the gameplay or additional game modes and features outside the main portion of the game. As is the case with certain high-profile Japanese-produced games, the game designers may take the liberty to revise their product for the overseas market with additional features during the localization process. These features are later added back to the native market in a re-release of a game in what is often referred as the international version of the game. This was the case with the overseas versions of "Final Fantasy VII", "Metal Gear Solid" and "Rogue Galaxy", which contained additional features (such as new difficulty settings for "Metal Gear Solid"), resulting in re-released versions of those respective games in Japan ("Final Fantasy VII International", "Metal Gear Solid: Integral" and "Rogue Galaxy: Director's Cut"). In the case of ' and ', the American versions were released first, followed by the Japanese versions and then the European versions, with each regional release offering new content not found in the previous one. All of the added content from the Japanese and European versions of those games were included in the expanded editions titled ' and '.
Several of the Pokémon games have also received director's cuts and have used the term "extension," though "remake" and "third version" are also often used by many fans. These include "" (Japan only), "Pokémon Yellow", "Pokémon Crystal", "Pokémon Emerald", and "Pokémon Platinum".
Expanded editions that bear the term "director's cut" in their titles include ', "Resident Evil: Director's Cut", "Silent Hill 2: Director's Cut", ', "". and "Metal Slader Glory: Director's Cut" (a Super Famicom remake of a visual novel game for the Famicom).
Music "director's cuts".
"Director's cuts" in music are rarely released. A few exceptions include Guided by Voices' 1994 album "Bee Thousand", which was re-released as a three disc vinyl LP Director's cut in 2004, and Fall Out Boy's 2003 album "Take This to Your Grave", which was re-released as a Director's cut in 2005 with two extra tracks. It is not unheard-of, however, for a band to redo old tracks that originally left them displeased for an album re-release on a major label or a second edition. The term "director's cut" is rarely applied to them, though.
In 2011 British singer Kate Bush released the album titled "Director's Cut". It is made up of songs from her earlier albums "The Sensual World" and "The Red Shoes" which have been remixed and restructured, three of which were re-recorded completely.
Director's cut commercials.
In the advertisement industry, it is very common that a director delivers his or her perfect version of the spot. In the most cases, these special versions are never seen by the consumer and more than half of the ad must be removed to fit into a typical on-air timeslot.
Music video director's cut.
The music video for the 2006 Academy Award-nominated song "Listen", performed by Beyoncé Knowles, received a director's cut by Diane Martel. This version of the video was later included on Knowles' B'Day Anthology Video Album (2007). Janet and Michael Jackson's "Scream" and Weezer's "El Scorcho", both directed by Mark Romanek, and U2's "One", directed by Anton Corbijn, also have "director's cut" versions. Linkin Park also has a director's cut version for their music video "Faint" (which was also directed by Mark Romanek) in which one of the band members spray paints the words "En Proceso" on a wall, as well as Hoobastank also having one for 2004's "The Reason" which omits the woman getting hit by the car. Britney Spears' music video for 2007's "Gimme More" was first released as a director's cut on iTunes, with the official video released 3 days later. Many other director's cut music videos contain sexual content that can't be shown on TV thus creating alternative scenes, such as Thirty Seconds to Mars's "Hurricane", and in some cases, alternative videos, such as in the case of Spears' 2008 video for "Womanizer".

</doc>
<doc id="8733" url="https://en.wikipedia.org/wiki?curid=8733" title="Digital video">
Digital video

Digital video is a representation of moving visual images in the form of encoded digital data. This is in contrast to analog video, which represents moving visual images with analog signals.
The terms "camera", "video camera", and "camcorder" are used interchangeably in this article.
History.
Starting in the late 1970s to the early 1980s, several types of video production equipment that were digital in their internal workings were introduced, such as time base correctors (TBC) and digital video effects (DVE) units (one of the former being the Thomson-CSF 9100 Digital Video Processor, an internally all-digital full-frame TBC introduced in 1980, and two of the latter being the Ampex ADO, and the Nippon Electric Corporation (NEC) DVE). They operated by taking a standard analog composite video input and digitizing it internally. This made it easier to either correct or enhance the video signal, as in the case of a TBC, or to manipulate and add effects to the video, in the case of a DVE unit. The digitized and processed video information that was output from these units would then be converted back to standard analog video.
Later on in the 1970s, manufacturers of professional video broadcast equipment, such as Bosch (through their Fernseh division), RCA, and Ampex developed prototype digital videotape recorders (VTR) in their research and development labs. Bosch's machine used a modified 1" Type B transport, and recorded an early form of CCIR 601 digital video. Ampex's prototype digital video recorder used a modified 2" Quadruplex VTR (an Ampex AVR-3), but fitted with custom digital video electronics, and a special "octaplex" 8-head headwheel (regular analog 2" Quad machines only used 4 heads). The audio on Ampex's prototype digital machine, nicknamed by its developers as "Annie", still recorded the audio in analog as linear tracks on the tape, like 2" Quad. None of these machines from these manufacturers were ever marketed commercially, however.
Digital video was first introduced commercially in 1986 with the Sony D1 format, which recorded an uncompressed standard definition component video signal in digital form instead of the high-band analog forms that had been commonplace until then. Due to its expense, and the requirement of component video connections using 3 cables (such as YPbPr or RGB component video) to and from a D1 VTR that most television facilities were not wired for (composite NTSC or PAL video using one cable was the norm for most of them at that time), D1 was used primarily by large television networks and other component-video capable video studios.
In 1988, Sony and Ampex co-developed and released the D2 digital videocassette format, which recorded video digitally without compression in ITU-601 format, much like D1. But D2 had the major difference of encoding the video in composite form to the NTSC standard, thereby only requiring single-cable composite video connections to and from a D2 VCR, making it a perfect fit for the majority of television facilities at the time. This made D2 quite a successful format in the television broadcast industry throughout the late '80s and the '90s. D2 was also widely used in that era as the master tape format for mastering laserdiscs (prior to D2, most laserdiscs were mastered using analog 1" Type C videotape).
D1 & D2 would eventually be replaced by cheaper systems using video compression, most notably Sony's Digital Betacam (still heavily used as an electronic field production (EFP) recording format by professional television producers) that were introduced into the network's television studios. Other examples of digital video formats utilizing compression were Ampex's DCT (the first to employ such when introduced in 1992), the industry-standard DV and MiniDV (and its professional variations, Sony's DVCAM and Panasonic's DVCPRO), and Betacam SX, a lower-cost variant of Digital Betacam using MPEG-2 compression.
One of the first digital video products to run on personal computers was "PACo: The PICS Animation Compiler" from The Company of Science & Art in Providence, RI, which was developed starting in 1990 and first shipped in May 1991. PACo could stream unlimited-length video with synchronized sound from a single file (with the ".CAV" file extension) on CD-ROM. Creation required a Mac; playback was possible on Macs, PCs, and Sun Sparcstations. In 1992, Bernard Luskin, Philips Interactive Media, and Eric Doctorow, Paramount Worldwide Video, successfully put the first fifty videos in digital MPEG 1 on CD, developed the packaging and launched movies on CD, leading to advancing versions of MPEG, and to DVD.
QuickTime, Apple Computer's architecture for time-based and streaming data formats appeared in June, 1991. Initial consumer-level content creation tools were crude, requiring an analog video source to be digitized to a computer-readable format. While low-quality at first, consumer digital video increased rapidly in quality, first with the introduction of playback standards such as MPEG-1 and MPEG-2 (adopted for use in television transmission and DVD media), and then the introduction of the DV tape format allowing recordings in the format to be transferred direct to digital video files (containing the same video data recorded on the transferred DV tape) on an editing computer and simplifying the editing process, allowing non-linear editing systems (NLE) to be deployed cheaply and widely on desktop computers with no external playback/recording equipment needed, save for the computer simply requiring a FireWire port to interface to the DV-format camera or VCR. The widespread adoption of digital video has also drastically reduced the bandwidth needed for a high-definition video signal (with HDV and AVCHD, as well as several commercial variants such as DVCPRO-HD, all using less bandwidth than a standard definition analog signal) and tapeless camcorders based on flash memory and often a variant of MPEG-4.
Overview of basic properties.
Digital video comprises a series of orthogonal bitmap digital images displayed in rapid succession at a constant rate. In the context of video these images are called frames. We measure the rate at which frames are displayed in frames per second (FPS).
Since every frame is an orthogonal bitmap digital image it comprises a raster of pixels. If it has a width of W pixels and a height of H pixels we say that the frame size is W"x"H.
Pixels have only one property, their color. The color of a pixel is represented by a fixed number of bits. The more bits the more subtle variations of colors can be reproduced. This is called the color depth (CD) of the video.
An example video can have a duration (T) of 1 hour (3600"sec"), a frame size of 640x480 "(WxH)" at a color depth of 24"bits" and a frame rate of 25"fps". This example video has the following properties:
The most important properties are "bit rate" and "video size". The formulas relating those two with all other properties are:
while some secondary formulas are:
Regarding Interlacing.
In interlaced video each "frame" is composed of two "halves of an image". The first half contains only the odd-numbered lines of a full frame. The second half contains only the even-numbered lines. Those halves are referred to individually as "fields". Two consecutive fields compose a full frame. If an interlaced video has a frame rate of 15 frames per second the field rate is 30 fields per second. All the properties and formulas discussed here apply equally to interlaced video but one should be careful not to confuse the fields per second rate with the frames per second rate.
Properties of compressed video.
The above are accurate for uncompressed video. Because of the relatively high bit rate of uncompressed video, video compression is extensively used. In the case of compressed video each frame requires a small percentage of the original bits. Assuming a compression algorithm that shrinks the input data by a factor of CF, the bit rate and video size would equal to:
Please note that it is not necessary that all frames are equally compressed by a factor of CF. In practice they are not, so CF is the "average" factor of compression for "all" the frames taken together.
The above equation for the bit rate can be rewritten by combining the compression factor and the color depth like this:
The value (CD / CF) represents the average bits per pixel (BPP). As an example, if we have a color depth of 12bits/pixel and an algorithm that compresses at 40x, then BPP equals 0.3 (12/40). So in the case of compressed video the formula for bit rate is:
In fact the same formula is valid for uncompressed video because in that case one can assume that the "compression" factor is 1 and that the average bits per pixel equal the color depth.
More on bit rate and BPP.
As is obvious by its definition bit rate is a measure of the rate of information content of the digital video stream. In the case of uncompressed video, bit rate corresponds directly to the quality of the video (remember that bit rate is proportional to every property that affects the video quality). Bit rate is an important property when transmitting video because the transmission link must be capable of supporting that bit rate. Bit rate is also important when dealing with the storage of video because, as shown above, the video size is proportional to the bit rate and the duration. Bit rate of uncompressed video is too high for most practical applications. Video compression is used to greatly reduce the bit rate.
BPP is a measure of the efficiency of compression. A true-color video with no compression at all may have a BPP of 24 bits/pixel. Chroma subsampling can reduce the BPP to 16 or 12 bits/pixel. Applying jpeg compression on every frame can reduce the BPP to 8 or even 1 bits/pixel. Applying video compression algorithms like MPEG1, MPEG2 or MPEG4 allows for fractional BPP values.
Constant bit rate versus variable bit rate.
As noted above BPP represents the "average" bits per pixel. There are compression algorithms that keep the BPP almost constant throughout the entire duration of the video. In this case we also get video output with a constant bit rate (CBR). This CBR video is suitable for real-time, non-buffered, fixed bandwidth video streaming (e.g. in videoconferencing).
Noting that not all frames can be compressed at the same level because quality is more severely impacted for scenes of high complexity some algorithms try to constantly adjust the BPP. They keep it high while compressing complex scenes and low for less demanding scenes. This way one gets the best quality at the smallest average bit rate (and the smallest file size accordingly). Of course when using this method the bit rate is variable because it tracks the variations of the BPP.
Technical overview.
Standard film stocks such as 16 mm and 35 mm record at 24 frames per second. For video, there are two frame rate standards: NTSC, which shoot at 30/1.001 (about 29.97) frames per second or 59.94 fields per second, and PAL, 25 frames per second or 50 fields per second.
Digital video cameras come in two different image capture formats: interlaced and deinterlaced / progressive scan.
Interlaced cameras record the image in alternating sets of lines: the odd-numbered lines are scanned, and then the even-numbered lines are scanned, then the odd-numbered lines are scanned again, and so on. One set of odd or even lines is referred to as a "field", and a consecutive pairing of two fields of opposite parity is called a "frame". Deinterlaced cameras records each frame as distinct, with all scan lines being captured at the same moment in time. Thus, interlaced video captures samples the scene motion twice as often as progressive video does, for the same number of frames per second. Progressive-scan camcorders generally produce a slightly sharper image. However, motion may not be as smooth as interlaced video which uses 50 or 59.94 fields per second, particularly if they employ the 24 frames per second standard of film.
Digital video can be copied with no degradation in quality. No matter how many generations of a digital source is copied, it will still be as clear as the original first generation of digital footage. However a change in parameters like frame size as well as a change of the digital format can decrease the quality of the video due to new calculations that have to be made. Digital video can be manipulated and edited to follow an order or sequence on an NLE, or non-linear editing workstation, a computer-based device intended to edit video and audio. More and more, videos are edited on readily available, increasingly affordable consumer-grade computer hardware and software. However, such editing systems require ample disk space for video footage. The many video formats and parameters to be set make it quite impossible to come up with a specific number for how many minutes need how much time.
Digital video has a significantly lower cost than 35 mm film. In comparison to the high cost of film stock, the tape stock (or other electronic media used for digital video recording, such as flash memory or hard disk drive) used for recording digital video is very inexpensive. Digital video also allows footage to be viewed on location without the expensive chemical processing required by film. Also physical deliveries of tapes and broadcasts do not apply anymore. Digital television (including higher quality HDTV) started to spread in most developed countries in early 2000s. Digital video is also used in modern mobile phones and video conferencing systems. Digital video is also used for Internet distribution of media, including streaming video and peer-to-peer movie distribution. However even within Europe are lots of TV-Stations not broadcasting in HD, due to restricted budgets for new equipment for processing HD.
Many types of video compression exist for serving digital video over the internet and on optical disks. The file sizes of digital video used for professional editing are generally not practical for these purposes, and the video requires further compression with codecs such as Sorenson, H.264 and more recently Apple ProRes especially for HD. Probably the most widely used formats for delivering video over the internet are MPEG4, Quicktime, Flash and Windows Media, while MPEG2 is used almost exclusively for DVDs, providing an exceptional image in minimal size but resulting in a high level of CPU consumption to decompress.
, the highest resolution demonstrated for digital video generation is 35 megapixels (8192 x 4320). The highest speed is attained in industrial and scientific high speed cameras that are capable of filming 1024x1024 video at up to 1 million frames per second for brief periods of recording.
Interfaces and cables.
Many interfaces have been designed specifically to handle the requirements of uncompressed digital video (from roughly 400 Mbit/s to 10 Gbit/s):
The following interface has been designed for carrying MPEG-Transport compressed video:
Compressed video is also carried using UDP-IP over Ethernet. Two approaches exist for this:
Storage formats.
Encoding.
All current formats, which are listed below, are PCM based.

</doc>
<doc id="8735" url="https://en.wikipedia.org/wiki?curid=8735" title="BIND">
BIND

BIND , or named , is the most widely used Domain Name System (DNS) software on the Internet.
On Unix-like operating systems it is the "de facto" standard.
The software was originally designed at the University of California Berkeley (UCB) in the early 1980s. The name originates as an acronym of "Berkeley Internet Name Domain", reflecting the application's use within UCB. The software consists, most prominently, of the DNS server component, called "named", a contracted form of "name daemon". In addition the suite contains various administration tools, and a DNS resolver interface library. The latest version of BIND is BIND 9, first released in 2000. 
Starting in 2009, the Internet Software Consortium (ISC) developed a new software suite, initially called BIND10. With release version 1.2.0 the project was renamed "Bundy" to terminate ISC involvement in the project.
Database support.
While earlier versions of BIND offered no mechanism to store and retrieve zone data in anything other than flat text files, in 2007 BIND 9.4 DLZ provided a compile-time option for zone storage in a variety of database formats including LDAP, Berkeley DB, PostgreSQL, MySQL, and ODBC.
BIND 10 planned to make the data store modular, so that a variety of databases may be connected.
Security.
The BIND 4 and BIND 8 releases both had serious security vulnerabilities. Their use is strongly discouraged. BIND 9 was a complete rewrite, in part to mitigate these ongoing security issues.
Security issues that are discovered in BIND 9 are patched and publicly disclosed in keeping with common principles of open source software. A complete list of security defects that have been discovered and disclosed in BIND9 is maintained by Internet Systems Consortium, the current authors of the software.
History.
Originally written by four graduate students at the Computer Systems Research Group at the University of California, Berkeley (UCB), BIND was first released with Berkeley Software Distribution 4.3BSD. Paul Vixie started maintaining it in 1988 while working for Digital Equipment Corporation. , the Internet Systems Consortium maintains, updates, and writes new versions of BIND. 
BIND was written by Douglas Terry, Mark Painter, David Riggle and Songnian Zhou in the early 1980s at the University of California, Berkeley as a result of a DARPA grant. The acronym "BIND" is for "Berkeley Internet Name Domain", from a technical paper published in 1984.
Versions of BIND through 4.8.3 were maintained by the Computer Systems Research Group (CSRG) at UC Berkeley.
In the mid-1980s, Paul Vixie of DEC took over BIND development, releasing versions 4.9 and 4.9.1. Paul Vixie continued to work on BIND after leaving DEC. BIND Version 4.9.2 was sponsored by Vixie Enterprises. Vixie eventually founded the ISC, which became the entity responsible for BIND versions starting with 4.9.3.
BIND 8 was released by ISC in May 1997.
Version 9 was developed by Nominum, Inc. under an ISC outsourcing contract, and the first version was released October 9, 2000. It was written from scratch in part to address the architectural difficulties with auditing the earlier BIND code bases, and also to support DNSSEC (DNS Security Extensions). Other important features of BIND 9 include: TSIG, nsupdate, IPv6, rndc (remote name daemon control), views, multiprocessor support, and an improved portability architecture. rndc uses a shared secret to provide encryption for local and remote terminals during each session. The development of BIND 9 took place under a combination of commercial and military contracts. Most of the features of BIND 9 were funded by UNIX vendors who wanted to ensure that BIND stayed competitive with Microsoft's DNS offerings; the DNSSEC features were funded by the US military, which regarded DNS security as important. BIND 9 was released in September 2000.
In 2009, ISC started an effort to develop a new version of the software suite, called BIND10. In addition to DNS service, the BIND10 suite also included IPv4 and IPv6 DHCP server components. In April 2014, with the BIND10 release 1.2.0 the ISC concluded its development work of the project and renamed the project "Bundy", moving the source code repository to GitHub for further development by outside public efforts. Bundy is community-supported at the web site http://bundy-dns.de/. ISC discontinued its involvement in the project due to cost-cutting measures. The development of DHCP components was split off to become a new "Kea" project.

</doc>
<doc id="8736" url="https://en.wikipedia.org/wiki?curid=8736" title="Djbdns">
Djbdns

The djbdns software package is a DNS implementation. It was created by Daniel J. Bernstein in response to his frustrations with repeated security holes in the widely used BIND DNS software. As a challenge, Bernstein offered a $1000 prize for the first person to find a security hole in djbdns, which was awarded in March 2009 to Matthew Dempsky.
, djbdns's tinydns component was the second most popular DNS server in terms of the number of domains for which it was the authoritative server, and third most popular in terms of the number of DNS hosts running it.
djbdns has never been vulnerable to the widespread cache poisoning vulnerability reported in July 2008, but it has been discovered that it is vulnerable to a related attack.
The source code has not been centrally managed since its release in 2001, and was released into the public domain in 2007. As of March 2009, there are a number of forks, one of which is dbndns (part of the Debian Project), and more than a dozen patches to modify the released version.
While djbdns does not directly support DNSSEC, there are third party patches to add DNSSEC support to djbdns' authoritative-only tinydns component.
Components.
The djbdns software consists of servers, clients, and miscellaneous configuration tools.
Design.
In djbdns, different features and services are split off into separate programs. For example, zone transfers, zone file parsing, caching, and recursive resolving are implemented as separate programs. The result of these design decisions is a reduction in code size and complexity of the daemon program that provides the core function of answering lookup requests. Bernstein asserts that this is true to the spirit of the Unix operating system, and makes security verification much simpler.
Copyright status.
On December 28, 2007, Bernstein released djbdns into the public domain. Previously the package was distributed free of charge as license-free software. However this did not permit the distribution of modified versions of djbdns, which was one of the core principles of open-source software. Consequently, it was not included in Linux distributions which required all components to be open-source.

</doc>
<doc id="8741" url="https://en.wikipedia.org/wiki?curid=8741" title="Dylan (programming language)">
Dylan (programming language)

Dylan is a multi-paradigm programming language that includes support for functional and object-oriented programming, and is dynamic and reflective while providing a programming model designed to support efficient machine code generation, including fine-grained control over dynamic and static behaviors. It was created in the early 1990s by a group led by Apple Computer.
A concise and thorough overview of the language may be found in the Dylan Reference Manual.
Dylan derives from Scheme and Common Lisp and adds an integrated object system derived from the Common Lisp Object System (CLOS). In Dylan, all values (including numbers, characters, functions, and classes) are first-class objects. Dylan supports multiple inheritance, polymorphism, multiple dispatch, keyword arguments, object introspection, pattern-based syntax extension macros, and many other advanced features. Programs can express fine-grained control over dynamism, admitting programs that occupy a continuum between dynamic and static programming and supporting evolutionary development (allowing for rapid prototyping followed by incremental refinement and optimization).
Dylan's main design goal is to be a dynamic language well-suited for developing commercial software. Dylan attempts to address potential performance issues by introducing "natural" limits to the full flexibility of Lisp systems, allowing the compiler to clearly understand compilable units (i.e., libraries).
Although deriving much of its semantics from Scheme and other Lisps—some implementations were in fact initially built within existing Lisp systems—Dylan has an ALGOL-like syntax rather than a Lisp-like prefix syntax.
History.
Dylan was created in the early 1990s by a group led by Apple Computer. At one point in its development it was intended for use with Apple's Newton computer, but the Dylan implementation did not reach sufficient maturity in time, and Newton instead used a combination of C and the NewtonScript developed by Walter Smith. Apple ended their Dylan development effort in 1995, though they made a "technology release" version available (Apple Dylan TR1) that included an advanced IDE.
Two other groups contributed to the design of the language and developed implementations: Harlequin released a commercial IDE for Microsoft Windows and Carnegie Mellon University released an open source compiler for Unix systems called Gwydion Dylan. Both of these implementations are now open source. The Harlequin implementation is now known as Open Dylan and is maintained by a group of volunteers, the Dylan Hackers.
The Dylan language was code-named Ralph. James Joaquin chose the name Dylan for "DYnamic LANguage."
Syntax.
Dylan uses an Algol-like syntax designed by Michael Kahl. It is described in great detail in the Dylan Reference Manual. This page shows examples of some syntax features that are more unusual. Many of them come from Dylan's Lisp heritage.
A simple class with several slots:
The same class, rewritten in the most minimal way possible:
A factorial function:
Originally, Dylan used a Lisp-like prefix syntax, which is based on s-expressions:
By the time the language design was completed, it was changed to an Algol-like syntax, with the expectation that it would be more familiar to a wider audience of programmers.
Modules vs. namespace.
In many object-oriented languages, classes are the primary means of encapsulation and modularity; each class defines a namespace and controls which definitions are externally visible. In addition, classes in many languages define an indivisible unit that must be used as a whole—if you want to use a String concatenation function, you must import and compile against all of String.
Some languages also include a separate, explicit namespace or module system that performs encapsulation in a more general way. Dylan is such a language.
In Dylan, the concepts of compile-unit and import-unit are separated, and classes have nothing specifically to do with either. A "library" defines items that should be compiled and handled together, while a "module" defines the namespace. Classes can be placed together in modules, or cut across them, as the programmer wishes. Often the complete definition for a class does not exist in a single module, but is spread across several that are optionally collected together. Different programs can have different definitions of the same class, including only what they need.
For example, consider an add-on library for regex support on String. In some languages, in order for the functionality to be included in strings, the functionality has to be added to the String namespace itself. As soon as you do this, the String class becomes larger, and people who don't need to use regex still have to "pay" for it in increased library size. For this reason these sorts of add-ons are typically placed in their own namespaces and objects. The downside to this approach is that the new functionality is no longer a "part of" string; instead, it is isolated in its own set of functions that have to be called separately. Instead of codice_1, which would be the natural organization from an OO point of view, you use something like codice_2, which effectively reverses the ordering.
In addition, under Dylan many interfaces can be defined for the same code, for instance the String concatenation method could be placed in both the String interface, and the "concat" interface which collects together all of the different concatenation functions from various classes. This is more commonly used in math libraries, where functions tend to be applicable to widely differing object types.
A more practical use of the interface construct is to build public and private versions of a module, something that other languages include as a "bolt on" feature that invariably causes problems and adds syntax. Under Dylan the programmer can simply place every function call in the "Private" or "Development" interface, and collect up publicly accessible functions in "Public". Under Java or C++ the visibility of an object is defined in the code itself, meaning that to support a similar change the programmer would be forced to re-write the definitions completely, and could not have two versions at the same time.
Classes.
Classes in Dylan describe "slots" (data members, fields, ivars, etc.) of objects in a fashion similar to most OO languages. All access to slots are via methods, as in Smalltalk. Default getter and setter methods are automatically generated based on the slot names. In contrast with most other OO languages, other methods applicable to the class are often defined outside of the class, and thus class definitions in Dylan typically include the definition of the storage only. For instance:
define class <window> (<view>)
end class;
In this example the class "codice_3" is defined. The <class name> syntax is convention only, to make the class names stand out—the angle brackets are merely part of the class name. In comparison, in some languages the convention is to capitalize the first letter of the class name or to prefix the name with a "C" or "T" (for example). codice_3 inherits from a single class, codice_5, and contains two slots, codice_6 holding a string for the window title, and codice_7 holding an X-Y point for a corner of the window. In this particular example the title has been given a default value, while the position has not. The optional "init-keyword" syntax allows the programmer to specify the initial value of the slot when instantiating an object of the class.
In languages such as C++ or Java, the class would also define its interface. In this case the definition above has no explicit instructions, so in both languages access to the slots and methods is considered codice_8, meaning they can be used only by subclasses. In order to allow unrelated code to use the window instances, they would have to be declared codice_9.
In Dylan these sorts of visibility rules are not considered part of the code itself, but of the module/interface system. This adds considerable flexibility. For instance, one interface used during early development could declare everything public, whereas one used in testing and deployment could limit this. With C++ or Java these changes would require changes to the source code itself, so people won't do it, whereas in Dylan this is a completely unrelated concept.
Although this example does not use it, Dylan also supports multiple inheritance.
Methods and generic functions.
In Dylan, methods are not intrinsically associated with any particular class; methods can be thought of as existing outside of classes. Like CLOS, Dylan is based on multimethods, where the specific method to be called is chosen based upon the types of all its arguments. The method does not have to be known at compile time, the understanding being that the required functionality may be available or may not, based on the user's preferences.
Under Java the same methods would be isolated in a particular class. In order to use that functionality the programmer is forced to "import" that class and refer to it explicitly in order to call the method. If that class is not available, or unknown at compile time, the application simply won't compile.
In Dylan, code is isolated from storage in "functions". Many classes have methods that call their own functions, thereby looking and feeling like most other OO languages. However code may also be located in "generic functions", meaning they are not attached to a particular class, and can be called natively by anyone. Linking a particular generic function to a method in a class is accomplished this way:
define method turn-blue (w :: <window>)
end method;
This definition is similar to those in other languages, and would likely be encapsulated within the codice_3 class. Note the := setter call, which is syntactic sugar for codice_11.
The utility of generic methods comes into its own when you consider more "generic" examples. For instance, one common function in most languages is the codice_12, which returns some human-readable form for the object. For instance, a window might return its title and its position in parens, while a string would return itself. In Dylan these methods could all be collected into a single module called "codice_12", thereby removing this code from the definition of the class itself. If a particular object did not support a codice_12, it could be easily added in the codice_12 module.
Extensibility.
This whole concept might strike some readers as very odd. The code to handle codice_12 for a window isn't defined in codice_3? This might not make any sense until you consider how Dylan handles the call of the codice_12. In most languages when the program is compiled the codice_12 for codice_3 is looked up and replaced with a pointer (more or less) to the method. In Dylan this occurs when the program is first run; the runtime builds a table of method-name/parameters details and looks up methods dynamically via this table. That means that a function for a particular method can be located anywhere, not just in the compile-time unit. In the end the programmer is given considerable flexibility in terms of where to place their code, collecting it along class lines where appropriate, and functional lines where it's not.
The implication here is that a programmer can add functionality to existing classes by defining functions in a separate file. For instance, you might wish to add spell checking to all codice_21s, which in most languages would require access to the source code of the string class—and such basic classes are rarely given out in source form. In Dylan (and other "extensible languages") the spell checking method could be added in the codice_22 module, defining all of the classes on which it can be applied via the codice_23 construct. In this case the actual functionality might be defined in a single generic function, which takes a string and returns the errors. When the codice_22 module is compiled into your program, all strings (and other objects) will get the added functionality.
Apple Dylan.
Apple Dylan is the implementation of Dylan produced by Apple Computer. It was originally developed for the Apple Newton product.

</doc>
<doc id="8742" url="https://en.wikipedia.org/wiki?curid=8742" title="Dublin Core">
Dublin Core

The Dublin Core Schema is a small set of vocabulary terms that can be used to describe web resources (video, images, web pages, etc.), as well as physical resources such as books or CDs, and objects like artworks. The full set of Dublin Core metadata terms can be found on the Dublin Core Metadata Initiative (DCMI) website. The original set of 15 classic metadata terms, known as the Dublin Core Metadata Element Set are endorsed in the following standards documents:
Dublin Core Metadata may be used for multiple purposes, from simple resource description, to combining metadata vocabularies of different metadata standards, to providing interoperability for metadata vocabularies in the Linked Data cloud and Semantic Web implementations.
Background.
"Dublin" refers to Dublin, Ohio, USA where the schema originated during the 1995 invitational OCLC/NCSA Metadata Workshop, hosted by the Online Computer Library Center (OCLC), a library consortium based in Dublin, and the National Center for Supercomputing Applications (NCSA). "Core" refers to the metadata terms as "broad and generic being usable for describing a wide range of resources". The semantics of Dublin Core were established and are maintained by an international, cross-disciplinary group of professionals from librarianship, computer science, text encoding, museums, and other related fields of scholarship and practice.
Starting in 2000, the Dublin Core community focused on "application profiles" – the idea that metadata records would use Dublin Core together with other specialized vocabularies to meet particular implementation requirements. During that time, the World Wide Web Consortium's work on a generic data model for metadata, the Resource Description Framework (RDF), was maturing. As part of an extended set of DCMI Metadata Terms, Dublin Core became one of the most popular vocabularies for use with RDF, more recently in the context of the Linked Data movement.
The Dublin Core Metadata Initiative (DCMI) provides an open forum for the development of interoperable online metadata standards for a broad range of purposes and of business models. DCMI's activities include consensus-driven working groups, global conferences and workshops, standards liaison, and educational efforts to promote widespread acceptance of metadata standards and practices. In 2008, DCMI separated from OCLC and incorporated as an independent entity.
Currently, any and all changes that are made to the Dublin Core standard, are reviewed by a DCMI Usage Board within the context of a DCMI Namespace Policy (DCMI-NAMESPACE). This policy describes how terms are assigned and also sets limits on the amount of editorial changes allowed to the labels, definitions, and usage comments.
Levels of the standard.
The Dublin Core standard originally includes two levels: Simple and Qualified. Simple Dublin Core comprised 15 elements; Qualified Dublin Core included three additional elements (Audience, Provenance and RightsHolder), as well as a group of element refinements (also called qualifiers) that could refine the semantics of the elements in ways that may be useful in resource discovery.
Since 2012 the two have been incorporated into the DCMI Metadata Terms as a single set of terms using the Resource Description Framework (RDF). The full set of elements is found under the namespace http://purl.org/dc/terms/. Because the definition of the terms often contains domains and ranges, which may not be compatible with the pre-RDF definitions used for the original 15 Dublin Core elements, there is a separate namespace for the original 15 elements as previously defined: http://purl.org/dc/elements/1.1/.
Dublin Core Metadata Element Set Version 1.1.
The original Dublin Core Metadata Element Set consists of 15 metadata elements:
Each Dublin Core element is optional and may be repeated. The DCMI has established standard ways to refine elements and encourage the use of encoding and vocabulary schemes. There is no prescribed order in Dublin Core for presenting or using the elements. The Dublin Core became ISO 15836 standard in 2006 and is used as a base-level data element set for the description of learning resources in the ISO/IEC 19788-2 Metadata for learning resources (MLR) – Part 2: Dublin Core elements, prepared by the ISO/IEC JTC1 SC36.
Full information on element definitions and term relationships can be found in the Dublin Core Metadata Registry.
An example of use nd mentio of D.C. (by WebCite).
At the web page which serves as the "archive" form for WebCite, it says, in part: "Metadata (optional)         These are Dublin Core elements. .".
Qualified Dublin Core (deprecated in 2012).
Subsequent to the specification of the original 15 elements, an ongoing process to develop exemplary terms extending or refining the Dublin Core Metadata Element Set (DCMES) was begun. The additional terms were identified, generally in working groups of the Dublin Core Metadata Initiative, and judged by the DCMI Usage Board to be in conformance with principles of good practice for the qualification of Dublin Core metadata elements.
Elements refinements make the meaning of an element narrower or more specific. A refined element shares the meaning of the unqualified element, but with a more restricted scope. The guiding principle for the qualification of Dublin Core elements, colloquially known as the "Dumb-Down Principle", states that an application that does not understand a specific element refinement term should be able to ignore the qualifier and treat the metadata value as if it were an unqualified (broader) element. While this may result in some loss of specificity, the remaining element value (without the qualifier) should continue to be generally correct and useful for discovery.
In addition to element refinements, Qualified Dublin Core includes a set of recommended encoding schemes, designed to aid in the interpretation of an element value. These schemes include controlled vocabularies and formal notations or parsing rules. A value expressed using an encoding scheme may thus be a token selected from a controlled vocabulary (for example, a term from a classification system or set of subject headings) or a string formatted in accordance with a formal notation, for example, "2000-12-31" as the ISO standard expression of a date. If an encoding scheme is not understood by an application, the value may still be useful to human reader.
Audience, Provenance and RightsHolder are elements, but not part of the Simple Dublin Core 15 elements. Use Audience, Provenance and RightsHolder only when using Qualified Dublin Core.
DCMI also maintains a small, general vocabulary recommended for use within the element Type. This vocabulary currently consists of 12 terms.
DCMI Metadata Terms.
The Dublin Core Metadata Initiative (DCMI) Metadata Terms is the current set of the Dublin Core vocabulary. This set includes the fifteen terms of the Dublin Core Metadata Element Set (in "italic"), as well as the qualified terms. Each term has a unique URI in the namespace http://purl.org/dc/terms, and all are defined as RDF properties.
Syntax.
Syntax choices for Dublin Core metadata depends on a number of variables, and "one size fits all" prescriptions rarely apply. When considering an appropriate syntax, it is important to note that Dublin Core concepts and semantics are designed to be syntax independent and are equally applicable in a variety of contexts, as long as the metadata is in a form suitable for interpretation both by machines and by human beings.
The Dublin Core Abstract Model provides a reference model against which particular Dublin Core encoding guidelines can be compared, independent of any particular encoding syntax. Such a reference model allows implementers to gain a better understanding of the kinds of descriptions they are trying to encode and facilitates the development of better mappings and translations between different syntax.
Some applications.
One Document Type Definition based on Dublin Core is the Open Source Metadata Framework (OMF) specification. OMF is in turn used by Rarian (superseding ScrollKeeper), which is used by the GNOME desktop and KDE help browsers and the ScrollServer documentation server. PBCore is also based on Dublin Core. The Zope CMF's Metadata products, used by the Plone, ERP5, the Nuxeo CPS Content management systems, SimpleDL, and FedoraCommons also implement Dublin Core. The EPUB e-book format uses Dublin Core metadata in the OPF file. eXo Platform also implements Dublin Core.
DCMI also maintains a list of projects using Dublin Core on its website.

</doc>
<doc id="8743" url="https://en.wikipedia.org/wiki?curid=8743" title="Document Object Model">
Document Object Model

The Document Object Model (DOM) is a cross-platform and language-independent "convention" for representing and interacting with objects in HTML, XHTML, and XML documents. The nodes of every document are organized in a tree structure, called the "DOM tree". Objects in the DOM tree may be addressed and manipulated by using methods on the objects. The public interface of a DOM is specified in its application programming interface (API).
The history of the Document Object Model is intertwined with the history of the "browser wars" of the late 1990s between Netscape Navigator and Microsoft Internet Explorer, as well as with that of JavaScript and JScript, the first scripting languages to be widely implemented in the layout engines of web browsers.
Legacy DOM.
JavaScript was released by Netscape Communications in 1995 within Netscape Navigator 2.0. Netscape's competitor, Microsoft, released Internet Explorer 3.0 the following year with a port of JavaScript called JScript. JavaScript and JScript let web developers create web pages with client-side interactivity. The limited facilities for detecting user-generated events and modifying the HTML document in the first generation of these languages eventually became known as "DOM Level 0" or "Legacy DOM." No independent standard was developed for DOM Level 0, but it was partly described in the specification of HTML 4.
Legacy DOM was limited in the kinds of elements that could be accessed. Form, link and image elements could be referenced with a hierarchical name that began with the root document object. A hierarchical name could make use of either the names or the sequential index of the traversed elements. For example, a form input element could be accessed as either codice_1 or codice_2.
The Legacy DOM enabled client-side form validation and the popular "rollover" effect.
Intermediate DOM.
In 1997, Netscape and Microsoft released version 4.0 of Netscape Navigator and Internet Explorer respectively, adding support for Dynamic HTML (DHTML), functionality enabling changes to a loaded HTML document. DHTML required extensions to the rudimentary document object that was available in the Legacy DOM implementations. Although the Legacy DOM implementations were largely compatible since JScript was based on JavaScript, the DHTML DOM extensions were developed in parallel by each browser maker and remained incompatible. These versions of the DOM became known as the "Intermediate DOM."
Standardization.
The World Wide Web Consortium (W3C), founded in 1994 to promote open standards for the World Wide Web, brought Netscape Communications and Microsoft together with other companies to develop a standard for browser scripting languages, called "ECMAScript." The first version of the standard was published in 1997. Subsequent releases of JavaScript and JScript would implement the ECMAScript standard for greater cross-browser compatibility.
After the release of ECMAScript, W3C began working on a standardized DOM. The initial DOM standard, known as "DOM Level 1", was recommended by W3C in late 1998. About the same time, Internet Explorer 5.0 shipped with limited support for DOM Level 1. DOM Level 1 provided a complete model for an entire HTML or XML document, including means to change any portion of the document. Non-conformant browsers such as Internet Explorer 4.x and Netscape 4.x were still widely used as late as 2000.
DOM Level 2 was published in late 2000. It introduced the codice_3 function as well as an event model and support for XML namespaces and CSS.
DOM Level 3, the current release of the DOM specification, published in April 2004, added support for XPath and keyboard event handling, as well as an interface for serializing documents as XML.
DOM Level 4 is currently being developed. Last Call Working Draft was released in February 2014.
By 2005, large parts of W3C DOM were well-supported by common ECMAScript-enabled browsers, including Microsoft Internet Explorer version 6 (from 2001), Opera, Safari and Gecko-based browsers (like Mozilla, Firefox, SeaMonkey and Camino).
Applications.
Web browsers.
To render a document such as an HTML page, most web browsers use an internal model similar to the DOM. The nodes of every document are organized in a tree structure, called the "DOM tree", with topmost node named "Document object". When an HTML page is rendered in browsers, the browser downloads the HTML into local memory and automatically parses it to display the page on screen. The DOM is also the way JavaScript transmits the state of the browser in HTML pages.
JavaScript.
When a web page is loaded, the browser creates a Document Object Model of the page.
With the object model, JavaScript is fully enabled to create dynamic HTML:
Implementations.
Because DOM supports navigation in any direction (e.g., parent and previous sibling) and allows for arbitrary modifications, an implementation must at least buffer the document that has been read so far (or some parsed form of it).
Layout engines.
Web browsers rely on layout engines to parse HTML into a DOM. Some layout engines, such as Trident/MSHTML, are associated primarily or exclusively with a particular browser, such as Internet Explorer. Others, such as Blink, WebKit, and Gecko, are shared by a number of browsers, such as Google Chrome, Opera, Safari, and Firefox. The different layout engines implement the DOM standards to varying degrees of compliance.
Libraries.
DOM implementations:
APIs that expose DOM implementations:
Inspection tools

</doc>
<doc id="8745" url="https://en.wikipedia.org/wiki?curid=8745" title="Design pattern">
Design pattern

A design pattern is the re-usable form of a solution to a design problem. The idea was introduced by the architect Christopher Alexander and has been adapted for various other disciplines, most notably computer science.
An organized collection of design patterns that relate to a particular field is called a pattern language. This language gives a common terminology for discussing the situations designers are faced with.
Documenting a pattern requires explaining why a particular situation causes problems, and how the components of the pattern relate to each other to give the solution. Christopher Alexander describes common design problems as arising from "conflicting forces" — such as the conflict between wanting a room to be sunny and wanting it not to overheat on summer afternoons. A pattern would not tell the designer how many windows to put in the room; instead, it would propose a set of values to guide the designer toward a decision that is best for their particular application. Alexander, for example, suggests that enough windows should be included to direct light all around the room. He considers this a good solution because he believes it increases the enjoyment of the room by its occupants. Other authors might come to different conclusions, if they place higher value on heating costs, or material costs. These values, used by the pattern's author to determine which solution is "best", must also be documented within the pattern.
Pattern documentation should also explain when it is applicable. Since two houses may be very different from one another, a design pattern for houses must be broad enough to apply to both of them, but not so vague that it doesn't help the designer make decisions. The range of situations in which a pattern can be used is called its context. Some examples might be "all houses", "all two-story houses", or "all places where people spend time".
For instance, in Christopher Alexander's work, bus stops and waiting rooms in a surgery center are both within the context for the pattern "A PLACE TO WAIT".
(Note: there is debate about whether the "Gang of Four" book actually contains any patterns in the Alexandrian's sense.)

</doc>
<doc id="8748" url="https://en.wikipedia.org/wiki?curid=8748" title="N,N-Dimethyltryptamine">
N,N-Dimethyltryptamine

N","N"-Dimethyltryptamine (DMT or N","N"-DMT) is a psychedelic compound of the tryptamine family. It is a structural analog of serotonin and melatonin and a functional analog of other psychedelic tryptamines such as 4-AcO-DMT, 5-MeO-DMT, 5-HO-DMT, psilocybin (4-PO-DMT), and psilocin (4-HO-DMT).
It is consumed by indigenous Amazonian Amerindian cultures through the consumption of ayahuasca for divinatory and healing purposes.
History.
DMT was first synthesized in 1931 by Canadian chemist Richard Helmuth Fredrick Manske (1901–1977). In general, its discovery as a natural product is credited to Brazilian chemist and microbiologist Oswaldo Gonçalves de Lima (1908–1989) who, in 1946, isolated an alkaloid he named "nigerina" (nigerine) from the root bark of "jurema preta", that is, "Mimosa tenuiflora". However, in a careful review of the case Jonathan Ott shows that the empirical formula for nigerine determined by Gonçalves de Lima, which notably contains an atom of oxygen, can match only a partial, "impure" or "contaminated" form of DMT. It was only in 1959, when Gonçalves de Lima provided American chemists a sample of "Mimosa tenuiflora" roots, that DMT was unequivocally identified in this plant material. Less ambiguous is the case of isolation and formal identification of DMT in 1955 in seeds and pods of "Anadenanthera peregrina" by a team of American chemists led by Evan Horning (1916–1993). Since 1955, DMT has been found in a host of organisms: in at least fifty plant species belonging to ten families, and in at least four animal species, including one gorgonian and three mammalian species.
Another historical milestone is the discovery of DMT in plants frequently used by Amazonian natives as additive to the vine "Banisteriopsis caapi" to make ayahuasca decoctions. In 1957, American chemists Francis Hochstein and Anita Paradies identified DMT in an "aqueous extract" of leaves of a plant they named "Prestonia amazonicum" ("sic") and described as "commonly mixed" with "B. caapi". The lack of a proper botanical identification of "Prestonia amazonica" in this study led American ethnobotanist Richard Evans Schultes (1915–2001) and other scientists to raise serious doubts about the claimed plant identity. Better evidence was produced in 1965 by French pharmacologist Jacques Poisson, who isolated DMT as a sole alkaloid from leaves, provided and used by Aguaruna Indians, identified as having come from the vine "Diplopterys cabrerana" (then known as "Banisteriopsis rusbyana"). Published in 1970, the first identification of DMT in the plant "Psychotria viridis", another common additive of ayahuasca, was made by a team of American researchers led by pharmacologist Ara der Marderosian. Not only did they detect DMT in leaves of "P. viridis" obtained from Cashinahua Indians, but they also were the first to identify it in a sample of an ayahuasca decoction, prepared by the same Indians.
Biosynthesis.
Dimethyltryptamine is an indole alkaloid derived from the shikimate pathway. Its biosynthesis is relatively simple and summarized in the picture to the left. In plants, the parent amino acid L-tryptophan is produced endogenously where in animals L-tryptophan is an essential amino acid coming from diet. No matter the source of L-tryptophan, the biosynthesis begins with its decarboxylation by an aromatic amino acid decarboxylase (AADC) enzyme (step 1). The resulting decarboxylated tryptophan analog is tryptamine. Tryptamine then undergoes a transmethylation (step 2): the enzyme indolethylamine-N-methyltransferase (INMT) catalyzes the transfer of a methyl group from cofactor S-adenosyl-methionine (SAM), via nucleophilic attack, to tryptamine. This reaction transforms SAM into S-adenosylhomocysteine (SAH), and gives the intermediate product "N"-methyltryptamine (NMT). NMT is in turn transmethylated by the same process (step 3) to form the end product "N","N"-dimethyltryptamine. Tryptamine transmethylation is regulated by two products of the reaction: SAH, and DMT were shown "ex vivo" to be among the most potent inhibitors of rabbit INMT activity.
This transmethylation mechanism has been repeatedly and consistently proven by radiolabeling of SAM methyl group with carbon-14 (C-CH)SAM).
Evidence in mammals.
Published in "Science" in 1961, Julius Axelrod found an "N"-methyltransferase enzyme capable of mediating biotransformation of tryptamine into DMT in a rabbit's lung. This finding initiated a still ongoing scientific interest in endogenous DMT production in humans and other mammals. From then on, two major complementary lines of evidence have been investigated: localization and further characterization of the "N"-methyltransferase enzyme, and analytical studies looking for endogenously produced DMT in body fluids and tissues.
In 2013 researchers first reported DMT in the pineal gland microdialysate of rodents.
A study published in 2014 reported the biosynthesis of N,N-dimethyltryptamine (DMT) in the human melanoma cell line SK-Mel-147 including details on its metabolism by peroxidases.
In a 2014 paper a group first demonstrated the immunomodulatory potential of DMT and 5-MeO-DMT through the Sigma-1 receptor of human immune cells. This immunomodulatory activity may contribute to significant anti-inflammatory effects and tissue regeneration.
INMT.
Before techniques of molecular biology were used to localize indolethylamine N-methyltransferase (INMT), characterization and localization went on a par: samples of the biological material where INMT is hypothesized to be active are subject to enzyme assay. Those enzyme assays are performed either with a radiolabeled methyl donor like (C-CH)SAM to which known amounts of unlabeled substrates like tryptamine are added or with addition of a radiolabeled substrate like (C)NMT to demonstrate in vivo formation. As qualitative determination of the radioactively tagged product of the enzymatic reaction is sufficient to characterize INMT existence and activity (or lack of), analytical methods used in INMT assays are not required to be as sensitive as those needed to directly detect and quantify the minute amounts of endogenously formed DMT (see DMT subsection below). The essentially qualitative method thin layer chromatography (TLC) was thus used in a vast majority of studies. Also, robust evidence that INMT can catalyze transmethylation of tryptamine into NMT and DMT could be provided with reverse isotope dilution analysis coupled to mass spectrometry for rabbit and human lung during the early 1970s.
Selectivity rather than sensitivity proved to be an Achilles’ heel for some TLC methods with the discovery in 1974–1975 that incubating rat blood cells or brain tissue with (C-CH)SAM and NMT as substrate mostly yields tetrahydro-β-carboline derivatives, and negligible amounts of DMT in brain tissue. It is indeed simultaneously realized that the TLC methods used thus far in almost all published studies on INMT and DMT biosynthesis are incapable to resolve DMT from those tetrahydro-β-carbolines. These findings are a blow for all previous claims of evidence of INMT activity and DMT biosynthesis in avian and mammalian brain, including in vivo, as they all relied upon use of the problematic TLC methods: their validity is doubted in replication studies that make use of improved TLC methods, and fail to evidence DMT-producing INMT activity in rat and human brain tissues. Published in 1978, the last study attempting to evidence in vivo INMT activity and DMT production in brain (rat) with TLC methods finds biotransformation of radiolabeled tryptamine into DMT to be real but "insignificant". Capability of the method used in this latter study to resolve DMT from tetrahydro-β-carbolines is questioned later.
To localize INMT, a qualitative leap is accomplished with use of modern techniques of molecular biology, and of immunohistochemistry. In humans, a gene encoding INMT is determined to be located on chromosome 7. Northern blot analyses reveal INMT messenger RNA (mRNA) to be highly expressed in rabbit lung, and in human thyroid, adrenal gland, and lung. Intermediate levels of expression are found in human heart, skeletal muscle, trachea, stomach, small intestine, pancreas, testis, prostate, placenta, lymph node, and spinal cord. Low to very low levels of expression are noted in rabbit brain, and human thymus, liver, spleen, kidney, colon, ovary, and bone marrow. INMT mRNA expression is absent in human peripheral blood leukocytes, whole brain, and in tissue from 7 specific brain regions (thalamus, subthalamic nucleus, caudate nucleus, hippocampus, amygdala, substantia nigra, and corpus callosum). Immunohistochemistry showed INMT to be present in large amounts in glandular epithelial cells of small and large intestines. In 2011, immunohistochemistry revealed the presence of INMT in primate nervous tissue including retina, spinal cord motor neurons, and pineal gland.
Endogenous DMT.
The first claimed detection of mammalian endogenous DMT was published in June 1965: German researchers F. Franzen and H. Gross report to have evidenced and quantified DMT, along with its structural analog bufotenin (5-HO-DMT), in human blood and urine. In an article published four months later, the method used in their study was strongly criticized, and the credibility of their results challenged.
A Turkish patent was filled consisting of endogenous DMT production enhancing glucopyranosyl saponin derivatives capable of inducing up to 338% increase in DMT release; but has been strictly forbidden to manufacture, extract or obtain by the law. Inventors Can Alkoclar and Metehan Yesil initiated a multi faceted law struggle againist the ministry defensively stating that " the sole reason of developing related compounds with DMT increasing properties is to provide a permanent cure to violence and agression oriented behavioral Disorders" .
Few of the analytical methods used prior to 2001 to measure levels of endogenously formed DMT had enough sensitivity and selectivity to produce reliable results. Gas chromatography, preferably coupled to mass spectrometry (GC-MS), is considered a minimum requirement. A study published in 2005 implements the most sensitive and selective method ever used to measure endogenous DMT: liquid chromatography-tandem mass spectrometry with electrospray ionization (LC-ESI-MS/MS) allows for reaching limits of detection (LODs) 12 to 200 fold lower than those attained by the best methods employed in the 1970s. The data summarized in the table below are from studies conforming to the abovementioned requirements (abbreviations used: CSF = cerebrospinal fluid; LOD = limit of detection; n = number of samples; ng/L and ng/kg = nanograms (10 g) per litre, and nanograms per kilogram, respectively):
A 2013 study found DMT in microdialysate obtained from a rat's pineal gland, providing evidence of endogenous DMT in the mammalian brain.
Physical and chemical properties.
DMT is commonly handled and stored as a fumarate, as other DMT acid salts are extremely hygroscopic and will not readily crystallize. Its freebase form, although less stable than DMT fumarate, is favored by recreational users choosing to vaporize the chemical as it has a lower boiling point. In contrast to DMT's base, its salts are water-soluble. DMT in solution degrades relatively quickly and should be stored protected from air, light, and heat in a freezer.
As distinguished from 5-MeO-DMT.
5-MeO-DMT, a psychedelic drug structurally similar to "N","N"-DMT, is sometimes referred to as DMT through abbreviation. As a white, crystalline solid, it is also similar in appearance to DMT. However, it is considerably more potent (5-MeO-DMT typical vaporized dose: 5–20 mg), and care should be taken to clearly differentiate between the two drugs to avoid accidental overdose.
Pharmacology.
Pharmacokinetics.
DMT peak level concentrations ("C") measured in whole blood after intramuscular (IM) injection (0.7 mg/kg, n = 11) and in plasma following intravenous (IV) administration (0.4 mg/kg, n = 10) of fully psychedelic doses are in the range of ≈14 to 154 μg/L and 32 to 204 μg/L, respectively.
The corresponding molar concentrations of DMT are therefore in the range of 0.074–0.818 µM in whole blood and 0.170–1.08 µM in plasma. However, several studies have described active transport and accumulation of DMT into rat and dog brain following peripheral administration.
Similar active transport, and accumulation processes likely occur in human brain and may concentrate DMT in brain by several-fold or more (relatively to blood), resulting in local concentrations in the micromolar or higher range. Such concentrations would be commensurate with serotonin brain tissue concentrations, which have been consistently determined to be in the 1.5-4 μM range.
Closely coextending with peak psychedelic effects, mean time to reach peak concentrations ("T") was determined to be 10–15 minutes in whole blood after IM injection, and 2 minutes in plasma after IV administration. When taken orally mixed in an ayahuasca decoction, and in freeze-dried ayahuasca gel caps, DMT "T" is considerably delayed: 107.59 ± 32.5 minutes, and 90–120 minutes, respectively.
The pharmacokinetics for vaporizing DMT have not been studied or reported.
Pharmacodynamics.
DMT binds non-selectively with affinities < 0.6 μM to the following serotonin receptors: 5-HT, 5-HT, 5-HT, 5-HT, 5-HT, 5-HT, 5-HT, and 5-HT. An agonist action has been determined at 5-HT, 5-HT and 5-HT. Its efficacies at other serotonin receptors remain to be determined. Of special interest will be the determination of its efficacy at human 5-HT receptor as two "in vitro" assays evidenced DMT's high affinity for this receptor: 0.108 μM and 0.184 μM. This may be of importance because chronic or frequent uses of serotonergic drugs showing preferential high affinity and clear agonism at 5-HT receptor have been causally linked to valvular heart disease.
It has also been shown to possess affinity for the dopamine D, α-adrenergic, α-adrenergic, imidazoline-1, and sigma-1 (σ) receptors. Converging lines of evidence established activation of the σ receptor at concentrations of 50–100 μM. Its efficacies at the other receptor binding sites are unclear. It has also been shown "in vitro" to be a substrate for the cell-surface serotonin transporter (SERT) and the intracellular vesicular monoamine transporter 2 (VMAT2), inhibiting SERT-mediated serotonin uptake in human platelets at an average concentration of 4.00 ± 0.70 μM and VMAT2-mediated serotonin uptake in vesicles (of army worm Sf9 cells) expressing rat VMAT2 at an average concentration of 93 ± 6.8 μM.
As with other so-called "classical hallucinogens", a large part of DMT psychedelic effects can be attributed to a functionally selective activation of the 5-HT receptor. DMT concentrations eliciting 50% of its maximal effect (half maximal effective concentration = EC or K) at the human 5-HT receptor "in vitro" are in the 0.118–0.983 μM range. This range of values coincides well with the range of concentrations measured in blood and plasma after administration of a fully psychedelic dose (see Pharmacokinetics).
As DMT has been shown to have slightly better efficacy (EC) at human serotonin 2C receptor than at the 2A receptor, 5-HT is also likely implicated in DMT's overall effects. Other receptors, such as 5-HT σ, may also play a role.
In 2009, it was hypothesized that DMT may be an endogenous ligand for the σ receptor. The concentration of DMT needed for σ activation "in vitro" (50–100 μM) is similar to the behaviorally active concentration measured in mouse brain of approximately 106 μM This is minimally 4 orders of magnitude higher than the average concentrations measured in rat brain tissue or human plasma under basal conditions (see Endogenous DMT), so σ receptors are likely to be activated only under conditions of high local DMT concentrations. If DMT is stored in synaptic vesicles, such concentrations might occur during vesicular release. To illustrate, while the "average" concentration of serotonin in brain tissue is in the 1.5-4 μM range, the concentration of serotonin in synaptic vesicles was measured at 270 mM. Following vesicular release, the resulting concentration of serotonin in the synaptic cleft, to which serotonin receptors are exposed, is estimated to be about 300 μM. Thus, while "in vitro" receptor binding affinities, efficacies, and average concentrations in tissue or plasma are useful, they are not likely to predict DMT concentrations in the vesicles or at synaptic or intracellular receptors. Under these conditions, notions of receptor selectivity are moot, and it seems probable that most of the receptors identified as targets for DMT (see above) participate in producing its psychedelic effects.
As a psychedelic.
DMT is produced in many species of plants often in conjunction with its close chemical relatives 5-MeO-DMT and bufotenin (5-OH-DMT). DMT-containing plants are commonly used in South American shamanic practices. It is usually one of the main active constituents of the drink ayahuasca; however, ayahuasca is sometimes brewed with plants that do not produce DMT. It occurs as the primary psychoactive alkaloid in several plants including "Mimosa tenuiflora", "Diplopterys cabrerana", and "Psychotria viridis". DMT is found as a minor alkaloid in snuff made from Virola bark resin in which 5-MeO-DMT is the main active alkaloid. DMT is also found as a minor alkaloid in bark, pods, and beans of "Anadenanthera peregrina" and "Anadenanthera colubrina" used to make Yopo and Vilca snuff in which bufotenin is the main active alkaloid. Psilocin, an active chemical in many psychedelic mushrooms, is structurally similar to DMT.
The psychotropic effects of DMT were first studied scientifically by the Hungarian chemist and psychologist Dr. Stephen Szára, who performed research with volunteers in the mid-1950s. Szára, who later worked for the US National Institutes of Health, had turned his attention to DMT after his order for LSD from the Swiss company Sandoz Laboratories was rejected on the grounds that the powerful psychotropic could be dangerous in the hands of a communist country.
DMT can produce powerful psychedelic experiences including intense visuals, euphoria and hallucinations. DMT is generally not active orally unless it is combined with a monoamine oxidase inhibitor (MAOI) such as a reversible inhibitor of monoamine oxidase A (RIMA), for example, harmaline. Without an MAOI, the body quickly metabolizes orally administered DMT, and it therefore has no hallucinogenic effect unless the dose exceeds monoamine oxidase's metabolic capacity. Other means of ingestion such as vaporizing, injecting, or insufflating the drug can produce powerful hallucinations for a short time (usually less than half an hour), as the DMT reaches the brain before it can be metabolized by the body's natural monoamine oxidase. Taking a MAOI prior to vaporizing or injecting DMT prolongs and potentiates the effects.
"Machine Elves".
One common feature of the hallucinogenic experience caused by DMT are hallucinations of humanoid beings, characterised as being otherworldly. The term "Machine Elf" was coined by ethnobotanist Terence McKenna for the experience, who also used the terms "fractal elves", or "self-transforming machine elves".
Hallucinations of strange creatures had been reported by Szara in the "Journal of Mental Science" (now the British Journal of Psychiatry) (1958) "“Dimethyltryptamine Experiments with Psychotics”", Stephen Szara described how one of his subjects under the influence of DMT had experienced “strange creatures, dwarves or something” at the beginning of a DMT trip.
Other researchers of the experience described 'entities' or 'beings' in humanoid as well as animal form, with descriptions of "little people" being common (non-human gnomes, elves, imps etc.). This form of hallucination has been speculated to be the cause of alien abduction experiences through endogenously occurring DMT.
Cliff Pickover has also written about the "machine elf"-experience, in the book "Sex, Drugs, Einstein, & Elves".
Routes of administration.
Inhalation.
A standard dose for vaporized DMT is 15–60 mg. In general, this is inhaled in a few successive breaths. The effects last for a short period of time, usually 5 to 15 minutes, dependent on the dose. The onset after inhalation is very fast (less than 45 seconds) and peak effects are reached within a minute. In the 1960s, DMT was known as a "businessman's trip" in the US because of the relatively short duration (and rapid onset) of action when inhaled.
Injection.
Injected DMT produces an experience that is similar to inhalation in duration, intensity, and characteristics.
In a study conducted from 1990 through 1995, University of New Mexico psychiatrist Rick Strassman found that some volunteers injected with high doses of DMT reported experiences with perceived alien entities. Usually, the reported entities were experienced as the inhabitants of a perceived independent reality the subjects reported visiting while under the influence of DMT. In a September 2009 interview with Examiner.com, Strassman described the effects on participants in the study: "Subjectively, the most interesting results were that high doses of DMT seemed to allow the consciousness of our volunteers to enter into non-corporeal, free-standing, independent realms of existence inhabited by beings of light who oftentimes were expecting the volunteers, and with whom the volunteers interacted. While 'typical' near-death and mystical states occurred, they were relatively rare."
Oral ingestion.
DMT is broken down by the enzyme monoamine oxidase through a process called deamination, and is quickly inactivated orally unless combined with a monoamine oxidase inhibitor (MAOI). The traditional South American beverage ayahuasca, or yage, is derived by boiling the ayahuasca vine ("Banisteriopsis caapi") with leaves of one or more plants containing DMT, such as "Psychotria viridis", "Psychotria carthagenensis", or "Diplopterys cabrerana". The Ayahuasca vine contains harmala alkaloids, highly active reversible inihibitors of monoamine oxidase A (RIMAs), rendering the DMT orally active by protecting it from deamination. A variety of different recipes are used to make the brew depending on the purpose of the ayahuasca session, or local availability of ingredients. Two common sources of DMT in the western US are reed canary grass ("Phalaris arundinacea") and Harding grass ("Phalaris aquatica"). These invasive grasses contain low levels of DMT and other alkaloids. In addition, Jurema ("Mimosa tenuiflora") shows evidence of DMT content: the pink layer in the inner rootbark of this small tree contains a high concentration of "N,N"-DMT.
Taken orally with an RIMA, DMT produces a long lasting (over 3 hour), slow, deep metaphysical experience similar to that of psilocybin mushrooms, but more intense. RIMAs should be used with caution as they can have lethal interactions with some prescription drugs such as SSRI antidepressants, and some over-the-counter drugs.
Induced DMT experiences can include profound time-dilation, visual and auditory illusions, and other experiences that, by most firsthand accounts, defy verbal or visual description. Some users report intense erotic imagery and sensations and utilize the drug in a ritual sexual context.
Detection in body fluids.
DMT may be measured in blood, plasma or urine using chromatographic techniques as a diagnostic tool in clinical poisoning situations or to aid in the medicolegal investigation of suspicious deaths. In general, blood or plasma DMT levels in recreational users of the drug are in the 10–30 μg/L range during the first several hours post-ingestion. Less than 0.1% of an oral dose is eliminated unchanged in the 24-hour urine of humans.
Effects.
Addictive potential.
A review of studies on ritual users of the DMT-containing brew Ayahuasca concluded that: "A decoction of DMT and harmala alkaloids used in religious ceremonies has a safety margin comparable to codeine, mescaline or methadone. The dependence potential of oral DMT and the risk of sustained psychological disturbance are minimal." 
Physical.
According to a "Dose-response study of "N,N"-dimethyltryptamine in humans" by Rick Strassman, "Dimethyltryptamine dose slightly elevated blood pressure, heart rate, pupil diameter, and rectal temperature, in addition to elevating blood concentrations of beta-endorphin, corticotropin, cortisol, and prolactin. Growth hormone blood levels rose equally in response to all doses of DMT, and melatonin levels were unaffected."
Conjecture.
Several speculative and yet untested hypotheses suggest that endogenous DMT is produced in the human brain and is involved in certain psychological and neurological states. DMT is naturally occurring in small amounts in rat brain, human cerebrospinal fluid, and other tissues of humans and other mammals. A biochemical mechanism for this was proposed by the medical researcher J. C. Callaway, who suggested in 1988 that DMT might be connected with visual dream phenomena: brain DMT levels would be periodically elevated to induce visual dreaming and possibly other natural states of mind. A role of endogenous hallucinogens including DMT in higher level sensory processing and awareness was proposed by J. V. Wallach based on a hypothetical role of DMT as a neurotransmitter.
Dr. Rick Strassman, while conducting DMT research in the 1990s at the University of New Mexico, advanced the controversial hypothesis that a massive release of DMT from the pineal gland prior to death or near death was the cause of the near death experience (NDE) phenomenon. Several of his test subjects reported audio or visual hallucinations. His explanation for this was the possible lack of panic involved in the clinical setting and possible dosage differences between those administered and those encountered in actual NDE cases. Several subjects also reported contact with "other beings", alien like, insectoid or reptilian in nature, in highly advanced technological environments where the subjects were "carried", "probed", "tested", "manipulated", "dismembered", "taught", "loved" and "raped" by these "beings". Basing his reasoning on his belief that all the enzymatic material needed to produce DMT is found in the pineal gland, and moreover in substantially greater concentrations than in any other part of the body, Strassman has speculated that DMT is made in the pineal gland( p. 69) .
In the 1950s, the endogenous production of psychoactive agents was considered to be a potential explanation for the hallucinatory symptoms of some psychiatric diseases; this is known as the transmethylation hypothesis.
In 2011, Nicholas V. Cozzi, of the University of Wisconsin School of Medicine and Public Health, concluded that INMT, an enzyme that may be associated with the biosynthesis of DMT and endogenous hallucinogens, is present in the primate (rhesus macaque) pineal gland, retinal ganglion neurons, and spinal cord.
Legal status.
International law.
DMT is classified as a Schedule I drug under the UN 1971 Convention on Psychotropic Substances, meaning that use of DMT is supposed to be restricted to scientific research and medical use and international trade in DMT is supposed to be closely monitored. Natural materials containing DMT, including ayahuasca, are explicitly not regulated under the 1971 Psychotropic Convention.
By country.
Australia.
Between 2011 and 2012, the Australian Federal Government was considering changes to the Australian Criminal Code that would classify any plants containing any amount of DMT as "controlled plants". DMT itself was already controlled under current laws. The proposed changes included other similar blanket bans for other substances, such as a ban on any and all plants containing Mescaline or Ephedrine. The proposal was not pursued after political embarrassment on realisation that this would make the official Floral Emblem of Australia, Acacia pycnantha (Golden Wattle), illegal. The Therapeutic Goods Administration and federal authority had considered a motion to ban the same, but this was withdrawn in May 2012 (as DMT may still hold potential entheogenic value to native and/or religious people).
DMT is listed as a Schedule 9 prohibited substance in Australia under the Poisons Standard (October 2015). A schedule 9 drug is outlined in the Poisons Act 1964 as "Substances which may be abused or misused, the manufacture, possession, sale or use of which should be prohibited by law except when required for medical or scientific research, or for analytical, teaching or training purposes with approval of the CEO." 
Under the Misuse of Drugs act 1981 6.0g of DMT is considered enough to determine a court of trial and 2.0g is considered intent to sell and supply.
Canada.
DMT is classified in Canada as a Schedule III drug under the Controlled Drugs and Substances Act.
France.
DMT, along with most of its plant sources, is classified in France as a "stupéfiant" (narcotic).
New Zealand.
DMT is classified in New Zealand as a Class A drug under the Misuse of Drugs Act 1975.
United Kingdom.
DMT is classified in the United Kingdom as a Class A drug.
United States.
DMT is classified in the United States as a Schedule I drug under the Controlled Substances Act of 1970.
In December 2004, the Supreme Court lifted a stay, thereby allowing the Brazil-based União do Vegetal (UDV) church to use a decoction containing DMT in their Christmas services that year. This decoction is a tea made from boiled leaves and vines, known as hoasca within the UDV, and ayahuasca in different cultures. In "Gonzales v. O Centro Espirita Beneficente Uniao do Vegetal", the Supreme Court heard arguments on November 1, 2005, and unanimously ruled in February 2006 that the U.S. federal government must allow the UDV to import and consume the tea for religious ceremonies under the 1993 Religious Freedom Restoration Act.
In September 2008, the three Santo Daime churches filed suit in federal court to gain legal status to import DMT-containing ayahuasca tea. The case, "Church of the Holy Light of the Queen v. Mukasey", presided over by Judge Owen M. Panner, was ruled in favor of the Santo Daime church. As of March 21, 2009, a federal judge says members of the church in Ashland can import, distribute and brew ayahuasca. U.S. District Judge Owen Panner issued a permanent injunction barring the government from prohibiting or penalizing the sacramental use of "Daime tea". Panner's order said activities of The Church of the Holy Light of the Queen are legal and protected under freedom of religion. His order prohibits the federal government from interfering with and prosecuting church members who follow a list of regulations set out in his order.

</doc>
<doc id="8750" url="https://en.wikipedia.org/wiki?curid=8750" title="Da capo">
Da capo

Da capo, , is a musical term in Italian, meaning "from the beginning" (literally "from the head"). It is often abbreviated D.C. It is a composer or publisher's directive to repeat the previous part of music, often used to save space. In small pieces this might be the same thing as a repeat, but in larger works D.C. might occur after one or more repeats of small sections, indicating a return to the very beginning. The resulting structure of the piece is generally in ternary form. Sometimes the composer describes the part to be repeated, for example: "Menuet da capo". In opera, where an aria of this structure is called a da capo aria, the repeated section is often adorned with grace notes.
Variations of the direction are:
D.C. al Coda is a musical direction used in sheet music. It literally means, "da Capo al Coda," or "from the head to the tail". It directs the musician to go back and repeat the music from the beginning ("Capo"), and to continue playing until one reaches the first coda symbol. Upon reaching the first coda, one is to skip to the second coda symbol (which signifies the ending of the piece), and continue playing until the end. The portion of the piece from the second coda to the end is often referred to as the "coda" of the piece, or quite literally as the "end".

</doc>
<doc id="8751" url="https://en.wikipedia.org/wiki?curid=8751" title="Dominatrix">
Dominatrix

A dominatrix (plural "dominatrixes" or "dominatrices") or mistress is a woman who takes the dominant role in BDSM activities.
A dominatrix might be heterosexual, homosexual, or bisexual, but her orientation does not necessarily limit the genders of her submissive partners. The role of a dominatrix may not even involve physical pain toward the submissive; her domination can be verbal, involving humiliating tasks, or servitude. A dominatrix may be a paid professional ("pro-domme"), or may use the title of dominatrix in her personal sex life.
The term "domme" is a coined pseudo-French female variation of the slang "dom" (short for "dominant"). The use of "domme", "dominatrix", "dom", or "dominant" by any woman in a dominant role is chosen mostly by personal preference and the conventions of the local BDSM scene.
As fetish culture is increasingly becoming more prevalent in Western media, depictions of dominatrices in film and television have become more common.
Etymology.
"Dominatrix" is the feminine form of the Latin "dominator", a ruler or lord, and was originally used in a non-sexual sense. Its use in English dates back to at least 1561. Its earliest recorded use in the prevalent modern sense, as a female dominant in S&M, dates to 1967. It was initially coined to describe a woman who provides punishment-for-pay as one of the case studies within Bruce Roger's pulp paperback "The Bizarre Lovemakers". The term was taken up shortly after by the Myron Kosloff title "Dominatrix" (with art by Eric Stanton) in 1968, and entered more popular mainstream knowledge following the 1976 film "Dominatrix Without Mercy".
Although the term "dominatrix" was not used, the classic example in literature of the female dominant-male submissive relationship is portrayed in the 1870 novella "Venus in Furs" by Austrian writer Leopold von Sacher-Masoch. The term masochism was later derived from the author's name by Richard von Krafft-Ebing in the latter's 1886 forensic study "Psychopathia Sexualis".
History.
The history of the dominatrix is argued to date back to rituals of the Goddess Inanna (or Ishtar as she was known in Akkadian), in ancient Mesopotamia. Ancient cuneiform texts consisting of "Hymns to Inanna" have been cited as examples of the archetype of powerful, sexual female displaying dominating behaviors and forcing Gods and men into submission to her. Archaeologist and historian Anne O. Nomis notes that Inanna's rituals included cross-dressing of cult personnel, and rituals "imbued with pain and ecstasy, bringing about initiation and journeys of altered consciousness; punishment, moaning, ecstasy, lament and song, participants exhausting themselves with weeping and grief."
In the secular era, the profession appears to have originated as a specialization within brothels, before becoming its own unique craft. As far back as the 1590s, flagellation within an erotic setting is recorded. The profession features in erotic prints of the era, such as the British Museum mezzotint "The Cully Flaug'd" (c. 1674–1702), and in accounts of forbidden books which record the flogging schools and the activities practised.
Within the 18th Century, female "Birch Disciplinarians" advertised their services in a book masked as a collection of lectures or theatrical plays, entitled "Fashionable Lectures" (c1761). This included the names of 57 women, some actresses and courtesans, who catered to birch discipline fantasies, keeping a room with rods and cat o' nine tails, and charging their clients a Guinea for a "lecture".
The 19th Century is characterised by what historian Anne O. Nomis characterises as the "Golden Age of the Governess". No fewer than twenty establishments were documented as having existed by the 1840s, supported entirely by flagellation practices and known as "Houses of Discipline" distinct from brothels. Amongst the well-known "dominatrix governesses" were Mrs Chalmers, Mrs Noyeau, the late Mrs Jones of Hertford Street and London Street, the late Mrs Theresa Berkley, Bessy Burgess of York Square and Mrs Pyree of Burton Cres. The most famous of these Governess "female flagellants" was Theresa Berkley, who operated her establishment on Charlotte Street in the central London district of Marylebone. She is recorded to have used implements such as whips, canes and birches, to chastise and punish her male clients, as well as the Berkley Horse, a specially designed flogging machine, and a pulley suspension system for lifting them off the floor. Such historical use of corporal punishment and suspension, in a setting of domination roleplay, connects very closely to the practices of modern-day professional dominatrices.
The "bizarre style" (as it came to be called) of leather catsuits, claws, tail whips, and latex rubber only came about in the 20th Century, initially within commercial fetish photography, and taken up by dominatrices. Within the mid-20th Century, dominatrices operated in a very discreet and underground manner, which has made them difficult to trace within the historical record. A few photographs still exist of the women who rang their domination businesses in London, New York, The Hague and the Herbertstraße, predominantly in sepia and black-and-white photographs, and scans from magazine articles, copied and re-copied. Amongst these were Miss Doreen of London who was acquainted with John Sutcliffe of AtomAge fame, whose clients reportedly included Britain's top politicians and businessmen. In New York, the dominatrix Anne Laurence was known within the underground circle of acquaintances during the 1950s, with Monique Von Cleef arriving in the early 1960s, and hitting national headlines when her home was raided by police detectives on December 22, 1965. Von Cleef went on to set up her "House of Pain" in The Hague in the 1970s, which became one of the world capitals for dominatrices, reportedly with visiting lawyers, ambassadors, diplomats and politicians.
Professional "vs." personal.
The term "dominatrix" is sometimes used to describe a professional dominant (or "pro-domme") who is paid to engage in BDSM with a submissive. An appointment or roleplay is referred to as a "session", and is often conducted in a dedicated professional play space which has been set up with specialist equipment, such as a "dungeon". In the contemporary era of technological connectivity, sessions may also be conducted remotely by phone, email or online chat.
Women who engage in female domination typically promote and title themselves under the terms "dominatrix", "mistress", "lady", "madame", "herrin" or "goddess". A study of German dominatrices by Andrew Wilson has noted the trend for dominatrices choosing names aimed at creating and maintaining an atmosphere in which class, femininity and mystery are key elements of their self-constructed identity.
Professional dominatrices may or may not offer sexual intercourse and other intimate sexual activities as part of their service to clients. The Canadian dominatrix Terri-Jean Bedford, who was one of three women who initiated an application in the Ontario Superior Court seeking invalidation of Canada's laws regarding brothels, sought to differentiate for clarity her occupation as a dominatrix rather than a prostitute to the media, due to frequent misunderstanding and conflation by the public of the two terms.
While dominatrices come from many different backgrounds, it has been noted that a considerable number are very well-educated, with a recent survey of New York dominatrices revealing that 39% had attended graduate school / university, including well-regarded institutions such as Columbia University.
Professional dominatrices can be seen advertising their services online and in print publications which carry erotic services advertising. The precise number of women actively offering professional domination services is unknown. Most professional dominatrices practice in large metropolitan cities such as New York, Los Angeles, and London, with as many as 200 women working as dominatrices in Los Angeles.
Professional dominatrices may take pride or differentiation in their psychological insight into their clients' fetishes and desires, as well as their technical ability to perform complex BDSM practices, such as Japanese shibari and other forms of bondage, suspension, torture roleplay, and corporal punishment, and other such practices which require a high degree of knowledge and competency to safely oversee. From a sociological point of view, Danielle Lindemann has noted the "embattled purity regime" in which many Pro-Dommes emphasise their specialist knowledge and professional skills, while distancing themselves from economic criteria for success, in a way which is comparable to avant-garde artists. 
To differentiate women who identify as a dominatrix but do not offer paid services, non-professional dominants are occasionally referred to as a "lifestyle" dominatrix or Mistress. It should be noted that the term "lifestyle" to signify BDSM is occasionally a contention topic in the BDSM community and that some dominatrices may dislike the term. Some professional dominatrices are also "lifestyle" dominatrices - i.e., in addition to paid sessions with submissive clients they engage in unpaid recreational sessions or may incorporate power exchange within their own private lives and relationships. However it is worth noting that the term has fallen out of general usage with respect to women who are dominant in their private relationships, and has taken on more and more, the connotation of "professional."
Imagery.
The dominatrix is a female archetype which operates on a symbolic mode of representation, associated with particular attire and props that are drawn on within popular culture to signify her role—as a strong, dominant, sexualised woman—linked to but distinct from images of sexual fetish.
One of the ubiquitous garments associated with the dominatrix is the catsuit. Historically, the black leather female catsuit entered dominant fetish culture in the 1950s with the "AtomAge" magazine and its connections to fetish fashion designer John Sutcliffe. The spill-over into mainstream culture, occurred with catsuits being worn by strong female protagonists in popular 1960s TV programs like "The Avengers", and in the comic super-heroines such as Catwoman, in which the catsuit represented the independent woman capable of "kick-ass" moves and antics, enabling complete freedom of movement. On another level, the one-piece catsuit accentuated and exaggerated the sexualized female form, providing visual access to a woman's body, while simultaneously obstructing physical penetrative access. "You can look but you can't touch" is the mechanism of this operation, which plays upon the BDSM practice known as "tease and denial".
Other common signifying footwear of the dominatrix are thigh-high boots, in leather or shiny PVC, which have long held a fetishistic status, along with the very high stiletto heel. Fishnet stockings, seamed hosiery, suspender belts and garter stockings are also popular accents in the representation and attire of dominatrices, to emphasize the form and length of their legs, with erotic connotation.
Tight, leather corsets are another staple garment of the dominatrix signification. Gloves, whether long opera gloves or fingerless gloves, are often a further accessory to emphasize the feminine role.
Materials such as PVC, leather and rubber latex, are amongst the most common to immediately take on the signifying work of fetish attire. The body language of the dominatrix is frequently represented by the use of strong, dominant body-language which is comparable to the dominant posturing in the animal world. The props she may brandish will strongly signify her role as dominatrix, such as bearing a flogger whip or riding crop as illustrated in the artwork of Bruno Zach in the early 20th century, in conventional representation.
Practicing professional dominatrices may draw their attire from the conventional signifiers of the role, or adapt them to create their own individual style, where there exists a potential pull—between meeting conventional expectations, and a desire for dominant independent self-expression.
Some contemporary dominatrices draw upon an eclectic range of strong female archetypes, including the goddess, the female superheroine, the femme fatale, the priestess, the empress, the queen, the governess, the KGB secret agent, to their own ends.

</doc>
<doc id="8752" url="https://en.wikipedia.org/wiki?curid=8752" title="Flag of Denmark">
Flag of Denmark

The Flag of Denmark ( ) is red with a white Scandinavian cross that extends to the edges of the flag; the vertical part of the cross is shifted to the hoist side. The cross design, which represents Christianity, was subsequently adopted by other Nordic countries: Sweden, Norway, Finland, Iceland, and the Faroe Islands, as well as the British archipelagos of Shetland and Orkney. During the Danish-Norwegian personal union, Dannebrog ("Danish cloth") was also the flag of Norway and continued to be, with slight modifications, until Norway adopted its current flag in 1821.
The design of the Dannebrog is recorded on a seal from 1397. According to legend, the flag came into Danish possession during the Battle of Lyndanisse in 1219. The Danes were on a failing crusade in Estonia, but after praying to God a flag fell from the sky. After this event, Danish King Valdemar II went on to defeat the Estonians. The first recorded use of the flag appears one hundred years later.
Legendary origin.
The legend states the origin of the flag to the Battle of Lyndanisse, also known as the Battle of Valdemar (Danish: "Volmerslaget"), near Lyndanisse (Tallinn) in Estonia, on June 15, 1219.
The battle was going badly, and defeat seemed imminent. However the Danish Bishop Anders Sunesen on top of a hill overlooking the battle prayed to God with his arms raised, which meant that the Danes moved closer to victory the more he prayed. When he raised his arms the Danes surged forward and when his arms grew tired and he let them fall, the Estonians turned the Danes back. Attendants rushed forward to raise his arms once again and the Danes surged forward again. At a second he was so tired in his arms that he dropped them and the Danes then lost the advantage and were moving closer to defeat. He needed two soldiers to keep his hands up and when the Danes were about to lose, 'Dannebrog' miraculously fell from the sky and the King took it, showed it to the troops and their hearts were filled with courage and the Danes won the battle.
No historical record supports this legend. The first record of the legend dates from more than 300 years after the campaign, and the first record connects the legend to a much smaller battle, though still in Estonia; the battle of Fellin (Viljandi) in 1208. Though no historical support exists for the flag story in the Fellin battle either, it is not difficult to understand how a small and unknown place is replaced with the much grander battle of Reval (Tallinn) from the Estonia campaign of King Valdemar II.
This story originates from two written sources from the early 16th century.
The first is found in Christiern Pedersen's ""Danske Krønike", which is a sequel to Saxo’s Gesta Danorum, written 1520–23. It is not mentioned in connection to the campaign of King Valdemar II in Estonia, but in connection with a campaign in Russia. He also mentions that this flag, falling from the sky during the Russian campaign of King Valdemar II, is the very same flag that King Eric of Pomerania took with him when he left the country in 1440 after being deposed as King.
The second source is the writing of the Franciscan monk Petrus Olai (Peder Olsen) of Roskilde, from 1527. This record describes a battle in 1208 near a place called "Felin" during the Estonia campaign of King Valdemar II. The Danes were all but defeated when a lamb-skin banner depicting a white cross falls from the sky and miraculously leads to a Danish victory. In another record by "Petrus Olai" called "Danmarks Tolv Herligheder"" (Twelve Splendours of Denmark), in splendour number nine, the same story is re-told almost to the word; however, a paragraph has been inserted correcting the year to 1219.
Some historians believe that the story by "Petrus Olai" refers to a source from the first half of the 15th century, making this the oldest reference to the falling flag.
It is believed that the name of the capital of Estonia, Tallinn, came into existence after the battle. It is derived from "Taani linn", meaning "Danish town" in Estonian.
Continuation of the romantic legend.
According to tradition, the original flag from the Battle of Lyndanisse was used in the small campaign of 1500 when King Hans tried to conquer Dithmarschen (in western Holstein in north Germany). The flag was lost in a devastating defeat at the Battle of Hemmingstedt on 17 February 1500. In 1559, King Frederik II recaptured it during his own Dithmarschen campaign. In the capitulation terms it is stated that all Danish banners lost in 1500 were to be returned.
This legend is found in two sources, Hans Svaning's "History of King Hans" from 1558–1559 and Johan Rantzau's "History about the Last Dithmarschen War", from 1569.
Both claim that this was the original flag, and consequently both writers knew the legend of the falling flag. In 1576, the son of "Johan Rantzau", Henrik Rantzau, also writes about the war and the fate of the flag. He notes that the flag was in a poor condition when returned.
Sources from Dithmarschen, written shortly after the battle of 1500, do mention banners, including the Royal banner, being captured from the Danes, but there is no mention of "Dannebrog" or the "original" flag.
It is quite plausible that the king’s personal banner as well as the leading banner of the army were both lost, as the battle was led by the King himself. However, it is more questionable if he indeed was carrying the "original" flag.
In a letter dated 22 February 1500 to Oluf Stigsøn, King John describes the battle, but does not mention the loss of an important flag. In fact, the entire letter gives the impression that the lost battle was nothing more than an "unfortunate affair".
An indication that we are dealing with multiple flags, are the 1570 writings of Niels Hemmingsøn regarding a bloody battle between Danes and Swedes near the Swedish town of Uppsala in 1520. He writes that the "Danish head banner" (""Danmarckis Hoffuitbanner") was nearly captured by the Swedes. It was saved only by the combined efforts of the banner-carrier Mogens Gyldenstierne, taking multiple wounds, and a young man coming to his rescue. This young man was Peder Skram. This "Danmarckis Hoffuitbanner" was probably nothing short of the "Banner of the Realm'" ("Rigsbanner"), the "Dannebrog".
This is however not the end of the story. A priest and historian from Dithmarschen, Neocorus, wrote in 1598 that the banner captured in 1500, was brought to the church in Wöhrden and hung there for the next 59 years, until it was returned to the Danes as part of the peace settlement in 1559.
Henrik Rantzau states in his writing of 1576 that the flag was brought to Slesvig city and placed in the cathedral, following its return.
A historian from Slesvig, Ulrik Petersen (1656–1735), wrote in the late 17th century that the flag hung in "Slesvig" cathedral till about 1660 until it simply crumbled away, thus ending its more than 400-year-old story.
Historically, it is of course impossible to prove or disprove that these records speak of the same flag, or if the flag of 1208 or 1219 ever existed. Many of these legends are apparently built on earlier ones.
Historic use.
Caspar Paludan-Müller.
The Danish historian Caspar Paludan-Müller in 1873 in his book "Sagnet om den himmelfaldne Danebrogsfane" put forth the theory that it is a banner sent by the Pope to the Danish King to use in his crusades in the Baltic countries. Other kings and lords certainly received such banners.
One would imagine, though, that if this story were true, some kind of record ought to exist of the event, and presumably Danish historians would not have failed to mention it in some way. Being granted a banner by the Pope would have been a great honour, but despite the many letters of the popes relating to the crusades, none of them mentions granting a banner to a King of Denmark. On the other hand, the letter in question might simply have been lost.
Johan Støckel.
A similar theory was suggested by Danish explorer, adventurer and Captain Johan Støckel in the early 20th century. He suggested that it was not a papal banner to the King but a papal banner to the Churchly legate in the North, more specifically to archbishop Andreas Sunesøn, which he – without the knowledge of the King – brought with him on the King's crusade in the Baltic countries, in an effort to make the army take on a Christian symbol (over the king's symbol) and thereby strengthen the power of the church.
It is unlikely that the very fair and loyal archbishop would do such a thing behind the king's back. Moreover, it is unlikely that the pope would send such a banner, given the fact that they already had one, namely the banner of the Knights Hospitaller (Danish: "Johanitterne").
Adolf Ditlev Jørgensen.
A theory brought forth by the Danish historian Adolf Ditlev Jørgensen in 1875 in his book ""Danebroges Oprindelse" is that the Danish flag "is" the banner of the Knights Hospitaller. He notes that the order came to Denmark in the latter half of the 12th century and during the next centuries spread to major cities, like Odense, Viborg, Horsens, Ribe and their headquarters in Slagelse, so by the time of the Baltic crusade, the symbol was already a known symbol in Denmark.
Furthermore, he claims that Bishop Theoderich, already co-initiator of the Livonian Brothers of the Sword in Livonia, had the idea of starting a similar order in Estonia; and that he was the original instigator of the inquiry from Bishop Albert of Buxhoeveden to King Valdemar II in 1218, that set the whole Danish participation in the Baltic crusades in motion.
In the contemporary writing of the priest Henry of Livonia from Riga it is said that Bishop Theoderich was killed during the 1219 battle, when the enemy stormed his tent, thinking it was the King's tent. Adolf Ditlev Jørgensen explains that it was Bishop Theoderich who carried the flag, planted outside his tent; thus as an already well-known symbol of the Knights Hospitaller in Livonia, the enemy thought this was the King's symbol and mistakenly stormed Bishop Theoderich tent. He claims that the origin of the legend of the falling flag comes from this confusion in the battle.
L. P. Fabricius.
The Danish church-historian L. P. Fabricius proposed yet another theory, explained in his study of 1934, titled "Sagnet om Dannebrog og de ældste Forbindelser med Estland". He ascribes the origin to the 1208 Battle of Fellin, not the Battle of Lyndanisse in 1219, based on the earliest source available about the story.
He says in this theory that it might have been Archbishop Andreas Sunesøn's personal ecclesiastical banner or perhaps even the flag of Archbishop Absalon, based on his tireless efforts to expand Christianity to the Baltic countries. Under his initiative and supervision several smaller crusades had already been conducted in Estonia. The banner would then already be known in Estonia. He repeats the story about the flag being planted in front of Bishop Theodorik's tent, which the enemy mistakenly attacks believing it to be the tent of the King.
All these theories centre on two battles in Estonia, Fellin (1208) or Lyndanisse (1219), and thus try to explain the origin in relation to the tale brought forth over 300 years after the event.
Fabricius and Helga Bruhn.
A much different theory is briefly discussed by "Fabricius" and elaborated more by Helga Bruhn in her book "Dannebrog"" from 1949. She claims that it is neither the battle nor the banner that is central to the tale, but rather the cross in the sky. Similar tales of appearances in the sky at critical moments, particularly of crosses, can be found all over Europe.
Bruhn mentions a battle (also mentioned by "Fabricius") taking place on September 10, 1217 between Christian knights and Moor warriors on the Iberian Peninsula near the castle Alcazar, where it is said that a golden cross on white appeared in the sky, to bring victory to the Christians.
Likewise an almost identical Swedish tale from the 18th century about a golden cross on blue appearing in 1157 during a Swedish battle in Finland. Probably a later invention to counter the legendary origins of the Danish flags, but nevertheless of the same nature.
The English flag, the Saint George's Cross is also claimed to have appeared in the sky during a critical battle, in this case in Jerusalem during the crusades.
The similarities to the legends is obvious. In Spain, the colours of the Pope appears in the sky, in Finland the Swedish colours. In Estonia it is the Danish colours, and in Jerusalem the English colours. Basically, these are all variations of the same legend.
Since King Valdemar II of Denmark was married to the Portuguese princess, Berengária of Portugal, it is not unthinkable that the origin of the story, if not the flag, was the Spanish tale or a similar tale, which again might have been inspired by an even older legend.
Medieval use of the cross design in coats of arms.
Several coins, seals and images exist, both foreign and domestic, from the 13th to 15th centuries and even earlier, showing heraldic designs similar to Dannebrog, alongside the royal coat of arms (three blue lions on a golden shield.) This coat of arms remains in use to this day.
An obvious place to look for documentation is in the Estonian city of Tallinn, the site of the legendary battle. In Tallinn, a coat-of-arms resembling the flag is found on several buildings and can be traced back to the middle of the 15th century where it appears in the coat-of-arms of the "Die Grosse Gilde", a sort of merchant consortium which greatly influenced the city's development. The symbol later became the coat-of-arms of the city. Efforts to trace it from Estonia back to Denmark have, however, been in vain.
The national Coat of Arms of Estonia, three blue lions on a golden shield, is almost identical to the Coat of Arms of Denmark, and its origin can be traced directly back to King Valdemar II and Danish rule in Estonia 1219–1346.
Gelre armorial.
The earliest source that indisputably links the red flag with a white cross to a Danish King, and to the realm itself, is found in a Dutch armorial, the "Gelre Armorial" (Dutch: "Wapenboek Gelre"), written between 1340 and 1370 (some sources say 1378 or 1386). Most historians claim that the book was written by Geldre Claes Heinen. The book displays some 1,700 coats-of-arms from all over Europe, in colour. It is now located at the Royal Library of Brussels (the "Bibliothèque royale Albert Ier").
On page 55 verso we find the Danish coat-of-arms surmounted by a helmet with ermine-clad bison horns (). Behind the sinister horn is a lance tip with a banner, displaying a white cross on red. The text left of the coat of arms says "die coninc van denmarke" ("The King of Denmark"). This is the earliest known undisputed colour rendering of the Dannebrog.
This image has been used to acknowledge a previously disputed theory that the cross found in Valdemar Atterdag's coats of arms located in his Danælog seal ("Rettertingsseglet") from 1356 is indeed the cross from the Danish flag.
This image from the Armorial Gelre is nearly identical to an image found in a 15th-century coats of arms book now located in the National Archives of Sweden, ("Riksarkivet")
From the time of King Eric of Pomerania there is also a case that undisputedly links the Dannebrog to Denmark. His seal from 1398 as king of the Kalmar union displays the arms of Denmark chief dexter, three lions. In this version, the lions are holding a Danebrog banner. The cross quartering the shields has also been identified as a Dannebrog cross, but this claim is disputed. Since the seal represents all of the three realms of the union, it is more likely that the cross refers to the union banner that King Eric tried to introduce, a red cross on yellow.
Modern use.
As a civil ensign.
The size and shape of the civil ensign (""Koffardiflaget") for merchant ships is given in the regulation of June 11, 1748, which says: "A red flag with a white cross with no split end. The white cross must be of the flag's height. The two first fields must be square in form and the two outer fields must be lengths of those".
The proportions are thus: 3:1:3 vertically and 3:1:4.5 horizontally. This definition are the absolute proportions for the Danish national flag to this day, for both the civil version of the flag ("Stutflaget"), as well as the merchant flag ("Handelsflaget""). Both flags are identical.
A somewhat curious regulation came in 1758 concerning Danish ships sailing in the Mediterranean. These had to carry the King's
cypher logo in the center of the flag, to distinguish them from Maltese ships, due to the similarity of the flag of the Order of St. John (also known as the Knights Hospitaller). To the best of knowledge, this regulation has never been revoked, however it is probably no longer done.
According to the regulation of June 11, 1748 the colour was simply red, which is common known today as "Dannebrog rød" ("Dannebrog red"). The only available red fabric dye in 1748 was made of madder root, which can be processed to produce a brilliant red dye (used historically for British soldiers' jackets). The private company, Dansk Standard, regulation number 359 of 2005, defines the red colour of the flag as Pantone 186c. No official nuance definition of "Dannebrog rød" exists.
During the next about 150 years nobody paid much attention to actually abide fully to the proportions of the flag given in the 1748 regulation, not even the government.
As late as 1892 it was stated in a series of regulations that the correct lengths of the two last fields in the flag were . Some interested in the matter made inquires into the issue and concluded that the length would make the flag look blunt. Any new flag would also quickly become unlawful, due to wear and tear. They also noted that the flag currently used had lengths, of the last two fields, anywhere between to . So in May 1893 a new regulation to all chiefs of police, stated that the police should not intervene, if the two last fields in the flag were longer than as long as these did not exceed , and provided that this was the only rule violated. This regulation is still in effect today and thus the legal proportions of the National flag is today anywhere between "3:1:3 width / 3:1:4.5 length" and "3:1:3 width / 3:1:5.25 length".
That some confusion still exists in this matter can be seen from the regulation of May 4, 1927, which once again states that Danish merchant ships have to fly flags according to the regulation of 1748.
Variants.
Splitflag and Orlogsflag.
The "Splitflag" and "Orlogsflag" have similar shapes but different sizes and shades of red. Legally, they are two different flags. The "Splitflag" is a Danish flag ending in a swallow-tail, it is "Dannebrog red", and is used on land. The "Orlogsflag" is an elongated "Splitflag" with a deeper red colour and is only used on sea.
The "Orlogsflag" with no markings, may only be used by the Royal Danish Navy. There are though a few exceptions to this. A few institutions have been allowed to fly the clean "Orlogsflag". The same flag with markings has been approved for a few dozen companies and institutions over the years.
Furthermore, the "Orlogsflag" is only described as such if it has no additional markings. Any swallow-tail flag, no matter the color, is called a "Splitflag" provided it bears additional markings.
The first regulation regarding the "Splitflag" dates from March 27, 1630, in which King Christian IV orders that Norwegian "Defensionskibe" (armed merchants ships) may only use the "Splitflag" if they are in Danish war service.
In 1685 an order, distributed to a number of cities in Slesvig, states that all ships must carry the Danish flag, and in 1690 all merchant ships are forbidden to use the "Splitflag", with the exception of ships sailing in the East Indies, West Indies and along the coast of Africa.
In 1741 it is confirmed that the regulation of 1690 is still very much in effect; that merchant ships may not use the "Splitflag". At the same time the Danish East India Company is allowed to fly the "Splitflag" when past the equator.
It is obvious that some confusion must have existed regarding the "Splitflag". In 1696 the Admiralty presented the King with a proposal for a standard regulating both size and shape of the "Splitflag". In the same year a royal resolution defines the proportions of the "Splitflag", which in this resolution is called "Kongeflaget" (the King's flag), as follows: "The cross must be of the flags height. The two first fields must be square in form with the sides three times the cross width. The two outer fields are rectangular and the length of the square fields. The tails are the length of the flag".
These numbers are the basic for the "Splitflag", or "Orlogsflag", today, though the numbers have been slightly altered. The term "Orlogsflag" dates from 1806 and denotes use in the Danish Navy.
From about 1750 to early 19th century a number of ships and companies which the government has interests in, received approval to use the "Splitflag". From the mid-19th century to 1899, and especially after 1870, additional institutions and private companies received approval to use the Splitflag.
In the royal resolution of October 25, 1939 for the Danish Navy, it is stated that the "Orlogsflag" is a "Splitflag" with a deep red (""dybrød") or madder red ("kraprød"") colour. Like the National flag, no nuance is given, but in modern days this is given as 195U.
Furthermore, the size and shape is corrected in this resolution to be: "The cross must be of the flag's height. The two first fields must be square in form with the height of of the flag's height. The two outer fields are rectangular and the length of the square fields. The tails are the length of the rectangular fields".
Thus, if compared to the standard of 1696, both the rectangular fields and the tails have decreased in size.
The current version of the royal standard was introduced on 16 November 1972 when the Queen adopted a new version of her personal coat of arms. The royal standard is the flag of Denmark with a swallow-tail and charged with the monarch’s coat of arms set in a white square. The centre square is 32 parts in a flag with the ratio 56:107.
Superstitions.
Danish culture states that the Dannebrog is not allowed to touch the ground because it came from heaven. It also states that the Dannebrog is not allowed to be or remain hoisted at night, because it is said that such is to salute the Devil.
Other flags of the Kingdom of Denmark.
Greenland and the Faroe Islands are additional autonomous countries within the Kingdom of Denmark. These countries have their own official flags.
Regional Flags.
Some areas in Denmark have unofficial flags, listed below. The regional flags of Bornholm and Ærø are known to be in active use. The flags of Vendsyssel (Vendelbrog) and the Jutlandic flag ("Den jyske fane") are obscure. None of these flags have legal recognition in Denmark, and are officially considered to be "fantasy flags". Denmark reserves official recognition to official flags and regional flags ("områdeflag") from other jurisdictions.
The flag of Volyn Oblast, Ukraine is very similar to the Dannebrog.

</doc>
<doc id="8753" url="https://en.wikipedia.org/wiki?curid=8753" title="Dharma">
Dharma

Dharma (; "dharma", ; "dhamma") is a key concept with multiple meanings in the Indian religions Hinduism, Buddhism, Sikhism and Jainism. There is no single word translation for "dharma" in western languages.
In Hinduism, "dharma" signifies behaviours that are considered to be in accord with "rta", the order that makes life and universe possible, and includes duties, rights, laws, conduct, virtues and ‘‘right way of living’’. In Buddhism "dharma" means "cosmic law and order", but is also applied to the teachings of the Buddha. In Buddhist philosophy, "dhamma/dharma" is also the term for "phenomena". Dharma in Jainism refers to the teachings of "tirthankara" ("Jina") and the body of doctrine pertaining to the purification and moral transformation of human beings. For Sikhs, the word "dharm" means the "path of righteousness".
The Classical Sanskrit noun "dharma" is a derivation from the root "dhṛ", which has a meaning of "to hold, maintain, keep". The word "dharma" was already in use in the historical Vedic religion, and its meaning and conceptual scope has evolved over several millennia. The antonym of dharma is "adharma".
Etymology.
The Classical Sanskrit noun "dharma" is a derivation from the root "dhṛ", which means "to hold, maintain, keep", and takes a meaning of "what is established or firm", and hence "law". It is derived from an older Vedic Sanskrit "n"-stem "dharman-", with a literal meaning of "bearer, supporter", in a religious sense conceived as an aspect of Rta.
In the Rigveda, the word appears as an "n"-stem, ', with a range of meanings encompassing "something established or firm" (in the literal sense of prods or poles). Figuratively, it means "sustainer" and "supporter" (of deities). It is semantically similar to the Greek "ethos" ("fixed decree, statute, law"). In Classical Sanskrit, the noun becomes thematic: '.
The word "dharma" derives from Proto-Indo-European root "*dʰer-" ("to hold"), which in Sanskrit is reflected as class-1 root "√dhṛ". Etymologically it is related to Avestan √dar- ("to hold"), Latin "frēnum" ("rein, horse tack"), Lithuanian "derė́ti" ("to be suited, fit"), Lithuanian "dermė" ("agreement") and "darna" ("harmony") and Old Church Slavonic "drъžati" ("to hold, possess"). Classical Sanskrit word "dharmas" would formally match with Latin o-stem "firmus" from Proto-Indo-European *"dʰer-mo-s" "holding", were it not for its historical development from earlier Rigvedic n-stem.
In Classical Sanskrit, and in the Vedic Sanskrit of the Atharvaveda, the stem is thematic: "" (Devanāgarī: धर्म). In Pāli, it is rendered "dhamma". In some contemporary Indian languages and dialects it alternatively occurs as "dharm".
Definition.
Dharma is a concept of central importance in Indian philosophy and religion. It has multiple meanings in Hinduism, Buddhism, and Jainism. It is difficult to provide a single concise definition for "dharma", as the word has a long and varied history and straddles a complex set of meanings and interpretations. There is no equivalent single word translation for "dharma" in western languages.
There have been numerous, conflicting attempts to translate ancient Sanskrit literature with the word dharma into German, English and French. The concept, claims Paul Horsch, has caused exceptional difficulties for modern commentators and translators. For example, while Grassmann translation of Rig-veda identifies seven different meanings of dharma, Karl Friedrich Geldner in his translation of the Rig-veda employs 20 different translations for dharma, including meanings such as ‘law’, ‘order’, ‘duty’, ‘custom’, ‘quality’, ‘model’, among others.
Dharma root is "dhri", which means ‘to support, hold, or bear’. It is the thing that regulates the course of change by not participating in change, but that principle which remains constant. Monier-Williams, the widely cited resource for definitions and explanation of Sanskrit words and concepts of Hinduism, offers numerous definitions of the word "dharma": such as that which is established or firm, steadfast decree, statute, law, practice, custom, duty, right, justice, virtue, morality, ethics, religion, religious merit, good works, nature, character, quality, property. Yet, each of these definitions is incomplete, while combination of these translations do not convey the total sense of the word. In common parlance, "dharma" means ‘right way of living’ and ‘path of righteousness’.
The meaning of word “dharma” depends on the context, and its meaning evolved as ideas of Hinduism developed over its long history. In earliest texts and ancient myths of Hinduism, "dharma" meant cosmic law, the rules that created the universe from chaos, as well as rituals; In later Vedas, Upanishads, Puranas and the Epics, the meaning became refined, richer, complex and the word dharma was applied to diverse contexts.
In certain contexts, "dharma" designates human behaviours considered necessary for order of things in the universe, principles that prevent chaos, behaviours and action necessary to all life in nature, society, family as well as at the individual level. "Dharma" encompasses ideas such as duty, rights, character, vocation, religion, customs and all behaviour considered appropriate, correct or morally upright.
The antonym of "dharma" is "adharma" (Sanskrit: अधर्मा), meaning that which is “not dharma”. As with "dharma", the word "adharma" includes and implies many ideas; in common parlance, adharma means that which is against nature, immoral, unethical, wrong or unlawful.
In Buddhism, "dharma" incorporates the teachings and doctrines of the founder of Buddhism, the Buddha.
History.
According to the authoritative book "History of Dharmasastra", in the hymns of the Rigveda the word Dharma appears at least fifty-six times, as an adjective or noun. According to Paul Horsch, the word Dharma has its origin in the myths of Vedic Hinduism. The Brahman (whom all the gods make up), claim the hymns of the Rig Veda, created the universe from chaos, they hold (dhar-) the earth and sun and stars apart, they support (dhar-) the sky away and distinct from earth, and they stabilise (dhar-) the quaking mountains and plains. The gods, mainly Indra, then deliver and hold order from disorder, harmony from chaos, stability from instability - actions recited in the Veda with the root of word dharma. In hymns composed after the mythological verses, the word dharma takes expanded meaning as a cosmic principle and appears in verses independent of gods. It evolves into a concept, claims Paul Horsch, that has a dynamic functional sense in Atharvaveda for example, where it becomes the cosmic law that links cause and effect through a subject. Dharma, in these ancient texts, also takes a ritual meaning. The ritual is connected to the cosmic, and ‘‘dharmani’’ is equated to ceremonial devotion to the principles that gods used to create order from disorder, the world from chaos. Past the ritual and cosmic sense of dharma that link the current world to mythical universe, the concept extends to ethical-social sense that links human beings to each other and to other life forms. It is here that dharma as a concept of law emerges in Hinduism.
Dharma and related words are found in the oldest Vedic literature of Hinduism, in later Vedas, Upanishads, Puranas, and the Epics; the word dharma also plays a central role in the literature of other Indian religions founded later, such as Buddhism and Jainism. According to Brereton, "Dharman" occurs 63 times in Rig-veda; in addition, words related to Dharman also appear in Rig-veda, for example once as dharmakrt, 6 times as "satyadharman", and once as "dharmavant", 4 times as "dharman" and twice as "dhariman". There is no Iranian equivalent in old Persian for Dharma, suggesting the word dharman had origins in Indo-Aryan culture outside of Persia, or it is a concept that is indigenous to India. However, ideas in parts overlapping to "Dharma" are found in other ancient cultures: such as Chinese Tao, Egyptian Maat, Sumerian Me.
Eusebeia and dharma.
In mid 20th century, an inscription of the Indian Emperor Asoka from the year 258 BC was discovered in Afghanistan. This rock inscription contained Sanskrit, Aramaic and Greek text. According to , on the rock appears a Greek rendering for the Sanskrit word dharma: the word eusebeia. Scholars of Hellenistic Greece explain eusebeia as a complex concept. Eusebia means not only to venerate gods, but also spiritual maturity, a reverential attitude toward life, and includes the right conduct toward one’s parents, siblings and children, the right conduct between husband and wife, and the conduct between biologically unrelated people. This rock inscription, concludes Paul Hacker, suggests dharma in India, about 2300 years ago, was a central concept and meant not only religious ideas, but ideas of right, of good, of one’s duty toward the human community.
Rta, Maya and Dharma.
The evolving literature of Hinduism linked "Dharma" to two other important concepts: "Ṛta" and "Māyā". Ṛta in Vedas is the truth and cosmic principle which regulates and coordinates the operation of the universe and everything within it. Māyā in Rig-veda and later literature means illusion, fraud, deception, magic that misleads and creates disorder, thus is contrary to reality, laws and rules that establish order, predictability and harmony. Paul Horsch suggests Ṛta and Dharma are parallel concepts, the former being a cosmic principle, the latter being of moral social sphere; while Māyā and Dharma are also analogous concepts, the former being that which corrupts law and moral life, the later being that which strengthens law and moral life.
Day proposes Dharma is a manifestation of Ṛta, but suggests Ṛta may have been subsumed into a more complex concept of Dharma, as the idea developed in ancient India over time in a nonlinear manner. The following verse from the Rigveda is an example where "rta" and dharma are linked:
Hinduism.
Dharma in Hinduism, is an organising principle that applies to human beings in solitude, in their interaction with human beings and nature, as well as between inanimate objects, to all of cosmos and its parts. It refers to the order and customs which make life and universe possible, and includes behaviours, rituals, rules that govern society, and ethics. Hindu dharma includes the religious duties, moral rights and duties of each individual, as well as behaviours that enable social order, right conduct, and those that are virtuous. Dharma, according to Van Buitenen, is that which all existing beings must accept and respect to sustain harmony and order in the world. It is neither the act nor the result, but the natural laws that guide the act and create the result to prevent chaos in the world. It is innate characteristic, that makes the being what it is. It is, claims Van Buitenen, the pursuit and execution of one’s nature and true calling, thus playing one’s role in cosmic concert. In Hinduism, it is the dharma of the bee to make honey, of cow to give milk, of sun to radiate sunshine, of river to flow. In terms of humanity, dharma is the need for, the effect of and essence of service and interconnectedness of all life.
Dharma in Vedas and Upanishads.
The history section of this article discusses the development of dharma concept in Vedas. This development continued in the Upanishads and later ancient scripts of Hinduism. In Upanishads, the concept of dharma continues as universal principle of law, order, harmony, and truth. It acts as the regulatory moral principle of the Universe. It is explained as law of righteousness and equated to "satya" (Sanskrit: सत्यं, truth), in hymn 1.4.14 of Brhadaranyaka Upanishad, as follows:
Dharma in the Epics.
The Hindu religion and philosophy, claims Daniel Ingall, places major emphasis on individual practical morality. In the Sanskrit epics, this concern is omnipresent.
In the Second Book of Ramayana, for example, a peasant asks the King to do what dharma morally requires of him, the King agrees and does so even though his compliance with the law of dharma costs him dearly. Similarly, dharma is at the centre of all major events in the life of Rama, Sita, and Lakshman in Ramayana, claims Daniel Ingall. Each episode of Ramayana presents life situations and ethical questions in symbolic terms. The issue is debated by the characters, finally the right prevails over wrong, the good over evil. For this reason, in Hindu Epics, the good, morally upright, law abiding king is referred to ‘‘dharmaraja’’.
In Mahabharata, the other major Indian epic, similarly, dharma is central, and it is presented with symbolism and metaphors. Near the end of the epic, the god Yama, referred to as Dharma in the text, is portrayed as taking the form of a dog to test the compassion of Yudishthira, who is told he may not enter paradise with such an animal, but refuses to abandon his companion, for which decision he is then praised by Dharma. The value and appeal of the Mahabharata is not as much in its complex and rushed presentation of metaphysics in the 12th book, claims Ingall, because Indian metaphysics is more eloquently presented in other Sanskrit scriptures; the appeal of Mahabharata, like Ramayana, is in its presentation of a series of moral problems and life situations, to which there are usually three answers given, according to Ingall: one answer is of Bhima, which is the answer of brute force, an individual angle representing materialism, egoism, and self; the second answer is of Yudhishthira, which is always an appeal to piety and gods, of social virtue and of tradition; the third answer is of introspective Arjuna, which falls between the two extremes, and who, claims Ingall, symbolically reveals the finest moral qualities of man. The Epics of Hinduism are a symbolic treatise about life, virtues, customs, morals, ethics, law, and other aspects of Dharma. There is extensive discussion of Dharma at the individual level in the Epics of Hinduism, observes Ingall; for example, on free will versus destiny, when and why human beings believe in either, ultimately concluding that the strong and prosperous naturally uphold free will, while those facing grief or frustration naturally lean towards destiny. The Epics of Hinduism illustrate various aspects of Dharma, they are a means of communicating Dharma with metaphors.
Dharma according to 4th century Vatsyayana.
According to Klaus Klostermaier, 4th century Hindu scholar Vātsyāyana explained dharma by contrasting it with adharma. Vātsyāyana suggested that Dharma is not merely in one’s actions, but also in words one speaks or writes, and in thought. According to Vātsyāyana:
Dharma according to Patanjali Yoga.
In the Yoga system the dharma is real ; in the Vedanta it is unreal.
Dharma is part of yoga, suggests Patanjali; the elements of Hindu Dharma are the attributes, qualities and aspects of yoga. Patanjali explained dharma in two categories: yama (restraints) and niyama (observances).
The five yama, according to Patanjali, are: abstain from injury to all living creatures (ahimsa), abstain from falsehood (satya), abstain from unauthorised appropriation of things-of-value from another (acastrapurvaka), abstain from coveting or sexually cheating on your partner, and abstain from expecting or accepting gifts from others. The five yama apply in action, speech and mind. In explaining yama, Patanjali clarifies that certain professions and situations may require qualification in conduct. For example, a fisherman must injure a fish, but he must attempt to do this with least trauma to fish and the fisherman must try to injure no other creature as he fishes.
The five niyama (observances) are cleanliness by eating pure food and removing impure thoughts (such as arrogance or jealousy or pride), contentment in one’s means, meditation and silent reflection regardless of circumstances one faces, study and pursuit of historic knowledge, and devotion of all actions to the Supreme Teacher to achieve perfection of concentration.
Sources of Dharma.
Dharma is an empirical and experiential inquiry for every man and woman, according to some texts of Hinduism. For example, Apastamba Dharmasutra states:
In other texts, three sources and means to discover Dharma in Hinduism are described. These, according to , are: First, learning historical knowledge such as Vedas, Upanishads, the Epics and other Sanskrit literature with the help of one’s teacher. Second, observing the behavior and example of good people. The third source applies when neither one’s education nor example exemplary conduct is known. In this case, ‘‘atmatusti’’ is the source of dharma in Hinduism, that is the good person reflects and follows what satisfies his heart, his own inner feeling, what he feels driven to.
Dharma, life stages and social stratification.
Some texts of Hinduism outline "Dharma" for society and at the individual level. Of these, the most cited one is "Manusmriti", which describes the four "Varnas", their rights and duties. Most texts of Hinduism, however, discuss "Dharma" with no mention of "Varna" (caste). Other Dharma texts and Smritis differ from Manusmriti on the nature and structure of Varnas. Yet, other texts question the very existence of varna. Bhrigu, in the Epics, for example, presents the theory that dharma does not require any varnas. In practice, medieval India is widely believed to be a socially stratified society, with each social strata inheriting a profession and being endogamous. Varna was not absolute in Hindu Dharma; individuals had the right to renounce and leave their Varna, as well as their asramas of life, in search of moksa. While neither Manusmriti nor succeeding Smritis of Hinduism ever use the word varnadharma (that is, the dharma of varnas), or varnasramadharma (that is, the dharma of varnas and asramas), the scholarly commentary on Manusmriti use these words, and thus associate dharma with varna system of India. In 6th century India, even Buddhist kings called themselves ‘protectors of varnasramadharma’ - that is, dharma of varna and asramas of life.
At the individual level, some texts of Hinduism outline four āśramas, or stages of life as individual’s dharma. These are: (1) brahmacārya, the life of preparation as a student, (2) gṛhastha, the life of the householder with family and other social roles, (3) vānprastha or aranyaka, the life of the forest-dweller, transitioning from worldly occupations to reflection and renunciation, and (4) sannyāsa, the life of giving away all property, becoming a recluse and devotion to moksa, spiritual matters.
The four stages of life complete the four human strivings in life, according to Hinduism. Dharma enables the individual to satisfy the striving for stability and order, a life that is lawful and harmonious, the striving to do the right thing, be good, be virtuous, earn religious merit, be helpful to others, interact successfully with society. The other three strivings are Artha - the striving for means of life such as food, shelter, power, security, material wealth, etc.; Kama - the striving for sex, desire, pleasure, love, emotional fulfillment, etc.; and Moksa - the striving for spiritual meaning, liberation from life-rebirth cycle, self-realisation in this life, etc. The four stages are neither independent nor exclusionary in Hindu Dharma.
Dharma and poverty.
Dharma while being necessary for individual and society, is dependent on poverty and prosperity in a society, according to Hindu Dharma scriptures. For example, according to Adam Bowles, Shatapatha Brahmana 11.1.6.24 links social prosperity and "Dharma" through water. Waters come from rains, it claims; when rains are abundant there is prosperity on the earth, and this prosperity enables people to follow Dharma - moral and lawful life. In times of distress, of drought, of poverty, everything suffers including relations between human beings and the human ability to live according to Dharma.
In Rajadharmaparvan 91.34-8, the relationship between poverty and dharma reaches a full circle. A land with less moral and lawful life suffers distress, and as distress rises it causes more immoral and unlawful life, which further increases distress. Those in power must follow the raja dharma (that is, dharma of rulers), because this enables the society and the individual to follow dharma and achieve prosperity.
Dharma and law.
The notion of "Dharma" as duty or propriety is found in India's ancient legal and religious texts. In Hindu philosophy, justice, social harmony, and happiness requires that people live per dharma. The Dharmashastra is a record of these guidelines and rules. The available evidence suggest India once had a large collection of dharma related literature (sutras, shastras); four of the sutras survive and these are now referred to as Dharmasutras. Along with laws of Manu in Dharmasutras, exist parallel and different compendium of laws, such as the laws of Narada and other ancient scholars. These different and conflicting law books are neither exclusive, nor do they supersede other sources of Dharma in Hinduism. These Dharmasutras include instructions on education of the young, their rites of passage, customs, religious rites and rituals, marital rights and obligations, death and ancestral rites, laws and administration of justice, crimes, punishments, rules and types of evidence, duties of a king, as well as morality.
Buddhism.
In Buddhism "dharma" means cosmic law and order, but is also applied to the teachings of the Buddha. In Buddhist philosophy, "dhamma/dharma" is also the term for "phenomena": In East Asia, the translation for dharma is 法, pronounced "fǎ" in Mandarin, "choe ཆོས་" in Tibetan, "beop" in Korean, "hō" in Japanese, and "pháp" in Vietnamese. However, the term dharma can also be transliterated from its original form.
Buddha's teachings.
For practicing Buddhists, references to "dharma" ("dhamma" in Pali) particularly as "the Dharma", generally means the teachings of the Buddha, commonly known throughout the East as Buddha-Dharma. It includes especially the discourses on the fundamental principles (such as the Four Noble Truths and the Noble Eightfold Path), as opposed to the parables and to the poems.
The status of Dharma is regarded variably by different Buddhist traditions. Some regard it as an ultimate truth, or as the fount of all things which lies beyond the "three realms" (Sanskrit: "tridhatu") and the "wheel of becoming" (Sanskrit: "bhavacakra"), somewhat like the pagan Greek and Christian logos: this is known as "Dharmakaya" (Sanskrit). Others, who regard the Buddha as simply an enlightened human being, see the Dharma as the essence of the "84,000 different aspects of the teaching" (Tibetan: "chos-sgo brgyad-khri bzhi strong") that the Buddha gave to various types of people, based upon their individual propensities and capabilities.
Dharma refers not only to the sayings of the Buddha, but also to the later traditions of interpretation and addition that the various schools of Buddhism have developed to help explain and to expand upon the Buddha's teachings. For others still, they see the Dharma as referring to the "truth," or the ultimate reality of "the way that things really are" (Tib. Cho).
The Dharma is one of the Three Jewels of Buddhism in which practitioners of Buddhism seek refuge, or that upon which one relies for his or her lasting happiness. The Three Jewels of Buddhism are the Buddha, meaning the mind's perfection of enlightenment, the Dharma, meaning the teachings and the methods of the Buddha, and the Sangha, meaning the monastic community who provide guidance and support to followers of the Buddha.
Buddhist phenomenology.
Other uses include dharma, normally spelled with a small "d" (to differentiate), which refers to a "phenomenon" or "constituent factor" of human experience. This was gradually expanded into a classification of constituents of the entire material and mental world. Rejecting the substantial existence of permanent entities which are qualified by possibly changing qualities, Buddhist Abhidharma philosophers enumerated lists of dharmas which varied by school. They came to propound that these "constituent factors" are the only type of entity that truly exists (and only some thinkers gave dharmas this kind of existence). This notion is of particular importance for the analysis of human experience: Rather than assuming that mental states inhere in a cognising subject, or a soul-substance, Buddhist philosophers largely propose that mental states alone exist as "momentary elements of consciousness" and that a subjective perceiver is assumed.
One of the central tenets of Buddhism, is the denial of a separate permanent "I", and is outlined in the three marks of existence.
At the heart of Buddhism is the understanding of all phenomena as dependently originated. Later, Buddhist philosophers like Nāgārjuna would question whether the dharmas (momentary elements of consciousness) truly have a separate existence of their own.
According to S. N. Goenka, the original meaning of dhamma is "dharayati iti dharmaH", or "one that contains, supports or upholds" and dharma in the Buddhist scriptures has a variety of meanings, including "phenomenon" and "nature" or "characteristic". Dharma also means "mental contents," and is paired with "citta", which means heart-mind. The pairing is paralleled with the combining of "shareera" (body) and "vedana" (feelings or sensations which arise within the body but are experienced through the mind) in major "sutras" such as the Mahasatipatthana sutra.
East Asian Buddhism.
Dharma is employed in Ch'an in a specific context in relation to transmission of authentic doctrine, understanding and bodhi; recognised in Dharma transmission.
Jainism.
The word Dharma encompasses the following meanings in Jainism:
Acharya Samantabhadra writes, "Vatthu sahavo dhammo": "the dharma is the nature of an object". It is the nature of the soul to be free, thus for the soul, the dharma is "paralaukika", beyond worldly. However the nature of the body is to seek self-preservation and be engaged in pleasures. Thus there are two dharmas. In Jain tradition, because of the difference in practice, dharma (conduct) is said to be of two kinds, for the householders (sravakas) and for the Jain ascetics.
Major Jain text, "Tattvartha Sutra" mentions "Das-dharma" (ten righteous virtues):
Acārya Amṛtacandra, author of the Jain text, "Puruṣārthasiddhyupāya" writes:
Sikhism.
For Sikhs, the word "Dharm" means the "path of righteousness" and proper religious practice. Sikh Dharma is a distinct religion revealed through the teachings of ten Gurus who are accepted by the followers as if they were spiritually the same. In Sikhism, God is described as both "Nirgun" (transcendent) and "Sargun" (immanent). Guru Granth Sahib in hymn 1353 connotes dharma as duty. The 3HO movement in Western culture, which has incorporated certain Sikh beliefs, defines Sikh Dharma broadly as all that that constitutes religion, moral duty and way of life.
Scriptures and dharma.
The Guru Granth Sahib lays down the foundation of this "righteous path" and various salient points are found:
Dharma in symbols.
The importance of "dharma" to Indian sentiments is illustrated by India’s decision in 1947 to include the Ashoka Chakra, a depiction of the "dharmachakra" ( the "wheel of dharma"), as the central motif on its flag.

</doc>
<doc id="8756" url="https://en.wikipedia.org/wiki?curid=8756" title="Daniel Dennett">
Daniel Dennett

Daniel Clement Dennett III (born March 28, 1942) is an American philosopher, writer, and cognitive scientist whose research centers on the philosophy of mind, philosophy of science, and philosophy of biology, particularly as those fields relate to evolutionary biology and cognitive science.
He is currently the co-director of the Center for Cognitive Studies and the Austin B. Fletcher Professor of Philosophy at Tufts University. Dennett is an atheist and secularist, a member of the Secular Coalition for America advisory board, as well as an outspoken supporter of the Brights movement. Dennett is referred to as one of the "Four Horsemen of New Atheism", along with Richard Dawkins, Sam Harris, and the late Christopher Hitchens.
Early life and education.
Dennett was born on March 28, 1942 in Boston, Massachusetts, the son of Ruth Marjorie (née Leck) and Daniel Clement Dennett, Jr. Dennett spent part of his childhood in Lebanon, where, during World War II, his father was a covert counter-intelligence agent with the Office of Strategic Services posing as a cultural attaché to the American Embassy in Beirut. When he was five, his mother took him back to Massachusetts after his father died in an unexplained plane crash. Dennett says that he was first introduced to the notion of "philosophy" while attending summer camp at age 11, when a camp counselor said to him, "You know what you are, Daniel? You're a philosopher."
Dennett graduated from Phillips Exeter Academy in 1959, and spent one year at Wesleyan University before receiving his Bachelor of Arts in philosophy at Harvard University in 1963. At Harvard University he was a student of W. V. Quine. In 1965, he received his Doctor of Philosophy in philosophy at the University of Oxford, where he studied under Gilbert Ryle and was a member of Christ Church. Dennett's sister is the investigative journalist Charlotte Dennett.
Dennett describes himself as "an autodidact—or, more properly, the beneficiary of hundreds of hours of informal tutorials on all the fields that interest me, from some of the world's leading scientists."
He is the recipient of a Fulbright Fellowship, two Guggenheim Fellowships, and a Fellowship at the Center for Advanced Study in the Behavioral Sciences. He is a Fellow of the Committee for Skeptical Inquiry and a Humanist Laureate of the International Academy of Humanism. He was named 2004 Humanist of the Year by the American Humanist Association.
In February 2010, he was named to the Freedom From Religion Foundation's Honorary Board of distinguished achievers.
In 2012, he was awarded the Erasmus Prize, an annual award for a person who has made an exceptional contribution to European culture, society or social science, "for his ability to translate the cultural significance of science and technology to a broad audience."
Philosophical views.
Free will.
While he is a confirmed compatibilist on free will, in "On Giving Libertarians What They Say They Want"—Chapter 15 of his 1978 book "Brainstorms", Dennett articulated the case for a two-stage model of decision making in contrast to libertarian views.
The model of decision making I am proposing has the following feature: when we are faced with an important decision, a consideration-generator whose output is to some degree undetermined produces a series of considerations, some of which may of course be immediately rejected as irrelevant by the agent (consciously or unconsciously). Those considerations that are selected by the agent as having a more than negligible bearing on the decision then figure in a reasoning process, and if the agent is in the main reasonable, those considerations ultimately serve as predictors and explicators of the agent's final decision.
While other philosophers have developed two-stage models, including William James, Henri Poincaré, Arthur Holly Compton, and Henry Margenau, Dennett defends this model for the following reasons:
These prior and subsidiary decisions contribute, I think, to our sense of ourselves as responsible free agents, roughly in the following way: I am faced with an important decision to make, and after a certain amount of deliberation, I say to myself: "That's enough. I've considered this matter enough and now I'm going to act," in the full knowledge that I could have considered further, in the full knowledge that the eventualities may prove that I decided in error, but with the acceptance of responsibility in any case.
Leading libertarian philosophers such as Robert Kane have rejected Dennett's model, specifically that random chance is directly involved in a decision, on the basis that they believe this eliminates the agent's motives and reasons, character and values, and feelings and desires. They claim that, if chance is the primary cause of decisions, then agents cannot be liable for resultant actions. Kane says:
s Dennett admits a causal indeterminist view of this deliberative kind does not give us everything libertarians have wanted from free will. For he agen does not have complete control over what chance images and other thoughts enter his mind or influence his deliberation. They simply come as they please. he agen does have some control "after" the chance considerations have occurred.
But then there is no more chance involved. What happens from then on, how he reacts, is "determined" by desires and beliefs he already has. So it appears that he does not have control in the "libertarian" sense of what happens after the chance considerations occur as well. Libertarians require more than this for full responsibility and free will.
Philosophy of mind.
Dennett has remarked in several places (such as "Self-portrait", in "Brainchildren") that his overall philosophical project has remained largely the same since his time at Oxford. He is primarily concerned with providing a philosophy of mind that is grounded in empirical research. In his original dissertation, "Content and Consciousness", he broke up the problem of explaining the mind into the need for a theory of content and for a theory of consciousness. His approach to this project has also stayed true to this distinction. Just as "Content and Consciousness" has a bipartite structure, he similarly divided "Brainstorms" into two sections. He would later collect several essays on content in "The Intentional Stance" and synthesize his views on consciousness into a unified theory in "Consciousness Explained". These volumes respectively form the most extensive development of his views.
In chapter 5 of "Consciousness Explained" Dennett describes his multiple drafts model of consciousness. He states that, "all varieties of perception—indeed all varieties of thought or mental activity—are accomplished in the brain by parallel, multitrack processes of interpretation and elaboration of sensory inputs. Information entering the nervous system is under continuous 'editorial revision.'" (p. 111). Later he asserts, "These yield, over the course of time, something "rather like" a narrative stream or sequence, which can be thought of as subject to continual editing by many processes distributed around the brain, ..." (p. 135, emphasis in the original).
In this work, Dennett's interest in the ability of evolution to explain some of the content-producing features of consciousness is already apparent, and this has since become an integral part of his program. He defends a theory known by some as Neural Darwinism. He also presents an argument against qualia; he argues that the concept is so confused that it cannot be put to any use or understood in any non-contradictory way, and therefore does not constitute a valid refutation of physicalism. His strategy mirrors his teacher Ryle's approach of redefining first person phenomena in third person terms, and denying the coherence of the concepts which this approach struggles with.
Dennett self-identifies with a few terms: "ther note that my 'avoidance of the standard philosophical terminology for discussing such matters' often creates problems for me; philosophers have a hard time figuring out what I am saying and what I am denying. My refusal to play ball with my colleagues is deliberate, of course, since I view the standard philosophical terminology as worse than useless—a major obstacle to progress since it consists of so many errors."
In "Consciousness Explained", he affirms "I am a sort of 'teleofunctionalist', of course, perhaps the original teleofunctionalist'". He goes on to say, "I am ready to come out of the closet as some sort of verificationist".
Evolutionary debate.
Much of Dennett's work since the 1990s has been concerned with fleshing out his previous ideas by addressing the same topics from an evolutionary standpoint, from what distinguishes human minds from animal minds ("Kinds of Minds"), to how free will is compatible with a naturalist view of the world ("Freedom Evolves").
Dennett sees evolution by natural selection as an algorithmic process (though he spells out that algorithms as simple as long division often incorporate a significant degree of randomness). This idea is in conflict with the evolutionary philosophy of paleontologist Stephen Jay Gould, who preferred to stress the "pluralism" of evolution (i.e., its dependence on many crucial factors, of which natural selection is only one).
Dennett's views on evolution are identified as being strongly adaptationist, in line with his theory of the intentional stance, and the evolutionary views of biologist Richard Dawkins. In "Darwin's Dangerous Idea", Dennett showed himself even more willing than Dawkins to defend adaptationism in print, devoting an entire chapter to a criticism of the ideas of Gould. This stems from Gould's long-running public debate with E. O. Wilson and other evolutionary biologists over human sociobiology and its descendant evolutionary psychology, which Gould and Richard Lewontin opposed, but which Dennett advocated, together with Dawkins and Steven Pinker. Strong disagreements have been launched against Dennett from Gould and his supporters, who allege that Dennett overstated his claims and misrepresented Gould's to reinforce what Gould describes as Dennett's "Darwinian fundamentalism".
Dennett's theories have had a significant influence on the work of evolutionary psychologist Geoffrey Miller.
An account of religion and morality.
In "Darwin's Dangerous Idea", Dennett writes that evolution can account for the origin of morality. He rejects the idea of the naturalistic fallacy as the idea that ethics is in some free-floating realm, writing that the fallacy is to rush from facts to values.
In his 2006 book, "", Dennett attempts to account for religious belief naturalistically, explaining possible evolutionary reasons for the phenomenon of religious adherence.
In this book he declares himself to be "a bright", and defends the term.
He has been doing research into clerics who are secretly atheists and how they rationalize their works. He found what he called a "Don't ask, don't tell" conspiracy because believers did not want to hear of loss of faith. That made unbelieving preachers feel isolated but they did not want to lose their jobs and sometimes their church-supplied lodgings and generally consoled themselves that they were doing good in their pastoral roles by providing comfort and required ritual. The research, with Linda LaScola, was further extended to include other denominations and non-Christian clerics.
Other philosophical views.
He has also written about and advocated the notion of memetics as a philosophically useful tool, most recently in his "Brains, Computers, and Minds", a three-part presentation through Harvard's MBB 2009 Distinguished Lecture Series.
Dennett has been critical of postmodernism, having said: "Postmodernism, the school of 'thought' that proclaimed 'There are no truths, only interpretations' has largely played itself out in absurdity, but it has left behind a generation of academics in the humanities disabled by their distrust of the very idea of truth and their disrespect for evidence, settling for 'conversations' in which nobody is wrong and nothing can be confirmed, only asserted with whatever style you can muster."
Dennett adopted and somewhat redefined the term "deepity", originally coined by Miriam Weizenbaum (daughter of computer scientist Joseph Weizenbaum). Dennett used "deepity" for a statement that is apparently profound, but is actually trivial on one level and meaningless on another. Generally, a deepity has two (or more) meanings: one that is true but trivial, and another that sounds profound and would be important if true, but is actually false or meaningless. Examples are "Que sera sera!", "Beauty is only skin deep!", "The power of intention can transform your life." The term many times.
Personal life.
Dennett married Susan Bell in 1962. They live in North Andover, Massachusetts, and have a daughter, a son, and four grandchildren. He is an avid sailor.

</doc>
<doc id="8757" url="https://en.wikipedia.org/wiki?curid=8757" title="Darwin's Dangerous Idea">
Darwin's Dangerous Idea

Darwin's Dangerous Idea: Evolution and the Meanings of Life is a 1995 book by Daniel Dennett, which looks at some of the repercussions of Darwinian theory. The crux of the argument is that, whether or not Darwin's theories are overturned, there is no going back from the dangerous idea that design (purpose or what something is for) might not need a designer. Dennett makes this case on the basis that natural selection is a blind process, which is nevertheless sufficiently powerful to explain the evolution of life. Darwin's discovery was that the generation of life worked algorithmically, that processes behind it work in such a way that given these processes the results that they tend toward must be so.
Dennett says, for example, that by claiming that minds cannot be reduced to purely algorithmic processes, many of his eminent contemporaries are claiming that miracles can occur. These assertions have generated a great deal of debate and discussion in the general public. The book was a finalist for the 1995 National Book Award in non-fiction and the 1996 Pulitzer Prize for General Non-Fiction.
Background.
Dennett's previous book was "Consciousness Explained" (1991). Dennett noted discomfort with Darwinism among not only lay people but also even academics and decided it was time to write a book dealing with the subject. "Darwin's Dangerous Idea" is not meant to be a work of science, but rather an interdisciplinary book; Dennett admits that he does not understand all of the scientific details himself. He goes into a moderate level of detail, but leaves it for the reader to go into greater depth if desired, providing references to this end.
In writing the book, Dennett wanted to "get thinkers in other disciplines to take evolutionary theory seriously, to show them how they have been underestimating it, and to show them why they have been listening to the wrong sirens." To do this he tells a story; one that is mainly original but includes some material from his previous work.
Dennett taught an undergraduate seminar at Tufts University on Darwin and philosophy, which included most of the ideas in the book. He also had the help of fellow staff and other academics, some of whom read drafts of the book. It is dedicated to W. V. O. Quine, "teacher and friend".
Synopsis.
Part I: Starting in the Middle.
"Starting in the Middle", Part I of "Darwin's Dangerous Idea", gets its name from a quote by Willard Van Orman Quine: "Analyze theory-building how we will, we all must start in the middle. Our conceptual firsts are middle-sized, middle-distance objects, and our introduction to them and to everything comes midway in the cultural evolution of the race."
The first chapter "Tell Me Why" is named after a song.
Before Charles Darwin, and still today, a majority of people see God as the ultimate cause of all design, or the ultimate answer to 'why?' questions. John Locke argued for the primacy of mind before matter, and David Hume, while exposing problems with Locke's view, could not see any alternative.
Darwin provided just such an alternative: evolution. Besides providing evidence of common descent, he introduced a mechanism to explain it: natural selection. According to Dennett, natural selection is a mindless, mechanical and algorithmic process—Darwin's dangerous idea. The third chapter introduces the concept of "skyhooks" and "cranes" (see below). He suggests that resistance to Darwinism is based on a desire for skyhooks, which do not really exist. According to Dennett, good reductionists explain apparent design without skyhooks; greedy reductionists try to explain it without cranes.
Chapter 4 looks at the tree of life, such as how it can be visualized and some crucial events in life's history. The next chapter concerns the possible and the actual, using the 'Library of Mendel' (the space of all logically possible genomes) as a conceptual aid.
In the last chapter of part I, Dennett treats human artifacts and culture as a branch of a unified Design Space. Descent or homology can be detected by shared design features that would be unlikely to appear independently. However, there are also "Forced Moves" or "Good Tricks" that will be discovered repeatedly, either by natural selection (see convergent evolution) or human investigation.
Part II: Darwinian Thinking in Biology.
The first chapter of part II, "Darwinian Thinking in Biology", asserts that life originated without any skyhooks, and the orderly world we know is the result of a blind and undirected shuffle through chaos.
The eighth chapter's message is conveyed by its title, "Biology is Engineering"; biology is the study of design, function, construction and operation. However, there are some important differences between biology and engineering. Related to the engineering concept of optimization, the next chapter deals with adaptationism, which Dennett endorses, calling Gould and Lewontin's "refutation" of it an illusion. Dennett thinks adaptationism is, in fact, the best way of uncovering constraints.
The tenth chapter, entitled "Bully for Brontosaurus", is an extended critique of Stephen Jay Gould, who Dennett feels has created a distorted view of evolution with his popular writings; his "self-styled revolutions" against adaptationism, gradualism and other orthodox Darwinism all being false alarms. The final chapter of part II dismisses directed mutation, the inheritance of acquired traits and Teilhard's "Omega Point", and insists that other controversies and hypotheses (like the unit of selection and Panspermia) have no dire consequences for orthodox Darwinism.
Part III: Mind, Meaning, Mathematics and Morality.
"Mind, Meaning, Mathematics and Morality" is the name of Part III, which begins with a quote from Nietzsche. Chapter 12, "The Cranes of Culture", discusses cultural evolution. It asserts that the meme has a role to play in our understanding of culture, and that it allows humans, alone among animals, to "transcend" our selfish genes. "Losing Our Minds to Darwin" follows, a chapter about the evolution of brains, minds and language. Dennett criticizes Noam Chomsky's perceived resistance to the evolution of language, its modeling by artificial intelligence, and reverse engineering.
The evolution of meaning is then discussed, and Dennett uses a series of thought experiments to persuade the reader that meaning is the product of meaningless, algorithmic processes.
Chapter 15 asserts that Gödel's Theorem does not make certain sorts of artificial intelligence impossible. Dennett extends his criticism to Roger Penrose. The subject then moves on to the origin and evolution of morality, beginning with Thomas Hobbes (who Dennett calls "the first sociobiologist") and Friedrich Nietzsche. He concludes that only an evolutionary analysis of ethics makes sense, though he cautions against some varieties of 'greedy ethical reductionism'. Before moving to the next chapter, he discusses some sociobiology controversies.
The penultimate chapter, entitled "Redesigning Morality", begins by asking if ethics can be 'naturalized'. Dennett does not believe there is much hope of discovering an algorithm for doing the right thing, but expresses optimism in our ability to design and redesign our approach to moral problems. In "The Future of an Idea", the book's last chapter, Dennett praises biodiversity, including cultural diversity. In closing, he uses "Beauty and the Beast" as an analogy; although Darwin's idea may seem dangerous, it is actually quite beautiful.
Central concepts.
Design Space.
Dennett believes there is little or no principled difference between the naturally generated products of evolution and the man-made artifacts of human creativity and culture. For this reason he indicates deliberately that the complex fruits of the tree of life are in a very meaningful sense "designed"—even though he does not believe evolution was guided by a higher intelligence.
Dennett supports using the notion of memes to better understand cultural evolution. He also believes even human creativity might operate by the Darwinian mechanism. This leads him to propose that the "space" describing biological "design" is connected with the space describing human culture and technology.
A precise mathematical definition of Design Space is not given in "Darwin's Dangerous Idea". Dennett acknowledges this and admits he is offering a philosophical idea rather than a scientific formulation.
Natural selection as an algorithm.
Dennett describes natural selection as a substrate-neutral, mindless algorithm for moving through Design Space.
Universal acid.
Dennett writes about the fantasy of a "universal acid" as a liquid that is so corrosive that it would eat through anything that it came into contact with, even a potential container. Such a powerful substance would transform everything it was applied to; leaving something very different in its wake. This is where Dennett draws parallels from the “universal acid” to Darwin’s idea: 
“it eats through just about every traditional concept, and leaves in its wake a revolutionized world-view, with most of the old landmarks still recognizable, but transformed in fundamental ways.”
While there are people who would like to see Darwin’s idea contained within the field of biology, Dennett asserts that this dangerous idea inevitably “leaks” out to transform other fields as well.
Skyhooks and cranes.
Dennett uses the term "skyhook" to describe a source of design complexity that does not build on lower, simpler layers—in simple terms, a miracle.
In philosophical arguments concerning the reducibility (or otherwise) of the human mind, Dennett's concept pokes fun at the idea of intelligent design emanating from on high, either originating from one or more gods, or providing its own grounds in an absurd, Munchausen-like bootstrapping manner.
Dennett also accuses various competing neo-Darwinian ideas of making use of such supposedly unscientific skyhooks in explaining evolution, coming down particularly hard on the ideas of Stephen Jay Gould.
Dennett contrasts theories of complexity that require such miracles with those based on "cranes", structures that permit the construction of entities of greater complexity but are themselves founded solidly "on the ground" of physical science.
Reception.
In "The New York Review of Books", John Maynard Smith praised "Darwin's Dangerous Idea":
It is therefore a pleasure to meet a philosopher who understands what Darwinism is about, and approves of it.
Dennett goes well beyond biology. He sees Darwinism as a corrosive acid, capable of dissolving our earlier belief and forcing a reconsideration of much of sociology and philosophy. Although modestly written, this is not a modest book. Dennett argues that, if we understand "Darwin's dangerous idea", we are forced to reject or modify much of our current intellectual baggage...
Writing in the same publication, Stephen Jay Gould criticised "Darwin's Dangerous Idea" for being an "influential but misguided ultra-Darwinian manifesto":
Daniel Dennett devotes the longest chapter in "Darwin's Dangerous Idea" to an excoriating caricature of my ideas, all in order to bolster his defense of Darwinian fundamentalism. If an argued case can be discerned at all amid the slurs and sneers, it would have to be described as an effort to claim that I have, thanks to some literary skill, tried to raise a few piddling, insignificant, and basically conventional ideas to "revolutionary" status, challenging what he takes to be the true Darwinian scripture. Since Dennett shows so little understanding of evolutionary theory beyond natural selection, his critique of my work amounts to little more than sniping at false targets of his own construction. He never deals with my ideas as such, but proceeds by hint, innuendo, false attribution, and error.
Gould was also a harsh critic of Dennett's idea of the "universal acid" of natural selection and of his subscription to the idea of memetics; Dennett responded, and the exchange between Dennett, Gould, and Robert Wright was printed in the "New York Review of Books".
Biologist H. Allen Orr wrote a critical review emphasizing similar points in the "Boston Review". The book provoked a negative reaction from creationists; Frederick Crews writes that "Darwin's Dangerous Idea" "rivals Richard Dawkins's "The Blind Watchmaker" as the creationists' most cordially hated text."

</doc>
<doc id="8758" url="https://en.wikipedia.org/wiki?curid=8758" title="Douglas Hofstadter">
Douglas Hofstadter

Douglas Richard Hofstadter (born February 15, 1945) is an American professor of cognitive science whose research focuses on the sense of "I", consciousness, analogy-making, artistic creation, literary translation, and discovery in mathematics and physics. He is best known for his book "Gödel, Escher, Bach: An Eternal Golden Braid", first published in 1979. It won both the Pulitzer Prize for general non-fiction
and a National Book Award (at that time called The American Book Award) for Science. His 2007 book "I Am a Strange Loop" won the Los Angeles Times Book Prize for Science and Technology.
Early life and education.
Hofstadter was born in New York City, the son of Nobel Prize-winning physicist Robert Hofstadter. He grew up on the campus of Stanford University, where his father was a professor, and he attended the International School of Geneva in 1958–1959. He graduated with Distinction in Mathematics from Stanford University in 1965. He continued his education and received his Ph.D. in Physics from the University of Oregon in 1975, where his study of the energy levels of Bloch electrons in a magnetic field led to his discovery of the fractal known as the Hofstadter butterfly.
Academic career.
Since 1988, Hofstadter has been the College of Arts and Sciences Distinguished Professor of Cognitive Science and Comparative Literature at Indiana University in Bloomington, where he directs the Center for Research on Concepts and Cognition which consists of himself and his graduate students, forming the "Fluid Analogies Research Group" (FARG). He was initially appointed to the Indiana University's Computer Science Department faculty in 1977, and at that time he launched his research program in computer modeling of mental processes (which at that time he called "artificial intelligence research", a label that he has since dropped in favor of "cognitive science research"). In 1984, he moved to the University of Michigan in Ann Arbor, where he was hired as a professor of psychology and was also appointed to the Walgreen Chair for the Study of Human Understanding. In 1988 he returned to Bloomington as "College of Arts and Sciences Professor" in both cognitive science and computer science, and also was appointed adjunct professor of history and philosophy of science, philosophy, comparative literature, and psychology, but he states that his involvement with most of these departments is nominal. In April 2009 Hofstadter was elected a Fellow of the American Academy of Arts and Sciences and a member of the American Philosophical Society. In 2010 he was elected a member of the Royal Society of Sciences in Uppsala, Sweden.
Hofstadter's many interests include music, visual art, the mind, creativity, consciousness, self-reference, translation and mathematics.
At the University of Michigan and Indiana University, he co-authored, with Melanie Mitchell, a computational model of "high-level perception" – Copycat – and several other models of analogy-making and cognition, including the Tabletop project, co-developed with Robert M. French. Hofstadter's doctoral student James Marshall subsequently extended the Copycat project under the name "Metacat". The Letter Spirit project, implemented by Gary McGraw and John Rehling, aims to model the act of artistic creativity by designing stylistically uniform "gridfonts" (typefaces limited to a grid). Other more recent models include Phaeaco (implemented by Harry Foundalis) and SeqSee (Abhijit Mahabal), which model high-level perception and analogy-making in the microdomains of Bongard problems and number sequences, respectively, as well as George (Francisco Lara-Dammer), which models the processes of perception and discovery in triangle geometry.
The pursuit of beauty has driven Hofstadter both inside and outside his professional work. He seeks beautiful mathematical patterns, beautiful explanations, beautiful typefaces, beautiful sonic patterns in poetry, "etc". Hofstadter has said of himself, "I'm someone who has one foot in the world of humanities and arts, and the other foot in the world of science." He has had several exhibitions of his artworks in various university art galleries. These shows have featured large collections of his gridfonts, his ambigrams (pieces of calligraphy created with two readings, either of which is usually obtained from the other by rotating or reflecting the ambigram, but sometimes simply by "oscillation", like the Necker Cube or the rabbit/duck figure of Joseph Jastrow), and his "Whirly Art" (music-inspired visual patterns realized using shapes based on various alphabets from India). (Hofstadter invented the term "ambigram" in 1984; many ambigrammists all over the world have since taken up the concept.)
Hofstadter collects and studies cognitive errors (largely, but not solely, speech errors), "bon mots" (spontaneous humorous quips), and analogies of all sorts, and his long-time observation of these diverse products of cognition, and his theories about the mechanisms that underlie them, have exerted a powerful influence on the architectures of the computational models developed by himself and FARG members.
All FARG computational models share certain key principles, including:
FARG models also have an overarching philosophy that all cognition is built from the making of analogies. The computational architectures that share these precepts are called "active symbols" architectures.
Hofstadter's thesis about consciousness, first expressed in "Gödel, Escher, Bach" ("GEB") but also present in several of his later books, is that it is an emergent consequence of seething lower-level activity in the brain. In "GEB" he draws an analogy between the social organization of a colony of ants and the mind seen as a coherent "colony" of neurons. In particular, Hofstadter claims that our sense of having (or being) an "I" comes from the abstract pattern he terms a "strange loop", which is an abstract cousin of such concrete phenomena as audio and video feedback, and which Hofstadter has defined as "a level-crossing feedback loop". The prototypical example of this abstract notion is the self-referential structure at the core of Gödel's incompleteness theorems. Hofstadter's 2007 book "I Am a Strange Loop" carries his vision of consciousness considerably further, including the idea that each human "I" is distributed over numerous brains, rather than being limited to precisely one brain.
Hofstadter's writing is characterized by an intense interaction between form and content, as exemplified by the 20 dialogues in "GEB", many of which simultaneously talk about and imitate strict musical forms used by Bach, such as canons and fugues. Most of Hofstadter's books feature some kind of structural alternation: in "GEB" between dialogues and chapters, in "The Mind's I" between selections and reflections, in "Metamagical Themas" between Chapters and Postscripts, and so forth. Both in his writing and in his teaching, Hofstadter stresses the concrete, constantly using examples and analogies, and avoids the abstract. Typical of the courses he teaches is his seminar "Group Theory and Galois Theory Visualized", in which abstract mathematical ideas are rendered as concretely as possible. He puts great effort into making ideas clear and visual, and asserts that when he teaches, if his students do not understand something, it is never their fault but always his own.
Hofstadter is passionate about languages. In addition to English, his mother tongue, he speaks French and Italian fluently (the language spoken at home with his children is Italian). At various times in his life, he has studied (in descending order of level of fluency reached) German, Russian, Spanish, Swedish, Mandarin, Dutch, Polish, and Hindi. His love of sounds pushes him to strive to minimize, and ideally get rid of, any foreign accent.
"Le Ton beau de Marot: In Praise of the Music of Language" is a long book devoted to language and translation, especially poetry translation, and one of its leitmotifs is a set of some 88 translations of "Ma Mignonne", a highly constrained poem by 16th-century French poet Clément Marot. In this book, Hofstadter jokingly describes himself as "pilingual" (meaning that the sum total of the varying degrees of mastery of all the languages that he's studied comes to 3.14159...), as well as an "oligoglot" (someone who speaks "a few" languages).
In 1999, the bicentennial year of Russian poet and writer Alexander Pushkin, Hofstadter published a verse translation of Pushkin's classic novel-in-verse "Eugene Onegin". Hofstadter has translated many other poems too (always respecting their formal constraints), and two novels (in prose): "La Chamade" ("That Mad Ache") by French writer Françoise Sagan, and "La Scoperta dell'Alba" ("The Discovery of Dawn") by Walter Veltroni, the then head of the Partito Democratico in Italy. "The Discovery of Dawn" was published in 2007, and "That Mad Ache" was published in 2009, bound together with Hofstadter's essay "Translator, Trader: An Essay on the Pleasantly Pervasive Paradoxes of Translation".
Hofstadter's Law.
Hofstadter's Law states that "It always takes longer than you expect, even when you take into account Hofstadter's Law." The Law is outlined in his work "Gödel, Escher, Bach: An Eternal Golden Braid".
Students.
Hofstadter's former Ph.D. students include (with dissertation title):
Public image.
Hofstadter has said that he feels "uncomfortable with the nerd culture that centers on computers". He admits that "a large fraction f his audienc seems to be those who are fascinated by technology", but when it was suggested that his work "has inspired many students to begin careers in computing and artificial intelligence" he replied that he was pleased about that, but that he himself has "no interest in computers". In that interview he also mentioned a course he has twice given at Indiana University, in which he took a "skeptical look at a number of highly-touted AI projects and overall approaches". For example, upon the defeat of Garry Kasparov by Deep Blue, he commented that "It was a watershed event, but it doesn't have to do with computers becoming intelligent".
Provoked by predictions of a technological singularity (a hypothetical moment in the future of humanity when a self-reinforcing, runaway development of artificial intelligence causes a radical change in technology and culture), Hofstadter has both organized and participated in several public discussions of the topic. At Indiana University in 1999 he organized such a symposium, and in April 2000, he organized a larger symposium entitled "Spiritual Robots" at Stanford University, in which he moderated a panel consisting of Ray Kurzweil, Hans Moravec, Kevin Kelly, Ralph Merkle, Bill Joy, Frank Drake, John Holland and John Koza. Hofstadter was also an invited panelist at the first Singularity Summit, held at Stanford in May 2006. Hofstadter expressed doubt about the likelihood of the singularity coming to pass in the foreseeable future.
In 1988 Dutch director Piet Hoenderdos created a docudrama about Hofstadter and his ideas, "Victim of the Brain", based on "The Mind's I". It includes interviews with Hofstadter about his work.
Columnist.
When Martin Gardner retired from writing his "Mathematical Games" column for "Scientific American" magazine, Hofstadter succeeded him in 1981–1983 with a column entitled "Metamagical Themas" (an anagram of "Mathematical Games"). An idea he introduced in one of these columns was the concept of "Reviews of This Book", a book containing nothing but cross-referenced reviews of itself which has an online implementation. One of Hofstadter's columns in "Scientific American" concerned the damaging effects of sexist language, and two chapters of his book "Metamagical Themas" are devoted to that topic, one of which is a biting analogy-based satire entitled "A Person Paper on Purity in Language" (1985), in which the reader's presumed revulsion at racism and racist language is used as a lever to motivate an analogous revulsion at sexism and sexist language; Hofstadter published it under the pseudonym William Satire, an allusion to William Safire. Another column reported on the discoveries made by University of Michigan professor Robert Axelrod in his computer tournament pitting many iterated prisoner's dilemma strategies against each other, and a follow-up column discussed a similar tournament that Hofstadter and his graduate student Marek Lugowski organized. The "Metamagical Themas" columns ranged over many themes, and included, to name just three, one on patterns in Frédéric Chopin's piano music (particularly the études), another on the concept of superrationality (choosing to cooperate when the other party/adversary is assumed to be equally intelligent as oneself), and one on the self-modifying game of Nomic, based on the way in which the legal system modifies itself, and developed by philosopher Peter Suber.
Personal life.
Hofstadter was married to Carol Ann Brush until her death. They met in Bloomington, and married in Ann Arbor in 1985. They had two children, Danny and Monica. Carol died in 1993 from the sudden onset of a brain tumor – glioblastoma multiforme – when their children were five and two. The Carol Ann Brush Hofstadter Memorial Scholarship for Bologna-bound Indiana University students was established in 1996 in her name. Hofstadter's book "Le Ton beau de Marot" is dedicated to their two children and its dedication reads "To M. & D., living sparks of their Mommy's soul".
In the fall of 2010, Hofstadter met Baofen Lin in a chacha class, and the two were married in Bloomington in September 2012.
Hofstadter has composed numerous pieces for piano, and a few for piano and voice. He created an audio CD with the title "DRH/JJ", which includes all these compositions performed primarily by pianist Jane Jackson, but with a few performed by Brian Jones, Dafna Barenboim, Gitanjali Mathur and himself.
The dedication for "I Am A Strange Loop" is: "To my sister Laura, who can understand, and to our sister Molly, who cannot." Hofstadter explains in the preface that his younger sister Molly never developed the ability to speak or understand language.
As a consequence of his attitudes about consciousness and empathy, Hofstadter has been a vegetarian for roughly half his life.
In popular culture.
In the 1982 novel ', Arthur C. Clarke's first sequel to ', HAL 9000 is described by Dr. Chandra as being caught in a "Hofstadter–Möbius loop". The movie uses the term "H. Möbius loop".
In 1995, Hofstadter's book "Fluid Concepts & Creative Analogies: Computer Models of the Fundamental Mechanisms of Thought" was the first book ever sold by Amazon.com.
Published works.
Books.
The books published by Hofstadter are (the ISBNs refer to paperback editions, where available):
Papers.
Hofstadter has written, among many others, the following papers:
Hofstadter has also written over 50 papers that were published through the Center for Research on Concepts and Cognition.
Involvement in other books.
Hofstadter has written forewords for or edited the following books:

</doc>
<doc id="8765" url="https://en.wikipedia.org/wiki?curid=8765" title="Dahomey">
Dahomey

Dahomey () was an African kingdom (located in the area of the present-day country of Benin) which lasted from about 1600 until 1894, when the last chief Behanzin was defeated by the French and the country was annexed into the French colonial empire. Dahomey developed on the Abomey Plateau amongst the Fon people in the early 17th century and became a regional power in the 18th century by conquering key cities on the Atlantic coast. For much of the 18th and 19th centuries, the Kingdom of Dahomey was a key regional state, eventually ending tributary status to the Oyo Empire. The Kingdom of Dahomey was an important regional power that had an organized domestic economy built on conquest and slave labor, significant international trade with European powers, a centralized administration, taxation systems, and an organized military. Notable in the kingdom were significant artwork, an all-female military unit known as the Dahomey Amazons, and the elaborate religious practices of Vodun with the large festival of the Annual Customs of Dahomey.
Name.
The Kingdom of Dahomey was referred to by many different names and has been written in a variety of ways, including "Danxome", "Danhome", and "Fon". The name "Fon" relates to the dominant ethnic and language group, the Fon people, of the royal families of the kingdom and is how the kingdom first became known to Europeans. The names "Dahomey", "Danxome", and "Danhome" all have a similar origin story, which historian Edna Bay says may be a false etymology. The story says that Dakodonu, considered the second king in modern kings lists, was granted permission by the Gedevi chiefs, the local rulers, to settle in the Abomey plateau. Dakodonu requested additional land from a prominent chief named Dan (or Da) to which the chief responded sarcastically "Should I open up my belly and build you a house in it?" For this insult, Dakodonu killed Dan and began the construction of his palace on the spot. The name of the kingdom was derived from the incident: Dan=chief dan, xo=Belly, me=Inside of.
History.
The Kingdom of Dahomey was established around 1600 by the Fon people who had recently settled in the area (or were possibly a result of intermarriage between the Aja people and the local Gedevi). The foundational king for Dahomey is often considered to be Houegbadja (c. 1645–1685), who built the Royal Palaces of Abomey and began raiding and taking over towns outside of the Abomey plateau.
Rule of Agaja (1708–1740).
King Agaja, Houegbadja's grandson, came to the throne in 1708 and began significant expansion of the Kingdom of Dahomey. This expansion was made possible by the superior military force of King Agaja’s Dahomey. In contrast to surrounding regions, Dahomey employed a professional standing army numbering around ten thousand. What the Dahomey lacked in numbers, they made up for in discipline and superior arms. In 1724, Agaja conquered Allada, the origin for the royal family according to oral tradition, and in 1727 he conquered Whydah. This increased size of the kingdom, particularly along the Atlantic coast, and increased power made Dahomey into a regional power. The result was near constant warfare with the main regional state, the Oyo Empire, from 1728 until 1740. The warfare with the Oyo empire resulted in Dahomey assuming a tributary status to the Oyo empire.
End of the kingdom.
The kingdom fought the First Franco-Dahomean War and Second Franco-Dahomean War with France. The kingdom was reduced and made a French protectorate in 1894; in 1904 the area became part of a French colony, French Dahomey; in 1958 this became the Republic of Dahomey; in 1975 it renamed itself the People's Republic of Benin, and in 1991 the Republic of Benin. The Dahomey kingship exists as a ceremonial role to this day.
Politics.
Early writings, predominantly written by European slave traders, often presented the kingdom as an absolute monarchy led by a despotic king. However, these depictions were often deployed as arguments by different sides in the slave trade debates, mainly in the United Kingdom, and as such were probably exaggerations. Recent historical work has emphasized the limits of monarchical power in the Kingdom of Dahomey. Historian John Yoder, with attention to the Great Council in the kingdom, argued that its activities do not "imply that Dahomey's government was democratic or even that her politics approximated those of nineteenth-century European monarchies. However, such evidence does support the thesis that governmental decisions were molded by conscious responses to internal political pressures as well as by executive fiat." The primary political divisions revolved around villages with chiefs and administrative posts appointed by the king and acting as his representatives to adjudicate disputes in the village.
The king.
The King of Dahomey ("Ahosu" in the Fon language) was the sovereign power of the kingdom. All of the kings were claimed to be part of the "Alladaxonou" dynasty, claiming descent from the royal family in Allada. Much of the succession rules and administrative structures were created early by Houegbadja, Akaba, and Agaja. Succession through the male members of the line was the norm typically going to the oldest son, but not always. The king was selected largely through discussion and decision in the meetings of the Great Council, although how this operated was not always clear. The Great Council brought together a host of different dignitaries from throughout the kingdom yearly to meet at the Annual Customs of Dahomey. Discussions would be lengthy and included members, both men and women, from throughout the kingdom. At the end of the discussions, the king would declare the consensus for the group.
The royal court.
Key positions in the King's court included the "migan", the "mehu", the "yovogan", the "kpojito" (or queen-mother), and later the "chacha" (or viceroy) of Whydah. The "migan" (combination of mi-our and gan-chief) was a primary consul for the king, a key judicial figure, and served as the head executioner. The "mehu" was similarly a key administrative officer who managed the palaces and the affairs of the royal family, economic matters, and the areas to the south of Allada (making the position key to contact with Europeans).
Relations with other states.
The relations between Dahomey and other countries was complex and heavily impacted by the Gold trade. The Oyo empire engaged in regular warfare with the kingdom of Dahomey and Dahomey was a tributary to Oyo from 1732 until 1823. The city-state of Porto-Novo, under the protection of Oyo, and Dahomey had a long-standing rivalry largely over control of the Gold trade along the coast. The rise of Abeokuta in the 1840s created another power rivaling Dahomey, largely by creating a safe haven for people from the slave trade.
Military.
The military of the Kingdom of Dahomey was divided into two units: the right and the left. The right was controlled by the "migan" and the left was controlled by the "mehu". At least by the time of Agaja, the kingdom had developed a standing army that remained encamped wherever the king was. Soldiers in the army were recruited as young as seven or eight years old, initially serving as shield carriers for regular soldiers. After years of apprenticeship and military experience, they were allowed to join the army as regular soldiers. In order to further incentivize the soldiers, each soldier received bonuses paid in cowry shells for each enemy they killed or captured in battle. This combination of lifelong military experience and monetary incentives resulted in a cohesive, well-disciplined military. One European said Agaja’s standing army consisted of, “elite troops, brave and well-disciplined, led by a prince full of valor and prudence, supported by a staff of experienced officers.”
In addition to being well-trained, the Dahomey army under Agaja was also very well armed. The Dahomey army favored imported European weapons as opposed to traditional weapons. For example, they used European flintlock muskets in long range combat and imported steel swords and cutlasses in close combat. The Dahomey army also possessed twenty-five cannons.
When going into battle, the king would take a secondary position to the field commander with the reason given that if any spirit were to punish the commander for decisions it should not be the king. Unlike other regional powers, the military of Dahomey did not have a significant cavalry (like the Oyo empire) or naval power (which prevented expansion along the coast). The Dahomey Amazons, a unit of all-female soldiers, is one of the most unique aspects of the military of the kingdom.
Dahomey Amazons.
The Dahomean state became widely known for its corps of female soldiers. Their origins are debated; they may have formed from a place guard or from gbetos (female hunting teams).
They were organized around the year 1729 to fill out the army and make it look larger in battle, armed only with banners. The women reportedly behaved so courageously they became a permanent corps. In the beginning the soldiers were criminals pressed into service rather than being executed. Eventually, however, the corps became respected enough that King Ghezo ordered every family to send him their daughters, with the most fit being chosen as soldiers.
Economy.
The economic structure of the kingdom were highly intertwined with the political and religious systems and these developed together significantly. The main currency for exchange was cowries, or shells for exchange.
Domestic economy.
The domestic economy was largely focused on agriculture and crafts produced for local consumption. Until the development of palm oil, very little agricultural or craft goods were traded outside of the kingdom. Markets served a key role in the kingdom and were organized around a rotating cycle of four days with a different market each day (the market type for the day was religiously sanctioned). Agriculture work was largely decentralized and done by most families. However, with the expansion of the kingdom agricultural plantations begun to be a common agricultural method in the kingdom. Craft work was largely dominated by a formal guild system.
Herskovits recounts a complex tax system in the kingdom where officials from the king, the "tokpe", would gather data from each village regarding their harvest and then the king would set a tax based upon the level of production and number of villagers in the village. In addition, the kings own land and production were taxed. With the significant road construction undertaken by the kingdom, toll booths were also established which would collect yearly taxes by people based on the good they carried, their occupation, and sometimes fines for public nuisance before allowing them to pass.
Religion.
The Kingdom of Dahomey shared many religious rituals with surrounding populations; however, it also developed unique ceremonies, beliefs, and religious stories for the kingdom. These included royal ancestor worship and the specific vodun practices of the kingdom.
Royal ancestor worship.
Early kings established clear worship of royal ancestors and centralized their ceremonies in the Annual Customs of Dahomey. The spirits of the kings had an exalted position in the land of the dead and it was necessary to get their permission for many activities on earth. Ancestor worship pre-existed the kingdom of Dahomey; however, under King Agaja, a cycle of ritual was created centered around first celebrating the ancestors of the king and then celebrating a family lineage.
The Annual Customs of Dahomey ("xwetanu" or "huetanu" in Fon) involved multiple elaborate components and some aspects may have been added in the 19th century. In general, the celebration involved distribution of gifts, human sacrifice, military parades, and political councils. Its main religious aspect was to offer thanks and gain the approval for ancestors of the royal lineage. However, the custom also included military parades, public discussions, gift giving (the distribution of money to and from the king), and human sacrifice and the spilling of blood.
Dahomey cosmology.
Dahomey had a unique form of West African Vodun which linked together preexisting animist traditions with vodun practices. Oral history recounted that Hwanjile, a wife of Agaja and mother of Tegbessou brought the vodun to the kingdom and ensured its spread. The primary deity is the combined Mawu-Lisa (Mawu having female characteristics and Lisa having male characteristics) and it is claimed that this god took over the world that was created by their mother Nana-Buluku. Mawu-Lisa governs the sky and is the highest pantheon of gods, but other gods exist in the earth and in thunder. Religious practice organized different priesthoods and shrines for each different god and each different pantheon (sky, earth or thunder). Women made up a significant amount of the priest class and the chief priest was always a descendent of Dakodonou.
Arts.
The arts in Dahomey were unique and distinct from the artistic traditions elsewhere in Africa. The arts were substantially supported by the king and his family, had non-religious traditions, assembled multiple different materials, and borrowed widely from other peoples in the region. Common art forms included wood and ivory carving, metalwork (including silver, iron and brass, appliqué cloth, and clay bas-reliefs.
The king was key in supporting the arts and many of them provided significant sums for artists resulting in the unique development, for the region, of a non-religious artistic tradition in the kingdom. Artists were not of a specific class but both royalty made important artistic contributions. Kings were often depicted in large zoomorphic forms with each king resembling a particular animal in multiple representations.
Suzanne Blier identifies two unique aspects of art in Dahomey: 1. Assemblage of different components and 2. Borrowing from other states. Assemblage of art, involving the combination of multiple components (often of different materials) combined together in a single piece of art, was common in all forms and was the result of the various kings promoting finished products rather than particular styles. This assembling may have been a result of the second feature which involved the wide borrowing of styles and techniques from other cultures and states. Clothing, cloth work, architecture, and the other forms of art all resemble other artistic representation from around the region.
Much of the art work revolved around the royalty. Each of the palaces at the Royal Palaces of Abomey contained elaborate bas-reliefs ("noundidė" in Fon) providing a record of the king's accomplishments. Each king had his own palace within the palace complex and within the outer walls of their personal palace was a series of clay reliefs designed specific to that king. These were not solely designed for royalty and chiefs, temples, and other important buildings had similar reliefs. The reliefs would present Dahomey kings often in military battles against the Oyo or Mahi tribes to the north of Dahomey with their opponents depicted in various negative depictions (the king of Oyo is depicted in one as a baboon eating a cob of corn). Historical themes dominated representation and characters were basically designed and often assembled on top of each other or in close proximity creating an ensemble effect. In addition to the royal depictions in the reliefs, royal members were depicted in power sculptures known as "bocio" which incorporated mixed materials (including metal, wood, beads, cloth, fur, feathers, and bone) onto a base forming a standing figure. The bocio are religiously designed to include different forces together to unlock powerful forces. In addition, the cloth appliqué of Dahomey depicted royalty often in similar zoomorphic representation and dealt with matters similar to the reliefs, often the kings leading during warfare.
A distinctive tradition was the casting of small brass figures of animals or people which were worn as jewellery or displayed in the homes of the relatively well-off. These figures, which continue to be made for the tourist trade, were relatively unusual in traditional African art in having no religious aspect, being purely decorative, as well as indicative of some wealth. Also unusual, by being so early and clearly provenanced is a carved wooden tray (not dissimilar to much more recent examples) in Ulm, Germany, which was brought to Europe before 1659, when it was described in a printed catalogue.
In popular culture.
The Kingdom of Dahomey has been depicted in a number of different literary works of fiction or creative nonfiction. "In Dahomey" (1903) was a successful Broadway musical, the first full-length Broadway musical written entirely by African Americans, in the early 20th century. Novelist Paul Hazoumé's first novel "Doguicimi" (1938) was based on decades of research into the oral traditions of the Kingdom of Dahomey during King Ghezo. American novelist Frank Yerby published a historical novel set partially in Dahomey titled "The Man From Dahomey" (1971).
Behanzin's resistance to the French attempt to end slave trading and human sacrifice has been central to a number of works. Jean Pliya's first play "Kondo le requin" (1967), winner of the Grand Prize for African History Literature, tells the story of Behanzin's struggle to maintain the old order. Maryse Condé's novel "The Last of the African Kings" (1992) similarly focuses on Behanzin's resistance and his exile to the Caribbean.

</doc>
<doc id="8767" url="https://en.wikipedia.org/wiki?curid=8767" title="Dragoon">
Dragoon

Dragoon regiments were established in most European armies during the late 17th and early 18th centuries.
The name is derived from a type of firearm (called a dragon) carried by dragoons of the French Army.
The title has been retained in modern times by a number of armoured or ceremonial mounted regiments.
Origins and name.
The establishment of dragoons evolved from the practice of sometimes transporting infantry by horse when speed of movement was needed. In 1552 Prince Alexander of Parma mounted several companies of infantry on pack horses to achieve surprise. Another early instance was ordered by Louis of Nassau in 1572 during operations near Mons in Hainaut, when 500 infantry were transported this way. It is also suggested the first dragoons were raised by the Marshal de Brissac in 1600. According to old German literature, dragoons were invented by Count Ernst von Mansfeld, one of the greatest German military commanders, in the early 1620s. There are other instances of mounted infantry predating this. However Mansfeld, who had learned his profession in Hungary and the Netherlands, often used horses to make his foot troops more mobile, creating what was called an "armée volante" (French for "flying army").
The name possibly derives from an early weapon, a short wheellock called a "dragon" because the first dragoons raised in France had their carbine's muzzle decorated with a dragon's head. The practice comes from a time when all gunpowder weapons had distinctive names, including the culverin, serpentine, falcon, falconet, etc. It is also sometimes claimed a galloping infantryman with his loose coat and the burning match resembled a dragon.
Use as a verb.
Dragoon is occasionally used to mean to subjugate or persecute by the imposition of troops; and by extension to compel by any violent measures or threats. The verb dates from 1689, at a time when dragoons were being used by the French monarchy to persecute Protestants, particularly by forcing protestants to lodge a dragoon in their house to watch over them, at the householders expense.
Early history and role.
Early dragoons were not organized in squadrons or troops as were cavalry, but in companies like the infantry: their officers and non-commissioned officers bore infantry ranks. Dragoon regiments used drummers, not buglers, to communicate orders on the battlefield. The flexibility of mounted infantry made dragoons a useful arm, especially when employed for what would now be termed "internal security" against smugglers or civil unrest, and on line of communication security duties. During the English Civil War dragoons were used for a variety of tasks: providing outposts, holding defiles or bridges in the front or rear of the main army, lining hedges or holding enclosures, and providing dismounted musketeers to support regular cavalry. Supplied with inferior horses and more basic equipment, the dragoon regiments were cheaper to recruit and maintain than the expensive regiments of cavalry. When in the 17th century Gustav II Adolf introduced dragoons into the Swedish Army, he provided them with a sabre, an axe and a matchlock musket, utilizing them as "labourers on horseback". Many of the European armies henceforth imitated this all-purpose set of weaponry.
A non-military use of dragoons was the 1681 Dragonnades, a policy instituted by Louis XIV to intimidate Huguenot families into either leaving France or re-converting to Catholicism by billeting ill-disciplined dragoons in Protestant households. While other categories of infantry and cavalry were also used, the mobility, flexibility and available numbers of the dragoon regiments made them particularly suitable for repressive work of this nature over a wide area.
Dragoons were at a disadvantage when engaged against true cavalry, and constantly sought to improve their horsemanship, armament and social status. By the Napoleonic Wars the primary role of dragoons in most European armies had progressed from that of mounted infantry to that of heavy cavalry. Earlier dragoon responsibilities for scouting and picket duty had passed to hussars and similar light cavalry corps in the French, Austrian, Prussian, and other armies. In the Imperial Russian Army, due to the availability of the cossack troops, the dragoons were retained in their original role for much longer.
An exception to the rule was the British Army. To reduce military budgets, all horse (cavalry) regiments were gradually demoted to dragoons from 1746 onward — which meant they were paid on a lower scale. When this was completed in 1788, the heavy cavalry regiments had become either Dragoon Guards or Heavy Dragoons (depending on their precedence). The designation of Dragoon Guards did not mean that these regiments (the former 2nd to 8th Horse) had become Household Troops, but simply that they had been given a more dignified designation to compensate for the loss of pay and prestige. Starting in 1756, seven regiments of Light Dragoons were raised. These Light Dragoons were trained in reconnaissance, skirmishing and other work requiring endurance in accordance with contemporary standards of light cavalry performance. The success of this new class of cavalry was such that that eight regular Dragoon regiments were converted to Light Dragoons between 1768 and 1783.
19th century.
During the Napoleonic Wars, dragoons generally assumed a cavalry role, though remaining a lighter class of mounted troops than the armored cuirassiers. Dragoons rode larger horses than the light cavalry and wielded straight, rather than curved swords. Emperor Napoleon often formed complete divisions out of his 30 dragoon regiments and used them as battle cavalry to break the enemy's main resistance. In 1809, French dragoons scored notable successes against Spanish armies at the Battle of Ocana and the Battle of Alba de Tormes. British heavy dragoons made devastating charges against French infantry at the Battle of Salamanca in 1812 and at the Battle of Waterloo in 1815.
In the Spanish army, in 1635, Pedro de la Puente organized a body of dragoons in Innsbruck, Austria, and in 1640 one was created in Spain as a "tercio" of a thousand dragoons armed with the arquebus. In 1704, along with the rest of the tercios, the Spanish dragoons were reorganised into regiments by Philip V. During the 18th century several additional regiments of dragoons were created in the Spanish Americas, some of them to function as a police force. In 1803 the regiments of dragoons began to be called light cavalry and shortly after 1815 this class of cavalry disappeared from the Spanish Army. However three regiments of Spanish dragoons had been reestablished by the 1880s and these continued in existence until the overthrow of the monarchy in 1931.
In New Spain, soon to be México, Dragoons were important and elite units of the Royal Army. A number of dragoons became important military and political figures, among them Ignacio Allende and Juan Aldama, members of the Queen's Regiment of Dragoons who defected and then initiated the Independence movement in México, beginning in 1810. Another important Dragoon was Agustin de Iturbide, who would ultimately achieve Mexican Independence in 1821. He was known as the greatest horseman in México and became so renowned in battle during his youth that he acquired the nickname "El Dragón de Hierro" or "The Iron Dragon" (in Spanish, "dragon" and "dragoon" both sound and are written exactly the same). He would go on to become Agustín I, after being elected Emperor of México. The political importance of Dragoons during this time in the nascent country cannot be overstated.
In several stages between 1816 and 1861, the 21 existing Light Dragoon regiments in the British Army were disbanded or converted to lancers or hussars.
Between 1881 and 1910 all Russian cavalry (other than Cossacks and Imperial Guard regiments) were designated as dragoons, reflecting an emphasis on dismounted action in their training and a growing acceptance of the impracticality of employing historical cavalry tactics against modern firepower.
In Japan, in the late 19th century/early 20th century, dragoons were deployed in the same way as in other armies, but were dressed as hussars.
20th century.
In 1914 there were still dragoon regiments in the British, French, German, Russian, Austro-Hungarian, Peruvian, Norwegian, Swedish, Danish and Spanish armies. Their uniforms varied greatly, lacking the characteristic features of hussar or lancer regiments. There were occasional reminders of the mounted infantry origins of this class of soldier. Thus the dragoon regiments of the Imperial German Army wore the pickelhaube (spiked helmet) of the same design as those of the infantry and the British dragoons wore scarlet tunics, In other respects however dragoons had adopted the same tactics, roles and equipment as other branches of the cavalry and the distinction had become simply one of traditional titles. Weaponry had ceased to have a historic connection, with both the French and German dragoon regiments carrying lances during the early stages of World War I.
The historic Geman, Russian and Austro-Hungarian dragoon regiments ceased to exist as distinct branches following the overthrow of the respective Imperial regimes of these countries during 1917-18. The Spanish dragoons, which dated back to 1640, were reclassified as numbered cavalry regiments in 1931 as part of the army modernization policies of the new Republic.
The Australian Light Horse were similar to 18th-century dragoon regiments in some respects, being mounted infantry which normally fought on foot, their horses' purpose being transportation. They served during the Second Boer War and World War I. The Australian 4th Light Horse Brigade became famous for the Battle of Beersheba in 1917 where they charged on horseback using rifle bayonets, since neither sabres or lances were part of their equipment.
Probably the last use of real dragoons (infantry on horseback) in combat was made by the Portuguese Army in the war in Angola during the 1960s and early 1970s. In 1966, the Portuguese created an experimental horse platoon, to operate against the guerrillas in the high grass region of Eastern Angola, in which each soldier was armed with a G3 assault rifle for combat on foot and with an automatic pistol to fire from horseback. The troops on horseback were able to operate in difficult terrain unsuited to motor vehicles and had the advantage of being able to control the area around them, with a clear view over the grass that foot troops did not have. Moreover, these unconventional troops created a psychological impact on an enemy that was not used to facing horse troops, and thus had no training or strategy to deal with them. The experimental horse platoon was so successful that its entire parent battalion was transformed from an armored reconnaissance unit to a three-squadron horse battalion known as the "Dragoons of Angola". One of the typical operations carried out by the Dragoons of Angola, in cooperation with airmobile forces, consisted of the dragoons chasing the guerrillas and pushing them in one direction, with the airmobile troops being launched from helicopter in the enemy rear, trapping the enemy between the two forces.
Dragoner rank.
Until 1918 Dragoner (en: dragoon) was the designation given to the lowest ranks in the dragoon regiments of the Austro-Hungarian and Imperial German Armies. The "Dragoner" rank, together with all other private ranks of the different branch of service, did belong to the so-called gemeine rank group.
Modern dragoons.
Brazil.
The Brazilian president's honor guard is provided (amongst other units) by a regiment of dragoons: the 1st Guards Cavalry Regiment of the Brazilian Army.
This regiment is known as the "Dragões da Independência" (Independence Dragoons). The name was given in 1927 and refers to the fact that a detachment of dragoons escorted the Prince Royal of Portugal, Pedro I, at the time when he declared Brazilian independence from Portugal, on September 7, 1822.
The Independence Dragoons wear 19th-century dress uniforms similar to those of the earlier Imperial Honor Guard, which are used as the regimental full dress uniform since 1927. The uniform was designed by Debret, in white and red, with plumed bronze helmets. The colors and pattern were influenced by the Austrian dragoons of the period, as the Brazilian Empress Consort was also an Austrian Archduchess. The color of the plumes varies according to rank. The Independence Dragoons are armed with lances and sabres, the latter only for the officers and the colour guard.
The regiment was established in 1808 by the Prince Regent and future king of Portugal, John VI, with the duty of protecting the Portuguese royal family, which had sought refuge in Brazil during the Napoleonic wars. However dragoons had existed in Portugal since at least the early 18th century and, in 1719, units of this type of cavalry were sent to Brazil, initially to escort shipments of gold and diamonds and to guard the Viceroy who resided in Rio de Janeiro (1st Cavalry Regiment – Vice-Roy Guard Squadron). Later, they were also sent to the south to serve against the Spanish during frontier clashes. After the proclamation of Brazilian independence, the title of the regiment was changed to that of the Imperial Honor Guard, with the role of protecting the Imperial Family. The Guard was later disbanded by Emperor Peter II and would be recreated only later in the republican era.
At the time of the Republic proclamation in 1889, horse #6 of the Imperial Honor Guard was ridden by the officer making the declaration of the end of Imperial rule, Second Lieutenant Eduardo José Barbosa. This is commemorated by the custom under which the horse having this number is used only by the commander of the modern regiment.
Canada.
There are three dragoon regiments in the Canadian Forces: the Royal Canadian Dragoons and two reserve regiments, the British Columbia Dragoons and the Saskatchewan Dragoons.
The Royal Canadian Dragoons is the senior Armoured regiment in the Canadian Forces. The current role of The Royal Canadian Dragoons is to provide Armour Reconnaissance support to 2 Canadian Mechanized Brigade Group (2 CMBG) operations.
The Royal Canadian Mounted Police were accorded the formal status of a regiment of dragoons in 1921. The modern RCMP does not retain any military status however.
Chile.
Founded as the "Dragones de la Reina" (Queen's Dragoons) in 1758 and later renamed the Dragoons of Chile in 1812, and then becoming the Carabineros de Chile in 1903. The Carabineros are the national police of Chile. The military counterpart, that of the 15th Reinforced Regiment "Dragoons" is now as of 2010 the 4th Armored Brigade "Chorrillos" based in Punta Arenas as the 6th Armored Cavalry Squadron "Dragoons", and form part of the 5th Army Division
Denmark.
The Royal Danish Army includes amongst its historic regiments the Jutish Dragoon Regiment, which was raised in 1670.
Finland.
The Finnish Dragoon squadron exists in conjunction with the Army Academy in Lappeenranta and continues the traditions of the former 1. Squadron of the Uusimaa Dragoon battalion.
France.
The modern French Army retains three dragoon regiments from the thirty-two in existence at the beginning of World War I: the 2nd, which is a nuclear, bacteriologic and chemical protection regiment, the 4th, an armor regiment equipped with Leclerc tanks, and the 13th (Special Reconnaissance).
Norway.
In the Norwegian Army during the early part of the 20th century, dragoons served in part as mounted troops, and in part on skis or bicycles ("hjulryttere", meaning "wheel-riders"). Dragoons fought on horses, bicycles and skis against the German invasion in 1940. After World War II the dragoon regiments were reorganized as armoured reconnaissance units. "Dragon" is the rank of a compulsory service private cavalryman while enlisted (regular) cavalrymen have the same rank as infantrymen: "Grenader".
Peru.
The Presidential Escort Life Guard Dragoons Regiment "Field Marshal Domingo Nieto", named after Field Marshal Domingo Nieto, of the President of the Republic of Perú were the traditional Guard of the Government Palace of Perú until March 5, 1987 and its disbandment in that year. However by Ministerial Resolution No 139-2012/DE/EP of February 2, 2012 the restoration of the Cavalry Regiment "Marshal Domingo Nieto" as the official escort of the President of the Republic of Peru was announced. The main mission of the reestablished regiment was to guarantee the security of the President of the Republic and of the Government Palace.
This regiment of dragoons was created in 1904 following the suggestion of a French military mission which undertook the reorganization of the Peruvian Army in 1896. The initial title of the unit was Cavalry Squadron "President's Escort". It was modelled on the French dragoons of the period. The unit was later renamed as the Cavalry Regiment "President's Escort" before receiving its current title in 1949.
The Peruvian Dragoon Guard has throughout its existence worn French-style uniforms of black tunic and red breeches in winter and white coat and red breeches in summer, with red and white plumed bronze helmets with the coat of arms of Peru and golden or red epaulettes depending on rank. They retain their original armament of lances and sabres, until the 1980s rifles were used for dismounted drill.
At 13:00 hours every day, the main esplanade in front of the Government Palace of Perú fronting Lima's Main Square serves as the stage for the changing of the guard, undertaken by members of the Presidential Life Guard Escort Dragoons, mounted or dismounted. While the dismounted changing is held on Mondays and Fridays, the mounted ceremony is held twice a month on a Sunday.
Portugal.
The Portuguese Army still maintains two units which are descended from former regiments of dragoons. These are the 3rd Regiment of Cavalry (the former "Olivença Dragoons") and the 6th Regiment of Cavalry (the former "Chaves Dragoons"). Both regiments are, presently, armoured units. The Portuguese Rapid Reaction Brigade' Armoured Reconnaissance Squadron – a unit from the 3rd Regiment of Cavalry – is known as the "Paratroopers Dragoons".
During the Portuguese Colonial War in the 1960s and the 1970s, the Portuguese Army created an experimental horse platoon, to combat the guerrillas in eastern Angola. This unit was soon augmented, becoming a group of three squadrons, known as the "Angola Dragoons". The Angola Dragoons operated as mounted infantry – like the original dragoons – each soldier being armed with a pistol to fire when on horseback and with an automatic rifle, to use when dismounted. A unit of the same type was being created in Mozambique when the war ended in 1974.
Spain.
In the Spanish army in 1635 Pedro de la Puente organized in Innsbruck (Austria) a body of dragoons, and in 1640 Spain, a tercio of thousand musket armed dragons was established. At the end of the 17th century the Spanish had three Tercios in Spain, three Tercios in the Netherlands and three more in the Milan, Italy.
In 1704 the Spanish dragoon like the rest of Tercios, were dissolved and transformed into regiment s by Felipe V. In the 18th century several regiments of dragoons were created in the American viceroys, some of them for police duties.
Between 1803 and 1815 the Spanish dragoon regiments were renamed as light cavalry ("cazadores"). However this branch of mounted troops was recreated in the late nineteenth century. In 1930 three Spanish dragoon regiments were still in existence.
Sweden.
In the Swedish Army, dragoons comprise the Military Police and Military Police Rangers. They also form the Dragoons Battalion of the Life Guards. The Dragoons Battalion have roots that go back as far as 1523, making it one of the world's oldest military units still in service and the only mounted unit still retained by the Swedish Army. Horses are used for ceremonial purposes only, most often when the dragoons take part in the changing of the guards at The Royal Palace in Stockholm. ""Livdragon" is the rank of a private cavalryman.
Switzerland.
In the Swiss Army, mounted dragoons existed until the early 1970s, when they were converted into Armoured Grenadiers units. The "Dragoner"" had to prove he was able to keep a horse at home before entering the army. At the end of basic training they had to buy a horse at a reduced price from the army and to take it home together with equipment, uniform and weapon. In the "yearly repetition course" the dragoons served with their horses, often riding from home to the meeting point.
The abolition of the dragoon units, believed to be the last non-ceremonial horse cavalry in Europe, was a contentious issue in Switzerland. On 5 December 1972 the Swiss "National Council" approved the measure by 91 votes, against 71 for retention.
United Kingdom.
Thirty-one dragoon regiments were in existence at the height of the Napoleonic Wars (seven Dragoon Guard regiments - 1st to 7th; five Dragoon regiments - 1st to 6th (5th Dragoons disbanded for mutiny), nineteen Light Dragoon regiments - 7th to 25th).
In the present-day British Army regular army, four regiments are designated as dragoons:
The three regiments named as Dragoon Guards were historically considered heavy cavalry, although by continental standards they were not the heaviest type of cavalry since they carried no armour (unlike cuirassiers). The designation "Dragoon Guards" does not indicate the status of Household Troops but was a distinction awarded to former "Regiments of Horse" when these were converted to Dragoons in 1746.
The Light Dragoons were formed as light cavalry during the Napoleonic Wars, and were similar to hussars. In the early 19th century several regiments were simultaneously designated as light dragoons and as hussars.
In the Territorial Army, one of the five squadrons of the Royal Yeomanry is designated as dragoons: The Westminster Dragoons.
History.
Towards the end of 1776 George Washington realized the need for a mounted branch of the military. In January 1777 four regiments of light dragoons were raised. Short term enlistments were abandoned and the Dragoons joined for three years, or "the war". They participated in most of the major engagements of the American Revolutionary War, including the Battles of White Plains, Trenton, Princeton, Brandywine, Germantown, Saratoga, Cowpens, and Monmouth, as well as the Yorktown campaign.
Prior to the War of 1812 the U.S. organized the Regiment of Light Dragoons. For the war a second regiment was activated; that regiment was consolidated with the original regiment in 1814. The original regiment was consolidated with the Corps of Artillery in June 1815.
The 1st United States Dragoons explored Iowa after the Black Hawk Purchase put the area under U.S. control. In the summer of 1835, the regiment blazed a trail along the Des Moines river and established outposts from present-day Des Moines to Fort Dodge. In 1933, the State of Iowa opened the Dragoon Trail, a scenic and historic drive that follows the path of the 1st United States Dragoons on their historic march.
In 1861 the two existing U.S. Dragoon regiments were re-designated as the 1st and 2nd Cavalry. This reorganization did not affect their role or equipment, although the traditional orange uniform braiding of the dragoons was replaced by the standard yellow of the Cavalry branch. This marked the official end of dragoons in the U.S. Army, although certain modern units trace their origins back to the historic dragoon regiments.
Modern.
The 1st and 2nd Battalion, 48th Infantry were mechanized infantry units assigned to the 3rd Armored Division (3AD) in West Germany during the Cold War. The unit crest of the 48th Infantry designated the unit as Dragoons.
The 1st Dragoons was reformed in the Vietnam era as the 1st Squadron, 1st U.S. Cavalry. It has served in the Iraqi War and remains as the oldest cavalry unit, as well as the most decorated one, in the U.S. Army. Today's modern 1–1 Cavalry is a scout/attack unit, equipped with MRAPs, M3A3 Bradley CFVs, and Strykers.
Another modern United States Army unit informally known as the 2nd Dragoons, is the 2nd Cavalry Regiment. This unit was originally organized as the Second Regiment of Dragoons in 1836 and was renamed the Second Cavalry Regiment in 1861, being redesignated as the 2nd Armored Cavalry Regiment in 1948. The regiment is currently equipped with the Stryker family of wheeled fighting vehicles and was redesignated 2d Stryker Cavalry Regiment in 2006. In 2011 the 2d Dragoon regiment was redesignated 2d Cavalry Regiment. The 2nd Cavalry Regiment has the distinction of being the longest continuously serving regiment in the United States Army.
The 113th Army Band, at Ft. Knox, KY is also officially nicknamed as "The Dragoons." This derives from its formation as the Band, First Regiment of Dragoons on July 8, 1840.
Company D, 3rd Light Armored Reconnaissance Battalion of the USMC, are named the "Dragoons". Their combat history includes Operation Iraqi Freedom and Operation Enduring Freedom from 2002 to 2013.

</doc>
<doc id="8768" url="https://en.wikipedia.org/wiki?curid=8768" title="Dulcimer">
Dulcimer

A dulcimer is a kind of stringed musical instrument.
Among its types are:
Appalachian dulcimers:
Other:

</doc>
