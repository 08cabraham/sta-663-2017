<doc id="4173" url="https://en.wikipedia.org/wiki?curid=4173" title="Babe Ruth">
Babe Ruth

George Herman "Babe" Ruth, Jr. (February 6, 1895 – August 16, 1948) was an American professional baseball player whose career in Major League Baseball (MLB) spanned 22 seasons, from 1914 through 1935. Nicknamed "The Bambino" and "The Sultan of Swat", he began his MLB career as a stellar left-handed pitcher for the Boston Red Sox, but achieved his greatest fame as a slugging outfielder for the New York Yankees. Ruth established many MLB batting (and some pitching) records, including career home runs (714), runs batted in (RBIs) (2,213), bases on balls (2,062), slugging percentage (.690), and on-base plus slugging (OPS) (1.164); the latter two still stand today. Ruth is regarded as one of the greatest sports heroes in American culture and is considered by many to be the greatest baseball player of all time. He was one of the first five inductees into the National Baseball Hall of Fame in 1936.
At age seven, Ruth was sent to St. Mary's Industrial School for Boys, a reformatory where he learned life lessons and baseball skills from Brother Matthias Boutlier of the Christian Brothers, the school's disciplinarian and a capable baseball player. In 1914, Ruth was signed to play minor-league baseball for the Baltimore Orioles but was soon sold to the Red Sox. By 1916, he had built a reputation as an outstanding pitcher who sometimes hit long home runs, a feat unusual for any player in the pre-1920 dead-ball era. Although Ruth twice won 23 games in a season as a pitcher and was a member of three World Series championship teams with Boston, he wanted to play every day and was allowed to convert to an outfielder. With regular playing time, he broke the MLB single-season home run record in 1919.
After that season, Red Sox owner Harry Frazee controversially sold Ruth to the Yankees, an act that, coupled with Boston's subsequent championship drought, popularized the "Curse of the Bambino" superstition. In his 15 years with New York, Ruth helped the Yankees win seven American League (AL) championships and four World Series championships. His big swing led to escalating home run totals that not only drew fans to the ballpark and boosted the sport's popularity but also helped usher in the live-ball era of baseball, in which it evolved from a low-scoring game of strategy to a sport where the home run was a major factor. As part of the Yankees' vaunted "Murderer's Row" lineup of 1927, Ruth hit 60 home runs, extending his MLB single-season record. He retired in 1935 after a short stint with the Boston Braves. During his career, Ruth led the AL in home runs during a season twelve times.
Ruth's legendary power and charismatic personality made him a larger-than-life figure in the "Roaring Twenties". During his career, he was the target of intense press and public attention for his baseball exploits and off-field penchants for drinking and womanizing. His often reckless lifestyle was tempered by his willingness to do good by visiting children at hospitals and orphanages. He was denied a job in baseball for most of his retirement, most likely due to poor behavior during parts of his playing career. In his final years, Ruth made many public appearances, especially in support of American efforts in World War II. In 1946, he became ill with cancer, and died two years later.
Early years.
George Herman Ruth, Jr., was born in 1895 at 216 Emory Street in Pigtown, a working class section of Baltimore, Maryland, named for its meat-packing plants. Its population included recent immigrants from Ireland, Germany and Italy, and African Americans. Ruth's parents, George Herman Ruth, Sr. (1871–1918), and Katherine Schamberger, were both of German American ancestry. According to the 1880 census, his parents John and Mary were born in Maryland. The paternal grandparents of Ruth, Sr. were from Prussia and Hanover, respectively. Ruth, Sr. had a series of jobs, including lightning rod salesman and streetcar operator, before becoming a counterman in a family-owned combination grocery and saloon on Frederick Street. George Ruth, Jr., was born in the house of his maternal grandfather, Pius Schamberger, a German immigrant and trade unionist. Only one of young George's seven siblings, his younger sister Mamie, survived infancy.
Many aspects of Ruth's childhood are undetermined, including the date of his parents' marriage. When young George was a toddler, the family moved to 339 South Goodyear Street, not far from the rail yards; by the time the boy was 6, his father had a saloon with an upstairs apartment at 426 West Camden Street. Details are equally scanty about why young George was sent at the age of 7 to St. Mary's Industrial School for Boys, a reformatory and orphanage. As an adult, Babe Ruth suggested that not only had he been running the streets and rarely attending school, he was drinking beer when his father was not looking. Some accounts say that, after a violent incident at his father's saloon, the city authorities decided this environment was unsuitable for a small child. At St. Mary's, which George Jr. entered on June 13, 1902, he was recorded as "incorrigible"; he spent much of the next twelve years there.
Although St. Mary's inmates received an education, students were also expected to learn work skills and help operate the school, particularly once the boys turned 12. Ruth became a shirtmaker, and was also proficient as a carpenter. He would adjust his own shirt collars, rather than having a tailor do it, even during his well-paid baseball career. The boys, aged 5 to 21, did most work around the facility, from cooking to shoemaking, and renovated St. Mary's in 1912. The food was simple, and the Xaverian Brothers who ran the school insisted on strict discipline; corporal punishment was common. Ruth's nickname there was "Niggerlips", as he had large facial features and was darker than most boys at the all-white reformatory.
Ruth was sometimes allowed to rejoin his family, or was placed at St. James's Home, a supervised residence with work in the community, but he was always returned to St. Mary's. He rarely was visited by his family; his mother died when he was 12 and by some accounts, he was permitted to leave St. Mary's only to attend the funeral. How Ruth came to play baseball there is uncertain: according to one account, his placement at St. Mary's was due in part to repeatedly breaking Baltimore's windows with long hits while playing street ball; by another, he was told to join a team on his first day at St. Mary's by the school's athletic director, Brother Herman, becoming a catcher even though left-handers rarely play that position. During his time there he also played third base and shortstop, again unusual for a left-hander, and was forced to wear mitts and gloves made for right-handers. He was encouraged in his pursuits by the school's Prefect of Discipline, Brother Matthias Boutlier, a native of Nova Scotia. A large man, Brother Matthias was greatly respected by the boys both for his strength and for his fairness. For the rest of his life, Ruth would praise Brother Matthias, and his running and hitting styles closely resembled his teacher's. Ruth stated, "I think I was born as a hitter the first day I ever saw him hit a baseball." The older man became a mentor and role model to George; biographer Robert W. Creamer commented on the closeness between the two:
The school's influence remained with Ruth in other ways: a lifelong Catholic, he would sometimes attend Mass after carousing all night, and he became a well-known member of the Knights of Columbus. He would visit orphanages, schools, and hospitals throughout his life, often avoiding publicity. He was generous to St. Mary's as he became famous and rich, donating money and his presence at fundraisers, and spending $5,000 to buy Brother Matthias a Cadillac in 1926—subsequently replacing it when it was destroyed in an accident. Nevertheless, his biographer Leigh Montville suggests that many of the off-the-field excesses of Ruth's career were driven by the deprivations of his time at St. Mary's.
Most of the boys at St. Mary's played baseball, with organized leagues at different levels of proficiency. Ruth later estimated that he played 200 games a year as he steadily climbed the ladder of success. Although he played all positions at one time or another (including infield positions generally reserved for right-handers), he gained stardom as a pitcher. According to Brother Matthias, Ruth was standing to one side laughing at the bumbling pitching efforts of fellow students, and Matthias told him to go in and see if he could do better. After becoming the best pitcher at St. Mary's, in 1913, when Ruth was 18, he was allowed to leave the premises to play weekend games on teams drawn from the community. He was mentioned in several newspaper articles, for both his pitching prowess and ability to hit long home runs.
Professional baseball.
Minor league, Baltimore Orioles.
In early 1914, Ruth was signed to a professional baseball contract by Jack Dunn, owner and manager of the minor-league Baltimore Orioles, an International League team. The circumstances of Ruth's signing cannot be stated with certainty, with historical fact obscured by stories that cannot all be true. By some accounts, Dunn was urged to attend a game between an all-star team from St. Mary's and one from another Xaverian facility, Mount St. Mary's College. Some versions have Ruth running away before the eagerly awaited game, to return in time to be punished, and then pitching St. Mary's to victory as Dunn watched. Others have Washington Senators pitcher Joe Engel, a Mount St. Mary's graduate, pitching in an alumni game after watching a preliminary contest between the college's freshmen and a team from St. Mary's, including Ruth. Engel watched Ruth play, then told Dunn about him at a chance meeting in Washington. Ruth, in his autobiography, stated only that he worked out for Dunn for a half-hour, and was signed. According to biographer Kal Wagenheim, there were legal difficulties to be straightened out as Ruth was supposed to remain at the school until he turned 21.
The train journey to spring training in Fayetteville, North Carolina, in early March was likely Ruth's first outside the Baltimore area. The rookie ballplayer was the subject of various pranks by the veterans, who were probably also the source of his famous nickname. There are various accounts of how Ruth came to be called Babe, but most center on his being referred to as "Dunnie's babe" or a variant. "Babe" was at that time a common nickname in baseball, with perhaps the most famous to that point being Pittsburgh Pirates pitcher and 1909 World Series hero Babe Adams, who appeared younger than he was.
Babe Ruth's first appearance as a professional ballplayer was in an intersquad game on March 7, 1914. Ruth played shortstop, and pitched the last two innings of a 15–9 victory. In his second at bat, Ruth hit a long home run to right, which was reported locally to be longer than a legendary shot hit in Fayetteville by Jim Thorpe. His first appearance against a team in organized baseball was an exhibition against the major-league Philadelphia Phillies: Ruth pitched the middle three innings, giving up two runs in the fourth, but then settling down and pitching a scoreless fifth and sixth. The following afternoon, Ruth was put in during the sixth inning against the Phillies and did not allow a run the rest of the way. The Orioles scored seven runs in the bottom of the eighth to overcome a 6–0 deficit, making Ruth the winning pitcher.
Once the regular season began, Ruth was a star pitcher who was also dangerous at the plate. The team performed well, yet received almost no attention from the Baltimore press. A third major league, the Federal League, had begun play, and the local franchise, the Baltimore Terrapins, restored that city to the major leagues for the first time since 1902. Few fans visited Oriole Park, where Ruth and his teammates labored in relative obscurity. Ruth may have been offered a bonus and a larger salary to jump to the Terrapins; when rumors to that effect swept Baltimore, giving Ruth the most publicity he had experienced to date, a Terrapins official denied it, stating it was their policy not to sign players under contract to Dunn.
The competition from the Terrapins caused Dunn to sustain large losses. Although by late June the Orioles were in first place, having won over two-thirds of their games, the paid attendance dropped as low as 150. Dunn explored a possible move by the Orioles to Richmond, Virginia, as well as the sale of a minority interest in the club. These possibilities fell through, leaving Dunn with little choice other than to sell his best players to major league teams to raise money. He offered Ruth to the reigning World Series champions, Connie Mack's Philadelphia Athletics, but Mack had his own financial problems. The Cincinnati Reds and New York Giants expressed interest in Ruth, but Dunn sold his contract, along with those of pitchers Ernie Shore and Ben Egan, to the Boston Red Sox of the American League (AL) on July 4. The sale price was announced as $25,000 but other reports lower the amount to half that, or possibly $8,500 plus the cancellation of a $3,000 loan. Ruth remained with the Orioles for several days while the Red Sox completed a road trip, and reported to the team in Boston on July 11.
MLB, Boston Red Sox (1914–19).
Developing star.
Ruth arrived in Boston on July 11, 1914, along with Egan and Shore. Ruth later told of meeting the woman he would first marry, Helen Woodford, that morning—she was then a 16-year-old waitress at Landers Coffee Shop, and Ruth related that she served him when he had breakfast there. Other stories, though, suggest the meeting happened on another day, and perhaps under other circumstances. Regardless of when he began to woo his first wife, he won his first game for the Red Sox that afternoon, he won his first game for the Red Sox, 4–3, over the Cleveland Naps. He pitched to catcher Bill Carrigan, who was also the Red Sox manager. Shore was given a start by Carrigan the next day; he won that and his second start and thereafter was pitched regularly. Ruth lost his second start, and was thereafter little used. As a batter, in his major-league debut, Ruth went 0-for-2 against left-hander Willie Mitchell, striking out in his first at bat, before being removed for a pinch hitter in the seventh inning. Ruth was not much noticed by the fans, as Bostonians watched the Red Sox's crosstown rivals, the Braves, begin a legendary comeback that would take them from last place on the Fourth of July to the 1914 World Series championship.
Egan was traded to Cleveland after two weeks on the Boston roster. During his time as a Red Sox, he kept an eye on the inexperienced Ruth, much as Dunn had in Baltimore. When he was traded, no one took his place as supervisor. Ruth's new teammates considered him brash, and would have preferred him, as a rookie, to remain quiet and inconspicuous. When Ruth insisted on taking batting practice despite his being both a rookie who did not play regularly, and a pitcher, he arrived to find his bats sawn in half. His teammates nicknamed him "the Big Baboon", a name the swarthy Ruth, who had disliked the nickname "Niggerlips" at St. Mary's, detested. Ruth had received a raise on promotion to the major leagues, and quickly acquired tastes for fine food, liquor, and women, among other temptations.
Manager Carrigan allowed Ruth to pitch two exhibition games in mid-August. Although Ruth won both against minor-league competition, he was not restored to the pitching rotation. It is uncertain why Carrigan did not give Ruth additional opportunities to pitch. There are legends—filmed for the screen in "The Babe Ruth Story" (1948)—that the young pitcher had a habit of signaling his intent to throw a curveball by sticking out his tongue slightly, and that he was easy to hit until this changed. Creamer pointed out that it is common for inexperienced pitchers to display such habits, and the need to break Ruth of his would not constitute a reason to not use him at all. The biographer suggested that Carrigan was unwilling to use Ruth due to poor behavior by the rookie.
On July 30, 1914, Boston owner Joseph Lannin had purchased the minor-league Providence Grays, members of the International League. The Providence team had been owned by several people associated with the Detroit Tigers, including star hitter Ty Cobb, and as part of the transaction, a Providence pitcher was sent to the Tigers. To soothe Providence fans upset at losing a star, Lannin announced that the Red Sox would soon send a replacement to the Grays. This was intended to be Ruth, but his departure for Providence was delayed when Cincinnati Reds owner Garry Herrmann claimed him off waivers. After Lannin wrote to Herrmann explaining that the Red Sox wanted Ruth in Providence so he could develop as a player, and would not release him to a major league club, Herrmann allowed Ruth to be sent to the minors. Carrigan later stated that Ruth was not sent down to Providence to make him a better player, but to help the Grays win the International League pennant (league championship).
Ruth joined the Grays on August 18, 1914. What was left of the Baltimore Orioles after Dunn's deals had managed to hold on to first place until August 15, after which they continued to fade, leaving the pennant race between Providence and Rochester. Ruth was deeply impressed by Providence manager "Wild Bill" Donovan, previously a star pitcher with a 25–4 win–loss record for Detroit in 1907; in later years, he credited Donovan with teaching him much about pitching. Ruth was called upon often to pitch, in one stretch starting (and winning) four games in eight days. On September 5 in Toronto, Ruth pitched a one-hit 9–0 victory, and hit his first professional home run, his only one as a minor leaguer, off Ellis Johnson. Recalled to Boston after Providence finished the season in first place, he pitched and won a game for the Red Sox against the New York Yankees on October 2, getting his first major league hit, a double. Ruth finished the season with a record of 2–1 as a major leaguer and 23–8 in the International League (for Baltimore and Providence). Once the season concluded, Ruth married Helen in Ellicott City, Maryland. Creamer speculated that they did not marry in Baltimore, where the newlyweds boarded with George Ruth, Sr., to avoid possible interference from those at St. Mary's—both bride and groom were not yet of age and Ruth remained on parole from that institution until his 21st birthday.
Ruth reported to his first major league spring training in Hot Springs, Arkansas, in March 1915. Despite a relatively successful first season, he was not slated to start regularly for the Red Sox, who had two stellar left-handed pitchers already: the established stars Dutch Leonard, who had broken the record for the lowest earned run average (ERA) in a single season; and Ray Collins, a 20-game winner in both 1913 and 1914. Ruth was ineffective in his first start, taking the loss in the third game of the season. Injuries and ineffective pitching by other Boston pitchers gave Ruth another chance, and after some good relief appearances, Carrigan allowed Ruth another start, and he won a rain-shortened seven inning game. Ten days later, the manager had him start against the New York Yankees at the Polo Grounds. Ruth took a 3–2 lead into the ninth, but lost the game 4–3 in 13 innings. Ruth, hitting ninth as was customary for pitchers, hit a massive home run into the upper deck in right field off of Jack Warhop. At the time, home runs were rare in baseball, and Ruth's majestic shot awed the crowd. The winning pitcher, Warhop, would in August 1915 conclude a major league career of eight seasons, undistinguished but for being the first major league pitcher to give up a home run to Babe Ruth.
Carrigan was sufficiently impressed by Ruth's pitching to give him a spot in the starting rotation. Ruth finished the 1915 season 18–8 as a pitcher; as a hitter, he batted .315 and had four home runs. The Red Sox won the AL pennant, but with the pitching staff healthy, Ruth was not called upon to pitch in the 1915 World Series against the Philadelphia Phillies. Boston won in five games; Ruth was used as a pinch hitter in Game Five, but grounded out against Phillies ace Grover Cleveland Alexander. Despite his success as a pitcher, Ruth was acquiring a reputation for long home runs; at Sportsman's Park against the St. Louis Browns, a Ruth hit soared over Grand Avenue, breaking the window of a Chevrolet dealership.
In 1916, there was attention focused on Ruth for his pitching, as he engaged in repeated pitching duels with the ace of the Washington Senators, Walter Johnson. The two met five times during the season, with Ruth winning four and Johnson one (Ruth had a no decision in Johnson's victory). Two of Ruth's victories were by the score of 1–0, one in a 13-inning game. Of the 1–0 shutout decided without extra innings, AL President Ban Johnson stated, "That was one of the best ball games I have ever seen." For the season, Ruth went 23–12, with a 1.75 ERA and nine shutouts, both of which led the league. Ruth's nine shutouts in 1916 set a league record for left-handers that would remain unmatched until Ron Guidry tied it in 1978. The Red Sox won the pennant and World Series again, this time defeating the Brooklyn Superbas (as the Dodgers were then known) in five games. Ruth started and won Game 2, 2–1, in 14 innings. Until another game of that length was played in 2005, this was the longest World Series game, and Ruth's pitching performance is still the longest postseason complete game victory.
Carrigan retired as player and manager after 1916, returning to his native Maine to be a businessman. Ruth, who played under four managers who are in the National Baseball Hall of Fame, always maintained that Carrigan, who is not enshrined there, was the best skipper he ever played for. There were other changes in the Red Sox organization that offseason, as Lannin sold the team to a three-man group headed by New York theatrical promoter Harry Frazee. Jack Barry was hired by Frazee as manager.
Emergence as a hitter.
Ruth went 24–13 with a 2.01 ERA and six shutouts in 1917, but the Sox finished in second place in the league, nine games behind the Chicago White Sox in the standings. On June 23 at Washington, Ruth made a memorable pitching start. When the home plate umpire called the first four pitches as balls, Ruth threw a punch at him, and was ejected from the game and later suspended for ten days. Ernie Shore was called in to relieve Ruth, and was allowed eight warm-up pitches. The runner who had reached base on the walk was caught stealing, and Shore retired all 26 batters he faced to win the game. Shore's feat was listed as a perfect game for many years; in 1991, Major League Baseball's (MLB) Committee on Statistical Accuracy caused it to be listed as a combined no-hitter. In 1917, Ruth was used little as a batter, other than his plate appearances while pitching, and hit .325 with two home runs.
The entry of the United States into World War I occurred at the start of the season, and overshadowed the sport. Conscription was introduced in September 1917, and most baseball players in the big leagues were of draft age. This included Barry, who was a player-manager, and who joined the Naval Reserve in an attempt to avoid the draft, only to be called up after the 1917 season. Frazee hired International League President Ed Barrow as Red Sox manager. Barrow had spent the previous 30 years in a variety of baseball jobs, though he never played the game professionally. With the major leagues shorthanded due to the war, Barrow had many holes in the Red Sox lineup to fill.
Ruth also noticed these vacancies in the lineup, and, dissatisfied in the role of a pitcher who appeared every four or five days, wanted to play every day at another position. Barrow tried Ruth at first base and in the outfield during the exhibition season, but as the team moved towards Boston and the season opener, restricted him to pitching. At the time, Ruth was possibly the best left-handed pitcher in baseball; allowing him to play another position was an experiment that could have backfired.
Inexperienced as a manager, Barrow had player Harry Hooper advise him on baseball game strategy. Hooper urged his manager to allow Ruth to play another position when he was not pitching, arguing to Barrow, who had invested in the club, that the crowds were larger on days when Ruth played, as they were attracted by his hitting. Barrow gave in early in May; Ruth promptly hit home runs in four consecutive games (one an exhibition), the last off of Walter Johnson. For the first time in his career (disregarding pinch-hitting appearances), Ruth was allowed a place in the batting order higher than ninth.
Although Barrow predicted that Ruth would beg to return to pitching the first time he experienced a batting slump, that did not occur. Barrow used Ruth primarily as an outfielder in the war-shortened 1918 season. Ruth hit .300, with 11 home runs, enough to secure him a share of the major league home run title with Tillie Walker of the Philadelphia Athletics. He was still occasionally used as a pitcher, and had a 13–7 record with a 2.22 ERA.
The Red Sox won their third pennant in four years, and faced the Chicago Cubs in the 1918 World Series, beginning on September 5, the earliest in history. The season was shortened as the government had ruled that baseball players eligible for the military would have to be inducted or work in critical war industries, such as armaments plants. Ruth pitched Game One for the Red Sox, a 1–0 shutout. Before Game Four, Ruth injured his left hand in a fight; he pitched anyway. He gave up seven hits and six walks, but was helped by outstanding fielding behind him and by his own batting efforts, as a fourth-inning triple by Ruth gave his team a 2–0 lead. The Cubs tied the game in the eighth inning, but the Red Sox scored to take a 3–2 again in the bottom of that inning. After Ruth gave up a hit and a walk to start the ninth inning, he was relieved on the mound by Joe Bush. To keep Ruth and his bat in the game, he was sent to play left field. Bush retired the side to give Ruth his second win of the Series, and the third and last World Series pitching victory of his career, against no defeats, in three pitching appearances. Ruth's effort gave his team a three-games-to-one lead, and two days later the Red Sox won their third Series in four years, four games to two. Before allowing the Cubs to score in Game Four, Ruth pitched consecutive scoreless innings, a record for the World Series that stood for more than 40 years until 1961, broken by Whitey Ford after Ruth's death. Ruth was prouder of that record than he was of any of his batting feats.
With the World Series over, Ruth gained exemption from the war draft by accepting a nominal position with a Pennsylvania steel mill. Many industrial establishments took pride in their baseball teams and sought to hire major leaguers. The end of the war in November set Ruth free to play baseball without such contrivances.
During the 1919 season, Ruth pitched in only 17 of his 130 games, compiling an 8–5 record as Barrow used him as a pitcher mostly in the early part of the season, when the Red Sox manager still had hopes of a second consecutive pennant. By late June, the Red Sox were clearly out of the race, and Barrow had no objection to Ruth concentrating on his hitting, if only because it drew people to the ballpark. Ruth had hit a home run against the Yankees on Opening Day, and another during a month-long batting slump that soon followed. Relieved of his pitching duties, Ruth began an unprecedented spell of slugging home runs, which gave him widespread public and press attention. Even his failures were seen as majestic—one sportswriter noted, "When Ruth misses a swipe at the ball, the stands quiver".
Two home runs by Ruth on July 5, and one in each of two consecutive games a week later, raised his season total to 11, tying his career best from 1918. The first record to fall was the AL single-season mark of 16, set by Ralph "Socks" Seybold in 1902. Ruth matched that on July 29, then pulled ahead toward the major league record of 24, set by Buck Freeman in 1899. Ruth reached this on September 8, by which time, writers had discovered that Ned Williamson of the 1884 Chicago White Stockings had hit 27—though in a ballpark where the distance to right field was only . On September 20, "Babe Ruth Day" at Fenway Park, Ruth won the game with a home run in the bottom of the ninth inning, tying Williamson. He broke the record four days later against the Yankees at the Polo Grounds, and hit one more against the Senators to finish with 29. The home run at Washington made Ruth the first major league player to hit a home run at all eight ballparks in his league. In spite of Ruth's hitting heroics, the Red Sox finished sixth, games behind the league champion White Sox.
Sale to New York.
As an out-of-towner from New York City, Frazee had been regarded with suspicion by Boston's sportswriters and baseball fans when he bought the team. He won them over with success on the field and a willingness to build the Red Sox by purchasing or trading for players. He offered the Senators $60,000 for Walter Johnson, but Washington owner Clark Griffith was unwilling. Even so, Frazee was successful in bringing other players to Boston, especially as replacements for players in the military. This willingness to spend for players helped the Red Sox secure the 1918 title. The 1919 season saw record-breaking attendance, and Ruth's home runs for Boston made him a national sensation. Nevertheless, on December 26, 1919, Frazee sold Ruth's contract to the New York Yankees.
Not all of the circumstances concerning the sale are known, but brewer and former congressman Jacob Ruppert, the New York team's principal owner, reportedly asked Yankee manager Miller Huggins what the team needed to be successful. "Get Ruth from Boston", Huggins supposedly replied, noting that Frazee was perennially in need of money to finance his theatrical productions. In any event, there was precedent for the Ruth transaction: when Boston pitcher Carl Mays left the Red Sox in a 1919 dispute, Frazee had settled the matter by selling Mays to the Yankees, though over the opposition of AL President Johnson.
According to one of Ruth's biographers, Jim Reisler, "why Frazee needed cash in 1919—and large infusions of it quickly—is still, more than 80 years later, a bit of a mystery". The often-told story is that Frazee needed money to finance the musical "No, No, Nanette", which was a Broadway hit and brought Frazee financial security. That play did not open until 1925, however, by which time Frazee had sold the Red Sox. Still, the story may be true in essence: "No, No, Nanette" was based on a Frazee-produced play, "My Lady Friends", which opened in 1919.
There were other financial pressures on Frazee, despite his team's success. Ruth, fully aware of baseball's popularity and his role in it, wanted to renegotiate his contract, signed before the 1919 season for $10,000 per year through 1921. He demanded that his salary be doubled, or he would sit out the season and cash in on his popularity through other ventures. Ruth's salary demands were causing other players to ask for more money. Additionally, Frazee still owed Lannin as much as $125,000 from the purchase of the club.
Although Ruppert and his co-owner, Colonel Tillinghast Huston, were both wealthy, and had aggressively purchased and traded for players in 1918 and 1919 to build a winning team, Ruppert faced losses in his brewing interests as Prohibition was implemented, and if their team left the Polo Grounds, where the Yankees were the tenants of the New York Giants, building a stadium in New York would be expensive. Nevertheless, when Frazee, who moved in the same social circles as Huston, hinted to the colonel that Ruth was available for the right price, the Yankees owners quickly pursued the purchase.
Frazee sold the rights to Babe Ruth for $100,000, the largest sum ever paid for a baseball player. The deal also involved a $350,000 loan from Ruppert to Frazee, secured by a mortgage on Fenway Park. Once it was agreed, Frazee informed Barrow, who, stunned, told the owner that he was getting the worse end of the bargain. Cynics have suggested that Barrow may have played a larger role in the Ruth sale, as less than a year after, he became the Yankee general manager, and in the following years made a number of purchases of Red Sox players from Frazee. The $100,000 price included $25,000 in cash, and notes for the same amount due November 1 in 1920, 1921, and 1922; Ruppert and Huston assisted Frazee in selling the notes to banks for immediate cash.
The transaction was contingent on Ruth signing a new contract, which was quickly accomplished—Ruth agreed to fulfill the remaining two years on his contract, but was given a $20,000 bonus, payable over two seasons. The deal was announced on January 6, 1920. Reaction in Boston was mixed: some fans were embittered at the loss of Ruth; others conceded that the slugger had become difficult to deal with. "The New York Times" suggested presciently, "The short right field wall at the Polo Grounds should prove an easy target for Ruth next season and, playing seventy-seven games at home, it would not be surprising if Ruth surpassed his home run record of twenty-nine circuit clouts next Summer." According to Reisler, "The Yankees had pulled off the sports steal of the century."
According to Marty Appel in his history of the Yankees, the transaction, "changed the fortunes of two high-profile franchises for decades". The Red Sox, winners of five of the first sixteen World Series, those played between 1903 and 1919, would not win another pennant until 1946, or another World Series until 2004, a drought attributed in baseball superstition to Frazee's sale of Ruth and sometimes dubbed the "Curse of the Bambino". The Yankees, on the other hand, had not won the AL championship prior to their acquisition of Ruth. They won seven AL pennants and four World Series with Ruth, and lead baseball with 40 pennants and 27 World Series titles in their history.
New York Yankees (1920–34).
Initial success (1920–23).
As a Yankee, Ruth's transition from a pitcher to a power-hitting outfielder became complete. In his fifteen-season Yankee career, consisting of over 2,000 games, Ruth broke many batting records, while making only five widely scattered appearances on the mound, winning all of them.
At the end of April 1920, the Yankees were 4–7, with the Red Sox leading the league with a 10–2 mark. Ruth had done little, having injured himself swinging the bat. Both situations began to change on May 1, when Ruth hit a ball completely out of the Polo Grounds, a feat believed only to have been previously accomplished by Joe Jackson. The Yankees won, 6–0, taking three out of four from the Red Sox. Ruth hit his second home run on May 2, and by the end of the month had set a major league record for home runs in a month with 11, and promptly broke it with 13 in June. Fans responded with record attendance: on May 16, Ruth and the Yankees drew 38,600 to the Polo Grounds, a record for the ballpark, and 15,000 fans were turned away. Large crowds jammed stadiums to see Ruth play when the Yankees were on the road.
The home runs kept coming; Ruth tied his own record of 29 on July 15, and broke it with home runs in both games of a doubleheader four days later. By the end of July, he had 37, but his pace slackened somewhat after that. Nevertheless, on September 4, he both tied and broke the organized baseball record for home runs in a season, snapping Perry Werden's 1895 mark of 44 in the minor Western League. The Yankees played well as a team, battling for the league lead early in the summer, but slumped in August in the AL pennant battle with Chicago and Cleveland. The championship was won by Cleveland, surging ahead after the Black Sox Scandal broke on September 28 and led to the suspension of many of the team's top players, including Joe Jackson. The Yankees finished third, but drew 1.2 million fans to the Polo Grounds, the first time a team had drawn a seven figure attendance. The rest of the league sold 600,000 more tickets, many fans there to see Ruth, who led the league with 54 home runs, 158 runs, and 137 runs batted in (RBIs).
Ruth was aided in his exploits, in 1920 and afterwards, by the fact that the A.J. Reach Company, maker of baseballs used in the major leagues, was using a more efficient machine to wind the yarn found within the baseball. When these went into play in 1920, the start of the live-ball era, the number of home runs increased by 184 over the previous year across the major leagues. Baseball statistician Bill James points out that while Ruth was likely aided by the change in the baseball, there were other factors at work, including the gradual abolition of the spitball (accelerated after the death of Ray Chapman, struck by a pitched ball thrown by Mays in August 1920) and the more frequent use of new baseballs (also a response to Chapman's death). Nevertheless, James theorizes that Ruth's 1920 explosion might have happened in 1919, had a full season of 154 games been played rather than 140, had Ruth refrained from pitching 133 innings that season, and if he were playing with any other home field but Fenway Park, where he hit only 9 of 29 home runs.
Yankees business manager Harry Sparrow had died early in the 1920 season; to replace him, Ruppert and Huston hired Barrow. Ruppert and Barrow quickly made a deal with Frazee for New York to acquire some of the players who would be mainstays of the early Yankee pennant-winning teams, including catcher Wally Schang and pitcher Waite Hoyt. The 21-year-old Hoyt became close to Ruth:
Ruth hit home runs early and often in the 1921 season, during which he broke Roger Connor's mark for home runs in a career, 138. Each of the almost 600 home runs Ruth hit in his career after that extended his own record. After a slow start, the Yankees were soon locked in a tight pennant race with Cleveland, winners of the 1920 World Series. On September 15, Ruth hit his 55th home run, shattering his year-old single season record. In late September, the Yankees visited Cleveland and won three out of four games, giving them the upper hand in the race, and clinched their first pennant a few days later. Ruth finished the regular season with 59 home runs, batting .378 and with a slugging percentage of .846.
The Yankees had high expectations when they met the New York Giants in the 1921 World Series, and the Yankees won the first two games with Ruth in the lineup. However, Ruth badly scraped his elbow during Game 2, sliding into third base (he had walked and stolen both second and third bases). After the game, he was told by the team physician not to play the rest of the series. Despite this advice, he did play in the next three games, and pinch-hit in Game Eight of the best-of-nine series, but the Yankees lost, five games to three. Ruth hit .316, drove in five runs and hit his first World Series home run.
After the Series, Ruth and teammates Bob Meusel and Bill Piercy participated in a barnstorming tour in the Northeast. A rule then in force prohibited World Series participants from playing in exhibition games during the offseason, the purpose being to prevent Series participants from replicating the Series and undermining its value. Baseball Commissioner Kenesaw Mountain Landis suspended the trio until May 20, 1922, and fined them their 1921 World Series checks. In August 1922, the rule was changed to allow limited barnstorming for World Series participants, with Landis's permission required.
On March 6, 1922, Ruth signed a new contract, for three years at $52,000 a year. The largest sum ever paid a ballplayer to that point, it represented 40% of the team's player payroll. Despite his suspension, Ruth was named the Yankees' new on-field captain prior to the 1922 season. During the suspension, he worked out with the team in the morning, and played exhibition games with the Yankees on their off days. He and Meusel returned on May 20, to a sellout crowd at the Polo Grounds, but Ruth batted 0-for-4, and was booed. On May 25, he was thrown out of the game for throwing dust in umpire George Hildebrand's face, then climbed into the stands to confront a heckler. Ban Johnson ordered him fined, suspended, and stripped of his captaincy. In his shortened season, Ruth appeared in 110 games, batted .315, with 35 home runs, and drove in 99 runs, but compared to his previous two dominating years, the 1922 season was a disappointment. Despite Ruth's off-year, Yankees managed to win the pennant to face the New York Giants for the second straight year in the World Series. In the Series, Giants manager John McGraw instructed his pitchers to throw him nothing but curveballs, and Ruth never adjusted. Ruth had just two hits in seventeen at bats, and the Yankees lost to the Giants for the second straight year, by 4–0 (with one tie game). Sportswriter Joe Vila called him, "an exploded phenomenon".
After the season, Ruth was a guest at an Elks Club banquet, set up by Ruth's agent with Yankee team support. There, each speaker, concluding with future New York mayor Jimmy Walker, censured him for his poor behavior. An emotional Ruth promised reform, and, to the surprise of many, followed through. When he reported to spring training, he was in his best shape as a Yankee, weighing only .
The Yankees's status as tenants of the Giants at the Polo Grounds had become increasingly uneasy, and in 1922 Giants owner Charles Stoneham stated that the Yankees's lease, expiring after that season, would not be renewed. Ruppert and Huston had long contemplated a new stadium, and had taken an option on property at 161st Street and River Avenue in the Bronx. Yankee Stadium was completed in time for the home opener on April 18, 1923, at which the Babe hit the first home run in what was quickly dubbed "the House that Ruth Built". The ballpark was designed with Ruth in mind: although the venue's left-field fence was further from home plate than at the Polo Grounds, Yankee Stadium's right-field fence was closer, making home runs easier to hit for left-handed batters. To spare Ruth's eyes, right field–his defensive position–was not pointed into the afternoon sun, as was traditional; left fielder Meusel was soon suffering headaches from squinting toward home plate.
The Yankees were never challenged, leading the league for most of the 1923 season and winning the AL pennant by 17 games. Ruth finished the season with a career-high .393 batting average and major-league leading 41 home runs (tied with Cy Williams). Another career high for Ruth in 1923 was his 45 doubles, and he reached base 379 times, then a major league record. For the third straight year, the Yankees faced the Giants in the World Series, which Ruth dominated. He batted .368, walked eight times, scored eight runs, hit three home runs and slugged 1.000 during the series, as the Yankees won their first World Series championship, four games to two.
Batting title and "bellyache" (1924–25).
In 1924, the Yankees were favored to become the first team to win four consecutive pennants. Plagued by injuries, they found themselves in a battle with the Senators. Although the Yankees won 18 of 22 at one point in September, the Senators beat out the Yankees by two games. Ruth hit .378, winning his only AL batting title, with a league-leading 46 home runs.
Ruth had kept up his efforts to stay in shape in 1923 and 1924, but by early 1925 weighed nearly . His annual visit to Hot Springs, Arkansas, where he exercised and took saunas early in the year, did him no good as he spent much of the time carousing in the resort town. He became ill while there, and suffered relapses during spring training. Ruth collapsed in Asheville, North Carolina, as the team journeyed north. He was put on a train for New York, where he was briefly hospitalized. A rumor circulated that he had died, prompting British newspapers to print a premature obituary. In New York, Ruth collapsed again and was found unconscious in his hotel bathroom. He was taken to a hospital where he suffered multiple convulsions. After sportswriter W. O. McGeehan wrote that Ruth's illness was due to binging on hot dogs and soda pop before a game, it became known as "the bellyache heard 'round the world". However, the exact cause of his ailment has never been confirmed and remains a mystery. Glenn Stout, in his history of the Yankees, notes that the Ruth legend is "still one of the most sheltered in sports"; he suggests that alcohol was at the root of Ruth's illness, pointing to the fact that Ruth remained six weeks at St. Vincent's Hospital but was allowed to leave, under supervision, for workouts with the team for part of that time. He concludes that the hospitalization was behavior-related. Playing just 98 games, Ruth had his worst season as a Yankee; he finished with a .290 average and 25 home runs. The Yankees finished next to last in the AL with a 69–85 record, their last season with a losing record until 1965.
Murderer's Row (1926–28).
Ruth spent part of the offseason of 1925–26 working out at Artie McGovern's gym, getting back into shape. Barrow and Huggins had rebuilt the team, surrounding the veteran core with good young players like Tony Lazzeri and Lou Gehrig. But New York was not expected to win the pennant.
Babe Ruth returned to his normal production during 1926, batting .372 with 47 home runs and 146 RBIs. The Yankees built a ten-game lead by mid-June, and coasted to win the pennant by three games. The St. Louis Cardinals had won the National League with the lowest winning percentage for a pennant winner to that point (.578) and the Yankees were expected to win the World Series easily. Although the Yankees won the opener in New York, St. Louis took Games Two and Three. In Game Four, Ruth hit three home runs, the first time this had been done in a World Series game, to lead the Yankees to victory; in the fifth game Ruth caught a ball as he crashed into the fence, described by baseball writers as a defensive gem. New York took that game, but Grover Cleveland Alexander won Game Six for St. Louis to tie the Series at three games each, then got very drunk. He was nevertheless inserted into Game Seven in the seventh inning and shut down the Yankees to win the game, 3–2, and win the Series. Ruth had hit his fourth home run of the Series earlier in the game, and was the only Yankee to reach base off Alexander, walking in the ninth inning before being caught stealing to end the game. Although Ruth's attempt to steal second is often deemed a baserunning blunder, Creamer pointed out that the Yankees' chances of tying the game would have been greatly improved with a runner in scoring position.
The 1926 Series was also known for Ruth's promise to Johnny Sylvester, a hospitalized 11-year-old, that he would hit a home run on his behalf. Sylvester had been injured in a fall from a horse, and a friend of Sylvester's father gave the boy two autographed baseballs signed by Yankees and Cardinals, and relayed a promise from Ruth, who did not know the boy, to hit a home run for him. After the Series, Ruth visited the boy in the hospital. When the matter became public, the press greatly inflated it, and by some accounts, Ruth saved a dying boy's life by visiting him, emotionally promising to hit a home run, and doing so.
The 1927 New York Yankees team is considered one of the greatest squads that ever took the field. Known as Murderer's Row because of the power of its lineup, the team won a then-AL-record 110 games, and took the AL pennant by 19 games, clinching first place on Labor Day. With little suspense as to the pennant race, the nation's attention turned to Ruth's pursuit of his own single-season home run record of 59. He was not alone in this chase: Gehrig proved to be a slugger capable of challenging Ruth for his home run crown, tying Ruth with 24 home runs late in June. Through July and August, they were never separated by more than two home runs. Gehrig took the lead, 45–44, in the first game of a doubleheader at Fenway Park early in September; Ruth responded with two of his own to take the lead, as it proved permanently—Gehrig finished with 47. Even so, as of September 6, Ruth was still several games off his 1921 pace, and going into the final series against the Senators, had only 57. He hit two in the first game of the series, including one off of Paul Hopkins, facing his first major league batter, to tie the record. The following day, September 30, he broke it with his 60th homer, in the eighth inning off Tom Zachary to break a 2–2 tie. "Sixty! Let's see some son of a bitch try to top that one", Ruth exulted after the game. In addition to his career-high 60 home runs, Ruth batted .356, drove in 164 runs and slugged .772. In the 1927 World Series, the Yankees swept the Pittsburgh Pirates in four games; the National Leaguers were disheartened after watching the Yankees take batting practice before Game One, with ball after ball leaving Forbes Field. According to Appel, "The 1927 New York Yankees. Even today, the words inspire awe ... all baseball success is measured against the '27 team."
Before the 1928 season, Ruth signed a new contract for an unprecedented $80,000 per year. The season started off well for the Yankees, who led the league in the early going. But the Yankees were plagued by injuries, erratic pitching and inconsistent play. The Philadelphia Athletics, rebuilding after some lean years, erased the Yankees' big lead and even took over first place briefly in early September. The Yankees, however, regained first place when they beat the Athletics three out of four games in a pivotal series at Yankee Stadium later that month, and clinched the pennant in the final weekend of the season. Ruth's play in 1928 mirrored his team's performance. He got off to a hot start and on August 1, he had 42 home runs. This put him ahead of his 60 home run pace from the previous season. He then slumped for the latter part of the season, and he hit just twelve home runs in the last two months. Ruth's batting average also fell to .323, well below his career average. Nevertheless, he ended the season with 54 home runs. The Yankees swept the favored Cardinals in four games in the World Series, with Ruth batting .625 and hitting three home runs in Game Four, including one off Alexander.
"Called shot" and final Yankee years (1929–34).
Before the 1929 season, Ruppert, who had bought out Huston in 1923, announced that the Yankees would wear uniform numbers to allow fans at cavernous Yankee Stadium to tell one player from another. The Cardinals and Indians had each experimented with uniform numbers; the Yankees were the first to use them on both home and away uniforms. As Ruth batted third, he was given number 3. According to a long-standing baseball legend, the Yankees adopted their now-iconic pinstriped uniforms in hopes of making Ruth look slimmer. In truth, though, they had been wearing pinstripes since Ruppert bought the team in 1915.
Although the Yankees started well, the Athletics soon proved they were the better team in 1929, splitting two series with the Yankees in the first month of the season, then taking advantage of a Yankee losing streak in mid-May to gain first place. Although Ruth performed well, the Yankees were not able to catch the Athletics—Connie Mack had built another great team. Tragedy struck the Yankees late in the year as manager Huggins died of erysipelas, a bacterial skin infection, on September 25, only ten days after he had last led the team. Despite past differences, Ruth praised Huggins and described him as a "great guy". The Yankees finished second, 18 games behind the Athletics. Ruth hit .345 during the season, with 46 home runs and 154 RBIs.
The Yankees hired Bob Shawkey as manager, their fourth choice. Ruth politicked for the job of player-manager, but was not seriously considered by Ruppert and Barrow; Stout deems this the first hint Ruth would have no future with the Yankees once he was done as a player. Shawkey, a former Yankees player and teammate of Ruth, was unable to command the slugger's respect. The Athletics won their second consecutive pennant and World Series, as the Yankees finished in third place, sixteen games back. During that season Ruth was asked by a reporter what he thought of his yearly salary of $80,000 being more than President Hoover's $75,000. His response was, "I know, but I had a better year than Hoover." In 1930, Ruth hit .359 with 49 home runs (his best in his years after 1928) and 153 RBIs, and pitched his first game in nine years, a complete game victory. At the end of the season, Shawkey was fired and replaced with Cubs manager Joe McCarthy, though Ruth again unsuccessfully sought the job.
McCarthy was a disciplinarian, but chose not to interfere with Ruth, and the slugger for his part did not seek conflict with the manager. The team improved in 1931, but was no match for the Athletics, who won 107 games, games in front of the Yankees. Ruth, for his part, hit .373, with 46 home runs and 163 RBIs. He had 31 doubles, his most since 1924. In the 1932 season, the Yankees went 107–47 and won the pennant. Ruth's effectiveness had decreased somewhat, but he still hit .341 with 41 home runs and 137 RBIs. Nevertheless, he twice was sidelined due to injury during the season.
The Yankees faced the Cubs, McCarthy's former team, in the 1932 World Series. There was bad blood between the two teams as the Yankees resented the Cubs only awarding half a World Series share to Mark Koenig, a former Yankee. The games at Yankee Stadium had not been sellouts; both were won by the home team, with Ruth collecting two singles, but scoring four runs as he was walked four times by the Cubs pitchers. In Chicago, Ruth was resentful at the hostile crowds that met the Yankees's train and jeered them at the hotel. The crowd for Game Three included New York Governor Franklin D. Roosevelt, the Democratic candidate for president, who sat with Chicago Mayor Anton Cermak. Many in the crowd threw lemons at Ruth, a sign of derision, and others (as well as the Cubs themselves) shouted abuse at Ruth and other Yankees. They were briefly silenced when Ruth hit a three-run home run off Charlie Root in the first inning, but soon revived, and the Cubs tied the score at 4–4 in the fourth inning. When Ruth came to the plate in the top of the fifth, the Chicago crowd and players, led by pitcher Guy Bush, were screaming insults at Ruth. With the count at two balls and one strike, Ruth gestured, possibly in the direction of center field, and after the next pitch (a strike), may have pointed there with one hand. Ruth hit the fifth pitch over the center field fence; estimates were that it traveled nearly . Whether or not Ruth intended to indicate where he planned to (and did) hit the ball, the incident has gone down in legend as Babe Ruth's called shot. The Yankees won Game Three, and the following day clinched the Series with another victory. During that game, Bush hit Ruth on the arm with a pitch, causing words to be exchanged and provoking a game-winning Yankee rally.
Ruth remained productive in 1933, as he batted .301, with 34 home runs, 103 RBIs, and a league-leading 114 walks, as the Yankees finished second, seven games behind the Senators. He was selected to play right field by Athletics manager Connie Mack in the first Major League Baseball All-Star Game, held on July 6, 1933, at Comiskey Park in Chicago. He hit the first home run in the All-Star Game's history, a two-run blast against Bill Hallahan during the third inning, which helped the AL win the game 4–2. During the final game of the 1933 season, as a publicity stunt organized by his team, Ruth was called upon and pitched a complete game victory against the Red Sox, his final appearance as a pitcher. Despite unremarkable pitching numbers, Ruth had a 5–0 record in five games for the Yankees, raising his career totals to 94–46.
In 1934, Ruth played in his last full season. By this time, years of high living were starting to catch up with him. His conditioning had deteriorated to the point that he could no longer field or run. He accepted a pay cut from Ruppert to $35,000, but was still the highest-paid player in the major leagues. He could still handle a bat, recording a .288 batting average with 22 home runs, statistics Reisler described as "merely mortal". Ruth was selected to the AL All-Star team for the second consecutive year. During the game, New York Giants pitcher Carl Hubbell struck out Ruth and four other future Hall-of-Famers consecutively. The Yankees finished second again, seven games behind the Tigers.
Boston Braves (1935).
Although Ruth knew he was nearly finished as a player, he desired to remain in baseball as a manager. He was often spoken of as a possible candidate as managerial jobs opened up, but in 1932, when he was mentioned as a contender for the Red Sox position, stated that he was not yet ready to leave the field. There were rumors that Ruth was a likely candidate each time when the Cleveland Indians, Cincinnati Reds, and Detroit Tigers were looking for a manager, but nothing came of them.
Just before the 1934 season, Ruppert offered to make Ruth manager of the Yankees' top minor-league team, the Newark Bears, but he was talked out of it by his wife, Claire (Helen had died in 1929), and his business manager, Christy Walsh. Shortly afterward, Tigers owner Frank Navin made a proposal to Ruppert and Barrow—if the Yankees traded Ruth to Detroit, Navin would name Ruth player-manager. Navin believed Ruth would not only bring a winning attitude to a team that hadn't finished higher than third since 1923, but would also revive the Tigers' sagging attendance figures. Navin asked Ruth to come to Detroit for an interview. However, Ruth balked, since Walsh had already arranged for him to take part in a celebrity golf tournament in Hawaii. Ruth and Navin negotiated over the phone while Ruth was in Hawaii, but those talks foundered when Navin refused to give Ruth a portion of the Tigers' box office proceeds.
Early in the 1934 season, Ruth began openly campaigning to become manager of the Yankees. However, the Yankee job was never a serious possibility. Ruppert always supported McCarthy, who would remain in his position for another 12 seasons. Ruth and McCarthy's relationship had been lukewarm at best, and Ruth's managerial ambitions further chilled their relations. By the end of the season, Ruth hinted that he would retire unless Ruppert named him manager of the Yankees. For his part, Ruppert wanted his slugger to leave the team without drama and hard feelings when the time came.
During the 1934–35 offseason, Ruth circled the world with his wife, including a barnstorming tour of the Far East. At his final stop before returning home, in the United Kingdom, Ruth was introduced to cricket by Australian player Alan Fairfax, and after having little luck in a cricketer's stance, stood as a baseball batter and launched some massive shots around the field, destroying the bat in the process. Although Fairfax regretted that he could not have the time to make Ruth a cricket player, Ruth had lost any interest in such a career upon learning that the best batsmen made only about $40 per week.
Also during the offseason, Ruppert had been sounding out the other clubs in hopes of finding one that would be willing to take Ruth as a manager and/or a player. However, the only serious offer came from Athletics owner-manager Connie Mack, who gave some thought to stepping down as manager in favor of Ruth. However, Mack later dropped the idea, saying that Ruth's wife would be running the team in a month if Ruth ever took over.
While the barnstorming tour was under way, Ruppert began negotiating with Boston Braves owner Judge Emil Fuchs, who wanted Ruth as a gate attraction. Although the Braves had enjoyed modest recent success, finishing fourth in the National League in both 1933 and 1934, the team performed poorly at the box office. Unable to afford the rent at Braves Field, Fuchs had considered holding dog races there when the Braves were not at home, only to be turned down by Landis. After a series of phone calls, letters, and meetings, the Yankees traded Ruth to the Braves on February 26, 1935. Ruppert had stated that he would not release Ruth to go to another team as a full-time player. For this reason, it was announced that Ruth would become a team vice president and would be consulted on all club transactions, in addition to playing. He was also made assistant manager to Braves skipper Bill McKechnie. In a long letter to Ruth a few days before the press conference, Fuchs promised Ruth a share in the Braves' profits, with the possibility of becoming co-owner of the team. Fuchs also raised the possibility of Ruth succeeding McKechnie as manager, perhaps as early as 1936. Ruppert called the deal "the greatest opportunity Ruth ever had".
There was considerable attention as Ruth reported for spring training. He did not hit his first home run of the spring until after the team had left Florida, and was beginning the road north in Savannah. He hit two in an exhibition against the Bears. Amid much press attention, Ruth played his first home game in Boston in over 16 years. Before an opening-day crowd of over 25,000, including five of New England's six state governors, Ruth accounted for all of the Braves' runs in a 4–2 defeat of the New York Giants, hitting a two-run home run, singling to drive in a third run and later in the inning scoring the fourth. Although age and weight had slowed him, he made a running catch in left field which sportswriters deemed the defensive highlight of the game.
Ruth had two hits in the second game of the season, but it quickly went downhill both for him and the Braves from there. The season soon settled down to a routine of Ruth performing poorly on the few occasions he even played at all, and the Braves losing most games. As April passed into May, Ruth's deterioration became even more pronounced. While he remained productive at the plate early on, he could do little else. His condition had deteriorated to the point that he could barely trot around the bases. His fielding had become so poor that three Braves pitchers told McKechnie that they would not take the mound if he was in the lineup. Before long, Ruth stopped hitting as well. He grew increasingly annoyed that McKechnie ignored most of his advice. For his part, McKechnie later said that Ruth's huge salary and refusal to stay with the team while on the road made it nearly impossible to enforce discipline.
Ruth soon realized that Fuchs had deceived him, and had no intention of making him manager or giving him any significant off-field duties. He later stated that his only duties as vice president consisted of making public appearances and autographing tickets. Ruth also found out that far from giving him a share of the profits, Fuchs wanted him to invest some of "his" money in the team in a last-ditch effort to improve its balance sheet. As it turned out, both Fuchs and Ruppert had known all along that Ruth's non-playing positions were meaningless.
By the end of the first month of the season, Ruth concluded he was finished even as a part-time player. As early as May 12, he asked Fuchs to let him retire. Ultimately, Fuchs persuaded Ruth to remain at least until after the Memorial Day doubleheader in Philadelphia. In the interim was a western road trip, at which the rival teams had scheduled days to honor him. In Chicago and St. Louis, Ruth performed poorly, and his batting average sank to .155, with only three home runs. In the first two games in Pittsburgh, Ruth had only one hit, though a long fly caught by Paul Waner probably would have been a home run in any other ballpark besides Forbes Field.
Ruth played in the third game of the Pittsburgh series on May 25, 1935, and added one more tale to his playing legend. Ruth went 4-for-4, including three home runs, though the Braves lost the game 11–7. The last two were off Ruth's old Cubs nemesis, Guy Bush. The final home run, both of the game and of Ruth's career, sailed over the upper deck in right field and out of the ballpark, the first time anyone had hit a fair ball completely out of Forbes Field. Ruth was urged to make this his last game, but he had given his word to Fuchs and played in Cincinnati and Philadelphia. The first game of the doubleheader in Philadelphia—the Braves lost both—was his final major league appearance. On June 2, after an argument with Fuchs, Ruth retired. He finished 1935 with a .181 average—easily his worst as a full-time position player—and the final six of his 714 home runs. The Braves, 10–27 when Ruth left, finished 38–115, at .248 the worst winning percentage in modern National League history. Insolvent like his team, Fuchs gave up control of the Braves before the end of the season; the National League took over the franchise at the end of the year.
Retirement.
1935–46.
Although Fuchs had given Ruth his unconditional release, no major league team expressed an interest in hiring him in any capacity. Ruth still hoped to be hired as a manager if he could not play anymore, but only one managerial position, Cleveland, became available between Ruth's retirement and the end of the 1937 season. Asked if he had considered Ruth for the job, Indians owner Alva Bradley replied negatively.
The writer Creamer believed Ruth was unfairly treated in never being given an opportunity to manage a major league club. The author believed there was not necessarily a relationship between personal conduct and managerial success, noting that McGraw, Billy Martin, and Bobby Valentine were winners despite character flaws. Team owners and general managers assessed Ruth's flamboyant personal habits as a reason to exclude him from a managerial job; Barrow said of him, "How can he manage other men when he can't even manage himself?"
Ruth played much golf and in a few exhibition baseball games, demonstrating a continuing ability to draw large crowds. This appeal contributed to the Dodgers hiring him as first base coach in 1938. But Brooklyn general manager Larry MacPhail made it clear when Ruth was hired that he would not be considered for the manager's job if, as expected, Burleigh Grimes retired at the end of the season. Although much was said about what Ruth could teach the younger players, in practice, his duties were to appear on the field in uniform and encourage base runners—he was not called upon to relay signs. He got along well with everyone except team captain Leo Durocher, who was hired as Grimes' replacement at season's end. Ruth returned to retirement, never again to work in baseball.
On July 4, 1939, Ruth spoke on Lou Gehrig Appreciation Day at Yankee Stadium as members of the 1927 Yankees and a sellout crowd turned out to honor the first baseman, forced into premature retirement by ALS disease, which would kill him in two years. The next week, Ruth went to Cooperstown, New York, for the formal opening of the Baseball Hall of Fame. Three years earlier he was one of the first five players elected to it. As radio broadcasts of baseball became popular, Ruth sought a job in that field, arguing that his celebrity and knowledge of baseball would assure large audiences, but he received no offers. During World War II, he made many personal appearances to advance the war effort, including his last appearance as a player at Yankee Stadium, in a 1943 exhibition for the Army–Navy Relief Fund. He hit a long fly ball off Walter Johnson; the blast left the field, curving foul, but Ruth circled the bases anyway. In 1946, he made a final effort to gain a job in baseball, contacting new Yankees boss MacPhail, but was sent a rejection letter.
Cancer and death (1946–48).
As early as the war years, doctors had cautioned Ruth to take better care of his health, and he grudgingly followed their advice, limiting his drinking and not going on a proposed trip to support the troops in the South Pacific. In 1946, Ruth began experiencing severe pain over his left eye, and had difficulty swallowing. In November 1946, he entered French Hospital in New York for tests, which revealed Ruth had an inoperable malignant tumor at the base of his skull and in his neck. It was a lesion known as nasopharyngeal carcinoma, or "lymphoepithelioma." His name and fame gave him access to experimental treatments, and he was one of the first cancer patients to receive both drugs and radiation treatment simultaneously. He was discharged from the hospital in February, having lost , and went to Florida to recuperate. He returned to New York and Yankee Stadium after the season started. The new commissioner, Happy Chandler (Judge Landis had died in 1944), proclaimed April 27, 1947, Babe Ruth Day around the major leagues, with the most significant observance to be in the Bronx. A number of teammates and others spoke in honor of Ruth, who briefly addressed the crowd of almost 60,000.
Around this time, developments in chemotherapy offered some hope. The doctors had not told Ruth that he had cancer because of his family's fear that he might do himself harm. They treated him with teropterin, a folic acid derivative; he may have been the first human subject. Ruth showed dramatic improvement during the summer of 1947, so much so that his case was presented by his doctors at a scientific meeting, without using his name. He was able to travel around the country, doing promotional work for the Ford Motor Company on American Legion Baseball. He appeared again at another day in his honor at Yankee Stadium in September, but was not well enough to pitch in an old-timers game as he had hoped.
The improvement was only a temporary remission, and by late 1947, Ruth was unable to help with the writing of his autobiography, "The Babe Ruth Story", which was almost entirely ghostwritten. In and out of the hospital in New York, he left for Florida in February 1948, doing what activities he could. After six weeks he returned to New York to appear at a book-signing party. He also traveled to California to witness the filming of the book.
On June 5, 1948, a "gaunt and hollowed out" Ruth visited Yale University to donate a manuscript of "The Babe Ruth Story" to its library. On June 13, Ruth visited Yankee Stadium for the final time in his life, appearing at the 25th anniversary celebrations of "The House that Ruth Built". By this time he had lost much weight and had difficulty walking. Introduced along with his surviving teammates from 1923, Ruth used a bat as a cane. Nat Fein's photo of Ruth taken from behind, standing near home plate and facing "Ruthville" (right field) became one of baseball's most famous and widely circulated photographs, and won the Pulitzer Prize.
Ruth made one final trip on behalf of American Legion Baseball, then entered Memorial Hospital, where he would die. He was never told he had cancer, but before his death, had surmised it. He was able to leave the hospital for a few short trips, including a final visit to Baltimore. On July 26, 1948, Ruth left the hospital to attend the premiere of the film "The Babe Ruth Story". Shortly thereafter, Ruth returned to the hospital for the final time. He was barely able to speak. Ruth's condition gradually became worse; only a few visitors were allowed to see him, one of whom was National League president and future Commissioner of Baseball Ford Frick. "Ruth was so thin it was unbelievable. He had been such a big man and his arms were just skinny little bones, and his face was so haggard", Frick said years later.
Thousands of New Yorkers, including many children, stood vigil outside the hospital in Ruth's final days. On August 16, 1948, at 8:01 pm, Ruth died in his sleep at the age of 53. Instead of a wake at a funeral home, his casket was taken to Yankee Stadium, where it remained for two days; 77,000 people filed past to pay him tribute. His funeral Mass took place at St. Patrick's Cathedral; a crowd estimated at 75,000 waited outside. Ruth was buried on a hillside in Section 25 at the Gate of Heaven Cemetery in Hawthorne, New York. An epitaph by Cardinal Spellman appears on his headstone. His second wife, Claire Merritt Ruth, would be interred with him 28 years later in 1976.
Personal life.
Ruth met Helen Woodford (1897–1929), by some accounts, in a coffee shop in Boston where she was a waitress, and they were married on October 17, 1914; he was 19 and she was 17. They adopted a daughter, Dorothy (1921–1989), in 1921. Ruth and Helen separated around 1925, reportedly due to his repeated infidelities. Their last public appearance together came during the 1926 World Series. Helen died at age 31 in a house fire in Watertown, Massachusetts, in January 1929, in a house owned by Edward Kinder, a dentist with whom she had been living as "Mrs. Kinder". In her book, "My Dad, the Babe", Dorothy claimed that she was Ruth's biological child by a mistress named Juanita Jennings. She died in 1989. 
On April 17, 1929, Ruth married actress and model Claire Merritt Hodgson (1897–1976) and adopted her daughter Julia; he was 34 and she was 31. By one account, Julia and Dorothy were, through no fault of their own, the reason for the seven-year rift in Ruth's relationship with teammate Lou Gehrig. Sometime in 1932, Gehrig's mother, during a conversation which she assumed was private, remarked, "It's a shame lair doesn't dress Dorothy as nicely as she dresses her own daughter." When the comment inevitably got back to Ruth, he angrily told Gehrig to tell his mother to mind her own business. Gehrig in turn took offense at what he perceived as Ruth's comment about his mother. The two men reportedly never spoke off the field until they reconciled at Yankee Stadium on Lou Gehrig Appreciation Day in 1939.
Although he was married through most of his baseball career, when Colonel Huston asked Ruth to tone down his lifestyle, the player said, "I'll promise to go easier on drinking and to get to bed earlier, but not for you, fifty thousand dollars, or two-hundred and fifty thousand dollars will I give up women. They're too much fun."
Memorial and museum.
On April 19, 1949, the Yankees unveiled a granite monument in Ruth's honor in center field of Yankee Stadium. The monument was located in the field of play next to a flagpole and similar tributes to Huggins and Gehrig until the stadium was remodeled from 1974–1975, which resulted in the outfield fences moving inward and enclosing the monuments from the playing field. This area was known thereafter as Monument Park. Yankee Stadium, "the House that Ruth Built", was replaced after the 2008 season with a new Yankee Stadium across the street from the old one; Monument Park was subsequently moved to the new venue behind the center field fence. Ruth's uniform number 3 has been retired by the Yankees, and he is one of five Yankees players or managers to have a granite monument within the stadium.
The Babe Ruth Birthplace Museum is located at 216 Emory Street, a Baltimore row house where Ruth was born, and three blocks west of Oriole Park at Camden Yards, where the AL's Baltimore Orioles play. The property was restored and opened to the public in 1973 by the non-profit Babe Ruth Birthplace Foundation, Inc. Ruth's widow, Claire, his two daughters, Dorothy and Julia, and his sister, Mamie, helped select and install exhibits for the museum.
Contemporary impact.
Ruth was the first baseball star to be the subject of overwhelming interest by the public. Baseball had developed star players before, such as Cobb and "Shoeless Joe" Jackson, but both men had uneasy relations with fans, in Cobb's case sometimes marked by violence. Ruth's biographers agree that he benefited from the timing of his ascension to "Home Run King", with an America hit hard by both the war and the 1918 flu pandemic longing for something to help put these traumas behind it. He also resonated in a country which felt, in the aftermath of the war, that it took second place to no one. Montville argues that as a larger-than-life figure capable of unprecedented athletic feats in the nation's largest city, Ruth became an icon of the significant social changes which marked the early 1920s. Glenn Stout notes in his history of the Yankees, "Ruth was New York incarnate—uncouth and raw, flamboyant and flashy, oversized, out of scale, and absolutely unstoppable".
Ruth became such a symbol of the United States during his lifetime that during World War II, Japanese soldiers yelled in English, "To hell with Babe Ruth", to anger American soldiers. (Ruth replied that he hoped that "every Jap that mention my name gets shot"). Creamer recorded that "Babe Ruth transcended sport, moved far beyond the artificial limits of baselines and outfield fences and sports pages". Wagenheim stated, "He appealed to a deeply rooted American yearning for the definitive climax: clean, quick, unarguable." According to Glenn Stout, "Ruth's home runs were exalted, uplifting experience that meant more to fans than any runs they were responsible for. A Babe Ruth home run was an event unto itself, one that meant anything was possible."
Ruth's penchant for hitting home runs altered how baseball is played. Prior to 1920, home runs were unusual, and managers tried to win games by getting a runner on base and bringing him around to score through such means as the stolen base, the bunt, and the hit and run. Advocates of what was dubbed "inside baseball", such as Giants manager McGraw, disliked the home run, considering it a blot on the purity of the game. According to sportswriter W. A. Phelon after the 1920 season, Ruth's breakout performance that season and the response in excitement and attendance, "settled, for all time to come, that the American public is nuttier over the Home Run than the Clever Fielding or the Hitless Pitching. Viva el Home Run and two times viva Babe Ruth, exponent of the home run, and overshadowing star." Bill James noted, "When the owners discovered that the fans "liked" to see home runs, and when the foundations of the games were simultaneously imperiled by disgrace n the Black Sox Scanda, then there was no turning back." While a few, such as McGraw and Cobb, decried the passing of the old-style play, teams quickly began to seek and develop sluggers.
According to contemporary sportswriter Grantland Rice, only two sports figures of the 1920s approached Ruth in popularity—boxer Jack Dempsey and racehorse Man o' War. One of the factors that contributed to Ruth's broad appeal was the uncertainty about his family and early life. Ruth appeared to exemplify the American success story, that even an uneducated, unsophisticated youth, without any family wealth or connections, can do something better than anyone else in the world. Montville notes that "the fog urrounding his childhoo will make him forever accessible, universal. He will be the patron saint of American possibility." Similarly, the fact that Ruth played when a relatively small portion of his fans had the opportunity to see him play, in the era before television coverage of baseball, allowed his legend to grow through word of mouth and the hyperbole of sports reporters. Reisler notes that recent sluggers who surpassed Ruth's 60-home run mark, such as Mark McGwire and Barry Bonds, generated much less excitement than when Ruth repeatedly broke the single-season home run record in the 1920s; Ruth dominated a relatively small sports world, while Americans of the present era have many sports available to watch.
Legacy.
Creamer termed Ruth "a unique figure in the social history of the United States". Ruth has even entered the language: a dominant figure in a field, whether within or outside sports, is often referred to as "the Babe Ruth" of that field. Similarly, "Ruthian" has come to mean in sports, "colossal, dramatic, prodigious, magnificent; with great power."
More books, Montville noted in 2006, have been written about Ruth than about any other member of the Baseball Hall of Fame. At least five of these books (including Creamer's and Wagenheim's) were written in 1973 and 1974, timed to capitalize on the increase in public interest in Ruth as Henry Aaron approached his career home run mark, which he broke on April 8, 1974. Aaron stated as he approached Ruth's record, "I can't remember a day this year or last when I did not hear the name of Babe Ruth."
Montville suggests that Ruth is probably even more popular today than he was when his career home run record was broken by Aaron. The longball era that Ruth started continues in baseball, to the delight of the fans. Owners build ballparks to encourage home runs, which are featured on "SportsCenter" and "Baseball Tonight" each evening during the season. The questions of performance-enhancing drug use, which have dogged recent home run hitters such as McGwire and Bonds, do nothing to diminish Ruth's reputation; his overindulgences with beer and hot dogs seem part of a simpler time.
Ruth has been named the greatest baseball player of all time in various surveys and rankings. In 1998, "The Sporting News" ranked him number one on the list of "Baseball's 100 Greatest Players". In 1999, baseball fans named Ruth to the Major League Baseball All-Century Team. He was named baseball's Greatest Player Ever in a ballot commemorating the 100th anniversary of professional baseball, in 1969. The Associated Press reported in 1993 that Muhammad Ali was tied with Babe Ruth as the most recognized athletes in America. In a 1999 ESPN poll, he was ranked as the second-greatest U.S. athlete of the century, behind Michael Jordan. In 1983, the United States Postal Service honored Ruth with the issuance of a twenty-cent stamp.
One long-term survivor of the craze over Ruth may be the Baby Ruth candy bar. The original company to market the confectionery, the Curtis Candy Company, maintained that the bar was named after Ruth Cleveland, daughter of former president Grover Cleveland. She died in 1904 and the bar was first marketed in 1921, at the height of the craze over the slugger. The slugger later sought to market candy bearing his name; he was refused a trademark because of the Baby Ruth bar. Corporate files from 1921 are no longer extant; the brand has changed hands several times and is now owned by the Nestlé company. The Ruth estate licensed his likeness for use in an advertising campaign for Baby Ruth in 1995. Due to a marketing arrangement, in 2005, the Baby Ruth bar became the official candy bar of Major League Baseball.
Montville notes the continuing relevance of Babe Ruth in American culture, over three-quarters of a century after he last swung a bat in a major league game:
 
External links.
 

</doc>
<doc id="4177" url="https://en.wikipedia.org/wiki?curid=4177" title="Barge">
Barge

A barge is a flat-bottomed boat, built mainly for river and canal transport of heavy goods. Some barges are not self-propelled and need to be towed or pushed by towboats. Canal barges, towed by draft animals on an adjacent towpath, contended with the railway in the early industrial revolution, but were outcompeted in the carriage of high-value items due to the higher speed, falling costs, and route flexibility of rail.
Etymology.
"Barge" is attested from 1300, from Old French "barge", from Vulgar Latin "barga". The word originally could refer to any small boat; the modern meaning arose around 1480. "Bark" "small ship" is attested from 1420, from Old French "barque", from Vulgar Latin "barca" (400 AD). The more precise meaning "three-masted ship" arose in the 17th century, and often takes the French spelling for disambiguation. Both are probably derived from the Latin "barica", from Greek "baris" "Egyptian boat", from Coptic "bari" "small boat", hieroglyphic Egyptian and similar "ba-y-r" for "basket-shaped boat". By extension, the term "embark" literally means to board the kind of boat called a "barque".
The long pole used to maneuver or propel a barge has given rise to the saying "I wouldn't touch that ubject/thin with a barge pole."
On the Great British canal system, the term 'barge' is used to describe a boat wider than a narrowboat, and the people who move barges are often known as lightermen. In the United States, deckhands perform the labor and are supervised by a leadman or the mate. The captain and pilot steer the towboat, which pushes one or more barges held together with rigging, collectively called 'the tow'. The crew live aboard the towboat as it travels along the inland river system or the intracoastal waterways. These towboats travel between ports and are also called line-haul boats.
Types.
Poles are used on barges to fend off the barge as it nears other vessels or a wharf. These are often called 'pike poles'. On shallow canals in the United Kingdom, long punt poles are used to manoeuvre or propel the barge.
Modern use.
Barges are used today for low-value bulk items, as the cost of hauling goods by barge is very low. Barges are also used for very heavy or bulky items; a typical American barge measures , and can carry up to about of cargo. The most common European barge measures and can carry up to about .
As an example, on June 26, 2006, a catalytic cracking unit reactor was shipped by barge from the Tulsa Port of Catoosa in Oklahoma to a refinery in Pascagoula, Mississippi. Extremely large objects are normally shipped in sections and assembled onsite, but shipping an assembled unit reduced costs and avoided reliance on construction labor at the delivery site (which in this case was still recovering from Hurricane Katrina). Of the reactor's journey, only about were traveled overland, from the final port to the refinery.
Self-propelled barges may be used as such when traveling downstream or upstream in placid waters; they are operated as an unpowered barge, with the assistance of a tugboat, when traveling upstream in faster waters. Canal barges are usually made for the particular canal in which they will operate.
Many barges, primarily Dutch Barges, which were originally designed for carrying cargo along the canals of Europe, are no longer large enough to compete in this industry with larger newer vessels. Many of these barges have been renovated and are now used as luxury Hotel Barges carrying holiday makers along the same canals on which they once carried grain or coal.
Towed or otherwise unpowered barges in the United States.
In primitive regions today and in all pre-development (lacking highways or railways) regions worldwide in times before industrial development and highways, barges were the predominant and most efficient means of inland transportation in many regions. This holds true today, for many areas of the world.
In such pre-industrialized, or poorly developed infrastructure regions, many barges are purpose-designed to be powered on waterways by long slender poles — thereby becoming known on American waterways as poleboats as the extensive west of North America was settled using the vast tributary river systems of the Mississippi drainage basin. Poleboats use muscle power of "walkers" along the sides of the craft pushing a pole against the streambed, canal, or lake bottom to move the vessel where desired. In settling the American west it was generally faster to navigate downriver from Brownsville, Pennsylvania, to the Ohio River confluence with the Mississippi and then pole upriver against the current to St. Louis than to travel overland on the rare primitive dirt roads for many decades after the American revolution.
Once the New York Central and Pennsylvania Railroads reached Chicago, that time dynamic changed, and American poleboats became less common, relegated to smaller rivers and more remote streams. On the Mississippi riverine system today, including that of other sheltered waterways, industrial barge trafficking in bulk raw materials such as coal, coke, timber, iron ore and other minerals is extremely common; in the developed world using huge cargo barges that connect in groups and trains-of-barges in ways that allow cargo volumes and weights considerably greater than those used by pioneers of modern barge systems and methods in the Victorian era.
Such barges need to be towed by tugboats or pushed by towboats. Canal barges, towed by draft animals on a waterway adjacent towpath were of fundamental importance in the early industrial revolution, whose major early engineering projects were efforts to build viaducts, aqueducts and especially canals to fuel and feed raw materials to nascent factories in the early industrial takeoff and take their goods to ports and cities for distribution.
The barge and canal system contended favorably with the railways in the early industrial revolution before around the 1850s–1860s — for example, the Erie Canal in New York State is credited by economic historians with giving the growth boost needed for New York City to eclipse Philadelphia as America's largest port and city — but such canal systems with their locks, need for maintenance and dredging, pumps and sanitary issues were eventually outcompeted in the carriage of high-value items by the railways due to the higher speed, falling costs, and route flexibility of rail transport. Barge and canal systems were nonetheless of great, perhaps even primary, economic importance until after World War I in Europe, particularly in the more developed nations of the Low Countries, France, Germany, Poland, and especially Great Britain which more or less made the system characteristically its own.

</doc>
<doc id="4178" url="https://en.wikipedia.org/wiki?curid=4178" title="Bill Schelter">
Bill Schelter

William Frederick Schelter (1947 – July 30, 2001) was a professor of mathematics at The University of Texas at Austin and a Lisp developer and programmer. Schelter is credited with the development of the GNU Common Lisp (GCL) implementation of Common Lisp and the GPL'd version of the computer algebra system Macsyma called Maxima. Schelter authored Austin Kyoto Common Lisp (AKCL) under contract with IBM. AKCL formed the foundation for Axiom, another computer algebra system. AKCL eventually became GNU Common Lisp. He is also credited with the first port of the GNU C compiler to the Intel 386 architecture, used in the original implementation of the Linux kernel.
Schelter obtained his Ph.D. at McGill University in 1972. His mathematical specialties were noncommutative ring theory and computational algebra and its applications, including automated theorem proving in geometry.
In the summer of 2001, age 54, he died suddenly of a heart attack while traveling in Russia.

</doc>
<doc id="4179" url="https://en.wikipedia.org/wiki?curid=4179" title="British English">
British English

British English is the English language as spoken and written in Great Britain or, more broadly, throughout the British Isles. Slight regional variations exist in formal, written English in the United Kingdom. For example, the adjective "wee" is almost exclusively used in parts of Scotland and Northern Ireland, whereas "little" is predominant elsewhere. Nevertheless, there is a meaningful degree of uniformity in "written" English within the United Kingdom, and this could be described by the term "British English". The forms of "spoken" English, however, vary considerably more than in most other areas of the world where English is spoken, so a uniform concept of British English is more difficult to apply to the spoken language. According to Tom McArthur in the "Oxford Guide to World English", British English shares "all the ambiguities and tensions in the word "British" and as a result can be used and interpreted in two ways, more broadly or more narrowly, within a range of blurring and ambiguity."
When distinguished from American English, the term "British English" is sometimes used broadly as a synonym for "Commonwealth English", the general dialect of English spoken amongst the former British colonies exclusive of the particular regionalisms of, for example, Australian or Canadian English.
History.
English is a West Germanic language that originated from the Anglo-Frisian dialects brought to Britain by Germanic settlers from various parts of what is now northwest Germany and the northern Netherlands. The resident population at this time was generally speaking Common Brittonic—the insular variety of continental Celtic, which was influenced by the Roman occupation. This group of languages (Welsh, Cornish, Cumbric) cohabited alongside English into the modern period, but due to their remoteness from the Germanic languages, influence on English was notably limited. However, the degree of influence remains debated, and it has recently been argued that its grammatical influence accounts for the substantial innovations noted between English and the other West Germanic languages. Initially, Old English was a diverse group of dialects, reflecting the varied origins of the Anglo-Saxon Kingdoms of England. One of these dialects, Late West Saxon, eventually came to dominate. The original Old English language was then influenced by two waves of invasion: the first was by speakers of the Scandinavian branch of the Germanic family, who conquered and colonised parts of Britain in the 8th and 9th centuries; the second was the Normans in the 11th century, who spoke Old Norman and ultimately developed an English variety of this called Anglo-Norman. These two invasions caused English to become "mixed" to some degree (though it was never a truly mixed language in the strictest sense of the word; mixed languages arise from the cohabitation of speakers of different languages, who develop a hybrid tongue for basic communication).
The more idiomatic, concrete and descriptive English is, the more it is from Anglo-Saxon origins. The more intellectual and abstract English is, the more it contains Latin and French influences (e.g. pig is the animal bred by the occupied Anglo-Saxons and pork is the animal eaten by the occupying Normans).
Cohabitation with the Scandinavians resulted in a significant grammatical simplification and lexical enrichment of the Anglo-Frisian core of English; the later Norman occupation led to the grafting onto that Germanic core of a more elaborate layer of words from the Romance branch of the European languages. This Norman influence entered English largely through the courts and government. Thus, English developed into a "borrowing" language of great flexibility and with a huge vocabulary.
Dialects.
Dialects and accents vary amongst the four countries of the United Kingdom, as well as within the countries themselves.
The major divisions are normally classified as English English (or English as spoken in England, which encompasses Southern English dialects, West Country dialects, East and West Midlands English dialects and Northern English dialects), Welsh English (not to be confused with the Welsh language), Irish English and Scottish English (not to be confused with the Scots language). The various British dialects also differ in the words that they have borrowed from other languages.
Following its last major survey of English Dialects (1949–1950), the University of Leeds has started work on a new project. In May 2007 the Arts and Humanities Research Council awarded a grant to a team led by Sally Johnson, Professor of Linguistics and Phonetics at Leeds University, to study British regional dialects.
Johnson's team are sifting through a large collection of examples of regional slang words and phrases turned up by the "Voices project" run by the BBC, in which they invited the public to send in examples of English still spoken throughout the country. The BBC Voices project also collected hundreds of news articles about how the British speak English from swearing through to items on language schools. This information will also be collated and analysed by Johnson's team both for content and for where it was reported. "Perhaps the most remarkable finding in the Voices study is that the English language is as diverse as ever, despite our increased mobility and constant exposure to other accents and dialects through TV and radio." Work by the team on this project is not expected to end before 2010. When covering the award of the grant on 1 June 2007, "The Independent" stated:
Regional.
The form of English most commonly associated with the upper class in the southern counties of England is called Received Pronunciation (RP). It derives from a mixture of the Midland and Southern dialects spoken in London in the early modern period and is frequently used as a model for teaching English to foreign learners. Although speakers from elsewhere in England may not speak with an RP accent, it is now a class dialect more than a local dialect. It may also be referred to as "the Queen's (or King's) English", "Public School English", "Posh" or "BBC English" as this was originally the form of English used on radio and television, although a wider variety of accents can be heard these days. About 2% of Britons speak RP, and it has evolved quite markedly over the last 40 years.
In the South East there are significantly different accents; the Cockney accent spoken by some East Londoners is strikingly different from RP. The Cockney rhyming slang can be (and was initially intended to be) difficult for outsiders to understand, although the extent of its use is often somewhat exaggerated.
Estuary English has been gaining prominence in recent decades: it has some features of RP and some of Cockney. In London itself, the broad local accent is still changing, partly influenced by Caribbean speech. Immigrants to the UK in recent decades have brought many more languages to the country. Surveys started in 1979 by the Inner London Education Authority discovered over 100 languages being spoken domestically by the families of the inner city's schoolchildren. As a result, Londoners speak with a mixture of accents, depending on ethnicity, neighbourhood, class, age, upbringing, and sundry other factors.
Since the mass internal immigration to Northamptonshire in the 1940s and its position between several major accent regions, it has become a source of various accent developments. In Northampton the older accent has been influenced by overspill Londoners. There is an accent known locally as the Kettering accent, which is a transitional accent between the East Midlands and East Anglian. It is the last southern midland accent to use the broad "a" in words like "bath"/"grass" (i.e. barth/grarss). Conversely "crass"/"plastic" use a slender "a". A few miles northwest in Leicestershire the slender "a" becomes more widespread generally. In the town of Corby, five miles (8 km) north, one can find Corbyite, which unlike the Kettering accent, is largely influenced by the West Scottish accent.
In addition, most British people can to some degree temporarily "swing" their accent towards a more neutral form of English at will, to reduce difficulty where very different accents are involved, or when speaking to foreigners.
Glottal stop.
In a number of forms of spoken British English, it is common for the phoneme to be realised as a glottal stop when it is in the intervocalic position, in a process called T-glottalisation. Once regarded as a Cockney feature, it has become much more widespread. It is still stigmatised when used in words like "later", but becoming very widespread at the end of words such as "not" (as in no interested). Other consonants subject to this usage in Cockney English are "p", as in paer and "k" as in baer.
Standardisation.
As with English around the world, the English language as used in the United Kingdom is governed by convention rather than formal code: there is no body equivalent to the Académie française or the Real Academia Española. Dictionaries (for example, "Oxford English Dictionary", "Longman Dictionary of Contemporary English", "Chambers Dictionary", "Collins Dictionary") record usage rather than attempting to prescribe it. In addition, vocabulary and usage change with time: words are freely borrowed from other languages and other strains of English, and neologisms are frequent.
For historical reasons dating back to the rise of London in the 9th century, the form of language spoken in London and the East Midlands became standard English within the Court, and ultimately became the basis for generally accepted use in the law, government, literature and education in Britain. To a considerable extent, modern British spelling was standardised in Samuel Johnson's "A Dictionary of the English Language" (1755), although previous writers had also played a significant role in this and much has changed since 1755. Scotland, which underwent parliamentary union with England only in 1707, still has a few independent standards, especially within its separate legal system.
Since the early 20th century, British authors have produced numerous books intended as guides to English grammar and usage, a few of which have achieved sufficient acclaim to have remained in print for long periods and to have been reissued in new editions after some decades. These include, most notably of all, Fowler's "Modern English Usage" and "The Complete Plain Words" by Sir Ernest Gowers. Detailed guidance on many aspects of writing British English for publication is included in style guides issued by various publishers including "The Times" newspaper, the Oxford University Press and the Cambridge University Press. The Oxford University Press guidelines were originally drafted as a single broadsheet page by Horace Henry Hart, and were at the time (1893) the first guide of their type in English; they were gradually expanded and eventually published, first as "Hart's Rules", and in 2002 as part of "The Oxford Manual of Style". Comparable in authority and stature to "The Chicago Manual of Style" for published American English, the Oxford Manual is a fairly exhaustive standard for published British English that writers can turn to in the absence of specific guidance from their publishing house.
Notes.
Citations

</doc>
<doc id="4181" url="https://en.wikipedia.org/wiki?curid=4181" title="Battle">
Battle

A battle is a combat in warfare between two or more armed forces, or combatants. A war sometimes consists of many battles. Battles generally are well defined in duration, area, and force commitment.
Wars and military campaigns are guided by strategy, whereas battles take place on a level of planning and execution known as operational mobility. German strategist Carl von Clausewitz stated that "the employment of battles ... to achieve the object of war" was the essence of strategy.
Etymology.
Battle is a loanword from the Old French "bataille", first attested in 1297, from Late Latin "battualia", meaning "exercise of soldiers and gladiators in fighting and fencing", from Late Latin (taken from Germanic) "battuere" "beat", from which the English word battery is also derived via Middle English "batri".
Characteristics.
The defining characteristic of the fight as a concept in Military science has been a dynamic one through the course of military history, changing with the changes in the organisation, employment and technology of military forces.
While the English military historian Sir John Keegan suggested an ideal definition of battle as "something which happens between two armies leading to the moral then physical disintegration of one or the other of them", the origins and outcomes of battles can rarely be summarized so neatly.
In general a battle during the 20th century was, and continues to be, defined by the combat between opposing forces representing major components of total forces committed to a military campaign, used to achieve specific military objectives. Where the duration of the battle is longer than a week, it is often for reasons of staff operational planning called an "operation". Battles can be planned, encountered, or forced by one force on the other when the latter is unable to withdraw from combat.
A battle always has as its purpose the reaching of a mission goal by use of military force. A victory in the battle is achieved when one of the opposing sides forces the other to abandon its mission, or to surrender its forces, or routs the other, i.e., forces it to retreat or renders it militarily ineffective for further combat operations. However, a battle may end in a Pyrrhic victory, which ultimately favors the defeated party. If no resolution is reached in a battle, it can result in a stalemate. A conflict in which one side is unwilling to reach a decision by a direct battle using conventional warfare often becomes an insurgency.
Until the 19th century the majority of battles were of short duration, many lasting a part of a day. (The Battle of Nations (1813) and the Battle of Gettysburg (1863) were exceptional in lasting three days.) This was mainly due to the difficulty of supplying armies in the field, or conducting night operations. The means of prolonging a battle was typically by employment of siege warfare. Improvements in transportation and the sudden evolving of trench warfare, with its siege-like nature during World War I in the 20th century, lengthened the duration of battles to days and weeks. This created the requirement for unit rotation to prevent combat fatigue, with troops preferably not remaining in a combat area of operations for more than a month. Trench warfare had become largely obsolete in conflicts between advanced armies by the start of the Second World War.
The use of the term "battle" in military history has led to its misuse when referring to almost any scale of combat, notably by strategic forces involving hundreds of thousands of troops that may be engaged in either a single battle at one time (Battle of Leipzig) or multiple operations (Battle of Kursk). The space a battle occupies depends on the range of the weapons of the combatants. A "battle" in this broader sense may be of long duration and take place over a large area, as in the case of the Battle of Britain or the Battle of the Atlantic. Until the advent of artillery and aircraft, battles were fought with the two sides within sight, if not reach, of each other. The depth of the battlefield has also increased in modern warfare with inclusion of the supporting units in the rear areas; supply, artillery, medical personnel etc. often outnumber the front-line combat troops.
Battles are, on the whole, made up of a multitude of individual combats, skirmishes and small engagements within the context of which the combatants will usually only experience a small part of the events of the battle's entirety. To the infantryman, there may be little to distinguish between combat as part of a minor raid or as a major offensive, nor is it likely that he anticipates the future course of the battle; few of the British infantry who went over the top on the first day on the Somme, July 1, 1916, would have anticipated that they would be fighting the same battle in five months' time. Conversely, some of the Allied infantry who had just dealt a crushing defeat to the French at the Battle of Waterloo fully expected to have to fight again the next day (at the Battle of Wavre).
Battlespace.
Battlespace is a unified strategy to integrate and combine armed forces for the military theatre of operations, including air, information, land, sea and space. It includes the environment, factors and conditions that must be understood to successfully apply combat power, protect the force, or complete the mission. This includes enemy and friendly armed forces; facilities; weather; terrain; and the electromagnetic spectrum within the operational areas and areas of interest.
Factors.
Battles are decided by various factors. The number and quality of combatants and equipment, the skill of the commanders of each army, and the terrain advantages are among the most prominent factors. A unit may charge with high morale but less discipline and still emerge victorious. This tactic was effectively used by the early French Revolutionary Armies.
Weapons and armour can be a decisive factor. On many occasions armies have achieved victories largely owing to the employment of more advanced weapons than those of their opponents. An extreme example was in the Battle of Omdurman, in which a large army of Sudanese Mahdists armed in a traditional manner were destroyed by an Anglo-Egyptian force equipped with Maxim guns.
On some occasions, simple weapons employed in an unorthodox fashion have proven advantageous, as with the Swiss pikemen who gained many victories through their ability to transform a traditionally defensive weapon into an offensive one. Likewise, the Zulus in the early 19th century were victorious in battles against their rivals in part because they adopted a new kind of spear, the iklwa. Even so, forces with inferior weapons have still emerged victorious at times, for example in the Wars of Scottish Independence and in the First Italo–Ethiopian War. Discipline within the troops is often of greater importance; at the Battle of Alesia, the Romans were greatly outnumbered but won because of superior training. 
Battles can also be determined by terrain. Capturing high ground, for example, has been the central strategy in innumerable battles. An army that holds the high ground forces the enemy to climb, and thus wear themselves down. Areas of dense vegetation, such as jungles and forest, act as force-multipliers, of benefit to inferior armies. Arguably, terrain is of less importance in modern warfare, due to the advent of aircraft, though terrain is still vital for camouflage, especially for guerrilla warfare.
Generals and commanders also play a decisive role during combat. Hannibal, Julius Caesar, Khalid ibn Walid and Napoleon Bonaparte were all skilled generals and, consequently, their armies were extremely successful. An army that can trust the commands of their leaders with conviction in its success invariably has a higher morale than an army that doubts its every move. The British in the naval Battle of Trafalgar, for example, owed its success to the reputation of celebrated admiral Lord Nelson.
Types.
Battles can be fought on land, at sea and, in the modern age, in the air. Naval battles have occurred since before the 5th century BC. Air battles have been far less common, due to their late conception, the most prominent being the Battle of Britain in 1940. However, since the Second World War land or sea battles have come to rely on air support. Indeed, during the Battle of Midway, five aircraft carriers were sunk without either fleet coming into direct contact.
There are numerous types of battles:
Battles frequently do not fit one particular type perfectly, and are usually hybrids of different types listed above.
A "decisive battle" is one of particular importance; often by bringing hostilities to an end, such as the Battle of Hastings or the Battle of Hattin, or as a turning point in the fortunes of the belligerents, such as the Battle of Stalingrad. A decisive battle can have political as well as military impact, changing the balance of power or boundaries between countries. The concept of the "decisive battle" became popular with the publication in 1851 of Edward Creasy's "The Fifteen Decisive Battles of the World". British military historians J.F.C. Fuller ("The Decisive Battles of the Western World") and B.H. Liddell Hart ("Decisive Wars of History"), among many others, have written books in the style of Creasy's work.
Land.
There is an obvious difference in the way battles have been fought throughout time. Early battles were probably fought between rival hunting bands as disorganized mobs. However, during the Battle of Megiddo, the first reliably documented battle in the fifteenth century BC, actual discipline was instilled in both armies. However, during the many wars of the Roman Empire, barbarians continued using mob tactics.
As the Age of Enlightenment dawned, armies began to fight in highly disciplined lines. Each would follow the orders from their officers and fight as a single unit instead of individuals. Each army was successively divided into regiments, battalions, companies, and platoons. These armies would march, line up, and fire in divisions.
Native Americans, on the other hand, did not fight in lines, utilizing instead guerrilla tactics. American colonists and European forces continued using disciplined lines, continuing into the American Civil War.
A new style, during World War I, known as trench warfare, developed nearly half a century later. This also led to radio for communication between battalions. Chemical warfare also emerged with the use of poisonous gas during World War I.
By World War II, the use of the smaller divisions, platoons and companies became much more important as precise operations became vital. Instead of the locked trench warfare of World War I, during World War II, a dynamic network of battles developed where small groups encountered other platoons. As a result, elite squads became much more recognized and distinguishable.
Maneuver warfare also developed with an astonishing pace with the advent of the tank, replacing the archaic cannons of the Enlightenment Age. Artillery has since gradually replaced the use of frontal troops. Modern battles now continue to resemble those of World War II, though prominent innovations have been added. Indirect combat through the use of aircraft and missiles now constitutes a large portion of wars in place of battles, where battles are now mostly reserved for capturing cities.
Naval.
One significant difference of modern naval battles as opposed to earlier forms of combat is the use of marines, which introduced amphibious warfare. Today, a marine is actually an infantry regiment that sometimes fights solely on land and is no longer tied to the navy. A good example of an old naval battle is the Battle of Salamis.
Most ancient naval battles were fought by fast ships using the battering ram to sink opposing fleets or steer close enough for boarding in hand-to-hand combat. Troops were often used to storm enemy ships as used by Romans and pirates. This tactic was usually used by civilizations that could not beat the enemy with ranged weaponry.
Another invention in the late Middle Ages was the use of Greek fire by the Byzantines, which was used to set enemy fleets on fire. Empty demolition ships utilized the tactic to crash into opposing ships and set it afire with an explosion. After the invention of cannons, naval warfare became useful as support units for land warfare.
During the 19th century, the development of mines led to a new type of naval warfare. The ironclad, first used in the American Civil War, resistant to cannons, soon made the wooden ship obsolete. The invention of military submarines, during World War I, brought naval warfare to both above and below the surface. With the development of military aircraft during World War II, battles were fought in the sky as well as below the ocean. Aircraft carriers have since become the central unit in naval warfare, acting as a mobile base for lethal aircraft.
Aerial.
Although the use of aircraft has for the most part always been used as a supplement to land or naval engagements, since their first major military use in World War I aircraft have increasingly taken on larger roles in warfare. During World War I, the primary use was for reconnaissance, and small-scale bombardment.
Aircraft began becoming much more prominent in the Spanish Civil War and especially World War II. Aircraft design began specializing, primarily into two types: bombers, which carried explosive payloads to bomb land targets or ships; and fighter-interceptors, which were used to either intercept incoming aircraft or to escort and protect bombers (engagements between fighter aircraft were known as dog fights). Some of the more notable aerial battles in this period include the Battle of Britain and the Battle of Midway.
Another important use of aircraft came with the development of the helicopter, which first became heavily used during the Vietnam War, and still continues to be widely used today to transport and augment ground forces.
Today, direct engagements between aircraft are rare – the most modern fighter-interceptors carry much more extensive bombing payloads, and are used to bomb precision land targets, rather than to fight other aircraft. Anti-aircraft batteries are used much more extensively to defend against incoming aircraft than interceptors. Despite this, aircraft today are much more extensively used as the primary tools for both army and navy, as evidenced by the prominent use of helicopters to transport and support troops, the use of aerial bombardment as the "first strike" in many engagements, and the replacement of the battleship with the aircraft carrier as the center of most modern navies.
Naming.
Battles are usually named after some feature of the battlefield geography, such as the name of a town, forest or river, commonly prefixed "Battle of...". Occasionally battles are named after the date on which they took place, such as The Glorious First of June.
In the Middle Ages it was considered important to settle on a suitable name for a battle which could be used by the chroniclers. For example, after Henry V of England defeated a French army on October 25, 1415, he met with the senior French herald and they agreed to name the battle after the nearby castle and so it was called the Battle of Agincourt.
In other cases, the sides adopted different names for the same battle, such as the Battle of Gallipoli which is known in Turkey as the Battle of Çanakkale. During the American Civil War, the Union tended to name the battles after the nearest watercourse, such as the Battle of Wilsons Creek and the Battle of Stones River, whereas the Confederates favoured the nearby towns, as in the Battles of Chancellorsville and Murfreesboro. Occasionally both names for the same battle entered the popular culture, such as the First and Second Battle of Bull Run, which are also referred to as the First and Second Battle of Manassas.
Sometimes in desert warfare, there is no nearby town name to use; map coordinates gave the name to the Battle of 73 Easting in the First Gulf War.
Some place names have become synonymous with the battles that took place there, such as the Passchendaele, Pearl Harbor, the Alamo, Thermopylae, or Waterloo. Military operations, many of which result in battle, are given codenames, which are not necessarily meaningful or indicative of the type or the location of the battle. Operation Market Garden and Operation Rolling Thunder are examples of battles known by their military codenames.
When a battleground is the site of more than one battle in the same conflict, the instances are distinguished by ordinal number, such as the First and Second Battles of Bull Run. An extreme case are the twelve Battles of the Isonzo—First to Twelfth—between Italy and Austria-Hungary during the First World War.
Some battles are named for the convenience of military historians so that periods of combat can be neatly distinguished from one another. Following the First World War, the British Battles Nomenclature Committee was formed to decide on standard names for all battles and subsidiary actions. To the soldiers who did the fighting, the distinction was usually academic; a soldier fighting at Beaumont Hamel on November 13, 1916 was probably unaware he was taking part in what the committee would call the "Battle of the Ancre".
Many combats are too small to merit a name. Terms such as "action", "skirmish", "firefight", "raid" or "offensive patrol" are used to describe small-scale battle-like encounters. These combats often take place within the time and space of a battle and while they may have an objective, they are not necessarily "decisive". Sometimes the soldiers are unable to immediately gauge the significance of the combat; in the aftermath of the Battle of Waterloo, some British officers were in doubt as to whether the day's events merited the title of "battle" or would be passed off as merely an "action".
Effects.
Battles affect the individuals who take part, as well as the political actors. Personal effects of battle range from mild psychological issues to permanent and crippling injuries. Some battle-survivors have nightmares about the conditions they encountered, or abnormal reactions to certain sights or sounds. Some suffer flashbacks. Physical effects of battle can include scars, amputations, lesions, loss of bodily functions, blindness, paralysis — and death.
Battles also affect politics. A decisive battle can cause the losing side to surrender, while a Pyrrhic Victory such as the Battle of Asculum can cause the winning side to reconsider its long-term goals. Battles in civil wars have often decided the fate of monarchs or political factions. Famous examples include the War of the Roses, as well as the Jacobite Uprisings. Battles also affect the commitment of one side or the other to the continuance of a war, for example the Battle of Incheon and the Battle of Hue during the Tet Offensive.

</doc>
<doc id="4182" url="https://en.wikipedia.org/wiki?curid=4182" title="Berry Berenson">
Berry Berenson

Berinthia "Berry" Berenson Perkins (April 14, 1948 – September 11, 2001) was an American photographer, actress, and model. Perkins, who was the widow of actor Anthony Perkins, died in the September 11 attacks as a passenger on American Airlines Flight 11.
Early life.
Berenson was born in Murray Hill, Manhattan. Her father, Robert Lawrence Berenson, was an American career diplomat turned shipping executive; he was of Lithuanian Jewish descent, and his family's original surname was Valvrojenski. Her mother was born Maria-Luisa Yvonne Radha de Wendt de Kerlor, better known as Gogo Schiaparelli, a socialite of Italian, Swiss, French, and Egyptian ancestry.
Her maternal grandmother was the Italian-born fashion designer Elsa Schiaparelli, and her maternal grandfather was Wilhelm de Wendt de Kerlor, a Theosophist and psychic medium. Her elder sister, Marisa Berenson, became a well-known model and actress. She also was a great-grandniece of Giovanni Schiaparelli, an Italian astronomer who believed he had discovered the supposed canals of Mars, and a second cousin, once removed, of art expert Bernard Berenson (1865–1959) and his sister Senda Berenson (1868–1954), an athlete and educator who was one of the first two women elected to the Basketball Hall of Fame.
Career.
Following a brief modeling career in the late 1960s, Berenson became a freelance photographer. By 1973, her photographs had been published in "Life", "Glamour", "Vogue" and "Newsweek".
She also appeared in several motion pictures, including "Cat People" with Malcolm McDowell. She starred opposite Anthony Perkins in the 1978 Alan Rudolph film "Remember My Name" and opposite Jeff Bridges in the 1979 film "Winter Kills".
Personal life and death.
On August 9, 1973, in Cape Cod, Massachusetts, Berenson married her "Remember My Name" costar Anthony Perkins. They had two sons: actor-musician Oz Perkins (born February 2, 1974) and folk/rock recording artist Elvis Perkins (born February 9, 1976). They remained married until Perkins' death from AIDS-related complications on September 12, 1992.
Berenson died at age 53 in the September 11 attacks aboard American Airlines Flight 11, one day before the ninth anniversary of Perkins' death. She was returning to her California home following a holiday on Cape Cod.
At the National September 11 Memorial & Museum, Berenson is memorialized at the North Pool, on Panel N-76.

</doc>
<doc id="4183" url="https://en.wikipedia.org/wiki?curid=4183" title="Botany">
Botany

Botany, also called plant science(s) or plant biology, is the science of plant life and a branch of biology. A botanist or plant scientist is a scientist who specializes in this field of study. The term "botany" comes from the Ancient Greek word ("botanē") meaning "pasture", "grass", or "fodder"; is in turn derived from ("boskein"), "to feed" or "to graze". Traditionally, botany has also included the study of fungi and algae by mycologists and phycologists respectively, with the study of these three groups of organisms remaining within the sphere of interest of the International Botanical Congress. Nowadays, botanists study approximately 400,000 species of living organisms of which some 260,000 species are vascular plants and about 248,000 are flowering plants.
Botany originated in prehistory as herbalism with the efforts of early humans to identify – and later cultivate – edible, medicinal and poisonous plants, making it one of the oldest branches of science. Medieval physic gardens, often attached to monasteries, contained plants of medical importance. They were forerunners of the first botanical gardens attached to universities, founded from the 1540s onwards. One of the earliest was the Padua botanical garden. These gardens facilitated the academic study of plants. Efforts to catalogue and describe their collections were the beginnings of plant taxonomy, and led in 1753 to the binomial system of Carl Linnaeus that remains in use to this day.
In the 19th and 20th centuries, new techniques were developed for the study of plants, including methods of optical microscopy and live cell imaging, electron microscopy, analysis of chromosome number, plant chemistry and the structure and function of enzymes and other proteins. In the last two decades of the 20th century, botanists exploited the techniques of molecular genetic analysis, including genomics and proteomics and DNA sequences to classify plants more accurately.
Modern botany is a broad, multidisciplinary subject with inputs from most other areas of science and technology. Research topics include the study of plant structure, growth and differentiation, reproduction, biochemistry and primary metabolism, chemical products, development, diseases, evolutionary relationships, systematics, and plant taxonomy. Dominant themes in 21st century plant science are molecular genetics and epigenetics, which are the mechanisms and control of gene expression during differentiation of plant cells and tissues. Botanical research has diverse applications in providing staple foods, materials such as timber, oil, rubber, fibre and drugs, in modern horticulture, agriculture and forestry, plant propagation, breeding and genetic modification, in the synthesis of chemicals and raw materials for construction and energy production, in environmental management, and the maintenance of biodiversity.
History.
Early botany.
Botany originated as herbalism, the study and use of plants for their medicinal properties. The early recorded history of botany includes many ancient writings and plant classifications. Examples of early botanical works have been found in ancient texts from India dating back to before 1100 BC, in archaic Avestan writings, and in works from China before it was unified in 221 BC.
Modern botany traces its roots back to Ancient Greece, specifically to Theophrastus (c. 371–287 BC), a student of Aristotle who invented and described many of its principles and is widely regarded in the scientific community as the "Father of Botany". His major works, "Enquiry into Plants" and "On the Causes of Plants", constitute the most important contributions to botanical science until the Middle Ages, almost seventeen centuries later.
Another work from Ancient Greece that made an early impact on botany is "De Materia Medica", a five-volume encyclopedia about herbal medicine written in the middle of the first century by Greek physician and pharmacologist Pedanius Dioscorides. "De Materia Medica" was widely read for more than 1,500 years. Important contributions from the medieval Muslim world include Ibn Wahshiyya's "Nabatean Agriculture", Abū Ḥanīfa Dīnawarī's (828–896) the "Book of Plants", and Ibn Bassal's "The Classification of Soils". In the early 13th century, Abu al-Abbas al-Nabati, and Ibn al-Baitar (d. 1248) wrote on botany in a systematic and scientific manner.
In the mid-16th century, "botanical gardens" were founded in a number of Italian universities – the Padua botanical garden in 1545 is usually considered to be the first which is still in its original location. These gardens continued the practical value of earlier "physic gardens", often associated with monasteries, in which plants were cultivated for medical use. They supported the growth of botany as an academic subject. Lectures were given about the plants grown in the gardens and their medical uses demonstrated. Botanical gardens came much later to northern Europe; the first in England was the University of Oxford Botanic Garden in 1621. Throughout this period, botany remained firmly subordinate to medicine.
German physician Leonhart Fuchs (1501–1566) was one of "the three German fathers of botany", along with theologian Otto Brunfels (1489–1534) and physician Hieronymus Bock (1498–1554) (also called Hieronymus Tragus). Fuchs and Brunfels broke away from the tradition of copying earlier works to make original observations of their own. Bock created his own system of plant classification.
Physician Valerius Cordus (1515–1544) authored a botanically and pharmacologically important herbal "Historia Plantarum" in 1544 and a pharmacopoeia of lasting importance, the "Dispensatorium" in 1546. Naturalist Conrad von Gesner (1516–1565) and herbalist John Gerard (1545–c. 1611) published herbals covering the medicinal uses of plants. Naturalist Ulisse Aldrovandi (1522–1605) was considered the "father of natural history", which included the study of plants. In 1665, using an early microscope, Polymath Robert Hooke discovered cells, a term he coined, in cork, and a short time later in living plant tissue.
Early modern botany.
During the 18th century, systems of plant identification were developed comparable to dichotomous keys, where unidentified plants are placed into taxonomic groups (e.g. family, genus and species) by making a series of choices between pairs of characters. The choice and sequence of the characters may be artificial in keys designed purely for identification (diagnostic keys) or more closely related to the natural or phyletic order of the taxa in synoptic keys. By the 18th century, new plants for study were arriving in Europe in increasing numbers from newly discovered countries and the European colonies worldwide. In 1753 Carl von Linné (Carl Linnaeus) published his Species Plantarum, a hierarchical classification of plant species that remains the reference point for modern botanical nomenclature. This established a standardised binomial or two-part naming scheme where the first name represented the genus and the second identified the species within the genus. For the purposes of identification, Linnaeus's "Systema Sexuale" classified plants into 24 groups according to the number of their male sexual organs. The 24th group, "Cryptogamia", included all plants with concealed reproductive parts, mosses, liverworts, ferns, algae and fungi.
Increasing knowledge of plant anatomy, morphology and life cycles led to the realisation that there were more natural affinities between plants than the artificial sexual system of Linnaeus had indicated. Adanson (1763), de Jussieu (1789), and Candolle (1819) all proposed various alternative natural systems of classification that grouped plants using a wider range of shared characters and were widely followed. The Candollean system reflected his ideas of the progression of morphological complexity and the later classification by Bentham and Hooker, which was influential until the mid-19th century, was influenced by Candolle's approach. Darwin's publication of the "Origin of Species" in 1859 and his concept of common descent required modifications to the Candollean system to reflect evolutionary relationships as distinct from mere morphological similarity.
Botany was greatly stimulated by the appearance of the first "modern" text book, Matthias Schleiden's "", published in English in 1849 as "Principles of Scientific Botany". Schleiden was a microscopist and an early plant anatomist who co-founded the cell theory with Theodor Schwann and Rudolf Virchow and was among the first to grasp the significance of the cell nucleus that had been described by Robert Brown in 1831.
In 1855, Adolf Fick formulated Fick's laws that enabled the calculation of the rates of molecular diffusion in biological systems.
Modern botany.
Building upon the gene-chromosome theory of heredity that originated with Gregor Mendel (1822–1884), August Weismann (1834–1914) proved that inheritance only takes place through gametes. No other cells can pass on inherited characters. The work of Katherine Esau (1898–1997) on plant anatomy is still a major foundation of modern botany. Her books "Plant Anatomy" and "Anatomy of Seed Plants" have been key plant structural biology texts for more than half a century.
The discipline of plant ecology was pioneered in the late 19th century by botanists such as Eugenius Warming, who produced the hypothesis that plants form communities, and his mentor and successor Christen C. Raunkiær whose system for describing plant life forms is still in use today. The concept that the composition of plant communities such as temperate broadleaf forest changes by a process of ecological succession was developed by Henry Chandler Cowles, Arthur Tansley and Frederic Clements. Clements is credited with the idea of climax vegetation as the most complex vegetation that an environment can support and Tansley introduced the concept of ecosystems to biology. Building on the extensive earlier work of Alphonse de Candolle, Nikolai Vavilov (1887–1943) produced accounts of the biogeography, centres of origin, and evolutionary history of economic plants.
Particularly since the mid-1960s there have been advances in understanding of the physics of plant physiological processes such as transpiration (the transport of water within plant tissues), the temperature dependence of rates of water evaporation from the leaf surface and the molecular diffusion of water vapour and carbon dioxide through stomatal apertures. These developments, coupled with new methods for measuring the size of stomatal apertures, and the rate of photosynthesis have enabled precise description of the rates of gas exchange between plants and the atmosphere. Innovations in statistical analysis by Ronald Fisher, Frank Yates and others at Rothamsted Experimental Station facilitated rational experimental design and data analysis in botanical research. The discovery and identification of the auxin plant hormones by Kenneth V. Thimann in 1948 enabled regulation of plant growth by externally applied chemicals. Frederick Campion Steward pioneered techniques of micropropagation and plant tissue culture controlled by plant hormones. The synthetic auxin 2,4-Dichlorophenoxyacetic acid or 2,4-D was one of the first commercial synthetic herbicides.
20th century developments in plant biochemistry have been driven by modern techniques of organic chemical analysis, such as spectroscopy, chromatography and electrophoresis. With the rise of the related molecular-scale biological approaches of molecular biology, genomics, proteomics and metabolomics, the relationship between the plant genome and most aspects of the biochemistry, physiology, morphology and behaviour of plants can be subjected to detailed experimental analysis. The concept originally stated by Gottlieb Haberlandt in 1902 that all plant cells are totipotent and can be grown "in vitro" ultimately enabled the use of genetic engineering experimentally to knock out a gene or genes responsible for a specific trait, or to add genes such as GFP that report when a gene of interest is being expressed. These technologies enable the biotechnological use of whole plants or plant cell cultures grown in bioreactors to synthesise pesticides, antibiotics or other pharmaceuticals, as well as the practical application of genetically modified crops designed for traits such as improved yield.
Modern morphology recognizes a continuum between the major morphological categories of root, stem (caulome), leaf (phyllome) and trichome. Furthermore, it emphasizes structural dynamics. Modern systematics aims to reflect and discover phylogenetic relationships between plants. Modern Molecular phylogenetics largely ignores morphological characters, relying on DNA sequences as data. Molecular analysis of DNA sequences from most families of flowering plants enabled the Angiosperm Phylogeny Group to publish in 1998 a phylogeny of flowering plants, answering many of the questions about relationships among angiosperm families and species. The theoretical possibility of a practical method for identification of plant species and commercial varieties by DNA barcoding is the subject of active current research.
Scope and importance.
The study of plants is vital because they underpin almost all animal life on Earth by generating a large proportion of the oxygen and food that provide humans and other organisms with aerobic respiration with the chemical energy they need to exist. Plants, algae and cyanobacteria are the major groups of organisms that carry out photosynthesis, a process that uses the energy of sunlight to convert water and carbon dioxide into sugars that can be used both as a source of chemical energy and of organic molecules that are used in the structural components of cells. As a by-product of photosynthesis, plants release oxygen into the atmosphere, a gas that is required by nearly all living things to carry out cellular respiration. In addition, they are influential in the global carbon and water cycles and plant roots bind and stabilise soils, preventing soil erosion. Plants are crucial to the future of human society as they provide food, oxygen, medicine, and products for people, as well as creating and preserving soil.
Historically, all living things were classified as either animals or plants and botany covered the study of all organisms not considered animals. Botanists examine both the internal functions and processes within plant organelles, cells, tissues, whole plants, plant populations and plant communities. At each of these levels, a botanist may be concerned with the classification (taxonomy), phylogeny and evolution, structure (anatomy and morphology), or function (physiology) of plant life.
The strictest definition of "plant" includes only the "land plants" or embryophytes, which include seed plants (gymnosperms, including the pines, and flowering plants) and the free-sporing cryptogams including ferns, clubmosses, liverworts, hornworts and mosses. Embryophytes are multicellular eukaryotes descended from an ancestor that obtained its energy from sunlight by photosynthesis. They have life cycles with alternating haploid and diploid phases. The sexual haploid phase of embryophytes, known as the gametophyte, nurtures the developing diploid embryo sporophyte within its tissues for at least part of its life, even in the seed plants, where the gametophyte itself is nurtured by its parent sporophyte. Other groups of organisms that were previously studied by botanists include bacteria (now studied in bacteriology), fungi (mycology) – including lichen-forming fungi (lichenology), non-chlorophyte algae (phycology), and viruses (virology). However, attention is still given to these groups by botanists, and fungi (including lichens) and photosynthetic protists are usually covered in introductory botany courses.
Paleobotanists study ancient plants in the fossil record to provide information about the evolutionary history of plants. Cyanobacteria, the first oxygen-releasing photosynthetic organisms on Earth, are thought to have given rise to the ancestor of plants by entering into an endosymbiotic relationship with an early eukaryote, ultimately becoming the chloroplasts in plant cells. The new photosynthetic plants (along with their algal relatives) accelerated the rise in atmospheric oxygen started by the cyanobacteria, changing the ancient oxygen-free, reducing, atmosphere to one in which free oxygen has been abundant for more than 2 billion years.
Among the important botanical questions of the 21st century are the role of plants as primary producers in the global cycling of life's basic ingredients: energy, carbon, oxygen, nitrogen and water, and ways that our plant stewardship can help address the global environmental issues of resource management, conservation, human food security, biologically invasive organisms, carbon sequestration, climate change, and sustainability.
Human nutrition.
Virtually all staple foods come either directly from primary production by plants, or indirectly from animals that eat them. Plants and other photosynthetic organisms are at the base of most food chains because they use the energy from the sun and nutrients from the soil and atmosphere, converting them into a form that can be used by animals. This is what ecologists call the first trophic level. The modern forms of the major staple foods, such as maize, rice, wheat and other cereal grasses, pulses, bananas and plantains, as well as flax and cotton grown for their fibres, are the outcome of prehistoric selection over thousands of years from among wild ancestral plants with the most desirable characteristics. Botanists study how plants produce food and how to increase yields, for example through plant breeding, making their work important to mankind's ability to feed the world and provide food security for future generations. Botanists also study weeds, which are a considerable problem in agriculture, and the biology and control of plant pathogens in agriculture and natural ecosystems. Ethnobotany is the study of the relationships between plants and people. When applied to the investigation of historical plant–people relationships ethnobotany may be referred to as archaeobotany or palaeoethnobotany.
Plant biochemistry.
Plant biochemistry is the study of the chemical processes used by plants. Some of these processes are used in their primary metabolism like the photosynthetic Calvin cycle and crassulacean acid metabolism. Others make specialized materials like the cellulose and lignin used to build their bodies, and secondary products like resins and aroma compounds.
Plants and various other groups of photosynthetic eukaryotes collectively known as "algae" have unique organelles known as chloroplasts. Chloroplasts are thought to be descended from cyanobacteria that formed endosymbiotic relationships with ancient plant and algal ancestors. Chloroplasts and cyanobacteria contain the blue-green pigment chlorophyll "a". Chlorophyll "a" (as well as its plant and green algal-specific cousin chlorophyll "b") absorbs light in the blue-violet and orange/red parts of the spectrum while reflecting and transmitting the green light that we see as the characteristic colour of these organisms. The energy in the red and blue light that these pigments absorb is used by chloroplasts to make energy-rich carbon compounds from carbon dioxide and water by oxygenic photosynthesis, a process that generates molecular oxygen (O) as a by-product.
The light energy captured by chlorophyll "a" is initially in the form of electrons (and later a proton gradient) that's used to make molecules of ATP and NADPH which temporarily store and transport energy. Their energy is used in the light-independent reactions of the Calvin cycle by the enzyme rubisco to produce molecules of the 3-carbon sugar glyceraldehyde 3-phosphate (G3P). Glyceraldehyde 3-phosphate is the first product of photosynthesis and the raw material from which glucose and almost all other organic molecules of biological origin are synthesized. Some of the glucose is converted to starch which is stored in the chloroplast. Starch is the characteristic energy store of most land plants and algae, while inulin, a polymer of fructose is used for the same purpose in the sunflower family Asteraceae. Some of the glucose is converted to sucrose (common table sugar) for export to the rest of the plant.
Unlike in animals (which lack chloroplasts), plants and their eukaryote relatives have delegated many biochemical roles to their chloroplasts, including synthesizing all their fatty acids, and most amino acids. The fatty acids that chloroplasts make are used for many things, such as providing material to build cell membranes out of and making the polymer cutin which is found in the plant cuticle that protects land plants from drying out. 
Plants synthesize a number of unique polymers like the polysaccharide molecules cellulose, pectin and xyloglucan from which the land plant cell wall is constructed.
Vascular land plants make lignin, a polymer used to strengthen the secondary cell walls of xylem tracheids and vessels to keep them from collapsing when a plant sucks water through them under water stress. Lignin is also used in other cell types like sclerenchyma fibers that provide structural support for a plant and is a major constituent of wood. Sporopollenin is a chemically resistant polymer found in the outer cell walls of spores and pollen of land plants responsible for the survival of early land plant spores and the pollen of seed plants in the fossil record. It is widely regarded as a marker for the start of land plant evolution during the Ordovician period.
The concentration of carbon dioxide in the atmosphere today is much lower than it was when plants emerged onto land during the Ordovician and Silurian periods. Many monocots like maize and the pineapple and some dicots like the Asteraceae have since independently evolved pathways like Crassulacean acid metabolism and the carbon fixation pathway for photosynthesis which avoid the losses resulting from photorespiration in the more common carbon fixation pathway. These biochemical strategies are unique to land plants.
Medicine and materials.
Phytochemistry is a branch of plant biochemistry primarily concerned with the chemical substances produced by plants during secondary metabolism. Some of these compounds are toxins such as the alkaloid coniine from hemlock. Others, such as the essential oils peppermint oil and lemon oil are useful for their aroma, as flavourings and spices (e.g., capsaicin), and in medicine as pharmaceuticals as in opium from opium poppies. Many medicinal and recreational drugs, such as tetrahydrocannabinol (active ingredient in cannabis), caffeine, morphine and nicotine come directly from plants. Others are simple derivatives of botanical natural products. For example, the pain killer aspirin is the acetyl ester of salicylic acid, originally isolated from the bark of willow trees, and a wide range of opiate painkillers like heroin are obtained by chemical modification of morphine obtained from the opium poppy. Popular stimulants come from plants, such as caffeine from coffee, tea and chocolate, and nicotine from tobacco. Most alcoholic beverages come from fermentation of carbohydrate-rich plant products such as barley (beer), rice (sake) and grapes (wine).
Plants can synthesise useful coloured dyes and pigments such as the anthocyanins responsible for the red colour of red wine, yellow weld and blue woad used together to produce Lincoln green, indoxyl, source of the blue dye indigo traditionally used to dye denim and the artist's pigments gamboge and rose madder.
Sugar, starch, cotton, linen, hemp, some types of rope, wood and particle boards, papyrus and paper, vegetable oils, wax, and natural rubber are examples of commercially important materials made from plant tissues or their secondary products. Charcoal, a pure form of carbon made by pyrolysis of wood, has a long history as a metal-smelting fuel, as a filter material and adsorbent and as an artist's material and is one of the three ingredients of gunpowder. Cellulose, the world's most abundant organic polymer, can be converted into energy, fuels, materials and chemical feedstock. Products made from cellulose include rayon and cellophane, wallpaper paste, biobutanol and gun cotton. Sugarcane, rapeseed and soy are some of the plants with a highly fermentable sugar or oil content that are used as sources of biofuels, important alternatives to fossil fuels, such as biodiesel.
Plant ecology.
Plant ecology is the science of the functional relationships between plants and their habitats—the environments where they complete their life cycles. Plant ecologists study the composition of local and regional floras, their biodiversity, genetic diversity and fitness, the adaptation of plants to their environment, and their competitive or mutualistic interactions with other species. The goals of plant ecology are to understand the causes of their distribution patterns, productivity, environmental impact, evolution, and responses to environmental change.
Plants depend on certain edaphic (soil) and climatic factors in their environment but can modify these factors too. For example, they can change their environment's albedo, increase runoff interception, stabilize mineral soils and develop their organic content, and affect local temperature. Plants compete with other organisms in their ecosystem for resources. They interact with their neighbours at a variety of spatial scales in groups, populations and communities that collectively constitute vegetation. Regions with characteristic vegetation types and dominant plants as well as similar abiotic and biotic factors, climate, and geography make up biomes like tundra or tropical rainforest.
Herbivores eat plants, but plants can defend themselves and some species are parasitic or even carnivorous. Other organisms form mutually beneficial relationships with plants. For example, mycorrhizal fungi and rhizobia provide plants with nutrients in exchange for food, ants are recruited by ant plants to provide protection, honey bees, bats and other animals pollinate flowers and humans and other animals act as dispersal vectors to spread spores and seeds.
Plants, climate and environmental change.
Plant responses to climate and other environmental changes can inform our understanding of how these changes affect ecosystem function and productivity. For example, plant phenology can be a useful proxy for temperature in historical climatology, and the biological impact of climate change and global warming. Palynology, the analysis of fossil pollen deposits in sediments from thousands or millions of years ago allows the reconstruction of past climates. Estimates of atmospheric concentrations since the Palaeozoic have been obtained from stomatal densities and the leaf shapes and sizes of ancient land plants. Ozone depletion can expose plants to higher levels of ultraviolet radiation-B (UV-B), resulting in lower growth rates. Moreover, information from studies of community ecology, plant systematics, and taxonomy is essential to understanding vegetation change, habitat destruction and species extinction.
Genetics.
Inheritance in plants follows the same fundamental principles of genetics as in other multicellular organisms. Gregor Mendel discovered the genetic laws of inheritance by studying inherited traits such as shape in "Pisum sativum" (peas). What Mendel learned from studying plants has had far reaching benefits outside of botany. Similarly, "jumping genes" were discovered by Barbara McClintock while she was studying maize. Nevertheless, there are some distinctive genetic differences between plants and other organisms.
Species boundaries in plants may be weaker than in animals, and cross species hybrids are often possible. A familiar example is peppermint, "Mentha" × "piperita", a sterile hybrid between "Mentha aquatica" and spearmint, "Mentha spicata". The many cultivated varieties of wheat are the result of multiple inter- and intra-specific crosses between wild species and their hybrids. Angiosperms with monoecious flowers often have self-incompatibility mechanisms that operate between the pollen and stigma so that the pollen either fails to reach the stigma or fails to germinate and produce male gametes. This is one of several methods used by plants to promote outcrossing. In many land plants the male and female gametes are produced by separate individuals. These species are said to be dioecious when referring to vascular plant sporophytes and dioicous when referring to bryophyte gametophytes.
Unlike in higher animals, where parthenogenesis is rare, asexual reproduction may occur in plants by several different mechanisms. The formation of stem tubers in potato is one example. Particularly in arctic or alpine habitats, where opportunities for fertilisation of flowers by animals are rare, plantlets or bulbs, may develop instead of flowers, replacing sexual reproduction with asexual reproduction and giving rise to clonal populations genetically identical to the parent. This is one of several types of apomixis that occur in plants. Apomixis can also happen in a seed, producing a seed that contains an embryo genetically identical to the parent.
Most sexually reproducing organisms are diploid, with paired chromosomes, but doubling of their chromosome number may occur due to errors in cytokinesis. This can occur early in development to produce an autopolyploid or partly autopolyploid organism, or during normal processes of cellular differentiation to produce some cell types that are polyploid (endopolyploidy), or during gamete formation. An allopolyploid plant may result from a hybridisation event between two different species. Both autopolyploid and allopolyploid plants can often reproduce normally, but may be unable to cross-breed successfully with the parent population because there is a mismatch in chromosome numbers. These plants that are reproductively isolated from the parent species but live within the same geographical area, may be sufficiently successful to form a new species. Some otherwise sterile plant polyploids can still reproduce vegetatively or by seed apomixis, forming clonal populations of identical individuals. Durum wheat is a fertile tetraploid allopolyploid, while bread wheat is a fertile hexaploid. The commercial banana is an example of a sterile, seedless triploid hybrid. Common dandelion is a triploid that produces viable seeds by apomictic seed.
As in other eukaryotes, the inheritance of endosymbiotic organelles like mitochondria and chloroplasts in plants is non-Mendelian. Chloroplasts are inherited through the male parent in gymnosperms but often through the female parent in flowering plants.
Molecular genetics.
A considerable amount of new knowledge about plant function comes from studies of the molecular genetics of model plants such as the Thale cress, "Arabidopsis thaliana", a weedy species in the mustard family (Brassicaceae). The genome or hereditary information contained in the genes of this species is encoded by about 135 million base pairs of DNA, forming one of the smallest genomes among flowering plants. "Arabidopsis" was the first plant to have its genome sequenced, in 2000. The sequencing of some other relatively small genomes, of rice ("Oryza sativa") and "Brachypodium distachyon", has made them important model species for understanding the genetics, cellular and molecular biology of cereals, grasses and monocots generally.
Model plants such as "Arabidopsis thaliana" are used for studying the molecular biology of plant cells and the chloroplast. Ideally, these organisms have small genomes that are well known or completely sequenced, small stature and short generation times. Corn has been used to study mechanisms of photosynthesis and phloem loading of sugar in plants. The single celled green alga "Chlamydomonas reinhardtii", while not an embryophyte itself, contains a green-pigmented chloroplast related to that of land plants, making it useful for study. A red alga "Cyanidioschyzon merolae" has also been used to study some basic chloroplast functions. Spinach, peas, soybeans and a moss "Physcomitrella patens" are commonly used to study plant cell biology.
"Agrobacterium tumefaciens", a soil rhizosphere bacterium, can attach to plant cells and infect them with a callus-inducing Ti plasmid by horizontal gene transfer, causing a callus infection called crown gall disease. Schell and Van Montagu (1977) hypothesised that the Ti plasmid could be a natural vector for introducing the Nif gene responsible for nitrogen fixation in the root nodules of legumes and other plant species. Today, genetic modification of the Ti plasmid is one of the main techniques for introduction of transgenes to plants and the creation of genetically modified crops.
Epigenetics.
Epigenetics is the study of mitotically and/or meiotically heritable changes in gene function that cannot be explained by changes in the underlying DNA sequence but cause the organism's genes to behave (or "express themselves") differently. One example of epigenetic change is the marking of the genes by DNA methylation which determines whether they will be expressed or not. Gene expression can also be controlled by repressor proteins that attach to silencer regions of the DNA and prevent that region of the DNA code from being expressed. Epigenetic marks may be added or removed from the DNA during programmed stages of development of the plant, and are responsible, for example, for the differences between anthers, petals and normal leaves, despite the fact that they all have the same underlying genetic code. Epigenetic changes may be temporary or may remain through successive cell divisions for the remainder of the cell's life. Some epigenetic changes have been shown to be heritable, while others are reset in the germ cells.
Epigenetic changes in eukaryotic biology serve to regulate the process of cellular differentiation. During morphogenesis, totipotent stem cells become the various pluripotent cell lines of the embryo, which in turn become fully differentiated cells. A single fertilized egg cell, the zygote, gives rise to the many different plant cell types including parenchyma, xylem vessel elements, phloem sieve tubes, guard cells of the epidermis, etc. as it continues to divide. The process results from the epigenetic activation of some genes and inhibition of others.
Unlike animals, many plant cells, particularly those of the parenchyma, do not terminally differentiate, remaining totipotent with the ability to give rise to a new individual plant. Exceptions include highly lignified cells, the sclerenchyma and xylem which are dead at maturity, and the phloem sieve tubes which lack nuclei. While plants use many of the same epigenetic mechanisms as animals, such as chromatin remodeling, an alternative hypothesis is that plants set their gene expression patterns using positional information from the environment and surrounding cells to determine their developmental fate.
Plant Evolution.
The chloroplasts of plants have a number of biochemical, structural and genetic similarities to cyanobacteria, (commonly but incorrectly known as "blue-green algae") and are thought to be derived from an ancient endosymbiotic relationship between an ancestral eukaryotic cell and a cyanobacterial resident.
The algae are a polyphyletic group and are placed in various divisions, some more closely related to plants than others. There are many differences between them in features such as cell wall composition, biochemistry, pigmentation, chloroplast structure and nutrient reserves. The algal division Charophyta, sister to the green algal division Chlorophyta, is considered to contain the ancestor of true plants. The Charophyte class Charophyceae and the land plant sub-kingdom Embryophyta together form the monophyletic group or clade Streptophytina.
Nonvascular land plants are embryophytes that lack the vascular tissues xylem and phloem. They include mosses, liverworts and hornworts. Pteridophytic vascular plants with true xylem and phloem that reproduced by spores germinating into free-living gametophytes evolved during the Silurian period and diversified into several lineages during the late Silurian and early Devonian. Representatives of the lycopods have survived to the present day. By the end of the Devonian period, several groups, including the lycopods, sphenophylls and progymnosperms, had independently evolved "megaspory" – their spores were of two distinct sizes, larger megaspores and smaller microspores. Their reduced gametophytes developed from megaspores retained within the spore-producing organs (megasporangia) of the sporophyte, a condition known as endospory. Seeds consist of an endosporic megasporangium surrounded by one or two sheathing layers (integuments). The young sporophyte develops within the seed, which on germination splits to release it. The earliest known seed plants date from the latest Devonian Famennian stage. Following the evolution of the seed habit, seed plants diversified, giving rise to a number of now-extinct groups, including seed ferns, as well as the modern gymnosperms and angiosperms. Gymnosperms produce "naked seeds" not fully enclosed in an ovary; modern representatives include conifers, cycads, "Ginkgo", and Gnetales. Angiosperms produce seeds enclosed in a structure such as a carpel or an ovary. Ongoing research on the molecular phylogenetics of living plants appears to show that the angiosperms are a sister clade to the gymnosperms.
Plant physiology.
Plant physiology encompasses all the internal chemical and physical activities of plants associated with life. Chemicals obtained from the air, soil and water form the basis of all plant metabolism. The energy of sunlight, captured by oxygenic photosynthesis and released by cellular respiration, is the basis of almost all life. Photoautotrophs, including all green plants, algae and cyanobacteria gather energy directly from sunlight by photosynthesis. Heterotrophs including all animals, all fungi, all completely parasitic plants, and non-photosynthetic bacteria take in organic molecules produced by photoautotrophs and respire them or use them in the construction of cells and tissues. Respiration is the oxidation of carbon compounds by breaking them down into simpler structures to release the energy they contain, essentially the opposite of photosynthesis.
Molecules are moved within plants by transport processes that operate at a variety of spatial scales. Subcellular transport of ions, electrons and molecules such as water and enzymes occurs across cell membranes. Minerals and water are transported from roots to other parts of the plant in the transpiration stream. Diffusion, osmosis, and active transport and mass flow are all different ways transport can occur. Examples of elements that plants need to transport are nitrogen, phosphorus, potassium, calcium, magnesium, and sulphur. In vascular plants, these elements are extracted from the soil as soluble ions by the roots and transported throughout the plant in the xylem. Most of the elements required for plant nutrition come from the chemical breakdown of soil minerals. Sucrose produced by photosynthesis is transported from the leaves to other parts of the plant in the phloem and plant hormones are transported by a variety of processes.
Plant hormones.
Plants are not passive, but respond to external signals such as light, touch, and injury by moving or growing towards or away from the stimulus, as appropriate. Tangible evidence of touch sensitivity is the almost instantaneous collapse of leaflets of "Mimosa pudica", the insect traps of Venus flytrap and bladderworts, and the pollinia of orchids.
The hypothesis that plant growth and development is coordinated by plant hormones or plant growth regulators first emerged in the late 19th century. Darwin experimented on the movements of plant shoots and roots towards light and gravity, and concluded "It is hardly an exaggeration to say that the tip of the radicle . . acts like the brain of one of the lower animals . . directing the several movements". About the same time, the role of auxins (from the Greek auxein, to grow) in control of plant growth was first outlined by the Dutch scientist Frits Went. The first known auxin, indole-3-acetic acid (IAA), which promotes cell growth, was only isolated from plants about 50 years later. This compound mediates the tropic responses of shoots and roots towards light and gravity. The finding in 1939 that plant callus could be maintained in culture containing IAA, followed by the observation in 1947 that it could be induced to form roots and shoots by controlling the concentration of growth hormones were key steps in the development of plant biotechnology and genetic modification.
Another class of phytohormones is the jasmonates, first isolated from the oil of "Jasminum grandiflorum" which regulates wound responses in plants by unblocking the expression of genes required in the systemic acquired resistance response to pathogen attack.
In addition to being the primary energy source for plants, light functions as a signalling device, providing information to the plant, such as how much sunlight the plant receives each day. This can result in adaptive changes in a process known as photomorphogenesis. Phytochromes are the photoreceptors in a plant that are sensitive to light.
Plant anatomy and morphology.
Plant anatomy is the study of the structure of plant cells and tissues, whereas plant morphology is the study of their external form.
All plants are multicellular eukaryotes, their DNA stored in nuclei. The characteristic features of plant cells that distinguish them from those of animals and fungi include a primary cell wall composed of the polysaccharides cellulose, hemicellulose and pectin, larger vacuoles than in animal cells and the presence of plastids with unique photosynthetic and biosynthetic functions as in the chloroplasts. Other plastids contain storage products such as starch (amyloplasts) or lipids (elaioplasts). Uniquely, streptophyte cells and those of the green algal order Trentepohliales divide by construction of a phragmoplast as a template for building a cell plate late in cell division.
The bodies of vascular plants including clubmosses, ferns and seed plants (gymnosperms and angiosperms) generally have aerial and subterranean subsystems. The shoots consist of stems bearing green photosynthesising leaves and reproductive structures. The underground vascularised roots bear root hairs at their tips and generally lack chlorophyll. Non-vascular plants, the liverworts, hornworts and mosses do not produce ground-penetrating vascular roots and most of the plant participates in photosynthesis. The sporophyte generation is nonphotosynthetic in liverworts but may be able to contribute part of its energy needs by photosynthesis in mosses and hornworts.
The root system and the shoot system are interdependent – the usually nonphotosynthetic root system depends on the shoot system for food, and the usually photosynthetic shoot system depends on water and minerals from the root system. Cells in each system are capable of creating cells of the other and producing adventitious shoots or roots. Stolons and tubers are examples of shoots that can grow roots. Roots that spread out close to the surface, such as those of willows, can produce shoots and ultimately new plants. In the event that one of the systems is lost, the other can often regrow it. In fact it is possible to grow an entire plant from a single leaf, as is the case with "Saintpaulia", or even a single cell – which can dedifferentiate into a callus (a mass of unspecialised cells) that can grow into a new plant.
In vascular plants, the xylem and phloem are the conductive tissues that transport resources between shoots and roots. Roots are often adapted to store food such as sugars or starch, as in sugar beets and carrots.
Stems mainly provide support to the leaves and reproductive structures, but can store water in succulent plants such as cacti, food as in potato tubers, or reproduce vegetatively as in the stolons of strawberry plants or in the process of layering. Leaves gather sunlight and carry out photosynthesis. Large, flat, flexible, green leaves are called foliage leaves. Gymnosperms, such as conifers, cycads, "Ginkgo", and gnetophytes are seed-producing plants with open seeds. Angiosperms are seed-producing plants that produce flowers and have enclosed seeds. Woody plants, such as azaleas and oaks, undergo a secondary growth phase resulting in two additional types of tissues: wood (secondary xylem) and bark (secondary phloem and cork). All gymnosperms and many angiosperms are woody plants. Some plants reproduce sexually, some asexually, and some via both means.
Although reference to major morphological categories such as root, stem, leaf, and trichome are useful, one has to keep in mind that these categories are linked through intermediate forms so that a continuum between the categories results. Furthermore, structures can be seen as processes, that is, process combinations.
Systematic botany.
Systematic botany is part of systematic biology, which is concerned with the range and diversity of organisms and their relationships, particularly as determined by their evolutionary history. It involves, or is related to, biological classification, scientific taxonomy and phylogenetics. Biological classification is the method by which botanists group organisms into categories such as genera or species. Biological classification is a form of scientific taxonomy. Modern taxonomy is rooted in the work of Carl Linnaeus, who grouped species according to shared physical characteristics. These groupings have since been revised to align better with the Darwinian principle of common descent – grouping organisms by ancestry rather than superficial characteristics. While scientists do not always agree on how to classify organisms, molecular phylogenetics, which uses DNA sequences as data, has driven many recent revisions along evolutionary lines and is likely to continue to do so. The dominant classification system is called Linnaean taxonomy. It includes ranks and binomial nomenclature. The nomenclature of botanical organisms is codified in the International Code of Nomenclature for algae, fungi, and plants (ICN) and administered by the International Botanical Congress.
Kingdom Plantae belongs to Domain Eukarya and is broken down recursively until each species is separately classified. The order is: Kingdom; Phylum (or Division); Class; Order; Family; Genus (plural "genera"); Species. The scientific name of a plant represents its genus and its species within the genus, resulting in a single world-wide name for each organism. For example, the tiger lily is "Lilium columbianum". "Lilium" is the genus, and "columbianum" the specific epithet. The combination is the name of the species. When writing the scientific name of an organism, it is proper to capitalise the first letter in the genus and put all of the specific epithet in lowercase. Additionally, the entire term is ordinarily italicised (or underlined when italics are not available).
The evolutionary relationships and heredity of a group of organisms is called its phylogeny. Phylogenetic studies attempt to discover phylogenies. The basic approach is to use similarities based on shared inheritance to determine relationships. As an example, species of "Pereskia" are trees or bushes with prominent leaves. They do not obviously resemble a typical leafless cactus such as an "Echinocactus". However, both "Pereskia" and "Echinocactus" have spines produced from areoles (highly specialised pad-like structures) suggesting that the two genera are indeed related.
Judging relationships based on shared characters requires care, since plants may resemble one another through convergent evolution in which characters have arisen independently. Some euphorbias have leafless, rounded bodies adapted to water conservation similar to those of globular cacti, but characters such as the structure of their flowers make it clear that the two groups are not closely related. The cladistic method takes a systematic approach to characters, distinguishing between those that carry no information about shared evolutionary history – such as those evolved separately in different groups (homoplasies) or those left over from ancestors (plesiomorphies) – and derived characters, which have been passed down from innovations in a shared ancestor (apomorphies). Only derived characters, such as the spine-producing areoles of cacti, provide evidence for descent from a common ancestor. The results of cladistic analyses are expressed as cladograms: tree-like diagrams showing the pattern of evolutionary branching and descent.
From the 1990s onwards, the predominant approach to constructing phylogenies for living plants has been molecular phylogenetics, which uses molecular characters, particularly DNA sequences, rather than morphological characters like the presence or absence of spines and areoles. The difference is that the genetic code itself is used to decide evolutionary relationships, instead of being used indirectly via the characters it gives rise to. Clive Stace describes this as having "direct access to the genetic basis of evolution." As a simple example, prior to the use of genetic evidence, fungi were thought either to be plants or to be more closely related to plants than animals. Genetic evidence suggests that the true evolutionary relationship of multicelled organisms is as shown in the cladogram below – fungi are more closely related to animals than to plants.
In 1998 the Angiosperm Phylogeny Group published a phylogeny for flowering plants based on an analysis of DNA sequences from most families of flowering plants. As a result of this work, many questions, such as which families represent the earliest branches of angiosperms, have now been answered. Investigating how plant species are related to each other allows botanists to better understand the process of evolution in plants. Despite the study of model plants and increasing use of DNA evidence, there is ongoing work and discussion among taxonomists about how best to classify plants into various taxa. Technological developments such as computers and electron microscopes have greatly increased the level of detail studied and speed at which data can be analysed.

</doc>
<doc id="4184" url="https://en.wikipedia.org/wiki?curid=4184" title="Bacillus thuringiensis">
Bacillus thuringiensis

Bacillus thuringiensis (or Bt) is a Gram-positive, soil-dwelling bacterium, commonly used as a biological pesticide. "B. thuringiensis" also occurs naturally in the gut of caterpillars of various types of moths and butterflies, as well on leaf surfaces, aquatic environments, animal feces, insect-rich environments, and flour mills and grain-storage facilities.
During sporulation, many Bt strains produce crystal proteins (proteinaceous inclusions), called δ-endotoxins, that have insecticidal action. This has led to their use as insecticides, and more recently to genetically modified crops using Bt genes. Many crystal-producing Bt strains, though, do not have insecticidal properties.
Taxonomy and discovery.
"B. thuringiensis" was first discovered in 1901 by Japanese biologist Ishiwata Shigetane. In 1911, "B. thuringiensis" was rediscovered in Germany by Ernst Berliner, who isolated it as the cause of a disease called "Schlaffsucht" in flour moth caterpillars. In 1976, Robert A. Zakharyan reported the presence of a plasmid in a strain of "B. thuringiensis" and suggested the plasmid's involvement in endospore and crystal formation. "B. thuringiensis" is closely related to "B.cereus", a soil bacterium, and "B.anthracis", the cause of anthrax; the three organisms differ mainly in their plasmids. Like other members of the genus, all three are aerobes capable of producing endospores.
Subspecies.
The are several dozen recognized subspecies of bacillus thuringiensis. Subspecies commonly used as insecticides include "Bacillus thuringiensis" subspecies "kurstaki" (Btk) and subspecies "israelensis" (Bti) and subspecies "aizawa" (Bta).
Mechanism of insecticidal action.
Upon sporulation, "B. thuringiensis" forms crystals of proteinaceous insecticidal δ-endotoxins (called crystal proteins or Cry proteins), which are encoded by "cry" genes. In most strains of "B. thuringiensis", the "cry" genes are located on a plasmid ("cry" is not a chromosomal gene in most strains).
Cry toxins have specific activities against insect species of the orders Lepidoptera (moths and butterflies), Diptera (flies and mosquitoes), Coleoptera (beetles), Hymenoptera (wasps, bees, ants and sawflies) and nematodes. Thus, "B. thuringiensis" serves as an important reservoir of Cry toxins for production of biological insecticides and insect-resistant genetically modified crops. When insects ingest toxin crystals, their alkaline digestive tracts denature the insoluble crystals, making them soluble and thus amenable to being cut with proteases found in the insect gut, which liberate the toxin from the crystal. The Cry toxin is then inserted into the insect gut cell membrane, paralyzing the digestive tract and forming a pore. The insect stops eating and starves to death; live Bt bacteria may also colonize the insect which can contribute to death. The midgut bacteria of susceptible larvae may be required for "B. thuringiensis" insecticidal activity.
In 1996 another class of insecticidal proteins in Bt was discovered; the vegetative insecticidal proteins (Vip). Vip proteins do not share sequence homology with Cry proteins, in general do not compete for the same receptors, and some kill different insects than do Cry proteins.
In 2000, a novel functional group of Cry protein, designated parasporin, was discovered from noninsecticidal "B. thuringiensis" isolates. The proteins of parasporin group are defined as "B. thuringiensis" and related bacterial parasporal proteins that are not hemolytic, but capable of preferentially killing cancer cells. As of January 2013, parasporins comprise six subfamilies (PS1 to PS6).
Use of spores and proteins in pest control.
Spores and crystalline insecticidal proteins produced by "B. thuringiensis" have been used to control insect pests since the 1920s and are often applied as liquid sprays. They are now used as specific insecticides under trade names such as DiPel and Thuricide. Because of their specificity, these pesticides are regarded as environmentally friendly, with little or no effect on humans, wildlife, pollinators, and most other beneficial insects, and are used in organic farming; however, the manuals for these products do contain many environmental and human health warnings, and a 2012 European regulatory peer review of five approved strains found, while data exist to support some claims of low toxicity to humans and the environment, the data are insufficient to justify many of these claims.
New strains of Bt are developed and introduced over time as insects develop resistance to Bt, or the desire occurs to force mutations to modify organism characteristics or to use homologous recombinant genetic engineering to improve crystal size and increase pesticidal activity or broaden the host range of Bt and obtain more effective formulations. Each new strain is given a unique number and registered with the U.S. EPA and allowances may be given for genetic modification depending on "its parental strains, the proposed pesticide use pattern, and the manner and extent to which the organism has been genetically modified". Formulations of Bt that are approved for organic farming in the US are listed at the website of the Organic Materials Review Institute (OMRI) and several university extension websites offer advice on how to use Bt spore or protein preparations in organic farming.
Use of Bt genes in genetic engineering of plants for pest control.
The Belgian company Plant Genetic Systems (now part of Bayer CropScience) was the first company (in 1985) to develop genetically modified crops (tobacco) with insect tolerance by expressing "cry" genes from "B. thuringiensis"; the resulting crops contain delta endotoxin. The Bt tobacco was never commercialized; tobacco plants are used to test genetic modifications since they are easy to manipulate genetically and are not part of the food supply.
Usage.
In 1995, potato plants producing CRY 3A Bt toxin were approved safe by the Environmental Protection Agency, making it the first human-modified pesticide-producing crop to be approved in the USA, though many plants produce pesticides naturally, including tobacco, coffee plants, cocoa, and black walnut. This was the 'New Leaf' potato, and it was removed from the market in 2001 due to lack of interest. For current crops and their acreage under cultivation, see genetically modified crops.
In 1996, genetically modified maize producing Bt Cry protein was approved, which killed the European corn borer and related species; subsequent Bt genes were introduced that killed corn rootworm larvae.
The Bt genes engineered into crops and approved for release include, singly and stacked: Cry1A.105, CryIAb, CryIF, Cry2Ab, Cry3Bb1, Cry34Ab1, Cry35Ab1, mCry3A, and VIP, and the engineered crops include corn and cotton.
Corn genetically modified to produce VIP was first approved in the US in 2010.
In India, by 2014, more than seven million cotton farmers, occupying twenty-six million acres, had adopted Bt cotton.
Monsanto developed a soybean expressing Cry1Ac and the glyphosate-resistance gene for the Brazilian market, which completed the Brazilian regulatory process in 2010.
Safety studies.
The use of Bt toxins as plant-incorporated protectants prompted the need for extensive evaluation of their safety for use in foods and potential unintended impacts on the environment.
Dietary risk assessment.
Concerns over the safety of consumption of genetically-modified plant materials that contain Cry proteins have been addressed in extensive dietary risk assessment studies. While the target pests are exposed to the toxins primarily through leaf and stalk material, Cry proteins are also expressed in other parts of the plant, including trace amounts in maize kernels which are ultimately consumed by both humans and animals.
Toxicology studies.
Animal models have been used to assess human health risk from consumption of products containing Cry proteins. The United States Environmental Protection Agency recognizes mouse acute oral feeding studies where doses as high as 5,000 mg/kg body weight resulted in no observed adverse effects. Research on other known toxic proteins suggests that toxicity occurs at much lower doses, further suggesting that Bt toxins are not toxic to mammals. The results of toxicology studies are further strengthened by the lack of observed toxicity from decades of use of "B. thuringiensis" and its crystalline proteins as an insecticidal spray.
Allergenicity studies.
Introduction of a new protein raised concerns regarding the potential for allergic responses in sensitive individuals. Bioinformatic analysis of known allergens has indicated there is no concern of allergic reactions as a result of consumption of Bt toxins. Additionally, skin prick testing using purified Bt protein resulted in no detectable production of toxin-specific IgE antibodies, even in atopic patients.
Digestibility studies.
Studies have been conducted to evaluate the fate of Bt toxins that are ingested in foods. Bt toxin proteins have been shown to digest within minutes of exposure to simulated gastric fluids. The instability of the proteins in digestive fluids is an additional indication that Cry proteins are unlikely to be allergenic, since most known food allergens resist degradation and are ultimately absorbed in the small intestine.
Ecological risk assessment.
Ecological risk assessment aims to ensure there is no unintended impact on non-target organisms and no contamination of natural resources as a result of the use of a new substance, such as the use of Bt in genetically-modified crops. The impact of Bt toxins on the environments where transgenic plants are grown has been evaluated to ensure no adverse effects outside of targeted crop pests.
Persistence in environment.
Concerns over possible environmental impact from accumulation of Bt toxins from plant tissues, pollen dispersal, and direct secretion from roots have been investigated. Bt toxins may persist in soil for over 200 days, with half-lives between 1.6 and 22 days. Much of the toxin is initially degraded rapidly by microorganisms in the environment, while some is adsorbed by organic matter and persists longer. Some studies, in contrast, claim that the toxins do not persist in the soil. Bt toxins are less likely to accumulate in bodies of water, but pollen shed or soil runoff may deposit them in an aquatic ecosystem. Fish species are not susceptible to Bt toxins if exposed.
Impact on non-target organisms.
The toxic nature of Bt proteins has an adverse impact on many major crop pests, but ecological risk assessments have been conducted to ensure safety of beneficial non-target organisms that may come into contact with the toxins. Widespread concerns over toxicity in non-target Lepidopterans, such as the monarch butterfly, have been disproved through proper exposure characterization, where it was determined that non-target organisms are not exposed to high enough amounts of the Bt toxins to have an adverse effect on the population. Soil-dwelling organisms, potentially exposed to Bt toxins through root exudates, are not impacted by the growth of Bt crops.
Insect resistance.
In November 2009, Monsanto scientists found the pink bollworm had become resistant to the first-generation Bt cotton in parts of Gujarat, India - that generation expresses one Bt gene, "Cry1Ac". This was the first instance of Bt resistance confirmed by Monsanto anywhere in the world. Monsanto immediately responded by introducing a second-generation cotton with multiple Bt proteins, which was rapidly adopted. Bollworm resistance to first-generation Bt cotton was also identified in Australia, China, Spain, and the United States.
Secondary pests.
Several studies have documented surges in "sucking pests" (which are not affected by Bt toxins) within a few years of adoption of Bt cotton. In China, the main problem has been with mirids, which have in some cases "completely eroded all benefits from Bt cotton cultivation". The increase in sucking pests depended on local temperature and rainfall conditions and increased in half the villages studied. The increase in insecticide use for the control of these secondary insects was far smaller than the reduction in total insecticide use due to Bt cotton adoption. Another study in five provinces in China found the reduction in pesticide use in Bt cotton cultivars is significantly lower than that reported in research elsewhere, consistent with the hypothesis suggested by recent studies that more pesticide sprayings are needed over time to control emerging secondary pests, such as aphids, spider mites, and lygus bugs.
Similar problems have been reported in India, with both mealy bugs and aphids although a survey of small Indian farms between 2002 and 2008 concluded Bt cotton adoption has led to higher yields and lower pesticide use, decreasing over time.
Controversies.
There are controversies around GMOs on several levels, including whether making them is ethical, whether food produced with them is safe, whether such food should be labeled and if so how, whether agricultural biotech is needed to address world hunger now or in the future, and more specifically to GM crops—intellectual property and market dynamics; environmental effects of GM crops; and GM crops' role in industrial agricultural more generally. There are also issues specific to Bt transgenic crops.
Lepidopteran toxicity.
The most publicised problem associated with Bt crops is the claim that pollen from Bt maize could kill the monarch butterfly. The paper produced a public uproar and demonstrations against Bt maize; however by 2001 several follow-up studies coordinated by the USDA had proven that "the most common types of Bt maize pollen are not toxic to monarch larvae in concentrations the insects would encounter in the fields."
Wild maize genetic mixing.
A study published in "Nature" in 2001 reported Bt-containing maize genes were found in maize in its center of origin, Oaxaca, Mexico. In 2002, paper concluded, "the evidence available is not sufficient to justify the publication of the original paper." A significant controversy happened over the paper and "Nature"s unprecedented notice.
A subsequent large-scale study, in 2005, failed to find any evidence of genetic mixing in Oaxaca. A 2007 study found the "transgenic proteins expressed in maize were found in two (0.96%) of 208 samples from farmers' fields, located in two (8%) of 25 sampled communities." Mexico imports a substantial amount of maize from the US, and due to formal and informal seed networks among rural farmers, many potential routes are available for transgenic maize to enter into food and feed webs. One study found small-scale (about 1%) introduction of transgenic sequences in sampled fields in Mexico; it did not find evidence for or against this introduced genetic material being inherited by the next generation of plants. That study was immediately criticized, with the reviewer writing, "Genetically, any given plant should be either nontransgenic or transgenic, therefore for leaf tissue of a single transgenic plant, a GMO level close to 100% is expected. In their study, the authors chose to classify leaf samples as transgenic despite GMO levels of about 0.1%. We contend that results such as these are incorrectly interpreted as positive and are more likely to be indicative of contamination in the laboratory."
Colony collapse disorder.
As of 2007, a new phenomenon called colony collapse disorder (CCD) began affecting bee hives all over North America. Initial speculation on possible causes included new parasites, pesticide use, and the use of Bt transgenic crops. The Mid-Atlantic Apiculture Research and Extension Consortium found no evidence that pollen from Bt crops is adversely affecting bees. According to the USDA, "Genetically modified (GM) crops, most commonly Bt corn, have been offered up as the cause of CCD. But there is no correlation between where GM crops are planted and the pattern of CCD incidents. Also, GM crops have been widely planted since the late 1990s, but CCD did not appear until 2006. In addition, CCD has been reported in countries that do not allow GM crops to be planted, such as Switzerland. German researchers have noted in one study a possible correlation between exposure to Bt pollen and compromised immunity to Nosema." The actual cause of CCD was unknown in 2007, and scientists believe it may have multiple exacerbating causes.
Beta-exotoxins.
Some isolates of "B. thuringiensis" produce a class of insecticidal small molecules called beta-exotoxin, the common name for which is thuringiensin. A consensus document produced by the OECD says: "Beta-exotoxin and the other "Bacillus" toxins may contribute to the insecticidal toxicity of the bacterium to lepidopteran, dipteran, and coleopteran insects. Beta-exotoxin is known to be toxic to humans and almost all other forms of life and its presence is prohibited in" B. thuringiensis" microbial products. Engineering of plants to contain and express only the genes for δ-endotoxins avoids the problem of assessing the risks posed by these other toxins that may be produced in microbial preparations."

</doc>
<doc id="4185" url="https://en.wikipedia.org/wiki?curid=4185" title="Bacteriophage">
Bacteriophage

A bacteriophage (informally, "phage" ) is a virus that infects and replicates within a bacterium. The term is derived from "bacteria" and the ("phagein"), "to devour". Bacteriophages are composed of proteins that encapsulate a DNA or RNA genome, and may have relatively simple or elaborate structures. Their genomes may encode as few as four genes, and as many as hundreds of genes. Phages replicate within the bacterium following the injection of their genome into its cytoplasm. Bacteriophages are among the most common and diverse entities in the biosphere.
Phages are widely distributed in locations populated by bacterial hosts, such as soil or the intestines of animals. One of the densest natural sources for phages and other viruses is sea water, where up to 9×10 virions per milliliter have been found in microbial mats at the surface, and up to 70% of marine bacteria may be infected by phages.
They have been used for over 90 years as an alternative to antibiotics in the former Soviet Union and Central Europe, as well as in France. They are seen as a possible therapy against multi-drug-resistant strains of many bacteria (see phage therapy). Nevertheless, phages of Inoviridae have been shown to complicate biofilms involved in pneumonia and cystic fibrosis, shelter the bacteria from drugs meant to eradicate disease and promote persistent infection.
Classification.
Bacteriophages occur abundantly in the biosphere, with different virions, genomes and lifestyles. Phages are classified by the International Committee on Taxonomy of Viruses (ICTV) according to morphology and nucleic acid.
Nineteen families are currently recognized by the ICTV that infect bacteria and archaea. Of these, only two families have RNA genomes and only five families are enveloped. Of the viral families with DNA genomes, only two have single-stranded genomes. Eight of the viral families with DNA genomes have circular genomes, while nine have linear genomes. Nine families infect bacteria only, nine infect archaea only, and one ("Tectiviridae") infects both bacteria and archaea.
History.
In 1896, Ernest Hanbury Hankin reported that something in the waters of the Ganges and Yamuna rivers in India had marked antibacterial action against cholera and could pass through a very fine porcelain filter. In 1915, British bacteriologist Frederick Twort, superintendent of the Brown Institution of London, discovered a small agent that infected and killed bacteria. He believed the agent must be one of the following:
Twort's work was interrupted by the onset of World War I and shortage of funding. Independently, French-Canadian microbiologist Félix d'Hérelle, working at the Pasteur Institute in Paris, announced on 3 September 1917, that he had discovered "an invisible, antagonistic microbe of the dysentery bacillus". For d’Hérelle, there was no question as to the nature of his discovery: "In a flash I had understood: what caused my clear spots was in fact an invisible microbe … a virus parasitic on bacteria." D'Hérelle called the virus a bacteriophage or bacteria-eater (from the Greek "phagein" meaning to eat). He also recorded a dramatic account of a man suffering from dysentery who was restored to good health by the bacteriophages. It was D'Herelle who conducted much research into bacteriophages and introduced the concept of phage therapy.
In 1969, Max Delbrück, Alfred Hershey and Salvador Luria were awarded the Nobel Prize in Physiology and Medicine for their discoveries of the replication of viruses and their genetic structure.
Phage therapy.
Phages were discovered to be antibacterial agents and were used in the former Soviet Republic of Georgia (pioneered there by Giorgi Eliava with help from the co-discoverer of bacteriophages, Felix d'Herelle) and the United States during the 1920s and 1930s for treating bacterial infections. They had widespread use, including treatment of soldiers in the Red Army. However, they were abandoned for general use in the West for several reasons:
Their use has continued since the end of the Cold War in Georgia and elsewhere in Central and Eastern Europe. Globalyz Biotech is an international joint venture that commercializes bacteriophage treatment and its various applications across the globe. The company has successfully used bacteriophages in administering Phage therapy to patients suffering from bacterial infections, including: Staphylococcus (including MRSA), Streptococcus, Pseudomonas, Salmonella, skin and soft tissue, gastrointestinal, respiratory, and orthopedic infections. In 1923, the Eliava Institute was opened in Tbilisi, Georgia, to research this new science and put it into practice.
The first regulated randomized, double blind clinical trial was reported in the Journal of Wound Care in June 2009, which evaluated the safety and efficacy of a bacteriophage cocktail to treat infected venous leg ulcers in human patients. The study was approved by the FDA as a Phase I clinical trial. Study results satisfactorily demonstrated safety of therapeutic application of bacteriophages, however it did not show efficacy. The authors explain that the use of certain chemicals that are part of standard wound care (e.g. lactoferrin, silver) may have interfered with bacteriophage viability. Another regulated clinical trial in Western Europe (treatment of ear infections caused by "Pseudomonas aeruginosa") was reported shortly after in the journal Clinical Otolaryngology in August 2009. The study concludes that bacteriophage preparations were safe and effective for treatment of chronic ear infections in humans. Additionally, there have been numerous animal and other experimental clinical trials evaluating the efficacy of bacteriophages for various diseases, such as infected burns and wounds, and cystic fibrosis associated lung infections, among others. Meanwhile, Western scientists are developing engineered viruses to overcome antibiotic resistance, and engineering the phage genes responsible for coding enzymes which degrade the biofilm matrix, phage structural proteins and also enzymes responsible for lysis of bacterial cell wall.
D'Herelle "quickly learned that bacteriophages are found wherever bacteria thrive: in sewers, in rivers that catch waste runoff from pipes, and in the stools of convalescent patients." This includes rivers traditionally thought to have healing powers, including India's Ganges River.
Replication.
Bacteriophages may have a lytic cycle or a lysogenic cycle, and a few viruses are capable of carrying out both. With "lytic phages" such as the T4 phage, bacterial cells are broken open (lysed) and destroyed after immediate replication of the virion. As soon as the cell is destroyed, the phage progeny can find new hosts to infect. Lytic phages are more suitable for phage therapy. Some lytic phages undergo a phenomenon known as lysis inhibition, where completed phage progeny will not immediately lyse out of the cell if extracellular phage concentrations are high. This mechanism is not identical to that of temperate phage going dormant and is usually temporary.
In contrast, the "lysogenic cycle" does not result in immediate lysing of the host cell. Those phages able to undergo lysogeny are known as temperate phages. Their viral genome will integrate with host DNA and replicate along with it fairly harmlessly, or may even become established as a plasmid. The virus remains dormant until host conditions deteriorate, perhaps due to depletion of nutrients; then, the endogenous phages (known as prophages) become active. At this point they initiate the reproductive cycle, resulting in lysis of the host cell. As the lysogenic cycle allows the host cell to continue to survive and reproduce, the virus is reproduced in all of the cell’s offspring.
An example of a bacteriophage known to follow the lysogenic cycle and the lytic cycle is the phage lambda of "E. coli."
Sometimes prophages may provide benefits to the host bacterium while they are dormant by adding new functions to the bacterial genome in a phenomenon called lysogenic conversion. Examples are the conversion of harmless strains of "Corynebacterium diphtheriae" or "Vibrio cholerae" by bacteriophages to highly virulent ones, which cause Diphtheria or cholera, respectively. Strategies to combat certain bacterial infections by targeting these toxin-encoding prophages have been proposed.
Attachment and penetration.
To enter a host cell, bacteriophages attach to specific receptors on the surface of bacteria, including lipopolysaccharides, teichoic acids, proteins, or even flagella. This specificity means a bacteriophage can infect only certain bacteria bearing receptors to which they can bind, which in turn determines the phage's host range. Host growth conditions also influence the ability of the phage to attach and invade them. As phage virions do not move independently, they must rely on random encounters with the right receptors when in solution (blood, lymphatic circulation, irrigation, soil water, etc.).
Myovirus bacteriophages use a hypodermic syringe-like motion to inject their genetic material into the cell. After making contact with the appropriate receptor, the tail fibers flex to bring the base plate closer to the surface of the cell; this is known as reversible binding. Once attached completely, irreversible binding is initiated and the tail contracts, possibly with the help of ATP present in the tail, injecting genetic material through the bacterial membrane.
Podoviruses lack an elongated tail sheath similar to that of a myovirus, so they instead use their small, tooth-like tail fibers to enzymatically degrade a portion of the cell membrane before inserting their genetic material.
Synthesis of proteins and nucleic acid.
Within minutes, bacterial ribosomes start translating viral mRNA into protein. For RNA-based phages, RNA replicase is synthesized early in the process. Proteins modify the bacterial RNA polymerase so it preferentially transcribes viral mRNA. The host’s normal synthesis of proteins and nucleic acids is disrupted, and it is forced to manufacture viral products instead. These products go on to become part of new virions within the cell, helper proteins that help assemble the new virions, or proteins involved in cell lysis. Walter Fiers (University of Ghent, Belgium) was the first to establish the complete nucleotide sequence of a gene (1972) and of the viral genome of bacteriophage MS2 (1976).
Virion assembly.
In the case of the T4 phage, the construction of new virus particles involves the assistance of helper proteins. The base plates are assembled first, with the tails being built upon them afterwards. The head capsids, constructed separately, will spontaneously assemble with the tails. The DNA is packed efficiently within the heads. The whole process takes about 15 minutes.
Release of virions.
Phages may be released via cell lysis, by extrusion, or, in a few cases, by budding. Lysis, by tailed phages, is achieved by an enzyme called endolysin, which attacks and breaks down the cell wall peptidoglycan. An altogether different phage type, the filamentous phages, make the host cell continually secrete new virus particles. Released virions are described as free, and, unless defective, are capable of infecting a new bacterium. Budding is associated with certain "Mycoplasma" phages. In contrast to virion release, phages displaying a lysogenic cycle do not kill the host but, rather, become long-term residents as prophage.
Genome structure.
Given the millions of different phages in the environment, phages genomes come in a variety of forms and sizes. RNA phage such as MS2 have the smallest genomes of only a few kilo bases. However, some DNA phages such as T4 may have large genomes with hundreds of genes.
Bacteriophage genomes can be highly mosaic, i.e. the genome of many phage species appear to be composed of numerous individual modules. These modules may be found in other phage species in different arrangements. Mycobacteriophages – bacteriophages with mycobacterial hosts – have provided excellent examples of this mosaicism. In these mycobacteriophages, genetic assortment may be the result of repeated instances of site-specific recombination and illegitimate recombination (the result of phage genome acquisition of bacterial host genetic sequences). It should be noted, however, that evolutionary mechanisms shaping the genomes of bacterial viruses vary between different families and depend on the type of the nucleic acid, characteristics of the virion structure, as well as the mode of the viral life cycle.
Systems biology.
Phage often have dramatic effects on their hosts. As a consequence, the transcription pattern of the infected bacterium may change considerably. For instance, infection of "Pseudomonas aeruginosa" by the temperate phage PaP3 changed the expression of 38% (2160/5633) of its host's genes. Many of these effects are probably indirect, hence the challenge becomes to identify the direct interactions among bacteria and phage.
Several attempts have been made to map protein-protein interactions among phage and their host. For instance, bacteriophage lambda was found to interact with its host E. coli by 31 interactions. However, a large-scale study revealed 62 interactions, most of which were new. Again, the significance of many of these interactions remains unclear but these studies suggest that there are most likely several key interactions and many indirect interactions whose role remains uncharacterized.
In the environment.
Metagenomics has allowed the in-water detection of bacteriophages that was not possible previously.
Bacteriophages have also been used in hydrological tracing and modelling in river systems, especially where surface water and groundwater interactions occur. The use of phages is preferred to the more conventional dye marker because they are significantly less absorbed when passing through ground waters and they are readily detected at very low concentrations. Non-polluted water may contain ca. 2×10 bacteriophages per mL.
Other areas of use.
Since 2006, the United States Food and Drug Administration (FDA) and United States Department of Agriculture (USDA) have approved several bacteriophage products. LMP-102 (Intralytix) was approved for treating ready-to-eat (RTE) poultry and meat products. In that same year, the FDA approved LISTEX (developed and produced by Micreos) using bacteriophages on cheese to kill "Listeria monocytogenes" bacteria, giving them generally recognized as safe (GRAS) status. In July 2007, the same bacteriophage were approved for use on all food products. In 2011 USDA confirmed that LISTEX is a clean label processing-aid and is included in USDA. Research in the field of food safety is continuing to see if lytic phages are a viable option to control other food-borne pathogens in various food products.
In 2011 the FDA cleared the first bacteriophage-based product for in vitro diagnostic use. The KeyPath MRSA/MSSA Blood Culture Test uses a cocktail of bacteriophage to detect "Staphylococcus aureus" in positive blood cultures and determine methicillin resistance or susceptibility. The test returns results in about 5 hours, compared to 2–3 days for standard microbial identification and susceptibility test methods. It was the first accelerated antibiotic susceptibility test approved by the FDA.
Government agencies in the West have for several years been looking to Georgia and the former Soviet Union for help with exploiting phages for counteracting bioweapons and toxins, such as anthrax and botulism. Developments are continuing among research groups in the US. Other uses include spray application in horticulture for protecting plants and vegetable produce from decay and the spread of bacterial disease. Other applications for bacteriophages are as biocides for environmental surfaces, e.g., in hospitals, and as preventative treatments for catheters and medical devices prior to use in clinical settings. The technology for phages to be applied to dry surfaces, e.g., uniforms, curtains, or even sutures for surgery now exists. Clinical trials reported in the "Lancet" show success in veterinary treatment of pet dogs with otitis.
Phage display is a different use of phages involving a library of phages with a variable peptide linked to a surface protein. Each phage's genome encodes the variant of the protein displayed on its surface (hence the name), providing a link between the peptide variant and its encoding gene. Variant phages from the library can be selected through their binding affinity to an immobilized molecule (e.g., botulism toxin) to neutralize it. The bound, selected phages can be multiplied by reinfecting a susceptible bacterial strain, thus allowing them to retrieve the peptides encoded in them for further study.
The SEPTIC bacterium sensing and identification method uses the ion emission and its dynamics during phage infection and offers high specificity and speed for detection.
Phage-ligand technology makes use of proteins, which are identified from bacteriophages, characterized and recombinantly expressed for various applications such as binding of bacteria and bacterial components (e.g. endotoxin) and lysis of bacteria.
Bacteriophages are also important model organisms for studying principles of evolution and ecology.
Model bacteriophages.
The following bacteriophages are extensively studied:

</doc>
<doc id="4187" url="https://en.wikipedia.org/wiki?curid=4187" title="Bactericide">
Bactericide

A bactericide or bacteriocide, sometimes abbreviated Bcidal, is a substance that kills bacteria. Bactericides are disinfectants, antiseptics, or antibiotics.
Bactericidal disinfectants.
The most used disinfectants are those applying
Bactericidal antiseptics.
As antiseptics (i.e., germicide agents that can be used on human or animal body, skin, mucoses, wounds and the like), few of the above-mentioned disinfectants can be used, under proper conditions (mainly concentration, pH, temperature and toxicity toward humans and animals). Among them, some important are
Others are generally not applicable as safe antiseptics, either because of their corrosive or toxic nature.
Bactericidal antibiotics.
Bactericidal antibiotics kill bacteria; bacteriostatic antibiotics slow their growth or reproduction.
Antibiotics that inhibit cell wall synthesis: the Beta-lactam antibiotics (penicillin derivatives (penams), cephalosporins (cephems), monobactams, and carbapenems) and vancomycin.
Also bactericidal are daptomycin, fluoroquinolones, metronidazole, nitrofurantoin, co-trimoxazole, telithromycin.
Aminoglycosidic antibiotics are usually considered bactericidal, although they may be bacteriostatic with some organisms
The distinction between bactericidal and bacteriostatic agents appears to be clear according to the basic/clinical definition, but this only applies under strict laboratory conditions and it is important to distinguish microbiological and clinical definitions. The distinction is more arbitrary when agents are categorized in clinical situations. The supposed superiority of bactericidal agents over bacteriostatic agents is of little relevance when treating the vast majority of infections with gram-positive bacteria, particularly in patients with uncomplicated infections and noncompromised immune systems. Bacteriostatic agents have been effectively used for treatment that are considered to require bactericidal activity. Furthermore some broad classes of antibacterial agents considered bacteriostatic can exhibit bactericidal activity against some bacteria on the basis of in vitro determination of MBC/MIC values. At high concentrations, bacteriostatic agents are often bactericidal against some susceptible organisms The ultimate guide to treatment of any infection must be clinical outcome.

</doc>
<doc id="4188" url="https://en.wikipedia.org/wiki?curid=4188" title="Brion Gysin">
Brion Gysin

Brion Gysin (19 January 1916 – 13 July 1986) was a painter, writer, sound poet, and performance artist born in Taplow, Buckinghamshire.
He is best known for his discovery of the cut-up technique, used by his friend, the novelist William S. Burroughs. With the engineer Ian Sommerville he invented the Dreamachine, a flicker device designed as an art object to be viewed with the eyes closed. It was in painting and drawing, however, that Gysin devoted his greatest efforts, creating calligraphic works inspired by the cursive Japanese "grass" script and Arabic script. Burroughs later stated that "Brion Gysin was the only man I ever respected."
Biography.
Early years.
John Clifford Brian Gysin was born at Taplow House, England, a Canadian military hospital. His mother, Stella Margaret Martin, was a Canadian from Deseronto, Ontario. His father, Leonard Gysin, a captain with the Canadian Expeditionary Force, was killed in action eight months after his son's birth. Stella returned to Canada and settled in Edmonton, Alberta where her son became "the only Catholic day-boy at an Anglican boarding school". Graduating at fifteen, Gysin was sent to Downside School in Stratton-on-the-Fosse, near Bath, Somerset in England, a prestigious college run by the Benedictines and known as "the Eton of Catholic public schools". Despite attending a Catholic school, Gysin became an atheist.
Surrealism.
In 1934, he moved to Paris to study "La Civilisation Française", an open course given at the Sorbonne where he made literary and artistic contacts through Marie Berthe Aurenche, Max Ernst's second wife. He joined the Surrealist Group and began frequenting Valentine Hugo, Leonor Fini, Salvador Dalí, Picasso and Dora Maar. A year later, he had his first exhibition at the "Galerie Quatre Chemins" in Paris with Ernst, Picasso, Hans Arp, Hans Bellmer, Victor Brauner, Giorgio de Chirico, Dalí, Marcel Duchamp, René Magritte, Man Ray and Yves Tanguy. On the day of the preview, however, he was expelled from the Surrealist Group by André Breton, who ordered the poet Paul Éluard to take down his pictures. Gysin was 19 years old. His biographer, John Geiger, suggests the arbitrary expulsion "had the effect of a curse. Years later, he blamed other failures on the Breton incident. It gave rise to conspiracy theories about the powerful interests who seek control of the art world. He gave various explanations for the expulsion, the more elaborate involving 'insubordination' or "lèse majesté" towards Breton".
After World War II.
After serving in the U.S. army during World War II, Gysin published a biography of Josiah "Uncle Tom" Henson titled, "To Master, a Long Goodnight: The History of Slavery in Canada" (1946). A gifted draughtsman, he took an 18-month course learning the Japanese language (including calligraphy) that would greatly influence his artwork. In 1949, he was among the first Fulbright Fellows. His goal: to research the history of slavery at the University of Bordeaux and in the Archivo de Indias in Seville, Spain, a project that he later abandoned. He moved to Tangier, Morocco after visiting the city with novelist and composer Paul Bowles in 1950.
Morocco and the Beat Hotel.
In 1954 in Tangier, Gysin opened a restaurant called The 1001 Nights, with his friend Mohamed Hamri, who was the cook. Gysin hired the Master Musicians of Jajouka from the village of Jajouka to perform alongside entertainment that included acrobats, a dancing boy and fire eaters. The musicians performed there for an international clientele that included William S. Burroughs. Gysin lost the business in 1958, and the restaurant closed permanently. That same year, Gysin returned to Paris, taking lodgings in a flophouse located at 9 rue Gît-le-Coeur that would become famous as the Beat Hotel. Working on a drawing, he discovered a Dada technique by accident:
William Burroughs and I first went into techniques of writing, together, back in room No. 15 of the Beat Hotel during the cold Paris spring of 1958... Burroughs was more intent on Scotch-taping his photos together into one great continuum on the wall, where scenes faded and slipped into one another, than occupied with editing the monster manuscript... "Naked Lunch" appeared and Burroughs disappeared. He kicked his habit with Apomorphine and flew off to London to see Dr Dent, who had first turned him on to the cure. While cutting a mount for a drawing in room No. 15, I sliced through a pile of newspapers with my Stanley blade and thought of what I had said to Burroughs some six months earlier about the necessity for turning painters' techniques directly into writing. I picked up the raw words and began to piece together texts that later appeared as "First Cut-Ups" in "Minutes to Go" (Two Cities, Paris 1960).
When Burroughs returned from London in September 1959, Gysin not only shared his discovery with his friend but the new techniques he had developed for it. Burroughs then put the techniques to use while completing "Naked Lunch" and the experiment dramatically changed the landscape of American literature. Gysin helped Burroughs with the editing of several of his novels including "Interzone", and wrote a script for a film version of "Naked Lunch", which was never produced. The pair collaborated on a large manuscript for Grove Press titled "The Third Mind" but it was determined that it would be impractical to publish it as originally envisioned. The book later published under that title incorporates little of this material. Interviewed for "The Guardian" in 1997, Burroughs explained that Gysin was "the only man that I've ever respected in my life. I've admired people, I've liked them, but he's the only man I've ever respected." In 1969, Gysin completed his finest novel, "The Process", a work judged by critic Robert Palmer as "a classic of 20th century modernism".
A consummate innovator, Gysin altered the cut-up technique to produce what he called permutation poems in which a single phrase was repeated several times with the words rearranged in a different order with each reiteration. An example of this is "I don't dig work, man/Man, work I don't dig." Many of these permutations were derived using a random sequence generator in an early computer program written by Ian Sommerville. Commissioned by the BBC in 1960 to produce material for broadcast, Gysin's results included "Pistol Poem", which was created by recording a gun firing at different distances and then splicing the sounds. That year, the piece was subsequently used as a theme for the Paris performance of Le Domaine Poetique, a showcase for experimental works by people like Gysin, François Dufrêne, Bernard Heidsieck, and Henri Chopin.
With Sommerville, he built the Dreamachine in 1961. Described as "the first art object to be seen with the eyes closed", the flicker device uses alpha waves in the 8-16 Hz range to produce a change of consciousness in receptive viewers.
Later years.
He also worked extensively with noted jazz soprano saxophonist Steve Lacy.
He recorded an album in 1986 with French musician Ramuntcho Matta, featuring himself singing/rapping his own texts, with performances by Don Cherry, Elli Medeiros, Steve Lacy, Lizzy Mercier Descloux and more. The album was reissued on CD in 1993 by Crammed Discs, under the title "Self-Portrait Jumping".
As a joke, Gysin contributed a recipe for marijuana fudge to a cookbook by Alice B. Toklas; it was unintentionally included for publication, becoming famous under the name Alice B. Toklas brownies.
A heavily edited version of his novel, "The Last Museum", was published posthumously in 1986 by Faber & Faber (London) and by Grove Press (New York).
Made an American Commander of the French Ordre des Arts et des Lettres in 1985, Gysin died of lung cancer a year later, on July 13, 1986. An obituary by Robert Palmer published in "The New York Times" fittingly described him as a man who "threw off the sort of ideas that ordinary artists would parlay into a lifetime career, great clumps of ideas, as casually as a locomotive throws off sparks".
Burroughs on the Gysin cut-up.
In a 1966 interview by Conrad Knickerbocker for The Paris Review, William S. Burroughs explained that Brion Gysin was, to his knowledge, "the first to create cut-ups".
INTERVIEWER: How did you become interested in the cut-up technique?
BURROUGHS: A friend, Brion Gysin, an American poet and painter, who has lived in Europe for thirty years, was, as far as I know, the first to create cut-ups. His cut-up poem, "Minutes to Go", was broadcast by the BBC and later published in a pamphlet. I was in Paris in the summer of 1960; this was after the publication there of "Naked Lunch". I became interested in the possibilities of this technique, and I began experimenting myself. Of course, when you think of it, "The Waste Land" was the first great cut-up collage, and Tristan Tzara had done a bit along the same lines. Dos Passos used the same idea in 'The Camera Eye' sequences in "USA". I felt I had been working toward the same goal; thus it was a major revelation to me when I actually saw it being done.
Influence.
According to José Férez Kuri, author of "Brion Gysin: Tuning in to the Multimedia Age" (2003) and co-curator of a major retrospective of the artist's work at The Edmonton Art Gallery in 1998, Gysin's wide range of "radical ideas would become a source of inspiration for artists of the Beat Generation, as well as for their successors (among them David Bowie, Mick Jagger, Keith Haring, and Laurie Anderson)". Other artists include Genesis P-Orridge, John Zorn (as displayed on the 2013's Dreamachines album) and Brian Jones.
Selected bibliography.
Gysin is the subject of John Geiger's biography, "Nothing Is True Everything Is Permitted: The Life of Brion Gysin", and features in "Chapel of Extreme Experience: A Short History of Stroboscopic Light and the Dream Machine", also by Geiger. "Man From Nowhere: Storming the Citadels of Enlightenment with William Burroughs and Brion Gysin", a biographical study of Burroughs and Gysin with a collection of homages to Gysin, was authored by Joe Ambrose, Frank Rynne, and Terry Wilson with contributions by Marianne Faithfull, John Cale, William S. Burroughs, John Giorno, Stanley Booth, Bill Laswell, Mohamed Hamri, Keith Haring and Paul Bowles. A monograph on Gysin was published in 2003 by Thames and Hudson.
Works.
Prose
Radio
Cinema
Music
Painting

</doc>
<doc id="4190" url="https://en.wikipedia.org/wiki?curid=4190" title="Bulgarian">
Bulgarian

Bulgarian refers to anything of or relating to Bulgaria and may refer directly to:
or

</doc>
<doc id="4191" url="https://en.wikipedia.org/wiki?curid=4191" title="BCG vaccine">
BCG vaccine

Bacillus Calmette–Guérin (BCG) vaccine is a vaccine primarily used against tuberculosis. In countries where tuberculosis is common one dose is recommended in healthy babies as close to the time of birth as possible. Babies with HIV/AIDS should not be vaccinated. In areas where tuberculosis is not common, only babies at high risk are typically immunized while suspected cases of tuberculosis are individually tested for and treated. Adults who do not have tuberculosis and have not been previously immunized but are frequently exposed to drug resistant tuberculosis may be immunized as well. It is also often used as part of the treatment of bladder cancer.
Rates of protection against tuberculosis infection vary widely and protection lasts between ten and twenty years. Among children it prevents about 20% from getting infected and among those who do get infected it protects half from developing disease. The vaccine is given by injection into the skin. Additional doses are not supported by evidence. It may also be used in the treatment of some types of bladder cancers.
Serious side effects are rare. Often there is redness, swelling, and mild pain at the site of injection. A small ulcer may also form with some scarring after healing. Side effects are more common and potentially more severe in those with poor immune function. It is not safe for use during pregnancy. The vaccine was originally developed from "Mycobacterium bovis" which is commonly found in cows. While it has been weakened, it is still live.
The BCG vaccine was first used medically in 1921. It is on the World Health Organization's List of Essential Medicines, the most important medication needed in a basic health system. The wholesale cost is $0.16 USD a dose as of 2014. In the United States it costs $100 to $200 USD. Each year the vaccine is given to about 100 million children.
Medical uses.
Tuberculosis.
The main use of BCG is for vaccination against tuberculosis. BCG vaccine can be administered after birth intradermally. BCG vaccination is recommended to be given intradermally. A previous BCG vaccination can cause a false positive Mantoux test, although a very high-grade reading is usually due to active disease.
The most controversial aspect of BCG is the variable efficacy found in different clinical trials, which appears to depend on geography. Trials conducted in the UK have consistently shown a protective effect of 60 to 80%, but those conducted elsewhere have shown no protective effect, and efficacy appears to fall the closer one gets to the equator.
A 1994 systematic review found that BCG reduces the risk of getting TB by about 50%. There are differences in effectiveness, depending on region, due to factors such as genetic differences in the populations, changes in environment, exposure to other bacterial infections, and conditions in the lab where the vaccine is grown, including genetic differences between the strains being cultured and the choice of growth medium.
A systematic review and meta analysis conducted in 2014 demonstrated that the BCG vaccine reduced infections by 19–27% and reduced progression to active TB by 71%. The studies included in this review were limited to those that used interferon gamma release assay.
The duration of protection of BCG is not clearly known. In those studies showing a protective effect, the data are inconsistent. The MRC study showed protection waned to 59% after 15 years and to zero after 20 years; however, a study looking at Native Americans immunized in the 1930s found evidence of protection even 60 years after immunization, with only a slight waning in efficacy.
BCG seems to have its greatest effect in preventing miliary TB or TB meningitis, so it is still extensively used even in countries where efficacy against pulmonary tuberculosis is negligible.
Recommendations.
In countries where tuberculosis is common one dose is recommended in healthy babies as close to the time of birth as possible. Babies with HIV/AIDS should not be vaccinated. In areas where tuberculosis is not common, only babies at high risk are typically immunized while suspected cases of tuberculosis are individually tested for and treated. Adults who do not have tuberculosis and have not been previously immunized but are frequently exposed to drug resistant tuberculosis may be immunized as well.
Reasons.
A number of possible reasons for the variable efficacy of BCG in different countries have been proposed. None have been proven, some have been disproved, and none can explain the lack of efficacy in both low-TB burden countries (US) and high-TB burden countries (India). The reasons for variable efficacy have been discussed at length in a WHO document on BCG.
Cancer.
BCG has been one of the most successful immunotherapies. BCG vaccine has been the "standard of care for patients with bladder cancer (NMIBC)" since 1977. By 2014 there were more than eight different considered biosimilar agents or strains used for the treatment of non–muscle-invasive bladder cancer (NMIBC).
Method of administration.
Except in neonates, a tuberculin skin test should always be done before administering BCG. A reactive tuberculin skin test is a contraindication to BCG. Someone with a positive tuberculin reaction is not given BCG, because the risk of severe local inflammation and scarring is high, not because of the common misconception that tuberculin reactors "are already immune" and therefore do not need BCG. People found to have reactive tuberculin skin tests should be screened for active tuberculosis. BCG is also contraindicated in certain people who have IL-12 receptor pathway defects.
BCG is given as a single intradermal injection at the insertion of the deltoid. If BCG is accidentally given subcutaneously, then a local abscess may form (a "BCG-oma") that can sometimes ulcerate, and may require treatment with antibiotics immediately, otherwise without treatment it could spread the infection causing severe damage to vital organs. However, it is important to note an abscess is not always associated with incorrect administration, and it is one of the more common complications that can occur with the vaccination. Numerous medical studies on treatment of these abscesses with antibiotics have been done with varying results, but the consensus is once pus is aspirated and analysed, provided no unusual bacilli are present, the abscess will generally heal on its own in a matter of weeks.
The characteristic raised scar BCG immunization leaves is often used as proof of prior immunization. This scar must be distinguished from that of small pox vaccination, which it may resemble.
Adverse effects.
BCG immunization generally causes some pain and scarring at the site of injection. The main adverse effects are keloids—large, raised scars. The insertion of deltoid is most frequently used because the local complication rate is smallest when that site is used. Nonetheless, the buttock is an alternative site of administration because it provides better cosmetic outcomes.
BCG vaccine should be given intradermally. If given subcutaneously, it may induce local infection and spread to the regional lymph nodes, causing either suppurative and nonsuppurative lymphadenitis. Conservative management is usually adequate for nonsuppurative lymphadenitis. If suppuration occurs, it may need needle aspiration. For nonresolving suppuration, surgical excision may be required. Evidence for the treatment of these complications is scarce.
Uncommonly, breast and gluteal abscesses can occur due to haematogenous and lymphangiomatous spread. Regional bone infection (BCG osteomyelitis or osteitis) and disseminated BCG infection are rare complications of BCG vaccination, but potentially life-threatening. Systemic antituberculous therapy may be helpful in severe complications.
If BCG is accidentally given to an immunocompromised patient (e.g., an infant with SCID), it can cause disseminated or life-threatening infection. The documented incidence of this happening is less than one per million immunizations given. In 2007, The WHO stopped recommending BCG for infants with HIV, even if there is a high risk of exposure to TB, because of the risk of disseminated BCG infection (which is approximately 400 per 100,000 in that higher risk context).
Usage.
The age of the person and the frequency with which BCG is given has always varied from country to country.
Manufacturer.
BCG is prepared from a strain of the attenuated (virulence-reduced) live bovine tuberculosis bacillus, "Mycobacterium bovis", that has lost its ability to cause disease in humans. Because the living bacilli evolve to make the best use of available nutrients, they become less well-adapted to human blood and can no longer induce disease when introduced into a human host. Still, they are similar enough to their wild ancestors to provide some degree of immunity against human tuberculosis. The BCG vaccine can be anywhere from 0 to 80% effective in preventing tuberculosis for a duration of 15 years; however, its protective effect appears to vary according to geography and the lab in which the vaccine strain was grown.
A number of different companies make BCG, sometimes using different genetic strains of the bacterium. This may result in different product characteristics. OncoTICE, used for bladder instillation for bladder cancer, was developed by Organon Laboratories (since acquired by Schering-Plough, and in turn acquired by Merck, Inc.). Pacis BCG, made from the Montréal (Institut Armand-Frappier) strain, was first marketed by Urocor in about 2002. Urocor was since acquired by Dianon Systems. Evans Vaccines (a subsidiary of PowderJect Pharmaceuticals). Statens Serum Institut in Denmark markets BCG vaccine prepared using Danish strain 1331. Japan BCG Laboratory markets its vaccine, based on the Tokyo 172 substrain of Pasteur BCG, in 50 countries worldwide.
According to a UNICEF report published in December 2015 on BCG vaccine supply security, global demand increased in 2015 from 123 to 152.2 million doses. In order to improve security and to iversif sources of affordable and flexible supply," UNICEF awarded seven new manufacturers contracts to produce BCG. Along with supply availability from existing manufacturers, and a "new WHO prequalified vaccine" the total supply will be "sufficient to meet both suppressed 2015 demand carried over to 2016, as well as total forecast demand through 2016-2018."
BCG supply shortage 2012-.
In the fall of 2011 the Sanofi Pasteur plant flooded causing problems with mold. The facility, located in Toronto, Ontario, Canada, produced BCG vaccine products, made with substrain Connaught, such as a tuberculosis vaccine ImmuCYST, a BCG Immunotherapeutic -a bladder cancer drug. By April 2012 the FDA had found dozens of documented problems with sterility at the plant including mold, nesting birds and rusted electrical conduits. The resulting closure of the plant for over two years resulting in shortages of bladder cancer and tuberculosis vaccines. On October 29, 2014 Health Canada gave the permission for Sanofi to resume production of BCG.
Preparation.
A weakened strain of bovine tuberculosis bacillus, "Mycobacterium bovis" is specially subcultured in a culture medium, usually Middlebrook 7H9.
Dried.
Some BCG vaccines are freeze dried and become fried powder. Sometimes the powder are sealed with vacuum in a glass ampoule. Such a glass ampoule has to be opened slowly to prevent the airflow from blowing out the powder. Then the powder has to be diluted with saline water before injecting.
History.
The history of BCG is tied to that of smallpox. Jean Antoine Villemin first recognized bovine tuberculosis in 1854 and transmitted it, and Robert Koch first distinguished "Mycobacterium bovis" from "Mycobacterium tuberculosis". Following the success of vaccination in preventing smallpox, established during the 18th century, scientists thought to find a corollary in tuberculosis by drawing a parallel between bovine tuberculosis and cowpox: it was hypothesized that infection with bovine tuberculosis might protect against infection with human tuberculosis. In the late 19th century, clinical trials using "M. bovis" were conducted in Italy with disastrous results, because "M. bovis" was found to be just as virulent as "M. tuberculosis".
Albert Calmette, a French physician and bacteriologist, and his assistant and later colleague, Camille Guérin, a veterinarian, were working at the Institut Pasteur de Lille (Lille, France) in 1908. Their work included subculturing virulent strains of the tubercle bacillus and testing different culture media. They noted a glycerin-bile-potato mixture grew bacilli that seemed less virulent, and changed the course of their research to see if repeated subculturing would produce a strain that was attenuated enough to be considered for use as a vaccine. BCG strain was isolated after 239 times subculturing during 13 years from virulent strain on glycerine potato medium. The research continued throughout World War I until 1919, when the now avirulent bacilli were unable to cause tuberculosis disease in research animals. They transferred to the Paris Pasteur Institute in 1919. The BCG vaccine was first used in humans in 1921.
Public acceptance was slow, and one disaster, in particular, did much to harm public acceptance of the vaccine. In the summer of 1930 in Lübeck, 240 infants were vaccinated in the first 10 days of life; almost all developed tuberculosis and 72 infants died. It was subsequently discovered that the BCG administered there had been contaminated with a virulent strain that was being stored in the same incubator, which led to legal action against the manufacturers of the vaccine.
Dr. R.G. Ferguson, working at the Fort Qu'Appelle Sanatorium in Saskatchewan, was among the pioneers in developing the practice of vaccination against tuberculosis. In 1928, BCG was adopted by the Health Committee of the League of Nations (predecessor to the WHO). Because of opposition, however, it only became widely used after World War II. From 1945 to 1948, relief organizations (International Tuberculosis Campaign or Joint Enterprises) vaccinated over 8 million babies in eastern Europe and prevented the predicted typical increase of TB after a major war.
BCG is very efficacious against tuberculous meningitis in the pediatric age group, but its efficacy against pulmonary tuberculosis appears to be variable. As of 2006, only a few countries do not use BCG for routine vaccination. Two countries that have never used it routinely are the USA and the Netherlands (in both countries, it is felt that having a reliable Mantoux test and being able to accurately detect active disease is more beneficial to society than vaccinating against a condition that is now relatively rare there).
Other names include "Vaccin Bilié de Calmette et Guérin vaccine" and "Bacille de Calmette et Guérin vaccine".
Research.
Tentative evidence exists for a beneficial non-specific effect of BCG vaccination on overall mortality in low income countries, or for its reducing other health problems including sepsis and respiratory infections when given early, with greater benefit the earlier it is used.

</doc>
<doc id="4192" url="https://en.wikipedia.org/wiki?curid=4192" title="Bunsen">
Bunsen

Bunsen may refer to:

</doc>
<doc id="4193" url="https://en.wikipedia.org/wiki?curid=4193" title="Common buzzard">
Common buzzard

The common buzzard ("Buteo buteo") is a medium-to-large bird of prey whose range covers most of Europe and extends into Asia. Over much of its range, it is resident year-round, but birds from the colder parts of the northern hemisphere typically migrate south (some well into the southern hemisphere) for the northern winter.
Description.
The common buzzard measures between in length with a wingspan and a body mass of , making it a medium-sized raptor.
This broad-winged raptor has a wide variety of plumages, and in Europe can be confused with the similar rough-legged buzzard ("Buteo lagopus") and the only distantly related European honey buzzard ("Pernis apivorus"), which mimics the common buzzard's plumage for a degree of protection from northern goshawks. The plumage can vary in Britain from almost pure white to black, but is usually shades of brown, with a pale 'necklace' of feathers.
Systematics.
The common buzzard was first described by Linnaeus in his Systema naturae in 1758 as "Falco buteo". Buzzard subspecies fall into two groups.
The western "Buteo" group is mainly resident or short-distance migrants. They are:
The eastern "vulpinus" group includes
Behaviour.
The common buzzard breeds in woodlands, usually on the fringes, but favours hunting over open land. It eats mainly small mammals, and will come to carrion. A great opportunist, it adapts well to a varied diet of pheasant, rabbit, other small mammals to medium mammals, snakes and lizards, and can often be seen walking over recently ploughed fields looking for worms and insects.
Buzzards do not normally form flocks, but several may be seen together on migration or in good habitat. The Victorian writer on Dartmoor, William Crossing, noted he had on occasions seen flocks of 15 or more at some places. Though a rare occurrence, as many as 20 buzzards can be spotted in one field area, approximately apart, so cannot be classed as a flock in the general sense, consisting of birds without a mate or territory. They are fiercely territorial, and, though rare, fights do break out if one strays onto another pair's territory, but dominant displays of aggression will normally see off the interloper. Pairs mate for life. To attract a mate (or impress his existing mate) the male performs a ritual aerial display before the beginning of spring. This spectacular display is known as 'the roller coaster'. He will rise high up in the sky, to turn and plummet downward, in a spiral, twisting and turning as he comes down. He then rises immediately upward to repeat the exercise.
The call is a plaintive "peea-ay", similar to a cat's meow.
Status.
In parts of its range it is increasing in numbers. In Ireland it became extinct about 1910, but began to slowly recolonise the country in the 1950s, and is now a common and familiar sight over much of Ireland.
Steppe buzzard.
The steppe buzzard, "B. (b.) vulpinus" breeds from east Europe eastward to the Far East, excluding Japan. It is a long-distance migrant, excepting some north Himalayan birds, and winters in Africa, India and southeastern Asia. In the open country favoured on the wintering grounds, steppe buzzards are often seen perched on roadside telephone poles.
The steppe buzzard is some times split off as a separate species, "B. vulpinus". Compared to the nominate form, it is slightly smaller ( long), longer winged and longer tailed. There are two colour morphs: the rufous form which gives this subspecies its scientific name ("vulpes" is Latin for "fox"), and a dark grey form.
The tail of "vulpinus" is paler than the nominate form, and often quite rufous, recalling North American red-tailed hawk. The upper wings have pale primary patches, and the primary flight feathers are also paler when viewed from below. Adults have a black trailing edge to the wings, and both morphs often have plain underparts, lacking the breast band frequently seen in "B. b. buteo".

</doc>
<doc id="4194" url="https://en.wikipedia.org/wiki?curid=4194" title="Bohrium">
Bohrium

Bohrium is a chemical element with symbol Bh and atomic number 107. It is named after Danish physicist Niels Bohr. It is a synthetic element (an element that can be created in a laboratory but is not found in nature) and radioactive; the most stable known isotope, Bh, has a half-life of approximately 61 seconds.
In the periodic table of the elements, it is a d-block transactinide element. It is a member of the 7th period and belongs to the group 7 elements. Chemistry experiments have confirmed that bohrium behaves as the heavier homologue to rhenium in group 7. The chemical properties of bohrium are characterized only partly, but they compare well with the chemistry of the other group 7 elements.
History.
Official discovery.
Two groups claim discovery of the element. Evidence of Bohrium was first reported in 1976 by a Russian research team led by Yuri Oganessian.
In 1981, a German research team led by Peter Armbruster and Gottfried Münzenberg at the Institute for Heavy Ion Research (Gesellschaft für Schwerionenforschung) in Darmstadt bombarded a target of bismuth-209 with accelerated nuclei of chromium-54 to produce 5 atoms of the isotope bohrium-262:
The IUPAC/IUPAP Transfermium Working Group (TWG) recognised the GSI collaboration as official discoverers in their 1992 report.
Proposed names.
The German group suggested the name "nielsbohrium" with symbol "Ns" to honor the Danish physicist Niels Bohr. The Soviet scientists at the Joint Institute for Nuclear Research in Dubna, Russia had suggested this name be given to element 105 (which was finally called dubnium) and the German team wished to recognise both Bohr and the fact that the Dubna team had been the first to propose the cold fusion reaction to solve the controversial problem of the naming of element 105. The Dubna team agreed with the German group's naming proposal for element 107.
There was an element naming controversy as to what the elements from 104 to 106 were to be called; the IUPAC adopted "unnilseptium" (symbol "Uns") as a temporary, systematic element name for this element. In 1994 a committee of IUPAC recommended that element 107 be named "bohrium", not "nielsbohrium", since there was no precedence for using a scientist's complete name in the naming of an element. This was opposed by the discoverers as there was some concern that the name might be confused with boron and in particular the distinguishing of the names of their respective oxyanions, "bohrate" and "borate". The matter was handed to the Danish branch of IUPAC which, despite this, voted in favour of the name "bohrium", and thus the name "bohrium" for element 107 was recognized internationally in 1997.
Isotopes.
Bohrium has no stable or naturally-occurring isotopes. Several radioactive isotopes have been synthesized in the laboratory, either by fusing two atoms or by observing the decay of heavier elements. Eleven different isotopes of bohrium have been reported with atomic masses 260–262, 264–267, 270–272, 274, one of which, bohrium-262, has a known metastable state. All of these decay only through alpha decay, although some unknown bohrium isotopes are predicted to undergo spontaneous fission.
Stability and half-lives.
The lighter isotopes usually have shorter half-lives; half-lives of under 100 ms for Bh, Bh, Bh, and Bh were observed. Bh, Bh, Bh, and Bh are more stable at around 1 s, and Bh and Bh have half-lives of about 10 s. The heaviest isotopes are the most stable, with Bh and Bh having measured half-lives of about 61 s and 54 s respectively. The unknown isotopes Bh and Bh are predicted to have even longer half-lives of around 90 minutes and 40 minutes respectively. Before its discovery, Bh was also predicted to have a long half-life of 90 minutes, but it was found to have a shorter half-life of only about 54 seconds.
The proton-rich isotopes with masses 260, 261, and 262 were directly produced by cold fusion, those with mass 262 and 264 were reported in the decay chains of meitnerium and roentgenium, while the neutron-rich isotopes with masses 265, 266, 267 were created in irradiations of actinide targets. The four most neutron-rich ones with masses 270, 271, 272, and 274 appear in the decay chains of 113, 115, 115, and 117 respectively. These eleven isotopes have half-lives ranging from 8 milliseconds to 1 minute.
Chemical properties.
Extrapolated.
Bohrium is the fourth member of the 6d series of transition metals and the heaviest member of group VII in the Periodic Table, below manganese, technetium and rhenium. All the members of the group readily portray their group oxidation state of +7 and the state becomes more stable as the group is descended. Thus bohrium is expected to form a stable +7 state. Technetium also shows a stable +4 state whilst rhenium exhibits stable +4 and +3 states. Bohrium may therefore show these lower states as well.
The heavier members of the group are known to form volatile heptoxides MO (M = metal), so bohrium should also form the volatile oxide BhO. The oxide should dissolve in water to form perbohric acid, HBhO.
Rhenium and technetium form a range of oxyhalides from the halogenation of the oxide. The chlorination of the oxide forms the oxychlorides MOCl, so BhOCl should be formed in this reaction. Fluorination results in MOF and MOF for the heavier elements in addition to the rhenium compounds ReOF and ReF. Therefore, oxyfluoride formation for bohrium may help to indicate eka-rhenium properties.
Bohrium is expected to be a solid under normal conditions and assume a hexagonal close-packed crystal structure (/ = 1.62), similar to its lighter congener rhenium.
Experimental.
In 1995, the first report on attempted isolation of the element was unsuccessful.
In 2000, it was confirmed that although relativistic effects are important, the 107th element does behave like a typical group 7 element.
In 2000, a team at the PSI conducted a chemistry reaction using atoms of Bh produced in the reaction between Bk and Ne ions. The resulting atoms were thermalised and reacted with a HCl/O mixture to form a volatile oxychloride. The reaction also produced isotopes of its lighter homologues, technetium (as Tc) and rhenium (as Re). The isothermal adsorption curves were measured and gave strong evidence for the formation of a volatile oxychloride with properties similar to that of rhenium oxychloride. This placed bohrium as a typical member of group 7.

</doc>
<doc id="4195" url="https://en.wikipedia.org/wiki?curid=4195" title="Barbara Olson">
Barbara Olson

Barbara Kay Olson (née Bracher; December 27, 1955 September 11, 2001) was an American lawyer and conservative television commentator who worked for CNN, Fox News Channel, and several other outlets. She was a passenger on American Airlines Flight 77 en route to a taping of Bill Maher's television show "Politically Incorrect" when it was flown into the Pentagon in the September 11 attacks. Her original plan had been to fly to California on September 10, but delayed until the next morning so that she could wake up with her husband on his birthday, September 11.
Early life.
Olson was born Barbara Kay Bracher in Houston, Texas. (Her older sister, Toni Bracher-Lawrence, was a member of the Houston City Council from 2004 to 2010.) She graduated from Waltrip High School and earned a Bachelor of Arts from the University of Saint Thomas in Houston. She earned a Juris Doctor degree from the Yeshiva University Benjamin N. Cardozo School of Law.
Career.
As a newcomer, she achieved a surprising measure of success, working for HBO and Stacy Keach Productions. In the early 1990s, she worked as an associate at the Washington, D.C.-based law firm of Wilmer Cutler & Pickering where she did civil litigation for several years before becoming an Assistant U.S. Attorney.
Olson's support in 1991 of Supreme Court nominee Clarence Thomas led to the formation of the Independent Women's Forum. At that time, Olson and friend Rosalie (Ricky) Gaull Silberman started an informal network of women who supported the Thomas nomination to the Supreme Court despite allegations of sexual harassment by Anita Hill, a former subordinate of Thomas at the Equal Employment Opportunity Commission. Olson, who had also worked under Thomas at the EEOC and was a close friend of Thomas, spoke out on his behalf during his contentious Senate confirmation hearings. Olson later helped edit "The Real Anita Hill", a book by David Brock that savaged Hill and portrayed the harassment claim as a political dirty trick (Brock later recanted his claims and apologized to Hill). The Independent Women's Forum continued on with a goal of retaining a high profile group of women to advocate for economic and political freedom and personal responsibility.
In 1994, Olson became chief investigative counsel for the U.S. House of Representatives Committee on Oversight and Government Reform. In that position, she led the Travelgate and Filegate investigations into the Clinton administration. She was later a partner in the Washington, D.C. office of the Birmingham, Alabama law firm Balch & Bingham.
Personal life.
She married Theodore Olson in 1996, becoming his third wife. Theodore went on to successfully represent presidential candidate George W. Bush in the Supreme Court case of "Bush v. Gore", and subsequently served as U.S. Solicitor General in the Bush administration.
Olson was a frequent critic of the Bill Clinton administration and wrote a book about then First Lady Hillary Rodham Clinton, "Hell to Pay: The Unfolding Story of Hillary Rodham Clinton" (1999). Olson's second book, "The Final Days: The Last, Desperate Abuses of Power by the Clinton White House" was published posthumously.
She was a resident of Great Falls, Virginia.
Death and legacy.
Olson was a passenger on American Airlines Flight 77 on her way to a taping of "Politically Incorrect" in Los Angeles, when it was flown into the Pentagon in the September 11 attacks. Her original plan had been to fly to California on September 10, but waited until the next day so that she could wake up with her husband on his birthday, September 11. Bill Maher, host of "Politically Incorrect", left a panel chair empty for a week in her memory. At the National September 11 Memorial, Olson's name is located on Panel S-70 of the South Pool, along with those of other passengers of Flight 77.

</doc>
<doc id="4196" url="https://en.wikipedia.org/wiki?curid=4196" title="Barnard's Star">
Barnard's Star

Barnard's Star is a very-low-mass red dwarf about six light-years away from Earth in the constellation of Ophiuchus. It is the fourth-closest known individual star to the Sun (after the three components of the Alpha Centauri system) and the closest star in the Northern Hemisphere. Despite its proximity, at a dim apparent magnitude of about nine, it is not visible with the unaided eye; however, it is much brighter in the infrared than it is in visible light.
The star is named for American astronomer E. E. Barnard. He was not the first to observe the star (it appeared on Harvard University plates in 1888 and 1890), but in 1916 he measured its proper motion as 10.3 arcseconds per year, which remains the largest known proper motion of any star relative to the Solar System.
Barnard's Star is among the most studied red dwarfs because of its proximity and favorable location for observation near the celestial equator. Historically, research on Barnard's Star has focused on measuring its stellar characteristics, its astrometry, and also refining the limits of possible extrasolar planets. Although Barnard's Star is an ancient star, it still experiences star flare events, one being observed in 1998.
The star has also been the subject of some controversy. For a decade, from the early 1960s to the early 1970s, Peter van de Kamp claimed that there were one or more gas giants in orbit around it. Although the presence of small terrestrial planets around Barnard's Star remains a possibility, Van de Kamp's specific claims of large gas giants were refuted in the mid-1970s.
Overview.
Barnard's Star is a red dwarf of the dim spectral type M4, and it is too faint to see without a telescope. Its apparent magnitude is 9.5. This compares with a magnitude of −1.5 for Sirius – the brightest star in the night sky – and about 6.0 for the faintest objects visible with the naked eye (this magnitude scale is logarithmic, so the magnitude of 9.54 is only about 1/27th of the brightness of the faintest star that can be seen with the naked eye (under good viewing conditions).
At 7–12 billion years of age, Barnard's Star is considerably older than the Sun, which is 4.5 billion years old, and it might be among the oldest stars in the Milky Way galaxy. Barnard's Star has lost a great deal of rotational energy, and the periodic slight changes in its brightness indicate that it rotates once in 130 days (the Sun rotates in 25). Given its age, Barnard's Star was long assumed to be quiescent in terms of stellar activity. However, in 1998, astronomers observed an intense stellar flare, surprisingly showing that Barnard's Star is a flare star. Barnard's Star has the variable star designation V2500 Ophiuchi. In 2003, Barnard's Star presented the first detectable change in the radial velocity of a star caused by its motion. Further variability in the radial velocity of Barnard's Star was attributed to its stellar activity.
The proper motion of Barnard's Star corresponds to a relative lateral speed of 90 km/s. The 10.3 seconds of arc it travels annually amount to a quarter of a degree in a human lifetime, roughly half the angular diameter of the full Moon.
The radial velocity of Barnard's Star towards the Sun is measured from its blue shift to be 110 km/s. Combined with its proper motion, this gives a true velocity relative to the Sun of 143 km/s. Barnard's Star will make its closest approach to the Sun around AD 11,800, when it approaches to within about 3.75 light-years. However, at that time, Barnard's Star will not be the nearest star, since Proxima Centauri will have moved even closer to the Sun. Barnard's Star will still be too dim to be seen with the naked eye at the time of its closest approach, since its apparent magnitude will be about 8.5 then. After that it will gradually recede from the Sun.
Barnard's Star has a mass of about 0.14 solar masses (), and a radius 15% to 20% of that of the Sun. Thus, although Barnard's Star has roughly 150 times the mass of Jupiter (), its radius is only 1.5 to 2.0 times larger, due to its much higher density. Its effective temperature is 3,100 kelvins, and it has a visual luminosity of 0.0004 solar luminosities. Barnard's Star is so faint that if it were at the same distance from Earth as the Sun is, it would appear only 100 times brighter than a full moon, comparable to the brightness of the Sun at 80 astronomical units.
Barnard's Star's has 10–32% of the solar metallicity. Metallicity is the proportion of stellar mass made up of elements heavier than helium and helps classify stars relative to the galactic population. Barnard's Star seems to be typical of the old, red dwarf population II stars, yet these are also generally metal-poor halo stars. While sub-solar, Barnard's Star's metallicity is higher than that of a halo star and is in keeping with the low end of the metal-rich disk star range; this, plus its high space motion, have led to the designation "intermediate population II star", between a halo and disk star.
Claims of a planetary system.
For a decade from 1963 to about 1973, a substantial number of astronomers accepted a claim by Peter van de Kamp that he had detected, by using astrometry, a perturbation in the proper motion of Barnard's Star consistent with its having one or more planets comparable in mass with Jupiter. Van de Kamp had been observing the star from 1938, attempting, with colleagues at the Swarthmore College observatory, to find minuscule variations of one micrometre in its position on photographic plates consistent with orbital perturbations that would indicate a planetary companion; this involved as many as ten people averaging their results in looking at plates, to avoid systemic individual errors. Van de Kamp's initial suggestion was a planet having about at a distance of 4.4 AU in a slightly eccentric orbit, and these measurements were apparently refined in a 1969 paper. Later that year, Van de Kamp suggested that there were two planets of 1.1 and .
Other astronomers subsequently repeated Van de Kamp's measurements, and two papers in 1973 undermined the claim of a planet or planets. George Gatewood and Heinrich Eichhorn, at a different observatory and using newer plate measuring techniques, failed to verify the planetary companion. Another paper published by John L. Hershey four months earlier, also using the Swarthmore observatory, found that changes in the astrometric field of various stars correlated to the timing of adjustments and modifications that had been carried out on the refractor telescope's objective lens; the claimed planet was attributed to an artifact of maintenance and upgrade work. The affair has been discussed as part of a broader scientific review.
Van de Kamp never acknowledged any error and published a further claim of two planets' existence as late as 1982; he died in 1995. Wulff Heintz, Van de Kamp's successor at Swarthmore and an expert on double stars, questioned his findings and began publishing criticisms from 1976 onwards. The two men were reported to have become estranged from each other because of this.
Refining planetary boundaries.
While not completely ruling out the possibility of planets, null results for planetary companions continued throughout the 1980s and 1990s, the latest based on interferometric work with the Hubble Space Telescope in 1999. By refining the values of a star's motion, the mass and orbital boundaries for possible planets are tightened: in this way astronomers are often able to describe what types of planets cannot orbit a given star.
M dwarfs such as Barnard's Star are more easily studied than larger stars in this regard because their lower masses render perturbations more obvious. Gatewood was thus able to show in 1995 that planets with (the lower limit for brown dwarfs) were impossible around Barnard's Star, in a paper which helped refine the negative certainty regarding planetary objects in general. In 1999, work with the Hubble Space Telescope further excluded planetary companions of with an orbital period of less than 1,000 days (Jupiter's orbital period is 4,332 days), while Kuerster determined in 2003 that within the habitable zone around Barnard's Star, planets are not possible with an ""M" sin "i" value greater than 7.5 times the mass of the Earth (), or with a mass greater than 3.1 times the mass of Neptune (much lower than van de Kamp's smallest suggested value).
Even though this research has greatly restricted the possible properties of planets around Barnard's Star, it has not ruled them out completely; terrestrial planets would be difficult to detect. NASA's Space Interferometry Mission, which was to begin searching for extrasolar Earth-like planets, was reported to have chosen Barnard's Star as an early search target. However, this mission was shut down in 2010. ESA's similar Darwin interferometry mission had the same goal, but was stripped of funding in 2007.
Exploration.
Project Daedalus.
Barnard's Star was studied as part of Project Daedalus. Undertaken between 1973 and 1978, the study suggested that rapid, unmanned travel to another star system was possible with existing or near-future technology. Barnard's Star was chosen as a target partly because it was believed to have planets.
The theoretical model suggested that a nuclear pulse rocket employing nuclear fusion (specifically, electron bombardment of deuterium and helium-3) and accelerating for four years could achieve a velocity of 12% of the speed of light. The star could then be reached in 50 years, within a human lifetime. Along with detailed investigation of the star and any companions, the interstellar medium would be examined and baseline astrometric readings performed.
The initial Project Daedalus model sparked further theoretical research. In 1980, Robert Freitas suggested a more ambitious plan: a self-replicating spacecraft intended to search for and make contact with extraterrestrial life. Built and launched in Jovian orbit, it would reach Barnard's Star in 47 years under parameters similar to those of the original Project Daedalus. Once at the star, it would begin automated self-replication, constructing a factory, initially to manufacture exploratory probes and eventually to create a copy of the original spacecraft after 1,000 years.
1998 flare.
In 1998 a stellar flare on Barnard's Star was detected based on changes in the spectral emissions on July 17, 1998, during an unrelated search for variations in the proper motion. Four years passed before the flare was fully analyzed, at which point it was suggested that the flare's temperature was 8000 K, more than twice the normal temperature of the star. Given the essentially random nature of flares, Diane Paulson, one of the authors of that study, noted that "the star would be fantastic for amateurs to observe".
The flare was surprising because intense stellar activity is not expected in stars of such age. Flares are not completely understood, but are believed to be caused by strong magnetic fields, which suppress plasma convection and lead to sudden outbursts: strong magnetic fields occur in rapidly rotating stars, while old stars tend to rotate slowly. For Barnard's Star to undergo an event of such magnitude is thus presumed to be a rarity. Research on the star's periodicity, or changes in stellar activity over a given timescale, also suggest it ought to be quiescent; 1998 research showed weak evidence for periodic variation in the star's brightness, noting only one possible starspot over 130 days.
Stellar activity of this sort has created interest in using Barnard's Star as a proxy to understand similar stars. It is hoped that photometric studies of its X-ray and UV emissions will shed light on the large population of old M dwarfs in the galaxy. Such research has astrobiological implications: given that the habitable zones of M dwarfs are close to the star, any planets would be strongly influenced by solar flares, winds, and plasma ejection events.
Environment.
Barnard's Star shares much the same neighborhood as the Sun. The neighbors of Barnard's Star are generally of red dwarf size, the smallest and most common star type. Its closest neighbor is currently the red dwarf Ross 154, at 1.66 parsecs (5.41 light years) distance. The Sun and Alpha Centauri are, respectively, the next closest systems. From Barnard's Star, the Sun would appear on the diametrically opposite side of the sky at coordinates RA=, Dec=, in the eastern part of the constellation Monoceros. The absolute magnitude of the Sun is 4.83, and at a distance of 1.834 parsecs, it would be a first-magnitude star, as Pollux is from the Earth.

</doc>
<doc id="4199" url="https://en.wikipedia.org/wiki?curid=4199" title="Bayer designation">
Bayer designation

A Bayer designation is a stellar designation in which a specific star is identified by a Greek letter, followed by the genitive form of its parent constellation's Latin name. The original list of Bayer designations contained 1,564 stars.
Most of the brighter stars were assigned their first systematic names by the German astronomer Johann Bayer in 1603, in his star atlas "Uranometria". Bayer assigned a lower-case Greek letter, such as alpha (α), beta (β), gamma (γ), etc., to each star he catalogued, combined with the Latin name of the star’s parent constellation in genitive (possessive) form. (See 88 modern constellations for the genitive forms.) For example, Aldebaran is designated "α Tauri" (pronounced "Alpha Tauri"), which means "Alpha of the constellation Taurus".
A single constellation may contain fifty or more stars, but the Greek alphabet has only twenty-four letters. When these ran out, Bayer began using Latin letters: upper case "A", followed by lower case "b" through "z" (omitting "j" and "v"), for a total of another 24 letters. Bayer never went beyond "z", but later astronomers added more designations using both upper and lower case Latin letters, the upper case letters following the lower case ones in general. Examples include "s Carinae" ("s" of the constellation Carina), "d Centauri" ("d" of the constellation Centaurus), "G Scorpii" ("G" of the constellation Scorpius), and "N Velorum" ("N" of the constellation Vela). The last upper-case letter used in this way was "Q".
Bayer did not catalog southern stars not visible from Germany, but later astronomers (notably Lacaille and Gould) supplemented Bayer's catalog with entries for southern constellations.
Order by magnitude class.
In most constellations, Bayer assigned Greek and Latin letters to stars within a constellation in rough order of apparent brightness, from brightest to dimmest. Since the brightest star in a majority of constellations is designated Alpha (α), many people wrongly assume that Bayer meant to order the stars exclusively by brightness. In Bayer's day, however, stellar brightness could not be measured precisely. Stars were traditionally assigned to one of six magnitude classes (the brightest to first magnitude, the dimmest to sixth), and Bayer typically ordered stars within a constellation by class: all the first-magnitude stars, followed by all the second-magnitude stars, and so on. Within each magnitude class, Bayer made no attempt to arrange stars by relative brightness. As a result, the brightest star in each class did not always get listed first in Bayer's order.
But in addition, Bayer did not always follow the magnitude class rule; he sometimes assigned letters to stars according to their location within a constellation, or the order of their rising, or to historical or mythological details. Occasionally the order looks quite arbitrary.
Of the 88 modern constellations, there are at least 30 in which "Alpha" is not the brightest star, and four of those lack an alpha star altogether. (Constellations with no alpha include Vela and Puppis – both formerly part of Argo Navis, whose alpha is Canopus in Carina.)
Bayer designations in Orion.
Orion provides a good example of Bayer's method. Bayer first designated Betelgeuse and Rigel, the two 1st-magnitude stars (those of magnitude 1.5 or less), as Alpha and Beta from north to south, with Betelgeuse (the shoulder) coming ahead of Rigel (the foot), even though the latter is usually the brighter. (Betelgeuse is a variable star and can at its maximum occasionally outshine Rigel.) Bayer then repeated the procedure for the stars of the 2nd magnitude (those between magnitudes 1.51 and 2.5), labeling them from "gamma" through "zeta" in "top-down" (north-to-south) order.
Various Bayer designation arrangements.
The "First to Rise in the East" order is used in a number of instances. Castor and Pollux of Gemini may be an example of this: Pollux is brighter than Castor, but the latter rises earlier and was assigned "alpha". In this case, Bayer may also have been influenced by the traditional order of the mythological names "Castor and Pollux": Castor is generally named first whenever the twins are mentioned.
Although the brightest star in Draco is Eltanin (Gamma Draconis), Thuban was assigned "alpha" (α) by Bayer because, due to precession, Thuban was the north pole star 4,000 years ago.
Sometimes there is no apparent order, as exemplified by the stars in Sagittarius, where Bayer's designations appear almost random to the modern eye. Alpha and Beta Sagittarii are perhaps the most anomalously designated stars in the sky. They are more than two magnitudes fainter than the brightest star (designated Epsilon), they lie several degrees south of the main pattern (the "teapot" asterism), they are more than 20 degrees off the ecliptic in a Zodiacal constellation, and they do not even rise from Bayer's native Germany (while Epsilon and several other brighter stars do). The order of the letters assigned in Sagittarius does correspond to the magnitudes as illustrated on Bayer's chart; but the latter do not agree with modern determinations of the magnitudes.
Bayer designations added by later astronomers generally were ordered by magnitude, but care was usually taken to avoid conflict with designations already assigned. In Libra, for example, the new designations sigma, tau, and upsilon were chosen to avoid conflict with Bayer's earlier designations, even though several stars with earlier letters are not as bright.
Bayer's miscellaneous labels.
Although Bayer did not use upper-case Latin letters (except "A") for "fixed stars", he did use them to label other items shown on his charts, such as neighboring constellations, miscellaneous astronomical objects, or reference lines like the Tropic of Cancer. In Cygnus, for example, Bayer's fixed stars run through "g", and on this chart Bayer employs "H" through "P" as miscellaneous labels, mostly for neighboring constellations. Bayer did not intend such labels as catalog designations, but some have survived to refer to astronomical objects: P Cygni for example is still used as a designation for Nova Cyg 1600. In charts for constellations that did not exhaust the Greek letters, Bayer sometimes used the left-over Greek letters for miscellaneous labels as well.
Revised Bayer designations.
Ptolemy designated four stars as "border stars", each shared by two constellations: Alpheratz (in Andromeda and Pegasus), Elnath (in Taurus and Auriga), Nu Boötis (in Boötes and Hercules), and Fomalhaut (in Piscis Austrinus and Aquarius). Bayer assigned the first three of these stars a Greek letter from both constellations: , , and . (He catalogued Fomalhaut only once, as Alpha Piscis Austrini.) When the International Astronomical Union (IAU) assigned definite boundaries to the constellations in 1930, it declared that stars and other celestial objects can belong to only one constellation. Consequently, the redundant second designation in each pair above has dropped out of use.
Bayer assigned two stars duplicate names by mistake: (duplicated as ) and (duplicated as ). He corrected these in a later atlas, and the duplicate names were no longer used.
Other cases of multiple Bayer designations arose when stars named by Bayer in one constellation were transferred by later astronomers to a different constellation. Bayer's Gamma and Omicron Scorpii, for example, were later reassigned from Scorpius to Libra and given the new names Sigma and Upsilon Librae. (To add to the confusion, the star now known as Omicron Scorpii was not named by Bayer but was assigned the designation o Scorpii (Latin lower case 'o') by Lacaille – which later astronomers misinterpreted as omicron once Bayer's omicron had been reassigned to Libra.)
A few stars no longer lie (according to the modern constellation boundaries) within the constellation for which they are named. The proper motion of Rho Aquilae, for example, recently carried it across the boundary into Delphinus.
Bayer designation styles.
Bayer designations are most often written as the Greek or Latin letter followed by the standard 3-character constellation abbreviation: α CMa, β Per; or occasionally with the constellation genitive in full: α Canis Majoris, β Persei. Earlier 4-letter abbreviations (α CMaj, β Pers) are rarely used today. The Greek letter names are sometimes written out as well: Alpha Canis Majoris, Beta Persei.
Other Bayer designations.
The Latin-letter extended designations are not as commonly used as the Greek-letter ones, but there are some exceptions such as h Persei (which is actually a star cluster) and P Cygni. Uppercase Latin Bayer designations in modern use do not go beyond Q; names such as R Leporis and W Ursae Majoris are variable star designations, not Bayer designations.
A further complication is the use of numeric superscripts to distinguish neighboring stars that Bayer (or a later astronomer) labeled with a common letter. Usually these are double stars (mostly optical doubles rather than true binary stars), but there are some exceptions such as the chain of stars π, π, π, π, π and π Orionis.

</doc>
<doc id="4200" url="https://en.wikipedia.org/wiki?curid=4200" title="Boötes">
Boötes

Boötes is a constellation in the northern sky, located between 0° and +60° declination, and 13 and 16 hours of right ascension on the celestial sphere. The name comes from the Greek Βοώτης, "Boōtēs", meaning herdsman or plowman (literally, ox-driver; from βοῦς "bous" “cow”). The "ö" in the name is a diaeresis, not an umlaut, meaning that each 'o' is to be pronounced separately.
One of the 48 constellations described by the 2nd century astronomer Ptolemy, Boötes is now one of the 88 modern constellations. It contains the fourth brightest star in the night sky, the orange-hued Arcturus. Boötes is home to many other bright stars, including eight above the fourth magnitude and an additional 21 above the fifth magnitude, making a total of 29 stars
History and mythology.
In ancient Babylon the stars of Boötes were known as SHU.PA. They were apparently depicted as the god Enlil, who was the leader of the Babylonian pantheon and special patron of farmers.
The name "Boötes" was first used by Homer in his Odyssey as a celestial reference point for navigation, described as "late-setting" or "slow to set", translated as the "Plowman". Exactly whom Boötes is supposed to represent in Greek mythology is not clear. According to one version, he was a son of Demeter, Philomenus, twin brother of Plutus, a ploughman who drove the oxen in the constellation Ursa Major. This is corroborated by the constellation's name, which itself means "ox-driver" or "herdsman." The ancient Greeks saw the asterism now called the "Big Dipper" or "Plough" as a cart with oxen. This influenced the name's etymology, derived from the Greek for "noisy" or "ox-driver". Another myth associated with Boötes tells that he invented the plow and was memorialized for his ingenuity as a constellation.
Another myth associated with Boötes by Hyginus is that of Icarius, who was schooled as a grape farmer and winemaker by Dionysus. Icarius made wine so strong that those who drank it appeared poisoned, which caused shepherds to avenge their supposedly poisoned friends by killing Icarius. Maera, Icarius's dog, brought his daughter Erigone to her father's body, whereupon both she and the dog committed suicide. Zeus then chose to honor all three by placing them in the sky as constellations: Icarius as Boötes, Erigone as Virgo, and Maera as Canis Major or Canis Minor.
Following another reading, the constellation is identified with Arcas and also referred to as Arcas and Arcturus, son of Zeus and Callisto. Arcas was brought up by his maternal grandfather Lycaon, to whom one day Zeus went and had a meal. To verify that the guest was really the king of the gods, Lycaon killed his grandson and prepared a meal made from his flesh. Zeus noticed and became very angry, transforming Lycaon into a wolf and gave back life to his son. In the meantime Callisto had been transformed into a she-bear, by Zeus's wife, Hera, who was angry at Zeus's infidelity. This is corroborated by the Greek name for Boötes, "Arctophylax", which means "Bear Watcher". Callisto in form of a bear was almost killed by her son who was out hunting. Zeus rescued her, taking her into the sky where she became Ursa Major, "the Great Bear". The name Arcturus (the constellation's brightest star) comes from the Greek word meaning "guardian of the bear". Sometimes Arcturus is depicted as leading the hunting dogs of nearby Canes Venatici and driving the bears of Ursa Major and Ursa Minor.
Several former constellations were formed from stars now included in Boötes. Quadrans Muralis, the Quadrant, was a constellation created near Beta Boötis from faint stars. It was invented in 1795 by Jérôme Lalande, an astronomer who used a quadrant to perform detailed astronometric measurements. Lalande worked with Nicole-Reine Lepaute and others to predict the 1758 return of Halley's Comet. Quadrans Muralis was formed from the stars of eastern Boötes, western Hercules, and Draco. It was originally called "Le Mural" by Jean Fortin in his 1795 Atlas Céleste; it was not given the name "Quadrans Muralis" until Johann Bode's 1801 "Uranographia". The constellation was quite faint, with its brightest stars reaching the 5th magnitude. Mons Maenalus, representing the Maenalus mountains, was created by Johannes Hevelius in 1687 at the foot of the constellation's figure. The mountain was named for the son of Lycaon, Maenalus. The mountain, one of Diana's hunting grounds, was also holy to Pan.
Non-Western astronomy.
The stars of Boötes were incorporated into many different Chinese constellations. Arcturus was part of the most prominent of these, variously designated as the celestial king's throne ("Tian Wang") or the Blue Dragon's horn ("Daijiao"); the name "Daijiao", meaning "great horn", is more common. Arcturus was given such importance in Chinese celestial mythology because of its status marking the beginning of the lunar calendar, as well as its status as the brightest star in the northern night sky. Two constellations flanked "Daijiao", "Yousheti" to the right and "Zuosheti" to the left; they represented companions that orchestrated the seasons. "Zuosheti" was formed from modern Zeta, Omicron, and Pi Boötis, while "Yousheti" was formed from modern Eta, Tau, and Upsilon Boötis. "Dixi", the Emperor's ceremonial banquet mat, was north of Arcturus, consisting of the stars 12, 11, and 9 Boötis. Another northern constellation was "Qigong", the Seven Dukes, which was mostly across the Boötes-Hercules border. It included either Delta Boötis or Beta Boötis as its terminus.
The other Chinese constellations made up of the stars of Boötes existed in the modern constellation's north; they are all representations of weapons. "Tianqiang", the spear, was formed from Iota, Kappa, and Theta Boötis; "Genghe", variously representing a lance or shield, was formed from Epsilon, Rho, and Sigma Boötis. There were also two weapons made up of a singular star. "Xuange", the halberd, was represented by Lambda Boötis, and "Zhaoyao", either the sword or the spear, was represented by Gamma Boötis.
Two Chinese constellations have an uncertain placement in Boötes. "Kangchi", the lake, was placed south of Arcturus, though its specific location is disputed. It may have been placed entirely in Boötes, on either side of the Boötes-Virgo border, or on either side of the Virgo-Libra border. The constellation "Zhouding", a bronze tripod-mounted container used for food, was sometimes cited as the stars 1, 2, and 6 Boötis. However, it has also been associated with three stars in Coma Berenices.
Characteristics.
Boötes is a constellation bordered by Virgo to the south, Coma Berenices and Canes Venatici to the west, Ursa Major to the northwest, Draco to the northeast, and Hercules, Corona Borealis and Serpens Caput to the east. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is 'Boo'. The official constellation boundaries, as set by Eugène Delporte in 1930, are defined by a polygon of 16 segments. In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates stretch from +7.36° to +55.1°. Covering 907 square degrees, Boötes culminates at midnight around 2 May and ranks 13th in area.
Colloquially, its pattern of stars has been likened to a kite or ice cream cone. However, depictions of Boötes have varied historically. Aratus described him circling the north pole, herding the two bears. Later ancient Greek depictions, described by Ptolemy, have him holding the reins of his hunting dogs (Canes Venatici) in his left hand, with a spear, club, or staff in his right hand. After Hevelius introduced Mons Maenalus in 1681, Boötes was often depicted standing on the Peloponnese mountain. By 1801, when Johann Bode published his "Uranographia", Boötes had acquired a sickle, which was also held in his left hand.
The placement of Arcturus has also been mutable through the centuries. Traditionally, Arcturus lay between his thighs, as Ptolemy depicted him. However, Germanicus Caesar deviated from this tradition by placing Arcturus "where his garment is fastened by a knot".
Notable features.
Stars.
In his "Uranometria", Johann Bayer used the Greek letters Alpha through to Omega and then A to k to label what he saw as the most prominent 35 stars in the constellation, with subsequent astronomers splitting Kappa, Mu, Nu and Pi as two stars each. Nu is also the same star as Psi Herculis. John Flamsteed numbered 54 stars for the constellation.
Located 36.7 light-years from Earth, Arcturus, or Alpha Boötis, is the brightest star in Boötes and the fourth brightest star in the sky at an apparent magnitude of −0.05; It is also the brightest star north of the celestial equator, just shading out Vega and Capella. Its name comes from the Greek for "bear-keeper". An orange giant of spectral class K1.5III, Arcturus is an ageing star that has exhausted its core supply of hydrogen and cooled and expanded to a diameter of 27 solar diameters, equivalent to approximately 32 million kilometers. Though its mass is approximately one solar mass (), Arcturus shines with 133 times the luminosity of the Sun (). Bayer located Arcturus above the herdsman's left knee in his "Uranometria". Nearby Eta Boötis, or Muphrid, is the uppermost star denoting the left leg. It is a 2.68-magnitude star 37 light-years distant with a spectral class of G0IV, indicating it has just exhausted its core hydrogen and is beginning to expand and cool. It is 9 times as luminous as the Sun and has 2.7 times its diameter. Analysis of its spectrum reveals that it is a spectroscopic binary. Muphrid and Arcturus lie only 3.3 light years away from each other. Viewed from Arcturus, Muphrid would have a visual magnitude of -2½, while Arcturus would be around visual magnitude -4½ when seen from Muphrid.
Marking the herdsman's head is Beta Boötis, or Nekkar, a yellow giant of magnitude 3.5 and spectral type G8IIIa. Like Arcturus, it has expanded and cooled off the main sequence—likely to have lived most of its stellar life as a blue-white B-type main sequence star. Its common name comes from the Arabic phrase for "ox-driver". It is 219 light-years away and has a luminosity of . Located 86 light-years distant, Gamma Boötis, or Seginus, is a white giant star of spectral class A7III, with a luminosity 34 times and diameter 3.5 times that of the Sun. It is a Delta Scuti variable, ranging between magnitudes 3.02 and 3.07 every 7 hours. Thes stars are short period (six hours at most) pulsating stars that have been used as standard candles and as subjects to study astroseismology. Delta Boötis is a wide double star with a primary of magnitude 3.5 and a secondary of magnitude 7.8. The primary is a yellow giant that has cooled and expanded to 10.4 times the diameter of the Sun. Of spectral class G8IV, it is around 121 light-years away, while the secondary is a yellow main sequence star of spectral type G0V. The two are thought to take 120,000 years to orbit each other. Mu Boötis, known as Alkalurops, is a triple star popular with amateur astronomers. It has an overall magnitude of 4.3 and is 121 light-years away. Its name is from the Arabic phrase for "club" or "staff". The primary appears to be of magnitude 4.3 and is blue-white. The secondary appears to be of magnitude 6.5, but is actually a close double star itself with a primary of magnitude 7.0 and a secondary of magnitude 7.6. The secondary and tertiary stars have an orbital period of 260 years. The primary has an absolute magnitude of 2.6 and is of spectral class F0. The secondary and tertiary stars are separated by 2 arcseconds; the primary and secondary are separated by 109.1 arcseconds at an angle of 171 degrees. Nu Boötis is an optical double star. The primary is an orange giant of magnitude 5.0 and the secondary is a white star of magnitude 5.0. The primary is 870 light-years away and the secondary is 430 light-years.
Epsilon Boötis, also known as "Izar" or "Pulcherrima", is a close triple star popular with amateur astronomers and the most prominent binary star in Boötes. The primary is a yellow- or orange-hued magnitude 2.5 giant star, the secondary is a magnitude 4.6 blue-hued main-sequence star, and the tertiary is a magnitude 12.0 star. The system is 210 light-years away. The name "Izar" comes from the Arabic word for "girdle" or "loincloth", referring to its location in the constellation. The name "Pulcherrima" comes from the Latin phrase for "most beautiful", referring to its contrasting colors in a telescope. The primary and secondary stars are separated by 2.9 arcseconds at an angle of 341 degrees; the primary's spectral class is K0 and it has a luminosity of . To the naked eye, Izar has a magnitude of 2.37. Nearby Rho and Sigma Boötis denote the herdsman's waist. Rho is an orange giant of spectral type K3III located around 160 light-years from Earth. It is ever so slightly variable, wavering by 0.003 of a magnitude from its average of 3.57. Sigma, a yellow-white main sequence star of spectral type F3V, is suspected of varying in brightness from 4.45 to 4.49. It is around 52 light-years distant.
Traditionally known as "Al Aulād al Dhiʼbah" (ألعولد ألذعب - "al aulād al dhiʼb"), "the Whelps of the Hyenas", Theta, Iota, Kappa and Lambda Boötis are a small group of stars in the far north of the constellation. The magnitude 4.05 Theta Boötis has a spectral type of F7 and an absolute magnitude of 3.8. Iota Boötis is a triple star with a primary of magnitude 4.8 and spectral class of A7, a secondary of magnitude 7.5, and a tertiary of magnitude 12.6. The primary is 97 light-years away. The primary and secondary stars are separated by 38.5 arcseconds, at an angle of 33 degrees. The primary and tertiary stars are separated by 86.7 arcseconds at an angle of 194 degrees. Both the primary and tertiary appear white in a telescope, but the secondary appears yellow-hued. Kappa Boötis is another wide double star. The primary is 155 light-years away and has a magnitude of 4.5. The secondary is 196 light-years away and has a magnitude of 6.6. The two components are separated by 13.4 arcseconds, at an angle of 236 degrees. The primary, with spectral class A7, appears white and the secondary appears bluish. An apparent magnitude 4.18 type A0p star, Lambda Boötis is the prototype of a class of chemically peculiar stars, only some of which pulsate as Delta Scuti type stars. The distinction between the Lambda Boötis stars as a class of stars with peculiar spectra, and the delta Scuti stars whose class describes pulsation in low-overtone pressure modes, is an important one. While many Lambda Boötis stars pulsate and are delta Scuti stars, not many delta Scuti stars have Lambda Boötis peculiarities, since the Lambda Boötis stars are a much rarer class whose members can be found both inside and outside the delta Scuti instability strip. Lambda Boötis stars are dwarf stars that can be either spectral class A or F. Like BL Boötis-type stars they are metal-poor. Scientists have had difficulty explaining the characteristics of Lambda Boötis stars, partly because only around 60 confirmed members exist, but also due to heterogeneity in the literature. Lambda has an absolute magnitude of 1.8.
There are two dimmer F-type stars, magnitude 4.83 12 Boötis, class F8; and magnitude 4.93 45 Boötis, class F5. Xi Boötis is a G8 yellow dwarf of magnitude 4.55, and absolute magnitude is 5.5. Two dimmer G-type stars are magnitude 4.86 31 Boötis, class G8, and magnitude 4.76 44 Boötis, class G0.
Of apparent magnitude 4.06, Upsilon Boötis has a spectral class of K5 and an absolute magnitude of −0.3. Dimmer than Upsilon Boötis is magnitude 4.54 Phi Boötis, with a spectral class of K2 and an absolute magnitude of −0.1. Just slightly dimmer than Phi at magnitude 4.60 is O Boötis, which, like Izar, has a spectral class of K0. O Boötis has an absolute magnitude of 0.2. The other four dim stars are magnitude 4.91 6 Boötis, class K4; magnitude 4.86 20 Boötis, class K3; magnitude 4.81 Omega Boötis, class K4; and magnitude 4.83 A Boötis, class K1.
There is one bright B-class star in Boötes; magnitude 4.93 Pi Boötis, also called Alazal. It has a spectral class of B9 and is 40 parsecs from Earth. There is also one M-type star, magnitude 4.81 34 Boötis. It is of class gM0.
Multiple stars.
Besides Pulcherrima and Alkalurops, there are several other binary stars in Boötes:
44 Boötis (i Boötis) is a double variable star 42 light-years away. It has an overall magnitude of 4.8 and appears yellow to the naked eye. The primary is of magnitude 5.3 and the secondary is of magnitude 6.1; their orbital period is 220 years. The secondary is itself an eclipsing variable star with a range of 0.6 magnitudes; its orbital period is 6.4 hours. It is a W Ursae Majoris variable that ranges in magnitude from a minimum of 7.1 to a maximum of 6.5 every 0.27 days. Both stars are G-type stars. Another eclipsing binary star is ZZ Boötis, which has two F2-type components of almost equal mass, and ranges in magnitude from a minimum of 6.79 to a maximum of 7.44 over a period of 5.0 days.
Variable stars.
Two of the brighter Mira-type variable stars in the constellation are R and S Boötis. Both are red giants that range greatly in magnitude—from 6.2 to 13.1 over 223.4 days, and 7.8 to 13.8 over a period of 270.7 days respectively. Also red giants, V and W Boötis are semi-regular variable stars that range in magnitude from 7.0 to 12.0 over a period of 258 days, and magnitude 4.7 to 5.4 over 450 days respectively.
BL Boötis is the prototype of its class of pulsating variable stars, the anomalous Cepheids. These stars are somewhat similar to Cepheid variables, but they do not have the same relationship between their period and luminosity. Their periods are similar to RRAB variables; however, they are far brighter than these stars. BL Boötis is a member of the cluster NGC 5466. Anomalous Cepheids are metal poor and have masses not much larger than the Sun's, on average, . BL Boötis type stars are a subtype of RR Lyrae variables.
T Boötis was a nova observed in April 1860 at a magnitude of 9.7. It has never been observed since, but that does not preclude the possibility of it being a highly irregular variable star or a recurrent nova.
Stars with planetary systems.
Extrasolar planets have been discovered encircling ten stars in Boötes as of 2012. Tau Boötis is orbited by a large planet, discovered in 1999. The host star itself is a magnitude 4.5 star of type F7V, 15.6 parsecs from Earth. It has a mass of and a radius of 1.331 solar radii (); a companion, GJ527B, orbits at a distance of 240 AU. Tau Boötis b, the sole planet discovered in the system, orbits at a distance of 0.046 AU every 3.31 days. Discovered through radial velocity measurements, it has a mass of 5.95 Jupiter masses (). This makes it a hot Jupiter. The host star and planet are tidally locked, meaning that the planet's orbit and the star's particularly high rotation are synchronized. Furthermore, a slight variability in the host star's light may be caused by magnetic interactions with the planet. Carbon monoxide is present in the planet's atmosphere. Tau Boötis b does not transit its star, rather, its orbit is inclined 46 degrees. Like Tau Boötis b, HAT-P-4 b is also a hot Jupiter. It is noted for orbiting a particularly metal-rich host star and being of low density. Discovered in 2007, HAT-P-4 b has a mass of and a radius of . It orbits every 3.05 days at a distance of 0.04 AU. HAT-P-4, the host star, is an F-type star of magnitude 11.2, 310 parsecs from Earth. It is larger than the Sun, with a mass of and a radius of .
Boötes is also home to multiple-planet systems. HD 128311 is the host star for a two-planet system, consisting of HD 128311 b and HD 128311 c, discovered in 2002 and 2005, respectively. HD 128311 b is the smaller planet, with a mass of ; it was discovered through radial velocity observations. It orbits at almost the same distance as Earth, at 1.099 AU; however, its orbital period is significantly longer at 448.6 days. The larger of the two, HD 128311 c, has a mass of and was discovered in the same manner. It orbits every 919 days inclined at 50°, and is 1.76 AU from the host star. The host star, HD 128311, is a K0V-type star located 16.6 parsecs from Earth. It is smaller than the Sun, with a mass of and a radius of ; it also appears below the threshold of naked-eye visibility at an apparent magnitude of 7.51.
There are several single-planet systems in Boötes. HD 132406 is a Sun-like star of spectral type G0V with an apparent magnitude of 8.45, 231.5 light-years from Earth. It has a mass of and a radius of . The star is orbited by a gas giant, HD 132406 b, discovered in 2007. HD 132406 orbits 1.98 AU from its host star with a period of 974 days and has a mass of . The planet was discovered by the radial velocity method. WASP-23 is a star with one orbiting planet, WASP-23 b. The planet, discovered by the transit method in 2010, orbits every 2.944 very close to its Sun, at 0.0376 AU. It is smaller than Jupiter, at and . Its star is a K1V type star of apparent magnitude 12.7, far below naked-eye visibility, and smaller than the Sun at and . HD 131496 is also encircled by one planet, HD 131496 b. The star is of type K0 and is located 110 parsecs from Earth; it appears at a visual magnitude of 7.96. It is significantly larger than the Sun, with a mass of and a radius of 4.6 solar radii. Its one planet, discovered in 2011 by the radial velocity method, has a mass of ; its radius is as yet undetermined. HD 131496 b orbits at a distance of 2.09 AU with a period of 883 days.
Another single planetary system in Boötes is the HD 132563 system, a triple star system. The parent star, technically HD 132563B, is a star of magnitude 9.47, 96 parsecs from Earth. It is almost exactly the size of the Sun, with the same radius and a mass only 1% greater. Its planet, HD 132563B b, was discovered in 2011 by the radial velocity method. , it orbits 2.62 AU from its star with a period of 1544 days. Its orbit is somewhat elliptical, with an eccentricity of 0.22. HD 132563B b is one of very few planets found in triple star systems; it orbits the isolated member of the system, which is separated from the other components, a spectroscopic binary, by 400 AU. Also discovered through the radial velocity method, albeit a year earlier, is HD 136418 b, a 2-Jupiter mass planet that orbits the star HD 136418 at a distance of 1.32 AU with a period of 464.3 days. Its host star is a magnitude 7.88 G5-type star, 98.2 parsecs from Earth. It has a radius of and a mass of .
WASP-14 b is one of the most massive and dense exoplanets known, with a mass of and a radius of . Discovered via the transit method, it orbits 0.036 AU from its host star with a period of 2.24 days. WASP-14 b has a density of 4.6 grams per cubic centimeter, making it one of the densest exoplanets known. Its host star, WASP-14, is an F5V-type star of magnitude 9.75, 160 parsecs from Earth. It has a radius of and a mass of . It also has a very high proportion of lithium.
Deep-sky objects.
Boötes is in a part of the celestial sphere facing away from the plane of our home Milky Way galaxy, and so does not have open clusters or nebulae. Instead, it has one bright globular cluster and many faint galaxies. The globular cluster NGC 5466 has an overall magnitude of 9.1 and a diameter of 11 arcminutes. It is a very loose globular cluster with fairly few stars and may appear as a rich, concentrated open cluster in a telescope. NGC 5466 is classified as a Shapley-Sawyer Concentration Class 12 cluster, reflecting its sparsity. Its fairly large diameter means that it has a low surface brightness, so it appears far dimmer than the catalogued magnitude of 9.1 and requires a large amateur telescope to view. Only approximately 12 stars are resolved by an amateur instrument.
Boötes has two bright galaxies. NGC 5248 (Caldwell 45) is a type Sc galaxy (a variety of spiral galaxy) of magnitude 10.2. It measures 6.5 by 4.9 arcminutes. 50 million light-years from Earth, NGC 5248 is a member of the Virgo Cluster of galaxies; it has dim outer arms and obvious H II regions, dust lanes, and young star clusters. NGC 5676 is another type Sc galaxy of magnitude 10.9. It measures 3.9 by 2.0 arcminutes. Other galaxies include NGC 5008, a type Sc emission-line galaxy, NGC 5548, a type S Seyfert galaxy, NGC 5653, a type S HII galaxy, NGC 5778 (also classified as NGC 5825), a type E galaxy that is the brightest of its cluster, NGC 5886, and NGC 5888, a type SBb galaxy. NGC 5698 is a barred spiral galaxy, notable for being the host of the 2005 supernova SN 2005bc, which peaked at magnitude 15.3.
Further away lies the 250 million light-year diameter Boötes void, a huge space largely empty of galaxies. Discovered by Robert Kirshner and colleagues in 1981, it is roughly 700 million light years from Earth.Beyond it and within the bounds of the constellation, lie two superclusters at around 830 million and 1 billion light years distant.
The Hercules–Corona Borealis Great Wall, the largest known structure in the Universe, covers a significant part of Boötes.
Meteor showers.
Boötes is home to the Quadrantid meteor shower, the most prolific annual meteor shower. It was discovered in January 1835 and named in 1864 by Alexander Hershell. The radiant is located in northern Boötes near Kappa Boötis, in its namesake former constellation of Quadrans Muralis. Quadrantid meteors are dim, but have a peak visible hourly rate of approximately 100 per hour on January 3–4. The zenithal hourly rate of the Quadrantids is approximately 130 meteors per hour at their peak; it is also a very narrow shower. The Quadrantids are notoriously difficult to observe because of a low radiant and often inclement weather. The parent body of the meteor shower has been disputed for decades; however, Peter Jenniskens has proposed 2003 EH, a minor planet, as the parent. 2003 EH may be linked to C/1490 Y, a comet previously thought to be a potential parent body for the Quadrantids. 2003 EH is a short-period comet of the Jupiter family; 500 years ago, it experienced a catastrophic breakup event. It is now dormant. The Quadrantids had notable displays in 1982, 1985, and 2004. Meteors from this shower often appear to have a blue hue and travel at a moderate speed of 41.5–43 kilometers per second.
On April 28, 1984, a remarkable outburst of the normally placid Alpha Bootids was observed by visual observer Frank Witte from 00:00 to 2:30 UTC. In a 6 cm telescope, he observed 433 meteors in a field of view near Arcturus with a diameter of less than 1°. Peter Jenniskens comments that this outburst resembled a "typical dust trail crossing". The Alpha Bootids normally begin on April 14, peaking on April 27 and 28, and finishing on May 12. Its meteors are slow-moving, with a velocity of 20.9 kilometers per second. They may be related to Comet 73P/Schwassmann-Wachmann 3, but this connection is only theorized.
The June Bootids, also known as the Iota Draconids, is a meteor shower associated with the comet 7P/Pons-Winnecke, first recognized on May 27, 1916, by William F. Denning. The shower, with its slow meteors, was not observed prior to 1916 because Earth did not cross the comet's dust trail until Jupiter perturbed Pons-Winnecke's orbit, causing it to come within 0.03 AU of Earth's orbit the first year the June Bootids were observed. In 1982, E. A. Reznikov discovered that the 1916 outburst was caused by material released from the comet in 1819. Another outburst of the June Bootids was not observed until 1998, because Comet Pons-Winnecke's orbit was not in a favorable position. However, on June 27, 1998, an outburst of meteors radiating from Boötes, later confirmed to be associated with Pons-Winnecke, was observed. They were incredibly long-lived, with trails of the brightest meteors lasting several seconds at times. Many fireballs, green-hued trails, and even some meteors that cast shadows were observed throughout the outburst, which had a maximum zenithal hourly rate of 200–300 meteors per hour. In 2002, two Russian astronomers determined that material ejected from the comet in 1825 was responsible for the 1998 outburst. Ejecta from the comet dating to 1819, 1825, and 1830 was predicted to enter Earth's atmosphere on June 23, 2004. The predictions of a shower less spectacular than the 1998 showing were borne out in a display that had a maximum zenithal hourly rate of 16–20 meteors per hour that night. The June Bootids are not expected to have another outburst in the next 50 years. Typically, only 1–2 dim, very slow meteors are visible per hour; the average June Bootid has a magnitude of 5.0. It is related to the Alpha Draconids and the Bootids-Draconids. The shower lasts from June 27 to July 5, with a peak on the night of June 28. The June Bootids are classified as a class III shower (variable), and has an average entry velocity of 18 kilometers per second. Its radiant is located 7 degrees north of Beta Boötis.
The Beta Bootids is a weak shower that begins on January 5, peaks on January 16, and ends on January 18. Its meteors travel at 43 kilometers/second. The January Bootids is a short, young meteor shower that begins on January 9, peaks from January 16 to January 18, and ends on January 18. The Phi Bootids is another weak shower radiating from Boötes. It begins on April 16, peaks on April 30 and May 1, and ends on May 12. Its meteors are slow-moving, with a velocity of 15.1 km/s. They were discovered in 2006. The shower's peak hourly rate can be as high as 6 meteors per hour. Though named for a star in Boötes, the Phi Bootid radiant has moved into Hercules. The meteor stream is associated with three different asteroids: 1620 Geographos, 2062 Aten, and 1978 CA. The Lambda Bootids, part of the Bootid-Coronae Borealid Complex, are a weak annual shower with moderately fast meteors; 41.75 km/s. The complex includes the Lambda Bootids, as well as the Theta Coronae Borealids and Xi Coronae Borealids. All of the Bootid-Coronae Borealid showers are Jupiter family comet showers; the streams in the complex have highly inclined orbits.
There are several minor showers in Boötes, some of whose existence is yet to be verified. The Rho Bootids radiate from near the namesake star, and were hypothesized in 2010. The average Rho Bootid has an entry velocity of 43 km/s. It peaks in November and lasts for 3 days. The Rho Bootid shower is part of the SMA complex, a group of meteor showers related to the Taurids, which is in turn linked to the comet 2P/Encke. However, the link to the Taurid shower remains unconfirmed and may be a chance correlation. Another such shower is the Gamma Bootids, which were hypothesized in 2006. Gamma Bootids have an entry velocity of 50.3 km/s. The Nu Bootids, hypothesized in 2012, have faster meteors, with an entry velocity of 62.8 km/s.
References.
Citations
References

</doc>
<doc id="4203" url="https://en.wikipedia.org/wiki?curid=4203" title="Bernardino Ochino">
Bernardino Ochino

Bernardino Ochino (1487–1564) was an Italian, who was raised a Roman Catholic and later turned to Protestantism.
Biography.
Bernardino Ochino was born in Siena, the son of the barber Domenico Ochino, and at the age of 7 or 8, in around 1504, was entrusted to the order of Franciscan Friars. From 1510 he studied medicine at Perugia.
Transfer to the Capuchins.
At the age of 38, Ochino transferred himself in 1534 to the newly founded Order of Friars Minor Capuchin. By then he was the close friend of Juan de Valdés, Pietro Bembo, Vittoria Colonna, Pietro Martire, Carnesecchi. In 1538 he was elected vicar-general of his order. In 1539, urged by Bembo, he visited Venice and delivered a course of sermons showing a sympathy with justification by faith, which appeared more clearly in his "Dialogues" published the same year. He was suspected and denounced, but nothing ensued until the establishment of the Inquisition in Rome in June 1542, at the instigation of Cardinal Giovanni Pietro Carafa. Ochino received a citation to Rome, and set out to obey it about the middle of August. According to his own statement, he was deterred from presenting himself at Rome by the warnings of Cardinal Contarini, whom he found at Bologna, dying of poison administered by the reactionary party.
Escape to Geneva.
Ochino turned aside to Florence, and after some hesitation went across the Alps to Geneva. He was cordially received by John Calvin, and published within two years several volumes of "Prediche", controversial tracts rationalizing his change of religion. He also addressed replies to marchioness Vittoria Colonna, Claudio Tolomei, and other Italian sympathizers who were reluctant to go to the same length as himself. His own breach with the Roman Catholic Church was final.
Augsburg and England.
In 1545 Ochino became minister of the Italian Protestant congregation at Augsburg. From this time dates his contact with Caspar Schwenckfeld. He was compelled to flee when, in January 1547, the city was occupied by the imperial forces for the Diet of Augsburg.
Ochino found asylum in England, where he was made a prebendary of Canterbury Cathedral, received a pension from Edward VI's privy purse, and composed his major work, the "Tragoedie or Dialoge of the unjuste usurped primacie of the Bishop of Rome". This text, originally written in Latin, is extant only in the 1549 translation of Bishop John Ponet. The form is a series of dialogues. Lucifer, enraged at the spread of Jesus's kingdom, convokes the fiends in council, and resolves to set up the pope as antichrist. The state, represented by the emperor Phocas, is persuaded to connive at the pope's assumption of spiritual authority; the other churches are intimidated into acquiescence; Lucifer's projects seem fully accomplished, when Heaven raises up Henry VIII of England and his son for their overthrow.
Several of Ochino's "Prediche" were translated into English by Anna Cooke; and he published numerous controversial treatises on the Continent.
Zürich.
In 1553 the accession of Mary I drove Ochino from England. He went to Basel, where Lelio Sozzini and the lawyer Martino Muralto were sent to secure Ochino as pastor of the Italian church at Zürich, which Ochino accepted. The Italian congregation there was composed mainly of refugees from Locarno. There for 10 years Ochino wrote books which gave increasing evidence of his alienation from the orthodoxy around him. The most important of these was the "Labyrinth", a discussion of the freedom of the will, covertly undermining the Calvinistic doctrine of predestination.
In 1563 a long simmering storm burst on Ochino with the publication of his "Thirty Dialogues", in one of which his adversaries maintained that he had justified polygamy under the disguise of a pretended refutation. His dialogues on divorce and against the Trinity were also considered heretical.
Poland, and death.
Ochino was not given opportunity to defend himself, and was banished from Zürich. After being refused admission by other Protestant cities, he directed his steps towards Poland, at that time the most tolerant state in Europe. He had not resided there long when an edict appeared (August 8, 1564) banishing all foreign dissidents. Fleeing the country, he encountered the plague at Pińczów; three of his four children were carried off; and he himself, worn out by misfortune, died in solitude and obscurity at Slavkov in Moravia, about the end of 1564.
Legacy.
Ochino's reputation among Protestants was low. He was charged by Thomas Browne in 1643 with the authorship of the legendary-apocryphal heretical treatise "De tribus Impostoribus", as well as with having carried his alleged approval of polygamy into practice.
His biographer Karl Benrath justified him, representing him as a fervent evangelist and at the same time as a speculative thinker with a passion for free inquiry. The picture is of Ochino always learning and unlearning and arguing out difficult questions with himself in his dialogues, frequently without attaining to any absolute conviction. 

</doc>
<doc id="4204" url="https://en.wikipedia.org/wiki?curid=4204" title="Bay of Quinte">
Bay of Quinte

The Bay of Quinte is a long, narrow bay shaped like the letter "Z" on the northern shore of Lake Ontario in the province of Ontario, Canada. It is just west of the head of the Saint Lawrence River that drains the Great Lakes into the Gulf of Saint Lawrence. It is located about east of Toronto and west of Montreal.
The name "Quinte" is derived from "Kente", which was the name of an early French Catholic mission located on the north shore of what is now Prince Edward County. Officially, in the Mohawk language, the community is called "Kenhtè:ke" which means "the place of the bay". The Cayuga name is "Tayęda:ne:gęˀ or Detgayę:da:negęˀ", "land of two logs."
The Bay, as it is known locally, provides some of the best trophy Walleye angling in North America as well as most sport fish common to the great lakes. The bay is subject to algae blooms in late summer which are a naturally occurring phenomenon and do not indicate pollution other than from agricultural runoff. Zebra mussels as well as the other invasive species found in the great lakes are present.
The Quinte area played a vital role in bootlegging during Prohibition in the United States, with large volumes of booze being produced in the area, and shipped via boat on the Bay to Lake Ontario finally arriving in New York State where it was distributed. Illegal sales of liquor accounted for many fortunes in and around Belleville.
Tourism in the area is significant, especially in the summer months due to the Bay of Quinte and its fishing, local golf courses, provincial parks, and wineries.
Geography.
The northern side of the bay is defined by Ontario's mainland, while the southern side follows the shore of the Prince Edward County headland. Beginning in the east with the outlet to Lake Ontario, the bay runs west-southwest for to Picton (although this section is also called Adolphus Reach), where it turns north-northwest for another as far as Deseronto. From there it turns south-southwest again for another , running past Big Island on the south and Belleville on the north. The width of the bay rarely exceeds two kilometers. The bay ends at Trenton (Quinte West) and the Trent River, both also on the north side. The Murray Canal has been cut through the "Carrying Place", the few miles separating the end of the bay and Lake Ontario on the west side. The Trent River is part of the Trent-Severn Waterway, a canal connecting Lake Ontario to Lake Simcoe and then Georgian Bay on Lake Huron.
There are several sub-bays off the Bay of Quinte, including Hay Bay, Big Bay, and Muscote Bay.
Quinte Region.
Quinte is also a region comprising several communities situated along the Bay of Quinte, including Belleville which is the largest city in the Quinte Region, and represents a midpoint between Montreal, Ottawa, and Toronto.
The Greater Bay of Quinte area includes the municipalities of Brighton, Quinte West, Belleville, Prince Edward County, and Greater Napanee as well as the Native Tyendinaga Mohawk Territory. Overall population of the area exceeds 200,000.
Mohawks of the Bay of Quinte.
The Mohawks of the Bay of Quinte (Kenhtè:ke Kanyen'kehá:ka) on traditional Tyendinaga Mohawk Territory. Their reserve Band number 244, their current land base, is a 73 km² (18000-acre) on the Bay of Quinte in southeastern Ontario, Canada, east of Belleville and immediately to the west of Deseronto.
The community takes its name from a variant spelling of Mohawk leader Joseph Brant's traditional Mohawk name, Thayendanegea (standardized spelling Thayentiné:ken), which means 'two pieces of fire wood beside each other'. Officially, in the Mohawk language, the community is called "Kenhtè:ke" (Tyendinaga) which means "on the bay" the birthplace of Tekanawí:ta. The Cayuga name is Tyendinaga, "Tayęda:ne:gęˀ or Detgayę:da:negęˀ", "land of two logs.")
Education.
The Quinte Region, specifically the City of Belleville, is home to "Loyalist College of Applied Arts and Technology." Other post-secondary schools in the region include; "Maxwell College of Advanced Technology," "CDI College," "Ontario Business College," and "Quinte Literacy." Secondary Schools in the region include "Albert College" (private school) and "Sir James Whitney" (a school for the deaf and severely hearing-impaired).
Industry and employment.
The Quinte Region is home to a large number of national and international food processing manufacturers. Quinte also houses a large number of industries in the plastics & packaging sector, transportation sector, logistics sector and advanced manufacturing sector, including the following (just a few of over 350 industries located in the Bay of Quinte Region) :

</doc>
<doc id="4207" url="https://en.wikipedia.org/wiki?curid=4207" title="Bassoon">
Bassoon

The bassoon is a woodwind instrument in the double reed family that typically plays music written in the bass and tenor clefs, and occasionally the treble. Appearing in its modern form in the 19th century, the bassoon figures prominently in orchestral, concert band, and chamber music literature. The bassoon is a non-transposing instrument known for its distinctive tone color, wide range, variety of character and agility. Listeners often compare its warm, dark, reedy timbre to that of a male baritone voice. Someone who plays the bassoon is called a bassoonist.
Etymology.
The word bassoon comes from French "basson" and from Italian "bassone" ("basso" with the augmentative suffix "-one").
Range.
The range of the bassoon begins at B (the first one below the bass staff) and extends upward over three octaves, roughly to the G above the treble staff (G). Higher notes are possible but difficult to produce, and rarely called for: orchestral and concert band parts rarely go higher than C or D. Even Stravinsky's famously difficult opening solo in "The Rite of Spring" only ascends to D.
A is possible with a special extension to the instrument—see "Extended techniques" below.
Construction.
The bassoon disassembles into six main pieces, including the reed. The bell (6), extending upward; the bass joint (or long joint) (5), connecting the bell and the boot; the boot (or butt) (4), at the bottom of the instrument and folding over on itself; the wing joint (or tenor joint) (3), which extends from boot to bocal; and the bocal (or crook) (2), a crooked metal tube that attaches the wing joint to a reed (1) (). Bassoons are double reed instruments like the oboe and the English horn.
A modern beginner's bassoon is generally made of maple, with medium-hardness types such as sycamore maple and sugar maple preferred. Less-expensive models are also made of materials such as polypropylene and ebonite, primarily for student and outdoor use; metal bassoons were made in the past but have not been produced by any major manufacturer since 1889. The bore of the bassoon is conical, like that of the oboe and the saxophone, and the two adjoining bores of the boot joint are connected at the bottom of the instrument with a U-shaped metal connector. Both bore and tone holes are precision-machined, and each instrument is finished by hand for proper tuning. The walls of the bassoon are thicker at various points along the bore; here, the tone holes are drilled at an angle to the axis of the bore, which reduces the distance between the holes on the exterior. This ensures coverage by the fingers of the average adult hand. Wooden instruments are lined with hard rubber along the interior of the wing and boot joints to prevent damage from moisture; wooden instruments are also stained and varnished. The end of the bell is usually fitted with a ring, either of metal, plastic or ivory. The joints between sections consist of a tenon fitting into a socket; the tenons are wrapped in either cork or string as a seal against air leaks. The bocal connects the reed to the rest of the instrument and is inserted into a socket at the top of the wing joint. Bocals come in many different lengths and styles, depending on the desired tuning and playing characteristics.
Folded upon itself, the bassoon stands tall, but the total sounding length is . Playing is facilitated by doubling the tube back on itself and by closing the distance between the widely spaced holes with a complex system of key work, which extends throughout nearly the entire length of the instrument. There are also short-reach bassoons made for the benefit of young or petite players.
Development.
Early history.
Music historians generally consider the dulcian to be the forerunner of the modern bassoon, as the two instruments share many characteristics: a double reed fitted to a metal crook, obliquely drilled tone holes and a conical bore that doubles back on itself. The origins of the dulcian are obscure, but by the mid-16th century it was available in as many as eight different sizes, from soprano to great bass. A full consort of dulcians was a rarity; its primary function seems to have been to provide the bass in the typical wind band of the time, either loud (shawms) or soft (recorders), indicating a remarkable ability to vary dynamics to suit the need. Otherwise, dulcian technique was rather primitive, with eight finger holes and two keys, indicating that it could play in only a limited number of key signatures.
The dulcian came to be known as "fagotto" in Italy. However, the usual etymology that equates "fagotto" with "bundle of sticks" is somewhat misleading, as the latter term did not come into general use until later. Some think it may resemble the Roman Fasces, a standard of bound sticks with an ax. A further discrepancy lies in the fact that the dulcian was carved out of a single block of wood—in other words, a single "stick" and not a bundle.
Circumstantial evidence indicates that the baroque bassoon was a newly invented instrument, rather than a simple modification of the old dulcian. The dulcian was not immediately supplanted, but continued to be used well into the 18th century by Bach and others. The man most likely responsible for developing the true bassoon was Martin Hotteterre (d.1712), who may also have invented the three-piece "flûte traversière" and the "hautbois" (baroque oboe). Some historians believe that sometime in the 1650s, Hotteterre conceived the bassoon in four sections (bell, bass joint, boot and wing joint), an arrangement that allowed greater accuracy in machining the bore compared to the one-piece dulcian. He also extended the compass down to B by adding two keys. An alternate view maintains Hotteterre was one of several craftsmen responsible for the development of the early bassoon. These may have included additional members of the Hotteterre family, as well as other French makers active around the same time. No original French bassoon from this period survives, but if it did, it would most likely resemble the earliest extant bassoons of Johann Christoph Denner and Richard Haka from the 1680s. Sometime around 1700, a fourth key (G♯) was added, and it was for this type of instrument that composers such as Antonio Vivaldi, Bach, and Georg Philipp Telemann wrote their demanding music. A fifth key, for the low E, was added during the first half of the 18th century. Notable makers of the 4-key and 5-key baroque bassoon include J.H. Eichentopf (c. 1678–1769), J. Poerschmann (1680–1757), Thomas Stanesby, Jr. (1668–1734), G.H. Scherer (1703–1778), and Prudent Thieriot (1732–1786).
Modern history.
Increasing demands on capabilities of instruments and players in the 19th century—particularly larger concert halls requiring greater volume and the rise of virtuoso composer-performers—spurred further refinement. Increased sophistication, both in manufacturing techniques and acoustical knowledge, made possible great improvements in the instrument's playability.
The modern bassoon exists in two distinct primary forms, the Buffet system and the Heckel system. Most of the world plays the Heckel system, while the Buffet system is primarily played in France, Belgium, and parts of Latin America.
Heckel (German) system.
The design of the modern bassoon owes a great deal to the performer, teacher, and composer Carl Almenräder. Assisted by the German acoustic researcher Gottfried Weber, he developed the 17-key bassoon with a range spanning four octaves. Almenräder's improvements to the bassoon began with an 1823 treatise describing ways of improving intonation, response, and technical ease of playing by augmenting and rearranging the keywork. Subsequent articles further developed his ideas. His employment at Schott gave him the freedom to construct and test instruments according to these new designs, and he published the results in "Caecilia", Schott's house journal. Almenräder continued publishing and building instruments until his death in 1846, and Ludwig van Beethoven himself requested one of the newly made instruments after hearing of the papers. In 1831, Almenräder left Schott to start his own factory with a partner, Johann Adam Heckel.
Heckel and two generations of descendants continued to refine the bassoon, and their instruments became the standard, with other makers following. Because of their superior singing tone quality (an improvement upon one of the main drawbacks of the Almenräder instruments), the Heckel instruments competed for prominence with the reformed Wiener system, a Boehm-style bassoon, and a completely keyed instrument devised by Charles-Joseph Sax, father of Adolphe Sax. F.W. Kruspe implemented a latecomer attempt in 1893 to reform the fingering system, but it failed to catch on. Other attempts to improve the instrument included a 24-keyed model and a single-reed mouthpiece, but both these had adverse effects on tone and were abandoned.
Coming into the 20th century, the Heckel-style German model of bassoon dominated the field. Heckel himself had made over 1,100 instruments by the turn of the 20th century (serial numbers begin at 3,000), and the British makers' instruments were no longer desirable for the changing pitch requirements of the symphony orchestra, remaining primarily in military band use.
Except for a brief 1940s wartime conversion to ball bearing manufacture, the Heckel concern has produced instruments continuously to the present day. Heckel bassoons are considered by many to be the best, although a range of Heckel-style instruments is available from several other manufacturers, all with slightly different playing characteristics. Companies that manufacture Heckel-system bassoons include: Wilhelm Heckel, Yamaha, Fox Products, W. Schreiber & Söhne, Püchner, Conn-Selmer, Linton, Moosmann Kohlert, Moennig/Adler, B.H. Bell, Walter, Leitzinger and Guntram Wolf. In addition, several factories in the People's Republic of China are producing inexpensive instruments under such labels as Laval, Haydn, and Lark, and these have been available in the West for some time now. However, they are generally of marginal quality and are usually avoided by serious players.
Because its mechanism is primitive compared to most modern woodwinds, makers have occasionally attempted to "reinvent" the bassoon. In the 1960s, Giles Brindley began to develop what he called the "logical bassoon," which aimed to improve intonation and evenness of tone through use of an electrically activated mechanism, making possible key combinations too complex for the human hand to manage. Brindley's logical bassoon was never marketed.
Buffet (French) system.
The Buffet system bassoon achieved its basic acoustical properties somewhat earlier than the Heckel. Thereafter, it continued to develop in a more conservative manner. While the early history of the Heckel bassoon included a complete overhaul of the instrument in both acoustics and key work, the development of the Buffet system consisted primarily of incremental improvements to the key work. This minimalist approach of the Buffet deprived it of improved consistency of intonation, ease of operation, and increased power, which is found in Heckel bassoons, but the Buffet is considered by some to have a more vocal and expressive quality. The conductor John Foulds lamented in 1934 the dominance of the Heckel-style bassoon, considering them too homogeneous in sound with the horn. The modern Buffet system has 22 keys with its range being about the same as the Heckel.
Compared to the Heckel bassoon, Buffet system bassoons have a narrower bore and simpler mechanism, requiring different fingerings for many notes. Switching between Heckel and Buffet requires extensive retraining. Buffet instruments are known for a reedier sound and greater facility in the upper registers, reaching e" and f" with far greater ease and less air resistance. French woodwind instruments' tone in general exhibits a certain amount of "edge," with more of a vocal quality than is usual elsewhere, and the Buffet bassoon is no exception. This type of sound can be beneficial in music by French composers, but has drawn criticism for being too intrusive. As with all bassoons, the tone varies considerably, depending on individual instrument and performer. In the hands of a lesser player, the Heckel bassoon can sound flat and woody, but good players succeed in producing a vibrant, singing tone. Conversely, a poorly played Buffet can sound buzzy and nasal, but good players succeed in producing a warm, expressive sound, different from—but not inferior to—the Heckel.
Though the United Kingdom once favored the French system, Buffet-system instruments are no longer made there and the last prominent British player of the French system retired in the 1980s. However, with continued use in some regions and its distinctive tone, the Buffet continues to have a place in modern bassoon playing, particularly in France, where it is originated. Buffet-model bassoons are currently made in Paris by Buffet Crampon and the atelier Ducasse (Romainville, France). The Selmer Company stopped fabrication of French system bassoons a few years ago. Some players, for example the late Gerald Corey in Canada, have learned to play both types and will alternate between them depending on the repertoire.
Use in ensembles.
Earlier ensembles.
Orchestras first used the bassoon to reinforce the bass line, and as the bass of the double reed choir (oboes and taille). Baroque composer Jean-Baptiste Lully and his "Les Petits Violons" included oboes and bassoons along with the strings in the 16-piece (later 21-piece) ensemble, as one of the first orchestras to include the newly invented double reeds. Antonio Cesti included a bassoon in his 1668 opera "Il pomo d'oro" (The Golden Apple). However, use of bassoons in concert orchestras was sporadic until the late 17th century when double reeds began to make their way into standard instrumentation. This was largely due to the spread of the "hautbois" to countries outside France. Increasing use of the bassoon as a "basso continuo" instrument meant that it began to be included in opera orchestras, first in France and later in Italy, Germany and England. Meanwhile, composers such as Joseph Bodin de Boismortier, Michel Corrette, Johann Ernst Galliard, Jan Dismas Zelenka, Johann Friedrich Fasch and Telemann wrote demanding solo and ensemble music for the instrument. Antonio Vivaldi brought the bassoon to prominence by featuring it in 37 concerti for the instrument.
By the mid-18th century, the bassoon's function in the orchestra was still mostly limited to that of a continuo instrument—since scores often made no specific mention of the bassoon, its use was implied, particularly if there were parts for oboes or other winds. Beginning in the early Rococo era, composers such as Joseph Haydn, Michael Haydn, Johann Christian Bach, Giovanni Battista Sammartini and Johann Stamitz included parts that exploited the bassoon for its unique color, rather than for its perfunctory ability to double the bass line. Orchestral works with fully independent parts for the bassoon would not become commonplace until the Classical era. Wolfgang Amadeus Mozart's "Jupiter" symphony is a prime example, with its famous bassoon solos in the first movement. The bassoons were generally paired, as in current practice, though the famed Mannheim orchestra boasted four.
Another important use of the bassoon during the Classical era was in the "Harmonie", a chamber ensemble consisting of pairs of oboes, horns and bassoons; later, two clarinets would be added to form an octet. The "Harmonie" was an ensemble maintained by German and Austrian noblemen for private music-making, and was a cost-effective alternative to a full orchestra. Haydn, Mozart, Ludwig van Beethoven and Franz Krommer all wrote considerable amounts of music for the "Harmonie".
Modern ensembles.
The modern symphony orchestra typically calls for two bassoons, often with a third playing the contrabassoon. Some works call for four or more players. The first player is frequently called upon to perform solo passages. The bassoon's distinctive tone suits it for both plaintive, lyrical solos such as Maurice Ravel's "Boléro" and more comical ones, such as the grandfather's theme in "Peter and the Wolf". Its agility suits it for passages such as the famous running line (doubled in the violas and cellos) in the overture to "The Marriage of Figaro". In addition to its solo role, the bassoon is an effective bass to a woodwind choir, a bass line along with the cellos and double basses, and harmonic support along with the French horns.
A wind ensemble will usually also include two bassoons and sometimes contrabassoon, each with independent parts; other types of concert wind ensembles will often have larger sections, with many players on each of first or second parts; in simpler arrangements there will be only one bassoon part and no contrabassoon. The bassoon's role in the concert band is similar to its role in the orchestra, though when scoring is thick it often cannot be heard above the brass instruments also in its range. "La Fiesta Mexicana", by H. Owen Reed, features the instrument prominently, as does the transcription of Malcolm Arnold's "Four Scottish Dances", which has become a staple of the concert band repertoire.
The bassoon is part of the standard wind quintet instrumentation, along with the flute, oboe, clarinet, and horn; it is also frequently combined in various ways with other woodwinds. Richard Strauss's "Duet-Concertino" pairs it with the clarinet as "concertante" instruments, with string orchestra in support. An ensemble known as the "reed quintet" also makes use of the bassoon. A reed quintet is made up of an oboe, clarinet, saxophone, bass clarinet, and bassoon.
The bassoon quartet has also gained favor in recent times. The bassoon's wide range and variety of tone colors make it well suited to grouping in a like-instrument ensemble. Peter Schickele's "Last Tango in Bayreuth" (after themes from "Tristan und Isolde") is a popular work; Schickele's fictional alter ego P. D. Q. Bach exploits the more humorous aspects with his quartet "Lip My Reeds," which at one point calls for players to perform on the reed alone. It also calls for a low A at the very end of the prelude section in the fourth bassoon part. It is written so that the first bassoon does not play; instead, the player's role is to place an extension in the bell of the fourth bassoon so that the note can be played.
Jazz.
The bassoon is infrequently used as a jazz instrument and rarely seen in a jazz ensemble. It first began appearing in the 1920s, including specific calls for its use in Paul Whiteman's group, the unusual octets of Alec Wilder, and a few other session appearances. The next few decades saw the instrument used only sporadically, as symphonic jazz fell out of favor, but the 1960s saw artists such as Yusef Lateef and Chick Corea incorporate bassoon into their recordings; Lateef's diverse and eclectic instrumentation saw the bassoon as a natural addition, while Corea employed the bassoon in combination with flautist Hubert Laws.
More recently, Illinois Jacquet, Ray Pizzi, Frank Tiberi, and Marshall Allen have both doubled on bassoon in addition to their saxophone performances. Bassoonist Karen Borca, a performer of free jazz, is one of the few jazz musicians to play only bassoon; Michael Rabinowitz, the Spanish bassoonist Javier Abad, and James Lassen, an American resident in Bergen, Norway, are others. Katherine Young plays the bassoon in the ensembles of Anthony Braxton. Lindsay Cooper, Paul Hanson, the Brazilian bassoonist Alexandre Silverio, Trent Jacobs and Daniel Smith are also currently using the bassoon in jazz. French bassoonists Jean-Jacques Decreux and Alexandre Ouzounoff have both recorded jazz, exploiting the flexibility of the Buffet system instrument to good effect.
Popular music.
The bassoon is even rarer as a regular member of rock bands. However, several 1960s pop music hits feature the bassoon, including "The Tears of a Clown" by Smokey Robinson and the Miracles (the bassoonist was Charles R. Sirard), "Jennifer Juniper" by Donovan, "The Turtles" "Happy Together"(third verse,overdub), "59th Street Bridge Song" by Harpers Bizarre, and the oompah bassoon underlying The New Vaudeville Band's "Winchester Cathedral". From 1974 to 1978, the bassoon was played by Lindsay Cooper in the British avant-garde band Henry Cow. In the 1970s it was played, in the British medieval/progressive rock band Gryphon, by Brian Gulland, as well as by the American band Ambrosia, where it was played by drummer Burleigh Drummond. The Belgian Rock in Opposition-band Univers Zero is also known for its use of the bassoon.
In the 1990s, Madonna Wayne Gacy provided bassoon for the alternative metal band Marilyn Manson as did Aimee DeFoe, in what is self-described as "grouchily lilting garage bassoon" in the indie-rock band Blogurt from Pittsburgh, Pennsylvania. More recently, These New Puritans's 2010 album Hidden makes heavy use of the instrument throughout; their principal songwriter, Jack Barnett, claimed repeatedly to be "writing a lot of music for bassoon" in the run-up to its recording. In early 2011, American hip-hop artist Kanye West updated his Twitter account to inform followers that he recently added the bassoon to a yet unnamed song.
The rock band Better Than Ezra took their name from a passage in Ernest Hemingway's "A Moveable Feast" in which the author comments that listening to an annoyingly talkative person is still “better than Ezra learning how to play the bassoon,” referring to Ezra Pound.
British psychedelic/progressive rock band Knifeworld features the bassoon playing of Chloe Herrington, who also plays for experimental chamber rock orchestra Chrome Hoof.
Technique.
The bassoon is held diagonally in front of the player, but unlike the flute, oboe and clarinet, it cannot be supported by the player's hands alone. Some means of additional support is required; the most common ones are a seat strap attached to the base of the boot joint, which is laid across the chair seat prior to sitting down, or a neck strap or shoulder harness attached to the top of the boot joint. Occasionally a spike similar to those used for the cello or the bass clarinet is attached to the bottom of the boot joint and rests on the floor. It is possible to play while standing up if the player uses a neck strap or similar harness, or if the seat strap is tied to the belt. Sometimes a device called a "balance hanger" is used when playing in a standing position. This is installed between the instrument and the neck strap, and shifts the point of support closer to the center of gravity.
The bassoon is played with both hands in a stationary position, the left above the right, with five main finger holes on the front of the instrument (nearest the audience) plus a sixth that is activated by an open-standing key. Five additional keys on the front are controlled by the little fingers of each hand. The back of the instrument (nearest the player) has twelve or more keys to be controlled by the thumbs, the exact number varying depending on model.
To stabilize the right hand, many bassoonists use an adjustable comma-shaped apparatus called a "crutch," or a hand rest, which mounts to the boot joint. The crutch is secured with a thumb screw, which also allows the distance that it protrudes from the bassoon to be adjusted. Players rest the curve of the right hand where the thumb joins the palm against the crutch. The crutch also keeps the right hand from tiring and enables the player to keep the finger pads flat on the finger holes and keys.
An aspect of bassoon technique not found on any other woodwind is called "flicking". It involves the left hand thumb momentarily pressing, or 'flicking' the high A, C and D keys at the beginning of certain notes in the middle octave to achieve a clean slur from a lower note. This eliminates cracking, or brief multiphonics that happens without the use of this technique.
Flicking is not universal amongst bassoonists; some American players, principally on the East Coast, use it sparingly, if at all. The rest use it virtually 100% of the time—it has become in essence part of the fingering.
The alternative method is "venting", which requires that the register key be used as part of the full fingering as opposed to being open momentarily at the start of the note. This is sometimes called the "European Style."
While flicking is used to higher notes, the whisper key is used for lower notes. From the A right below middle C and lower, the whisper key is pressed with the left thumb and held for the duration of the note. This prevents cracking, as low notes can sometimes crack into a higher octave. Both flicking and using the whisper key is especially important to ensure notes speak properly during slurring between high and low registers.
While bassoons are usually critically tuned at the factory, the player nonetheless has a great degree of flexibility of pitch control through the use of breath support, embouchure, and reed profile. Players can also use alternate fingerings to adjust the pitch of many notes. Similar to other woodwind instruments, the length of the bassoon can be increased to lower pitch or decreased to raise pitch. On the bassoon, this is done preferably by changing the bocal to one of a different length, (lengths are denoted by a number on the bocal, usually starting at 0 for the shortest length, and 3 for the longest, but there are some manufacturers who will use other numbers) but it is possible to push the bocal in or out to adjust the pitch.
Embouchure.
The bassoon embouchure is a very important aspect of producing a full, round bassoon tone, but can be difficult to obtain as a beginner. The bassoon embouchure is made by putting one's lips together as if one were whistling and then dropping the jaw down as in a yawning motion (without actually yawning or opening the mouth). Both sets of teeth should be covered by the lips in order to protect the reed and control applied pressure. The reed is then placed in the mouth, forming a seal around the reed with the lips and facial muscles.
Extended techniques.
Many extended techniques can be performed on the bassoon, such as multiphonics, flutter-tonguing, circular breathing, double tonguing, and harmonics. In the case of the bassoon, flutter-tonguing may be accomplished by "gargling" in the back of the throat as well as by the conventional method of rolling Rs. Multiphonics on the bassoon can be achieved by using particular alternative fingerings.
Also, using certain fingerings, notes may be produced on the instrument that sound lower pitches than the actual range of the instrument. These "impossible notes" tend to sound very gravelly and out of tune, but technically sound below the low B. Alternatively, lower notes can be produced by inserting a small paper or rubber tube into the end of the bell, which converts the lower B into a lower note such as an A natural; this lowers the pitch of the instrument, but has the positive effect of bringing the lowest register (which is typically quite sharp) into tune. A notable piece that calls for the use of a low A bell is Carl Nielsen's Wind Quintet, op. 43, which includes an optional low A for the final cadence of the work. Bassoonists sometimes use the end bell segment of an English horn or clarinet if one is available instead of a specially made extension. This often yields unsatisfactory results, though, as the resultant A can be quite sharp. The idea of using low A was begun by Richard Wagner, who wanted to extend the range of the bassoon. Many passages in his later operas require the low A as well as the B-flat above. (This is impossible on a normal bassoon using an A extension as the fingering for the B-flat yields the low A.) These passages are typically realized on the contrabassoon, as recommended by the composer. Some bassoons have been made to allow bassoonists to realize similar passages. These bassoons are made with a "Wagner bell," which is an extended bell with a key for both the low A and the low B-flat. Bassoons with Wagner bells suffer similar intonational deficiencies as a bassoon with an A extension. Another composer who has required the bassoon to be chromatic down to low A is Gustav Mahler. Richard Strauss also calls for the low A in his opera "Intermezzo".
Modern Fingering.
The left thumb alone operates nine keys. B1, B1, C1, D1, D4, C4 (also B4), two keys when combined create A4, and the whisper pad. Additional notes can be created with these keys. The D1 and bottom key above whisper key on the tenor joint creates both C#2 and C#3. The same bottom tenor-joint key is also used, with additional fingering setup on the instrument, to create E4 and F4. D4 and C4 together create C#4. When the two keys on the tenor joint to create A4 are used with slightly altered fingering on the boot join, B4 is created. The whisper pad is used throughout the instrument's register, as well as in fingerings to create a muted or more piercing sound. The right thumb operates four keys. The top lever, which is very thin, is used in creating B2 and 3, and is used in B4, C4, D4, F4, and E4. The large, circular key (otherwise known as the "pancake key"), is used in mostly the bass register. From B1 to E1, it is held constantly. It is also used, like the whisper pad, in additional fingerings for muting the sound. For example, in Ravel's "Boléro", the bassoon is asked to play the ostinto on G4. This is easy to perform with the normal fingering for G4, but with the E1 key (pancake key). The next key assigned to the right thumb is known as the "spatula key". Its primary use is F#1 and F#2. The bottom key is used the least often than the other ones. With the combination of the back-most key of the smallest finger of the right hand, makes G#1 and G#2. This is most advantageous in pieces like Dukas's "The Sorcerer's Apprentice". The four fingers on left hand all have at least two assignments each. The index fingers has three assignments. The top-most key on the back side of the tenor join is primarily used for E4. Rarely is it used as a trill key. Its main assignment has three different options. It is a key with a small hole drilled into it. The player can lift the finger completely off the key. The player can also slide the finger down, so the key is closed, but the hole is open. The player can also close the hole with the key pressed down. The middle finger typically stays on the centre hole on the tenor joint. It can also move to a lever used for E4 and a rarely-used trill key. The ring finger operates, on most models, one key. Some models, like a Polisi Artist bassoon has two assignments. The upper assignment is used for alternate fingerings in the alto register. The smallest finger operates two side keys on the bass joint. The lower key is typically used for C#1. The upper key is used for E1, E3, F3, F#3, A4, B4, B4, C4, C#4, and D4. The four fingers of the right hand have at least one assignment each. The index finger stays over one hole, except when E4 is played. A side key at the top of the boot is used. The middle finger remains stationary over a whole with a ring around. The ring, with other pads, are lifted when the smallest finger on the right hands pushes a lever. The ring finger typically remains stationary on the lower ring-finger-key. However, the upper-ring-finger key can be used in place of the top thumb key on the front of the boot joint. The smallest finger operates three keys. The back-most one, closest to the bassoonist, is held down throughout most of the bass register. The key is not used after F1 is played. F#2 is created with this key, as well as G4, B4, B4, and C4. The lowest key for the smallest finger on the right hand is primarily used for G#1 and 2, but can be used E4, and F4.The front-most key is used with the bottom thumb key on the boot joint to create G#1 and 2.
Learning the bassoon.
The complicated fingering and the problem of reeds make the bassoon more of a challenge to learn than some of the other woodwind instruments. Cost is another big factor in a person's decision to pursue the bassoon. Prices range from $8,000 up to $25,000 for a good-quality instrument. In North America, schoolchildren typically take up bassoon only after starting on another reed instrument, such as clarinet or saxophone.
Students in America often begin to pursue the study of bassoon performance and technique in the middle years of their music education. Students are often provided with a school instrument and encouraged to pursue lessons with private instructors. Students typically receive instruction in proper posture, hand position, embouchure, tone production, and reed making.
Reeds and reed construction.
Modern reeds.
Bassoon reeds, made of "Arundo donax" cane, are often made by the players themselves, although beginner bassoonists tend to buy their reeds from professional reed makers or use reeds made by their teachers. Reeds begin with a length of tube cane that is split into three or four pieces using a tool called a cane splitter. The cane is then trimmed and "gouged" to the desired thickness, leaving the bark attached. After soaking, the gouged cane is cut to the proper shape and milled to the desired thickness, or "profile", by removing material from the bark side. This can be done by hand with a file; more frequently it is done with a machine or tool designed for the purpose. After the profiled cane has soaked once again it is folded over in the middle. Prior to soaking, the reed maker will have lightly scored the bark with parallel lines with a knife; this ensures that the cane will assume a cylindrical shape during the forming stage. On the bark portion, the reed maker binds on one, two, or three coils or loops of brass wire to aid in the final forming process. The exact placement of these loops can vary somewhat depending on the reed maker. The bound reed blank is then wrapped with thick cotton or linen thread to protect it, and a conical steel mandrel (which sometimes has been heated in a flame) is quickly inserted in between the blades. Using a special pair of pliers, the reed maker presses down the cane, making it conform to the shape of the mandrel. (The steam generated by the heated mandrel causes the cane to permanently assume the shape of the mandrel.) The upper portion of the cavity thus created is called the "throat," and its shape has an influence on the final playing characteristics of the reed. The lower, mostly cylindrical portion will be reamed out with a special tool called a reamer, allowing the reed to fit on the bocal.
After the reed has dried, the wires are tightened around the reed, which has shrunk after drying, or replaced completely. The lower part is sealed (a nitrocellulose-based cement such as Duco may be used) and then wrapped with thread to ensure both that no air leaks out through the bottom of the reed and that the reed maintains its shape. The wrapping itself is often sealed with Duco or clear nail varnish (polish). Electrical tape can also be used as a wrapping for amateur reed makers. The bulge in the wrapping is sometimes referred to as the "Turk's head"—it serves as a convenient handle when inserting the reed on the bocal.
To finish the reed, the end of the reed blank, originally at the center of the unfolded piece of cane, is cut off, creating an opening. The blades above the first wire are now roughly long. For the reed to play, a slight bevel must be created at the tip with a knife, although there is also a machine that can perform this function. Other adjustments with the reed knife may be necessary, depending on the hardness, the profile of the cane, and the requirements of the player. The reed opening may also need to be adjusted by squeezing either the first or second wire with the pliers. Additional material may be removed from the sides (the "channels") or tip to balance the reed. Additionally, if the "e" in the bass clef staff is sagging in pitch, it may be necessary to "clip" the reed by removing from its length using a pair of very sharp scissors or the equivalent.
Playing styles of individual bassoonists vary greatly; because of this, most advanced players will make their own reeds, in the process customizing them to their individual playing requirements. Many companies and individuals do offer reeds for sale, but even with store-bought reeds, players must know how to make adjustments to suit their particular playing style.
Early reeds.
Little is known about the early construction of the bassoon reed, as few examples survive, and much of what is known is only what can be gathered from artistic representations. The earliest known written instructions date from the middle of the 17th century, describing the reed as being held together by wire or resined thread; the earliest actual reeds that survive are more than a century younger, a collection of 21 reeds from the late 18th-century Spanish "bajon".

</doc>
<doc id="4210" url="https://en.wikipedia.org/wiki?curid=4210" title="Bipedalism">
Bipedalism

Bipedalism is a form of terrestrial locomotion where an organism moves by means of its two rear limbs or legs. An animal or machine that usually moves in a bipedal manner is known as a biped , meaning "two feet" (from the Latin "bis" for "double" and "pes" for "foot"). Types of bipedal movement include walking, running, or hopping.
Few modern species are habitual bipeds whose normal method of locomotion is two-legged. Within mammals, habitual bipedalism has evolved multiple times, with the macropods, kangaroo rats and mice, springhare, hopping mice, pangolins and homininan apes, as well as various other extinct groups evolving the trait independently. In the Triassic period some groups of archosaurs (a group that includes the ancestors of crocodiles) developed bipedalism; among their descendants the dinosaurs, all the early forms and many later groups were habitual or exclusive bipeds; the birds descended from one group of exclusively bipedal dinosaurs.
A larger number of modern species intermittently or briefly use a bipedal gait. Several non-archosaurian lizard species move bipedally when running, usually to escape from threats. Many primate and bear species will adopt a bipedal gait in order to reach food or explore their environment. Several arboreal primate species, such as gibbons and indriids, exclusively walk on two legs during the brief periods they spend on the ground. Many animals rear up on their hind legs whilst fighting or copulating. A few animals commonly stand on their hind legs, in order to reach food, to keep watch, to threaten a competitor or predator, or to pose in courtship, but do not move bipedally.
Definition.
The word is derived from the Latin words "bi(s)" 'two' and "ped-" 'foot', as contrasted with quadruped 'four feet'.
Advantages.
Limited and exclusive bipedalism can offer a species several advantages. Bipedalism raises the head; this allows a greater field of vision with improved detection of distant dangers or resources, access to deeper water for wading animals and allows the animals to reach higher food sources with their mouths. While upright, non-locomotory limbs become free for other uses, including manipulation (in primates and rodents), flight (in birds), digging (in giant pangolin), combat (in bears, great apes and the large monitor lizard) or camouflage (in certain species of octopus). The maximum bipedal speed appears less fast than the maximum speed of quadrupedal movement with a flexible backbone – both the ostrich and the red kangaroo can reach speeds of , while the cheetah can exceed .
Bipedality in kangaroo rats has been hypothesized to improve locomotor performance, which could aid in escaping from predators.
Facultative and obligate bipedalism.
Zoologists often label behaviors, including bipedalism, as "facultative" (i.e. optional) or "obligate" (the animal has no reasonable alternative). Even this distinction is not completely clear-cut — for example, humans normally walk and run in biped fashion, but almost all can crawl on hands and knees when necessary. There are even reports of humans who normally walk on all fours with their feet but not their knees on the ground, but these cases are a result of conditions such as Uner Tan syndrome — very rare genetic neurological disorders rather than normal behavior. Even if one ignores exceptions caused by some kind of injury or illness, there are many unclear cases, including the fact that "normal" humans can crawl on hands and knees. This article therefore avoids the terms "facultative" and "obligate", and focuses on the range of styles of locomotion "normally" used by various groups of animals.
Movement.
There are a number of states of movement commonly associated with bipedalism.
Bipedal animals.
The great majority of living terrestrial vertebrates are quadrupeds, with bipedalism exhibited by only a handful of living groups. Humans, gibbons and large birds walk by raising one foot at a time. On the other hand, most macropods, smaller birds, lemurs and bipedal rodents move by hopping on both legs simultaneously. Tree kangaroos are able to walk or hop, most commonly alternating feet when moving arboreally and hopping on both feet simultaneously when on the ground.
Amphibians.
There are no known living or fossil bipedal amphibians.
Extant reptiles.
Many species of lizards become bipedal during high-speed, sprint locomotion, including the world's fastest lizard, the spiny-tailed iguana (genus "Ctenosaura").
Early reptiles and lizards.
The first known biped is the bolosaurid "Eudibamus" whose fossils date from 290 million years ago. Its long hindlegs, short forelegs, and distinctive joints all suggest bipedalism. The species was extinct before the dinosaurs appeared.
Archosaurs (include birds, crocodiles, and dinosaurs).
Birds.
All birds are bipeds when on the ground, a feature inherited from their dinosaur ancestors.
Other archosaurs.
Bipedalism evolved more than once in archosaurs, the group that includes both dinosaurs and crocodilians. All dinosaurs are thought to be descended from a fully bipedal ancestor, perhaps similar to "Eoraptor". Bipedal movement also re-evolved in a number of other dinosaur lineages such as the iguanodons. Some extinct members of the crocodilian line, a sister group to the dinosaurs and birds, also evolved bipedal forms - a crocodile relative from the triassic, "Effigia okeeffeae", is thought to be bipedal. Pterosaurs were previously thought to have been bipedal, but recent trackways have all shown quadrupedal locomotion. Bipedalism also evolved independently among the dinosaurs. Dinosaurs diverged from their archosaur ancestors approximately 230 million years ago during the Middle to Late Triassic period, roughly 20 million years after the Permian-Triassic extinction event wiped out an estimated 95% of all life on Earth. Radiometric dating of fossils from the early dinosaur genus "Eoraptor" establishes its presence in the fossil record at this time. Paleontologists suspect "Eoraptor" resembles the common ancestor of all dinosaurs; if this is true, its traits suggest that the first dinosaurs were small, bipedal predators. The discovery of primitive, dinosaur-like ornithodirans such as "Marasuchus" and "Lagerpeton" in Argentinian Middle Triassic strata supports this view; analysis of recovered fossils suggests that these animals were indeed small, bipedal predators.
Mammals.
A number of groups of extant mammals have independently evolved bipedalism as their main form of locomotion - for example humans, giant pangolins, the extinct giant ground sloths, numerous species of jumping rodents and macropods. Humans, as their bipedalism has been extensively studied, are documented in the next section. Macropods are believed to have evolved bipedal hopping only once in their evolution, at some time no later than 45 million years ago.
Bipedal movement is less common among mammals, most of which are quadrupedal. All primates possess some bipedal ability, though most species primarily use quadrupedal locomotion on land. Primates aside, the macropods (kangaroos, wallabies and their relatives), kangaroo rats and mice, hopping mice and springhare move bipedally by hopping. Very few mammals other than primates commonly move bipedally by an alternating gait rather than hopping. Exceptions are the ground pangolin and in some circumstances the tree kangaroo. 
Primates.
Most bipedal animals move with their backs close to horizontal, using a long tail to balance the weight of their bodies. The primate version of bipedalism is unusual because the back is close to upright (completely upright in humans). Many primates can stand upright on their hind legs without any support. 
Chimpanzees, bonobos, gibbons and baboons exhibit forms of bipedalism. Injured chimpanzees and bonobos have been capable of sustained bipedalism.
Geladas, although often quadrupedal, will move between adjacent feeding patches with a squatting, shuffling bipedal form of locomotion .
Three captive primates, one macaque Natasha and two chimps, Oliver and Poko (chimpanzee), were found to move bipedally . Natasha switched to exclusive bipedalism after an illness, while Poko was discovered in captivity in a tall, narrow cage. Oliver reverted to knuckle-walking after developing arthritis.
In addition, non-human primates often use bipedal locomotion when carrying food. One hypothesis for human bipedalism is thus that it evolved as a result of differentially successful survival from carrying food to share with group members, although there are other hypotheses, as discussed below.
Limited bipedalism.
Limited bipedalism in mammals.
Other mammals engage in limited, non-locomotory, bipedalism. A number of other animals, such as rats, raccoons, and beavers will squat on their hindlegs to manipulate some objects but revert to four limbs when moving (the beaver will move bipedally if transporting wood for their dams, as will the raccoon when holding food). Bears will fight in a bipedal stance to use their forelegs as weapons. A number of mammals will adopt a bipedal stance in specific situations such as for feeding or fighting. Ground squirrels and meerkats will stand on hind legs to survey their surroundings, but will not walk bipedally. Dogs can stand or move on two legs if trained, or if birth defect or injury precludes quadrupedalism. The gerenuk antelope stands on its hind legs while eating from trees, as did the extinct giant ground sloth and chalicotheres. The spotted skunk will walk on its front legs when threatened, rearing up on its front legs while facing the attacker so that its anal glands, capable of spraying an offensive oil, face its attacker.
Limited bipedalism in non-mammals.
Bipedalism is unknown among the amphibians. Among the non-archosaur reptiles bipedalism is rare, but it is found in the 'reared-up' running of lizards such as agamids and monitor lizards. Many reptile species will also temporarily adopt bipedalism while fighting. One genus of basilisk lizard can run bipedally across the surface of water for some distance. Among arthropods, cockroaches are known to move bipedally at high speeds. Bipedalism is rarely found outside terrestrial animals, though at least two types of octopus walk bipedally on the sea floor using two of their arms, allowing the remaining arms to be used to camouflage the octopus as a mat of algae or a floating coconut.
Evolution of human bipedalism.
There are at least twelve distinct hypotheses as to how and why bipedalism evolved in humans, and also some debate as to when. Bipedalism evolved well before the large human brain or the development of stone tools. Bipedal specializations are found in "Australopithecus" fossils from 4.2-3.9 million years ago. The evolution of bipedalism was accompanied by significant evolutions in the spine including the forward movement in position of the foramen magnum, where the spinal cord leaves the cranium. Recent evidence regarding modern human sexual dimorphism (physical differences between male and female) in the lumbar spine has been seen in pre-modern primates such as "Australopithecus africanus". This dimorphism has been seen as an evolutionary adaptation of females to bear lumbar load better during pregnancy, an adaptation that non-bipedal primates would not need to make. Adapting bipedalism would have required less shoulder stability, which allowed the shoulder and other limbs to become more independent of each other and adapt for specific suspensory behaviors. In addition to the change in shoulder stability, changing locomotion would have increased the demand for shoulder mobility, which would have propelled the evolution of bipedalism forward. The different hypotheses are not necessarily mutually exclusive and a number of selective forces may have acted together to lead to human bipedalism. It is important to distinguish between adaptations for bipedalism and adaptations for running, which came later still.
Possible reasons for the evolution of human bipedalism include freeing the hands for tool use and carrying, sexual dimorphism in food gathering, changes in climate and habitat (from jungle to savanna) that favored a more elevated eye-position, and to reduce the amount of skin exposed to the tropical sun.
Savanna-based theory.
According to the savanna-based theory, hominines descended from the trees and adapted to life on the savanna by walking erect on two feet. The theory suggests that early hominids were forced to adapt to bipedal locomotion on the open savanna after they left the trees. This theory is closely related to the knuckle-walking hypothesis, which states that human ancestors used quadrupedal locomotion on the savanna, as evidenced by morphological characteristics found in "Australopithecus anamensis" and "Australopithecus afarensis" forelimbs, and that it is less parsimonious to assume that knuckle walking developed twice in Genus' Pan and Gorilla instead of evolving it once as synapomorphy for Pan and Gorilla before losing it in Australopithecus. The evolution of an orthograde posture would have been very helpful on a savanna as it would allow the ability to look over tall grasses in order to watch out for predators, or terrestrially hunt and sneak up on prey. It was also suggested in P.E. Wheeler's "The evolution of bipedality and loss of functional body hair in hominids", that a possible advantage of bipedalism in the savanna was reducing the amount of surface area of the body exposed to the sun, helping regulate body temperature. In fact, Elizabeth Vrba’s turnover pulse hypothesis supports the savanna-based theory by explaining the shrinking of forested areas due to global warming and cooling, which forced animals out into the open grasslands and caused the need for hominids to acquire bipedality.
Rather, the bipedal adaptation hominines had already achieved was used in the savanna. The fossil record shows that early bipedal hominines were still adapted to climbing trees at the time they were also walking upright. It is possible that Bipedalism evolved in the trees, and was later applied to the Savannah as a vestigial trait. Humans and orangutans are both unique to a bipedal reactive adaptation when climbing on thin branches, in which they have increased hip and knee extension in relation to the diameter of the branch, which can increase an arboreal feeding range and can be attributed to a convergent evolution of bipedalism evolving in arboreal environments. Hominine fossils found in dry grassland environments led anthropologists to believe hominines lived, slept, walked upright, and died only in those environments because no hominine fossils were found in forested areas. However, fossilization is a rare occurrence—the conditions must be just right in order for an organism that dies to become fossilized for somebody to find later, which is also a rare occurrence. The fact that no hominine fossils were found in forests does not ultimately lead to the conclusion that no hominines ever died there. The convenience of the savanna-based theory caused this point to be overlooked for over a hundred years.
Some of the fossils found actually showed that there was still an adaptation to arboreal life. For example, Lucy, the famous "Australopithecus afarensis", found in Hadar in Ethiopia, which may have been forested at the time of Lucy’s death, had curved fingers that would still give her the ability to grasp tree branches, but she walked bipedally. “Little Foot,” the collection of "Australopithecus africanus" foot bones, has a divergent big toe as well as the ankle strength to walk upright. “Little Foot” could grasp things using his feet like an ape, perhaps tree branches, and he was bipedal. Ancient pollen found in the soil in the locations in which these fossils were found suggest that the area used to be much more wet and covered in thick vegetation and has only recently become the arid desert it is now.
Traveling efficiency hypothesis.
An alternative explanation is the mixture of savanna and scattered forests increased terrestrial travel by proto-humans between clusters of trees, and bipedalism offered greater efficiency for long-distance travel between these clusters than quadrupedalism. In an experiment monitoring chimpanzee metabolic rate via oxygen consumption, it was found that the quadrupedal and bipedal energy costs were very similar, implying that this transition in early ape-like ancestors would have not have been very difficult or energetically costing. This increased travel efficiency is likely to have been selected for as it assisted the wide dispersal of early hominids across the Savannah to create start populations.
Postural feeding hypothesis.
The postural feeding hypothesis has been recently supported by Dr. Kevin Hunt, a professor at Indiana University. This hypothesis asserts that chimpanzees were only bipedal when they eat. While on the ground, they would reach up for fruit hanging from small trees and while in trees, bipedalism was used to reach up to grab for an overhead branch. These bipedal movements may have evolved into regular habits because they were so convenient in obtaining food. Also, Hunt's hypotheses states that these movements coevolved with chimpanzee arm-hanging, as this movement was very effective and efficient in harvesting food. When analyzing fossil anatomy, "Australopithecus afarensis" has very similar features of the hand and shoulder to the chimpanzee, which indicates hanging arms. Also, the "Australopithecus" hip and hind limb very clearly indicate bipedalism, but these fossils also indicate very inefficient locomotive movement when compared to humans. For this reason, Hunt argues that bipedalism evolved more as a terrestrial feeding posture than as a walking posture.
A similar study conducted by Thorpe et al. looked at how the most arboreal great ape, the orangutan, held onto supporting branches in order to navigate branches that were too flexible or unstable otherwise. They found that in more than 75% of locomotive instances the orangutans used their hands to stabilize themselves while they navigated thinner branches. They hypothesized that increased fragmentation of forests where A. afarensis as well as other ancestors of modern humans and other apes resided could have contributed to this increase of bipedalism in order to navigate the diminishing forests. Their findings also shed light on a couple of discrepancies observed in the anatomy of A. afarensis, such as the ankle joint, which allowed it to “wobble” and long, highly flexible forelimbs. The idea that bipedalism started from walking in trees explains both the increased flexibility in the ankle as well as the long limbs which would be used to grab hold of branches.
Provisioning model.
One theory on the origin of bipedalism is the behavioral model presented by C. Owen Lovejoy, known as "male provisioning". Lovejoy theorizes that the evolution of bipedalism was linked to monogamy. In the face of long inter-birth intervals and low reproductive rates typical of the apes, early hominids engaged in pair-bonding that enabled greater parental effort directed towards rearing offspring. Lovejoy proposes that male provisioning of food would improve the offspring survivorship and increase the pair's reproductive rate. Thus the male would leave his mate and offspring to search for food and return carrying the food in his arms walking on his legs. This model is supported by the reduction ("feminization") of the male canine teeth in early hominids such as "Sahelanthropus tchadensis" and "Ardipithecus ramidus", which along with low body size dimorphism in "Ardipithecus" and "Australopithecus", suggests a reduction in inter-male antagonistic behavior in early hominids. In addition, this model is supported by a number of modern human traits associated with concealed ovulation (permanently enlarged breasts, lack of sexual swelling) and low sperm competition (moderate sized testes, low sperm mid-piece volume) that argues against recent adaptation to a polygynous reproductive system.
However, this model has generated some controversy, as others have argued that early bipedal hominids were instead polygynous. Among most monogamous primates, males and females are about the same size. That is sexual dimorphism is minimal, and other studies have suggested that Australopithecus afarensis males were nearly twice the weight of females. However, Lovejoy's model posits that the larger range a provisioning male would have to cover (to avoid competing with the female for resources she could attain herself) would select for increased male body size to limit predation risk. Furthermore, as the species became more bipedal, specialized feet would prevent the infant from conveniently clinging to the mother - hampering the mother's freedom and thus make her and her offspring more dependent on resources collected by others. Modern monogamous primates such as gibbons tend to be also territorial, but fossil evidence indicates that "Australopithecus afarensis" lived in large groups. However, while both gibbons and hominids have reduced canine sexual dimorphism, female gibbons enlarge ('masculinize') their canines so they can actively share in the defense of their home territory. Instead, the reduction of the male hominid canine is consistent with reduced inter-male aggression in a group living primate.
Early bipedalism in homininae model.
Recent studies of 4.4 million years old "Ardipithecus ramidus" suggest bipedalism, it is thus possible that bipedalism evolved very early in homininae and was reduced in chimpanzee and gorilla when they became more specialized. According to Richard Dawkins in his book "The Ancestor's Tale", chimps and bonobos are descended from "Australopithecus" gracile type species while gorillas are descended from Paranthropus. These apes may have once been bipedal, but then lost this ability when they were forced back into an arboreal habitat, presumably by those australopithecines who eventually became us (see Homininae). Early homininaes such as "Ardipithecus ramidus" may have possessed an arboreal type of bipedalism that later independently evolved towards knuckle-walking in chimpanzees and gorillas and towards efficient walking and running in modern humans (see figure). It is also proposed that one cause of Neanderthal extinction was a less efficient running.
Warning display (aposematic) model.
Joseph Jordania from the University of Melbourne recently (2011) suggested that bipedalism was one of the central elements of the general defense strategy of early hominids, based on aposematism, or warning display and intimidation of potential predators and competitors with exaggerated visual and audio signals. According to this model, hominids were trying to stay as visible and as loud as possible all the time. Several morphological and behavioral developments were employed to achieve this goal: upright bipedal posture, longer legs, long tightly coiled hair on the top of the head, body painting, threatening synchronous body movements, loud voice and extremely loud rhythmic singing/stomping/drumming on external subjects. Slow locomotion and strong body odor (both characteristic for hominids and humans) are other features often employed by aposematic species to advertise their non-profitability for potential predators.
Other behavioural models.
There are a variety of ideas which promote a specific change in behaviour as the key driver for the evolution of hominid bipedalism. For example, Wescott (1967) and later Jablonski & Chaplin (1993) suggest that bipedal threat displays could have been the transitional behaviour which led to some groups of apes beginning to adopt bipedal postures more often. Others ("e.g." Dart 1925) have offered the idea that the need for more vigilance against predators could have provided the initial motivation. Dawkins ("e.g." 2004) has argued that it could have begun as a kind of fashion that just caught on and then escalated through sexual selection. And it has even been suggested ("e.g." Tanner 1981:165) that male phallic display could have been the initial incentive, as well as increased sexual signaling in upright female posture.
Thermoregulatory model.
The thermoregulatory model explaining the origin of bipedalism is one of the simplest theories so far advanced, but it is a viable explanation. Dr. Peter Wheeler, a professor of evolutionary biology, proposes that bipedalism raises the amount of body surface area higher above the ground which results in a reduction in heat gain and helps heat dissipation. When a hominid is higher above the ground, the organism accesses more favorable wind speeds and temperatures. During heat seasons, greater wind flow results in a higher heat loss, which makes the organism more comfortable. Also, Wheeler explains that a vertical posture minimizes the direct exposure to the sun whereas quadrupedalism exposes more of the body to direct exposure. Analysis and interpretations of Ardipithecus reveal that this hypothesis needs modification to consider that the forest and woodland environmental preadaptation of early-stage hominid bipedalism preceded further refinement of bipedalism by the pressure of natural selection. This then allowed for the more efficient exploitation of the hotter conditions ecological niche, rather than the hotter conditions being hypothetically bipedalism's initial stimulus.
Carrying models.
Charles Darwin wrote that "Man could not have attained his present dominant position in the world without the use of his hands, which are so admirably adapted to the act of obedience of his will" Darwin (1871:52) and many models on bipedal origins are based on this line of thought. Gordon Hewes (1961) suggested that the carrying of meat "over considerable distances" (Hewes 1961:689) was the key factor. Isaac (1978) and Sinclair et al. (1986) offered modifications of this idea as indeed did Lovejoy (1981) with his 'provisioning model' described above. Others, such as Nancy Tanner (1981) have suggested that infant carrying was key, whilst others have suggested stone tools and weapons drove the change. This stone tools theory is very unlikely, as though ancient humans were known to hunt, the discovery of tools was not discovered for thousands of years after the origin of bipedalism, temporally preventing it from being a driving force of evolution.
Wading models.
The observation that large Primates, including especially the great apes, that predominantly move quadrupedally on dry land, tend to switch to bipedal locomotion in waist deep water, has led to the idea that the origin of human bipedalism may have been influenced by waterside environments. This idea, labelled "The Wading Hypothesis", has been promoted for several decades by Elaine Morgan, as part of the aquatic ape hypothesis, which also proposes that swimming, diving and aquatic food sources exerted a strong influence on many aspects of human evolution, including bipedalism. The "aquatic ape hypothesis" is not accepted by or considered a serious theory within the anthropological scholarly community. Others, however, cite bipedalism among a cluster of other adaptations unique among primates, including voluntary control of breathing, hairlessness, subcutaneous fat and several other traits that are difficult to explain with more conventional theories.
Since 2000 Carsten Niemitz has published a series of papers and a book on a variant of the wading hypothesis, which he calls The Amphibian Generalist Theory. ("Amphibische Generalistentheorie").
Other theories have been proposed that suggest wading and the exploitation of aquatic food sources (providing essential nutrients for human brain evolution or critical fallback foods) may have exerted evolutionary pressures on human ancestors promoting adaptations which later assisted full-time bipedalism. It has also been thought that consistent water-based food sources had developed early hominid dependency and facilitated dispersal along seas and rivers.
Physiology.
Bipedal movement occurs in a number of ways, and requires many mechanical and neurological adaptations. Some of these are described below.
Biomechanics.
Standing.
Energy-efficient means of standing bipedally involve constant adjustment of balance, and of course these must avoid overcorrection. The difficulties associated with simple standing in upright humans are highlighted by the greatly increased risk of falling present in the elderly, even with minimal reductions in control system effectiveness.
Shoulder stability.
Shoulder stability would decrease with the evolution of bipedalism. Shoulder mobility would increase because the need for a stable shoulder is only present in arboreal habitats. Shoulder mobility would support suspensory locomotion behaviors which are present in human bipedalism. The forelimbs are freed from weight bearing capabilities which makes the shoulder a place of evidence for the evolution of bipedalism.
Walking.
Walking is characterized by an "inverted pendulum" movement in which the center of gravity vaults over a stiff leg with each step. Force plates can be used to quantify the whole-body kinetic & potential energy, with walking displaying an out-of-phase relationship indicating exchange between the two. Interestingly, this model applies to all walking organisms regardless of the number of legs, and thus bipedal locomotion does not differ in terms of whole-body kinetics.
In humans, walking is composed of several separate processes:
Running.
Running is characterized by a spring-mass movement. Kinetic and potential energy are in phase, and the energy is stored & released from a spring-like limb during foot contact. Again, the whole-body kinetics are similar to animals with more limbs.
Musculature.
Bipedalism requires strong leg muscles, particularly in the thighs. Contrast in domesticated poultry the well muscled legs, against the small and bony wings. Likewise in humans, the quadriceps and hamstring muscles of the thigh are both so crucial to bipedal activities that each alone is much larger than the well-developed biceps of the arms.
Respiration.
A biped has the ability to breathe while running, without strong coupling to stride cycle. Humans usually take a breath every other stride when their aerobic system is functioning. During a sprint the anaerobic system kicks in and breathing slows until the anaerobic system can no longer sustain a sprint.
Bipedal robots.
For nearly the whole of the 20th century, bipedal robots were very difficult to construct and robot locomotion involved only wheels, treads, or multiple legs. Recent cheap and compact computing power has made two-legged robots more feasible. Some notable biped robots are ASIMO, HUBO, MABEL and QRIO. Recently, spurred by the success of creating a fully passive, un-powered bipedal walking robot, those working on such machines have begun using principles gleaned from the study of human and animal locomotion, which often relies on passive mechanisms to minimize power consumption.

</doc>
<doc id="4211" url="https://en.wikipedia.org/wiki?curid=4211" title="Bootstrapping">
Bootstrapping

In general parlance, bootstrapping usually refers to a self-starting process that is supposed to proceed without external input. In computer technology the term (usually shortened to booting) usually refers to the process of loading the basic software into the memory of a computer after power-on or general reset, especially the operating system which will then take care of loading other software as needed.
The term appears to have originated in the early 19th century United States (particularly in the phrase "pull oneself over a fence by one's bootstraps"), to mean an absurdly impossible action, an adynaton.
Etymology.
Tall boots may have a tab, loop or handle at the top known as a bootstrap, allowing one to use fingers or a boot hook tool to help pulling the boots on. The saying "to pull oneself up by one's bootstraps" was already in use during the 19th century as an example of an impossible task. The idiom dates at least to 1834, when it appeared in the "Workingman's Advocate": "It is conjectured that Mr. Murphee will now be enabled to hand himself over the Cumberland river or a barn yard fence by the straps of his boots." In 1860 it appeared in a comment on metaphysical philosophy: "The attempt of the mind to analyze itself an effort analogous to one who would lift himself by his own bootstraps." Bootstrap as a metaphor, meaning to better oneself by one's own unaided efforts, was in use in 1922. This metaphor spawned additional metaphors for a series of self-sustaining processes that proceed without external help.
The term is sometimes attributed to a story in Rudolf Erich Raspe's "", but in that story Baron Munchausen pulls himself (and his horse) out of a swamp by his hair (specifically, his pigtail), not by his bootstraps and no explicit reference to bootstraps has been found elsewhere in the various versions of the Munchausen tales.
Applications.
Computing.
Software loading and execution.
Booting is the process of starting a computer, specifically with regard to starting its software. The process involves a chain of stages, in which at each stage a smaller simpler program loads and then executes the larger more complicated program of the next stage. It is in this sense that the computer "pulls itself up by its bootstraps", i.e. it improves itself by its own efforts. Booting is a chain of events that starts with execution of hardware-based procedures and may then hand-off to firmware and software which is loaded into main memory. Booting often involves processes such as performing self-tests, loading configuration settings, loading a BIOS, resident monitors, a hypervisor, an operating system, or utility software.
The computer term bootstrap began as a metaphor in the 1950s. In computers, pressing a bootstrap button caused a hardwired program to read a bootstrap program from an input unit. The computer would then execute the bootstrap program, which caused it to read more program instructions. It became a self-sustaining process that proceeded without external help from manually entered instructions. As a computing term, bootstrap has been used since at least 1953.
Software development.
Bootstrapping can also refer to the development of successively more complex, faster programming environments. The simplest environment will be, perhaps, a very basic text editor (e.g., ed) and an assembler program. Using these tools, one can write a more complex text editor, and a simple compiler for a higher-level language and so on, until one can have a graphical IDE and an extremely high-level programming language.
Historically, bootstrapping also refers to an early technique for computer program development on new hardware. The technique described in this paragraph has been replaced by the use of a cross compiler executed by a pre-existing computer. Bootstrapping in program development began during the 1950s when each program was constructed on paper in decimal code or in binary code, bit by bit (1s and 0s), because there was no high-level computer language, no compiler, no assembler, and no linker. A tiny assembler program was hand-coded for a new computer (for example the IBM 650) which converted a few instructions into binary or decimal code: A1. This simple assembler program was then rewritten in its just-defined assembly language but with extensions that would enable the use of some additional mnemonics for more complex operation codes. The enhanced assembler's source program was then assembled by its predecessor's executable (A1) into binary or decimal code to give A2, and the cycle repeated (now with those enhancements available), until the entire instruction set was coded, branch addresses were automatically calculated, and other conveniences (such as conditional assembly, macros, optimisations, etc.) established. This was how the early assembly program SOAP (Symbolic Optimal Assembly Program) was developed. Compilers, linkers, loaders, and utilities were then coded in assembly language, further continuing the bootstrapping process of developing complex software systems by using simpler software.
The term was also championed by Doug Engelbart to refer to his belief that organizations could better evolve by improving the process they use for improvement (thus obtaining a compounding effect over time). His SRI team that developed the NLS hypertext system applied this strategy by using the tool they had developed to improve the tool.
Compilers.
The development of compilers for new programming languages first developed in an existing language but then rewritten in the new language and compiled by itself, is another example of the bootstrapping notion. Using an existing language to bootstrap a new language is one way to solve the "chicken or the egg" causality dilemma.
Installers.
During the installation of computer programs it is sometimes necessary to update the installer or package manager itself. The common pattern for this is to use a small executable bootstrapper file (e.g. setup.exe) which updates the installer and starts the real installation after the update. Sometimes the bootstrapper also installs other prerequisites for the software during the bootstrapping process.
Overlay networks.
A bootstrapping node, also known as a rendezvous host, is a node in an overlay network that provides initial configuration information to newly joining nodes so that they may successfully join the overlay network.
Discrete event simulation.
A type of computer simulation called discrete event simulation represents the operation of a system as a chronological sequence of events. A technique called "bootstrapping the simulation model" is used, which bootstraps initial data points using a pseudorandom number generator to schedule an initial set of pending events, which schedule additional events, and with time, the distribution of event times approaches its steady state—the bootstrapping behavior is overwhelmed by steady-state behavior.
Artificial intelligence and machine learning.
Bootstrapping is a technique used to iteratively improve a classifier's performance. Seed AI is a hypothesized type of artificial intelligence capable of recursive self-improvement. Having improved itself, it would become better at improving itself, potentially leading to an exponential increase in intelligence. No such AI is known to exist, but it remains an active field of research.
Seed AI is a significant part of some theories about the technological singularity: proponents believe that the development of seed AI will rapidly yield ever-smarter intelligence (via bootstrapping) and thus a new era.
Statistics.
Bootstrapping is a resampling technique used to obtain estimates of summary statistics.
Business.
Bootstrapping in business means starting a business without external help or capital. Such startups fund the development of their company through internal cash flow and are cautious with their expenses. Generally at the start of a venture, a small amount of money will be set aside for the bootstrap process. Bootstrapping can also be a supplement for econometric models. Bootstrapping was also expanded upon in the book "Bootstrap Business", by Richard Christiansen.
Biology.
Richard Dawkins in his book "River Out of Eden" used the computer bootstrapping concept to explain how biological cells differentiate: "Different cells receive different combinations of chemicals, which switch on different combinations of genes, and some genes work to switch other genes on or off. And so the bootstrapping continues, until we have the full repertoire of different kinds of cells."
Phylogenetics.
Bootstrapping analysis gives a way to judge the strength of support for clades on phylogenetic trees. A number is written by a node, which reflects the percentage of bootstrap trees which also resolve the clade at the endpoints of that branch.
Law.
Bootstrapping is a rule preventing the admission of hearsay evidence in conspiracy cases.
Linguistics.
Bootstrapping is a theory of language acquisition.
Physics.
Bootstrapping is using very general consistency criteria to determine the form of a quantum theory from some assumptions on the spectrum of particles.
Electronics.
Bootstrapping is a form of positive feedback in analog circuit design.
Electric power grid.
An electric power grid is almost never brought down intentionally. Generators and power stations are started and shut down as necessary. A typical power station requires power for start up prior to being able to generate power. This power is obtained from the grid, so if the entire grid is down these stations cannot be started.
Therefore, to get a grid started, there must be at least a small number of power stations that can start entirely on their own. A black start is the process of restoring a power station to operation without relying on external power. In the absence of grid power, one or more black starts are used to bootstrap the grid.
Cellular networks.
A Bootstrapping Server Function (BSF) is an intermediary element in cellular networks which provides application independent functions for mutual authentication of user equipment and servers unknown to each other and for 'bootstrapping' the exchange of secret session keys afterwards. The term 'bootstrapping' is related to building a security relation with a previously unknown device first and to allow installing security elements (keys) in the device and the BSF afterwards.
Media.
A media bootstrap is the process whereby a story or meme is deliberately (but artificially) produced by self and peer-referential journalism, originally within a tight circle of media content originators, often commencing with stories written within the same media organization. This story is then expanded into a general media "accepted wisdom" with the aim of having it accepted as self-evident "common knowledge" by the reading, listening and viewing publics. The key feature of a media bootstrap is that as little hard, verifiable, external evidence as possible is used to support the story, preference being given to the citation (often unattributed) of other media stories, i.e. "journalists interviewing journalists".
Because the campaign is usually originated and at least initially concocted internally by a media organization with a particular agenda in mind, within a closed loop of reportage and opinionation, the campaign is said to have "pulled itself up by its own bootstraps".
A bootstrap campaign should be distinguished from a genuine news story of genuine interest, such as a natural disaster that kills thousands, or the death of a respected public figure. It is legitimate for these stories to be given coverage across all media platforms. What distinguishes a bootstrap from a real story is the contrived and organized manner in which the bootstrap appears to come out of nowhere. A bootstrap commonly claims to be tapping a hitherto unrecognized phenomenon within society.
As self-levitating by pulling on one's bootstraps is physically impossible, this is often used by the bootstrappers themselves to deny the possibility that the bootstrap campaign is indeed concocted and artificial. They assert that it has arisen via a groundswell of public opinion. Media campaigns that are openly admitted as concocted (e.g. a public service campaign titled "Let's Clean Up Our City") are usually ignored by other media organizations for reasons related to competition. On the other hand, the true bootstrap welcomes the participation of other media organizations, indeed encourages it, as this participation gains the bootstrap notoriety and, most importantly, legitimacy.

</doc>
<doc id="4213" url="https://en.wikipedia.org/wiki?curid=4213" title="Baltic languages">
Baltic languages

The Baltic languages belong to the Balto-Slavic branch of the Indo-European language family, and are spoken by the Balts. Baltic languages are spoken mainly in areas extending east and southeast of the Baltic Sea in Northern Europe. Scholars usually regard them as a single language family divided into two groups: Western Baltic (containing only extinct languages), and Eastern Baltic (containing two living languages, Lithuanian and Latvian). The range of the Eastern Balts once reached to the Ural mountains. Although related, the Lithuanian, the Latvian, and particularly the Old Prussian vocabularies differ substantially from one another and are not mutually intelligible. Old Prussian (a Western Baltic language which went extinct in the 18th century) ranks as the most archaic of the Baltic languages.
Branches.
The Baltic languages are generally thought to form a single family with two branches, Eastern and Western. However, these are sometimes classified as independent branches of Balto-Slavic.
"("†"—Extinct language)"
Geographic distribution.
Speakers of modern Baltic languages are generally concentrated within the borders of Lithuania and Latvia, and in emigrant communities in the United States, Canada, Australia and states of the former Soviet Union. Historically the languages were spoken over a larger area: West to the mouth of the Vistula river in present-day Poland, at least as far East as the Dniepr river in present-day Belarus, perhaps even to Moscow, perhaps as far south as Kiev. Key evidence of Baltic language presence in these regions is found in hydronyms (names of bodies of water) in the regions that are characteristically Baltic. Use of hydronyms is generally accepted to determine the extent of these cultures' influence, but "not" the date of such influence. Historical expansion of the usage of Slavic languages in the South and East, and Germanic languages in the West reduced the geographic distribution of Baltic languages to a fraction of the area that they formerly covered.
Prehistory and history.
Although the various Baltic tribes were mentioned by ancient historians as early as 98 B.C., the first attestation of a Baltic language was in about 1350, with the creation of the "Elbing Prussian Vocabulary", a German to Prussian translation dictionary. It is also believed that Baltic languages are among the most archaic of the remaining Indo-European languages, despite their late attestation. Lithuanian was first attested in a hymnal translation in 1545; the first printed book in Lithuanian, a Catechism by Martynas Mažvydas was published in 1547 in Königsberg, Prussia (now Kaliningrad, Russia). Latvian appeared in a hymnal in 1530 and in a printed Catechism in 1585. One reason for the late attestation is that the Baltic peoples resisted Christianization longer than any other Europeans, which delayed the introduction of writing and isolated their languages from outside influence.
With the establishment of a German state in Prussia, and the eradication or flight of much of the Baltic Prussian population in the 13th century, the remaining Prussians began to be assimilated, and by the end of the 17th century, the Prussian language had become extinct.
During the years of the Polish–Lithuanian Commonwealth (1569–1795), official documents were written in Polish, Ruthenian and Latin. After the Partitions of Commonwealth, most of the Baltic lands were under the rule of the Russian Empire, where the native languages were sometimes prohibited from being written down, or used publicly (see Lithuanian press ban).
Relationship with other Indo-European languages.
The Baltic languages are of particular interest to linguists because they retain many archaic features, which are believed to have been present in the early stages of the Proto-Indo-European language. However, linguists have had a hard time establishing the precise relationship of the Baltic languages to other languages in the Indo-European family. Several of the extinct Baltic languages have a limited or nonexistent written record, their existence being known only from the records of ancient historians and personal or place names. All of the languages in the Baltic group (including the living ones) were first written down relatively late in their probable existence as distinct languages. These two factors combined with others have obscured the history of the Baltic languages, leading to a number of theories regarding their position in the Indo-European family.
The Baltic languages show a close relationship with the Slavic languages, and are grouped with them in a Balto-Slavic family by most scholars. This family is considered to have developed from a common ancestor, Proto-Balto-Slavic. Later on, several lexical, phonological and morphological dialectisms developed, separating the various Balto-Slavic languages from each other. Although it is generally agreed upon that the Slavic languages developed from a single more-or-less unified dialect (Proto-Slavic) that split off from common Balto-Slavic, there is more disagreement about the relationship between the Baltic languages.
The traditional view is that the Balto-Slavic languages split into two branches, Baltic and Slavic, with each branch developing as a single common language (Proto-Baltic and Proto-Slavic) for some time afterwards. Proto-Baltic is then thought to have split into East Baltic and West Baltic branches. However, more recent scholarship has suggested that there was no unified Proto-Baltic stage, but that Proto-Balto-Slavic split directly into three groups: Slavic, East Baltic and West Baltic. Under this view, the Baltic family is paraphyletic, and consists of all Balto-Slavic languages that are not Slavic. This would imply that Proto-Baltic, the last common ancestor of all Baltic languages, would be identical to Proto-Balto-Slavic itself, rather than distinct from it.
Finally, there is a minority of scholars who argue that Baltic descended directly from Proto-Indo-European, without an intermediate common Balto-Slavic stage. They argue that the many similarities and shared innovations between Baltic and Slavic are due to several millennia of contact between the groups, rather than shared heritage.

</doc>
<doc id="4214" url="https://en.wikipedia.org/wiki?curid=4214" title="Bioinformatics">
Bioinformatics

Bioinformatics is an interdisciplinary field that develops methods and software tools for understanding biological data. As an interdisciplinary field of science, bioinformatics combines computer science, statistics, mathematics, and engineering to analyze and interpret biological data. Bioinformatics has been used for "in silico" analyses of biological queries using mathematical and statistical techniques.
Bioinformatics is both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis "pipelines" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidate genes and nucleotides (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences.
Introduction.
Bioinformatics has become an important part of many areas of biology. In experimental molecular biology, bioinformatics techniques such as image and signal processing allow extraction of useful results from large amounts of raw data. In the field of genetics and genomics, it aids in sequencing and annotating genomes and their observed mutations. It plays a role in the text mining of biological literature and the development of biological and gene ontologies to organize and query biological data. It also plays a role in the analysis of gene and protein expression and regulation. Bioinformatics tools aid in the comparison of genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology. At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. In structural biology, it aids in the simulation and modeling of DNA, RNA, and protein structures as well as molecular interactions.
History.
Historically, the term "bioinformatics" did not mean what it means today. Paulien Hogeweg and Ben Hesper coined it in 1970 to refer to the study of information processes in biotic systems. This definition placed bioinformatics as a field parallel to biophysics (the study of physical processes in biological systems) or biochemistry (the study of chemical processes in biological systems).
Sequences.
Computers became essential in molecular biology when protein sequences became available after Frederick Sanger determined the sequence of insulin in the early 1950s. Comparing multiple sequences manually turned out to be impractical. A pioneer in the field was Margaret Oakley Dayhoff, who has been hailed by David Lipman, director of the National Center for Biotechnology Information, as the "mother and father of bioinformatics." Dayhoff compiled one of the first protein sequence databases, initially published as books and pioneered methods of sequence alignment and molecular evolution. Another early contributor to bioinformatics was Elvin A. Kabat, who pioneered biological sequence analysis in 1970 with his comprehensive volumes of antibody sequences released with Tai Te Wu between 1980 and 1991.
Goals.
To study how normal cellular activities are altered in different disease states, the biological data must be combined to form a comprehensive picture of these activities. Therefore, the field of bioinformatics has evolved such that the most pressing task now involves the analysis and interpretation of various types of data. This includes nucleotide and amino acid sequences, protein domains, and protein structures. The actual process of analyzing and interpreting data is referred to as computational biology. Important sub-disciplines within bioinformatics and computational biology include:
The primary goal of bioinformatics is to increase the understanding of biological processes. What sets it apart from other approaches, however, is its focus on developing and applying computationally intensive techniques to achieve this goal. Examples include: pattern recognition, data mining, machine learning algorithms, and visualization. Major research efforts in the field include sequence alignment, gene finding, genome assembly, drug design, drug discovery, protein structure alignment, protein structure prediction, prediction of gene expression and protein–protein interactions, genome-wide association studies, the modeling of evolution and cell division/mitosis.
Bioinformatics now entails the creation and advancement of databases, algorithms, computational and statistical techniques, and theory to solve formal and practical problems arising from the management and analysis of biological data.
Over the past few decades, rapid developments in genomic and other molecular research technologies and developments in information technologies have combined to produce a tremendous amount of information related to molecular biology. Bioinformatics is the name given to these mathematical and computing approaches used to glean understanding of biological processes.
Common activities in bioinformatics include mapping and analyzing DNA and protein sequences, aligning DNA and protein sequences to compare them, and creating and viewing 3-D models of protein structures.
Relation to other fields.
Bioinformatics is a science field that is similar to but distinct from biological computation and computational biology. Biological computation uses bioengineering and biology to build biological computers, whereas bioinformatics uses computation to better understand biology. Bioinformatics and computational biology have similar aims and approaches, but they differ in scale: bioinformatics organizes and analyzes basic biological data, whereas computational biology builds theoretical models of biological systems, just as mathematical biology does with mathematical models.
Analyzing biological data to produce meaningful information involves writing and running software programs that use algorithms from graph theory, artificial intelligence, soft computing, data mining, image processing, and computer simulation. The algorithms in turn depend on theoretical foundations such as discrete mathematics, control theory, system theory, information theory, and statistics.
Sequence analysis.
Since the Phage Φ-X174 was sequenced in 1977, the DNA sequences of thousands of organisms have been decoded and stored in databases. This sequence information is analyzed to determine genes that encode proteins, RNA genes, regulatory sequences, structural motifs, and repetitive sequences. A comparison of genes within a species or between different species can show similarities between protein functions, or relations between species (the use of molecular systematics to construct phylogenetic trees). With the growing amount of data, it long ago became impractical to analyze DNA sequences manually. Today, computer programs such as BLAST are used daily to search sequences from more than 260 000 organisms, containing over 190 billion nucleotides. These programs can compensate for mutations (exchanged, deleted or inserted bases) in the DNA sequence, to identify sequences that are related, but not identical. A variant of this sequence alignment is used in the sequencing process itself. The so-called shotgun sequencing technique (which was used, for example, by The Institute for Genomic Research (TIGR) to sequence the first bacterial genome, "Haemophilus influenzae") does not produce entire chromosomes. Instead it generates the sequences of many thousands of small DNA fragments (ranging from 35 to 900 nucleotides long, depending on the sequencing technology). The ends of these fragments overlap and, when aligned properly by a genome assembly program, can be used to reconstruct the complete genome. Shotgun sequencing yields sequence data quickly, but the task of assembling the fragments can be quite complicated for larger genomes. For a genome as large as the human genome, it may take many days of CPU time on large-memory, multiprocessor computers to assemble the fragments, and the resulting assembly usually contains numerous gaps that must be filled in later. Shotgun sequencing is the method of choice for virtually all genomes sequenced today, and genome assembly algorithms are a critical area of bioinformatics research.
Following the goals that the Human Genome Project left to achieve after its closure in 2003, a new project developed by the National Human Genome Research Institute in the U.S appeared. The so-called ENCODE project is a collaborative data collection of the functional elements of the human genome that uses next-generation DNA-sequencing technologies and genomic tiling arrays, technologies able to generate automatically large amounts of data with lower research costs but with the same quality and viability.
Another aspect of bioinformatics in sequence analysis is annotation. This involves computational gene finding to search for protein-coding genes, RNA genes, and other functional sequences within a genome. Not all of the nucleotides within a genome are part of genes. Within the genomes of higher organisms, large parts of the DNA do not serve any obvious purpose.
Genome annotation.
In the context of genomics, annotation is the process of marking the genes and other biological features in a DNA sequence. This process needs to be automated because most genomes are too large to annotate by hand, not to mention the desire to annotate as many genomes as possible, as the rate of sequencing has ceased to pose a bottleneck. Annotation is made possible by the fact that genes have recognisable start and stop regions, although the exact sequence found in these regions can vary between genes.
The first genome annotation software system was designed in 1995 by Owen White, who was part of the team at The Institute for Genomic Research that sequenced and analyzed the first genome of a free-living organism to be decoded, the bacterium "Haemophilus influenzae". White built a software system to find the genes (fragments of genomic sequence that encode proteins), the transfer RNAs, and to make initial assignments of function to those genes. Most current genome annotation systems work similarly, but the programs available for analysis of genomic DNA, such as the GeneMark program trained and used to find protein-coding genes in "Haemophilus influenzae", are constantly changing and improving.
Computational evolutionary biology.
Evolutionary biology is the study of the origin and descent of species, as well as their change over time. Informatics has assisted evolutionary biologists by enabling researchers to:
Future work endeavours to reconstruct the now more complex tree of life.
The area of research within computer science that uses genetic algorithms is sometimes confused with computational evolutionary biology, but the two areas are not necessarily related.
Comparative genomics.
The core of comparative genome analysis is the establishment of the correspondence between genes (orthology analysis) or other genomic features in different organisms. It is these intergenomic maps that make it possible to trace the evolutionary processes responsible for the divergence of two genomes. A multitude of evolutionary events acting at various organizational levels shape genome evolution. At the lowest level, point mutations affect individual nucleotides. At a higher level, large chromosomal segments undergo duplication, lateral transfer, inversion, transposition, deletion and insertion.
Ultimately, whole genomes are involved in processes of hybridization, polyploidization and endosymbiosis, often leading to rapid speciation. The complexity of genome evolution poses many exciting challenges to developers of mathematical models and algorithms, who have recourse to a spectra of algorithmic, statistical and mathematical techniques, ranging from exact, heuristics, fixed parameter and approximation algorithms for problems based on parsimony models to Markov Chain Monte Carlo algorithms for Bayesian analysis of problems based on probabilistic models.
Many of these studies are based on the homology detection and protein families computation.
Pan genomics.
Pan genomics is a concept introduced in 2005 by Tettelin and Medini which eventually took root in bioinformatics. Pan genome is the complete gene repertoire of a particular taxonomic group: although initially applied to closely related strains of a species, it can be applied to a larger context like genus, phylum etc. It is divided in two parts- The Core genome: Set of genes common to all the genomes under study (These are often housekeeping genes vital for survival) and The Dispensable/Flexible Genome: Set of genes not present in all but one or some genomes under study.
Genetics of disease.
With the advent of next-generation sequencing we are obtaining enough sequence data to map the genes of complex diseases such as diabetes, infertility, breast cancer or Alzheimer's Disease. Genome-wide association studies are a useful approach to pinpoint the mutations responsible for such complex diseases. Through these studies, thousands of DNA variants have been identified that are associated with similar diseases and traits. Furthermore, the possibility for genes to be used at prognosis, diagnosis or treatment is one of the most essential applications. Many studies are discussing both the promising ways to choose the genes to be used and the problems and pitfalls of using genes to predict disease presence or prognosis.
Analysis of mutations in cancer.
In cancer, the genomes of affected cells are rearranged in complex or even unpredictable ways. Massive sequencing efforts are used to identify previously unknown point mutations in a variety of genes in cancer. Bioinformaticians continue to produce specialized automated systems to manage the sheer volume of sequence data produced, and they create new algorithms and software to compare the sequencing results to the growing collection of human genome sequences and germline polymorphisms. New physical detection technologies are employed, such as oligonucleotide microarrays to identify chromosomal gains and losses (called comparative genomic hybridization), and single-nucleotide polymorphism arrays to detect known "point mutations". These detection methods simultaneously measure several hundred thousand sites throughout the genome, and when used in high-throughput to measure thousands of samples, generate terabytes of data per experiment. Again the massive amounts and new types of data generate new opportunities for bioinformaticians. The data is often found to contain considerable variability, or noise, and thus Hidden Markov model and change-point analysis methods are being developed to infer real copy number changes.
However, with the breakthroughs that the next-generation sequencing technology is providing to the field of Bioinformatics, cancer genomics may be drastically change. This new methods and software allow bioinformaticians to sequence in a rapid and affordable way many cancer genomes. This could mean a more flexible process to classify types of cancer by analysis of cancer driven mutations in the genome. Furthermore, individual tracking of patients during the progression of the disease may be possible in the future with the sequence of cancer samples.
Another type of data that requires novel informatics development is the analysis of lesions found to be recurrent among many tumors.
Gene and protein expression.
Analysis of gene expression.
The expression of many genes can be determined by measuring mRNA levels with multiple techniques including microarrays, expressed cDNA sequence tag (EST) sequencing, serial analysis of gene expression (SAGE) tag sequencing, massively parallel signature sequencing (MPSS), RNA-Seq, also known as "Whole Transcriptome Shotgun Sequencing" (WTSS), or various applications of multiplexed in-situ hybridization. All of these techniques are extremely noise-prone and/or subject to bias in the biological measurement, and a major research area in computational biology involves developing statistical tools to separate signal from noise in high-throughput gene expression studies. Such studies are often used to determine the genes implicated in a disorder: one might compare microarray data from cancerous epithelial cells to data from non-cancerous cells to determine the transcripts that are up-regulated and down-regulated in a particular population of cancer cells.
Analysis of protein expression.
Protein microarrays and high throughput (HT) mass spectrometry (MS) can provide a snapshot of the proteins present in a biological sample. Bioinformatics is very much involved in making sense of protein microarray and HT MS data; the former approach faces similar problems as with microarrays targeted at mRNA, the latter involves the problem of matching large amounts of mass data against predicted masses from protein sequence databases, and the complicated statistical analysis of samples where multiple, but incomplete peptides from each protein are detected.
Analysis of regulation.
Regulation is the complex orchestration of events starting with an extracellular signal such as a hormone and leading to an increase or decrease in the activity of one or more proteins. Bioinformatics techniques have been applied to explore various steps in this process. For example, promoter analysis involves the identification and study of sequence motifs in the DNA surrounding the coding region of a gene. These motifs influence the extent to which that region is transcribed into mRNA. Expression data can be used to infer gene regulation: one might compare microarray data from a wide variety of states of an organism to form hypotheses about the genes involved in each state. In a single-cell organism, one might compare stages of the cell cycle, along with various stress conditions (heat shock, starvation, etc.). One can then apply clustering algorithms to that expression data to determine which genes are co-expressed. For example, the upstream regions (promoters) of co-expressed genes can be searched for over-represented regulatory elements. Examples of clustering algorithms applied in gene clustering are k-means clustering, self-organizing maps (SOMs), hierarchical clustering, and consensus clustering methods such as the Bi-CoPaM. The later, namely Bi-CoPaM, has been actually proposed to address various issues specific to gene discovery problems such as consistent co-expression of genes over multiple microarray datasets.
Structural bioinformatics.
Protein structure prediction is another important application of bioinformatics. The amino acid sequence of a protein, the so-called primary structure, can be easily determined from the sequence on the gene that codes for it. In the vast majority of cases, this primary structure uniquely determines a structure in its native environment. (Of course, there are exceptions, such as the bovine spongiform encephalopathy – a.k.a. Mad Cow Disease – prion.) Knowledge of this structure is vital in understanding the function of the protein. Structural information is usually classified as one of "secondary", "tertiary" and "quaternary" structure. A viable general solution to such predictions remains an open problem. Most efforts have so far been directed towards heuristics that work most of the time.
One of the key ideas in bioinformatics is the notion of homology. In the genomic branch of bioinformatics, homology is used to predict the function of a gene: if the sequence of gene "A", whose function is known, is homologous to the sequence of gene "B," whose function is unknown, one could infer that B may share A's function. In the structural branch of bioinformatics, homology is used to determine which parts of a protein are important in structure formation and interaction with other proteins. In a technique called homology modeling, this information is used to predict the structure of a protein once the structure of a homologous protein is known. This currently remains the only way to predict protein structures reliably.
One example of this is the similar protein homology between hemoglobin in humans and the hemoglobin in legumes (leghemoglobin). Both serve the same purpose of transporting oxygen in the organism. Though both of these proteins have completely different amino acid sequences, their protein structures are virtually identical, which reflects their near identical purposes.
Other techniques for predicting protein structure include protein threading and "de novo" (from scratch) physics-based modeling.
Network and systems biology.
"Network analysis" seeks to understand the relationships within biological networks such as metabolic or protein-protein interaction networks. Although biological networks can be constructed from a single type of molecule or entity (such as genes), network biology often attempts to integrate many different data types, such as proteins, small molecules, gene expression data, and others, which are all connected physically, functionally, or both.
"Systems biology" involves the use of computer simulations of cellular subsystems (such as the networks of metabolites and enzymes that comprise metabolism, signal transduction pathways and gene regulatory networks) to both analyze and visualize the complex connections of these cellular processes. Artificial life or virtual evolution attempts to understand evolutionary processes via the computer simulation of simple (artificial) life forms.
Molecular interaction networks.
Tens of thousands of three-dimensional protein structures have been determined by X-ray crystallography and protein nuclear magnetic resonance spectroscopy (protein NMR) and a central question in structural bioinformatics is whether it is practical to predict possible protein–protein interactions only based on these 3D shapes, without performing protein–protein interaction experiments. A variety of methods have been developed to tackle the protein–protein docking problem, though it seems that there is still much work to be done in this field.
Other interactions encountered in the field include Protein–ligand (including drug) and protein–peptide. Molecular dynamic simulation of movement of atoms about rotatable bonds is the fundamental principle behind computational algorithms, termed docking algorithms, for studying molecular interactions.
Others.
Literature analysis.
The growth in the number of published literature makes it virtually impossible to read every paper, resulting in disjointed sub-fields of research. Literature analysis aims to employ computational and statistical linguistics to mine this growing library of text resources. For example:
The area of research draws from statistics and computational linguistics.
High-throughput image analysis.
Computational technologies are used to accelerate or fully automate the processing, quantification and analysis of large amounts of high-information-content biomedical imagery. Modern image analysis systems augment an observer's ability to make measurements from a large or complex set of images, by improving accuracy, objectivity, or speed. A fully developed analysis system may completely replace the observer. Although these systems are not unique to biomedical imagery, biomedical imaging is becoming more important for both diagnostics and research. Some examples are:
High-throughput single cell data analysis.
Computational techniques are used to analyse high-throughput, low-measurement single cell data, such as that obtained from flow cytometry. These methods typically involve finding populations of cells that are relevant to a particular disease state or experimental condition.
Biodiversity informatics.
Biodiversity informatics deals with the collection and analysis of biodiversity data, such as taxonomic databases, or microbiome data. Examples of such analyses include phylogenetics, niche modelling, species richness mapping, or species identification tools.
Databases.
Databases are essential for bioinformatics research and applications. There is a huge number of available databases covering almost everything from DNA and protein sequences, molecular structures, to phenotypes and biodiversity. Databases generally fall into one of three types. Some contain data resulting directly from empirical methods such as gene knockouts. Others consist of predicted data, and most contain data from both sources. There are meta-databases that incorporate data compiled from multiple other databases. Some others are specialized, such as those specific to an organism. These databases vary in their format, way of accession and whether they are public or not. Some of the most commonly used databases are listed below. For a more comprehensive list, please check the link at the beginning of the subsection.
Please keep in mind that this is a quick sampling and generally most computation data is supported by wet lab data as well.
Software and tools.
Software tools for bioinformatics range from simple command-line tools, to more complex graphical programs and standalone web-services available from various bioinformatics companies or public institutions.
Open-source bioinformatics software.
Many free and open-source software tools have existed and continued to grow since the 1980s. The combination of a continued need for new algorithms for the analysis of emerging types of biological readouts, the potential for innovative "in silico" experiments, and freely available open code bases have helped to create opportunities for all research groups to contribute to both bioinformatics and the range of open-source software available, regardless of their funding arrangements. The open source tools often act as incubators of ideas, or community-supported plug-ins in commercial applications. They may also provide "de facto" standards and shared object models for assisting with the challenge of bioinformation integration.
The range of open-source software packages includes titles such as Bioconductor, BioPerl, Biopython, BioJava, BioJS, BioRuby, Bioclipse, EMBOSS, .NET Bio, Orange with its bioinformatics add-on, Apache Taverna, UGENE and GenoCAD. To maintain this tradition and create further opportunities, the non-profit Open Bioinformatics Foundation have supported the annual Bioinformatics Open Source Conference (BOSC) since 2000.
An alternative method to build public bioinformatics databases is to use the MediaWiki engine with the "WikiOpener" extension. This system allows the database to be accessed and updated by all experts in the field.
Web services in bioinformatics.
SOAP- and REST-based interfaces have been developed for a wide variety of bioinformatics applications allowing an application running on one computer in one part of the world to use algorithms, data and computing resources on servers in other parts of the world. The main advantages derive from the fact that end users do not have to deal with software and database maintenance overheads.
Basic bioinformatics services are classified by the EBI into three categories: SSS (Sequence Search Services), MSA (Multiple Sequence Alignment), and BSA (Biological Sequence Analysis). The availability of these service-oriented bioinformatics resources demonstrate the applicability of web-based bioinformatics solutions, and range from a collection of standalone tools with a common data format under a single, standalone or web-based interface, to integrative, distributed and extensible bioinformatics workflow management systems.
Bioinformatics workflow management systems.
A Bioinformatics workflow management system is a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in a Bioinformatics application. Such systems are designed to
Some of the platforms giving this service: Galaxy, Kepler, Taverna, UGENE, Anduril.
Education platforms.
Software platforms designed to teach bioinformatics concepts and methods include Rosalind and online courses offered through the Swiss Institute of Bioinformatics Training Portal. The Canadian Bioinformatics Workshops provides videos and slides from training workshops on their website under a Creative Commons license. The 4273π project or 4273pi project also offers open source educational materials for free. The course runs on low cost raspberry pi computers and has been used to teach adults and school pupils. 4273π is actively developed by a consortium of academics and research staff who have run research level bioinformatics using raspberry pi computers and the 4273π operating system. 
Conferences.
There are several large conferences that are concerned with bioinformatics. Some of the most notable examples are Intelligent Systems for Molecular Biology (ISMB), European Conference on Computational Biology (ECCB), and Research in Computational Molecular Biology (RECOMB).

</doc>
<doc id="4216" url="https://en.wikipedia.org/wiki?curid=4216" title="Brian De Palma">
Brian De Palma

Brian Russell De Palma (born September 11, 1940) is an American film director and screenwriter. He was part of the New Hollywood wave of filmmaking.
In a career spanning over 40 years, he is best known for his suspense, psychological thriller and crime films. He directed successful and popular films such as the supernatural horror "Carrie", the erotic crime thriller "Dressed to Kill", the thriller "Blow Out", the crime dramas "Scarface", "The Untouchables" and "Carlito's Way", and the action spy film "".
Early life.
De Palma, who is of Italian ancestry, was born in Newark, New Jersey, the son of Vivienne (née Muti) and Anthony Federico De Palma, an orthopedic surgeon. He was raised in Philadelphia, Pennsylvania and New Hampshire, and attended various Protestant and Quaker schools, eventually graduating from Friends' Central School. When he was in high school, he built computers. He won a regional science-fair prize for a project titled "An Analog Computer to Solve Differential Equations".
1960s and early career.
Enrolled at Columbia as a physics student, De Palma became enraptured with the filmmaking process after viewing "Citizen Kane" and "Vertigo". De Palma subsequently enrolled at the newly coed Sarah Lawrence College as a graduate student in their theater department in the early 1960s, becoming one of the first male students among a female population. Once there, influences as various as drama teacher Wilford Leach, the Maysles brothers, Michelangelo Antonioni, Jean-Luc Godard, Andy Warhol and Alfred Hitchcock impressed upon De Palma the many styles and themes that would shape his own cinema in the coming decades. An early association with a young Robert De Niro resulted in "The Wedding Party". The film, which was co-directed with Leach and producer Cynthia Munroe, had been shot in 1963 but remained unreleased until 1969, when De Palma's star had risen sufficiently within the Greenwich Village filmmaking scene. De Niro was unknown at the time; the credits mistakenly display his name as "Robert ." The film is noteworthy for its invocation of silent film techniques and an insistence on the jump-cut for effect. De Palma followed this with various small films for the NAACP and The Treasury Department.
During the 1960s, De Palma began making a living producing documentary films, notably "The Responsive Eye", a 1966 movie about "The Responsive Eye" op-art exhibit curated by William Seitz for Museum of Modern Art in 1965. In an interview with Gelmis from 1969, De Palma described the film as "very good and very successful. It's distributed by Pathe Contemporary and makes lots of money. I shot it in four hours, with synched sound. I had two other guys shooting people's reactions to the paintings, and the paintings themselves."
"Dionysus in 69" (1969) was De Palma's other major documentary from this period. The film records The Performance Group's performance of Euripides' The Bacchae, starring, amongst others, De Palma regular William Finley. The play is noted for breaking traditional barriers between performers and audience. The film's most striking quality is its extensive use of the split-screen. De Palma recalls that he was "floored" by this performance upon first sight, and in 1973 recounts how he "began to try and figure out a way to capture it on film. I came up with the idea of split-screen, to be able to show the actual audience involvement, to trace the life of the audience and that of the play as they merge in and out of each other."
De Palma's most significant features from this decade are "Greetings" (1968) and "Hi, Mom!" (1970). Both films star Robert De Niro and espouse a Leftist revolutionary viewpoint common to their era. "Greetings" was entered into the 19th Berlin International Film Festival, where it won a Silver Bear award. His other major film from this period is the slasher comedy "Murder a la Mod". Each of these films contains experiments in narrative and intertextuality, reflecting De Palma's stated intention to become the "American Godard" while integrating several of the themes which permeated Hitchcock's work.
"Greetings" is about three New Yorkers dealing with the draft. The film is often considered the first to deal explicitly with the draft. The film is noteworthy for its use of various experimental techniques to convey its narrative in ultimately unconventional ways. Footage was sped up, rapid cutting was used to distance the audience from the narrative, and it was difficult to discern with whom the audience must ultimately align. "Greetings" ultimately grossed over $1 million at the box office and cemented De Palma's position as a bankable filmmaker.
After the success of his 1968 breakthrough, De Palma and his producing partner, Charles Hirsch, were given the opportunity by Sigma 3 to make an unofficial sequel of sorts, initially entitled "Son of Greetings", and subsequently released as "Hi, Mom!". While "Greetings" accentuated its varied cast, "Hi, Mom!" focuses on De Niro's character, Jon Rubin, an essential carry-over from the previous film. The film is ultimately significant insofar as it displays the first enunciation of De Palma's style in all its major traits – voyeurism, guilt, and a hyper-consciousness of the medium are all on full display, not just as hallmarks, but built into this formal, material apparatus itself.
These traits come to the fore in "Hi, Mom!"'s "Be Black, Baby" sequence. This sequence parodies cinéma vérité, the dominant documentary tradition of the 1960s, while simultaneously providing the audience with a visceral and disturbingly emotional experience. De Palma describes the sequence as a constant invocation of Brechtian distanciation: “First of all, I am interested in the medium of film itself, and I am constantly standing outside and making people aware that they are always watching a film. At the same time I am evolving it. In "Hi, Mom!" for instance, there is a sequence where you are obviously watching a ridiculous documentary and you are told that and you are aware of it, but it still sucks you in. There is a kind of Brechtian alienation idea here: you are aware of what you are watching at the same time that you are emotionally involved with it.”
"Be Black, Baby" was filmed in black and white stock on 16 mm, in low-light conditions that stress the crudity of the direct cinema aesthetic. It is precisely from this crudity that the film itself gains a credibility of “realism.” In an interview with Michael Bliss, De Palma notes “e Black, Bab was rehearsed for almost three weeks... In fact, it's all scripted. But once the thing starts, they just go with the way it's going. I specifically got a very good documentary camera filmmaker (Robert Elfstrom) to just shoot it like a documentary to follow the action." Furthermore, "I wanted to show in "Hi, Mom!" how you can really involve an audience. You take an absurd premise – "Be Black, Baby" – and totally involve them and really frighten them at the same time. It's very Brechtian. You suck 'em in and annihilate 'em. Then you say, "It's just a movie, right? It's not real." It's just like television. You're sucked in all the time, and you're being lied to in a very documentary-like setting. The "Be Black, Baby" section of "Hi, Mom!" is probably the most important piece of film I've ever done."
Transition to Hollywood.
In the 1970s, De Palma went to Hollywood where he worked on bigger budget films. In 1970, De Palma left New York for Hollywood at age thirty to make "Get to Know Your Rabbit", starring Orson Welles and Tommy Smothers. Making the film was a crushing experience for De Palma as Tommy Smothers didn't like a lot of De Palma's ideas.
After several small, studio and independent released films that included stand-outs "Sisters", "Phantom of the Paradise", and "Obsession", a small film based on a novel called "Carrie" was released directed by Brian De Palma. The psychic thriller "Carrie" is seen by some as De Palma's bid for a blockbuster. In fact, the project was small, underfunded by United Artists, and well under the cultural radar during the early months of production, as Stephen King's source novel had yet to climb the bestseller list. De Palma gravitated toward the project and changed crucial plot elements based upon his own predilections, not the saleability of the novel. The cast was young and relatively new, though the stars Sissy Spacek and John Travolta had gained considerable attention for previous work in, respectively, film and episodic sitcoms. "Carrie" became a hit, the first genuine box-office success for De Palma. It garnered Spacek and Piper Laurie Oscar nominations for their performances. Preproduction for the film had coincided with the casting process for George Lucas's "Star Wars", and many of the actors cast in De Palma's film had been earmarked as contenders for Lucas's movie, and vice versa. The "shock ending" finale is effective even while it upholds horror-film convention, its suspense sequences are buttressed by teen comedy tropes, and its use of split-screen, split-diopter and slow motion shots tell the story visually rather than through dialogue.
The financial and critical success of "Carrie" allowed De Palma to pursue more personal material. "The Demolished Man" was a novel that had fascinated De Palma since the late 1950s and appealed to his background in mathematics and avant-garde storytelling. Its unconventional unfolding of plot (exemplified in its mathematical layout of dialogue) and its stress on perception have analogs in De Palma's filmmaking. He sought to adapt it on numerous occasions, though the project would carry a substantial price tag, and has yet to appear onscreen (Steven Spielberg's adaptation of Philip K. Dick's "Minority Report" bears striking similarities to De Palma's visual style and some of the themes of "The Demolished Man"). The result of his experience with adapting "The Demolished Man" was "The Fury", a science fiction psychic thriller that starred Kirk Douglas, Carrie Snodgress, John Cassavetes and Amy Irving. The film was admired by Jean-Luc Godard, who featured a clip in his mammoth Histoire(s) du cinéma, and Pauline Kael who championed both "The Fury" and De Palma. The film boasted a larger budget than "Carrie", though the consensus view at the time was that De Palma was repeating himself, with diminishing returns. As a film it retains De Palma's considerable visual flair, but points more toward his work in mainstream entertainments such as "The Untouchables" and "", the thematic complex thrillers for which he is now better known.
For many film-goers, De Palma's gangster films, most notably "Scarface" and "Carlito's Way", pushed the envelope of violence and depravity, and yet greatly vary from one another in both style and content and also illustrate De Palma's evolution as a film-maker. In essence, the excesses of "Scarface" contrast with the more emotional tragedy of "Carlito's Way". Both films feature Al Pacino in what has become a fruitful working relationship. In 1984, he directed the music video of Bruce Springsteen's song "Dancing in the Dark".
Later into the 1990s and 2000s, De Palma did other films. He attempted to do dramas and a few thrillers plus science fiction. Some of these movies ("Mission: Impossible", "Carlito's Way") worked and some others ("The Bonfire of the Vanities", "Raising Cain", "Mission to Mars") failed at the box office. Of these films, "The Bonfire of the Vanities" would be De Palma's biggest box office disaster, losing millions. Another later movie from De Palma, "Redacted", unleashed a controversy over its subject of American involvement in Iraq, and supposed atrocities committed there. It received limited release in the United States.
In 2012, his film "Passion" was selected to compete for the Golden Lion at the 69th Venice International Film Festival.
Trademarks and style.
Themes.
De Palma's films can fall into two categories, his psychological thrillers ("Sisters", "Body Double", "Obsession", "Dressed to Kill", "Blow Out", "Raising Cain") and his mainly commercial films ("Scarface", "The Untouchables", "Carlito's Way", and "Mission: Impossible"). He has often produced "De Palma" films one after the other before going on to direct a different genre, but would always return to his familiar territory. Because of the subject matter and graphic violence of some of De Palma's films, such as "Dressed to Kill", "Scarface" and "Body Double", they are often at the center of controversy with the Motion Picture Association of America, film critics and the viewing public.
De Palma is known for quoting and referencing other directors' work throughout his career. Michelangelo Antonioni's "Blowup" and Francis Ford Coppola's "The Conversation" plots were used for the basis of "Blow Out". "The Untouchables" finale shoot out in the train station is a clear borrow from the Odessa Steps sequence in Sergei Eisenstein's "The Battleship Potemkin". The main plot from "Rear Window" was used for "Body Double", while it also used elements of "Vertigo". "Vertigo" was also the basis for "Obsession". "Dressed to Kill" was a note-for-note homage to Hitchcock's "Psycho", including such moments as the surprise death of the lead actress and the exposition scene by the psychiatrist at the end.
Camera shots.
Film critics have often noted De Palma's penchant for unusual camera angles and compositions throughout his career. He often frames characters against the background using a canted angle shot. Split-screen techniques have been used to show two separate events happening simultaneously. To emphasize the dramatic impact of a certain scene De Palma has employed a 360-degree camera pan. Slow sweeping, panning and tracking shots are often used throughout his films, often through precisely-choreographed long takes lasting for minutes without cutting. Split focus shots, often referred to as "di-opt", are used by De Palma to emphasize the foreground person/object while simultaneously keeping a background person/object in focus. Slow-motion is frequently used in his films to increase suspense.
Personal life.
De Palma dated actress Margot Kidder in the early 1970s. He has been married and divorced three times, to actress Nancy Allen (1979–1983), producer Gale Anne Hurd (1991–1993), and Darnell Gregorio (1995–1997). He has one daughter from his marriage to Gale Anne Hurd, Lolita de Palma, born in 1991, and one daughter from his marriage to Darnell Gregorio, Piper De Palma, born in 1996. He resides in Manhattan, New York.
Legacy.
De Palma is often cited as a leading member of the New Hollywood generation of film directors, a distinct pedigree who either emerged from film schools or are overtly cine-literate. His contemporaries include Martin Scorsese, Paul Schrader, John Milius, George Lucas, Francis Ford Coppola, Steven Spielberg, John Carpenter, and Ridley Scott. His artistry in directing and use of cinematography and suspense in several of his films has often been compared to the work of Alfred Hitchcock. Psychologists have been intrigued by De Palma's fascination with pathology, by the aberrant behavior aroused in characters who find themselves manipulated by others.
De Palma has encouraged and fostered the filmmaking careers of directors such as Quentin Tarantino, Mark Romanek and Keith Gordon. During an interview with De Palma, Tarantino said that "Blow Out" is one of his all time favorite films, and that after watching "Scarface" he knew how to make his own film. Terrence Malick credits seeing De Palma's early films on college campus tours as a validation of independent film, and subsequently switched his attention from philosophy to filmmaking.
Critics who frequently admire De Palma's work include Pauline Kael and Roger Ebert, among others. Kael wrote in her review of "Blow Out", "At forty, Brian De Palma has more than twenty years of moviemaking behind him, and he has been growing better and better. Each time a new film of his opens, everything he has done before seems to have been preparation for it." In his review of "Femme Fatale", Roger Ebert wrote about the director: "De Palma deserves more honor as a director. Consider also these titles: "Sisters", "Blow Out", "The Fury", "Dressed to Kill", "Carrie", "Scarface", "Wise Guys", "Casualties of War", "Carlito's Way", "Mission: Impossible". Yes, there are a few failures along the way ("Snake Eyes", "Mission to Mars", "The Bonfire of the Vanities"), but look at the range here, and reflect that these movies contain treasure for those who admire the craft as well as the story, who sense the glee with which De Palma manipulates images and characters for the simple joy of being good at it. It's not just that he sometimes works in the style of Hitchcock, but that he has the nerve to."
Criticisms.
Julie Salamon has written that "e Palm was: a perverse misogynist." De Palma has responded to accusations of misogyny by saying: "I'm always attacked for having an erotic, sexist approach-- chopping up women, putting women in peril. I'm making suspense movies! What else is going to happen to them?"
David Thomson wrote in his entry for De Palma, "There is a self-conscious cunning in De Palma's work, ready to control everything except his own cruelty and indifference."
References.
Notes
Bibliography
Further reading

</doc>
<doc id="4218" url="https://en.wikipedia.org/wiki?curid=4218" title="North American B-25 Mitchell">
North American B-25 Mitchell

The North American B-25 Mitchell is an American twin-engine, medium bomber manufactured by North American Aviation (NAA). It was named in honor of Major General William "Billy" Mitchell, a pioneer of U.S. military aviation. Used by many Allied air forces, the B-25 served in every theater of World War II and after the war ended many remained in service, operating across four decades. Produced in numerous variants, nearly 10,000 Mitchells rolled from NAA factories. These included a few limited models, such as the United States Marine Corps' PBJ-1 patrol bomber and the United States Army Air Forces' F-10 reconnaissance aircraft and AT-24 trainers.
Design and development.
The Air Corps issued a circular (Number 38-385) in March 1938 describing the performance they required from the next bombers — a payload of with a range of at more than . Those performance specifications led NAA to submit their NA-40 design. The NA-40 had benefited from the North American XB-21 (NA-39) of 1936 which was the company's partly-successful design for an earlier medium bomber that had been initially accepted and ordered but then cancelled. However, the company's experience from the XB-21 contributed to the design and development of the NA-40. The single NA-40 built flew first at the end of January 1939. It went through several modifications to correct problems. These improvements included fitting 1,600 hp Wright R-2600 "Double Cyclone" radial engines, in March 1939 which solved the lack of power.
In March 1939, North American delivered the substantially redesigned and improved NA-40 (as NA-40B) to the United States Army Air Corps for evaluation. It was in competition with other manufacturers' designs (Douglas 7B, Stearman X-100, and the Martin Model 167F) but failed to win orders. The aircraft was originally intended to be an attack bomber for export to the United Kingdom and France, both of which had a pressing requirement for such aircraft in the early stages of World War II. However, the French had already opted for a revised Douglas 7B (as the DB-7). Unfortunately, the NA-40B was destroyed in a crash on 11 April 1939 while undergoing testing. Although the crash was not considered due to a fault with the aircraft design, the Army ordered the DB-7 as the A-20.
The Air Corps issued a specification for a medium bomber in March 1939: over at NAA used the NA-40B design to develop the NA-62 which competed for the medium bomber contract. There was no YB-25 for prototype service tests. In September 1939, the Air Corps ordered the NA-62 into production as the B-25, along with the other new Air Corps medium bomber, the Martin B-26 Marauder "off the drawing board".
The NA-40 lost out to the Douglas A-20 in the attack type competition, but NAA developed a more advanced design, the NA-40B, which in turn led to the NA-62, B-25 Mitchell bomber.
Early into B-25 production, NAA incorporated a significant redesign to the wing dihedral. The first nine aircraft had a constant-dihedral, meaning the wing had a consistent, upward angle from the fuselage to the wingtip. This design caused stability problems. "Flattening" the outer wing panels by giving them a slight anhedral angle just outboard of the engine nacelles nullified the problem, and gave the B-25 its gull wing configuration. Less noticeable changes during this period included an increase in the size of the tail fins and a decrease in their inward tilt at their tops.
NAA continued design and development in 1940 and 1941. Both the B-25A and B-25B series entered AAF service. The B-25B was operational in 1942. Combat requirements lead to further developments. Before the year was over, NAA was producing the B-25C and B-25D series at different plants. Also in 1942, the manufacturer began design work on the cannon-armed B-25G series. The NA-100 of 1943 and 1944 was an interim armament development at the Kansas City complex known as the B-25D2. Similar armament upgrades by U.S-based commercial modification centers involved about half of the B-25G series. Further development led to the B-25H, B-25J and B-25J2. The gunship design concept dates to late 1942 and NAA sent a field technical representative to the SWPA. The factory produced B-25G entered production during the NA-96 order followed by the redesigned B-25H gunship. The B-25J reverted to the bomber role, but it, too, could be outfitted as a strafer.
North American Aviation manufactured the greatest number of aircraft in World War II. It was the first time a company had produced trainers, bombers and fighters simultaneously (the AT-6/SNJ Texan, B-25 Mitchell, and the P-51 Mustang). It produced B-25s at both its Inglewood main plant and an additional 6,608 aircraft at its Kansas City, Kansas plant at Fairfax Airport.
Postwar, the USAF placed a contract for the TB-25L trainer in 1952. This was a modification program by Hayes of Birmingham, Alabama. Its primary role was reciprocal engine pilot training.
A development of the B-25 was the North American XB-28, designed as a high-altitude bomber. Two prototypes were built with the second prototype, the XB-28A, evaluated as a photo-reconnaissance platform but the aircraft did not enter production.
Operational history.
Asia-Pacific.
The majority of B-25s in American service were used in the war against Japan in Asia and the Pacific. The Mitchell fought from the Northern Pacific to the South Pacific and the Far East. These areas included the campaigns in the Aleutian Islands, Papua New Guinea, the Solomon Islands, New Britain, China, Burma and the island hopping campaign in the Central Pacific. The aircraft’s potential as a ground-attack aircraft emerged during the Pacific war. The jungle environment reduced the usefulness of medium-level bombing, and made low-level attack the best tactic. Using similar mast height level tactics and skip bombing, the B-25 proved itself to be a capable anti-shipping weapon and sank many enemy sea vessels of various types. An ever-increasing number of forward firing guns made the B-25 a formidable strafing aircraft for island warfare. The strafer versions were the B-25C1/D1, the B-25J1 and with the NAA strafer nose, the J2 sub-series.
In Burma, the B-25 was often used to attack Japanese communication links, especially bridges in central Burma. It also helped supply the besieged troops at Imphal in 1944. The China Air Task Force, the Chinese American Composite Wing, the First Air Commando Group, the 341st Bomb Group, and eventually, the relocated 12th Bomb Group, all operated the B-25 in the China Burma India Theater (CBI). Many of these missions involved battle field isolation, interdiction and close air support.
Later in the war, as the AAF acquired bases in other parts of the Pacific, the Mitchell could strike targets in Indochina, Formosa and Kyushu, increasing the usefulness of the B-25. It was also used in some of the shortest raids of the Pacific War, striking from Saipan against Guam and Tinian. The 41st Bomb Group used it against Japanese-occupied islands that had been bypassed by the main campaign, such as happened in the Marshall Islands.
Middle East and Italy.
The first B-25s arrived in Egypt and were carrying out independent operations by October 1942. There operations against Axis airfields and motorized vehicle columns supported the ground actions of the Second Battle of El Alamein. From there, the aircraft took part in the rest of the campaign in North Africa, the invasion of Sicily and the advance up Italy. In the Strait of Messina to the Aegean Sea the B-25 conducted sea sweeps as part of the coastal air forces. In Italy, the B-25 was used in the ground attack role, concentrating on attacks against road and rail links in Italy, Austria and the Balkans. The B-25 had a longer range than the Douglas A-20 Havoc and Douglas A-26 Invaders, allowing it to reach further into occupied Europe. The five bombardment groups - 20 squadrons - of the Ninth and Twelfth Air Forces that used the B-25 in the Mediterranean Theater of Operations were the only U.S. units to employ the B-25 in Europe.
Europe.
The RAF received nearly 900 Mitchells, using them to replace Douglas Bostons, Lockheed Venturas and Vickers Wellington bombers. The Mitchell entered active RAF service on 22 January 1943. At first, it was used to bomb targets in occupied Europe. After the Normandy invasion, the RAF and France used Mitchells in support of the Allies in Europe. Several squadrons moved to forward airbases on the continent. The AAF did not use the B-25 in combat in the ETO.
Military operators.
USAAF.
The B-25B first gained fame as the bomber used in the 18 April 1942 Doolittle Raid, in which 16 B-25Bs led by Lieutenant Colonel Jimmy Doolittle attacked mainland Japan, four months after the bombing of Pearl Harbor. The mission gave a much-needed lift in spirits to the Americans, and alarmed the Japanese, who had believed their home islands to be inviolable by enemy forces. Although the amount of actual damage done was relatively minor, it forced the Japanese to divert troops for home defense for the remainder of the war.
The raiders took off from the carrier and successfully bombed Tokyo and four other Japanese cities without loss. Fifteen of the bombers subsequently crash-landed en route to recovery fields in eastern China. These losses were the result of the task force being spotted by a Japanese vessel, forcing the bombers to take off early, fuel exhaustion, stormy nighttime conditions with zero visibility, and lack of electronic homing aids at the recovery bases. Only one B-25 bomber landed intact, in Siberia where its five-man crew was interned and the aircraft confiscated. Of the 80 aircrew, 69 survived their historic mission and eventually made it back to American lines.
Following a number of additional modifications, including the addition of Plexiglas dome for navigational sightings to replace the overhead window for the navigator and heavier nose armament, de-icing and anti-icing equipment, the B-25C entered AAF operations. Through block 20 the B-25C and B-25D differed only in location of manufacture: C series at Inglewood, California; D series at Kansas City, Kansas. After block 20 some NA-96 began the transition to the G series while some NA-87 acquired interim modifications eventually produced as the B-25D2 and ordered as the NA-100. NAA built a total of 3,915 B-25Cs and Ds during World War II.
Although the B-25 was originally designed to bomb from medium altitudes in level flight, it was used frequently in the Southwest Pacific theatre in treetop-level strafing and missions with parachute-retarded fragmentation bombs against Japanese airfields in New Guinea and the Philippines. These heavily armed Mitchells were field-modified at Townsville, Australia, under the diirection of Major Paul I. "Pappy" Gunn and North American tech rep Jack Fox, These "commerce destroyers" were also used on strafing and skip bombing missions against Japanese shipping trying to resupply their armies.
Under the leadership of Lieutenant General George C. Kenney, Mitchells of the Far East Air Forces and its exising components, the Fifth and Thirteenth Air Forces devastated Japanese targets in the Southwest Pacific Theater during 1944 to 1945. The USAAF played a significant role in pushing the Japanese back to their home islands. The type operated with great effect in the Central Pacific, Alaska, North Africa, Mediterranean and China-Burma-India (CBI) theaters.
The AAF Antisubmarine Command made great use of the B-25 in 1942 and 1943. Some of the earliest B-25 Bomb Groups also flew the Mitchell on coastal patrols after the Pearl Harbor attack, prior to the AAFAC organization. Many of the two dozen or so Antisubmarine Squadrons flew the B-25C, D and G series in the American Theater Antisubmarine campaign, often in the distinctive, white sea search camouflage.
Combat developments.
Use as a gunship.
In anti-shipping operations, the AAF had urgent need for hard-hitting aircraft, and North American responded with the B-25G. In this series the transparent nose and bombardier/navigator position was changed for a shorter, hatched nose with two fixed .50 in (12.7 mm) machine guns and a 75 mm (2.95 in) M4 cannon, one of the largest weapons fitted to an aircraft, similar to the experimental British 57 mm gun-armed Mosquito Mk. XVIII and the German Ju 88P heavy cannon (up to a 75 mm long-barrel "Bordkanone BK 7,5"). The shorter nose placed the cannon breech behind the pilot where it could be manually loaded and serviced by the navigator; his crew station was moved to just behind the pilot. The navigator signalled the pilot when the gun was ready and the pilot fired the weapon using a button on his control wheel.
The RAF, USN and USSR each conducted trials with this series but none adopted it. The G series comprised one prototype, five pre-production C conversions, 58 C series modifications and 400 production aircraft for a total of 464 B-25G. In its final version, the G-12, an interim armament modification, eliminated the lower Bendix turret and added a starboard dual gun pack, waist guns and a canopy for the tail gunner to improve the view when firing the single tail gun. In April 1945 the air depots in Hawaii refurbished about two dozen of these and included the eight gun nose and rocket launchers in the upgrade.
The B-25H series continued the development of the gunship concept. NAA Inglewood produced 1000. The H had even more firepower. Most replaced the M4 gun with the lighter T13E1, designed specifically for the aircraft but 20-odd H-1 block aircraft completed by the Republic Aviation modification center at Evansville had the M4 and two machine gun nose armament. The 75 mm (2.95 in) gun fired at a muzzle velocity of . Due to its low rate of fire (about four rounds could be fired in a single strafing run), relative ineffectiveness against ground targets, and the substantial recoil, the 75 mm gun was sometimes removed from both G and H models and replaced with two additional .50 in (12.7 mm) machine guns as a field modification. In the new FEAF these were re-designated the G1 and H1 series respectively.
The H series normally came from the factory mounting four fixed, forward-firing .50  (12.7 mm) machine guns in the nose; four more fixed guns in forward-firing, individual gun packages; two more in the manned dorsal turret, re-located forward to a position just behind the cockpit; one each in a pair of new waist positions, introduced simultaneously with the forward-relocated dorsal turret; and lastly, a pair of guns in a new tail gunner's position. Company promotional material bragged that the B-25H could "bring to bear 10 machine guns coming and four going, in addition to the 75 mm cannon, eight rockets and 3,000 lb (1,360 kg) of bombs."
The H had a modified cockpit with single flight controls operated by pilot. The co-pilot's station and controls were deleted, and instead had a smaller seat used by the navigator/cannoneer, The radio operator crew position was aft the bomb bay with access to the waist guns. Factory production total were 405 B-25Gs and 1,000 B-25Hs, with 248 of the latter being used by the Navy as PBJ-1H. Elimination of the co-pilot saved weight, moving the dorsal turret forward counterbalanced in part the waist guns and the manned rear turret.
Return to medium bomber.
Following the two gunship series NAA again produced the medium bomber configuration with the B-25J series.. It optimized the mix of the interim NA-100 and the H series having both the bombardier's station and fixed guns of the D and the forward turret and refined armament of the H series. NAA also produced a strafer nose first shipped to air depots as kits, then introduced on the production line in alternating blocks with the bombardier nose. The solid-metal "strafer" nose housed eight centerline Browning M2 .50 calibre machine guns. The remainder of the armament was as in the H-5. NAA also supplied kits to mount eight underwing 5 "high velocity airborne rockets" (HVAR) just outside the propeller arcs. These were mounted on zero length launch rails, four to a wing.
The final, and the most built, series of the Mitchell, the B-25J, looked less like earlier series apart from the well-glazed bombardier's nose of nearly-identical appearance to the earliest B-25 subtypes. Instead, the J followed the overall configuration of the H series from the cockpit aft. It had the forward dorsal turret and other armament and airframe advancements. All J models included four .50 in (12.7 mm) light-barrel Browning AN/M2 guns in a pair of "fuselage package", conformal gun pods each flanking the lower cockpit, each pod containing two Browning M2s. By 1945, however, combat squadrons removed these. The J series restored the co-pilot's seat and dual flight controls. The factory made available kits to the Air Depot system to create the strafer-nose B-25J-2. This configuration carried a total of 18 .50 in (12.7 mm) light-barrel AN/M2 Browning M2 machine guns: eight in the nose, four in the flank-mount conformal gun pod packages, two in the dorsal turret, one each in the pair of waist positions, and a pair in the tail – with 14 of the guns either aimed directly forward, or aimed to fire directly forward for strafing missions. Some aircraft had eight 5 in (130 mm) high-velocity aircraft rockets (HVAR). NAA introduced the J-2 into production in alternating blocks at the J-22. Total J series production was 4,318.
Flight characteristics.
The B-25 was a safe and forgiving aircraft to fly. With one engine out, 60° banking turns into the dead engine were possible, and control could be easily maintained down to 145 mph (230 km/h). The pilot had to remember to maintain engine-out directional control at low speeds after takeoff with rudder; if this maneuver was attempted with ailerons, the aircraft could snap out of control. The tricycle landing gear made for excellent visibility while taxiing. The only significant complaint about the B-25 was the extremely high noise level produced by its engines; as a result, many pilots eventually suffered from varying degrees of hearing loss.
The high noise level was due to design and space restrictions in the engine cowlings which resulted in the exhaust "stacks" protruding directly from the cowling ring and partly covered by a small triangular fairing. This arrangement directed exhaust and noise directly at the pilot and crew compartments. Crew members and operators on the air show circuit frequently commented that "the B-25 is the fastest way to turn aviation fuel directly into noise".
Durability.
The Mitchell was an exceptionally sturdy aircraft that could withstand tremendous punishment. One B-25C of the 321st Bomb Group was nicknamed "Patches" because its crew chief painted all the aircraft's flak hole patches with high-visibility zinc chromate primer. By the end of the war, this aircraft had completed over 300 missions, had been belly-landed six times and had over 400 patched holes. The airframe of "Patches" was so distorted from battle damage that straight-and-level flight required 8° of left aileron trim and 6° of right rudder, causing the aircraft to "crab" sideways across the sky.
Post war (USAF) use.
In 1947 legislation created an independent United States Air Force and by that time the B-25 inventory numbered only a few hundred. Some B-25s continued in service into the 1950s in a variety of training, reconnaissance and support roles. The principal use during this period was undergraduate training of multi-engine aircraft pilots slated for reciprocating engine or turboprop cargo, aerial refuelling or reconnaissance aircraft. Others were assigned to units of the Air National Guard in training roles in support of Northrop F-89 Scorpion and Lockheed F-94 Starfire operations. 
In its USAF tenure, many B-25s received the so-called "Hayes modification" and as a result, surviving B-25 often have exhaust system with a semi-collector ring that splits emissions into two different systems. The upper seven cylinders are collected by a ring while the other cylinders remain directed to individual ports.
TB-25J-25-NC Mitchell, "44-30854", the last B-25 in the USAF inventory, assigned at March AFB, California as of March 1960, was flown to Eglin AFB, Florida, from Turner Air Force Base, Georgia, on 21 May 1960, the last flight by a USAF B-25, and presented by Brigadier General A. J. Russell, Commander of SAC's 822d Air Division at Turner AFB, to the Air Proving Ground Center Commander, Brigadier General Robert H. Warren, who in turn presented the bomber to Valparaiso, Florida Mayor Randall Roberts on behalf of the Niceville-Valparaiso Chamber of Commerce. Four of the original Tokyo Raiders were present for the ceremony, Colonel Davy Jones, Colonel Jack Simms, Lieutenant Colonel Joseph Manske, and retired Master Sergeant Edwin W. Horton. It was donated back to the Air Force Armament Museum c. 1974 and marked as Doolittle's "40-2344".
U.S. Navy and USMC.
The U.S. Navy designation for the Mitchell as the PBJ-1 and apart from increased use of radar, it was configured like its Army counterparts series. The designation standing for Patrol (P) Bomber (B) built by North American Aviation (J), first variant (-1) under the existing American naval aircraft designation system of the era. The PBJ had its origin in an inter-service agreement of mid-1942 between the Navy and the USAAF exchanging the Boeing Renton plant for the Kansas plant for B-29 production. The Boeing XPBB Sea Ranger flying boat competing for B-29 engines, was cancelled in exchange for part of the Kansas City Mitchell production. Other terms included the inter-service transfer of 50 B-25C and 152 B-25D to the Navy. The bombers carried Navy bureau numbers (BuNos), beginning with BuNo 34998. The first PBJ-1 arrived in February 1943 and nearly all reached Marine Corps squadrons, beginning with Marine Bombing Squadron 413 (VMB-413). Following the AAFAC format, the Marine Mitchells had search radar in a retractable radome replacing the ventral turret. Later D and J series had nose mounted APS-3 radar; and later still, J and H series mounted radar in the starboard wingtip. The large quantities of B-25H and J series became known as PBJ-1H and PBJ-1J respectively. These aircraft often operated along with earlier PBJ series in Marine squadrons.
The PBJs were operated almost exclusively by the Marine Corps as land-based bombers. To operate them, the U.S. Marine Corps established a number of Marine bomber squadrons (VMB), beginning with VMB-413, in March 1943 at MCAS Cherry Point, North Carolina. Eight VMB squadrons were flying PBJs by the end of 1943, forming the initial Marine medium bombardment group. Four more squadrons were in the process of formation in late 1945, but had not yet deployed by the time the war ended.
Operational use of the Marine Corps PBJ-1s began in March 1944. The Marine PBJs operated from the Philippines, Saipan, Iwo Jima and Okinawa during the last few months of the Pacific war. Their primary mission was the long range interdiction of enemy shipping trying to run the blockade which was strangling Japan. The weapon of choice during these missions was usually the five-inch HVAR rocket, eight of which could be carried. Some VMB-612 intruder PBJ-1D and J series flew without top turrets to save weight and increase range on night patrols, especially towards the end of the war when air superiority existed.
During the war the Navy tested the cannon-armed G series and conducted carrier trial with an H equipped with arresting gear. After World War II, some PBJs stationed at the Navy's rocket laboratory in Inyokern, California, the present-day Naval Air Weapons Station China Lake, tested various air-to-ground rockets and arrangements. One arrangement was a twin-barrel nose arrangement that could fire 10 spin-stabilized five-inch rockets in one salvo.
Royal Air Force.
The Royal Air Force (RAF) was an early customer for the B-25 via Lend-Lease. The first Mitchells were given the service name Mitchell I by the RAF and were delivered in August 1941, to No. 111 Operational Training Unit based in the Bahamas. These bombers were used exclusively for training and familiarization and never achieved operational status. The B-25Cs and Ds were designated Mitchell II. Altogether, 167 B-25Cs and 371 B-25Ds were delivered to the RAF. The RAF tested the cannon-armed G series but did not adopt the series nor the follow on H series.
By the end of 1942 the RAF had taken delivery of a total of 93 Mitchell marks I and II. Some served with squadrons of No. 2 Group RAF, the RAF's tactical medium bomber force. The first RAF operation with the Mitchell II took place on 22 January 1943, when six aircraft from No. 180 Squadron RAF attacked oil installations at Ghent. After the invasion of Europe (by which point 2 Group was part of Second Tactical Air Force), all four Mitchell squadrons moved to bases in France and Belgium (Melsbroek) to support Allied ground forces. The British Mitchell squadrons were joined by No. 342 (Lorraine) Squadron of the French Air Force in April 1945.
As part of its move from Bomber Command, No 305 (Polish) Squadron flew Mitchell IIs from September to December 1943 before converting to the de Havilland Mosquito. In addition to No. 2 Group, the B-25 was used by various second-line RAF units in the UK and abroad. In the Far East, No. 3 PRU, which consisted of Nos. 681 and 684 Squadrons, flew the Mitchell (primarily Mk IIs) on photographic reconnaissance sorties.
The RAF was allocated 316 B-25J which entered service as the Mitchell III. Deliveries took place between August 1944 and August 1945. However, only about 240 of these bombers actually reached Britain, with some being diverted to No. 111 OTU in the Bahamas, some crashing during delivery and some being retained in the United States.
Royal Canadian Air Force.
The Royal Canadian Air Force (RCAF) used the B-25 Mitchell for training during the war. Post-war use saw continued operations with most of 162 Mitchells received. The first B-25s had originally been diverted to Canada from RAF orders. These included one Mitchell I, 42 Mitchell IIs, and 19 Mitchell IIIs. No 13 (P) Squadron was formed unofficially at RCAF Rockcliffe in May 1944 and used Mitchell IIs on high-altitude aerial photography sorties. No. 5 OTU (Operational Training Unit) at Boundary Bay, British Columbia and Abbotsford, British Columbia, operated the B-25D Mitchell in the training role together with B-24 Liberators for Heavy Conversion as part of the BCATP. The RCAF retained the Mitchell until October 1963.
No 418 (Auxiliary) Squadron received its first Mitchell IIs in January 1947. It was followed by No 406 (auxiliary), which flew Mitchell IIs and IIIs from April 1947 to June 1958. No 418 Operated a mix of IIs and IIIs until March 1958. No 12 Squadron of Air Transport Command also flew Mitchell IIIs along with other types from September 1956 to November 1960. In 1951, the RCAF received an additional 75 B-25Js from USAF stocks to make up for attrition and to equip various second-line units.
Royal Australian Air Force.
The Australians received Mitchells by the spring of 1944. The joint Australian-Dutch No. 18 (Netherlands East Indies) Squadron RAAF had more than enough Mitchells for one squadron, so the surplus went to re-equip the RAAF's No. 2 Squadron, replacing their Beauforts.
Dutch Air Force.
During World War II, the Mitchell served in fairly large numbers with the Air Force of the Dutch government-in-exile. They participated in combat in the East Indies as well as on the European front. On 30 June 1941, the Netherlands Purchasing Commission, acting on behalf of the Dutch government-in-exile in London, signed a contract with North American Aviation for 162 B-25C aircraft. The bombers were to be delivered to the Netherlands East Indies to help deter any Japanese aggression into the region.
In February 1942, the British Overseas Airways Corporation (BOAC) agreed to ferry 20 Dutch B-25s from Florida to Australia travelling via Africa and India, and an additional ten via the South Pacific route from California. During March, five of the bombers on the Dutch order had reached Bangalore, India and 12 had reached Archerfield in Australia. It was agreed that the B-25s in Australia would be used as the nucleus of a new squadron, designated No. 18. This squadron was staffed jointly by Australian and Dutch aircrews plus a smattering of aircrews from other nations, and operated - at least initially - under Royal Australian Air Force command.
The B-25s of No. 18 Squadron were painted with the Dutch national insignia (at this time a rectangular Netherlands flag) and carried NEIAF serials. Discounting the ten "temporary" B-25s delivered to 18 Squadron in early 1942, a total of 150 Mitchells were taken on strength by the NEIAF, 19 in 1942, 16 in 1943, 87 in 1944, and 28 in 1945. They flew bombing raids against Japanese targets in the East Indies. In 1944, the more capable B-25J Mitchell replaced most of the earlier C and D models.
In June 1940, No. 320 Squadron RAF had been formed from personnel formerly serving with the Royal Dutch Naval Air Service who had escaped to England after the German occupation of the Netherlands. Equipped with various British aircraft, No. 320 Squadron flew anti-submarine patrols, convoy escort missions, and performed air-sea rescue duties. They acquired the Mitchell II in September 1943, performing operations over Europe against gun emplacements, railway yards, bridges, troops and other tactical targets. They moved to Belgium in October 1944, and transitioned to the Mitchell III in 1945. No. 320 Squadron was disbanded in August 1945. Following the war, B-25s were used in Indonesia.
Soviet Air Force.
The U.S. supplied 862 B-25s (B, D, G, and J types) to the Soviet Union under Lend-Lease during World War II via the Alaska–Siberia ALSIB ferry route.
Other damaged aircraft arrived or crashed in the Far East of Russia, and one Doolittle Raid aircraft landed there short of fuel after attacking Japan. The lone airworthy aircraft to reach the Soviet Union was lost in a hangar fire in the early 50s while undergoing routine maintenance. In general, the B-25 was operated as a ground-support and tactical daylight bomber (as similar Douglas A-20 Havocs were used). It saw action in fights from Stalingrad (with B/D models) to the German surrender during May 1945 (with G/J types).
B-25s that remained in Soviet Air Force service after the war were assigned the NATO reporting name "Bank".
China.
Well over 100 B-25Cs and Ds were supplied to the Nationalist Chinese during the Second Sino-Japanese War. In addition, a total of 131 B-25Js were supplied to China under Lend-Lease.
The four squadrons of the 1st BG (1st, 2nd, 3rd, and 4th) of the 1st Medium Bomber Group were formed during the war. They formerly operated Russian-built Tupolev SB bombers, then transferred to the B-25. The 1st BG was under the command of CACW (Chinese-American Composite Wing) while operating B-25s. Following the end of the war in the Pacific, these four bombardment squadrons were established to fight against the Communist insurgency that was rapidly spreading throughout the country. During the Chinese Civil War, Chinese Mitchells fought alongside de Havilland Mosquitos.
In December 1948, the Nationalists were forced to retreat to the island of Taiwan, taking many of their Mitchells with them. However, some B-25s were left behind and were impressed into service with the air force of the new People's Republic of China.
Brazilian Air Force.
During the war, the Força Aérea Brasileira (FAB) received a few B-25s under Lend-Lease. Brazil declared war against the Axis powers in August 1942 and participated in the war against the U-boats in the southern Atlantic. The last Brazilian B-25 was finally declared surplus in 1970.
Free French.
The Royal Air Force issued at least 21 Mitchell IIIs to No 342 Squadron, which was made up primarily of Free French aircrews. Following the liberation of France, this squadron transferred to the newly formed French Air Force ("Armée de l'Air") as GB I/20 Lorraine. The aircraft continued in operation after the war, with some being converted into fast VIP transports. They were struck off charge in June 1947.
Variants.
Trainer variants.
Most models of the B-25 were used at some point as training aircraft.
Survivors.
Today, many B-25s are kept in airworthy condition by air museums and collectors.
At 9:40 on Saturday, 28 July 1945, a USAAF B-25D crashed in thick fog into the north side of the Empire State Building between the 79th and 80th floors. Fourteen people died — 11 in the building and the three occupants of the aircraft, including the pilot, Colonel William F. Smith. Betty Lou Oliver, an elevator attendant, survived the impact and a subsequent uncontrolled descent in the elevator.

</doc>
<doc id="4219" url="https://en.wikipedia.org/wiki?curid=4219" title="British Open (disambiguation)">
British Open (disambiguation)

The British Open is the Open Championship men's golf tournament.
British Open may also refer to:

</doc>
<doc id="4224" url="https://en.wikipedia.org/wiki?curid=4224" title="Bobby Charlton">
Bobby Charlton

Sir Robert "Bobby" Charlton CBE (born 11 October 1937) is an English former football player, regarded as one of the greatest midfielders of all time, and an essential member of the England team who won the World Cup in 1966, the year he also won the Ballon d'Or. He played almost all of his club football at Manchester United, where he became renowned for his attacking instincts and passing abilities from midfield and his ferocious long-range shot. He was also well known for his fitness and stamina. He was cautioned only twice in his career; once against Argentina in the 1966 World Cup, and once in a league match against Chelsea. His elder brother Jack, who was also in the World Cup-winning team, is a former defender for Leeds United and international manager.
Born in Ashington, Northumberland, Charlton made his debut for the Manchester United first-team in 1956, and over the next two seasons gained a regular place in the team, during which time he survived the Munich air disaster of 1958 after being rescued by Harry Gregg. After helping United to win the Football League in 1965, he won a World Cup medal with England in 1966 and another Football League title with United the following year. In 1968, he captained the Manchester United team that won the European Cup, scoring two goals in the final to help his team be the first English side to win the competition. He has scored more goals for United (249) than any other player and held the distinction of being England's all-time top goal scorer (49) from May 1968 to September 2015, when Wayne Rooney beat his England goal scoring record. Charlton held the record for most appearances for Manchester United (758), before being surpassed by Ryan Giggs.
He was selected for four World Cups (1958, 1962, 1966, and 1970), and helped England to win the competition in 1966. At the time of his retirement from the England team in 1970, he was the nation's most capped player, having turned out 106 times at the highest level. This record has since been held by Bobby Moore and Peter Shilton.
He left Manchester United to become manager of Preston North End for the 1973–74 season. He changed to player-manager the following season. He next accepted a post as a director with Wigan Athletic, then became a member of Manchester United's board of directors in 1984 and remains one as of the 2015/16 season.
Early life.
Charlton is related to several professional footballers on his mother's side of the family: his uncles were Jack Milburn (Leeds United and Bradford City), George Milburn (Leeds United and Chesterfield), Jim Milburn (Leeds United and Bradford City) and Stan Milburn (Chesterfield, Leicester City and Rochdale), and legendary Newcastle United and England footballer Jackie Milburn, was his mother's cousin. However, Charlton credits much of the early development of his career to his grandfather Tanner and his mother Cissie. His elder brother, Jack, initially went to work applying to the Police Service before also becoming a professional footballer with Leeds United.
Club career.
On 9 February 1953, then a Bedlington Grammar School pupil, Charlton was spotted playing for East Northumberland schools by Manchester United chief scout Joe Armstrong. Charlton went on to play for England schoolboys and the 15-year-old signed with United on 1 January 1953, along with Wilf McGuinness, also aged 15. Initially his mother was reluctant to let him commit to an insecure football career, so he began an apprenticeship as an electrical engineer; however he went on to turn professional in October 1954.
Charlton became one of the famed Busby Babes, the collection of talented footballers who emerged through the system at Old Trafford in the 1940s, 1950s and 1960s as Matt Busby set about a long-term plan of rebuilding the club after the Second World War. He worked his way through the pecking order of teams, scoring regularly for the youth and reserve sides before he was handed his first team debut against Charlton Athletic in October 1956. At the same time, he was doing his National service with the Royal Army Ordnance Corps in Shrewsbury, where Busby had advised him to apply as it meant he could still play for Manchester United at the weekend. Also doing his army service in Shrewsbury at the same time was his United team-mate Duncan Edwards.
Charlton played 14 times for United in that first season, scoring twice on his debut and managing a total of 12 goals in all competitions, and including a hat-trick in a 5–1 away win over Charlton Athletic in the February. United won the league championship but were denied the 20th century's first "double" when they controversially lost the 1957 FA Cup Final to Aston Villa. Charlton, still only 19, was selected for the game, which saw United goalkeeper Ray Wood carried off with a broken cheekbone after a clash with Villa centre forward Peter McParland. Though Charlton was a candidate to go in goal to replace Wood (in the days before substitutes, and certainly before goalkeeping substitutes), it was teammate Jackie Blanchflower who ended up between the posts.
Charlton was an established player by the time the next season was fully underway, which saw United, as current League champions, become the first English team to compete in the European Cup. Previously, the Football Association had scorned the competition but United made progress, reaching the semi-finals where they lost to holders Real Madrid. Their reputation was further enhanced the next season as they reached the quarter finals to play Red Star Belgrade. In the first leg at home, United won 2–1. The return in Yugoslavia saw Charlton score twice as United stormed 3–0 ahead, although the hosts came back to earn a 3–3 draw. However, United maintained their aggregate lead to reach the last four and were in jubilant mood as they left to catch their flight home, thinking of an important League game against Wolves at the weekend.
Munich air disaster.
The aeroplane which took the United players and staff home from Zemun Airport needed to stop in Munich to refuel. This was carried out in worsening weather, and by the time the refuelling was complete and the call was made for the passengers to re-board the aircraft, the wintry showers had taken hold and snow had settled heavily on the runway and around the airport. There were two aborted take-offs which led to concern on board, and the passengers were advised by a stewardess to disembark again while a minor technical error was fixed.
The team was back in the airport terminal barely ten minutes when the call to reconvene on the plane came, and a number of passengers began to feel nervous. Charlton and teammate Dennis Viollet swapped places with Tommy Taylor and David Pegg, who had decided they would be safer at the back of the plane.
The plane clipped the fence at the end of the runway on its next take-off attempt and a wing tore through a nearby house, setting it alight. The wing and part of the tail came off and hit a tree and a wooden hut, the plane spinning along the snow until coming to a halt. It had been cut in half.
Charlton, strapped into his seat, had fallen out of the cabin and when United goalkeeper Harry Gregg (who had somehow got through a hole in the plane unscathed and begun a one-man rescue mission) found him, he thought he was dead. That said, he grabbed both Charlton and Viollet by their trouser waistbands and dragged them away from the plane in constant fear that it would explode. Gregg returned to the plane to try to help the appallingly injured Busby and Blanchflower, and when he turned around again, he was relieved to see that Charlton and Viollet, both of whom he had presumed to be dead, had got out of their detached seats and were looking into the wreckage.
Charlton suffered cuts to his head and severe shock and was in hospital for a week. Seven of his teammates had perished at the scene, including Taylor and Pegg, with whom he and Viollet had swapped seats prior to the fatal take-off attempt. Club captain Roger Byrne was also killed, along with Mark Jones, Billy Whelan, Eddie Colman and Geoff Bent. Duncan Edwards died a fortnight later from the injuries he had sustained. In total, the crash claimed 23 lives. Initially, ice on the wings was blamed, but a later inquiry declared that slush on the runway had made a safe take-off almost impossible.
Of the 44 passengers and crew (including the 17-strong Manchester United squad), 23 people (eight of them Manchester United players) died as a result of their injures in the crash. Charlton survived with minor injuries. Of the eight other players who survived, two of them were injured so badly that they never played again.
Charlton was the first injured survivor to leave hospital. Harry Gregg and Bill Foulkes were not hospitalized since they escaped uninjured. He arrived back in England on 14 February 1958, eight days after the crash. As he convalesced with family in Ashington, he spent some time kicking a ball around with local youths, and a famous photograph of him was taken. He was still only 20 years old, yet now there was an expectation that he would help with the rebuilding of the club as Busby's aides tried to piece together what remained of the season.
Resuming his career.
Charlton returned to playing in an FA cup tie against West Bromwich Albion on 1 March; the game was a draw and United won the replay 1–0. Not unexpectedly, United went out of the European Cup to Milan in the semi-finals to a 5–2 aggregate defeat and fell behind in the League. Yet somehow they reached their second consecutive FA Cup final, and the big day at Wembley coincided with Busby's return to work. However, his words could not inspire a side which was playing on a nation's goodwill and sentiment, and Nat Lofthouse scored twice to give Bolton Wanderers side a 2–0 win.
Further success with Manchester United came at last when they beat Leicester City 3–1 in the FA Cup final of 1963, with Charlton finally earning a winners' medal in his third final. Busby's post-Munich rebuilding programme continued to progress with two League championships within three seasons, with United taking the title in 1965 and 1967. A successful (though trophyless) season with Manchester United had seen him take the honours of "Football Writers' Association Footballer of the Year" and "European Footballer Of The Year" into the competition.
In 1968, Manchester United reached the European Cup final, ten seasons after Munich. Even though other clubs had taken part in the competition in the intervening decade, the team which got to this final was still the first English side to do so. On a highly emotional night at Wembley, Charlton scored twice in a 4–1 win after extra time against Benfica and, as United captain, lifted the trophy.
During the early 1970s, Manchester United were no longer competing among the top teams in England, and at several stages were battling against relegation. At times, Charlton was not on speaking terms with United's other superstars George Best and Denis Law, and Best refused to play in Charlton's testimonial match against Celtic, saying that "to do so would be hypocritical". Charlton left Manchester United at the end of the 1972–73 season, having scored 249 goals and set a club record of 758 appearances, a record which Ryan Giggs broke in the 2008 UEFA Champions League Final.
His last game was against Chelsea at Stamford Bridge on 28 April 1973, and before the game the BBC cameras for "Match of the Day" captured the Chelsea chairman handing Charlton a commemorative cigarette case. The match ended in a 1-0 defeat. His final goal came a month earlier, on 31 March, in a 2-0 win at Southampton, also in the First Division.
International career.
Charlton's emergence as the country's leading young football talent was completed when he was called up to join the England squad for a British Home Championship game against Scotland at Hampden Park on 19 April 1958, just over two months after he had survived the Munich air disaster.
Charlton was handed his debut as England romped home 4–0, with the new player gaining even more admirers after scoring a magnificent thumping volley dispatched with authority after a cross by the left winger Tom Finney. He scored both goals in his second game as England beat Portugal 2–1 in a friendly at Wembley; and overcame obvious nerves on a return to Belgrade to play his third match against Yugoslavia. England lost that game 5–0 and Charlton played poorly.
1958 World Cup.
He was selected for the squad which competed at the 1958 World Cup in Sweden, but did not kick a ball, something at which critics expressed surprise and bewilderment, even allowing for his lacklustre performance in Belgrade.
In 1959 he scored a hat-trick as England demolished the US 8–1; and his second England hat-trick came in 1961 in an 8–0 thrashing of Mexico. He also managed to score in every British Home Championship tournament he played in except 1963 in an association with the tournament which lasted from 1958 to 1970 and included 16 goals and ten tournament victories (five shared).
1962 World Cup.
He played in qualifiers for the 1962 World Cup in Chile against Luxembourg and Portugal and was named in the squad for the finals themselves. His goal in the 3–1 group win over Argentina was his 25th for England in just 38 appearances, and he was still only 24 years old, but his individual success could not be replicated by that of the team, which was eliminated in the quarter final by Brazil, who went on to win the tournament.
By now, England were coached by Alf Ramsey who had managed to gain sole control of the recruitment and team selection procedure from the committee-based call-up system which had lasted up to the previous World Cup. Ramsey had already cleared out some of the older players who had been reliant on the loyalty of the committee for their continued selection – it was well known that decorum on the pitch at club level had been just as big a factor in playing for England as ability and form. Luckily for Charlton, he had all three.
A hat-trick in the 8–1 rout of Switzerland in June 1963 took Charlton's England goal tally to 30, equalling the record jointly held by Tom Finney and Nat Lofthouse and Charlton's 31st goal against Wales in October the same year gave him the record alone.
Charlton's role was developing from traditional inside-forward to what today would be termed an attacking midfield player, with Ramsey planning to build the team for the 1966 World Cup around him. When England beat the USA 10-0 in a friendly on 27 May 1964, he scored one goal, his 33rd at senior level for England.
His goals became a little less frequent, and indeed Jimmy Greaves, playing purely as a striker, would overtake Charlton's England tally in October 1964. Nevertheless, he was still scoring and creating freely and as the tournament was about to start, he was expected to become one of its stars and galvanise his established reputation as one of the world's best footballers.
1966 World Cup.
England drew the opening game of the tournament 0–0 with Uruguay, and Charlton scored the first goal in the 2–0 win over Mexico. This was followed by an identical scoreline against France, allowing England to qualify for the quarter finals.
England defeated Argentina 1–0 – the game was the only international match in which Charlton received a caution – and faced Portugal in the semi finals. This turned out to be one of Charlton's most important games for England.
Charlton opened the scoring with a crisp side-footed finish after a run by Roger Hunt had forced the Portuguese goalkeeper out of his net; his second was a sweetly struck shot after a run and pull-back from Geoff Hurst. Charlton and Hunt were now England's joint-highest scorers in the tournament with three each, and a final against West Germany beckoned.
The final turned out to be one of Charlton's quieter days; he and a young Franz Beckenbauer effectively marked each other out of the game. England won 4–2 after extra time.
Euro 1968.
Charlton's next England game was his 75th as England beat Northern Ireland; 2 caps later and he had become England's second most-capped player, behind the veteran Billy Wright, who was approaching his 100th appearance when Charlton was starting out and ended with 105 caps.
Weeks later he scored his 45th England goal in a friendly against Sweden, breaking the record of 44 set the previous year by Jimmy Greaves. He was then in the England team which made it to the semi-finals of the 1968 European Championships where they were knocked out by Yugoslavia in Florence. During the match Charlton struck a Yugoslav post. England defeated the Soviet Union 2–0 in the third place match.
In 1969, Charlton was appointed an OBE for services to football. More milestones followed as he won his 100th England cap on 21 April 1970 against Northern Ireland, and was made captain by Ramsey for the occasion. Inevitably, he scored. This was his 48th goal for his country – his 49th and final goal would follow a month later in a 4–0 win over Colombia during a warm-up tour for the 1970 World Cup, designed to get the players adapted to altitude conditions. Charlton's inevitable selection by Ramsey for the tournament made him the first – and still, to date, only – England player to feature in four World Cup squads.
1970 World Cup.
Shortly before the World Cup Charlton was involved in the Bogotá Bracelet incident in which he and Bobby Moore were accused of stealing a bracelet from a jewellery store. Moore was later arrested and detained for four days before being granted a conditional release, while Charlton was not arrested.
England began the tournament with two victories in the group stages, plus a memorable defeat against Brazil. Charlton played in all three, though was substituted for Alan Ball in the final game of the group against Czechoslovakia. Ramsey, confident of victory and progress to the quarter final, wanted Charlton to rest.
England duly reached the last eight where they again faced West Germany. Charlton controlled the midfield and suppressed Franz Beckenbauer's runs from deep as England coasted to a 2–0 lead. Beckenbauer pulled a goal back for the Germans and Ramsey replaced the ageing and tired Charlton with Colin Bell who further tested the German keeper Maier and also provided a great cross for Geoff Hurst who uncharacteristically squandered the chance. West Germany, who had a habit of coming back from behind, eventually scored twice – a back header from Uwe Seeler made it 2–2. In extra-time, Geoff Hurst had a goal mysteriously ruled out after which Gerd Müller's goal won the match 3–2. England were out and, after a record 106 caps and 49 goals, Charlton decided to end his international career at the age of 32. On the flight home from Mexico, he asked Ramsey not to consider him again. His brother Jack, two years his senior but 71 caps his junior, did likewise.
Despite popular opinion the substitution did not change the game as Franz Beckenbauer had scored before Charlton left the field, hence Charlton had failed to cancel out the German. Charlton himself conceded that the substitution did not affect the game in a BBC documentary. His caps record lasted until 1973 when Bobby Moore overtook him, and Charlton currently lies seventh in the all-time England appearances list behind Moore, Wayne Rooney, Ashley Cole, Steven Gerrard, David Beckham and Peter Shilton, whose own England career began in the first game after Charlton's had ended. Charlton's goalscoring record was surpassed by Wayne Rooney on 8 September 2015, when Rooney scored a penalty in a 2–0 win over Switzerland in a qualifying match for UEFA Euro 2016.
Management career and directorships.
Charlton became the manager of Preston North End in 1973, signing his former United and England team-mate Nobby Stiles as player-coach. His first season ended in relegation and although he began playing again he left Preston early in the 1975–76 season after a disagreement with the board over the transfer of John Bird to Newcastle United. He was awarded the CBE that year and began a casual association with the BBC for punditry on matches which continued for many years. In early 1976, he scored once in 3 league appearances for Waterford United.
He joined Wigan Athletic as a director, and was briefly caretaker manager there in 1983. He then spent some time playing in South Africa. He also built up several businesses in areas such as travel, jewellery and hampers, and ran soccer schools in the UK, the US, Canada, Australia and China. In 1984, he was invited to become member of the board of directors at Manchester United, partly because of his football knowledge and partly because it was felt that the club needed a "name" on the board after the resignation of Sir Matt Busby. He remains a director of Manchester United as of 2014 and his continued presence was a factor in placating many fans opposed to the club's takeover by Malcolm Glazer.
Personal life and retirement.
He met his wife, Norma Ball, at an ice rink in Manchester in 1959 and they married in 1961. They have two daughters – Suzanne and Andrea. Suzanne was a weather forecaster for the BBC during the 1990s. They now have grandchildren, including Suzanne's son Robert, who is named in honour of his grandfather.
In 2007, while publicising his forthcoming autobiography, Charlton revealed that he had a long-running feud with his brother, Jack. They have rarely spoken since a falling-out between his wife Norma and his mother Cissie (who died on 25 March 1996 at the age of 83). Charlton did not see his mother after 1992 as a result of the feud.
Jack presented him with his BBC Sports Personality of the Year Lifetime Achievement Award on 14 December 2008. He said that he was 'knocked out' as he was presented the award by his brother. He received a standing ovation as he stood waiting for his prize.
Charlton helped to promote Manchester's bids for the 1996 and 2000 Olympic Games and the 2002 Commonwealth Games, England's bid for the 2006 FIFA World Cup and London's successful bid for the 2012 Summer Olympics. He received a knighthood in 1994 and was an Inaugural Inductee to the English Football Hall of Fame in 2002. On accepting his award he commented "I'm really proud to be included in the National Football Museum's Hall of Fame. It's a great honour. If you look at the names included I have to say I couldn't argue with them. They are all great players and people I would love to have played with." He is also the (honorary) president of the National Football Museum, an organisation about which he said "I can't think of a better Museum anywhere in the world." On 14 December 2008 Charlton was awarded the prestigious BBC Sports Personality of the Year Lifetime Achievement Award.
On 2 March 2009, Charlton was given the freedom of the city of Manchester, stating "I'm just so proud, it's fantastic. It's a great city. I have always been very proud of it."
Charlton is involved in a number of charitable activities including fund raising for cancer hospitals. Charlton became involved in the cause of land mine clearance after visits to Bosnia and Cambodia and supports the Mines Advisory Group as well as founding his own charity Find a Better Way which funds research into improved civilian landmine clearance.
In January 2011 Charlton was voted the 4th greatest Manchester United player of all time by the readers of Inside United and ManUtd.com, behind Ryan Giggs (who topped the poll), Eric Cantona and George Best.
He is a member of the Laureus World Sports Academy. On 6 February 2012 Sir Bobby Charlton was taken to hospital after falling ill, and subsequently had a gallstone removed. This prevented him from collecting a Lifetime Achievement award at the Laureus World Sports Awards.
On 15 February 2016 Manchester United announced the South Stand of Old Trafford would be renamed in honour of Sir Bobby Charlton. The unveiling will take place at the home game against Everton on 2 April 2016.
Honours.
Club.
ref:
National.
ref:
Individual.
ref:

</doc>
<doc id="4227" url="https://en.wikipedia.org/wiki?curid=4227" title="Barry Lyndon">
Barry Lyndon

Barry Lyndon is a 1975 British-American period drama film written, produced, and directed by Stanley Kubrick, based on the 1844 novel "The Luck of Barry Lyndon" by William Makepeace Thackeray. It stars Ryan O'Neal, Marisa Berenson, Patrick Magee, and Hardy Krüger. The film recounts the exploits of a fictional 18th-century Irish adventurer. Exteriors were shot on location in Ireland, England and Germany. 
At the 1975 Academy Awards, the film won four Oscars in production categories. Although having had a modest commercial success and a mixed reception from critics on release, "Barry Lyndon" is today regarded as one of Kubrick's finest films. In numerous polls, including those of "Village Voice" (1999), "Sight & Sound" (2002), "Time" (2005) and BBC, it has been named one of the greatest films ever made.
Plot.
Act I.
An omniscient (though possibly unreliable) narrator relates that in 1750s Ireland, the father of Redmond Barry is killed in a duel over a sale of some horses. The widow, disdaining offers of marriage, devotes herself to her only son.
As a teenager, Barry becomes strongly attached to his older cousin, Nora Brady. Though she charms him during a card game, she later shows interest in a well-off British Army captain, John Quin, much to Barry's dismay. Nora and her family plan to leverage their position through marriage, while Barry holds Quin in contempt and shoots him in a duel. Barry heads through the countryside towards Dublin, but is robbed of purse pistol and sword by Captain Feeney, an infamous highwayman. Dejected, Barry enlists in the British army after hearing a promotional spiel, where he encounters Captain Grogan, a family friend. Grogan informs him that he did not in fact kill Quin—Barry's dueling pistol was loaded with tow. The duel was staged by Nora's family to be rid of Barry so that their finances would be secured through the impending marriage.
Barry’s regiment is sent to Germany to fight in the Seven Years' War, where Captain Grogan is fatally wounded by the French in a skirmish at the Battle of Minden. Barry deserts the army, stealing an officer courier's uniform, horse, and identification papers. En route to neutral Holland he encounters the Prussian Captain Potzdorf, who, seeing through his disguise, offers him the choice of being turned back over to the British where he will be shot as a deserter, or enlisting in the Prussian Army. Barry enlists in his second army and later receives a special commendation from Frederick the Great for saving Potzdorf's life in a battle.
After the war ends in 1763, Barry is employed by Captain Potzdorf's uncle in the Prussian Ministry of Police to become the servant of the Chevalier de Balibari, a professional gambler. The Prussians suspect he is a spy and send Barry as an undercover agent to verify this. Barry reveals himself to the Chevalier right away and they become confederates in cheating at cards. After he and the Chevalier cheat the Prince of Tübingen at the card table, the Prince accuses the Chevalier (without proof) and refuses to pay his debt and demands satisfaction. When Barry relays this to his Prussian handlers, they (still suspecting that the Chevalier is a spy) are wary of allowing another meeting between the Chevalier and the Prince. So, the Prussians arrange for the Chevalier to be expelled from the country. Barry conveys this plan to the Chevalier, who flees in the night. The next morning, Barry, under disguise as the Chevalier, is escorted from Prussian territory by Prussian officers.
For the next few years, Barry and the Chevalier travel the spas and parlors of Europe, profiting from their gambling with Barry forcing payment from reluctant debtors with duels. Seeing that his life is going nowhere, Barry decides to marry into wealth. At a gambling table in Spa, he encounters the beautiful and wealthy Countess of Lyndon (Marisa Berenson). He seduces and later marries her after the death of her elderly husband, Sir Charles Lyndon (Frank Middlemass).
Act II.
In 1773, Barry takes the Countess' last name in marriage and settles in England to enjoy her wealth, still with no money of his own. Lord Bullingdon, Lady Lyndon's 10-year-old son by Sir Charles, does not approve of the marriage and quickly comes to hate Barry, calling him a "common opportunist" who does not love his mother. Barry retaliates by subjecting Bullingdon to systematic physical abuse. 
The Countess bears Barry a son, Bryan Patrick, but the marriage is unhappy: Barry is openly unfaithful and enjoys spending his wife's money in self-indulgent spending sprees while keeping his wife in seclusion.
Some years later, Barry's mother comes to live with him at the Lyndon estate. She warns her son that if Lady Lyndon were to die, all her wealth would go to her first-born son Lord Bullingdon, leaving Barry and Patrick penniless. Barry's mother advises him to obtain a noble title to protect himself. To further this goal, he cultivates the acquaintance of the influential Lord Wendover and begins to expend even larger sums of money to ingratiate himself to high society. All this effort is wasted, however, during a birthday party for Lady Lyndon. A now adult Lord Bullingdon publicly lists the reasons for his hatred of his stepfather and his intention to leave the family estate for as long as Barry remains alive. Seething with hatred, Barry savagely assaults Bullingdon until he is pulled off him by the guests. This loses Barry all the powerful friends he has worked so hard to make and he is cast out of polite society. Bullingdon leaves the estate and England itself for parts unknown.
In contrast to his mistreatment of his stepson, Barry proves an overindulgent and doting father to Bryan, with whom he spends all his time after Bullingdon's departure. He cannot refuse his son anything, and succumbs to Bryan's insistence on receiving a full-grown horse for his ninth birthday. The spoiled Bryan disobeys his parents' direct instructions that Bryan ride the horse only in the presence of his father, is thrown by the horse, and dies a few days later from his injuries.
The grief-stricken Barry turns to alcohol, while Lady Lyndon seeks solace in religion, assisted by the Reverend Samuel Runt, who had been tutor first to Lord Bullingdon and then to Bryan. Left in charge of the families' affairs while Barry and Lady Lyndon grieve, Barry's mother dismisses the Reverend, both because the family no longer needs (nor can afford, due to Barry's spending debts) a tutor and for fear that his influence worsens Lady Lyndon's condition. Plunging even deeper into grief, Lady Lyndon later attempts suicide (though she ingests only enough poison to make herself ill). The Reverend and the family's accountant and emissary Graham then seek out Lord Bullingdon. Upon hearing of these events, Lord Bullingdon returns to England where he finds Barry drunk in a gentlemen's club, mourning the loss of his son rather than being with Lady Lyndon. Bullingdon demands satisfaction for Barry's public assault, challenging him to a duel.
The duel with pistols is held in a tithe barn. A coin-toss gives Bullingdon the right of first fire, but his pistol misfires as it is being cocked. Barry, reluctant to shoot Bullingdon, magnanimously fires into the ground, but the unmoved Bullingdon refuses to let the duel end, claiming he has not received "satisfaction". In the second round, Bullingdon shoots Barry in his left leg. At a nearby inn, a surgeon informs Barry that the leg will need to be amputated below the knee if he is to survive.
While Barry is recovering, Bullingdon takes control of the estate. He sends a very nervous Graham to the inn with a proposition: Lord Bullingdon will grant Barry an annuity of 500 guineas per year on the condition that he leave England forever and separate from Lady Lyndon. Otherwise, with his credit and bank accounts exhausted, Barry's creditors and bill collectors will assuredly see that he is jailed. Defeated, Barry accepts. 
The narrator states that Barry went first to Ireland with his mother, then to the European continent to resume his former profession of gambler (though without his former success) and that he never saw Lady Lyndon again. The final scene (set in December 1789) shows a middle-aged Lady Lyndon signing Barry's annuity cheque as her son looks on.
Critic Tim Robey suggests that the film "makes you realise that the most undervalued aspect of Kubrick's genius could well be his way with actors." He adds that the supporting cast is a "glittering procession of cameos, not from star names but from vital character players."
Cast.
The cast featured Leon Vitali as the older Lord Bullingdon, who would then become Kubrick's personal assistant, working as the casting director on his following films, and supervising film-to-video transfers for Kubrick. Their relationship lasted until Kubrick's death. The film's cinematographer, John Alcott, appears at the men's club in the non-speaking role of the man asleep in a chair near the title character when Lord Bullingdon challenges Barry to a duel. Kubrick's daughter Vivian also appears (in an uncredited role) as a guest at Bryan's birthday party.
Kubrick stalwarts Patrick Magee (who had played the handicapped writer in "A Clockwork Orange") and Philip Stone (who had played Alex's father in "A Clockwork Orange", and would go on to play the dead caretaker Grady in "The Shining") are featured as the Chevalier du Balibari and as Graham, respectively.
Production.
Development.
After "", Kubrick made plans for a film about Napoleon Bonaparte. During pre-production, however, Sergei Bondarchuk and Dino De Laurentiis' "Waterloo" was released and subsequently failed at the box office. As a result, Kubrick's financiers pulled their funding for the film and he turned his attention to his next film, "A Clockwork Orange". Subsequently, Kubrick showed an interest in Thackeray's "Vanity Fair" but dropped the project when a serialised version for television was produced. He told an interviewer, "At one time, "Vanity Fair" interested me as a possible film but, in the end, I decided the story could not be successfully compressed into the relatively short time-span of a feature film...as soon as I read "Barry Lyndon" I became very excited about it."
Having garnered Oscar nominations for "Dr. Strangelove", "2001: A Space Odyssey" and "A Clockwork Orange", Kubrick's reputation in the early 1970s was that of "a perfectionist auteur who loomed larger over his movies than any concept or star." His studio—Warner Bros.—was therefore "eager to bankroll" his next project, which Kubrick kept "shrouded in secrecy" from the press partly due to the furor surrounding the controversially violent "A Clockwork Orange" (particularly in the UK) and partly due to his "long-standing paranoia about the tabloid press."
Having felt compelled to set aside his plans for a film about Napoleon Bonaparte, Kubrick set his sights on Thackeray's 1844 "satirical picaresque about the fortune-hunting of an Irish rogue," "Barry Lyndon", the setting of which allowed Kubrick to take advantage of the copious period research he had done for the now-aborted "Napoleon". At the time, Kubrick merely announced that his next film would star Ryan O'Neal (deemed "a seemingly un-Kubricky choice of leading man") and Marisa Berenson, a former "Vogue" and "Time" magazine cover model, and be shot largely in Ireland. So heightened was the secrecy surrounding the film that "Even Berenson, when Kubrick first approached her, was told only that it was to be an 18th-century costume piece n she was instructed to keep out of the sun in the months before production, to achieve the period-specific pallor he required."
Principal photography.
Principal photography took 300 days, from spring 1973 through early 1974, with a break for Christmas.
Many of the film's exteriors were shot in Ireland, playing "itself, England, and Prussia during the Seven Years' War." Drawing inspiration from "the landscapes of Watteau and Gainsborough," Kubrick and cinematographer Alcott also relied on the "scrupulously researched art direction" of Ken Adam and Roy Walker. Alcott, Adam and Walker would be among those who would win Oscars for their "amazing work" on the film.
Several of the interior scenes were filmed in Powerscourt House, a famous 18th-century mansion in County Wicklow, Republic of Ireland. The house was destroyed in an accidental fire several months after filming (November 1974), so the film serves as a record of the lost interiors, particularly the "Saloon" which was used for more than one scene. The Wicklow Mountains are visible, for example, through the window of the Saloon during a scene set in Berlin. Other locations included Kells Priory (the English Redcoat encampment) Blenheim Palace, Castle Howard (exteriors of the Lyndon estate), Huntington Castle, Clonegal (exterior), Corsham Court (various interiors and the music room scene), Petworth House (chapel, and so on.), Stourhead (lake and temple), Longleat, and Wilton House (interior and exterior) in England, Dunrobin Castle (exterior and garden as Spa) in Scotland, Dublin Castle in Ireland (the chevalier's home), Ludwigsburg Palace near Stuttgart and Frederick the Great's Neues Palais at Potsdam near Berlin (suggesting Berlin's main street Unter den Linden as construction in Potsdam had just begun in 1763). Some exterior shots were also filmed at Waterford Castle (now a luxury hotel and golf course) and Little Island, Waterford. Moorstown Castle in Tipperary also featured. Several scenes were filmed at Castletown House outside Carrick-on-Suir, Co. Tipperary, and at Youghal, Co. Cork.
Cinematography.
The film—as with "almost every Kubrick film"—is a "showcase for major innovation in technique." While "2001: A Space Odyssey" had featured "revolutionary effects," and "The Shining" would later feature heavy use of the Steadicam, "Barry Lyndon" saw a considerable number of sequences shot "without recourse to electric light." Cinematography was overseen by director of photography John Alcott (who won an Oscar for his work), and is particularly noted for the technical innovations that made some of its most spectacular images possible. To achieve photography without electric lighting "or the many densely furnished interior scenes... meant shooting by candlelight," which is known to be difficult in still photography, "let alone with moving images."
Kubrick was "determined not to reproduce the set-bound, artificially lit look of other costume dramas from that time." After "tinkern with different combinations of lenses and film stock," the production got hold of three super-fast 50mm lenses (Carl Zeiss Planar 50mm f/0.7) developed by Zeiss for use by NASA in the Apollo moon landings, which Kubrick had discovered. These super-fast lenses "with their huge aperture (the film actually features the lowest f-stop in film history) and fixed focal length" were problematic to mount, and were extensively modified into three versions by Cinema Products Corp. for Kubrick so to gain a wider angle of view, with input from optics expert Richard Vetter of Todd-AO. The rear element of the lens had to be 2.5mm away from the film plane, requiring special modification to the rotating camera shutter. This allowed Kubrick and Alcott to shoot scenes lit with actual candles to an average lighting volume of only three candela, "recreating the huddle and glow of a pre-electrical age." In addition, Kubrick had the entire film push-developed by one stop.
Although Kubrick's express desire was to avoid electric lighting where possible, most shots were achieved with conventional lenses and lighting, but were lit to deliberately mimic natural light rather than for compositional reasons. In addition to potentially seeming more realistic, these methods also gave a particular period look to the film which has often been likened to 18th-century paintings (which were, of course, depicting a world devoid of electric lighting), in particular owing "a lot to William Hogarth, with whom Thackeray had always been fascinated."
According to critic Tim Robey, the film has a "stately, painterly, often determinedly static quality." For example, to help light some interior scenes, lights were placed outside and aimed through the windows, which were covered in a diffuse material to scatter the light evenly through the room rather than being placed inside for maximum use as most conventional films do. A sign of this method occurs in the scene where Barry duels Lord Bullingdon. Though it appears to be lit entirely with natural light, one can see that the light coming in through the cross-shaped windows in the tithe barn appears blue in color, while the main lighting of the scene coming in from the side is not. This is because the light through the cross-shaped windows is daylight from the sun, which when recorded on the film stock used by Kubrick showed up as blue-tinted compared to the incandescent electric light coming in from the side.
Despite such slight tinting effects, this method of lighting not only gave the look of natural daylight coming in through the windows, but it also protected the historic locations from the damage caused by mounting the lights on walls or ceilings and the heat from the lights. This helped the film "fit... perfectly with Kubrick's gilded-cage aesthetic – the film is consciously a museum piece, its characters pinned to the frame like butterflies."
Music.
The film's period setting allowed Kubrick to indulge his penchant for classical music, and the film score uses pieces by Johann Sebastian Bach (an arrangement of the Concerto for violin and oboe in C minor), Antonio Vivaldi (Cello Concerto in E-Minor, a transcription of the Cello Sonata in E Minor RV 40), Giovanni Paisiello, Wolfgang Amadeus Mozart, and Franz Schubert (German Dance No. 1 in C major, Piano Trio in E-Flat, Opus 100 and Impromptu No. 1 in C minor), as well as the Hohenfriedberger March. The piece most associated with the film, however, is the main title music: George Frideric Handel's stately "Sarabande" from the Suite in D minor HWV 437. Originally for solo harpsichord, the versions for the main and end titles are performed very romantically with orchestral strings, harpsichord, and timpani. It is used at various points in the film, in various arrangements, to indicate the implacable working of impersonal fate.
The score also includes Irish folk music, including Seán Ó Riada's song "Women of Ireland", arranged by Paddy Moloney and performed by The Chieftains.
Reception.
The film "was not the commercial success Warner Bros. had been hoping for" within the United States, although it fared better in Europe. This mixed reaction saw the film (in the words of one retrospective review) "greeted, on its release, with dutiful admiration – but not love. Critics... rail against the perceived coldness of Kubrick's style, the film's self-conscious artistry and slow pace. Audiences, on the whole, rather agreed..." This "air of disappointment" factored into Kubrick's decision to next film Stephen King's "The Shining" – a project that would not only please him artistically, but also be more likely to succeed financially. Still, several other critics, including Gene Siskel, praised the film's technical quality and strong narrative, and Siskel himself counted it as one of the five best films of the year.
In recent years, the film has gained a more positive reaction. it holds a 96% "Certified Fresh" rating on Rotten Tomatoes based on 52 reviews, eight of which are from the site's "top critics." Roger Ebert added the film to his 'Great Movies' list on 9 September 2009, writing, "It defies us to care, it asks us to remain only observers of its stately elegance", and it "must be one of the most beautiful films ever made."
Director Martin Scorsese has named "Barry Lyndon" as his favorite Kubrick film. Quotations from its script have also appeared in such disparate works as Ridley Scott's "The Duellists", Scorsese's "The Age of Innocence", and Wes Anderson's "Rushmore".
Awards.
In 1976, at the 48th Academy Awards, the film won four awards, for Best Art Direction (Ken Adam, Roy Walker, Vernon Dixon), Best Cinematography (John Alcott), Best Costume Design (Milena Canonero, Ulla-Britt Söderlund) and Best Musical Score (Leonard Rosenman, "for his arrangements of Schubert and Handel".) Kubrick was nominated three times, for Best Director, Best Picture, and Best Adapted Screenplay.
Kubrick won the British Academy of Film and Television Arts Award for Best Direction. John Alcott won for Best Cinematography. "Barry Lyndon" was also nominated for Best Film, Art Direction, and Costume Design.
Source novel.
Kubrick based his adapted screenplay on William Makepeace Thackeray's "The Luck of Barry Lyndon" (republished as the novel "Memoirs of Barry Lyndon, Esq.)," a picaresque tale written and published in serial form in 1844.
The film departs from the novel in several ways. In Thackeray's writings, events are related in the first person by Barry himself. A comic tone pervades the work, as Barry proves both a raconteur and an unreliable narrator. Kubrick's film, by contrast, presents the story objectively. Though the film contains voice-over (by actor Michael Hordern), the comments expressed are not Barry's, but those of an omniscient, although not entirely impartial, narrator. This change in perspective alters the tone of the story; Thackeray tells a jaunty, humorous tale, but Kubrick's telling is essentially tragic, albeit with a satirical tone. Kubrick felt that using a first-person narrative would not be useful in a film adaptation:
Kubrick also changed the plot. For example, the novel does not include a final duel. The film begins with a duel where Barry's father is shot dead, and duels recur throughout the film.

</doc>
<doc id="4230" url="https://en.wikipedia.org/wiki?curid=4230" title="Cell (biology)">
Cell (biology)

The cell (from Latin "cella", meaning "small room") is the basic structural, functional, and biological unit of all known living organisms. A cell is the smallest unit of life that can replicate independently, and cells are often called the "building blocks of life". The study of cells is called cell biology.
Cells consist of cytoplasm enclosed within a membrane, which contains many biomolecules such as proteins and nucleic acids. Organisms can be classified as unicellular (consisting of a single cell; including bacteria) or multicellular (including plants and animals). While the number of cells in plants and animals varies from species to species, humans contain more than 10 trillion (10) cells. Most plant and animal cells are visible only under the microscope, with dimensions between 1 and 100 micrometres.
The cell was discovered by Robert Hooke in 1665, who named the biological unit for its resemblance to cells inhabited by Christian monks in a monastery. Cell theory, first developed in 1839 by Matthias Jakob Schleiden and Theodor Schwann, states that all organisms are composed of one or more cells, that cells are the fundamental unit of structure and function in all living organisms, that all cells come from preexisting cells, and that all cells contain the hereditary information necessary for regulating cell functions and for transmitting information to the next generation of cells. Cells emerged on Earth at least 3.5 billion years ago.
Anatomy.
Cells are of two types, eukaryotic, which contain a nucleus, and prokaryotic, which do not. Prokaryotes are single-celled organisms, while eukaryotes can be either single-celled or multicellular.
Prokaryotic cells.
Prokaryotic cells were the first form of life on Earth, characterised by having vital biological processes including cell signaling and being self-sustaining. They are simpler and smaller than eukaryotic cells, and lack membrane-bound organelles such as the nucleus. Prokaryotes include two of the domains of life, bacteria and archaea. The DNA of a prokaryotic cell consists of a single chromosome that is in direct contact with the cytoplasm. The nuclear region in the cytoplasm is called the nucleoid. Most prokaryotes are the smallest of all organisms ranging from 0.5 to 2.0 µm in diameter.
A prokaryotic cell has three architectural regions:
Eukaryotic cells.
Plants, animals, fungi, slime moulds, protozoa, and algae are all eukaryotic. These cells are about fifteen times wider than a typical prokaryote and can be as much as a thousand times greater in volume. The main distinguishing feature of eukaryotes as compared to prokaryotes is compartmentalization: the presence of membrane-bound organelles (compartments) in which specific metabolic activities take place. Most important among these is a cell nucleus, an organelle that houses the cell's DNA. This nucleus gives the eukaryote its name, which means "true kernel (nucleus)". Other differences include:
Subcellular components.
All cells, whether prokaryotic or eukaryotic, have a membrane that envelops the cell, regulates what moves in and out (selectively permeable), and maintains the electric potential of the cell. Inside the membrane, the cytoplasm takes up most of the cell's volume. All cells (except red blood cells which lack a cell nucleus and most organelles to accommodate maximum space for hemoglobin) possess DNA, the hereditary material of genes, and RNA, containing the information necessary to build various proteins such as enzymes, the cell's primary machinery. There are also other kinds of biomolecules in cells. This article lists these primary components of the cell, then briefly describes their function.
Membrane.
The cell membrane, or plasma membrane, is a biological membrane that surrounds the cytoplasm of a cell. In animals, the plasma membrane is the outer boundary of the cell, while in plants and prokaryotes it is usually covered by a cell wall. This membrane serves to separate and protect a cell from its surrounding environment and is made mostly from a double layer of phospholipids, which are amphiphilic (partly hydrophobic and partly hydrophilic). Hence, the layer is called a phospholipid bilayer, or sometimes a fluid mosaic membrane. Embedded within this membrane is a variety of protein molecules that act as channels and pumps that move different molecules into and out of the cell. The membrane is said to be 'semi-permeable', in that it can either let a substance (molecule or ion) pass through freely, pass through to a limited extent or not pass through at all. Cell surface membranes also contain receptor proteins that allow cells to detect external signaling molecules such as hormones.
Cytoskeleton.
The cytoskeleton acts to organize and maintain the cell's shape; anchors organelles in place; helps during endocytosis, the uptake of external materials by a cell, and cytokinesis, the separation of daughter cells after cell division; and moves parts of the cell in processes of growth and mobility. The eukaryotic cytoskeleton is composed of microfilaments, intermediate filaments and microtubules. There are a great number of proteins associated with them, each controlling a cell's structure by directing, bundling, and aligning filaments. The prokaryotic cytoskeleton is less well-studied but is involved in the maintenance of cell shape, polarity and cytokinesis. The subunit protein of microfilaments is a small, monomeric protein called actin. The subunit of microtubules is a dimeric molecule called tubulin. Intermediate filaments are heteropolymers whose subunits vary among the cell types in different tissues. But some of the subunit protein of intermediate filaments include vimentin, desmin, lamin (lamins A, B and C), keratin (multiple acidic and basic keratins), neurofilament proteins (NF - L, NF - M).
Genetic material.
Two different kinds of genetic material exist: deoxyribonucleic acid (DNA) and ribonucleic acid (RNA). Cells use DNA for their long-term information storage. The biological information contained in an organism is encoded in its DNA sequence. RNA is used for information transport (e.g., mRNA) and enzymatic functions (e.g., ribosomal RNA). Transfer RNA (tRNA) molecules are used to add amino acids during protein translation.
Prokaryotic genetic material is organized in a simple circular DNA molecule (the bacterial chromosome) in the nucleoid region of the cytoplasm. Eukaryotic genetic material is divided into different, linear molecules called chromosomes inside a discrete nucleus, usually with additional genetic material in some organelles like mitochondria and chloroplasts (see endosymbiotic theory).
A human cell has genetic material contained in the cell nucleus (the nuclear genome) and in the mitochondria (the mitochondrial genome). In humans the nuclear genome is divided into 46 linear DNA molecules called chromosomes, including 22 homologous chromosome pairs and a pair of sex chromosomes. The mitochondrial genome is a circular DNA molecule distinct from the nuclear DNA. Although the mitochondrial DNA is very small compared to nuclear chromosomes, it codes for 13 proteins involved in mitochondrial energy production and specific tRNAs.
Foreign genetic material (most commonly DNA) can also be artificially introduced into the cell by a process called transfection. This can be transient, if the DNA is not inserted into the cell's genome, or stable, if it is. Certain viruses also insert their genetic material into the genome.
Organelles.
Organelles are parts of the cell which are adapted and/or specialized for carrying out one or more vital functions, analogous to the organs of the human body (such as the heart, lung, and kidney, with each organ performing a different function). Both eukaryotic and prokaryotic cells have organelles, but prokaryotic organelles are generally simpler and are not membrane-bound.
There are several types of organelles in a cell. Some (such as the nucleus and golgi apparatus) are typically solitary, while others (such as mitochondria, chloroplasts, peroxisomes and lysosomes) can be numerous (hundreds to thousands). The cytosol is the gelatinous fluid that fills the cell and surrounds the organelles.
Structures outside the cell membrane.
Many cells also have structures which exist wholly or partially outside the cell membrane. These structures are notable because they are not protected from the external environment by the semipermeable cell membrane. In order to assemble these structures, their components must be carried across the cell membrane by export processes.
Cell wall.
Many types of prokaryotic and eukaryotic cells have a cell wall. The cell wall acts to protect the cell mechanically and chemically from its environment, and is an additional layer of protection to the cell membrane. Different types of cell have cell walls made up of different materials; plant cell walls are primarily made up of cellulose, fungi cell walls are made up of chitin and bacteria cell walls are made up of peptidoglycan.
Prokaryotic.
Capsule.
A gelatinous capsule is present in some bacteria outside the cell membrane and cell wall. The capsule may be polysaccharide as in pneumococci, meningococci or polypeptide as "Bacillus anthracis" or hyaluronic acid as in streptococci.
Capsules are not marked by normal staining protocols and can be detected by India ink or methyl blue; which allows for higher contrast between the cells for observation.
Flagella.
Flagella are organelles for cellular mobility. The bacterial flagellum stretches from cytoplasm through the cell membrane(s) and extrudes through the cell wall. They are long and thick thread-like appendages, protein in nature. A different type of flagellum is found in archaea and a different type is found in eukaryotes.
Fimbria.
A fimbria also known as a pilus is a short, thin, hair-like filament found on the surface of bacteria. Fimbriae, or pili are formed of a protein called pilin (antigenic) and are responsible for attachment of bacteria to specific receptors of human cell (cell adhesion). There are special types of specific pili involved in bacterial conjugation.
Cellular processes.
Growth and metabolism.
Between successive cell divisions, cells grow through the functioning of cellular metabolism. Cell metabolism is the process by which individual cells process nutrient molecules. Metabolism has two distinct divisions: catabolism, in which the cell breaks down complex molecules to produce energy and reducing power, and anabolism, in which the cell uses energy and reducing power to construct complex molecules and perform other biological functions.
Complex sugars consumed by the organism can be broken down into simpler sugar molecules called monosaccharides such as glucose. Once inside the cell, glucose is broken down to make adenosine triphosphate (ATP), a molecule that possesses readily available energy, through two different pathways.
Replication.
Cell division involves a single cell (called a "mother cell") dividing into two daughter cells. This leads to growth in multicellular organisms (the growth of tissue) and to procreation (vegetative reproduction) in unicellular organisms. Prokaryotic cells divide by binary fission, while eukaryotic cells usually undergo a process of nuclear division, called mitosis, followed by division of the cell, called cytokinesis. A diploid cell may also undergo meiosis to produce haploid cells, usually four. Haploid cells serve as gametes in multicellular organisms, fusing to form new diploid cells.
DNA replication, or the process of duplicating a cell's genome, always happens when a cell divides through mitosis or binary fission. This occurs during the S phase of the cell cycle.
In meiosis, the DNA is replicated only once, while the cell divides twice. DNA replication only occurs before meiosis I. DNA replication does not occur when the cells divide the second time, in meiosis II. Replication, like all cellular activities, requires specialized proteins for carrying out the job.
Protein synthesis.
Cells are capable of synthesizing new proteins, which are essential for the modulation and maintenance of cellular activities. This process involves the formation of new protein molecules from amino acid building blocks based on information encoded in DNA/RNA. Protein synthesis generally consists of two major steps: transcription and translation.
Transcription is the process where genetic information in DNA is used to produce a complementary RNA strand. This RNA strand is then processed to give messenger RNA (mRNA), which is free to migrate through the cell. mRNA molecules bind to protein-RNA complexes called ribosomes located in the cytosol, where they are translated into polypeptide sequences. The ribosome mediates the formation of a polypeptide sequence based on the mRNA sequence. The mRNA sequence directly relates to the polypeptide sequence by binding to transfer RNA (tRNA) adapter molecules in binding pockets within the ribosome. The new polypeptide then folds into a functional three-dimensional protein molecule.
Movement or motility.
Unicellular organisms can move in order to find food or escape predators. Common mechanisms of motion include flagella and cilia.
In multicellular organisms, cells can move during processes such as wound healing, the immune response and cancer metastasis. For example, in wound healing in animals, white blood cells move to the wound site to kill the microorganisms that cause infection. Cell motility involves many receptors, crosslinking, bundling, binding, adhesion, motor and other proteins. The process is divided into three steps – protrusion of the leading edge of the cell, adhesion of the leading edge and de-adhesion at the cell body and rear, and cytoskeletal contraction to pull the cell forward. Each step is driven by physical forces generated by unique segments of the cytoskeleton.
Multicellularity.
Cell specialization.
Multicellular organisms are organisms that consist of more than one cell, in contrast to single-celled organisms.
In complex multicellular organisms, cells specialize into different cell types that are adapted to particular functions. In mammals, major cell types include skin cells, muscle cells, neurons, blood cells, fibroblasts, stem cells, and others. Cell types differ both in appearance and function, yet are genetically identical. Cells are able to be of the same genotype but of different cell type due to the differential expression of the genes they contain.
Most distinct cell types arise from a single totipotent cell, called a zygote, that differentiates into hundreds of different cell types during the course of development. Differentiation of cells is driven by different environmental cues (such as cell–cell interaction) and intrinsic differences (such as those caused by the uneven distribution of molecules during division).
Origin of multicellularity.
Multicellularity has evolved independently at least 25 times, including in some prokaryotes, like cyanobacteria, myxobacteria, actinomycetes, "Magnetoglobus multicellularis" or "Methanosarcina". However, complex multicellular organisms evolved only in six eukaryotic groups: animals, fungi, brown algae, red algae, green algae, and plants. It evolved repeatedly for plants (Chloroplastida), once or twice for animals, once for brown algae, and perhaps several times for fungi, slime molds, and red algae. Multicellularity may have evolved from colonies of interdependent organisms, from cellularization, or from organisms in symbiotic relationships.
The first evidence of multicellularity is from cyanobacteria-like organisms that lived between 3 and 3.5 billion years ago. Other early fossils of multicellular organisms include the contested Grypania spiralis and the fossils of the black shales of the Palaeoproterozoic Francevillian Group Fossil B Formation in Gabon.
The evolution of multicellularity from unicellular ancestors has been replicated in the laboratory, in evolution experiments using predation as the selective pressure.
Origins.
The origin of cells has to do with the origin of life, which began the history of life on Earth.
Origin of the first cell.
There are several theories about the origin of small molecules that led to life on the early Earth. They may have been carried to Earth on meteorites (see Murchison meteorite), created at deep-sea vents, or synthesized by lightning in a reducing atmosphere (see Miller–Urey experiment). There is little experimental data defining what the first self-replicating forms were. RNA is thought to be the earliest self-replicating molecule, as it is capable of both storing genetic information and catalyzing chemical reactions (see RNA world hypothesis), but some other entity with the potential to self-replicate could have preceded RNA, such as clay or peptide nucleic acid.
Cells emerged at least 3.5 billion years ago. The current belief is that these cells were heterotrophs. The early cell membranes were probably more simple and permeable than modern ones, with only a single fatty acid chain per lipid. Lipids are known to spontaneously form bilayered vesicles in water, and could have preceded RNA, but the first cell membranes could also have been produced by catalytic RNA, or even have required structural proteins before they could form.
Origin of eukaryotic cells.
The eukaryotic cell seems to have evolved from a symbiotic community of prokaryotic cells. DNA-bearing organelles like the mitochondria and the chloroplasts are descended from ancient symbiotic oxygen-breathing proteobacteria and cyanobacteria, respectively, which were endosymbiosed by an ancestral archaean prokaryote.
There is still considerable debate about whether organelles like the hydrogenosome predated the origin of mitochondria, or vice versa: see the hydrogen hypothesis for the origin of eukaryotic cells.

</doc>
<doc id="4231" url="https://en.wikipedia.org/wiki?curid=4231" title="Buffy the Vampire Slayer (film)">
Buffy the Vampire Slayer (film)

Buffy the Vampire Slayer is a 1992 American comedy horror film about a Valley girl cheerleader named Buffy who learns that it is her fate to hunt vampires. The film starred Kristy Swanson, Donald Sutherland, Paul Reubens, Rutger Hauer, Luke Perry and Hilary Swank. It was a moderate success at the box office, but received mixed reception from critics. The film was taken in a different direction from the one its writer Joss Whedon intended, and five years later he created the darker and acclaimed TV series of the same name.
Plot.
High school senior Buffy Summers (Kristy Swanson) is introduced as a stereotypical, shallow cheerleader at Hemery High School in Los Angeles. She is a carefree popular mean girl whose main concerns are shopping and spending time with her rich, snooty friends and her boyfriend, Jeffrey. While at school one day, she is approached by a man who calls himself Merrick (Donald Sutherland). He informs her that she is The Slayer, or Chosen One, destined to kill vampires, and he is a Watcher whose duty it is to guide and train her. She initially rebukes his claims, but is convinced that he is right when he is able to describe a recurring dream of hers in detail. In addition, Buffy is exhibiting uncanny abilities not known to her, including heightened agility, senses, and endurance, yet she repeatedly tries Merrick's patience with her frivolous nature and sharp-tongued remarks.
Meanwhile Oliver Pike (Luke Perry), and best friend Benny (David Arquette), who resented Buffy and her friends due to differing social circles, are out drinking when they are attacked by vampires. Benny is turned but Oliver is saved by Merrick. As a vampire, Benny visits Oliver and tries to get him to join him. Later, when Oliver and his boss are discussing Benny, Oliver tells him to run if he sees him. Not only this, but a studious girl from Buffy's class, Cassandra, is abducted one night by Amilyn (Paul Reubens), acolyte of a local vampire king Lothos (Rutger Hauer), whom she is sacrificed. When her body is found, the news spreads through LA and Hemery High, but her murder is met with indifference from Buffy's clique.
When Oliver encounters Amilyn and his tribe of vampires, he attempts to run them over with his vehicle, which causes Amilyn to lose his arm, but he is still unable to escape from them, and Buffy and Merrick arrive to rescue him. Amilyn flees the fight to talk to Lothos who now realizes Buffy is the slayer. After this encounter, Buffy and Oliver start a friendship, which eventually becomes romantic and Oliver becomes Buffy's partner in fighting the undead.
During a basketball game, Buffy finds out that one of the players is a vampire. After a quick chase to a parade float storage yard, Buffy finally confronts Lothos, shortly after she and Oliver take down his gang. Lothos puts Buffy in a hypnotic trance, which is broken due to Merrick's intervention. Lothos turns on Merrick and impales him with the stake he attempted to use on him. Lothos leaves, saying that Buffy is not ready. As Merrick dies, he tells Buffy to do things her own way rather than live by the rules of others. Because of her new life, responsibilities, and heartbreak, Buffy becomes emotionally shocked and starts dropping her Slayer duties. When she arrives at school, she attempts to explain everything to her friends, but they refuse to understand her as they are more concerned with their upcoming school dance, and Buffy falls out with them as she realizes she is outgrowing their immature, selfish behavior.
At the senior dance, Buffy tries to patch things up with her friends but they turn against her, and she is dismayed to find Jeffrey has dumped her for one of her friends. However, she meets up with Oliver and as they start to dance and eventually kiss, Lothos leads the remainder of his minions to the school and attacks the students and the attending faculty. Buffy confronts the vampires outside while Oliver fights the vampiric Benny. After overpowering the vampires, she confronts Lothos inside the school and stabs Amilyn. Lothos hypnotises Buffy again but she uses a cross and hairspray to create a makeshift flame-thrower and burns Lothos before escaping back into the gym. Buffy sees everybody recover from the attack, but Lothos emerges again getting into a fight with Buffy, who then stakes him.
As all of the survivors leave, Buffy and Oliver decide to finish their dance. The film then ends with the two of them leaving the dance on a motorcycle, and a news crew interviewing the students and the principal about the attack during the credits.
Continuity with the television show.
Many of the details given in the film differ from the continuity of the later television series. For example, Buffy's age and history is dissimilar; she is a senior in high school in the film, but the series starts with her as a sophomore. In the film, her parents are wealthy but negligent socialites who care little for her and spend their time at parties and golf tournaments; in the TV show, Buffy has a caring, single mother named Joyce. Both the vampires' and Slayer's abilities are depicted differently. The vampires in the film die like humans, while in the TV show they turn to dust, and unlike the TV show their faces remain human, albeit pale and fanged, whereas in the series they are able to take on a demonic aspect. Joss Whedon has expressed his disapproval with the movie's interpretation of the script, stating "I finally sat down and had written it and somebody had made it into a movie, and I felt like — well, that's not quite her. It's a start, but it's not quite the girl. On the movie, Merrick never mentioned he was a watcher, here he was just a mysterious guy who was more of a guide or never helped her and was very strict with her. The TV series, they mentioned a watcher when Rupert Giles mentions it to Buffy in Welcome to Hellmouth and what they do."
According to the "Official Buffy Watcher's Guide", Whedon wrote the pilot to the TV series as a sequel to his original script, which is why the show makes references to events that did not occur in the film. In 1999, Dark Horse Comics released a graphic novel adaptation of Whedon's original script under the title "The Origin". Whedon stated: "The "Origin" comic, though I have issues with it, CAN pretty much be accepted as canonical. They did a cool job of combining the movie script with the series, that was nice, and using the series Merrick and not a certain OTHER thespian who shall remain hated."
Box office.
The film debuted at #5 at the North American box office and eventually grossed $16,624,456 against a $7 million production budget.
Home releases.
The film was released on VHS and Laserdisc in the U.S. in 1992 by Fox Video and re-released in 1995 under the "Twentieth Century Fox Selections" banner. It was released on DVD in the US in 2001 and on Blu-ray in 2011.
Soundtrack.
The soundtrack does not include every song played in the film, which also included "In the Wind" by War Babies and "Inner Mind" by Eon.
Possible remake.
On May 25, 2009, "The Hollywood Reporter" reported that Roy Lee and Doug Davison of Vertigo Entertainment were working with Fran Rubel Kuzui and Kaz Kuzui on a re-envisioning or relaunch of the "Buffy" film for the big screen. The film would not be a sequel or prequel to the existing film or television franchise and Joss Whedon would have no involvement in the project. None of the characters, cast, or crew from the television series would be featured. Television series executive producer Marti Noxon later reflected that this story might have been produced by the studio in order to frighten Whedon into taking the reins of the project. On November 22, 2010, "The Hollywood Reporter" confirmed that Warner Bros. had picked up the movie rights to the remake. The film was set for release sometime in 2012. 20th Century Fox, which usually holds rights to the more successful "Buffy"/"Angel" television franchise, will retain merchandising and some distribution rights.
The idea of the remake caused wrath among fans of the TV series, since Whedon is not involved and the project does not have any connection with the show and will not conform to the continuity maintained with the "Buffy the Vampire Slayer Season Eight" and "Season Nine" comic book titles. Not only the fandom, but the main cast members of both "Buffy" and "Angel" series, expressed disagreement with the report on Twitter and in recent interviews. Sarah Michelle Gellar said, "I think it's a horrible idea. To try to do a "Buffy" without Joss Whedon... to be incredibly non-eloquent: that's the dumbest idea I've ever heard." Proposed shooting locations included Black Wood and other areas in rural England, due to budgetary constraints and the potential setting as being outside of the city, an unusual change for the franchise.
In December 2011, more than a year after the official reboot announcement, the "Los Angeles Times" site reported that Whit Anderson, the writer picked for the new "Buffy" movie, had her script rejected by the producers behind the project, and that a new writer was being sought. Sources also stated that "If you're going to bring it back, you have to do it right. nderso came in with some great ideas and she had reinvented some of the lore and it was pretty cool but in the end there just wasn't enough on the page."
As of May 2015, there have been no further developments of the reboot.

</doc>
<doc id="4232" url="https://en.wikipedia.org/wiki?curid=4232" title="Barter">
Barter

Barter is a system of exchange where goods or services are directly exchanged for other goods or services without using a medium of exchange, such as money. It is distinguishable from gift economies in many ways; one of them is that the reciprocal exchange is immediate and not delayed in time. It is usually bilateral, but may be multilateral (i.e., mediated through barter organizations) and, in most developed countries, usually only exists parallel to monetary systems to a very limited extent. Barter, as a replacement for money as the method of exchange, is used in times of monetary crisis, such as when the currency may be either unstable (e.g., hyperinflation or deflationary spiral) or simply unavailable for conducting commerce.
Economists since Adam Smith, looking at non-specific archaic societies as examples, have used the inefficiency of barter to explain the emergence of money, the economy, and hence the discipline of economics itself. However, ethnographic studies have shown no present or past society has used barter without any other medium of exchange or measurement, nor have anthropologists found evidence that money emerged from barter, instead finding that gift-giving (credit extended on a personal basis with an inter-personal balance maintained over the long term) was the most usual means of exchange of gifts and services.
Since the 1830s, barter in some western market economies has been aided by exchanges that use alternative currencies based on the labour theory of value, and are designed to prevent profit taking by intermediators. Examples include the Owenite socialists, the Cincinnati Time store, and more recently Ithaca HOURS (Time banking) and the LETS system.
Economic theory.
Adam Smith on the origin of money.
Adam Smith, the father of modern economics, sought to demonstrate that markets (and economies) pre-existed the state, and hence should be free of government regulation. He argued (against conventional wisdom) that money was not the creation of governments. Markets emerged, in his view, out of the division of labour, by which individuals began to specialize in specific crafts and hence had to depend on others for subsistence goods. These goods were first exchanged by barter. Specialization depended on trade, but was hindered by the "double coincidence of wants" which barter requires, i.e., for the exchange to occur, each participant must want what the other has. To complete this hypothetical history, craftsmen would stockpile one particular good, be it salt or metal, that they thought no one would refuse. This is the origin of money according to Smith. Money, as a universally desired medium of exchange, allows each half of the transaction to be separated.
Barter is characterized in Adam Smith's "The Wealth of Nations" by a disparaging vocabulary: "higgling, haggling, swapping, dickering." It has also been characterized as negative reciprocity, or "selfish profiteering."
Anthropologists have argued, in contrast, "that when something resembling barter "does" occur in stateless societies it is almost always between strangers." Barter occurred between strangers, not fellow villagers, and hence cannot be used to naturalistically explain the origin of money without the state. Since most people engaged in trade knew each other, exchange was fostered through the extension of credit. Marcel Mauss, author of 'The Gift', argued that the first economic contracts were to "not" act in one's economic self-interest, and that before money, exchange was fostered through the processes of reciprocity and redistribution, not barter. Everyday exchange relations in such societies are characterized by generalized reciprocity, or a non-calculative familial "communism" where each takes according to their needs, and gives as they have.
Limitations.
The limitations of barter are often explained in terms of its inefficiencies in facilitating exchange in comparison to money.
It is said that barter is 'inefficient' because:
History.
Silent trade.
Other anthropologists have questioned whether barter is typically between "total" strangers, a form of barter known as "silent trade". Silent trade, also called silent barter, dumb barter ("dumb" here used in its old meaning of "mute"), or depot trade, is a method by which traders who cannot speak each other's language can trade without talking. However, Benjamin Orlove has shown that while barter occurs through "silent trade" (between strangers), it also occurs in commercial markets as well. "Because barter is a difficult way of conducting trade, it will occur only where there are strong institutional constraints on the use of money or where the barter symbolically denotes a special social relationship and is used in well-defined conditions. To sum up, multipurpose money in markets is like lubrication for machines - necessary for the most efficient function, but not necessary for the existence of the market itself."
In his analysis of barter between coastal and inland villages in the Trobriand Islands, Keith Hart highlighted the difference between highly ceremonial gift exchange between community leaders, and the barter that occurs between individual households. The haggling that takes place between strangers is possible because of the larger temporary political order established by the gift exchanges of leaders. From this he concludes that barter is "an atomized interaction predicated upon the presence of society" (i.e. that social order established by gift exchange), and not typical between complete strangers.
Times of monetary crisis.
As Orlove noted, barter may occur in commercial economies, usually during periods of monetary crisis. During such a crisis, currency may be in short supply, or highly devalued through hyperinflation. In such cases, money ceases to be the universal medium of exchange or standard of value. Money may be in such short supply that it becomes an item of barter itself rather than the means of exchange. Barter may also occur when people cannot afford to keep money (as when hyperinflation quickly devalues it).
Exchanges.
Economic historian Karl Polanyi has argued that where barter is widespread, and cash supplies limited, barter is aided by the use of credit, brokerage, and money as a unit of account (i.e. used to price items). All of these strategies are found in ancient economies including Ptolemaic Egypt. They are also the basis for more recent barter exchange systems.
While one-to-one bartering is practiced between individuals and businesses on an informal basis, organized barter exchanges have developed to conduct third party bartering which helps overcome some of the limitations of barter. A barter exchange operates as a broker and bank in which each participating member has an account that is debited when purchases are made, and credited when sales are made.
Modern barter and trade has evolved considerably to become an effective method of increasing sales, conserving cash, moving inventory, and making use of excess production capacity for businesses around the world. Businesses in a barter earn trade credits (instead of cash) that are deposited into their account. They then have the ability to purchase goods and services from other members utilizing their trade credits – they are not obligated to purchase from those whom they sold to, and vice versa. The exchange plays an important role because they provide the record-keeping, brokering expertise and monthly statements to each member. Commercial exchanges make money by charging a commission on each transaction either all on the buy side, all on the sell side, or a combination of both. Transaction fees typically run between 8 and 15%.
Utopian socialism.
The Owenite socialists in Britain and the United States in the 1830s were the first to attempt to organize barter exchanges. Owenism developed a "theory of equitable exchange" as a critique of the exploitative wage relationship between capitalist and labourer, by which all profit accrued to the capitalist. To counteract the uneven playing field between employers and employed, they proposed "schemes of labour notes based on labour time, thus institutionalizing Owen's demand that human labour, not money, be made the standard of value." This alternate currency eliminated price variability between markets, as well as the role of merchants who bought low and sold high. The system arose in a period where paper currency was an innovation. Paper currency was an I.O.U. circulated by a bank (a promise to pay, not a payment in itself). Both merchants and an unstable paper currency created difficulties for direct producers.
An alternate currency, denominated in labour time, would prevent profit taking by middlemen; all goods exchanged would be priced only in terms of the amount of labour that went into them as expressed in the maxim 'Cost the limit of price'. It became the basis of exchanges in London, and in America, where the idea was implemented at the New Harmony communal settlement by Josiah Warren in 1826, and in his Cincinnati 'Time store' in 1827. Warren ideas were adopted by other Owenites and currency reformers, even though the labour exchanges were relatively short lived.
In England, about 30 to 40 cooperative societies sent their surplus goods to an "exchange bazaar" for direct barter in London, which later adopted a similar labour note. The British Association for Promoting Cooperative Knowledge established an "equitable labour exchange" in 1830. This was expanded as the National Equitable Labour Exchange in 1832 on Grays Inn Road in London. These efforts became the basis of the British cooperative movement of the 1840s. In 1848, the socialist and first self-designated anarchist Pierre-Joseph Proudhon postulated a system of "time chits". In 1875, Karl Marx wrote of "Labor Certificates" ("Arbeitszertifikaten") in his Critique of the Gotha Program of a "certificate from society that he laboure has furnished such and such an amount of labour", which can be used to draw "from the social stock of means of consumption as much as costs the same amount of labour."
Twentieth century experiments.
The first exchange system was the Swiss WIR Bank. It was founded in 1934 as a result of currency shortages after the stock market crash of 1929. "WIR" is both an abbreviation of Wirtschaftsring and the word for "we" in German, reminding participants that the economic circle is also a community.
In Spain (particularly the Catalonia region) there is a growing number of exchange markets. These barter markets or swap meets work without money. Participants bring things they do not need and exchange them for the unwanted goods of another participant. Swapping among three parties often helps satisfy tastes when trying to get around the rule that money is not allowed.
Michael Linton originated the term "local exchange trading system" (LETS) in 1983 and for a time ran the Comox Valley LETSystems in Courtenay, British Columbia. LETS networks use interest-free local credit so direct swaps do not need to be made. For instance, a member may earn credit by doing childcare for one person and spend it later on carpentry with another person in the same network. In LETS, unlike other local currencies, no scrip is issued, but rather transactions are recorded in a central location open to all members. As credit is issued by the network members, for the benefit of the members themselves, LETS are considered mutual credit systems.
Modern developments.
According to the International Reciprocal Trade Association, the industry trade body, more than 450,000 businesses transacted $10 billion globally in 2008 – and officials expect trade volume to grow by 15% in 2009.
It is estimated that over 450,000 businesses in the United States were involved in barter exchange activities in 2010. There are approximately 400 commercial and corporate barter companies serving all parts of the world. There are many opportunities for entrepreneurs to start a barter exchange. Several major cities in the U.S. and Canada do not currently have a local barter exchange. There are two industry groups in the United States, the National Association of Trade Exchanges (NATE) and the International Reciprocal Trade Association (IRTA). Both offer training and promote high ethical standards among their members. Moreover, each has created its own currency through which its member barter companies can trade. NATE's currency is the known as the BANC and IRTA's currency is called Universal Currency (UC).
In Canada, the largest barter exchange is Tradebank, founded in 1987.
In the United States, the largest barter exchange and corporate trade group is International Monetary Systems, founded in 1985, now with representation in various countries.
In Australia and New Zealand the largest barter exchange is Bartercard, founded in 1991, with offices in the United Kingdom,United States, Cyprus,UAE and Thailand.
Corporate barter focuses on larger transactions, which is different from a traditional, retail oriented barter exchange. Corporate barter exchanges typically use media and advertising as leverage for their larger transactions. It entails the use of a currency unit called a "trade-credit". The trade-credit must not only be known and guaranteed, but also be valued in an amount the media and advertising could have been purchased for had the "client" bought it themselves (contract to eliminate ambiguity and risk).
Soviet bilateral trade is occasionally called "barter trade", because although the purchases were denominated in U.S. dollars, the transactions were credited to an international clearing account, avoiding the use of hard cash.
Tax implications.
In the United States, Karl Hess used bartering to make it harder for the IRS to seize his wages and as a form of tax resistance. Hess explained how he turned to barter in an op-ed for "The New York Times" in 1975. However the IRS now requires barter exchanges to be reported as per the Tax Equity and Fiscal Responsibility Act of 1982. Barter exchanges are considered taxable revenue by the IRS and must be reported on a 1099-B form. According to the IRS, "The fair market value of goods and services exchanged must be included in the income of both parties."
Other countries, though, do not have the reporting requirement that the U.S. does concerning proceeds from barter transactions, but taxation is handled the same way as a cash transaction. If one barters for a profit, one pays the appropriate tax; if one generates a loss in the transaction, they have a loss. Bartering for business is also taxed accordingly as business income or business expense. Many barter exchanges require that one register as a business.

</doc>
<doc id="4233" url="https://en.wikipedia.org/wiki?curid=4233" title="Berthe Morisot">
Berthe Morisot

Berthe Marie Pauline Morisot (; January 14, 1841 – March 2, 1895) was a painter and a member of the circle of painters in Paris who became known as the Impressionists. She was described by Gustave Geffroy in 1894 as one of "les trois grandes dames" of Impressionism alongside Marie Bracquemond and Mary Cassatt.
In 1864, she exhibited for the first time in the highly esteemed Salon de Paris. Sponsored by the government, and judged by Academicians, the Salon was the official, annual exhibition of the Académie des beaux-arts in Paris. Her work was selected for exhibition in six subsequent Salons until, in 1874, she joined the "rejected" Impressionists in the first of their own exhibitions, which included Paul Cézanne, Edgar Degas, Claude Monet, Camille Pissarro, Pierre-Auguste Renoir, and Alfred Sisley. It was held at the studio of the photographer Nadar.
She was married to Eugène Manet, the brother of her friend and colleague Édouard Manet.
Early life and education.
Morisot was born in Bourges, France, into an affluent bourgeois family. Her father, Edmé Tiburce Morisot, was the prefect (senior administrator) of the department of Cher. Her mother, Marie-Joséphine-Cornélie Thomas, was the great-niece of Jean-Honoré Fragonard, one of the most prolific Rococo painters of the ancien régime. She had two older sisters, Yves (1838–1893) and Edma (1839–1921), plus a younger brother, Tiburce, born in 1848. The family moved to Paris in 1852, when Morisot was a child.
It was common practice for daughters of bourgeois families to receive art education, so Berthe and her sisters Yves and Edma were taught privately by Geoffroy-Alphonse Chocarne and . In 1857 Guichard introduced Berthe and Edma to the Louvre gallery where they could learn by looking, and from 1858 they learned by copying paintings. He also introduced them to the works of Gavarni.
As art students, Berthe and Edma worked closely together until Edma married Adolphe Pontillon, a naval officer, moved to Cherbourg, had children, and had less time to paint. Letters between the sisters show a loving relationship, underscored by Berthe's regret at the distance between them and Edma's withdrawal from painting. Edma wholeheartedly supported Berthe's continued work and their families always remained close. Edma wrote "“… I am often with you in thought, dear Berthe. I’m in your studio and I like to slip away, if only for a quarter of an hour, to breathe that atmosphere that we shared for many years…”".
Her sister Yves married Theodore Gobillard, a tax inspector, in 1866, and was painted by Edgar Degas as "Mrs Theodore Gobillard" (Metropolitan Museum of Art, New York).
Morisot registered as a copyist at the Louvre where she befriended other artists and teachers including Camille Corot, the pivotal landscape painter of the Barbizon School who also excelled in figure painting. In 1860, under Corot's influence she took up the plein air (outdoors) method of working. By 1863 she was studying under , another Barbizon painter. In the winter of 1863–64 she studied sculpture under Aimé Millet, but none of her sculpture is known to survive.
Impressionism.
Morisot's first appearance in the Salon de Paris came at the age of twenty-three in 1864, with the acceptance of two landscape paintings. She continued to show regularly in the Salon, to generally favorable reviews, until 1873, the year before the first Impressionist exhibition. She exhibited with the Impressionists from 1874 onwards, only missing the exhibition in 1878 when her daughter was born.
Morisot's mature career began in 1872. She found an audience for her work with Durand-Ruel, the private dealer, who bought twenty-two paintings. In 1877, she was described by the critic for "Le Temps" as the "one real Impressionist in this group." She chose to exhibit under her full maiden name instead of using a pseudonym or her married name. In the 1880 exhibition, many reviews judged Morisot among the best, including "Le Figaro" critic Albert Wolff.
Manet.
In 1868 Morisot became friends with Édouard Manet who painted several portraits of her, including a striking study in a black veil while in mourning for her father. Correspondence between them shows warm affection, and Manet gave her an easel as a Christmas present. To her dismay he interfered with one of her Salon submissions whilst he was engaged to transport it, mistaking her self-criticism as an invitation to add corrections.
Although Manet is regarded as the master and Morisot as the follower, there is evidence that their relationship was reciprocal. Records show Manet's appreciation of her distinctive original style and compositional decisions, some of which he incorporated into his own work. It was Morisot who persuaded Manet to attempt plein air painting, which she had been practising since having been introduced to it by Corot.
Morisot drew Manet into the circle of painters who became known as the Impressionists. In 1874, she married Manet's brother, Eugène, and they had one daughter, Julie, who became the subject for many of her mother's paintings. Julie's memoirs, "Growing Up with the Impressionists: The Diary of Julie Manet", were published in 1987.
Style and technique.
Morisot’s works are almost always small in scale. She worked in oil paint, watercolors, or pastel, and sketched using various drawing media. Around 1880 she began painting on unprimed canvases—a technique Manet and Eva Gonzalès also experimented with at the time—and her brushwork became looser. In 1888–89, her brushstrokes transitioned from short, rapid strokes to long, sinuous ones that define form. The outer edges of her paintings were often left unfinished, allowing the canvas to show through and increasing the sense of spontaneity. After 1885, she worked mostly from preliminary drawings before beginning her oil paintings.
Morisot creates a sense of space and depth through the use of color. Although her color palette was somewhat limited, her fellow impressionists regarded her as a "virtuoso colorist". She typically made expansive use of white, whether used as a pure white or mixed with other colors. In her large painting, "The Cherry Tree", colors are more vivid but are still used to emphasize form.
Subjects.
Morisot painted what she experienced on a daily basis. Her paintings reflect the 19th-century cultural restrictions of her class and gender. She avoided urban and street scenes and seldom painted the nude figure. Like her fellow Impressionist Mary Cassatt, she focused on domestic life and portraits in which she could use family and personal friends as models, including her daughter Julie. Prior to the 1860s, Morisot painted subjects in line with the Barbizon school before turning to scenes of contemporary femininity. Paintings like "The Cradle" (1872), in which she depicted current trends for nursery furniture, reflect her sensitivity to fashion and advertising, both of which would have been apparent to her female audience. Her works also include landscapes, portraits, garden settings and boating scenes. Later in her career Morisot worked with more ambitious themes, such as nudes. Corresponding with Morisot's interest in nude subjects, Morisot also began to focus more on preliminary drawings, completing many drypoints, charcoal, and color pencil drawings.
Personal life.
Morisot was married to Eugène Manet, the brother of her friend and colleague Édouard Manet, from 1874 until his death in 1892. In 1878 she gave birth to her only child, Julie, who posed frequently for her mother and other Impressionist artists, including Renoir and her uncle Édouard.
Death.
Morisot died on 2 March 1895, in Paris, of pneumonia contracted while attending to her daughter Julie's similar illness, and thus orphaning her at the age of 16. She was interred in the Cimetière de Passy.
Popular culture.
She was portrayed by actress Marine Delterme in the eponymous 2012 French biographical TV film directed by Caroline Champetier. The character of Beatrice de Clerval in Elizabeth Kostova's "The Swan Thieves" is largerly based on Morisot.
Art market.
Morisot's work sold comparatively well. She achieved the two highest prices at a Hôtel Drouot auction in 1875, the "Interior (Young Woman with Mirror)" sold for 480 francs, and her pastel "On the Lawn" sold for 320 francs. Her works averaged 250 francs, the best relative prices at the auction.
In February 2013, Morisot became the highest priced female artist, when "After Lunch" (1881), a portrait of a young redhead in a straw hat and purple dress, sold for $10.9 million at a Christie's auction. The painting achieved roughly three times its upper estimate, exceeding the $10.7 million for a sculpture by Louise Bourgeois in 2012.
Works.
Selection of Works.
This limited selection is based on the book "Berthe Morisot" by Charles F. Stuckey, William P. Scott and Susan G. Lindsay, which is in turn drawn from the 1961 catalogue by Marie-Louise Bataille, Rouaart Denis and Georges Wildenstein. There are variations between the dates of execution, first showing and purchase. Titles may vary between sources.

</doc>
<doc id="4237" url="https://en.wikipedia.org/wiki?curid=4237" title="Barnard College">
Barnard College

Barnard College is a private women's liberal arts college in the United States and one of the Seven Sisters. Founded in 1889, it has been affiliated with Columbia University since 1900. Barnard's campus stretches along Broadway between 116th and 120th Streets in the Morningside Heights neighborhood in the borough of Manhattan, in New York City. It is directly across Broadway from Columbia's campus and near several other academic institutions and has been used by Barnard since 1898.
History.
Columbia College, Columbia University admitted only men for undergraduate study for 229 years. Barnard College was founded to provide an undergraduate education for women comparable to that of Columbia and other Ivy League schools. The college was named after Frederick Augustus Porter Barnard, an American educator and mathematician, who served as the tenth president of Columbia from 1864 to 1889. He advocated equal educational privileges for men and women, preferably in a coeducational setting, and began proposing in 1879 that Columbia admit women.
The board of trustees repeatedly rejected Barnard's suggestion, but in 1883 agreed to create a detailed syllabus of study for women. While they could not attend Columbia classes, those who passed examinations based on the syllabus would receive a degree. The first such woman graduate received her bachelor's degree in 1887. A former student of the program, Annie Nathan Meyer, and other prominent New York women persuaded the board in 1889 to create a women's college connected to Columbia.
Barnard College's original 1889 home was a rented brownstone at 343 Madison Avenue, where a faculty of six offered instruction to 14 students in the School of Arts, as well as to 22 "specials", who lacked the entrance requirements in Greek and so enrolled in science. When Columbia University announced in 1892 its impending move to Morningside Heights, Barnard built a new campus on 119th-120th Streets with gifts from Mary E. Brinckerhoff, Elizabeth Milbank Anderson and Martha T. Fiske. Milbank, Brinckerhoff, and Fiske Halls, built in 1897–1898, were listed on the National Register of Historic Places in 2003.
Ella Weed supervised the college in its first four years; Emily James Smith succeeded her as Barnard's first dean. As the college grew it needed additional space, and in 1903 it received the three blocks south of 119th Street from Anderson who had purchased a former portion of the Bloomingdale Asylum site from the New York Hospital. By the mid-20th century Barnard had succeeded in its original goal of providing an elite education to women. Between 1920 and 1974, only the much larger Hunter College and University of California, Berkeley produced more women graduates who later received doctorate degrees.
Students' Hall, now known as Barnard Hall, was built in 1916. Brooks and Hewitt Halls were built in 1906–1907 and 1926–1927, respectively. They were listed on the National Register of Historic Places in 2003.
Jessica Garretson Finch is credited with coining the phrase, "current events," while teaching at Barnard College in the 1890s.
Relationship with Columbia University.
The "Barnard Bulletin" in 1976 described the relationship between the college and Columbia University as "intricate and ambiguous". Barnard president Debora Spar said in 2012 that "the relationship is admittedly a complicated one, a unique one and one that may take a few sentences to explain to the outside community".
The college's front gates state Barnard College of Columbia University. Barnard describes itself as "both an independently incorporated educational institution and an official college of Columbia University", and advises students to state "Barnard College, Columbia University" or "Barnard College of Columbia University" on résumés. Columbia describes Barnard as an affiliated institution that is a faculty of the university or is "in partnership with" it.
"The New York Times" in 2013 called Barnard "an undergraduate women's college of Columbia University", and an academic journal described Barnard in 1991 as a former affiliate that became a school within the university. Facebook includes Barnard students and alumnae within the Columbia interest group. Both the college and Columbia evaluate Barnard faculty for tenure, and Barnard graduates receive Columbia University diplomas signed by both the Barnard and Columbia presidents.
Smith and Columbia president Seth Low worked to open Columbia classes to Barnard students. By 1900 they could attend Columbia classes in philosophy, political science, and several scientific fields. That year Barnard formalized an affiliation with the university which made available to its students the instruction and facilities of Columbia. Franz Boas, who taught at both Columbia and Barnard in the early 1900s, was among those faculty members who reportedly found Barnard students superior to their male Columbia counterparts. From 1955 Columbia and Barnard students could register for the other school's classes with the permission of the instructor; from 1973 no permission was needed.
By the 1940s other undergraduate and graduate divisions of Columbia University admitted women. Columbia president William J. McGill predicted in 1970 that Barnard College and Columbia College would merge within five years, and by the mid-1970s most Columbia dormitories were coed. The university's financial difficulties during the decade increased its desire to merge, but Barnard resisted doing so because of Columbia's large debt. The college's marketing emphasized the Columbia relationship, however, the "Bulletin" in 1976 stating that Barnard described it as identical to the one between Harvard College and Radcliffe College ("who are merged in practically everything but name at this point").
After a decade of failed negotiations for a merger with Barnard akin to Harvard and Radcliffe, Columbia College instead began admitting women in 1983. Applications to Columbia rose 56% that year, making admission more selective, and nine Barnard students transferred to Columbia. Eight students admitted to both Columbia and Barnard chose Barnard, while 78 chose Columbia. Within a few years, however, selectivity rose at both schools as they received more women applicants than expected.
The Columbia-Barnard affiliation continued. Barnard pays Columbia about $5 million a year under the terms of the "interoperate relationship", which the two schools renegotiate every 15 years. Despite the affiliation Barnard is legally and financially separate from Columbia, with an independent faculty and board of trustees. It is responsible for its own separate admissions, health, security, guidance and placement services, and has its own alumnae association. Nonetheless, Barnard students participate in the academic, social, athletic and extracurricular life of the broader University community on a reciprocal basis. The affiliation permits the two schools to share some academic resources; for example, only Barnard has an urban studies department, and only Columbia has a computer science department. Most Columbia classes are open to Barnard students and vice versa. Barnard students and faculty are represented in the University Senate, and student organizations such as the "Columbia Daily Spectator" are open to all students. Barnard students play on Columbia athletics teams, and Barnard uses Columbia email, telephone and network services.
Admissions.
Admissions to Barnard is considered most selective by "U.S. News & World Report". It is the most selective women's college in the nation; in 2008, Barnard had the lowest acceptance rate of the five Seven Sisters that remain single-sex in admissions.
The class of 2019's admission rate was 19.5% of the 6,655 applicants, the lowest acceptance rate in the institution's history. The early-decision admission rate was 47.7%, out of 392 applications. The median SAT Combined was 2060, with median subscores of 660 in Math, 690 in Critical Reading, and 700 in Writing. The Median ACT score was 30. Of the women in the class of 2012, 89.4% ranked in first or second decile at their high school (of the 41.3% ranked by their schools). The average GPA of the class of 2012 was 94.3 on a 100-point scale and 3.88 on a 4.0 scale.
For the class of 2011, Barnard College admitted 28.7% of those who applied. The median ACT score was 30, while the median combined SAT score was 2100.
In 2015 Barnard announced that it would admit transgender women who "consistently live and identify as women, regardless of the gender assigned to them at birth", and would continue to support and enroll those students who transitioned to males after they had already been admitted.
Academic ranking.
In the 2014 U.S. News & World Report rankings, Barnard was ranked as the 32nd best liberal arts college in the country. The ranking came under widespread criticism, as it only accounted for institution-specific resources. Greg Brown, chief operating officer at Barnard, said, "I believe that our ranking is lower than it should be, primarily because the methodology simply can't account for the Barnard-Columbia relationship. Because the Columbia relationship doesn't fit neatly into any of the survey categories, it is essentially ignored. Rankings are inherently limited in this way."
In 1998, then president Judith Shapiro compared the ranking service to the "equivalent of "Sport's Illustrated" swimsuit issue." According to Shapiro's letter, "Such a ranking system certainly does more harm than good in terms of educating the public." On June 19, 2007, following a meeting of the Annapolis Group, which represents over 100 liberal arts colleges, Barnard announced that it would no longer participate in the U.S. News annual survey, and that they would fashion their own way to collect and report common data.
Barnard Library.
Barnard's Wollman Library is located in Adele Lehman Hall. Its collection includes nearly 200,000 volumes in support of the undergraduate curriculum. It also houses an archival collection of official and student publications, photographs, letters, alumnae scrapbooks and other material that documents Barnard's history from its founding in 1889 to the present day. Among the special collections are the Overbury Collection and a small collection of other rare books. The Overbury Collection consists of 3,300 items, including special and first edition books as well as manuscript materials by and about American women authors. Alumnae Books is a collection of books donated by Barnard alumnae authors. Conflicting accounts list either Richard B. Snow or Philip M. Chu as the architect of Lehman Hall... as well as of the Amherst College library and one of the libraries at Princeton University. The building opened in 1959.
Zine Collection.
Birthed from a proposal by longtime zinester Jenna Freedman, Barnard collects zines in an effort to document third-wave feminism and Riot Grrrl culture. The Zine Collection complements Barnard's women's studies research holdings, giving room to voices of girls and women otherwise under or not at all represented in the book stacks. According to its library collection development policy, "Barnard's zines are written by women (cis- and transgender) with an emphasis on zines by women of color. We collect zines on feminism and femme identity by people of all genders. The zines are personal and political publications on activism, anarchism, body image, third wave feminism, gender, parenting, queer community, riot grrrl, sexual assault, trans experience, and other topics."
Barnard's collection documents movements and trends in feminist thought through the personal work of artists, writers, and activists. Currently, the Barnard Zine Collection has approximately 7,000 items, including zines about race, gender, sexuality, childbirth, motherhood, politics, and relationships. Barnard attempts to collect two copies of each zine, one of which circulates with the second copy archived for preservation. To facilitate circulation, Barnard zines are cataloged in CLIO (the Columbia/Barnard OPAC) and OCLC's Worldcat.
Culture and student life.
Student organizations.
Every Barnard student is part of the Student Government Association (SGA), which elects a representative student government. SGA aims to facilitate the expression of opinions on matters that directly affect the Barnard community. Members of the Executive Board and the Representative Council of SGA promote these goals through active communication between students, faculty, and administration. The Executive Board includes the President of SGA, Vice President, Vice President for Campus Life, Vice President for Communications,and Vice President of Finance. Members of the Representative Council include the Senior Representative to the Board of Trustees, Junior Representative to the Board of Trustees, University Senator, Representative for Campus Policy, Representative for Academic Affairs, Representative for Diversity, Representative for Student Services, Representative for Student Interests, Representative for College Relations, Representative for Arts and Culture, Representative for Campus Affairs, and Representative for Information and Technology. In addition to these members the President and Vice President of each Class Council also sit on the Representative Council.
Student groups include theatre and vocal music groups, language clubs, literary magazines, a freeform radio station called WBAR, a biweekly magazine called the "Barnard Bulletin", community service groups, and others. Barnard students can also join extracurricular activities or organizations at Columbia University, while Columbia University students are allowed in most, but not all, Barnard organizations.
Barnard's McIntosh Activities Council (commonly known as McAC), named after the first President of Barnard, Millicent McIntosh, organizes various community focused events on campus, such as Big Sub and Midnight Breakfast. McAC is made up of five sub-committees which are the Mosaic committee (formerly known as Multicultural), the Wellness committee, the Network committee, the Community committee, and the Action committee. Each committee has a different focus, such as hosting and publicizing identity and cultural events (Mosaic), having health and wellness related events (Wellness), giving students opportunities to be involved with Alumnae and various professionals (Network), planning events that bring the entire student body together (Community), and planning community service events that give back to the surrounding community (Action).
In 2011, Barnard's SGA and McAC will work together to bring back the Greek Games, an old but quite famous Barnard tradition.
Barnard College officially banned sororities in 1913, but Barnard students continue to participate in Columbia's five National Panhellenic Conference sororities—Alpha Chi Omega, Alpha Omicron Pi, Delta Gamma, Kappa Alpha Theta, and Sigma Delta Tau—and the National Pan-Hellenic Council Sororities- Alpha Kappa Alpha (Lambda chapter) and Delta Sigma Theta (Rho chapter) as well as other sororities in the Multicultural Greek Council. Two National Panhellenic Conference organizations were founded at Barnard College. The Alpha Omicron Pi Fraternity, founded on January 2, 1897, left campus during the 1913 ban but returned to establish its Alpha chapter in 2013. The Alpha Epsilon Phi, founded on October 24, 1909, is no longer on campus. As of 2010, Barnard does not fully recognize the National Panhellenic Conference sororities at Columbia, but it does provide some funding to account for Barnard students living in Columbia housing through these organizations.
Athletics.
Barnard athletes compete in the Ivy League (NCAA Division I) through the Columbia/Barnard Athletic Consortium, which was established in 1983. Through this arrangement, Barnard is the only women's college offering Division I athletics. There are 15 intercollegiate teams, and students also compete at the intramural and club levels.
From 1975–1983, before the establishment of the Columbia/Barnard Athletic Consortium, Barnard students competed as the "Barnard Bears". Prior to 1975, students referred to themselves as the "Barnard honeybears".
Seven Sisters—student collaborations.
Established within the Barnard Student Government Association (SGA), The Seven Sisters Governing Board represents Barnard College as part of the Seven Sisters Coalition, which is a group of representatives from student councils of the historic Seven Sisters colleges. The reps on the coordinating board of Seven Sisters Coalition are rotating every year to hold the annual Seven Sisters Conference in a serious but informal setting. The first Seven Sisters Conference was hosted by SGA student representatives at Barnard College in 2009. In fall 2013, the conference was hosted by Vassar college during the first weekend of November. The major topic focused on inner college collaborations and differences in student government structures among Seven Sisters Colleges. The Seven Sisters Coordinating Board of Barnard brought six Barnard student representatives to attend the Fall Semester conference, which was hosted at Vassar College in the past fall semester. Based on the Coalition Coordinating Board Constitution established in February 2013, Students delegates were initiating projects in the aspects of public relations,alumni outreach and website management to promote the presence and development of the seven sisters culture. Meanwhile, The Barnard delegates engaged in discussions about the various structures of the student governments among the historic seven sisters colleges.
Sustainability.
Barnard College has issued a statement affirming its commitment to environmental sustainability, a major part of which is the goal of reducing its greenhouse gas emissions by 30% by 2017. Student EcoReps work as a resource on environmental issues for students in Barnard's residence halls, while the student-run Earth Coalition works on outreach initiatives such as local park clean-ups, tutoring elementary school students in environmental education, and sponsoring environmental forums. Barnard earned a "C-" for its sustainability efforts on the College Sustainability Report Card 2009 published by the Sustainable Endowments Institute. Its highest marks were in Student Involvement and Food and Recycling, receiving a "B" in both categories.
Liberal arts requirements.
The liberal arts requirements are called the Nine Ways of Knowing. Students must take one year of one laboratory science, study a single foreign language for four semesters, and complete one 3-credit course in each of the following categories: reason and value, social analysis, historical studies, cultures in comparison, quantitative and deductive reasoning, literature, and visual and performing arts. The use of AP or IB credit to fulfill these requirements is very limited, but Nine Ways of Knowing courses may overlap with major or minor requirements. In addition to the Nine Ways of Knowing, students must complete a first-year seminar, a first-year English course, and one semester of physical education.
Controversies.
In the spring of 1960 Columbia University president Grayson Kirk complained to the president of Barnard that Barnard students were wearing inappropriate clothing. The garments in question were pants and Bermuda shorts. The administration forced the student council to institute a dress code. Students would be allowed to wear shorts and pants only at Barnard and only if the shorts were no more than two inches above the knee and the pants were not tight. Barnard women crossing the street to enter the Columbia campus wearing shorts or pants were required to cover themselves with a long coat similar to a jilbab.
In March 1968, "The New York Times" ran an article on students who cohabited, identifying one of the persons they interviewed as a student at Barnard College from New Hampshire named "Susan". Barnard officials searched their records for women from New Hampshire and were able to determine that "Susan" was the pseudonym of a student (Linda LeClair) who was living with her boyfriend, a student at Columbia University. She was called before Barnard's student-faculty administration judicial committee, where she faced the possibility of expulsion. A student protest included a petition signed by 300 other Barnard women, admitting that they too had broken the regulations against cohabitating. The judicial committee reached a compromise and the student was allowed to remain in school, but was denied use of the college cafeteria and barred from all social activities. The student briefly became a focus of intense national attention. She eventually dropped out of Barnard.
References.
Notes
Sources

</doc>
<doc id="4240" url="https://en.wikipedia.org/wiki?curid=4240" title="Order of Saint Benedict">
Order of Saint Benedict

The Order of Saint Benedict (OSB; Latin: "Ordo Sancti Benedicti"), also knownin reference to the color of its members' habitsas the Black Monks, is a Catholic religious order of independent monastic communities that observe the Rule of Saint Benedict. Each community (monastery, priory or abbey) within the order maintains its own autonomy, while the order itself represents their mutual interests. The terms "Order of Saint Benedict" and "Benedictine Order" are, however, also used to refer to "all" Benedictine communities collectively, sometimes giving the incorrect impression that there exists a generalate or motherhouse with jurisdiction over them.
Internationally, the order is governed by the Benedictine Confederation, a body, established in 1883 by Pope Leo XIII's Brief "Summum semper", whose head is known as the Abbot Primate. Individuals whose communities are members of the order generally add the initials "OSB" after their names.
Historical development.
The monastery at Subiaco in Italy, established by Saint Benedict of Nursia circa 529, was the first of the dozen monasteries he founded. He later founded the Abbey of Monte Cassino. There is no evidence, however, that he intended to found an order and the Rule of Saint Benedict presupposes the autonomy of each community. When Monte Cassino was sacked by the Lombards about the year 580, the monks fled to Rome, and it seems probable that this constituted an important factor in the diffusion of a knowledge of Benedictine monasticism.
It was from the monastery of St. Andrew in Rome that Augustine, the prior, and his forty companions set forth in 595 on their mission for the evangelization of England. At various stopping places during the journey, the monks left behind them traditions concerning their rule and form of life, and probably also some copies of the Rule. Lérins Abbey, for instance, founded by Honoratus in 375, probably received its first knowledge of the Benedictine Rule from the visit of St. Augustine and his companions in 596.
Gregory of Tours says that at Ainay, in the sixth century, the monks "followed the rules of Basil, Cassian, Caesarius, and other fathers, taking and using whatever seemed proper to the conditions of time and place", and doubtless the same liberty was taken with the Benedictine Rule when it reached them. In Gaul and Switzerland, it supplemented the much stricter Irish or Celtic Rule introduced by Columbanus and others. In many monasteries it eventually entirely displaced the earlier codes.
By the ninth century, however, the Benedictine had become the standard form of monastic life throughout the whole of Western Europe, excepting Scotland, Wales, and Ireland, where the Celtic observance still prevailed for another century or two. Largely through the work of Benedict of Aniane, it became the rule of choice for monasteries throughout the Carolingian empire.
Monastic scriptoria flourished from the ninth through the twelfth centuries. Sacred Scripture was always at the heart of every monastic scriptorium. As a general rule those of the monks who possessed skill as writers made this their chief, if not their sole active work. An anonymous writer of the ninth or tenth century speaks of six hours a day as the usual task of a scribe, which would absorb almost all the time available for active work in the day of a medieval monk.
In the Middle Ages monasteries were often founded by the nobility. Cluny Abbey was founded by William I, Duke of Aquitaine in 910. The abbey was noted for its strict adherence to the Rule of St. Benedict. The abbot of Cluny was the superior of all the daughter houses, through appointed priors.
One of the earliest reforms of Benedictine practice was that initiated in 980 by Romuald, who founded the Camaldolese community.
England.
The English Benedictine Congregation is the oldest of the nineteen Benedictine congregations. Augustine of Canterbury and his monks established the first English Benedictine monastery at Canterbury soon after their arrival in 597. Other foundations quickly followed. Through the influence of Wilfrid, Benedict Biscop, and Dunstan, the Benedictine Rule spread with extraordinary rapidity, and in the North it was adopted in most of the monasteries that had been founded by the Celtic missionaries from Iona. Many of the episcopal sees of England were founded and governed by the Benedictines, and no less than nine of the old cathedrals were served by the black monks of the priories attached to them. Monasteries served as hospitals and places of refuge for the weak and homeless. The monks studied the healing properties of plants and minerals to alleviate the sufferings of the sick.
Germany was evangelized by English Benedictines. Willibrord and Boniface preached there in the seventh and eighth centuries and founded several abbeys.
In the English Reformation, all monasteries were dissolved and their lands confiscated by the Crown, forcing their Catholic members to flee into exile on the Continent. During the 19th century they were able to return to England, including to Selby Abbey in Yorkshire, one of the few great monastic churches to survive the Dissolution.
St. Mildred's Priory, on the Isle of Thanet, Kent, was built in 1027 on the site of an abbey founded in 670 by the daughter of the first Christian King of Kent. Currently the priory is home to a community of Benedictine nuns. Five of the most notable English abbeys are the Basilica of St Gregory the Great at Downside, commonly known as Downside Abbey, The Abbey of St Edmund, King and Martyr commonly known as Douai Abbey in Upper Woolhampton, Reading, Berkshire, Ealing Abbey in Ealing, West London, St. Lawrence's in Yorkshire (Ampleforth Abbey), and Worth Abbey. Prinknash Abbey, used by Henry VIII as a hunting lodge, was officially returned to the Benedictines four hundred years later, in 1928. During the next few years, so-called Prinknash Park was used as a home until it was returned to the order.
Since the Oxford Movement, there has also been a modest flourishing of Benedictine monasticism in the Anglican Church and Protestant Churches. Anglican Benedictine Abbots are invited guests of the Benedictine Abbot Primate in Rome at Abbatial gatherings at Sant'Anselmo. There are an estimated 2,400 celibate Anglican Religious (1,080 men and 1,320 women) in the Anglican Communion as a whole, some of whom have adopted the Rule of St. Benedict.
As of 2015, the English Congregation consists of three abbeys of nuns and ten abbeys of monks. Members of the congregation are found in England, Wales, the United States of America, Peru and Zimbabwe.
Monastic Libraries in England.
The forty-eighth rule of Saint Benedict prescribes extensive and habitual "holy reading" for the brethren. Three primary types of reading were done by the monks during this time. Monks would read privately during their personal time, as well as publicly during services and at meal times. In addition to these three mentioned in the Rule, monks would also read in the infirmary.
However, Benedictine monks were disallowed worldly possessions, thus necessitating the preservation and collection of sacred texts in monastic libraries for communal use. For the sake of convenience, the books in the monastery were housed in a few different places, namely the sacristy, which contained books for the choir and other liturgical books, the rectory, which housed books for public reading such as sermons and lives of the saints, and the library, which contained the largest collection of books and was typically in the cloister.
The first record of a monastic library in England is in Canterbury. To assist with Augustine of Canterbury's English mission, Pope Gregory the Great gave him nine books which included the Gregorian Bible in two volumes, the Psalter of Augustine, two copies of the Gospels, two martyrologies, an Exposition of the Gospels and Epistles, and a Psalter. Theodore of Tarsus brought Greek books to Canterbury more than seventy years later, when he founded a school for the study of Greek.
France.
Monasteries were among the institutions of the Catholic Church swept away during the French Revolution. Monasteries were again allowed to form in the 19th century under the Bourbon Restoration. Later that century, under the Third French Republic, laws were enacted preventing religious teaching. The original intent was to allow secular schools. Thus in 1880 and 1882, Benedictine teaching monks were effectively exiled; this was not completed until 1901.
United States.
The first Benedictine to live in the United States was Pierre-Joseph Didier. He came to the United States in 1790 from Paris and served in the Ohio and St. Louis areas until his death. The first actual Benedictine monastery founded was St. Vincent, located in Latrobe, Pennsylvania. It was founded in 1832 by Bonifice Wimmer, a German monk, who sought to serve German immigrants in America. In 1856, Wimmer started to lay the foundations for St. John's Abbey in Minnesota. By his death in 1887, Wimmer had sent Benedictine monks to Kansas, New Jersey, North Carolina, Georgia, Florida, Alabama, Illinois, and Colorado.
Wilmer also asked for Benedictine sisters to be sent to America by St. Walburg's Convent in Eichstätt, Bavaria. In 1852, St. Benedict Riepp and two others sisters founded St. Mary's in Pennsylvania. Soon they would send sisters to Michigan, New Jersey, and Minnesota.
By 1854, Swiss monks began to arrive and founded St. Meinrad's Abbey in Indiana, and they soon spread to Arkansas and Louisiana. They were soon followed by Swiss sisters.
There are now over 100 Benedictine houses across America. Most Benedictine houses are part of one of four large Federations: American-Cassinese, Swiss-American, St. Scholastica, and St. Benedict. The federations mostly are made up of monasteries that share the same lineage. For instance the American-Cassinese Federation included the 22 monasteries that descended from Boniface Wimmer.
Organization.
Today, Benedictine monasticism is fundamentally different from other Western religious orders insofar as its individual communities are not part of a religious order with "Generalates" and "Superiors General". Rather, in modern times, the various autonomous houses have formed themselves loosely into congregations (for example, Cassinese, English, Solesmes, Subiaco, Camaldolese, Sylvestrines) that in turn are represented in the Benedictine Confederation that came into existence through Pope Leo XIII's Apostolic Brief "Summum semper" on 12 July 1883. This organization facilitates dialogue of Benedictine communities with each other and the relationship between Benedictine communities and other religious orders and the church at large. 
The Rule of Saint Benedict is also used by a number of religious orders that began as reforms of the Benedictine tradition such as the Cistercians and Trappists although none of these groups are part of the Benedictine Confederation. 
The largest number of Benedictines are Roman Catholics, but there are also some within the Anglican Communion and occasionally within other Christian denominations as well, for example, within the Lutheran Church. 
Benedictine vow and life.
Section 17 in chapter 58 of the Rule of Saint Benedict states the solemn promise candidates for reception into a Benedictine community are required to make: a promise of stability (i.e. to remain in the same community), "conversatio morum" (an idiomatic Latin phrase suggesting "conversion of manners"; see below) and obedience (to the community's superior, seen as holding the place of Christ within it). This solemn commitment tends to be referred to as the "Benedictine vow" and is the Benedictine antecedent and equivalent of the evangelical counsels professed by candidates for reception into a religious order.
Much scholarship over the last fifty years has been dedicated to the translation and interpretation of "conversatio morum". The older translation "conversion of life" has generally been replaced with phrases such as "onversion t a monastic manner of life", drawing from the Vulgate's use of "conversatio" as a translation of "citizenship" or "homeland" in Philippians 3:20. Some scholars have claimed that the vow formula of the Rule is best translated as "to live in this place as a monk, in obedience to its rule and abbot."
Benedictine abbots and abbesses have full jurisdiction of their abbey and thus absolute authority over the monks or nuns who are resident. This authority includes the power to assign duties, to decide which books may or may not be read, to regulate comings and goings, and to punish and to excommunicate, in the sense of an enforced isolation from the monastic community.
A tight communal timetablethe horariumis meant to ensure that the time given by God is not wasted but used in God's service, whether for prayer, work, meals, spiritual reading or sleep.
Although Benedictines do not take a vow of silence, hours of strict silence are set, and at other time silence is maintained as much as is practically possible. Social conversations tend to be limited to communal recreation times. But such details, like the many other details of the daily routine of a Benedictine house that the Rule of St Benedict leaves to the discretion of the superior, are set out in its 'customary'. A ' customary' is the code adopted by a particular Benedictine house, adapting the Rule to local conditions. 
In the Roman Catholic Church, according to the nuns of the 1983 Code of Canon Law, a Benedictine abbey is a "religious institute" and its members are therefore members of the consecrated life. While Canon Law 588 §1 explains that Benedictine monks are "neither clerical nor lay", they can, however, be ordained. Benedictine Oblates endeavor to embrace the spirit of the Benedictine vow in their own life in the world.

</doc>
<doc id="4241" url="https://en.wikipedia.org/wiki?curid=4241" title="Bayezid I">
Bayezid I

Bayezid I (; ; nicknamed "Yıldırım" (Ottoman Turkish: ییلدیرم), "The Lightning"; 1360 – 8 March 1403) was the Ottoman Sultan from 1389 to 1402. He was the son of Murad I and Gülçiçek Hatun.
Biography.
The first major role of Bayezid was as governor of Kütahya, city that was conquered from the Germiyanids. He was an impetuous soldier, earning the nickname of Lightning in a battle against the Karamanids.
Bayezid ascended to the throne following the death of his father Murad I, who was killed by Serbian knight Miloš Obilić during (15 June), or immediately after (16 June), the Battle of Kosovo in 1389, by which Serbia became a vassal of the Ottoman Sultanate. Immediately after obtaining the throne, he had his younger brother strangled to avoid a plot. In 1390, Bayezid took as a wife Princess Olivera Despina, the daughter of Prince Lazar of Serbia, who also lost his life in Kosovo. Bayezid recognized Stefan Lazarević, the son of Lazar, as the new Serbian leader (later despot), with considerable autonomy.
The upper Serbia resisted the Ottomans until general Pashayigit captured the city of Skopje in 1391, converting the city to an important base of operations.
Meanwhile, the sultan began unifying Anatolia under his rule. Forcible expansion into Muslim territories could endanger the Ottoman relationship with the gazis, who were an important source of warriors for this ruling house on the European frontier. So Bayezid began the practice to first secure "fatwa"s, or legal rulings from Islamic scholars, justifying their wars against these Muslim states. However he suspected the loyalty of his Muslim Turkoman followers, for Bayezid relied heavily on his Serbian and Byzantine vassal troops to perform these conquests.
In a single campaign over the summer and fall of 1390, Bayezid conquered the beyliks of Aydin, Saruhan and Menteşe. His major rival Sulayman, the emir of Karaman, responded by allying himself with the ruler of Sivas, Kadi Burhan al-Din and the remaining Turkish beyliks. Nevertheless, Bayezid pushed on and in the fall and winter of 1390 overwhelmed the remaining beyliks -- Hamid, Teke, and Germiyan—as well as taking the cities of Akşehir and Niğde, as well as their capital Konya from the Karaman. At this point, Bayezid accepted peace proposals from Karaman (1391), concerned that further advances would antagonize his Turkoman followers and lead them to ally with Kadi Burhan al-Din. Once peace had been made with Karaman, Bayezid moved north against Kastamonu which had given refuge to many fleeing from his forces, and conquered both that city as well as Sinop.
From 1389 to 1395 he conquered Bulgaria and northern Greece. In 1394 Bayezid crossed the River Danube to attack Wallachia, ruled at that time by Mircea the Elder. The Ottomans were superior in number, but on 10 October 1394 (or 17 May 1395), in the Battle of Rovine, on forested and swampy terrain, the Wallachians won the fierce battle and prevented Bayezid's army from advancing beyond the Danube.
In 1394, Bayezid laid siege to Constantinople, the capital of the Byzantine Empire. Anadoluhisarı fortress was built between 1393 and 1394 as part of preparations for the Second Ottoman Siege of Constantinople, which took place in 1395. On the urgings of the Byzantine emperor Manuel II Palaeologus a new crusade was organized to defeat him. This proved unsuccessful: in 1396 the Christian allies, under the leadership of the King of Hungary and future Holy Roman Emperor (in 1433) Sigismund, were defeated in the Battle of Nicopolis. Bayezid built the magnificent Ulu Cami in Bursa, to celebrate this victory.
Thus the siege of Constantinople continued, lasting until 1402. The beleaguered Byzantines had their reprieve when Bayezid fought the Timurid Empire in the East. At this time, the empire of Bayezid included Thrace (except Constantinople), Macedonia, Bulgaria, and parts of Serbia in Europe. In Asia, his domains extended to the Taurus Mountains. His army was considered one of the best in the Islamic world. In 1400, the Central Asian warlord Timur succeeded in rousing the local Turkic beyliks who had been vassals of the Ottomans to join him in his attack on Bayezid, who was also considered one of the most powerful rulers in the Muslim world during that period. In the fateful Battle of Ankara, on 20 July 1402, Bayezid was captured by Timur and the Ottoman army was defeated. Many writers claim that Bayezid was mistreated by the Timurids. However, writers and historians from Timur's own court reported that Bayezid was treated well, and that Timur even mourned his death. One of Bayezid's sons, Mustafa Çelebi, was captured with him and held captive in Samarkand until 1405.
Four of Bayezid's sons, specifically Süleyman Çelebi, İsa Çelebi, Mehmed Çelebi, and Musa Çelebi, however, escaped from the battlefield and later started a civil war for the Ottoman throne known as the Ottoman Interregnum. After Mehmed's victory, his coronation as Mehmed I, and the death of all four but Mehmed, Bayezid's other son Mustafa Çelebi emerged from hiding and began two failed rebellions against his brother Mehmed and, after Mehmed's death, his nephew Murat II.
Legacy.
A commando battalion in the Pakistan Army is named Yaldaram Battalion after him.
Yildirim Beyazit University, a state university in Turkey, is also named after him.
Marriages and progeny.
His mother was Gülçiçek Hatun who was of ethnic Greek descent.
In fiction.
The defeat of Bayezid became a popular subject for later Western writers, composers, and painters. They embellished the legend that he was taken by Timur to Samarkand with a cast of characters to create an oriental fantasy that has maintained its appeal. Christopher Marlowe's play "Tamburlaine the Great" was first performed in London in 1587, three years after the formal opening of English-Ottoman trade relations when William Harborne sailed for Constantinople as an agent of the Levant Company.
In 1648, the play "Le Gran Tamerlan et Bejezet" by Jean Magnon appeared in London, and in 1725, Handel's "Tamerlano" was first performed and published in London; Vivaldi's version of the story, "Bajazet", was written in 1735. Magnon had given Bayezid an intriguing wife and daughter; the Handel and Vivaldi renditions included, as well as Tamerlane and Bayezid and his daughter, a prince of Byzantium and a princess of Trebizond (Trabzon) in a passionate love story. A cycle of paintings in Schloss Eggenberg, near Graz in Austria, translated the theme to a different medium; this was completed in the 1670s shortly before the Ottoman army attacked the Habsburgs in central Europe.
Bayezid (spelled Bayazid) is a central character in the Robert E. Howard story "Lord of Samarcand."
ged 47–4

</doc>
<doc id="4242" url="https://en.wikipedia.org/wiki?curid=4242" title="Bayezid II">
Bayezid II

Bayezid II or Sultân Bayezid-î Velî (December 3, 1447 – May 26, 1512) (Ottoman Turkish: بايزيد ثانى "Bāyezīd-i sānī", Turkish:"II. Bayezid" or "II. Beyazıt") was the eldest son and successor of Mehmed II, ruling as Sultan of the Ottoman Empire from 1481 to 1512. During his reign, Bayezid II consolidated the Ottoman Empire and thwarted a Safavid rebellion soon before abdicating his throne to his son, Selim I. He is most notable for evacuating Jews from Spain after the proclamation of the Alhambra Decree and resettling them throughout the Ottoman Empire.
Early life.
Bayezid II was the son of Mehmed II (1432–81). His mother's identity is undetermined; there are two main theories, that she was Emine Gülbahar Hatun or Sitti Mükrime Hatun.
Bayezid II married Gülbahar Hatun, who was the mother of his eldest son Şehzade Ahmet, as well as Bayezid II's heir and successor, Selim I and nephew of Sitti Mükrime Hatun.
Fight for the throne.
Bayezid II's overriding concern was the quarrel with his brother Cem, who claimed the throne and sought military backing from the Mamluks in Egypt. Having been defeated by his brother's armies, Cem sought protection from the Knights of St. John in Rhodes. Eventually, the Knights handed Cem over to Pope Innocent VIII (1484–1492). The Pope thought of using Cem as a tool to drive the Turks out of Europe, but, as the papal crusade failed to come to fruition, Cem was left to languish and die in a Neapolitan prison. Bayezid II paid both the Knights Hospitaller and the pope to keep his brother prisoner.
Reign.
Bayezid II ascended the Ottoman throne in 1481. Like his father, Bayezid II was a patron of western and eastern culture and unlike many other Sultans, worked hard to ensure a smooth running of domestic politics, which earned him the epithet of "the Just". Throughout his reign, Bayezid II engaged in numerous campaigns to conquer the Venetian possessions in Morea, accurately defining this region as the key to future Ottoman naval power in the Eastern Mediterranean. The last of these wars ended in 1501 with Bayezid II in control of the whole Peloponnese. Rebellions in the east, such as that of the Qizilbash, plagued much of Bayezid II's reign and were often backed by the Shah of Persia, Ismail, who was eager to promote Shi'ism to undermine the authority of the Ottoman state. Ottoman authority in Anatolia was indeed seriously threatened during this period, and at one point Bayezid II's grand vizier, Ali Pasha, was killed in battle against rebels.
Jewish and Muslim immigration.
In July 1492, the new state of Spain expelled its Jewish and Muslim populations as part of the Spanish Inquisition. Bayezid II sent out the Ottoman Navy under the command of Admiral Kemal Reis to Spain in 1492 in order to evacuate them safely to Ottoman lands. He sent out proclamations throughout the empire that the refugees were to be welcomed. He granted the refugees the permission to settle in the Ottoman Empire and become Ottoman citizens. He ridiculed the conduct of Ferdinand II of Aragon and Isabella I of Castile in expelling a class of people so useful to their subjects. "You venture to call Ferdinand a wise ruler," he said to his courtiers — "he who has impoverished his own country and enriched mine!" Bayezid addressed a firman to all the governors of his European provinces, ordering them not only to refrain from repelling the Spanish refugees, but to give them a friendly and welcome reception. He threatened with death all those who treated the Jews harshly or refused them admission into the empire. Moses Capsali, who probably helped to arouse the sultan's friendship for the Jews, was most energetic in his assistance to the exiles. He made a tour of the communities, and was instrumental in imposing a tax upon the rich, to ransom the Jewish victims of the persecutions then prevalent.
The Muslims and Jews of al-Andalus (Iberia) contributed much to the rising power of the Ottoman Empire by introducing new ideas, methods and craftsmanship. The first printing press in Constantinople was established by the Sephardic Jews in 1493. It is reported that under Bayezid's reign, Jews enjoyed a period of cultural flourishing, with the presence of such scholars as the Talmudist and scientist Mordecai Comtino; astronomer and poet Solomon ben Elijah Sharbiṭ ha-Zahab; Shabbethai ben Malkiel Cohen, and the liturgical poet Menahem Tamar.
Succession.
On September 14, 1509, Constantinople was devastated by an earthquake. Bayezid II's final years saw a succession battle between his sons Selim I and Ahmet. Ahmet unexpectedly captured Karaman, an Ottoman city, and began marching to Constantinople to exploit his triumph. Fearing for his safety, Selim staged a revolt in Thrace but was defeated by Bayezid and forced to flee back to the Crimean Peninsula. Bayezid II developed fears that Ahmet might in turn kill him to gain the throne and refused to allow his son to enter Constantinople.
Selim returned from Crimea and, with support from the Janissaries, forced his father to abdicate the throne on April 25, 1512. Beyazid departed for retirement in his native Demotika, but he died on May 26, 1512 at Büyükçekmece before reaching his destination, and only a month after his abdication. He was buried next to the Bayezid Mosque in Istanbul.

</doc>
<doc id="4243" url="https://en.wikipedia.org/wiki?curid=4243" title="Boxing">
Boxing

Boxing is a martial art and combat sport in which two people throw punches at each other, usually with gloved hands. Historically, the goals have been to weaken and knock down the opponent.
Amateur boxing is both an Olympic and Commonwealth sport and is a common fixture in most international games—it also has its own World Championships. Boxing is supervised by a referee over a series of one- to three-minute intervals called rounds. The result is decided when an opponent is deemed incapable to continue by a referee, is disqualified for breaking a rule, resigns by throwing in a towel, or is pronounced the winner or loser based on the judges' scorecards at the end of the contest. In the event that both fighters gain equal scores from the judges, the fight is considered a draw (professional boxing). In Olympic boxing, due to the fact that a winner must be declared, in the case of a draw - the judges use technical criteria to choose the most deserving winner of the bout.
While people have fought in hand-to-hand combat since before the dawn of history, the origin of boxing as an organized sport may be its acceptance by the ancient Greeks as an Olympic game in BC 688. Boxing evolved from 16th- and 18th-century prizefights, largely in Great Britain, to the forerunner of modern boxing in the mid-19th century, again initially in Great Britain and later in the United States.
History.
Ancient history.
"See also Ancient Greek boxing
The earliest known depiction of boxing comes from a Sumerian relief in Iraq from the 3rd millennium BC. Later depictions from the 2nd millennium BC are found in reliefs from the Mesopotamian nations of Assyria and Babylonia, and in Hittite art from Asia Minor. The earliest evidence for fist fighting with any kind of gloves can be found on Minoan Crete (c. 1500–900 BC), and on Sardinia, if we consider the boxing statues of Prama mountains (c. 2000–1000 BC).
Boxing was a popular spectator sport in Ancient Rome. In order for the fighters to protect themselves against their opponents they wrapped leather thongs around their fists. Eventually harder leather was used and the thong soon became a weapon. The Romans even introduced metal studs to the thongs to make the cestus which then led to a more sinister weapon called the myrmex ('limb piercer'). Fighting events were held at Roman Amphitheatres. The Roman form of boxing was often a fight until death to please the spectators who gathered at such events. However, especially in later times, purchased slaves and trained combat performers were valuable commodities, and their lives were not given up without due consideration. Often slaves were used against one another in a circle marked on the floor. This is where the term ring came from. In AD 393, during the Roman gladiator period, boxing was abolished due to excessive brutality. It was not until the late 17th century that boxing re-surfaced in London.
Early London prize ring rules.
Records of Classical boxing activity disappeared after the fall of the Western Roman Empire when the wearing of weapons became common once again and interest in fighting with the fists waned. However, there are detailed records of various fist-fighting sports that were maintained in different cities and provinces of Italy between the 12th and 17th centuries. There was also a sport in ancient Rus called Kulachniy Boy or "Fist Fighting".
As the wearing of swords became less common, there was renewed interest in fencing with the fists. The sport would later resurface in England during the early 16th century in the form of bare-knuckle boxing sometimes referred to as prizefighting. The first documented account of a bare-knuckle fight in England appeared in 1681 in the "London Protestant Mercury", and the first English bare-knuckle champion was James Figg in 1719. This is also the time when the word "boxing" first came to be used. It should be noted, that this earliest form of modern boxing was very different. Contests in Mr. Figg's time, in addition to fist fighting, also contained fencing and cudgeling. On 6 January 1681, the first recorded boxing match took place in Britain when Christopher Monck, 2nd Duke of Albemarle (and later Lieutenant Governor of Jamaica) engineered a bout between his butler and his butcher with the latter winning the prize.
Early fighting had no written rules. There were no weight divisions or round limits, and no referee. In general, it was extremely chaotic. An early article on boxing was published in Nottingham, 1713, by Sir Thomas Parkyns, a successful Wrestler from Bunny, Nottinghamshire, who had practised the techniques he described. The article, a single page in his manual of wrestling and fencing, "Progymnasmata: The inn-play, or Cornish-hugg wrestler", described a system of headbutting, punching, eye-gouging, chokes, and hard throws, not recognized in boxing today.
The first boxing rules, called the Broughton's rules, were introduced by champion Jack Broughton in 1743 to protect fighters in the ring where deaths sometimes occurred. Under these rules, if a man went down and could not continue after a count of 30 seconds, the fight was over. Hitting a downed fighter and grasping below the waist were prohibited. Broughton encouraged the use of 'mufflers', a form of padded bandage or mitten, to be used in 'jousting' or sparring sessions in training, and in exhibition matches.
These rules did allow the fighters an advantage not enjoyed by today's boxers; they permitted the fighter to drop to one knee to begin a 30-second count at any time. Thus a fighter realizing he or she was in trouble had an opportunity to recover. However, this was considered "unmanly" and was frequently disallowed by additional rules negotiated by the Seconds of the Boxers. Intentionally going down in modern boxing will cause the recovering fighter to lose points in the scoring system. Furthermore, as the contestants did not have heavy leather gloves and wristwraps to protect their hands, they used different punching technique to preserve their hands because the head was a common target to hit full out as almost all period manuals have powerful straight punches with the whole body behind them to the face (including forehead) as the basic blows.
Marquess of Queensberry rules (1867).
In 1867, the Marquess of Queensberry rules were drafted by John Chambers for amateur championships held at Lillie Bridge in London for Lightweights, Middleweights and Heavyweights. The rules were published under the patronage of the Marquess of Queensberry, whose name has always been associated with them.
There were twelve rules in all, and they specified that fights should be "a fair stand-up boxing match" in a 24-foot-square or similar ring. Rounds were three minutes with one-minute rest intervals between rounds. Each fighter was given a ten-second count if he or she were knocked down, and wrestling was banned.
The introduction of gloves of "fair-size" also changed the nature of the bouts. An average pair of boxing gloves resembles a bloated pair of mittens and are laced up around the wrists.
The gloves can be used to block an opponent's blows. As a result of their introduction, bouts became longer and more strategic with greater importance attached to defensive maneuvers such as slipping, bobbing, countering and angling. Because less defensive emphasis was placed on the use of the forearms and more on the gloves, the classical forearms outwards, torso leaning back stance of the bare knuckle boxer was modified to a more modern stance in which the torso is tilted forward and the hands are held closer to the face.
Late 19th and early 20th centuries.
Through the late nineteenth century, the martial art of boxing or prizefighting was primarily a sport of dubious legitimacy. Outlawed in England and much of the United States, prizefights were often held at gambling venues and broken up by police. Brawling and wrestling tactics continued, and riots at prizefights were common occurrences. Still, throughout this period, there arose some notable bare knuckle champions who developed fairly sophisticated fighting tactics.
The English case of "R v. Coney" in 1882 found that a bare-knuckle fight was an assault occasioning actual bodily harm, despite the consent of the participants. This marked the end of widespread public bare-knuckle contests in England.
The first world heavyweight champion under the Queensberry Rules was "Gentleman Jim" Corbett, who defeated John L. Sullivan in 1892 at the Pelican Athletic Club in New Orleans.
The first instance of film censorship in the United States occurred in 1897 when several states banned the showing of prize fighting films from the state of Nevada, where it was legal at the time.
Throughout the early twentieth century, boxers struggled to achieve legitimacy. They were aided by the influence of promoters like Tex Rickard and the popularity of great champions such as John L. Sullivan.
Rules.
The "Marquess of Queensberry rules" have been the general rules governing modern boxing since their publication in 1867.
A boxing match typically consists of a determined number of three-minute rounds, a total of up to 9 to 12 rounds. A minute is typically spent between each round with the fighters in their assigned corners receiving advice and attention from their coach and staff. The fight is controlled by a referee who works within the ring to judge and control the conduct of the fighters, rule on their ability to fight safely, count knocked-down fighters, and rule on fouls.
Up to three judges are typically present at ringside to score the bout and assign points to the boxers, based on punches that connect, defense, knockdowns, and other, more subjective, measures. Because of the open-ended style of boxing judging, many fights have controversial results, in which one or both fighters believe they have been "robbed" or unfairly denied a victory. Each fighter has an assigned corner of the ring, where his or her coach, as well as one or more "seconds" may administer to the fighter at the beginning of the fight and between rounds. Each boxer enters into the ring from their assigned corners at the beginning of each round and must cease fighting and return to their corner at the signaled end of each round.
A bout in which the predetermined number of rounds passes is decided by the judges, and is said to "go the distance". The fighter with the higher score at the end of the fight is ruled the winner. With three judges, unanimous and split decisions are possible, as are draws. A boxer may win the bout before a decision is reached through a knock-out ; such bouts are said to have ended "inside the distance". If a fighter is knocked down during the fight, determined by whether the boxer touches the canvas floor of the ring with any part of their body other than the feet as a result of the opponent's punch and not a slip, as determined by the referee, the referee begins counting until the fighter returns to his or her feet and can continue.
Should the referee count to ten, then the knocked-down boxer is ruled "knocked out" (whether unconscious or not) and the other boxer is ruled the winner by knockout (KO). A "technical knock-out" (TKO) is possible as well, and is ruled by the referee, fight doctor, or a fighter's corner if a fighter is unable to safely continue to fight, based upon injuries or being judged unable to effectively defend themselves. Many jurisdictions and sanctioning agencies also have a "three-knockdown rule", in which three knockdowns in a given round result in a TKO. A TKO is considered a knockout in a fighter's record. A "standing eight" count rule may also be in effect. This gives the referee the right to step in and administer a count of eight to a fighter that he feels may be in danger, even if no knockdown has taken place. After counting the referee will observe the fighter, and decide if he is fit to continue. For scoring purposes, a standing eight count is treated as a knockdown.
In general, boxers are prohibited from hitting below the belt, holding, tripping, pushing, biting, or spitting. The boxer's shorts are raised so the opponent is not allowed to hit to the groin area with intent to cause pain or injury. Failure to abide by the former may result in a foul. They also are prohibited from kicking, head-butting, or hitting with any part of the arm other than the knuckles of a closed fist (including hitting with the elbow, shoulder or forearm, as well as with open gloves, the wrist, the inside, back or side of the hand). They are prohibited as well from hitting the back, back of the neck or head (called a "rabbit-punch") or the kidneys. They are prohibited from holding the ropes for support when punching, holding an opponent while punching, or ducking below the belt of their opponent (dropping below the waist of your opponent, no matter the distance between).
If a "clinch" – a defensive move in which a boxer wraps his or her opponents arms and holds on to create a pause – is broken by the referee, each fighter must take a full step back before punching again (alternatively, the referee may direct the fighters to "punch out" of the clinch). When a boxer is knocked down, the other boxer must immediately cease fighting and move to the furthest neutral corner of the ring until the referee has either ruled a knockout or called for the fight to continue.
Violations of these rules may be ruled "fouls" by the referee, who may issue warnings, deduct points, or disqualify an offending boxer, causing an automatic loss, depending on the seriousness and intentionality of the foul. An intentional foul that causes injury that prevents a fight from continuing usually causes the boxer who committed it to be disqualified. A fighter who suffers an accidental low-blow may be given up to five minutes to recover, after which they may be ruled knocked out if they are unable to continue. Accidental fouls that cause injury ending a bout may lead to a "no contest" result, or else cause the fight to go to a decision if enough rounds (typically four or more, or at least three in a four-round fight) have passed.
Unheard of these days, but common during the early 20th Century in North America, a "newspaper decision (NWS)" might be made after a no decision bout had ended. A "no decision" bout occurred when, by law or by pre-arrangement of the fighters, if both boxers were still standing at the fight's conclusion and there was no knockout, no official decision was rendered and neither boxer was declared the winner. But this did not prevent the pool of ringside newspaper reporters from declaring a consensus result among themselves and printing a newspaper decision in their publications. Officially, however, a "no decision" bout resulted in neither boxer winning or losing. Boxing historians sometimes use these unofficial newspaper decisions in compiling fight records for illustrative purposes only. Often, media outlets covering a match will personally score the match, and post their scores as an independent sentence in their report.
Professional vs. amateur boxing.
Throughout the 17th to 19th centuries, boxing bouts were motivated by money, as the fighters competed for prize money, promoters controlled the gate, and spectators bet on the result. The modern Olympic movement revived interest in amateur sports, and amateur boxing became an Olympic sport in 1908. In their current form, Olympic and other amateur bouts are typically limited to three or four rounds, scoring is computed by points based on the number of clean blows landed, regardless of impact, and fighters wear protective headgear, reducing the number of injuries, knockdowns, and knockouts. Currently scoring blows in amateur boxing are subjectively counted by ringside judges, but the Australian Institute for Sport has demonstrated a prototype of an Automated Boxing Scoring System, which introduces scoring objectivity, improves safety, and arguably makes the sport more interesting to spectators. Professional boxing remains by far the most popular form of the sport globally, though amateur boxing is dominant in Cuba and some former Soviet republics. For most fighters, an amateur career, especially at the Olympics, serves to develop skills and gain experience in preparation for a professional career.
Amateur boxing.
Amateur boxing may be found at the collegiate level, at the Olympic Games and Commonwealth Games, and in many other venues sanctioned by amateur boxing associations. Amateur boxing has a point scoring system that measures the number of clean blows landed rather than physical damage. Bouts consist of three rounds of three minutes in the Olympic and Commonwealth Games, and three rounds of three minutes in a national ABA (Amateur Boxing Association) bout, each with a one-minute interval between rounds.
Competitors wear protective headgear and gloves with a white strip or circle across the knuckle. There are cases however, where white ended gloves are not required but any solid color may be worn. The white end just is a way to make it easier for judges to score clean hits. Each competitor must have their hands properly wrapped, pre-fight, for added protection on their hands and for added cushion under the gloves. Gloves worn by the fighters must be twelve ounces in weight unless, the fighters weigh under 165 pounds, thus allowing them to wear 10 ounce gloves. A punch is considered a scoring punch only when the boxers connect with the white portion of the gloves. Each punch that lands cleanly on the head or torso with sufficient force is awarded a point. A referee monitors the fight to ensure that competitors use only legal blows. A belt worn over the torso represents the lower limit of punches – any boxer repeatedly landing low blows below the belt is disqualified. Referees also ensure that the boxers don't use holding tactics to prevent the opponent from swinging. If this occurs, the referee separates the opponents and orders them to continue boxing. Repeated holding can result in a boxer being penalized or ultimately disqualified. Referees will stop the bout if a boxer is seriously injured, if one boxer is significantly dominating the other or if the score is severely imbalanced. Amateur bouts which end this way may be noted as "RSC" (referee stopped contest) with notations for an outclassed opponent (RSCO), outscored opponent (RSCOS), injury (RSCI) or head injury (RSCH).
Professional boxing.
Professional bouts are usually much longer than amateur bouts, typically ranging from ten to twelve rounds, though four-round fights are common for less experienced fighters or club fighters. There are also some two- and three-round professional bouts, especially in Australia. Through the early twentieth century, it was common for fights to have unlimited rounds, ending only when one fighter quit, benefiting high-energy fighters like Jack Dempsey. Fifteen rounds remained the internationally recognized limit for championship fights for most of the twentieth century until the early 1980s, when the death of boxer Duk Koo Kim eventually prompted the World Boxing Council and other organizations sanctioning professional boxing to reduce the limit to twelve rounds.
Headgear is not permitted in professional bouts, and boxers are generally allowed to take much more damage before a fight is halted. At any time, the referee may stop the contest if he believes that one participant cannot defend himself due to injury. In that case, the other participant is awarded a technical knockout win. A technical knockout would also be awarded if a fighter lands a punch that opens a cut on the opponent, and the opponent is later deemed not fit to continue by a doctor because of the cut. For this reason, fighters often employ cutmen, whose job is to treat cuts between rounds so that the boxer is able to continue despite the cut. If a boxer simply quits fighting, or if his corner stops the fight, then the winning boxer is also awarded a technical knockout victory. In contrast with amateur boxing, professional male boxers have to be bare-chested.
Boxing styles.
Definition of style.
"Style" is often defined as the strategic approach a fighter takes during a bout. No two fighters' styles are alike, as it is determined by that individual's physical and mental attributes. Three main styles exist in boxing: outside fighter ("boxer"), brawler (or "slugger"), and Inside fighter ("swarmer"). These styles may be divided into several special subgroups, such as counter puncher, etc. The main philosophy of the styles is, that each style has an advantage over one, but disadvantage over the other one. It follows the rock-paper-scissors scenario - boxer beats brawler, brawler beats swarmer, and swarmer beats boxer.
Boxer/out-fighter.
A classic "boxer" or stylist (also known as an "out-fighter") seeks to maintain distance between himself and his opponent, fighting with faster, longer range punches, most notably the jab, and gradually wearing his opponent down. Due to this reliance on weaker punches, out-fighters tend to win by point decisions rather than by knockout, though some out-fighters have notable knockout records. They are often regarded as the best boxing strategists due to their ability to control the pace of the fight and lead their opponent, methodically wearing him down and exhibiting more skill and finesse than a brawler. Out-fighters need reach, hand speed, reflexes, and footwork.
Notable out-fighters include Muhammad Ali, Larry Holmes, Joe Calzaghe Wilfredo Gómez, 
Salvador Sanchez, Cecilia Brækhus, Gene Tunney, Ezzard Charles, Willie Pep, Meldrick Taylor, Ricardo Lopez, Floyd Mayweather, Roy Jones, Jr., Sugar Ray Leonard, Miguel Vazquez, Sergio "Maravilla" Martínez, Vitali Klitschko, Wladimir Klitschko, and Guillermo Rigondeaux. This style was also used by fictional boxer Apollo Creed.
Boxer-puncher.
A boxer-puncher is a well-rounded boxer who is able to fight at close range with a combination of technique and power, often with the ability to knock opponents out with a combination and in some instances a single shot. Their movement and tactics are similar to that of an out-fighter (although they are generally not as mobile as an out-fighter), but instead of winning by decision, they tend to wear their opponents down using combinations and then move in to score the knockout. A boxer must be well rounded to be effective using this style.
Notable boxer-punchers include Muhammad Ali, Wladimir Klitschko, Lennox Lewis, Joe Louis, Wilfredo Gómez, Oscar de la Hoya, Archie Moore, Miguel Cotto, Nonito Donaire, Sam Langford, Henry Armstrong, Sugar Ray Robinson, Tony Zale, Carlos Monzón, Alexis Argüello, Erik Morales, Terry Norris, Marco Antonio Barrera, Naseem Hamed, Thomas Hearns ,
Counter puncher.
Counter punchers are slippery, defensive style fighters who often rely on their opponent's mistakes in order to gain the advantage, whether it be on the score cards or more preferably a knockout. They use their well-rounded defense to avoid or block shots and then immediately catch the opponent off guard with a well placed and timed punch. A fight with a skilled counter-puncher can turn into a war of attrition, where each shot landed is a battle in itself. Thus, fighting against counter punchers requires constant feinting and the ability to avoid telegraphing ones attacks. To be truly successful using this style they must have good reflexes, a high level of prediction and awareness, pinpoint accuracy and speed, both in striking and in footwork.
Notable counter punchers include Muhammad Ali, Vitali Klitschko, Evander Holyfield, Max Schmeling, Chris Byrd, Jim Corbett, Jack Johnson, Bernard Hopkins, Laszlo Papp, Jerry Quarry, Anselmo Moreno, James Toney, Marvin Hagler, Juan Manuel Márquez, Humberto Soto, Floyd Mayweather, Jr., Roger Mayweather, Pernell Whitaker, Sergio Gabriel Martinez and Guillermo Rigondeaux.
Counter punchers usually wear their opponents down by causing them to miss their punches. The more the opponent misses, the faster they tire, and the psychological effects of being unable to land a hit will start to sink in. The counter puncher often tries to outplay their opponent entirely, not just in a physical sense, but also in a mental and emotional sense. This style can be incredibly difficult, especially against seasoned fighters, but winning a fight without getting hit is often worth the pay-off. They usually try to stay away from the center of the ring, in order to outmaneuver and chip away at their opponents. A large advantage in counter-hitting is the forward momentum of the attacker, which drives them further into your return strike. As such, knockouts are more common than one would expect from a defensive style.
Brawler/slugger.
A brawler is a fighter who generally lacks finesse and footwork in the ring, but makes up for it through sheer punching power. Mainly Irish, Irish-American, Puerto Rican, Mexican, and Mexican-American boxers popularized this style. Many brawlers tend to lack mobility, preferring a less mobile, more stable platform and have difficulty pursuing fighters who are fast on their feet. They may also have a tendency to ignore combination punching in favor of continuous beat-downs with one hand and by throwing slower, more powerful single punches (such as hooks and uppercuts). Their slowness and predictable punching pattern (single punches with obvious leads) often leaves them open to counter punches, so successful brawlers must be able to absorb substantial amounts of punishment. However, not all brawler/slugger fighters are not mobile; some can move around and switch styles if needed but still have the brawler/slugger style such as Wilfredo Gómez, Prince Naseem Hamed and Danny García.
A brawler's most important assets are power and chin (the ability to absorb punishment while remaining able to continue boxing). Examples of this style include George Foreman, Rocky Marciano, Julio Cesar Chavez, Roberto Duran, Danny García, Wilfredo Gómez, Sonny Liston, John L. Sullivan, Max Baer, Prince Naseem Hamed, Ray Mancini, David Tua, Arturo Gatti, Micky Ward, Brandon Ríos, Ruslan Provodnikov, Michael Katsidis, James Kirkland, Marcos Maidana, Jake Lamotta, Manny Pacquiao, and Ireland's John Duddy. This style of boxing was also used by fictional boxers Rocky Balboa and James "Clubber" Lang.
Brawlers tend to be more predictable and easy to hit but usually fare well enough against other fighting styles because they train to take punches very well. They often have a higher chance than other fighting styles to score a knockout against their opponents because they focus on landing big, powerful hits, instead of smaller, faster attacks. Oftentimes they place focus on training on their upper body instead of their entire body, to increase power and endurance. They also aim to intimidate their opponents because of their power, stature and ability to take a punch.
Swarmer/in-fighter.
In-fighters/swarmers (sometimes called "pressure fighters") attempt to stay close to an opponent, throwing intense flurries and combinations of hooks and uppercuts. A successful in-fighter often needs a good "chin" because swarming usually involves being hit with many jabs before they can maneuver inside where they are more effective. In-fighters operate best at close range because they are generally shorter and have less reach than their opponents and thus are more effective at a short distance where the longer arms of their opponents make punching awkward. However, several fighters tall for their division have been relatively adept at in-fighting as well as out-fighting.
The essence of a swarmer is non-stop aggression. Many short in-fighters utilize their stature to their advantage, employing a bob-and-weave defense by bending at the waist to slip underneath or to the sides of incoming punches. Unlike blocking, causing an opponent to miss a punch disrupts his balance, permits forward movement past the opponent's extended arm and keeps the hands free to counter. A distinct advantage that in-fighters have is when throwing uppercuts where they can channel their entire bodyweight behind the punch; Mike Tyson was famous for throwing devastating uppercuts. Marvin Hagler was known for his hard "chin", punching power, body attack and the stalking of his opponents. Some in-fighters, like Mike Tyson, have been known for being notoriously hard to hit. The key to a swarmer is aggression, endurance, chin, and bobbing-and-weaving.
Notable in-fighters include Julio César Chávez, Miguel Cotto, Joe Frazier, Danny García, Mike Tyson, Manny Pacquiao, Saúl Álvarez, Rocky Marciano, Jack Dempsey, Wayne McCullough, Gerry Penalosa, Harry Greb, David Tua, Ricky Hatton and Gennady Golovkin.
Combinations of styles.
All fighters have primary skills with which they feel most comfortable, but truly elite fighters are often able to incorporate auxiliary styles when presented with a particular challenge. For example, an out-fighter will sometimes plant his feet and counter punch, or a slugger may have the stamina to pressure fight with his power punches.
Style matchups.
There is a generally accepted rule of thumb about the success each of these boxing styles has against the others. In general, an in-fighter has an advantage over an out-fighter, an out-fighter has an advantage over a brawler, and a brawler has an advantage over an in-fighter; these form a cycle with each style being stronger relative to one, and weaker relative to another, with none dominating, as in rock-paper-scissors. Naturally, many other factors, such as the skill level and training of the combatants, determine the outcome of a fight, but the widely held belief in this relationship among the styles is embodied in the cliché amongst boxing fans and writers that "styles make fights."
Brawlers tend to overcome swarmers or in-fighters because, in trying to get close to the slugger, the in-fighter will invariably have to walk straight into the guns of the much harder-hitting brawler, so, unless the former has a very good chin and the latter's stamina is poor, the brawler's superior power will carry the day. A famous example of this type of match-up advantage would be George Foreman's knockout victory over Joe Frazier in their original bout "The Sunshine Showdown".
Although in-fighters struggle against heavy sluggers, they typically enjoy more success against out-fighters or boxers. Out-fighters prefer a slower fight, with some distance between themselves and the opponent. The in-fighter tries to close that gap and unleash furious flurries. On the inside, the out-fighter loses a lot of his combat effectiveness, because they cannot throw the hard punches. The in-fighter is generally successful in this case, due to their intensity in advancing on their opponent and their good agility, which makes them difficult to evade. For example, the swarming Joe Frazier, though easily dominated by the slugger George Foreman, was able to create many more problems for the boxer Muhammad Ali in their three fights. Joe Louis, after retirement, admitted that he hated being crowded, and that swarmers like untied/undefeated champ Rocky Marciano would have caused him style problems even in his prime.
The boxer or out-fighter tends to be most successful against a brawler, whose slow speed (both hand and foot) and poor technique makes them an easy target to hit for the faster out-fighter. The out-fighter's main concern is to stay alert, as the brawler only needs to land one good punch to finish the fight. If the out-fighter can avoid those power punches, they can often wear the brawler down with fast jabs, tiring them out. If they are successful enough, they may even apply extra pressure in the later rounds in an attempt to achieve a knockout. Most classic boxers, such as Muhammad Ali, enjoyed their best successes against sluggers.
An example of a style matchup was the historical fight of Julio César Chávez, a swarmer or in-fighter, against Meldrick Taylor, the boxer or out-fighter (see Julio César Chávez vs. Meldrick Taylor). The match was nicknamed "Thunder Meets Lightning" as an allusion to punching power of Chávez and blinding speed of Taylor. Chávez was the epitome of the "Mexican" style of boxing. Taylor's hand and foot speed and boxing abilities gave him the early advantage, allowing him to begin building a large lead on points. Chávez remained relentless in his pursuit of Taylor and due to his greater punching power Chávez slowly punished Taylor. Coming into the later rounds, Taylor was bleeding from the mouth, his entire face was swollen, the bones around his eye socket had been broken, he had swallowed a considerable amount of his own blood, and as he grew tired, Taylor was increasingly forced into exchanging blows with Chávez, which only gave Chávez a greater chance to cause damage. While there was little doubt that Taylor had solidly won the first three quarters of the fight, the question at hand was whether he would survive the final quarter. Going into the final round, Taylor held a secure lead on the scorecards of two of the three judges. Chávez would have to knock Taylor out to claim a victory, whereas Taylor merely needed to stay away from the Mexican legend. However, Taylor did not stay away, but continued to trade blows with Chávez. As he did so, Taylor showed signs of extreme exhaustion, and every tick of the clock brought Taylor closer to victory unless Chávez could knock him out.
With about a minute left in the round, Chávez hit Taylor squarely with several hard punches and stayed on the attack, continuing to hit Taylor with well-placed shots. Finally, with about 25 seconds to go, Chávez landed a hard right hand that caused Taylor to stagger forward towards a corner, forcing Chávez back ahead of him. Suddenly Chávez stepped around Taylor, positioning him so that Taylor was trapped in the corner, with no way to escape from Chávez' desperate final flurry. Chávez then nailed Taylor with a tremendous right hand that dropped the younger man. By using the ring ropes to pull himself up, Taylor managed to return to his feet and was given the mandatory 8-count. Referee Richard Steele asked Taylor twice if he was able to continue fighting, but Taylor failed to answer. Steele then concluded that Taylor was unfit to continue and signaled that he was ending the fight, resulting in a TKO victory for Chávez with only two seconds to go in the bout.
Equipment.
Since boxing involves forceful, repetitive punching, precautions must be taken to prevent damage to bones in the hand. Most trainers do not allow boxers to train and spar without wrist wraps and boxing gloves. Hand wraps are used to secure the bones in the hand, and the gloves are used to protect the hands from blunt injury, allowing boxers to throw punches with more force than if they did not utilize them. Gloves have been required in competition since the late nineteenth century, though modern boxing gloves are much heavier than those worn by early twentieth-century fighters. Prior to a bout, both boxers agree upon the weight of gloves to be used in the bout, with the understanding that lighter gloves allow heavy punchers to inflict more damage. The brand of gloves can also affect the impact of punches, so this too is usually stipulated before a bout. Both sides are allowed to inspect the wraps and gloves of the opponent to help ensure both are within agreed upon specifications and no tampering has taken place.
A mouth guard is important to protect the teeth and gums from injury, and to cushion the jaw, resulting in a decreased chance of knockout. Both fighters must wear soft soled shoes to reduce the damage from accidental (or intentional) stepping on feet. While older boxing boots more commonly resembled those of a professional wrestler, modern boxing shoes and boots tend to be quite similar to their amateur wrestling counterparts.
Boxers practice their skills on two basic types of punching bags. A small, tear-drop-shaped "speed bag" is used to hone reflexes and repetitive punching skills, while a large cylindrical "heavy bag" filled with sand, a synthetic substitute, or water is used to practice power punching and body blows. In addition to these distinctive pieces of equipment, boxers also utilize sport-nonspecific training equipment to build strength, speed, agility, and stamina. Common training equipment includes free weights, rowing machines, jump rope, and medicine balls.
Boxing matches typically take place in a boxing ring, a raised platform surrounded by ropes attached to posts rising in each corner. The term "ring" has come to be used as a metaphor for many aspects of prize fighting in general.
Technique.
Stance.
The modern boxing stance differs substantially from the typical boxing stances of the 19th and early 20th centuries. The modern stance has a more upright vertical-armed guard, as opposed to the more horizontal, knuckles-facing-forward guard adopted by early 20th century hook users such as Jack Johnson.
In a fully upright stance, the boxer stands with the legs shoulder-width apart and the rear foot a half-step in front of the lead man. Right-handed or orthodox boxers lead with the left foot and fist (for most penetration power). Both feet are parallel, and the right heel is off the ground. The lead (left) fist is held vertically about six inches in front of the face at eye level. The rear (right) fist is held beside the chin and the elbow tucked against the ribcage to protect the body. The chin is tucked into the chest to avoid punches to the jaw which commonly cause knock-outs and is often kept slightly off-center. Wrists are slightly bent to avoid damage when punching and the elbows are kept tucked in to protect the ribcage. Some boxers fight from a crouch, leaning forward and keeping their feet closer together. The stance described is considered the "textbook" stance and fighters are encouraged to change it around once it's been mastered as a base. Case in point, many fast fighters have their hands down and have almost exaggerated footwork, while brawlers or bully fighters tend to slowly stalk their opponents.
Left-handed or southpaw fighters use a mirror image of the orthodox stance, which can create problems for orthodox fighters unaccustomed to receiving jabs, hooks, or crosses from the opposite side. The southpaw stance, conversely, is vulnerable to a straight right hand.
North American fighters tend to favor a more balanced stance, facing the opponent almost squarely, while many European fighters stand with their torso turned more to the side. The positioning of the hands may also vary, as some fighters prefer to have both hands raised in front of the face, risking exposure to body shots.
Modern boxers can sometimes be seen tapping their cheeks or foreheads with their fists in order to remind themselves to keep their hands up (which becomes difficult during long bouts). Boxers are taught to push off with their feet in order to move effectively. Forward motion involves lifting the lead leg and pushing with the rear leg. Rearward motion involves lifting the rear leg and pushing with the lead leg. During lateral motion the leg in the direction of the movement moves first while the opposite leg provides the force needed to move the body.
Punches.
There are four basic punches in boxing: the jab, cross, hook and uppercut. Any punch other than a jab is considered a power punch. If a boxer is right-handed (orthodox), his left hand is the lead hand and his right hand is the rear hand. For a left-handed boxer or southpaw, the hand positions are reversed. For clarity, the following discussion will assume a right-handed boxer.
These different punch types can be thrown in rapid succession to form combinations or "combos." The most common is the jab and cross combination, nicknamed the "one-two combo." This is usually an effective combination, because the jab blocks the opponent's view of the cross, making it easier to land cleanly and forcefully.
A large, swinging circular punch starting from a cocked-back position with the arm at a longer extension than the hook and all of the fighter's weight behind it is sometimes referred to as a "roundhouse," "haymaker," or sucker-punch. Relying on body weight and centripetal force within a wide arc, the roundhouse can be a powerful blow, but it is often a wild and uncontrolled punch that leaves the fighter delivering it off balance and with an open guard.
Wide, looping punches have the further disadvantage of taking more time to deliver, giving the opponent ample warning to react and counter. For this reason, the haymaker or roundhouse is not a conventional punch, and is regarded by trainers as a mark of poor technique or desperation. Sometimes it has been used, because of its immense potential power, to finish off an already staggering opponent who seems unable or unlikely to take advantage of the poor position it leaves the puncher in.
Another unconventional punch is the rarely used bolo punch, in which the opponent swings an arm out several times in a wide arc, usually as a distraction, before delivering with either that or the other arm.
An illegal punch to the back of the head or neck is known as a rabbit punch.
Defense.
There are several basic maneuvers a boxer can use in order to evade or block punches, depicted and discussed below.
Philly Shell or Shoulder roll defense -This is actually a variation of the cross-arm defense. The lead arm (left for an orthodox fighter and right for a southpaw) is placed across the torso usually somewhere in between the belly button and chest and the lead hand rests on the opposite side of the fighter's torso. The back hand is placed on the side of the face (right side for orthodox fighters and left side for southpaws). The lead shoulder is brought in tight against the side of the face (left side for orthodox fighters and right side for southpaws). This style is used by fighters who like to counterpunch.
To execute this guard a fighter must be very athletic and experienced. This style is so effective for counterpunching because it allows fighters to slip punches by rotating and dipping their upper body and causing blows to glance off the fighter. After the punch glances off, the fighter's back hand is in perfect position to hit their out-of-position opponent. The shoulder lean is used in this stance. To execute the shoulder lean a fighter rotates and ducks (to the right for orthodox fighters and to the left for southpaws) when their opponents punch is coming towards them and then rotates back towards their opponent while their opponent is bringing their hand back.
The fighter will throw a punch with their back hand as they are rotating towards their undefended opponent. The weakness to this style is that when a fighter is stationary and not rotating they are open to be hit so a fighter must be athletic and well conditioned to effectively execute this style. To beat this style, fighters like to jab their opponents shoulder causing the shoulder and arm to be in pain and to demobilize that arm. Fighters that used this defense include Sugar Ray Robinson, Ken Norton (also used this defense), Pernell Whitaker, James Toney, and Floyd Mayweather Jr.
Floyd Mayweather, Jr. employed the use of a check hook against Ricky Hatton, which sent Hatton flying head first into the corner post and being knocked down. Hatton managed to get himself to his feet after the knockdown but was clearly dazed and it was only a matter of moments before Mayweather landed a flurry of punches which sent Hatton crashing to the canvas, giving Mayweather a TKO victory in the 10th round and handing Hatton his first defeat.
Ring corner.
In boxing, each fighter is given a corner of the ring where he rests in between rounds for 1 minute and where his trainers stand. Typically, three men stand in the corner besides the boxer himself; these are the trainer, the assistant trainer and the cutman. The trainer and assistant typically give advice to the boxer on what he is doing wrong as well as encouraging him if he is losing. The cutman is a cutaneous doctor responsible for keeping the boxer's face and eyes free of cuts and blood. This is of particular importance because many fights are stopped because of cuts that threaten the boxer's eyes.
In addition, the corner is responsible for stopping the fight if they feel their fighter is in grave danger of permanent injury. The corner will occasionally throw in a white towel to signify a boxer's surrender (the idiomatic phrase "to throw in the towel", meaning to give up, derives from this practice). This can be seen in the fight between Diego Corrales and Floyd Mayweather. In that fight, Corrales' corner surrendered despite Corrales' steadfast refusal.
Medical concerns.
Knocking a person unconscious or even causing concussion may cause permanent brain damage. There is no clear division between the force required to knock a person out and the force likely to kill a person. Since 1980, more than 200 amateur boxers, professional boxers and Toughman fighters have died due to ring or training injuries. In 1983, the "Journal of the American Medical Association" called for a ban on boxing. The editor, Dr. George Lundberg, called boxing an "obscenity" that "should not be sanctioned by any civilized society." Since then, the British, Canadian and Australian Medical Associations also have called for bans on boxing.
Supporters of the ban state that boxing is the only sport where hurting the other athlete is the goal. Dr. Bill O'Neill, boxing spokesman for the British Medical Association, has supported the BMA's proposed ban on boxing: "It is the only sport where the intention is to inflict serious injury on your opponent, and we feel that we must have a total ban on boxing." Opponents respond that such a position is misguided opinion, stating that amateur boxing is scored solely according to total connecting blows with no award for "injury". They observe that many skilled professional boxers have had rewarding careers without inflicting injury on opponents by accumulating scoring blows and avoiding punches winning rounds scored 10-9 by the 10-point must system, and they note that there are many other sports where concussions are much more prevalent.
In 2007, one study of amateur boxers showed that protective headgear did not prevent brain damage, and another found that amateur boxers faced a high risk of brain damage. The Gothenburg study analyzed temporary levels of neurofiliment light in cerebral spinal fluid which they conclude is evidence of damage, even though the levels soon subside. More comprehensive studies of neurologiocal function on larger samples performed by Johns Hopkins University and accident rates analyzed by National Safety Council show amateur boxing is a comparatively safe sport.
In 1997, the American Association of Professional Ringside Physicians was established to create medical protocols through research and education to prevent injuries in boxing.
Professional boxing is forbidden in Iceland, Iran and North Korea. It was banned in Sweden until 2007 when the ban was lifted but strict restrictions, including four three-minute rounds for fights, were imposed. It was banned in Albania from 1965 till the fall of Communism in 1991; it is now legal there. Norway legalized professional boxing in December 2014.
Boxing Hall of Fame.
The sport of boxing has two internationally recognized boxing halls of fame; the International Boxing Hall of Fame (IBHOF) and the World Boxing Hall of Fame (WBHF), with the IBHOF being the more widely recognized boxing hall of fame.
The WBHF was founded by Everett L. Sanders in 1980. Since its inception the WBHOF has never had a permanent location or museum, which has allowed the more recent IBHOF to garner more publicity and prestige. Among the notable names in the WBHF are Ricardo "Finito" Lopez, Gabriel "Flash" Elorde, Michael Carbajal, Khaosai Galaxy, Henry Armstrong, Jack Johnson, Roberto Durán, George Foreman, Ceferino Garcia and Salvador Sanchez. Boxing's International Hall of Fame was inspired by a tribute an American town held for two local heroes in 1982. The town, Canastota, New York, (which is about east of Syracuse, via the New York State Thruway), honored former world welterweight/middleweight champion Carmen Basilio and his nephew, former world welterweight champion Billy Backus. The people of Canastota raised money for the tribute which inspired the idea of creating an official, annual hall of fame for notable boxers.
The International Boxing Hall of Fame opened in Canastota in 1989. The first inductees in 1990 included Jack Johnson, Benny Leonard, Jack Dempsey, Henry Armstrong, Sugar Ray Robinson, Archie Moore, and Muhammad Ali. Other world-class figures include Salvador Sanchez, Jose Napoles, Roberto "Manos de Piedra" Durán, Ricardo Lopez, Gabriel "Flash" Elorde, Vicente Saldivar, Ismael Laguna, Eusebio Pedroza, Carlos Monzón, Azumah Nelson, Rocky Marciano, Pipino Cuevas and Ken Buchanan. The Hall of Fame's induction ceremony is held every June as part of a four-day event.
The fans who come to Canastota for the Induction Weekend are treated to a number of events, including scheduled autograph sessions, boxing exhibitions, a parade featuring past and present inductees, and the induction ceremony itself.
Boxer rankings.
There are various organizations and websites, that rank boxers in both weight class and pound-for-pound manner.

</doc>
<doc id="4246" url="https://en.wikipedia.org/wiki?curid=4246" title="Bollywood">
Bollywood

Bollywood is the sobriquet for the Hindi language film industry, based in Mumbai, the capital of Maharashtra, the second most populous state of the Republic of India. The term is often incorrectly used as a synecdoche to refer to the whole of Indian cinema; however, it is only a part of the large Indian film industry, which includes other production centres producing films in many languages. 
Bollywood is one of the largest film producers in India, representing 43% of net box office revenue, while Telugu cinema, and Tamil cinema representing 36%, and rest of the regional cinema constitutes 21% as of 2014, bollywood is also one of the largest centers of film production in the world. It is more formally referred to as Hindi cinema. Bollywood is classified as the biggest movie industry in the world in terms of amount of people employed and number of films produced. In just 2011 alone, over 3.5 billion tickets were sold across the globe which in comparison is 900,000 tickets more than Hollywood. Also in comparison, Bollywood makes approximately 1,041 films yearly, as opposed to less than 500 films made by Hollywood yearly.
Etymology.
The name "Bollywood" is a portmanteau derived from Bombay (the former name for Mumbai) and Hollywood, the center of the American film industry. However, unlike Hollywood, Bollywood does not exist as a physical place. Some deplore the name, arguing that it makes the industry look like a poor cousin to Hollywood.
The naming scheme for "Bollywood" was inspired by "Tollywood", the name that was used to refer to the cinema of West Bengal. Dating back to 1932, "Tollywood" was the earliest Hollywood-inspired name, referring to the Bengali film industry based in Tollygunge, Calcutta, whose name is reminiscent of "Hollywood" and was the centre of the cinema of India at the time. It was this "chance juxtaposition of two pairs of rhyming syllables," Holly and Tolly, that led to the portmanteau name "Tollywood" being coined. The name "Tollywood" went on to be used as a nickname for the Bengali film industry by the popular Kolkata-based "Junior Statesman" youth magazine, establishing a precedent for other film industries to use similar-sounding names, eventually leading to the term "Bollywood" being coined. However, more popularly, Tollywood is now used to refer to the Telugu Film Industry in Telangana & Andhra Pradesh. The term "Bollywood" itself has origins in the 1970s, when India overtook America as the world's largest film producer. Credit for the term has been claimed by several different people, including the lyricist, filmmaker and scholar Amit Khanna, and the journalist Bevinda Collaco.
History.
"Raja Harishchandra" (1913), by Dadasaheb Phalke, is known as the first silent feature film made in India. By the 1930s, the industry was producing over 200 films per annum. The first Indian sound film, Ardeshir Irani's "Alam Ara" (1931), was a major commercial success. There was clearly a huge market for talkies and musicals; Bollywood and all the regional film industries quickly switched to sound filming.
The 1930s and 1940s were tumultuous times: India was buffeted by the Great Depression, World War II, the Indian independence movement, and the violence of the Partition. Most Bollywood films were unabashedly escapist, but there were also a number of filmmakers who tackled tough social issues, or used the struggle for Indian independence as a backdrop for their plots.
In 1937, Ardeshir Irani, of "Alam Ara" fame, made the first colour film in Hindi, "Kisan Kanya". The next year, he made another colour film, a version of "Mother India". However, colour did not become a popular feature until the late 1950s. At this time, lavish romantic musicals and melodramas were the staple fare at the cinema.
Golden Age.
Following India's independence, the period from the late 1940s to the 1960s is regarded by film historians as the "Golden Age" of Hindi cinema. Some of the most critically acclaimed Hindi films of all time were produced during this period. Examples include the Guru Dutt films "Pyaasa" (1957) and "Kaagaz Ke Phool" (1959) and the Raj Kapoor films "Awaara" (1951), "Shree 420" (1955) and Dilip Kumar's "Aan" (1952). These films expressed social themes mainly dealing with working-class urban life in India; "Awaara" presented the city as both a nightmare and a dream, while "Pyaasa" critiqued the unreality of city life. Some of the most famous epic films of Hindi cinema were also produced at the time, including Mehboob Khan's "Mother India" (1957), which was nominated for the Academy Award for Best Foreign Language Film, and K. Asif's "Mughal-e-Azam" (1960). "Madhumati" (1958), directed by Bimal Roy and written by Ritwik Ghatak, popularised the theme of reincarnation in Western popular culture. Other acclaimed mainstream Hindi filmmakers at the time included Kamal Amrohi and Vijay Bhatt. Successful actors at the time included Dev Anand, Dilip Kumar, Raj Kapoor and Guru Dutt, while successful actresses included Nargis, Vyjayanthimala, Meena Kumari, Nutan, Madhubala, Waheeda Rehman and Mala Sinha.
While commercial Hindi cinema was thriving, the 1950s also saw the emergence of a new Parallel Cinema movement. Though the movement was mainly led by Bengali cinema, it also began gaining prominence in Hindi cinema. Early examples of Hindi films in this movement include Chetan Anand's "Neecha Nagar" (1946) and Bimal Roy's "Do Bigha Zamin" (1953). Their critical acclaim, as well as the latter's commercial success, paved the way for Indian neorealism and the "Indian New Wave". Some of the internationally acclaimed Hindi filmmakers involved in the movement included Mani Kaul, Kumar Shahani, Ketan Mehta, Govind Nihalani, Shyam Benegal and Vijaya Mehta.
Ever since the social realist film "Neecha Nagar" won the Grand Prize at the first Cannes Film Festival, Hindi films were frequently in competition for the Palme d'Or at the Cannes Film Festival throughout the 1950s and early 1960s, with some of them winning major prizes at the festival. Guru Dutt, while overlooked in his own lifetime, had belatedly generated international recognition much later in the 1980s. Dutt is now regarded as one of the greatest Asian filmmakers of all time, alongside the more famous Indian Bengali filmmaker Satyajit Ray. The 2002 "Sight & Sound" critics' and directors' poll of greatest filmmakers ranked Dutt at No. 73 on the list. Some of his films are now included among the greatest films of all time, with "Pyaasa" (1957) being featured in Time magazine's "All-TIME" 100 best movies list, and with both "Pyaasa" and "Kaagaz Ke Phool" (1959) tied at No. 160 in the 2002 "Sight & Sound" critics' and directors' poll of all-time greatest films. Several other Hindi films from this era were also ranked in the "Sight & Sound" poll, including Raj Kapoor's "Awaara" (1951), Vijay Bhatt's "Baiju Bawra" (1952), Mehboob Khan's "Mother India" (1957) and K. Asif's "Mughal-e-Azam" (1960) all tied at No. 346 on the list.
Modern cinema.
In the late 1960s and early 1970s, romance movies and action films starred actors like Rajesh Khanna, Dharmendra, Sanjeev Kumar and Shashi Kapoor and actresses like Sharmila Tagore, Mumtaz and Asha Parekh. In the mid-1970s, romantic confections made way for gritty, violent films about gangsters (see Indian mafia) and bandits. Amitabh Bachchan, the star known for his "angry young man" roles, rode the crest of this trend with actors like Mithun Chakraborty, Anil Kapoor and Sunny Deol, which lasted into the early 1990s. Actresses from this era included Hema Malini, Jaya Bachchan and Rekha.
Some Hindi filmmakers such as Shyam Benegal continued to produce realistic Parallel Cinema throughout the 1970s, alongside Mani Kaul, Kumar Shahani, Ketan Mehta, Govind Nihalani and Vijaya Mehta. However, the 'art film' bent of the Film Finance Corporation came under criticism during a Committee on Public Undertakings investigation in 1976, which accused the body of not doing enough to encourage commercial cinema. The 1970s thus saw the rise of commercial cinema in the form of enduring films such as "Sholay" (1975), which consolidated Amitabh Bachchan's position as a lead actor. The devotional classic "Jai Santoshi Ma" was also released in 1975. Another important film from 1975 was "Deewar", directed by Yash Chopra and written by Salim-Javed. A crime film pitting "a policeman against his brother, a gang leader based on real-life smuggler Haji Mastan", portrayed by Amitabh Bachchan; it was described as being "absolutely key to Indian cinema" by Danny Boyle. The most internationally acclaimed Hindi film of the 1980s was Mira Nair's "Salaam Bombay!" (1988), which won the Camera d'Or at the 1988 Cannes Film Festival and was nominated for the Academy Award for Best Foreign Language Film.
During the late 1980s and early 1990s, the pendulum swung back toward family-centric romantic musicals with the success of such films as "Qayamat Se Qayamat Tak" (1988), "Maine Pyar Kiya" (1989), "Dil" (1990), "Hum Aapke Hain Kaun" (1994), "Dilwale Dulhania Le Jayenge" (1995) and "Kuch Kuch Hota Hai" (1998) making stars of a new generation of actors (such as Aamir Khan, Salman Khan and Shahrukh Khan) and actresses (such as Madhuri Dixit, Sridevi, Juhi Chawla). In that point of time, action and comedy films were also successful, with actors like Govinda and actresses such as Raveena Tandon and Karisma Kapoor appearing in popular comedy films, and stunt actor Akshay Kumar gaining popularity for performing dangerous stunts in action films in his well known Khiladi (film series) and other action films. Furthermore, this decade marked the entry of new performers in arthouse and independent films, some of which succeeded commercially, the most influential example being "Satya" (1998), directed by Ram Gopal Varma and written by Anurag Kashyap. The critical and commercial success of "Satya" led to the emergence of a distinct genre known as "Mumbai noir", urban films reflecting social problems in the city of Mumbai. This led to a resurgence of Parallel Cinema by the end of the decade. These films often featured actors like Nana Patekar, Manoj Bajpai, Manisha Koirala, Tabu and Urmila Matondkar, whose performances were usually critically acclaimed.
The 2000s saw a growth in Bollywood's popularity across the world. This led the nation's filmmaking to new heights in terms of production values, cinematography and innovative story lines as well as technical advances in areas such as special effects and animation. Some of the largest production houses, among them Yash Raj Films and Dharma Productions were the producers of new modern films. Some popular films of the decade were "Koi... Mil Gaya" (2003), "Kal Ho Naa Ho" (2003), "Veer-Zaara" (2004), "Dhoom" (2004), "Hum Tum" (2004), "Dhoom 2" (2006), "Krrish" (2006), and "Jab We Met" (2007). These films starred established actors. However, the mid-2000s also saw the rise of popular actors like Hrithik Roshan, Saif Ali Khan, Shahid Kapoor, and Abhishek Bachchan, as well as actresses like Rani Mukerji, Preity Zinta, Aishwarya Rai, Kareena Kapoor, and Priyanka Chopra.
In the early 2010s, established actors like Salman Khan and Akshay Kumar became known for making big-budget "masala" entertainers like "Dabangg" and "Rowdy Rathore" opposite younger actresses like Sonakshi Sinha. These films were often not the subject of critical acclaim, but were nonetheless major commercial successes. While most stars from the 2000s continued their successful careers into the next decade, the 2010s also saw the rise of a new generation of actors like Ranbir Kapoor, Imran Khan, Ranveer Singh, and Arjun Kapoor, as well as actresses like Vidya Balan, Katrina Kaif, Deepika Padukone, Kangana Ranaut, Anushka Sharma, and Parineeti Chopra.
Hindi films can achieve distribution across at least 22 of India’s 29 states. The Hindi film industry has preferred films that appeal to all segments of the audience (see the discussion in Ganti, 2004, cited in references), and has resisted making films that target narrower audiences. It was believed that aiming for a broad spectrum would maximise box office receipts. However, filmmakers may be moving towards accepting some box-office segmentation, between films that appeal to rural Indians, and films that appeal to urban and international audiences.
Influences for Bollywood.
Gokulsing and Dissanayake identify six major influences that have shaped the conventions of Indian popular cinema:
Influence of Bollywood.
Perhaps the biggest influence of Bollywood has been on nationalism in India itself, where along with rest of Indian cinema, it has become part and parcel of the 'Indian story'. In the words of the economist and Bollywood biographer Lord Meghnad Desai,
Cinema actually has been the most vibrant medium for telling India its own story, the story of its struggle for independence, its constant struggle to achieve national integration and to emerge as a global presence.
In the 2000s, Bollywood began influencing musical films in the Western world, and played a particularly instrumental role in the revival of the American musical film genre. Baz Luhrmann stated that his musical film "Moulin Rouge!" (2001) was directly inspired by Bollywood musicals. The film incorporated an Indian-themed play based on the ancient Sanskrit drama "Mṛcchakatika" and a Bollywood-style dance sequence with a song from the film "China Gate". The critical and financial success of "Moulin Rouge!" renewed interest in the then-moribund Western musical genre, and subsequently films such as "Chicago, The Producers, Rent", "Dreamgirls", "Hairspray", "", "Across the Universe", "The Phantom of the Opera", "Enchanted" and "Mamma Mia!" were produced, fuelling a renaissance of the genre.
A. R. Rahman, an Indian film composer, wrote the music for Andrew Lloyd Webber's "Bombay Dreams", and a musical version of "Hum Aapke Hain Koun" has played in London's West End. The Bollywood musical "Lagaan" (2001) was nominated for the Academy Award for Best Foreign Language Film, and two other Bollywood films "Devdas" (2002) and "Rang De Basanti" (2006) were nominated for the BAFTA Award for Best Film Not in the English Language. Danny Boyle's "Slumdog Millionaire" (2008), which has won four Golden Globes and eight Academy Awards, was also directly inspired by Bollywood films, and is considered to be a "homage to Hindi commercial cinema". The theme of reincarnation was also popularised in Western popular culture through Bollywood films, with "Madhumati" (1958) inspiring the Hollywood film "The Reincarnation of Peter Proud" (1975), which in turn inspired the Bollywood film "Karz" (1980), which in turn influenced another Hollywood film "Chances Are" (1989). The 1975 film "Chhoti Si Baat" is believed to have inspired "Hitch" (2005), which in turn inspired the Bollywood film "Partner" (2007).
The influence of Bollywood "filmi" music can also be seen in popular music elsewhere in the world. In 1978, technopop pioneers Haruomi Hosono and Ryuichi Sakamoto of the Yellow Magic Orchestra produced an electronic album "Cochin Moon" based on an experimental fusion between electronic music and Bollywood-inspired Indian music. Devo's 1988 hit song "Disco Dancer" was inspired by the song "I am a Disco Dancer" from the Bollywood film "Disco Dancer" (1982). The 2002 song "Addictive", sung by Truth Hurts and produced by DJ Quik and Dr. Dre, was lifted from Lata Mangeshkar's "Thoda Resham Lagta Hai" from "Jyoti" (1981). The Black Eyed Peas' Grammy Award winning 2005 song "Don't Phunk with My Heart" was inspired by two 1970s Bollywood songs: "Ye Mera Dil Yaar Ka Diwana" from "Don" (1978) and "Ae Nujawan Hai Sub" from "Apradh" (1972). Both songs were originally composed by Kalyanji Anandji, sung by Asha Bhosle, and featured the dancer Helen. Also in 2005, the Kronos Quartet re-recorded several R. D. Burman compositions, with Asha Bhosle as the singer, into an album "You've Stolen My Heart: Songs from R.D. Burman's Bollywood", which was nominated for "Best Contemporary World Music Album" at the 2006 Grammy Awards. "Filmi" music composed by A. R. Rahman (who would later win two Academy Awards for the "Slumdog Millionaire" soundtrack) has frequently been sampled by musicians elsewhere in the world, including the Singaporean artist Kelly Poon, the Uzbek artist Iroda Dilroz, the French rap group La Caution, the American artist Ciara, and the German band Löwenherz, among others. Many Asian Underground artists, particularly those among the overseas Indian diaspora, have also been inspired by Bollywood music.
Genre conventions.
Bollywood films are mostly musicals and are expected to contain catchy music in the form of song-and-dance numbers woven into the script. A film's success often depends on the quality of such musical numbers. Indeed, a film's music is often released before the movie and helps increase the audience.
Indian audiences expect full value for their money, with a good entertainer generally referred to as "paisa" "vasool", (literally, "money's worth"). Songs and dances, love triangles, comedy and dare-devil thrills are all mixed up in a three-hour extravaganza with an intermission. They are called "masala" films, after the Hindi word for a spice mixture. Like "masalas", these movies are a mixture of many things such as action, comedy, romance and so on. Most films have heroes who are able to fight off villains all by themselves.
Bollywood plots have tended to be melodramatic. They frequently employ formulaic ingredients such as star-crossed lovers and angry parents, love triangles, family ties, sacrifice, corrupt politicians, kidnappers, conniving villains, courtesans with hearts of gold, long-lost relatives and siblings separated by fate, dramatic reversals of fortune, and convenient coincidences.
There have always been Indian films with more artistic aims and more sophisticated stories, both inside and outside the Bollywood tradition (see Parallel Cinema). They often lost out at the box office to movies with more mass appeal. Bollywood conventions are changing, however. A large Indian diaspora in English-speaking countries, and increased Western influence at home, have nudged Bollywood films closer to Hollywood models.
Film critic Lata Khubchandani writes, "our earliest films ... had liberal doses of sex and kissing scenes in them. Strangely, it was after Independence the censor board came into being and so did all the strictures." Plots now tend to feature Westernised urbanites dating and dancing in clubs rather than centring on pre-arranged marriages. Though these changes can widely be seen in contemporary Bollywood, traditional conservative ways of Indian culture continue to exist in India outside the industry and an element of resistance by some to western-based influences. Despite this, Bollywood continues to play a major role in fashion in India. Some studies into fashion in India have revealed that some people are unaware that the changing nature of fashion in Bollywood films are often influenced by globalisation; many consider the clothes worn by Bollywood actors as authentically Indian.
Cast and crew.
Bollywood employs people from all parts of India. It attracts thousands of aspiring actors and actresses, all hoping for a break in the industry. Models and beauty contestants, television actors, theatre actors and even common people come to Mumbai with the hope and dream of becoming a star. Just as in Hollywood, very few succeed. Since many Bollywood films are shot abroad, many foreign extras are employed too.
Very few non-Indian actors are able to make a mark in Bollywood, though many have tried from time to time. There have been some exceptions, of which one recent example is the hit film "Rang De Basanti", where the lead actress is Alice Patten, an Englishwoman. "Kisna", "Lagaan", and "" also featured foreign actors. Of late, Emma Brown Garett, an Australian born actress, has starred in a few Indian films.
Bollywood can be very clannish, and the relatives of film-industry insiders have an edge in getting coveted roles in films or being part of a film's crew. However, industry connections are no guarantee of a long career: competition is fierce and if film industry scions do not succeed at the box office, their careers will falter. Some of the biggest stars, such as Shah Rukh Khan, Madhuri Dixit, Rajesh Khanna, Dharmendra, and Akshay Kumar have succeeded despite a lack of any show business connections. For film clans, see List of Hindi film clans.
Sound.
Sound in Bollywood films was once rarely recorded on location (otherwise known as sync sound). Therefore, the sound was usually created (or re-created) entirely in the studio, with the actors reciting their lines as their images appear on-screen in the studio in the process known as "looping in the sound" or ADR—with the foley and sound effects added later. This created several problems, since the sound in these films usually occurs a frame or two earlier or later than the mouth movements or gestures. The actors had to act twice: once on-location, once in the studio—and the emotional level on set is often very difficult to re-create. Commercial Indian films, not just the Hindi-language variety, are known for their lack of ambient sound, so there is a silence underlying everything instead of the background sound and noises usually employed in films to create aurally perceivable depth and environment.
The ubiquity of ADR in Bollywood cinema became prevalent in the early 1960s with the arrival of the Arriflex 3 camera, which required a blimp (cover) to shield the sound of the camera, for which it was notorious, from on-location filming. Commercial Indian filmmakers, known for their speed, never bothered to blimp the camera, and its excessive noise required that everything had to be re-created in the studio. Eventually, this became the standard for Indian films.
The trend was bucked in 2001, after a 30-year hiatus of synchronised sound, with the film "Lagaan", in which producer-star Aamir Khan insisted that the sound be done on location. This opened up a heated debate on the use and economic feasibility of on-location sound, and several Bollywood films have employed on-location sound since then.
Makeup.
In 1955 the Bollywood group Cine Costume Make-Up Artist & Hair Dressers' Association (CCMAA) created a rule that did not allow women to obtain memberships as makeup artists. However, in 2014 the Supreme Court of India ruled that this rule was in violation of the Indian constitutional guarantees granted under Article 14 (right to equality), 19(1)(g) (freedom to carry out any profession) and Article 21 (right to liberty). The judges of the Supreme Court of India stated that the ban on women makeup artist members had no "rationale nexus" to the cause sought to be achieved and was "unacceptable, impermissible and inconsistent" with the constitutional rights guaranteed to the citizens. The Court also found illegal the rule which mandated that for any artist, female or male, to work in the industry, they must have domicile status of five years in the state where they intend to work. In 2015 it was announced that Charu Khurana had become the first woman to be registered by the Cine Costume Make-Up Artist & Hair Dressers' Association.
Bollywood song and dance.
Bollywood film music is called filmi music (from Hindi, meaning "of films"). Songs from Bollywood movies are generally pre-recorded by professional playback singers, with the actors then lip synching the words to the song on-screen, often while dancing. While most actors, especially today, are excellent dancers, few are also singers. One notable exception was Kishore Kumar, who starred in several major films in the 1950s while also having a stellar career as a playback singer. K. L. Saigal, Suraiyya, and Noor Jehan were also known as both singers and actors. Some actors in the last thirty years have sung one or more songs themselves; for a list, see Singing actors and actresses in Indian cinema.
Songs are what make and break the movie; they determine if it is going to be a flop or a hit: “Few films without successful musical tracks, and even fewer without any songs and dances, succeed” With the increase of globalization, there has also been a change in the type of music that Bollywood films entail; the lyrics of the songs have increasingly been a mix of Hindi and English languages, as opposed to the strict Hindi prior to Globalization. Also, with the inspiration of global trends, such as Salsa, Pop and Hip Hop, there has been a modification of the type of music heard in Bollywood films.
Playback singers are prominently featured in the opening credits and have their own fans who will go to an otherwise lackluster movie just to hear their favourites. Going by the quality as well as the quantity of the songs they rendered, most notable singers of Bollywood are Lata Mangeshkar, Asha Bhosle, Geeta Dutt, Shamshad Begum, Kavita Krishnamurthy, Sadhana Sargam and Alka Yagnik among female playback singers; and K. L. Saigal, Talat Mahmood, Mukesh, Mohammed Rafi, Manna Dey, Hemant Kumar, Kishore Kumar, Kumar Sanu, Udit Narayan and Sonu Nigam among male playback singers. Kishore Kumar and Mohammed Rafi are often considered arguably the finest of the singers that have lent their voice to Bollywood songs, followed by Lata Mangeshkar, who, through the course of a career spanning over six decades, has recorded thousands of songs for Indian movies. The composers of film music, known as music directors, are also well-known. Their songs can make or break a film and usually do. Remixing of film songs with modern beats and rhythms is a common occurrence today, and producers may even release remixed versions of some of their films' songs along with the films' regular soundtrack albums.
The dancing in Bollywood films, especially older ones, is primarily modelled on Indian dance: classical dance styles, dances of historic northern Indian courtesans (tawaif), or folk dances. In modern films, Indian dance elements often blend with Western dance styles (as seen on MTV or in Broadway musicals), though it is usual to see Western pop "and" pure classical dance numbers side by side in the same film. The hero or heroine will often perform with a troupe of supporting dancers. Many song-and-dance routines in Indian films feature unrealistically instantaneous shifts of location or changes of costume between verses of a song. If the hero and heroine dance and sing a duet, it is often staged in beautiful natural surroundings or architecturally grand settings. This staging is referred to as a "picturisation".
Songs typically comment on the action taking place in the movie, in several ways. Sometimes, a song is worked into the plot, so that a character has a reason to sing. Other times, a song is an externalisation of a character's thoughts, or presages an event that has not occurred yet in the plot of the movie. In this case, the event is often two characters falling in love. The songs are also often referred to as a "dream sequence", and anything can happen that would not normally happen in the real world.
Previously song and dance scenes often used to be shot in Kashmir, but due to political unrest in Kashmir since the end of the 1980s, those scenes have since then often been shot in Western Europe, particularly in Switzerland and Austria.
Renowned contemporary Bollywood dancers include Hrithik Roshan, Ranbir Kapoor, Aishwarya Rai Bachchan, Madhuri Dixit, Malaika Arora Khan, and Shahid Kapoor. Older Bollywood dancers are people such as Helen, known for her cabaret numbers, Cuckoo Moray, Parveen Babi, Waheeda Rahman, Meena Kumari, and Shammi Kapoor.
For the last few decades Bollywood producers have been releasing the film's soundtrack, as tapes or CDs, before the main movie release, hoping that the music will pull audiences into the cinema later. Often the soundtrack is more popular than the movie. In the last few years some producers have also been releasing music videos, usually featuring a song from the film. However, some promotional videos feature a song which is not included in the movie.
Dialogues and lyrics.
The film script or lines of dialogue (called "dialogues" in Indian English) and the song lyrics are often written by different people.
Dialogues are usually written in an unadorned Hindi that would be understood by the largest possible audience. Some movies, however, have used regional dialects to evoke a village setting, or old-fashioned, courtly, Persian-influenced Urdu in Mughal era historical films. Jyotika Virdi, in her book "The cinematic imagiNation" , wrote about the presence of Urdu in Hindi films: "Urdu is often used in film titles, screenplay, lyrics, the language of love, war, and martyrdom." However, she further discussed its decline over the years: "The extent of Urdu used in commercial Hindi cinema has not been stable ... the decline of Urdu is mirrored in Hindi films ... It is true that many Urdu words have survived and have become part of Hindi cinema's popular vocabulary. But that is as far as it goes." Contemporary mainstream movies also make great use of English. According to "Bollywood Audiences Editorial", "English has begun to challenge the ideological work done by Urdu." Some movie scripts are first written in Latin script. Characters may shift from one language to the other to express a certain atmosphere (for example, English in a business setting and Hindi in an informal one).
Cinematic language, whether in dialogues or lyrics, is often melodramatic and invokes God, family, mother, duty, and self-sacrifice liberally. Song lyrics are often about love. Bollywood song lyrics, especially in the old movies, frequently use the poetic vocabulary of court Urdu, with many Persian loanwords. Another source for love lyrics is the long Hindu tradition of poetry about the amours of Krishna, Radha, and the gopis, as referenced in films such as "Jhanak Jhanak Payal Baje" and "Lagaan".
Music directors often prefer working with certain lyricists, to the point that the lyricist and composer are seen as a team. This phenomenon is compared to the pairings of American composers and songwriters that created old-time Broadway musicals.
Finances.
Bollywood films are multi-million dollar productions, with the most expensive productions costing up to 1 billion rupees (roughly USD 20 million). The latest Science fiction movie "Ra.One" was made at an immense budget of 1.35 billion (roughly USD 27 million), making it the most expensive movie ever produced in Bollywood. Sets, costumes, special effects, and cinematography were less than world-class up until the mid-to-late 1990s, although with some notable exceptions. As Western films and television gain wider distribution in India itself, there is an increasing pressure for Bollywood films to attain the same production levels, particularly in areas such as action and special effects. Recent Bollywood films have employed international technicians to improve in these areas, such as "Krrish" (2006) which has action choreographed by Hong Kong based Tony Ching. The increasing accessibility to professional action and special effects, coupled with rising film budgets, has seen an explosion in the action and sci-fi genres.
Sequences shot overseas have proved a real box office draw, so Mumbai film crews are increasingly filming in Australia, Canada, New Zealand, the United Kingdom, the United States, continental Europe and elsewhere. Nowadays, Indian producers are winning more and more funding for big-budget films shot within India as well, such as "Lagaan", "Devdas" and other recent films.
Funding for Bollywood films often comes from private distributors and a few large studios. Indian banks and financial institutions were forbidden from lending money to movie studios. However, this ban has now been lifted. As finances are not regulated, some funding also comes from illegitimate sources, such as the Mumbai underworld. The Mumbai underworld has been known to be involved in the production of several films, and are notorious for patronising several prominent film personalities. On occasion, they have been known to use money and muscle power to get their way in cinematic deals. In January 2000, Mumbai mafia hitmen shot Rakesh Roshan, a film director and father of star Hrithik Roshan. In 2001, the Central Bureau of Investigation seized all prints of the movie "Chori Chori Chupke Chupke" after the movie was found to be funded by members of the Mumbai underworld.
Another problem facing Bollywood is widespread copyright infringement of its films. Often, bootleg DVD copies of movies are available before the prints are officially released in cinemas. Manufacturing of bootleg DVD, VCD, and VHS copies of the latest movie titles is a well established 'small scale industry' in parts of South Asia and South East Asia. The Federation of Indian Chambers of Commerce and Industry (FICCI) estimates that the Bollywood industry loses $100 million annually in loss of revenue from unlicensed home videos and DVDs. Besides catering to the homegrown market, demand for these copies is large amongst some sections of the Indian diaspora, too. (In fact, bootleg copies are the only way people in Pakistan can watch Bollywood movies, since the Government of Pakistan has banned their sale, distribution and telecast). Films are frequently broadcast without compensation by countless small cable TV companies in India and other parts of South Asia. Small convenience stores run by members of the Indian diaspora in the US and the UK regularly stock tapes and DVDs of dubious provenance, while consumer copying adds to the problem. The availability of illegal copies of movies on the Internet also contributes to the industry's losses.
Satellite TV, television and imported foreign films are making huge inroads into the domestic Indian entertainment market. In the past, most Bollywood films could make money; now fewer tend to do so. However, most Bollywood producers make money, recouping their investments from many sources of revenue, including selling ancillary rights. There are also increasing returns from theatres in Western countries like the United Kingdom, Canada, and the United States, where Bollywood is slowly getting noticed. As more Indians migrate to these countries, they form a growing market for upscale Indian films.
For a comparison of Hollywood and Bollywood financial figures, see chart. It shows tickets sold in 2002 and total revenue estimates. Bollywood sold 3.6 billion tickets and had total revenues (theatre tickets, DVDs, television and so on.) of US$1.3 billion, whereas Hollywood films sold 2.6 billion tickets and generated total revenues (again from all formats) of US$51 billion.
Advertising.
Many Indian artists used to make a living by hand-painting movie billboards and posters (The well-known artist M.F. Hussain used to paint film posters early in his career). This was because human labour was found to be cheaper than printing and distributing publicity material. Now, a majority of the huge and ubiquitous billboards in India's major cities are created with computer-printed vinyl. The old hand-painted posters, once regarded as ephemera, are becoming increasingly collectible as folk art.
Releasing the film music, or music videos, before the actual release of the film can also be considered a form of advertising. A popular tune is believed to help pull audiences into the theatres.
Bollywood publicists have begun to use the Internet as a venue for advertising. Most of the better-funded film releases now have their own websites, where browsers can view trailers, stills, and information about the story, cast, and crew.
Bollywood is also used to advertise other products. Product placement, as used in Hollywood, is widely practised in Bollywood.
Bollywood movie stars appear in print and television advertisements for other products, such as watches or soap (see Celebrity endorsement). Advertisers say that a star endorsement boosts sales.
International Shoots.
With the increasing prominence of international setting such as Switzerland, Paris, New York and so on, it does not entail that the people and cultures residing in these exotic settings are represented. Contrary to these spaces and geographies being filmed as they are, they are actually Indianized by adding Bollywood actors and Hindi speaking extras to them. While immersing in Bollywood films, viewers get to see their local experiences duplicated in different locations around the world.
Rao states that “Media representation can depict India’s shifting relation with the world economy, but must retain its ‘‘Indianness’’ in moments of dynamic hybridity” , where “Indianness” refers to the cultural identity and political affiliation. With Bollywood’s popularity among diasporic audiences, “Indianness” poses a problem, but at the same time, it gives back to its homeland audience, a sense of uniqueness from other immigrant groups.
Awards.
The Filmfare Awards ceremony is one of the most prominent film events given for Hindi films in India. The Indian screen magazine "Filmfare" started the first Filmfare Awards in 1954, and awards were given to the best films of 1953. The ceremony was referred to as the "Clare Awards" after the magazine's editor. Modelled after the poll-based merit format of the Academy of Motion Picture Arts and Sciences, individuals may submit their votes in separate categories. A dual voting system was developed in 1956. The Filmfare awards are frequently accused of bias towards commercial success rather than artistic merit.
As the Filmfare, the National Film Awards were introduced in 1954. Since 1973, the Indian government has sponsored the National Film Awards, awarded by the government run Directorate of Film Festivals (DFF). The DFF screens not only Bollywood films, but films from all the other regional movie industries and independent/art films. These awards are handed out at an annual ceremony presided over by the President of India. Under this system, in contrast to the National Film Awards, which are decided by a panel appointed by Indian Government, the Filmfare Awards are voted for by both the public and a committee of experts.
Prestigious Film Awards ceremonies held within India are:
Prestigious Film Awards Ceremonies held overseas are:
Most of these award ceremonies are lavishly staged spectacles, featuring singing, dancing, and numerous celebrities.
Popularity and appeal.
Besides being popular among the India diaspora, such far off locations as Nigeria to Egypt to Senegal and to Russia generations of non-Indian fans have grown up with Bollywood during the years, bearing witness to the cross-cultural appeal of Indian movies. Over the last years of the 20th century and beyond, Bollywood progressed in its popularity as it entered the consciousness of Western audiences and producers, with Western actors now actively seeking roles in Bollywood movies.
Africa.
Historically, Hindi films have been distributed to some parts of Africa, largely by Lebanese businessmen. "Mother India" (1957), for example, continued to be played in Nigeria decades after its release. Indian movies have also gained ground so as to alter the style of Hausa fashions, songs have also been copied by Hausa singers and stories have influenced the writings of Nigerian novelists. Stickers of Indian films and stars decorate taxis and buses in Northern Nigeria, while posters of Indian films adorn the walls of tailor shops and mechanics' garages in the country. Unlike in Europe and North America where Indian films largely cater to the expatriate Indian market yearning to keep in touch with their homeland, in West Africa, as in many other parts of the world, such movies rose in popularity despite the lack of a significant Indian audience, where movies are about an alien culture, based on a religion wholly different, and, for the most part, a language that is unintelligible to the viewers. One such explanation for this lies in the similarities between the two cultures. Other similarities include wearing turbans; the presence of animals in markets; porters carrying large bundles, chewing sugar cane; youths riding Bajaj motor scooters; wedding celebrations, and so forth. With the strict Muslim culture, Indian movies were said to show "respect" toward women, where Hollywood movies were seen to have "no shame". In Indian movies women were modestly dressed, men and women rarely kiss, and there is no nudity, thus Indian movies are said to "have culture" that Hollywood films lack. The latter choice was a failure because "they don't base themselves on the problems of the people," where the former is based socialist values and on the reality of developing countries emerging from years of colonialism. Indian movies also allowed for a new youth culture to follow without such ideological baggage as "becoming western."
Several Bollywood personalities have avenued to the continent for both shooting movies and off-camera projects. The film "Padmashree Laloo Prasad Yadav" (2005) was one of many movies shot in South Africa. "Dil Jo Bhi Kahey" (2005) was shot almost entirely in Mauritius, which has a large ethnically Indian population.
Ominously, however, the popularity of old Bollywood versus a new, changing Bollywood seems to be diminishing the popularity on the continent. The changing style of Bollywood has begun to question such an acceptance. The new era features more sexually explicit and violent films. Nigerian viewers, for example, commented that older films of the 1950s and 1960s had culture to the newer, more westernised picturisations. The old days of India avidly "advocating decolonization ... and India's policy was wholly influenced by his missionary zeal to end racial domination and discrimination in the African territories" were replaced by newer realities. The emergence of Nollywood, Africa's local movie industry has also contributed to the declining popularity of Bollywood films. A greater globalised world worked in tandem with the sexualisation of Indian films so as to become more like American films, thus negating the preferred values of an old Bollywood and diminishing Indian soft power.
Additionally, classic Bollywood actors like Kishore Kumar and Amitabh Bachchan have historically enjoyed popularity in Egypt and Somalia. In Ethiopia, Bollwood movies are shown alongside Hollywood productions in Piazza theatres, such as the Cinema Ethiopia in Addis Ababa. In the Maghreb, Bollywood films are also broadcast, though local aesthetics tend much more toward expressive or auteur cinema than commercial fare.
Asia.
Bollywood films are widely watched in South Asian countries, including Afghanistan, Bangladesh, Nepal, Pakistan and Sri Lanka.
Many Pakistanis watch Bollywood films, as they understand Hindi (due to its linguistic similarity to Urdu). Pakistan banned the legal import of Bollywood movies in 1965. However, trade in unlicensed DVDs and illegal cable broadcasts ensured the continued popularity of Bollywood releases in Pakistan. Exceptions were made for a few films, such as the 2006 colorised re-release of the classic "Mughal-e-Azam" or the 2006 film "Taj Mahal". Early in 2008, the Pakistani government eased the ban and allowed the import of even more movies; 16 were screened in 2008. Continued easing followed in 2009 and 2010. The new policy is opposed by nationalists and representatives of Pakistan's small film industry but is embraced by cinema owners, who are making profits after years of low receipts.
Bollywood movies are so much popular in Nepal that, Bollywood movies earn more than Nepali movies. Actors like Shah Rukh Khan, Salman Khan, Akshay Kumar are most popular in Nepal and their movies sees the audience full pack all over the Cinema halls and also are so popular in Afghanistan due to the country's proximity to the Indian subcontinent and cultural perspectives present in the movies. A number of Bollywood movies were filmed inside Afghanistan while some dealt with the country, including "Dharmatma", "Kabul Express", "Khuda Gawah" and "Escape From Taliban". Hindi films have been popular in Arab countries, including Palestine, Jordan, Egypt and the Gulf countries.
Imported Indian films are usually subtitled in Arabic upon the film's release. Since the early 2000s, Bollywood has progressed in Israel. Special channels dedicated to Indian films have been displayed on cable television. Bollywood films are popular in Southeast Asia (particularly in Maritime Southeast Asia) and Central Asia (particularly in Uzbekistan and Tajikistan).
Bollywood films are widely appreciated in East Asian countries such as China, Japan, and South Korea. Some Hindi movies had success in the China and South Korea, Japan in the 1940s and 1950s and are popular till today. The most popular Hindi films in that country were "Dr. Kotnis Ki Amar Kahani" (1946), "Awaara" (1951) and "Do Bigha Zamin" (1953). Raj Kapoor was a famous movie star in China, and the song "Awara Hoon" ("I am a Tramp") was popular in the country. Since then, Hindi films significantly declined in popularity in China, until the Academy Award nominated "Lagaan" (2001) became the first Indian film to have a nationwide release there in decades. The Chinese filmmaker He Ping was impressed by "Lagaan", especially its soundtrack, and thus hired the film's music composer A. R. Rahman to score the soundtrack for his film "Warriors of Heaven and Earth" (2003). Several older Hindi films have a cult following in Japan, particularly the films directed by Guru Dutt.
Indian films are the most popular foreign films in Tajikistan, and Hindi-Urdu departments are very large in the country.
Middle East/North Africa.
Bollywood movies and celebrities enjoy wide popularity and appeal in the Arab world. There are channels such as MBC Bollywood and Zee Aflam, which show Hindi movies and serials. In Egypt, Bollywood movies used to be massively popular in the 1970's and 1980's. In 1987 however, Bollywood films were restricted to only a handful of films by the Egyptian Government. Bollywood movies are regularly screened in Dubai cinemas because of the high demand. Recently in Turkey, Bollywood has been gaining popularity as Barfi! was the first Hindi film to have a wide theatrical release.
Europe.
The awareness of Hindi cinema is substantial in the United Kingdom, where they frequently enter the UK top ten. Many films, such as "Kabhi Khushi Kabhie Gham" (2001) have been set in London. Bollywood is also appreciated in France, Germany, the Netherlands, and the Scandinavian countries. Various Bollywood movies are dubbed in German and shown on the German television channel RTL II on a regular basis.
Bollywood films are particularly popular in the former Soviet Union. Bollywood films have been dubbed into Russian, and shown in prominent theatres such as Mosfilm and Lenfilm.
Ashok Sharma, Indian Ambassador to Suriname, who has served three times in the Commonwealth of Independent States region during his diplomatic career said:
The film "Mera Naam Joker" (1970), sought to cater to such an appeal and the popularity of Raj Kapoor in Russia, when it recruited Russian actress Kseniya Ryabinkina for the movie. In the contemporary era, "" (2005) was shot entirely in Russia. After the collapse of the Soviet film distribution system, Hollywood occupied the void created in the Russian film market. This made things difficult for Bollywood as it was losing market share to Hollywood. However, Russian newspapers report that there is a renewed interest in Bollywood among young Russians.
North America.
Bollywood has experienced a marked growth in revenue in Canada and the United States, particularly popular amongst the South Asian communities in large cities, such as Toronto, Chicago, and New York City. Yash Raj Films, one of India's largest production houses and distributors, reported in September 2005 that Bollywood films in the United States earn around $100 million a year through theatre screenings, video sales and the sale of movie soundtracks. In other words, films from India do more business in the United States than films from any other non-English speaking country. Numerous films in the mid-1990s and onwards have been largely, or entirely, shot in New York, Los Angeles, Vancouver and Toronto. Bollywood's immersion in the traditional Hollywood domain was further tied with such films as "The Guru" (2002) and "" (2007) trying to popularise the Bollywood-theme for Hollywood.
Oceania.
Bollywood is not as successful in the Oceanic countries and Pacific Islands such as New Guinea. However, it ranks second to Hollywood in countries such as Fiji, with its large Indian minority, Australia and New Zealand.
Australia is one of the countries where there is a large South Asian Diaspora. Bollywood is popular amongst non-Asians in the country as well. Since 1997 the country has provided a backdrop for an increasing number of Bollywood films. Indian filmmakers have been attracted to Australia's diverse locations and landscapes, and initially used it as the setting for song-and-dance sequences, which demonstrated the contrast between the values. However, nowadays, Australian locations are becoming more important to the plot of Bollywood films. Hindi films shot in Australia usually incorporate aspects of Australian lifestyle. The Yash Raj Film "Salaam Namaste" (2005) became the first Indian film to be shot entirely in Australia and was the most successful Bollywood film of 2005 in the country. This was followed by "Heyy Babyy" (2007) "Chak De! India" (2007) and "Singh Is Kinng" (2008) which turned out to be box office successes. Following the release of "Salaam Namaste", on a visit to India the then prime minister John Howard also sought, having seen the film, to have more Indian movies shooting in the country to boost tourism, where the Bollywood and cricket nexus, was further tightened with Steve Waugh's appointment as tourism ambassador to India. Australian actress Tania Zaetta, who co-starred in "Salaam Namaste", among other Bollywood films, expressed her keenness to expand her career in Bollywood.
South America.
Bollywood movies are not influential in many countries of South America, though Bollywood culture and dance is recognised. However, due to significant South Asian diasporic communities in Suriname and Guyana, Hindi-language movies are popular. In 2006, "Dhoom 2" became the first Bollywood film to be shot in Rio de Janeiro, Brazil.
In January 2012, it was announced that UTV Motion Pictures would be releasing movies in Peru, starting with "Guzaarish".
Plagiarism.
Constrained by rushed production schedules and small budgets, some Bollywood writers and musicians have been known to resort to plagiarism. Ideas, plot lines, tunes or riffs have been copied from other Indian film industries or foreign films (including Hollywood and other Asian films) without acknowledgement of the original source. This has led to criticism towards the film industry.
Before the 1990s, this could be done with impunity. Copyright enforcement was lax in India and few actors or directors ever saw an official contract. The Hindi film industry was not widely known to non-Indian audiences (excluding the Soviet states), who would not even be aware that their material was being copied. Audiences may also not have been aware of the plagiarism since many audiences in India were unfamiliar with foreign films and music. While copyright enforcement in India is still somewhat lenient, Bollywood and other film industries are much more aware of each other now and Indian audiences are more familiar with foreign movies and music. Organisations like the India EU Film Initiative seek to foster a community between film makers and industry professional between India and the EU.
One of the common justifications of plagiarism in Bollywood in the media is that producers often play a safer option by remaking popular Hollywood films in an Indian context. Screenwriters generally produce original scripts, but due to financial uncertainty and insecurity over the success of a film many were rejected. Screenwriters themselves have been criticised for lack of creativity which happened due to tight schedules and restricted funds in the industry to employ better screenwriters. Certain filmmakers see plagiarism in Bollywood as an integral part of globalisation where American and western cultures are firmly embedding themselves into Indian culture, which is manifested, amongst other mediums, in Bollywood films. Vikram Bhatt, director of films such as "Raaz", a remake of "What Lies Beneath", and "Kasoor", a remake of "Jagged Edge", has spoken about the strong influence of American culture and desire to produce box office hits based along the same lines in Bollywood. He said, "Financially, I would be more secure knowing that a particular piece of work has already done well at the box office. Copying is endemic everywhere in India. Our TV shows are adaptations of American programmes. We want their films, their cars, their planes, their Diet Cokes and also their attitude. The American way of life is creeping into our culture." Mahesh Bhatt has said, "If you hide the source, you're a genius. There's no such thing as originality in the creative sphere".
There have been very few cases of film copyright violations taken to court because of serious delays in the legal process, and due to the long time they take to decide a case. There have been some notable cases of conflict though. The makers of "Partner" (2007) and "Zinda" (2005) have been targeted by the owners and distributors of the original films, "Hitch" and "Oldboy". American Studio Twentieth Century Fox brought the Mumbai-based B.R. Films to court over its forthcoming "Banda Yeh Bindaas Hai", allegedly an illegal remake of its 1992 film "My Cousin Vinny". B.R. Films eventually settled out of court by paying the studio at a cost of about $200,000, paving the way for the film's release. Some on the other hand do comply with copyright law, with Orion Pictures in 2008 securing the rights to remake the Hollywood film "Wedding Crashers".

</doc>
<doc id="4248" url="https://en.wikipedia.org/wiki?curid=4248" title="Bowls">
Bowls

Bowls or lawn bowls is a sport in which the objective is to roll biased balls so that they stop close to a smaller ball called a "jack" or "kitty". It is played on a bowling green which may be flat (for "flat-green bowls") or convex or uneven (for "crown green bowls"). It is normally played outdoors (although there are many indoor venues) and the outdoor surface is either natural grass, artificial turf, or cotula (in New Zealand).
History.
It has been traced certainly to the 13th century, and conjecturally to the 12th. William Fitzstephen (d. about 1190), in his biography of Thomas Becket, gives a graphic sketch of the London of his day and, writing of the summer amusements of the young men, says that on holidays they were "exercised in Leaping, Shooting, Wrestling, Casting of Stones n jactu lapidu, and Throwing of Javelins fitted with Loops for the Purpose, which they strive to fling before the Mark; they also use Bucklers, like fighting Men." It is commonly supposed that by jactus lapidum, Fitzstephen meant the game of bowls, but though it is possible that round stones may sometimes have been employed in an early variety of the game - and there is a record of iron bowls being used, though at a much later date, on festive occasions at Nairn, - nevertheless the inference seems unwarranted. The jactus lapidum of which he speaks may have been more akin to shotput. It is beyond dispute, however, that the game, at any rate in a rudimentary form, was played in the 13th century. A manuscript of that period in the royal library, Windsor (No. 20, E iv.), contains a drawing representing two players aiming at a small cone instead of an earthenware ball or jack. The world's oldest surviving bowling green is the Southampton Old Bowling Green, which was first used in 1299.
Another manuscript of the same century has a crude but spirited picture which brings us into close touch with the existing game. Three figures are introduced and a jack. The first player's bowl has come to rest just in front of the jack; the second has delivered his bowl and is following after it with one of those eccentric contortions still not unusual on modern greens, the first player meanwhile making a repressive gesture with his hand, as if to urge the bowl to stop short of his own; the third player is depicted as in the act of delivering his bowl. A 14th-century manuscript, Book of Prayers, in the Francis Douce collection in the Bodleian Library at Oxford contains a drawing in which two persons are shown, but they bowl to no mark. Strutt (Sports and Pastimes) suggests that the first player's bowl may have been regarded by the second player as a species of jack; but in that case it is not clear what was the first player's target. In these three earliest illustrations of the pastime it is worth noting that each player has one bowl only, and that the attitude in delivering it was as various five or six hundred years ago as it is today. In the third he stands almost upright; in the first he kneels; in the second he stoops, halfway between the upright and the kneeling position.
The game eventually came under the ban of king and parliament, both fearing it might jeopardise the practice of archery, then so important in battle. Statutes forbidding it and other sports were enacted in the reigns of Edward III, Richard II and other monarchs. Even when, on the invention of gunpowder and firearms, the bow had fallen into disuse as a weapon of war, the prohibition was continued. The discredit attaching to bowling alleys, first established in London in 1455, probably encouraged subsequent repressive legislation, for many of the alleys were connected with taverns frequented by the dissolute and gamesters. The word "bowls" occurs for the first time in the statute of 1511 in which Henry VIII confirmed previous enactments against unlawful games. By a further act of 1541—which was not repealed until 1845—artificers, labourers, apprentices, servants and the like were forbidden to play bowls at any time except Christmas, and then only in their master's house and presence. It was further enjoined that any one playing bowls outside his own garden or orchard was liable to a penalty of 6s. 8d.(6 shillings and 8 pence), while those possessed of lands of the yearly value of £100 might obtain licences to play on their own private greens.
In 1864 William Wallace Mitchell (1803–1884), a Glasgow Cotton Merchant, published his "Manual of Bowls Playing" following his work as the secretary formed in 1849 by Scottish bowling clubs which became the basis of the rules of the modern game.
Young Mitchell was only 11 when he played on Kilmarnock Bowling green, the oldest club in Scotland, instituted in 1740.
The patenting of the first lawn mower in 1830, in Britain, is strongly believed to have been the catalyst, world-wide, for the preparation of modern-style greens, sporting ovals, playing fields, pitches, grass courts, etc. This is turn led to the codification of modern rules for many sports, including lawn bowls, most football codes, lawn tennis and others.
National Bowling Associations were established in the late 1800s. In the then Victorian Colony (now State of Victoria in Australia), the (Royal) Victorian Bowling Association was formed in 1880 and The Scottish Bowling Association was established in 1892, although there had been a failed attempt in 1848 by 200 Scottish clubs.
Today the sport is played in over 40 countries with more than 50 member national authorities.
The home of the modern game is still Scotland with the World Bowls centre in Edinburgh at Caledonia House,1 Redheughs Rigg, South Gyle, Edinburgh, EH12 9DQ.
Game.
Lawn bowls is usually played on a large, rectangular, precisely levelled and manicured grass or synthetic surface known as a bowling green which is divided into parallel playing strips called rinks. In the simplest competition, singles, one of the two opponents flips a coin to see who wins the "mat" and begins a segment of the competition (in bowling parlance, an "end"), by placing the mat and rolling the jack to the other end of the green to serve as a target. Once it has come to rest, the jack is aligned to the centre of the rink and the players take turns to roll their bowls from the mat towards the jack and thereby build up the "head".
A bowl may curve outside the rink boundary on its path, but must come to rest within the rink boundary to remain in play. Bowls falling into the ditch are dead and removed from play, except in the event when one has "touched" the jack on its way. "Touchers" are marked with chalk and remain alive in play even though they are in the ditch. Similarly if the jack is knocked into the ditch it is still alive unless it is out of bounds to the side resulting in a "dead" end which is replayed, though according to international rules the jack is "respotted" to the centre of the rink and the end is continued. After each competitor has delivered all of their bowls (four each in singles and pairs, three each in triples, and two bowls each in fours), the distance of the closest bowls to the jack is determined (the jack may have been displaced) and points, called "shots", are awarded for each bowl which a competitor has closer than the opponent's nearest to the jack. For instance, if a competitor has bowled two bowls closer to the jack than their opponent's nearest, they are awarded two shots. The exercise is then repeated for the next end, a game of bowls typically being of twenty-one ends.
Lawn bowls is played on grass and variations from green to green are common. Greens come in all shapes and sizes, fast, slow, big crown, small crown and so on.
Scoring.
Scoring systems vary from competition to competition. Games can be decided when:
Games to a specified number of ends may also be drawn. The draw may stand, or the opponents may be required to play an extra end to decide the winner. These provisions are always published beforehand in the event's Conditions of Play.
In the Laws of the Sport of Bowls
the winner in a singles game is the first player to score 21 shots. In all other disciplines (pairs, triples, fours) the winner is the team who has scored the most shots after 21/25 ends of play. Often local tournaments will play shorter games (often 10 or 12 ends). Some competitions use a "set" scoring system, with the first to seven points awarded a set in a best-or-three or best-of-five set match. As well as singles competition, there can be two (pairs), three (triples) and four-player (fours) teams. In these, teams bowl alternately, with each player within a team bowling all their bowls, then handing over to the next player. The team captain or "skip" always plays last and is instrumental in directing his team's shots and tactics. The current method of scoring in the professional tour (World Bowls Tour) is sets. Each set consists of nine ends and the player with the most shots at the end of a set wins the set. If the score is tied the set is halved. If a player wins two sets, or gets a win and a tie, that player wins the game. If each player wins a set, or both sets end tied, there is a 3-end tiebreaker to determine a winner.
Bias of bowls.
Bowls are designed to travel a curved path because of a weight bias which was originally produced by inserting weights in one side of the bowl. This is no longer permitted by the rules and bias is now produced entirely by the shape of the bowl. A bowler determines the bias direction of the bowl in his hand by a dimple or symbol on one side. Regulations determine the minimum bias allowed, and the range of diameters (11.6 to 13.1 cm), but within these rules bowlers can and do choose bowls to suit their own preference. They were originally made from lignum vitae, a dense wood giving rise to the term "woods" for bowls, but are now more typically made of a hard plastic composite material.
Bowls were once only available coloured black or brown but they are now available in a variety of colours. They have unique symbol markings engraved on them for identification. Since many bowls look the same, coloured, adhesive stickers or labels are also used to mark the bowls of each team in bowls matches. Some local associations agree on specific colours for stickers for each of the clubs in their area. Provincial or national colours are often assigned in national and international competitions. These stickers are used by officials to distinguish teams.
Bowls have symbols unique to the set of four for identification. The side of the bowl with a larger symbol within a circle indicates the side away from the bias. That side with a smaller symbol within a smaller circle is the bias side toward which the bowl will turn. It is not uncommon for players to deliver a "wrong bias" shot from time to time and see their carefully aimed bowl crossing neighbouring rinks rather than heading towards their jack.
When bowling there are several types of delivery. "Draw" shots are those where the bowl is rolled to a specific location without causing too much disturbance of bowls already in the head. For a right-handed bowler, "forehand draw" or "finger peg" is initially aimed to the right of the jack, and curves in to the left. The same bowler can deliver a "backhand draw" or "thumb peg" by turning the bowl over in his hand and curving it the opposite way, from left to right. In both cases, the bowl is rolled as close to the jack as possible, unless tactics demand otherwise. A "drive" or "fire" or "strike" involves bowling with force with the aim of knocking either the jack or a specific bowl out of play - and with the drive's speed, there is virtually no noticeable (or, at least, much less) curve on the shot. An "upshot" or "yard on" shot involves delivering the bowl with an extra degree of weight (often referred to as "controlled" weight or "rambler"), enough to displace the jack or disturb other bowls in the head without killing the end. A "block" shot is one that is intentionally placed short to defend from a drive or to stop an oppositions draw shot. The challenge in all these shots is to be able to adjust line and length accordingly, the faster the delivery, the narrower the line or "green".
Variations of play.
Particularly in team competition there can be a large number of bowls on the green towards the conclusion of the end, and this gives rise to complex tactics. Teams "holding shot" with the closest bowl will often make their subsequent shots not with the goal of placing the bowl near the jack, but in positions to make it difficult for opponents to get their bowls into the head, or to places where the jack might be deflected to if the opponent attempts to disturb the head.
There are many different ways to set up the game. Crown Green Bowling utilises the entire green. A player can send the jack anywhere on the green in this game and the green itself is more akin to a golf green, with lots of undulation. It is only played with two bowls each, the Jack also has a bias and is only slightly smaller than the Bowls. The game is played usually to 21-up in Singles and Doubles format with some competitions playing to 31-up. The Panel (Professional Crown Green Bowls) is played at the Red Lion, Westhoughton daily and is played to 41-up with greenside betting throughout play.
Singles, triples and fours and Australian pairs are some ways the game can be played. In singles, two people play against each other and the first to reach 21, 25 or 31 shots (as decided by the controlling body) is the winner. In one variation of singles play, each player uses two bowls only and the game is played over 21 ends. A player concedes the game before the 21st end if the score difference is such that it is impossible to draw equal or win within the 21 ends. If the score is equal after 21 ends, an extra end is played to decide the winner. An additional scoring method is set play. This comprises two sets over nine ends. Should a player win a set each, they then play a further 3 ends that will decide the winner.
Pairs allows both people on a team to play Skip and Lead. The lead throws two bowls, the skip delivers two, then the lead delivers his remaining two, the skip then delivers his remaining two bowls. Each end, the leads and skips switch positions. This is played over 21 ends or sets play. Triples is with three players while Fours is with four players in each team and is played over 21 ends.
Another pairs variation is 242 pairs (also known as Australian Pairs). In the first end of the game the A players lead off with 2 bowls each, then the B players play 4 bowls each, before the A players complete the end with their final 2 bowls. The A players act as lead and skip in the same end. In the second end the roles are reversed with the A players being in the middle. This alternating pattern continues through the game which is typically over 15 ends.
Short Mat Bowls is an all-year sport unaffected by weather conditions and it does not require a permanent location as the rink mats can be rolled up and stowed away. This makes it particularly appropriate for small communities as it can be played in village halls, schools, sports and social clubs, hotels and so on. where space is restricted and is also required for other purposes: it is even played on North Sea oil rigs where space is really at a premium.
Bowls are played by the blind and paraplegic. Blind bowlers are extremely skilful. A string is run out down the centre of the lane & wherever the jack lands it is moved across to the string and the length is called out by a sighted marker, when the woods are sent the distance from the jack is called out, in yards, feet and inches-the position in relation to the jack is given using the clock, 12.00 is behind the jack..
Tra bowls.
In the province of West-Flanders (and surrounding regions), tra bowls is the most popular variation of bowls. As opposed to playing it on a flat or uneven terrain, the terrain is made smooth but hollow (tra just means "hollow road" in Flemish). The hollow road causes the path to be curving even more.
The balls are biased in the same way as the lawn bowls balls but with a diameter of about 20 cm, a thickness of 12 cm and a weight of about 2 kg, they are a bit bigger than usual bowls. The target is an unmovable feather or metal plate on the ground, instead of a small ball. The length of the tra is about 18 m.
The scoring is also different, as a point is awarded for every shot that brings the ball closer to the target than any opponent's ball. This causes pure blocking strategies to be less effective.
In 1972, the West-Flemish tra bowls federation was founded to uniform the local differing rules and to organise a match calendar. Meanwhile, they also organise championships and tournaments.
Competitions.
There is a World Indoor Bowls Championships and also World Bowls Events.
Bowls is one of the "core sports" that must be included at each edition of the Commonwealth Games. With the exception of the 1966 Games, the sport has been included in all Games since their inception in 1930. Glasgow, Scotland hosted the 2014 Commonwealth Games, with Jo Edwards (New Zealand) and Darren Burnett (Scotland) winning the singles gold medals. Gold Coast, Australia will host the 2018 Commonwealth Games.

</doc>
<doc id="4249" url="https://en.wikipedia.org/wiki?curid=4249" title="Barcelonnette">
Barcelonnette

Barcelonnette () is a commune of France and a subprefecture in the department of Alpes-de-Haute-Provence, in the Provence-Alpes-Côte d'Azur region. It is located in the southern French Alps, at the crossroads between Provence, Piedmont and the Dauphiné, and is the largest town in the Ubaye Valley. The town's inhabitants are known as "Barcelonnettes".
Toponymy.
Barcelonnette was founded and named in 1231, by Ramon Berenguer IV, Count of Provence. While the town's name is generally seen as a diminutive form of Barcelona in Catalonia, Albert Dauzat and Charles Rostaing point out an earlier attestation of the name "Barcilona" in Barcelonnette in around 1200, and suggest that it is derived instead from two earlier stems signifying a mountain, *"bar" and *"cin" (the latter of which is also seen in the name of Mont Cenis).
In the Vivaro-Alpine dialect of Occitan, the town is known as "Barcilona de Provença" or more rarely "Barciloneta" according to the classical norm; under the Mistralian norm it is called "Barcilouna de Prouvença" or "Barcilouneto". In "Valéian" (the dialect of Occitan spoken in the Ubaye Valley), it is called "Barcilouna de Prouvença" or "Barcilounéta". "Barcino Nova" is the town's Latin name meaning "new Barcelona"; "Barcino" was the Roman name for Barcelona in Catalonia from its foundation by Emperor Augustus in 10 BC, and it was only changed to "Barcelona" in the Middle Ages.
The inhabitants of the town are called "Barcelonnettes", or "Vilandroises" in Valéian.
History.
Origins.
The Barcelonnette region was populated by Ligures from the 1st millennium BC onwards, and the arrival of the Celts several centuries later led to the formation of a mixed Celto-Ligurian people, the Vesubians. Polybius described the Vesubians as belligerent but nonetheless civilised and mercantile, and Julius Caesar praised their bravery. The work "History of the Gauls" also places the Vesubians in the Ubaye Valley.
Following the Roman conquest of Provence, Barcelonnette was included in a small province with modern Embrun as its capital and governed by Albanus Bassalus. This was integrated soon afterwards into Gallia Narbonensis. In 36 AD, Emperor Nero transferred Barcelonnette to the province of the Cottian Alps. The town was known as "Rigomagensium" under the Roman Empire and was the capital of a civitas (a provincial subdivision), though no Roman money has yet been found in the canton of Barcelonnette.
Medieval town.
The town of Barcelonnette was founded in 1231 by Ramon Berenguer IV, Count of Provence. According to Charles Rostaing, this act of formal "foundation", according certain privileges to the town, was a means of regenerating the destroyed town of "Barcilona". The town was afforded a "consulat" (giving it the power to administer and defend itself) in 1240.
Control of the area in the Middle Ages swung between the Counts of Savoy and of Provence. In 1388, after Count Louis II of Provence had left to conquer Naples, the Count of Savoy Amadeus VIII took control of Barcelonnette; however, it returned to Provençal control in 1390, with the d'Audiffret family as its lords. On the death of Louis II in 1417 it reverted to Savoy, and, although Count René again retook the area for Provence in 1471, it had returned to Savoyard dominance by the start of the 16th century, by which point the County of Provence had become united with the Kingdom of France due to the death of Count Charles V in 1481.
Ancien Régime.
During Charles V's invasion of Provence in 1536, Francis I of France sent the Count of Fürstenberg's 6000 "Landsknechte" to ravage the area in a scorched earth policy. Barcelonnette and the Ubaye Valley remained under French sovereignty until the second Treaty of Cateau-Cambrésis on 3 April 1559.
In 1588 the troops of François, Duke of Lesdiguières entered the town and set fire to the church and convent during their campaign against the Duke of Savoy. In 1600, after the Treaty of Vervins, conflict returned between Henry IV of France and Savoy, and Lesdiguières retook Barcelonnette until the conclusion of the Treaty of Lyon on 17 January the following year. In 1628, during the War of the Mantuan Succession, Barcelonnette and the other towns of the Ubaye Valley were pillaged and burned by Jacques du Blé d'Uxelles and his troops, as they passed through towards Italy to the Duke of Mantua's aid. The town was retaken by the Duke of Savoy in 1630; and in 1691 it was captured by the troops of the Marquis de Vins during the War of the League of Augsburg.
Between 1614 and 1713, Barcelonnette was the seat of one of the four prefectures under the jurisdiction of the Senate of Nice. At this time, the community of Barcelonnette successfully purchased the "seigneurie" of the town as it was put to auction by the Duke of Savoy; it thereby gained its own justicial powers. In 1646, a college was founded in Barcelonnette.
A "significant" part of the town's inhabitants had, by the 16th century, converted to Protestantism, and were repressed during the French Wars of Religion.
The "viguerie" of Barcelonnette (also comprising Saint-Martin and Entraunes) was reattached to France in 1713 as part of a territorial exchange with the Duchy of Savoy during the Treaties of Utrecht. The town remained the site of a "viguerie" until the French Revolution. A decree of the council of state on 25 December 1714 reunited Barcelonnete with the general government of Provence.
Revolution.
Barcelonnette was one of few settlements in Haute-Provence to acquire a Masonic Lodge before the Revolution, in fact having two:
In March 1789, riots took place as a result of a crisis in wheat production. In July, the Great Fear of aristocratic reprisal against the ongoing French Revolution struck France, arriving in the Barcelonnette area on 31 July 1789 (when the news of the storming of the Bastille first reached the town) before spreading towards Digne.
This agitation continued in the Ubaye Valley; a new revolt broke out on 14 June, and famine was declared in April 1792. The patriotic society of the commune was one of the first 21 created in Alpes-de-Haute-Provence, in spring 1792, by the envoys of the departmental administration. Around a third of the male population attended at the club. Another episode of political violence occurred in August 1792.
Barcelonnette was the seat of the District of Barcelonnette from 1790 to 1800.
Modern history.
In December 1851, the town was home to a movement of republican resistance towards Napoleon III's coup. Though only a minority of the population, the movement rebelled on Sunday 7 December, the day after the news of the coup arrived. Town officials and gendarmes were disarmed and placed in the maison d'arrêt. A committee of public health was created on 8 December; on 9 December the inhabitants of Jausiers and its surroundings formed a colony under the direction of general councillor Brès, and Mayor Signoret of Saint-Paul-sur-Ubaye. This was stopped, however, on 10 December before it could reach Barcelonnette, as the priest of the subprefecture had intervened. On 11 December, several officials escaped and found refuge in L'Argentière in Piedmont. The arrival of troops on 16 December put a final end to the republican resistance without bloodshed, and 57 insurgents were tried; 38 were condemned to deportation (though several were pardoned in April).
Between 1850 and 1950, Barcelonnette was the source of a wave of emigration to Mexico. Among these emigrants was Jean Baptiste Ebrard, founder of the Liverpool department store chain in Mexico; Marcelo Ebrard, the head of government of Mexico City from 2006 to 2012, is also descended from them. On the edges of Barcelonnette and Jausiers there are several houses and villas of colonial style (known as "maisons mexicaines"), constructed by emigrants to Mexico who returned to France between 1870 and 1930. A plaque in the town commemorates the deaths of ten Mexican citizens who returned to Barcelonnette to fight in the First World War.
During the Second World War, 26 Jews were arrested in Barcelonnette before being deported. The 89th "compagnie de travailleurs étrangers" (Company of Foreign Workers), consisting of foreigners judged as undesirable by the Third Republic and the Vichy regime and committed to forced labour, was established in Barcelonnette.
The 11th Battalion of "Chasseurs alpins" was garrisoned at Barcelonnette between 1948 and 1990.
Geography.
Barcelonnette is situated in the wide and fertile Ubaye Valley, of which it is the largest town. It lies at an elevation of 1132 m (3717 ft) on the right bank of the Ubaye River, and is surrounded by mountains which reach peaks of over 3000 m; the tallest of these is the Needle of Chambeyron at 3412 m. Barcelonnette is situated 210 km from Turin, 91 km from Nice and 68 km from Gap.
Biodiversity.
As a result of its relief and geographic situation, the Ubaye Valley has an "abundance of plant and animal species". The fauna is largely constituted of golden eagles, marmots, ibex and vultures, and the flora includes a large proportion of larches, génépis and white asphodels.
Climate.
The Ubaye Valley has an alpine climate and winters are harsh as a result of the altitude, but there are only light winds as a result of the relief. There are on average almost 300 days of sun and 700 mm of rain per year.
Hazards.
None of the 200 communes of the department is entirely free of seismic risk; the canton of Barcelonnette is placed in zone 1b (low risk) by the determinist classification of 1991 based on seismic history, and zone 4 (average risk) according to the probabilistic EC8 classification of 2011. The commune is also vulnerable to avalanches, forest fires, floods, and landslides. Barcelonnette is also exposed to the possibility of a technological hazard in that road transport of dangerous materials is allowed to pass through on the RD900.
The town has been subject to several orders of natural disaster: floods and mudslides in 1994 and 2008, and landslides in 1996 and 1999. The strongest recorded earthquakes in the region occurred on 5 April 1959, with its epicentre at Saint-Paul-sur-Ubaye and a recorded intensity of 6.5 at Barcelonnette, and on 17 February 1947, with its epicentre at Prazzo over the Italian border.
The subprefecture has been situated since 1978 in a "maison mexicaine", the Villa l'Ubayette, constructed between 1901 and 1903.
Population.
In 1471, the community of Barcelonnette (including several surrounding parishes) comprised 421 fires (households). In 1765, it had 6674 inhabitants, but emigration, particularly to Mexico, slowed the town's growth in the period before the Second World War. According to the census of 2007, Barcelonnette has a population of 2766 (municipal population) or 2939 (total) across a total of 16.42 km. The town is characterised by low population density. Between 1990 and 1999 the town's annual mean population growth was -0.6%, though between 1999 and 2007 this increased to an average of -0.1%.
Economy.
The city is mainly a tourist and resort centre, serving many ski lodges. The Pra Loup resort is 7 km from Barcelonnette; Le Sauze is 5 km away. It and the Ubaye Valley are served by the Barcelonnette - Saint-Pons Airport. Notably, Barcelonnette is the only subprefecture of France not be served by rail transport; the Ubaye line which would have linked Chorges to Barcelonnette was never completed as a result of the First World War and the construction of the Serre-Ponçon Dam between 1955 and 1961.
Education.
An "école normale" (an institute for training primary school teachers) was founded in Barcelonnette in 1833, and remained there until 1888 when it was transferred to Digne. The "lycée André-Honnorat de Barcelonnette", originally the "collège Saint-Maurice" and renamed after the politician André Honnorat in 1919, is located in the town; Pierre-Gilles de Gennes and Carole Merle both studied there. Currently, three schools exist in Barcelonnette: a public nursery school, a public elementary school, and a private school (under a contract by which the teachers are paid by the national education system).
In 2010 the "lycée André-Honnorat" opened a boarding school aimed at gifted students of poorer social backgrounds, in order to give them better conditions in which to study. It is located in the "Quartier Craplet", formerly the garrison of the 11th Battalion of "Chasseurs Alpins" and then the French Army's "Centre d'instruction et d'entraînement au combat en montagne" (CIECM).
International links.
Barcelonnette is twinned with:
It is also the site of a Mexican honorary consulate.

</doc>
<doc id="4251" url="https://en.wikipedia.org/wiki?curid=4251" title="Bahá'í Faith">
Bahá'í Faith

The Bahá'í Faith ( "Bahá'iyyat", "Bahá'iyya" ) is a monotheistic religion which emphasizes the spiritual unity of all humankind. Three core principles establish a basis for Bahá'í teachings and doctrine: the unity of God, that there is only one God who is the source of all creation; the unity of religion, that all major religions have the same spiritual source and come from the same God; and the unity of humanity, that all humans have been created equal, coupled with the unity in diversity, that diversity of race and culture are seen as worthy of appreciation and acceptance. According to the Bahá'í Faith's teachings, the human purpose is to learn to know and to love God through such methods as prayer, reflection and being of service to humanity.
The Bahá'í Faith was founded by Bahá'u'lláh in 19th-century Persia. Bahá'u'lláh was exiled for his teachings from Persia to the Ottoman Empire and died while officially still a prisoner. After Bahá'u'lláh's death, under the leadership of his son, `Abdu'l-Bahá, the religion spread from its Persian and Ottoman roots, and gained a footing in Europe and America, and was consolidated in Iran, where it suffers intense persecution. After the death of `Abdu'l-Bahá, the leadership of the Bahá'í community entered a new phase, evolving from a single individual to an administrative order with both elected bodies and appointed individuals. There are probably more than 5 million Bahá'ís around the world in more than 200 countries and territories.
In the Bahá'í Faith, religious history is seen to have unfolded through a series of divine messengers, each of whom established a religion that was suited to the needs of the time and to the capacity of the people. These messengers have included Abrahamic figures—Moses, Jesus, Muhammad, as well as figures from Indian religions like Krishna, Buddha, and others. For Bahá'ís, the most recent messengers are the Báb and Bahá'u'lláh. In Bahá'í belief, each consecutive messenger prophesied of messengers to follow, and Bahá'u'lláh's life and teachings fulfilled the end-time promises of previous scriptures. Humanity is understood to be in a process of collective evolution, and the need of the present time is for the gradual establishment of peace, justice and unity on a global scale.
Etymology.
The word "Bahá'í " is used either as an adjective to refer to the Bahá'í Faith or as a term for a follower of Bahá'u'lláh. The word is not a noun meaning the religion as a whole. It is derived from the Arabic "Bahá"' (), meaning "glory" or "splendor". The term "Bahaism" (or "Baha'ism") is still used, mainly in a pejorative sense.
Beliefs.
Three core principles establish a basis for Bahá'í teachings and doctrine: the unity of God, the unity of religion, and the unity of humanity. From these postulates stems the belief that God periodically reveals his will through divine messengers, whose purpose is to transform the character of humankind and to develop, within those who respond, moral and spiritual qualities. Religion is thus seen as orderly, unified, and progressive from age to age.
God.
The Bahá'í writings describe a single, personal, inaccessible, omniscient, omnipresent, imperishable, and almighty God who is the creator of all things in the universe. The existence of God and the universe is thought to be eternal, without a beginning or end. Though inaccessible directly, God is nevertheless seen as conscious of creation, with a will and purpose that is expressed through messengers termed Manifestations of God.
Bahá'í teachings state that God is too great for humans to fully comprehend, or to create a complete and accurate image of, by themselves. Therefore, human understanding of God is achieved through his revelations via his Manifestations. In the Bahá'í religion, God is often referred to by titles and attributes (for example, the All-Powerful, or the All-Loving), and there is a substantial emphasis on monotheism; such doctrines as the Trinity are seen as compromising, if not contradicting, the Bahá'í view that God is single and has no equal. The Bahá'í teachings state that the attributes which are applied to God are used to translate Godliness into human terms and also to help individuals concentrate on their own attributes in worshipping God to develop their potentialities on their spiritual path. According to the Bahá'í teachings the human purpose is to learn to know and love God through such methods as prayer, reflection, and being of service to others.
Religion.
Bahá'í notions of progressive religious revelation result in their accepting the validity of the well known religions of the world, whose founders and central figures are seen as Manifestations of God. Religious history is interpreted as a series of dispensations, where each "manifestation" brings a somewhat broader and more advanced revelation that is rendered as a text of scripture and passed on through history with greater or lesser reliability but at least true in substance, suited for the time and place in which it was expressed. Specific religious social teachings (for example, the direction of prayer, or dietary restrictions) may be revoked by a subsequent manifestation so that a more appropriate requirement for the time and place may be established. Conversely, certain general principles (for example, neighbourliness, or charity) are seen to be universal and consistent. In Bahá'í belief, this process of progressive revelation will not end; however, it is believed to be cyclical. Bahá'ís do not expect a new manifestation of God to appear within 1000 years of Bahá'u'lláh's revelation.
Bahá'í beliefs are sometimes described as syncretic combinations of earlier religious beliefs. Bahá'ís, however, assert that their religion is a distinct tradition with its own scriptures, teachings, laws, and history. While the religion was initially seen as a sect of Islam, most religious specialists now see it as an independent religion, with its religious background in Shi'a Islam being seen as analogous to the Jewish context in which Christianity was established. Muslim institutions and clergy, both Sunni and Shia, consider Bahá'ís to be deserters or apostates from Islam, which has led to Bahá'ís being persecuted. Bahá'ís describe their faith as an independent world religion, differing from the other traditions in its relative age and in the appropriateness of Bahá'u'lláh's teachings to the modern context. Bahá'u'lláh is believed to have fulfilled the messianic expectations of these precursor faiths.
Human beings.
The Bahá'í writings state that human beings have a "rational soul", and that this provides the species with a unique capacity to recognize God's station and humanity's relationship with its creator. Every human is seen to have a duty to recognize God through His messengers, and to conform to their teachings. Through recognition and obedience, service to humanity and regular prayer and spiritual practice, the Bahá'í writings state that the soul becomes closer to God, the spiritual ideal in Bahá'í belief. When a human dies, the soul passes into the next world, where its spiritual development in the physical world becomes a basis for judgment and advancement in the spiritual world. Heaven and Hell are taught to be spiritual states of nearness or distance from God that describe relationships in this world and the next, and not physical places of reward and punishment achieved after death.
The Bahá'í writings emphasize the essential equality of human beings, and the abolition of prejudice. Humanity is seen as essentially one, though highly varied; its diversity of race and culture are seen as worthy of appreciation and acceptance. Doctrines of racism, nationalism, caste, social class, and gender-based hierarchy are seen as artificial impediments to unity. The Bahá'í teachings state that the unification of humanity is the paramount issue in the religious and political conditions of the present world.
Teachings.
Summary.
Shoghi Effendi, the appointed head of the religion from 1921 to 1957, wrote the following summary of what he considered to be the distinguishing principles of Bahá'u'lláh's teachings, which, he said, together with the laws and ordinances of the "Kitáb-i-Aqdas" constitute the bedrock of the Bahá'í Faith:
Social principles.
The following principles are frequently listed as a quick summary of the Bahá'í teachings. They are derived from transcripts of speeches given by `Abdu'l-Bahá during his tour of Europe and North America in 1912. The list is not authoritative and a variety of such lists circulate.
With specific regard to the pursuit of world peace, Bahá'u'lláh prescribed a world-embracing collective security arrangement as necessary for the establishment of a lasting peace.
Mystical teachings.
Although the Bahá'í teachings have a strong emphasis on social and ethical issues, there exist a number of foundational texts that have been described as mystical. The "Seven Valleys" is considered Bahá'u'lláh's "greatest mystical composition." It was written to a follower of Sufism, in the style of `Attar, The Persian Muslim poet, and sets forth the stages of the soul's journey towards God. It was first translated into English in 1906, becoming one of the earliest available books of Bahá'u'lláh to the West. The "Hidden Words" is another book written by Bahá'u'lláh during the same period, containing 153 short passages in which Bahá'u'lláh claims to have taken the basic essence of certain spiritual truths and written them in brief form.
Covenant.
The Bahá'í teachings speak of both a "Greater Covenant", being universal and endless, and a "Lesser Covenant", being unique to each religious dispensation. The Lesser Covenant is viewed as an agreement between a Messenger of God and his followers and includes social practices and the continuation of authority in the religion. At this time Bahá'ís view Bahá'u'lláh's revelation as a binding lesser covenant for his followers; in the Bahá'í writings being firm in the covenant is considered a virtue to work toward. The Greater Covenant is viewed as a more enduring agreement between God and humanity, where a Manifestation of God is expected to come to humanity about every thousand years, at times of turmoil and uncertainty.
With unity as an essential teaching of the religion, Bahá'ís follow an administration they believe is divinely ordained, and therefore see attempts to create schisms and divisions as efforts that are contrary to the teachings of Bahá'u'lláh. Schisms have occurred over the succession of authority, but any Bahá'í divisions have had relatively little success and have failed to attract a sizeable following. The followers of such divisions are regarded as Covenant-breakers and shunned, essentially excommunicated.
Canonical texts.
The "canonical texts" are the writings of the Báb, Bahá'u'lláh, `Abdu'l-Bahá, Shoghi Effendi and the Universal House of Justice, and the authenticated talks of `Abdu'l-Bahá. The writings of the Báb and Bahá'u'lláh are considered as divine revelation, the writings and talks of `Abdu'l-Bahá and the writings of Shoghi Effendi as authoritative interpretation, and those of the Universal House of Justice as authoritative legislation and elucidation. Some measure of divine guidance is assumed for all of these texts. Some of Bahá'u'lláh's most important writings include the Kitáb-i-Aqdas, literally the "Most Holy Book", which is his book of laws, the Kitáb-i-Íqán, literally the "Book of Certitude", which became the foundation of much of Bahá'í belief, the Gems of Divine Mysteries, which includes further doctrinal foundations, and the Seven Valleys and the Four Valleys which are mystical treatises.
History.
Bahá'í history follows a sequence of leaders, beginning with the Báb's declaration in Shiraz, Iran on the evening of 22 May 1844, and ultimately resting on an administrative order established by the central figures of the religion. The Bahá'í community was mostly confined to the Persian and Ottoman empires until after the death of Bahá'u'lláh in 1892, at which time he had followers in 13 countries of Asia and Africa. Under the leadership of his son, `Abdu'l-Bahá, the religion gained a footing in Europe and America, and was consolidated in Iran, where it still suffers intense persecution. After the death of `Abdu'l-Bahá in 1921, the leadership of the Bahá'í community entered a new phase, evolving from a single individual to an administrative order with both elected bodies and appointed individuals.
The Báb.
On the evening of 22 May 1844, Siyyid `Alí-Muhammad of Shiraz, Iran proclaimed that he was "the Báb" ( "the Gate"), referring to his later claim to the station of Mahdi, the Twelfth Imam of Shi`a Islam. His followers were therefore known as Bábís. As the Báb's teachings spread, which the Islamic clergy saw as a threat, his followers came under increased persecution and torture. The conflicts escalated in several places to military sieges by the Shah's army. The Báb himself was imprisoned and eventually executed in 1850.
Bahá'ís see the Báb as the forerunner of the Bahá'í Faith, because the Báb's writings introduced the concept of "He whom God shall make manifest", a Messianic figure whose coming, according to Bahá'ís, was announced in the scriptures of all of the world's great religions, and whom Bahá'u'lláh, the founder of the Bahá'í Faith, claimed to be in 1863. The Báb's tomb, located in Haifa, Israel, is an important place of pilgrimage for Bahá'ís. The remains of the Báb were brought secretly from Iran to the Holy Land and eventually interred in the tomb built for them in a spot specifically designated by Bahá'u'lláh. The main written works translated into English of the Báb's are collected in Selections from the Writings of the Báb out of the estimated 135 works.
Bahá'u'lláh.
Mírzá Husayn `Alí Núrí was one of the early followers of the Báb, and later took the title of Bahá'u'lláh. He was arrested and imprisoned for this involvement in 1852. Bahá'u'lláh relates that in 1853, while incarcerated in the dungeon of the Síyáh-Chál in Tehran, he received the first intimations that he was the one anticipated by the Báb.
Shortly thereafter he was expelled from Tehran to Baghdad, in the Ottoman Empire; then to Constantinople (now Istanbul); and then to Adrianople (now Edirne). In 1863, at the time of his banishment from Baghdad to Constantinople, Bahá'u'lláh declared his claim to a divine mission to his family and followers. Tensions then grew between him and Subh-i-Azal, the appointed leader of the Bábís who did not recognize Bahá'u'lláh's claim. Throughout the rest of his life Bahá'u'lláh gained the allegiance of most of the Bábís, who came to be known as Bahá'ís. Beginning in 1866, he began declaring his mission as a Messenger of God in letters to the world's religious and secular rulers, including Pope Pius IX, Napoleon III, and Queen Victoria.
In 1868 Bahá'u'lláh was banished by Sultan Abdülâziz a final time to the Ottoman penal colony of `Akká, in present-day Israel. Towards the end of his life, the strict and harsh confinement was gradually relaxed, and he was allowed to live in a home near `Akká, while still officially a prisoner of that city. He died there in 1892. Bahá'ís regard his resting place at Bahjí as the Qiblih to which they turn in prayer each day.
Bahá'u'lláh wrote many written works taken as scripture in the religion of which only a fraction have been translated into English. There have been 15,000 works both small and large noted – the most significant of which are the Most Holy Book, the Book of Certitude, the Hidden Words, and the Seven Valleys. There is also a series of compilation volumes of smaller works the most significant of which is the Gleanings from the Writings of Bahá'u'lláh.
`Abdu'l-Bahá.
`Abbás Effendi was Bahá'u'lláh's eldest son, known by the title of `Abdu'l-Bahá (Servant of Bahá). His father left a Will that appointed `Abdu'l-Bahá as the leader of the Bahá'í community, and designated him as the "Centre of the Covenant", "Head of the Faith", and the sole authoritative interpreter of Bahá'u'lláh's writings. `Abdu'l-Bahá had shared his father's long exile and imprisonment, which continued until `Abdu'l-Bahá's own release as a result of the Young Turk Revolution in 1908. Following his release he led a life of travelling, speaking, teaching, and maintaining correspondence with communities of believers and individuals, expounding the principles of the Bahá'í Faith.
It is estimated that `Abdu'l-Bahá wrote over 27,000 works mostly in the form of letters of which only a fraction have been translated into English. Among the more well known are "The Secret of Divine Civilization", the "Tablet to Auguste-Henri Forel", and "Some Answered Questions". Additionally notes taken of a number of his talks were published in various volumes like "Paris Talks" during his journeys to the West.
Bahá'í administration.
Bahá'u'lláh's "Kitáb-i-Aqdas" and "The Will and Testament of `Abdu'l-Bahá" are foundational documents of the Bahá'í administrative order. Bahá'u'lláh established the elected Universal House of Justice, and `Abdu'l-Bahá established the appointed hereditary Guardianship and clarified the relationship between the two institutions. In his Will, `Abdu'l-Bahá appointed his eldest grandson, Shoghi Effendi, as the first "Guardian" of the Bahá'í Faith, serving as head of the religion until his death, for 36 years.
Shoghi Effendi throughout his lifetime translated Bahá'í texts; developed global plans for the expansion of the Bahá'í community; developed the Bahá'í World Centre; carried on a voluminous correspondence with communities and individuals around the world; and built the administrative structure of the religion, preparing the community for the election of the Universal House of Justice. He died in 1957 under conditions that did not allow for a successor to be appointed.
At local, regional, and national levels, Bahá'ís elect members to nine-person Spiritual Assemblies, which run the affairs of the religion. There are also appointed individuals working at various levels, including locally and internationally, which perform the function of propagating the teachings and protecting the community. The latter do not serve as clergy, which the Bahá'í Faith does not have. The Universal House of Justice, first elected in 1963, remains the successor and supreme governing body of the Bahá'í Faith, and its 9 members are elected every five years by the members of all National Spiritual Assemblies. Any male Bahá'í, 21 years or older, is eligible to be elected to the Universal House of Justice; all other positions are open to male and female Bahá'ís.
International plans.
In 1937, Shoghi Effendi launched a seven-year plan for the Bahá'ís of North America, followed by another in 1946. In 1953, he launched the first international plan, the Ten Year World Crusade. This plan included extremely ambitious goals for the expansion of Bahá'í communities and institutions, the translation of Bahá'í texts into several new languages, and the sending of Bahá'í pioneers into previously unreached nations. He announced in letters during the Ten Year Crusade that it would be followed by other plans under the direction of the Universal House of Justice, which was elected in 1963 at the culmination of the Crusade. The House of Justice then launched a nine-year plan in 1964, and a series of subsequent multi-year plans of varying length and goals followed, guiding the direction of the international Bahá'í community.
Annually, on 21 April, the Universal House of Justice sends a ‘Ridván’ message to the worldwide Bahá’í community, which generally gives an update on the progress made concerning the current plan, and provides further guidance for the year to come. The Bahá'ís around the world are currently being encouraged to focus on capacity building through children's classes, youth groups, devotional gatherings, and a systematic study of the religion known as study circles. Further focuses are involvement in social action and participation in the prevalent discourses of society. The years from 2001 until 2021 represent four successive five-year plans, culminating in the centennial anniversary of the passing of `Abdu'l-Bahá.
Demographics.
A Bahá'í published document reported 4.74 million Bahá'ís in 1986 growing at a rate of 4.4%. Bahá'í sources since 1991 usually estimate the worldwide Bahá'í population to be above 5 million. The "World Christian Encyclopedia" estimated 7.1 million Bahá'ís in the world in 2000, representing 218 countries, and 7.3 million in 2010 with the same source. They further state: "The Baha'i Faith is the only religion to have grown faster in every United Nations region over the past 100 years than the general population; Baha’i was thus the fastest-growing religion between 1910 and 2010, growing at least twice as fast as the population of almost every UN region." This source's only systematic flaw was to consistently have a higher estimate of Christians than other cross-national data sets.
From its origins in the Persian and Ottoman Empires, by the early 20th century there were a number of converts in South and South East Asia, Europe, and North America. During the 1950s and 1960s, vast travel teaching efforts brought the religion to almost every country and territory of the world. By the 1990s, Bahá'ís were developing programs for systematic consolidation on a large scale, and the early 21st century saw large influxes of new adherents around the world. The Bahá'í Faith is currently the largest religious minority in Iran, Panama, and Belize; the second largest international religion in Bolivia, Zambia, and Papua New Guinea; and the third largest international religion in Chad and Kenya.
According to "The World Almanac and Book of Facts 2004":
The Bahá'í religion was listed in "The Britannica Book of the Year" (1992–present) as the second most widespread of the world's independent religions in terms of the number of countries represented. According to "Britannica", the Bahá'í Faith (as of 2002) is established in 247 countries and territories; represents over 2,100 ethnic, racial, and tribal groups; has scriptures translated into over 800 languages; and has an estimated seven million adherents worldwide. Additionally, Bahá'ís have self-organized in most of the nations of the world.
The Bahá'í religion was ranked by the Foreign Policy magazine as the world's second fastest growing religion by percentage (1.7%) in 2007.
Social practices.
Laws.
The laws of the Bahá'í Faith primarily come from the "Kitáb-i-Aqdas", written by Bahá'u'lláh. The following are a few examples of basic laws and religious observances.
While some of the laws from the Kitáb-i-Aqdas are applicable at the present time and may be enforced to a degree by the administrative institutions, Bahá'u'lláh has provided for the progressive application of other laws that are dependent upon the existence of a predominantly Bahá'í society. The laws, when not in direct conflict with the civil laws of the country of residence, are binding on every Bahá'í, and the observance of personal laws, such as prayer or fasting, is the sole responsibility of the individual.
Marriage.
The purpose of marriage in the Bahá'i faith is mainly to foster spiritual harmony, fellowship and unity between a man and a woman and to provide a stable and loving environment for the rearing of children. The Bahá'í teachings on marriage call it a "fortress for well-being and salvation" and place marriage and the family as the foundation of the structure of human society. Bahá'u'lláh highly praised marriage, discouraged divorce and homosexuality, and required chastity outside of marriage; Bahá'u'lláh taught that a husband and wife should strive to improve the spiritual life of each other. Interracial marriage is also highly praised throughout Bahá'í scripture.
Bahá'ís intending to marry are asked to obtain a thorough understanding of the other's character before deciding to marry. Although parents should not choose partners for their children, once two individuals decide to marry, they must receive the consent of all living biological parents, whether they are Bahá'í or not. The Bahá'í marriage ceremony is simple; the only compulsory part of the wedding is the reading of the wedding vows prescribed by Bahá'u'lláh which both the groom and the bride read, in the presence of two witnesses. The vows are "We will all, verily, abide by the Will of God."
Work.
Monasticism is forbidden, and Bahá'ís attempt to ground their spirituality in ordinary daily life. Performing useful work, for example, is not only required but considered a form of worship. Bahá'u'lláh prohibited a mendicant and ascetic lifestyle. The importance of self-exertion and service to humanity in one's spiritual life is emphasised further in Bahá'u'lláh's writings, where he states that work done in the spirit of service to humanity enjoys a rank equal to that of prayer and worship in the sight of God.
Places of worship.
Most Bahá'í meetings occur in individuals' homes, local Bahá'í centers, or rented facilities. Worldwide, there are currently seven Bahá'í Houses of Worship, with an eighth near completion in Chile, and a further seven planned as of April 2012. Bahá'í writings refer to an institution called a "Mashriqu'l-Adhkár" (Dawning-place of the Mention of God), which is to form the center of a complex of institutions including a hospital, university, and so on. The first ever Mashriqu'l-Adhkár in `Ishqábád, Turkmenistan, has been the most complete House of Worship.
Calendar.
The Bahá'í calendar is based upon the calendar established by the Báb. The year consists of 19 months, each having 19 days, with four or five intercalary days, to make a full solar year. The Bahá'í New Year corresponds to the traditional Persian New Year, called Naw Rúz, and occurs on the vernal equinox, 21 March, at the end of the month of fasting. Bahá'í communities gather at the beginning of each month at a meeting called a Feast for worship, consultation and socializing.
Each of the 19 months is given a name which is an attribute of God; some examples include Bahá’ (Splendour), ‘Ilm (Knowledge), and Jamál (Beauty). The Bahá'í week is familiar in that it consists of seven days, with each day of the week also named after an attribute of God. Bahá'ís observe 11 Holy Days throughout the year, with work suspended on 9 of these. These days commemorate important anniversaries in the history of the religion.
Symbols.
The symbols of the religion are derived from the Arabic word Bahá’ ( "splendor" or "glory"), with a numerical value of 9, which is why the most common symbol is the nine-pointed star. The ringstone symbol and calligraphy of the Greatest Name are also often encountered. The former consists of two five-pointed stars interspersed with a stylized Bahá’ whose shape is meant to recall the three onenesses, while the latter is a calligraphic rendering of the phrase Yá Bahá'u'l-Abhá ( "O Glory of the Most Glorious!").The five-pointed star is the symbol of the Bahá'í Faith. In the Bahá'í Faith, the star is known as the Haykal (), and it was initiated and established by the Báb. The Báb and Bahá'u'lláh wrote various works in the form of a pentagram.
Socio-economic development.
Since its inception the Bahá'í Faith has had involvement in socio-economic development beginning by giving greater freedom to women, promulgating the promotion of female education as a priority concern, and that involvement was given practical expression by creating schools, agricultural coops, and clinics.
The religion entered a new phase of activity when a message of the Universal House of Justice dated 20 October 1983 was released. Bahá'ís were urged to seek out ways, compatible with the Bahá'í teachings, in which they could become involved in the social and economic development of the communities in which they lived. Worldwide in 1979 there were 129 officially recognized Bahá'í socio-economic development projects. By 1987, the number of officially recognized development projects had increased to 1482.
United Nations.
Bahá'u'lláh wrote of the need for world government in this age of humanity's collective life. Because of this emphasis the international Bahá'í community has chosen to support efforts of improving international relations through organizations such as the League of Nations and the United Nations, with some reservations about the present structure and constitution of the UN. The Bahá'í International Community is an agency under the direction of the Universal House of Justice in Haifa, and has consultative status with the following organizations:
The Bahá'í International Community has offices at the United Nations in New York and Geneva and representations to United Nations regional commissions and other offices in Addis Ababa, Bangkok, Nairobi, Rome, Santiago, and Vienna. In recent years an Office of the Environment and an Office for the Advancement of Women were established as part of its United Nations Office. The Bahá'í Faith has also undertaken joint development programs with various other United Nations agencies. In the 2000 Millennium Forum of the United Nations a Bahá'í was invited as the only non-governmental speaker during the summit.
Persecution.
Bahá'ís continue to be persecuted in Islamic countries, as Islamic leaders do not recognize the Bahá'í Faith as an independent religion, but rather as apostasy from Islam. The most severe persecutions have occurred in Iran, where over 200 Bahá'ís were executed between 1978 and 1998, and in Egypt. The rights of Bahá'ís have been restricted to greater or lesser extents in numerous other countries, including Afghanistan, Indonesia, Iraq, Morocco, and several countries in sub-Saharan Africa.
Iran.
The marginalization of the Iranian Bahá'ís by current governments is rooted in historical efforts by Muslim clergy to persecute the religious minority. When the Báb started attracting a large following, the clergy hoped to stop the movement from spreading by stating that its followers were enemies of God. These clerical directives led to mob attacks and public executions. Starting in the twentieth century, in addition to repression that impacted individual Bahá'ís, centrally directed campaigns that targeted the entire Bahá'í community and its institutions were initiated. In one case in Yazd in 1903 more than 100 Bahá'ís were killed. Bahá'í schools, such as the Tarbiyat boys' and girls' schools in Tehran, were closed in the 1930s and 40s, Bahá'í marriages were not recognized and Bahá'í texts were censored.
During the reign of Mohammad Reza Pahlavi, to divert attention from economic difficulties in Iran and from a growing nationalist movement, a campaign of persecution against the Bahá'ís was instituted. An approved and coordinated anti-Bahá'í campaign (to incite public passion against the Bahá'ís) started in 1955 and it included the spreading of anti-Bahá'í propaganda on national radio stations and in official newspapers. In the late 1970s the Shah's regime consistently lost legitimacy due to criticism that it was pro-Western. As the anti-Shah movement gained ground and support, revolutionary propaganda was spread which alleged that some of the Shah's advisors were Bahá'ís. Bahá'ís were portrayed as economic threats, and as supporters of Israel and the West, and societal hostility against the Bahá'ís increased.
Since the Islamic Revolution of 1979 Iranian Bahá'ís have regularly had their homes ransacked or have been banned from attending university or from holding government jobs, and several hundred have received prison sentences for their religious beliefs, most recently for participating in study circles. Bahá'í cemeteries have been desecrated and property has been seized and occasionally demolished, including the House of Mírzá Buzurg, Bahá'u'lláh's father. The House of the Báb in Shiraz, one of three sites to which Bahá'ís perform pilgrimage, has been destroyed twice.
According to a US panel, attacks on Bahá'ís in Iran increased under Mahmoud Ahmadinejad's presidency. The United Nations Commission on Human Rights revealed an October 2005 confidential letter from Command Headquarters of the Armed Forces of Iran ordering its members to identify Bahá'ís and to monitor their activities. Due to these actions, the Special Rapporteur of the United Nations Commission on Human Rights stated on 20 March 2006, that she "also expresses concern that the information gained as a result of such monitoring will be used as a basis for the increased persecution of, and discrimination against, members of the Bahá'í faith, in violation of international standards. The Special Rapporteur is concerned that this latest development indicates that the situation with regard to religious minorities in Iran is, in fact, deteriorating.
On 14 May 2008, members of an informal body known as the "Friends" that oversaw the needs of the Bahá'í community in Iran were arrested and taken to Evin prison. The Friends court case has been postponed several times, but was finally underway on 12 January 2010. Other observers were not allowed in the court. Even the defence lawyers, who for two years have had minimal access to the defendants, had difficulty entering the courtroom. The chairman of the U.S. Commission on International Religious Freedom said that it seems that the government has already predetermined the outcome of the case and is violating international human rights law. Further sessions were held on 7 February 2010, 12 April 2010 and 12 June 2010. On 11 August 2010 it became known that the court sentence was 20 years imprisonment for each of the seven prisoners which was later reduced to ten years. After the sentence, they were transferred to Gohardasht prison. In March 2011 the sentences were reinstated to the original 20 years. On 3 January 2010, Iranian authorities detained ten more members of the Baha'i minority, reportedly including Leva Khanjani, granddaughter of Jamaloddin Khanjani, one of seven Baha'i leaders jailed since 2008 and in February, they arrested his son, Niki Khanjani.
The Iranian government claims that the Bahá'í Faith is not a religion, but is instead a political organization, and hence refuses to recognize it as a minority religion. However, the government has never produced convincing evidence supporting its characterization of the Bahá'í community. Also, the government's statements that Bahá'ís who recanted their religion would have their rights restored, attest to the fact that Bahá'ís are persecuted solely for their religious affiliation. The Iranian government also accuses the Bahá'í Faith of being associated with Zionism because the Bahá'í World Centre is located in Haifa, Israel. These accusations against the Bahá'ís have no basis in historical fact, and the accusations are used by the Iranian government to use the Bahá'ís as "scapegoats". In fact it was the Iranian leader Naser al-Din Shah Qajar who banished Bahá'u'lláh from Persia to the Ottoman Empire and Bahá'u'lláh was later exiled by the Ottoman Sultan, at the behest of the Persian Shah, to territories further away from Iran and finally to Acre in Syria, which only a century later was incorporated into the state of Israel.
Egypt.
Bahá'í institutions and community activities have been illegal under Egyptian law since 1960. All Bahá'í community properties, including Bahá'í centers, libraries, and cemeteries, have been confiscated by the government and fatwas have been issued charging Bahá'ís with apostasy.
The Egyptian identification card controversy began in the 1990s when the government modernized the electronic processing of identity documents, which introduced a de facto requirement that documents must list the person's religion as Muslim, Christian, or Jewish (the only three religions officially recognized by the government). Consequently, Bahá'ís were unable to obtain government identification documents (such as national identification cards, birth certificates, death certificates, marriage or divorce certificates, or passports) necessary to exercise their rights in their country unless they lied about their religion, which conflicts with Bahá'í religious principle. Without documents, they could not be employed, educated, treated in hospitals, travel outside of the country, or vote, among other hardships. Following a protracted legal process culminating in a court ruling favorable to the Bahá'ís, the interior minister of Egypt released a decree on 14 April 2009, amending the law to allow Egyptians who are not Muslim, Christian, or Jewish to obtain identification documents that list dash in place of one of the three recognized religions. The first identification cards were issued to two Bahá'ís under the new decree on 8 August 2009.

</doc>
<doc id="4257" url="https://en.wikipedia.org/wiki?curid=4257" title="Burgundians">
Burgundians

The Burgundians (; ; ; ) were a large East Germanic or Vandal tribe, or group of tribes, who lived in the area of modern Poland in the time of the Roman empire. 
In the late Roman period, as the empire came under pressure from many such "barbarian" peoples, a powerful group of Burgundians and other Vandalic tribes moved westwards towards the Roman frontiers along the Rhine Valley, making them neighbors of the Franks, forming their kingdoms to the north, and the Suebic Alemanni who were settling to their south, also near the Rhine. They established themselves in Worms, but with Roman cooperation their descendants eventually established the Kingdom of the Burgundians much further south, and within the empire, in the western Alps region where modern Switzerland, France and Italy meet. This later became a component of the Frankish empire. The name of this Kingdom survives in the regional appellation, Burgundy, which is a region in modern France, representing only a part of that kingdom.
Another part of Burgundians stayed in their previous homeland in Oder-Vistula basin and formed a contingent in Attila's Hunnic army by 451.
Before clear documentary evidence begins, the Burgundians may have originally emigrated from mainland Scandinavia to the Baltic island of Bornholm, and from there to the Vistula basin, in the middle of modern Poland.
Name.
The name of the Burgundians has since remained connected to the area of modern France that still bears their name: see the later history of Burgundy. Between the 6th and 20th centuries, however, the boundaries and political connections of this area have changed frequently, with none of the changes having had anything to do with the original Burgundians. The name "Burgundians" used here and generally used by English writers to refer to the Burgundiones is a later formation and more precisely refers to the inhabitants of the territory of Burgundy which was named from the people called Burgundiones. The descendants of the Burgundians today are found primarily in historical Burgundy and among the west Swiss.
History.
Background.
The Burgundians had a tradition of Scandinavian origin which finds support in place-name evidence and archaeological evidence (Stjerna) and many consider their tradition to be correct (e.g. Musset, p. 62). The Burgundians are believed to have then emigrated to the Baltic island of Bornholm ("the island of the Burgundians" in Old Norse). However, by about 250 the population of Bornholm had largely disappeared from the island. Most cemeteries ceased to be used, and those that were still used had few burials (Stjerna, in Nerman 1925:176). In "Þorsteins saga Víkingssonar" ("The Saga of Thorstein, Viking's Son"), the Veseti settled in an island or holm, which was called Borgund's holm, i.e. Bornholm. Alfred the Great's translation of "Orosius" uses the name "Burgenda land". The poet and early mythologist Viktor Rydberg (1828–1895), ("Our Fathers' Godsaga") asserted from an early medieval source, "Vita Sigismundi", that they themselves retained oral traditions about their Scandinavian origin.
Early Roman sources such as Tacitus and Pliny the Elder knew little concerning the Germanic peoples east of the Elbe river, or on the Baltic Sea. Pliny (IV.28) however mentions them among the Vandalic or Eastern Germanic Germani peoples, including also the Goths. Claudius Ptolemy lists them as living between the Suevus (probably the Oder) and Vistula rivers, north of the Lugii, and south of the coast dwelling tribes. Around the mid 2nd century AD, there was a significant migration by Germanic tribes of Scandinavian origin (Rugii, Goths, Gepidae, Vandals, Burgundians, and others) towards the south-east, creating turmoil along the entire Roman frontier. These migrations culminated in the Marcomannic Wars, which resulted in widespread destruction and the first invasion of Italy in the Roman Empire period. Jordanes reports that during the 3rd century, the Burgundians living in the Vistula basin were almost annihilated by Fastida, king of the Gepids, whose kingdom was at the mouth of the Vistula.
In the late 3rd century, the Burgundians appear on the east bank of the Rhine, confronting Roman Gaul. Zosimus (1.68) reports them being defeated by the emperor Probus in 278 in Gaul. At this time they were led by a Vandal king. A few years later, Claudius Mamertinus mentions them along with the Alamanni, a Suebic people. These two peoples had moved into the Agri Decumates on the eastern side of the Rhine, an area today referred to still as Swabia, at times attacking Roman Gaul together and sometimes fighting each other. He also mentions that the Goths had previously defeated the Burgundians.
Ammianus Marcellinus, on the other hand, claimed that the Burgundians were descended from Romans. The Roman sources do not speak of any specific migration from Poland by the Burgundians (although other Vandalic peoples are more clearly mentioned as having moved west in this period), and so there have historically been some doubts about the link between the eastern and western Burgundians.
In 369/370, the Emperor Valentinian I enlisted the aid of the Burgundians in his war against the Alemanni.
Approximately four decades later, the Burgundians appear again. Following Stilicho's withdrawal of troops to fight Alaric I the Visigoth in AD 406-408, the northern tribes crossed the Rhine and entered the Empire in the "Völkerwanderung", or Germanic migrations. Among them were the Alans, Vandals, the Suevi, and possibly some Burgundians. A part of Burgundians migrated westwards and settled as "foederati" in the Roman province of Germania Secunda along the Middle Rhine. Another part of Burgundians stayed in their previous homeland in Oder-Vistula interfluvial and formed a contingent in Attila's Hunnic army by 451.
Kingdom.
Establishment.
In 411, the Burgundian king Gundahar (or "Gundicar") set up a puppet emperor, Jovinus, in cooperation with Goar, king of the Alans. With the authority of the Gallic emperor that he controlled, Gundahar settled on the left (Roman) bank of the Rhine, between the river Lauter and the Nahe, seizing Worms, Speyer, and Strassburg. Apparently as part of a truce, the Emperor Honorius later officially "granted" them the land, (Prosper, a. 386) with its capital at the old Celtic Roman settlement of Borbetomagus (present Worms).
Despite their new status as "foederati", Burgundian raids into Roman Upper Gallia Belgica became intolerable and were ruthlessly brought to an end in 436, when the Roman general Aëtius called in Hun mercenaries who overwhelmed the Rhineland kingdom in 437. Gundahar was killed in the fighting, reportedly along with the majority of the Burgundian tribe. (Prosper; "Chronica Gallica 452"; Hydatius; and Sidonius Apollinaris)
The destruction of Worms and the Burgundian kingdom by the Huns became the subject of heroic legends that were afterwards incorporated in the "Nibelungenlied"—on which Wagner based his Ring Cycle—where King Gunther (Gundahar) and Queen Brünhild hold their court at Worms, and Siegfried comes to woo Kriemhild. (In Old Norse sources the names are "Gunnar", "Brynhild", and "Gudrún" as normally rendered in English.) In fact, the "Etzel" of the "Nibelungenlied" is based on Attila the Hun.
Settlement in Savoy.
For reasons not cited in the sources, the Burgundians were granted "foederati" status a second time, and in 443 were resettled by Aëtius in the region of "Sapaudia". ("Chronica Gallica 452") Though the precise geography is uncertain, "Sapaudia" corresponds to the modern-day Savoy, and the Burgundians probably lived near "Lugdunum", known today as Lyon. (Wood 1994, Gregory II, 9) A new king Gundioc or "Gunderic", presumed to be Gundahar's son, appears to have reigned following his father's death. (Drew, p. 1) The historian Pline tells us that Gonderic reigned the areas of Saône, Dauphiny, Savoie and a part of Provence. He set up Vienne as the capital of the kingdom of Burgundy. In all, eight Burgundian kings of the house of Gundahar ruled until the kingdom was overrun by the Franks in 534.
As allies of Rome in its last decades, the Burgundians fought alongside Aëtius and a confederation of Visigoths and others in the battle against Attila at the Battle of Châlons (also called "The Battle of the Catalaunian Fields") in 451. The alliance between Burgundians and Visigoths seems to have been strong, as Gundioc and his brother Chilperic I accompanied Theodoric II to Spain to fight the Sueves in 455. (Jordanes, "Getica", 231)
Aspirations to the Empire.
Also in 455, an ambiguous reference "infidoque tibi Burdundio ductu" (Sidonius Apollinaris in "Panegyr. Avit". 442.) implicates an unnamed treacherous Burgundian leader in the murder of the emperor Petronius Maximus in the chaos preceding the sack of Rome by the Vandals. The Patrician Ricimer is also blamed; this event marks the first indication of the link between the Burgundians and Ricimer, who was probably Gundioc's brother-in-law and Gundobad's uncle, (John Malalas, 374)
The Burgundians, apparently confident in their growing power, negotiated in 456 a territorial expansion and power sharing arrangement with the local Roman senators. (Marius of Avenches)
In 457, Ricimer overthrew another emperor, Avitus, raising Majorian to the throne. This new emperor proved unhelpful to Ricimer and the Burgundians. The year after his ascension, Majorian stripped the Burgundians of the lands they had acquired two years earlier. After showing further signs of independence, he was murdered by Ricimer in 461.
Ten years later, in 472, Ricimer–who was by now the son-in-law of the Western Emperor Anthemius–was plotting with Gundobad to kill his father-in-law; Gundobad beheaded the emperor (apparently personally). ("Chronica Gallica 511"; John of Antioch, fr. 209; Jordanes, "Getica", 239) Ricimer then appointed Olybrius; both died, surprisingly of natural causes, within a few months. Gundobad seems then to have succeeded his uncle as Patrician and king-maker, and raised Glycerius to the throne. (Marius of Avenches; John of Antioch, fr. 209)
In 474, Burgundian influence over the empire seems to have ended. Glycerius was deposed in favor of Julius Nepos, and Gundobad returned to Burgundy, presumably at the death of his father Gundioc. At this time or shortly afterward, the Burgundian kingdom was divided between Gundobad and his brothers, Godigisel, Chilperic II, and Gundomar I. (Gregory, II, 28)
Consolidation of the Kingdom.
According to Gregory of Tours, the years following Gundobad's return to Burgundy saw a bloody consolidation of power. Gregory states that Gundobad murdered his brother Chilperic, drowning his wife and exiling their daughters (one of whom was to become the wife of Clovis the Frank, and was reputedly responsible for his conversion). This is contested by, e.g., Bury, who points out problems in much of Gregory's chronology for the events.
C.500, when Gundobad and Clovis were at war, Gundobad appears to have been betrayed by his brother Godegisel, who joined the Franks; together Godegisel's and Clovis' forces "crushed the army of Gundobad." (Marius a. 500; Gregory, II, 32) Gundobad was temporarily holed up in Avignon, but was able to re-muster his army and sacked Vienne, where Godegisel and many of his followers were put to death. From this point, Gundobad appears to have been the sole king of Burgundy. (e.g., Gregory, II, 33) This would imply that his brother Gundomar was already dead, though there are no specific mentions of the event in the sources.
Either Gundobad and Clovis reconciled their differences, or Gundobad was forced into some sort of vassalage by Clovis' earlier victory, as the Burgundian king appears to have assisted the Franks in 507 in their victory over Alaric II the Visigoth.
During the upheaval, sometime between 483-501, Gundobad began to set forth the "Lex Gundobada" (see below), issuing roughly the first half, which drew upon the "Lex Visigothorum". (Drew, p. 1) Following his consolidation of power, between 501 and his death in 516, Gundobad issued the second half of his law, which was more originally Burgundian.
Fall.
The Burgundians were extending their power over southeastern Gaul; that is, northern Italy, western Switzerland, and southeastern France. In 493 Clovis, king of the Franks, married the Burgundian princess Clotilda (daughter of Chilperic), who converted him to the Catholic faith.
At first allied with Clovis' Franks against the Visigoths in the early 6th century, the Burgundians were eventually conquered at Autun by the Franks in 532 after a first attempt in the Battle of Vézeronce. The Burgundian kingdom was made part of the Merovingian kingdoms, and the Burgundians themselves were by and large absorbed as well.
Physical appearance.
The 5th century Gallo-Roman poet and landowner Sidonius, who at one point lived with the Burgundians, described them as a long-haired people of immense physical size:
Language.
The Burgundian language belonged to the East Germanic language group. It appears to have become extinct during the late sixth century.
Little is known of the language. Some proper names of Burgundians are recorded, and some words used in the area in modern times are thought to be derived from the ancient Burgundian language, but it is often difficult to distinguish these from Germanic words of other origin, and in any case the modern form of the words is rarely suitable to infer much about the form in the old language.
Culture.
Religion.
Somewhere in the east the Burgundians had converted to the Arian form of Christianity from their native Germanic polytheism. Their Arianism proved a source of suspicion and distrust between the Burgundians and the Catholic Western Roman Empire. Divisions were evidently healed or healing circa AD 500, however, as Gundobad, one of the last Burgundian kings, maintained a close personal friendship with Avitus, the bishop of Vienne. Moreover, Gundobad's son and successor, Sigismund, was himself a Catholic, and there is evidence that many of the Burgundian people had converted by this time as well, including several female members of the ruling family.
Law.
The Burgundians left three legal codes, among the earliest from any of the Germanic tribes.
The Liber Constitutionum sive Lex Gundobada ("The Book of the Constitution following the Law of Gundobad"), also known as the "Lex Burgundionum", or more simply the "Lex Gundobada" or the "Liber", was issued in several parts between 483 and 516, principally by Gundobad, but also by his son, Sigismund. (Drew, p. 6–7) It was a record of Burgundian customary law and is typical of the many Germanic law codes from this period. In particular, the "Liber" borrowed from the "Lex Visigothorum" (Drew, p. 6) and influenced the later "Lex Ribuaria". (Rivers, p. 9) The "Liber" is one of the primary sources for contemporary Burgundian life, as well as the history of its kings.
Like many of the Germanic tribes, the Burgundians' legal traditions allowed the application of separate laws for separate ethnicities. Thus, in addition to the "Lex Gundobada", Gundobad also issued (or codified) a set of laws for Roman subjects of the Burgundian kingdom, the "Lex Romana Burgundionum" ("The Roman Law of the Burgundians").
In addition to the above codes, Gundobad's son Sigismund later published the "Prima Constitutio". 

</doc>
<doc id="4260" url="https://en.wikipedia.org/wiki?curid=4260" title="Dots and Boxes">
Dots and Boxes

Dots and boxes (also known as Boxes, Squares, Paddocks, Pigs in a Pen, Square-it, Dots and Dashes, Dit Dot Dash, Dots, Line Game, Smart Dots, Dot Boxing, or, simply, the Dot Game) is a pencil and paper game for two players (or sometimes, more than two) first published in 1889 by Édouard Lucas.
Starting with an empty grid of dots, players take turns, adding a single horizontal or vertical line between two unjoined adjacent dots. A player who completes the fourth side of a 1×1 box earns one point and takes another turn. (The points are typically recorded by placing in the box an identifying mark of the player, such as an initial). The game ends when no more lines can be placed. The winner of the game is the player with the most points.
The board may be of any size. When short on time, 2×2 boxes (created by a square of 9 dots) is good for beginners, and 5×5 is good for experts.
The diagram on the right shows a game being played on the 2×2 board. The second player (B) plays the mirror image of the first player's move, hoping to divide the board into two pieces and tie the game. The first player (A) makes a "sacrifice" at move 7; B accepts the sacrifice, getting one box. However, B must now add another line, and connects the center dot to the center-right dot, causing the remaining boxes to be joined together in a "chain" as shown at the end of move 8. With A's next move, A gets them all, winning 3–1.
Strategy.
For most novice players, the game begins with a phase of more-or-less random connecting of dots, where the only strategy is to avoid adding the third side to any box. This continues until all the remaining (potential) boxes are joined together into "chains" – groups of one or more adjacent boxes in which any move gives all the boxes in the chain to the opponent. At this point, players typically take all available boxes, then open the smallest available chain to their opponent. For example, a novice player faced with a situation like position 1 in the diagram on the right, in which some boxes can be captured, may take all the boxes in the chain, resulting in position 2. But with their last move, they have to open the next (and larger) chain, and the novice loses the game.
A more experienced player faced with position 1 will instead play the "double-cross strategy", taking all but 2 of the boxes in the chain and leaving position 3. The opponent will take these last two boxes, but will then be forced to open the next chain. By moving to position 3, player A wins. The same double-cross strategy applies however many long chains there are: a player using this strategy will take all but two of the boxes in each chain, and take all the boxes in the last chain. If the chains are long enough then this player will certainly win.
The next level of strategic complexity, between experts who would both use the double-cross strategy if they were allowed to do so, is a battle for "control": An expert player tries to force their opponent to be the one to open the first long chain, because the player who first opens a long chain usually loses. Against a player who doesn't understand the concept of a sacrifice, the expert simply has to make the correct number of sacrifices to encourage the opponent to hand him the first chain long enough to ensure a win. If the other player also knows to offer sacrifices, the expert also has to manipulate the number of available sacrifices through earlier play.
In combinatorial game theory dots and boxes is an impartial game, and many positions can be analyzed using Sprague–Grundy theory. However, dots and boxes lacks the normal play convention of most impartial games where the last player to move wins, which complicates the analysis considerably.
Unusual grids.
Dots and boxes need not be played on a rectangular grid. It can be played on a triangular grid or a hexagonal grid. There is also a variant in Bolivia when it is played in a Chakana or Inca Cross grid, which adds more complications to the game.
Dots-and-boxes has a dual form called "strings-and-coins". This game is played on a network of coins (vertices) joined by strings (edges). Players take turns to cut a string. When a cut leaves a coin with no strings, the player pockets the coin and takes another turn. The winner is the player who pockets the most coins. Strings-and-coins can be played on an arbitrary graph.
A variant played in Poland allows a player to claim a region of several squares as soon as its boundary is completed.
In the Netherlands it is called "kamertje verhuren" ("rent-a-room"), where the outer border already has lines. In analysis of dots-and-boxes, starting with outer lines is called a "Swedish board" while the standard version is called an "American board". An intermediate version with the outer left and bottom sides starting with lines is called an "Icelandic board".
Board game.
A board game version of Dots and Boxes is available through Shapeways. Instead of drawing lines on paper players place tiles in slots edge up in a specially designed board. When a spot on the board is surrounded by a player they place one of their tiles on the board with their side facing up. The board does not come with playing pieces but was designed to be used with "Scrabble" tiles. One player plays with the lettered side and the other is represented by the blank side.

</doc>
<doc id="4261" url="https://en.wikipedia.org/wiki?curid=4261" title="Big Brother (Nineteen Eighty-Four)">
Big Brother (Nineteen Eighty-Four)

Big Brother is a fictional character and symbol in George Orwell's novel "Nineteen Eighty-Four". He is ostensibly the leader (either the actual enigmatic dictator or perhaps a symbolic figurehead) of Oceania, a totalitarian state wherein the ruling Party wields total power "for its own sake" over the inhabitants.
In the society that Orwell describes, every citizen is under constant surveillance by the authorities, mainly by telescreens (with the exception of the Proles). The people are constantly reminded of this by the slogan "Big Brother is watching you": a maxim which is ubiquitously on display. In modern culture the term "Big Brother" has entered the lexicon as a synonym for abuse of government power, particularly in respect to civil liberties, often specifically related to mass surveillance.
Purported origins.
In the essay section of his novel "1985", Anthony Burgess states that Orwell got the idea for the name of Big Brother from advertising billboards for educational correspondence courses from a company called "Bennett's", current during World War II. The original posters showed J. M. Bennett himself: a kindly-looking old man offering guidance and support to would-be students with the phrase "Let me be your father" attached. According to Burgess, after Bennett's death, his son took over the company, and the posters were replaced with pictures of the son (who looked imposing and stern in contrast to his father's kindly demeanor) with the text "Let me be your big brother."
Additional speculation from Douglas Kellner of UCLA argued that Big Brother represents Joseph Stalin.
Appearance inside the novel.
Existence.
In the novel, it is never made clear whether Big Brother is or had been a real person, or is a fictional personification of the Party, similar to Britannia and Uncle Sam. Big Brother is described as appearing on posters and telescreens as a handsome man in his mid-40s.
In Party propaganda, Big Brother is presented as one of the founders of the Party, along with Goldstein. At one point, Winston Smith, the protagonist of Orwell's novel, tries "to remember in what year he had first heard mention of Big Brother. He thought it must have been at some time in the sixties, but it was impossible to be certain. In the Party histories, of course, Big Brother figured as the leader and guardian of the Revolution since its very earliest days. His exploits had been gradually pushed backwards in time until already they extended into the fabulous world of the forties and the thirties, when the capitalists in their strange cylindrical hats still rode through the streets of London..."
In the book "The Theory and Practice of Oligarchical Collectivism", read by Winston Smith and purportedly written by Goldstein, Big Brother is referred to as infallible and all-powerful. No-one has ever seen him and there is a reasonable certainty that he will never die. He is simply "the guise in which the Party chooses to exhibit itself to the world", since the emotions of love, fear and reverence are more easily focussed on an individual (if only a face on the hoardings and a voice on the telescreens), than an organisation. When Winston Smith is later arrested, O'Brien repeats that Big Brother will never die. When Smith asks if Big Brother exists, O'Brien describes him as "the embodiment of the Party" and says that he will exist as long as the Party exists. When Winston asks "Does Big Brother exist the same way I do?" (meaning is Big Brother an actual human being), O'Brien replies "You do not exist" (meaning that Smith is now an unperson; an example of doublethink).
Cult of personality.
A spontaneous ritual of devotion to Big Brother ("BB") is illustrated at the end of the "Two Minutes Hate":
Though Oceania's Ministry of Truth, Ministry of Plenty, and Ministry of Peace each have names with meanings deliberately opposite to their real purpose, the Ministry of Love is perhaps the most straightforward: "rehabilitated thought criminals" leave the Ministry as loyal subjects who have been brainwashed into adoring (loving) Big Brother, hence its name.
Legacy.
Since the publication of "Nineteen Eighty-Four" the phrase "Big Brother" has come into common use to describe any prying or overly-controlling authority figure, and attempts by government to increase surveillance.
Big Brother and other Orwellian imagery are often referenced in the type of joke known as the Russian reversal.
The magazine "Book" ranked Big Brother No. 59 on its 100 Best Characters in Fiction Since 1900 list. "Wizard" magazine rated him the 75th greatest villain of all time.
It is commonly assumed that Keyser Soze, from the 1995 film "Usual Suspects", is the "Big Brother" the criminal underworld.
The worldwide reality television show "Big Brother" is based on the novel's concept of people being under constant surveillance. In 2000, after the U.S. version of the CBS program "Big Brother" premiered, the Estate of George Orwell sued CBS and its production company "Orwell Productions, Inc." in federal court in Chicago for copyright and trademark infringement. The case was "Estate of Orwell v. CBS", 00-c-5034 (ND Ill). On the eve of trial, the case settled worldwide to the parties' "mutual satisfaction"; the amount that CBS paid to the Orwell Estate was not disclosed. CBS had not asked the Estate for permission. Under current laws the novel will remain under copyright protection until 2020 in the European Union and until 2044 in the United States.
The iconic image of Big Brother (played by David Graham) played a key role in Apple's 1984 television commercial introducing the Macintosh. The Orwell Estate viewed the Apple commercial as a copyright infringement, and sent a cease-and-desist letter to Apple and its advertising agency. The commercial was never televised again. Subsequent (now posthumous) ads featuring Steve Jobs (for a variety of products including audio books) have mimicked the format and appearance of that original ad campaign, with the appearance of Steve Jobs nearly identical to that of Big Brother. In 2008, the Simpsons animated television series spoofed the Apple Big Brother commercial in an episode entitled "Mypods and Boomsticks."
The December 2002 issue of "Gear" magazine featured a story about technologies and trends that could violate personal privacy moving society closer to a "Big Brother" state and utilised a recreation of the movie poster from the film version of "1984" created by Dallmeierart.com.
Computer company Microsoft patented in 2011 a product distribution system with a camera or capture device that monitors the viewers that consume the product, allowing the provider to take "remedial action" if the actual viewers do not match the distribution license. The system has been compared with "1984"'s telescreen surveillance system.

</doc>
<doc id="4266" url="https://en.wikipedia.org/wiki?curid=4266" title="Binary search algorithm">
Binary search algorithm

In computer science, a binary search or half-interval search algorithm finds the position of a target value within a sorted array. The binary search algorithm can be classified as a dichotomic divide-and-conquer search algorithm and executes in logarithmic time.
Overview.
The binary search algorithm begins by comparing the target value to the value of the middle element of the sorted array. If the target value is equal to the middle element's value, then the position is returned and the search is finished. If the target value is less than the middle element's value, then the search continues on the lower half of the array; or if the target value is greater than the middle element's value, then the search continues on the upper half of the array. This process continues, eliminating half of the elements, and comparing the target value to the value of the middle element of the remaining elements - until the target value is either found (and its associated element position is returned), or until the entire array has been searched (and "not found" is returned).
Example.
 Sorted array: L = , 3, 4, 6, 8, 9, 1
Number guessing game.
This rather simple game begins something like "I'm thinking of an integer between forty and sixty inclusive, and to your guesses I'll respond 'Higher', 'Lower', or 'Yes!' as might be the case."
Supposing that "N" is the number of possible values (here, twenty-one, as "inclusive" was stated), then at most formula_1 questions are required to determine the number, since each question halves the search space. Note that one less question (iteration) is required than for the general algorithm, since the number is already constrained to be within a particular range.
Even if the number to guess can be arbitrarily large, in which case there is no upper bound "N", the number can be found in at most formula_2 steps (where "k" is the (unknown) selected number) by first finding an upper bound with one-sided binary search. For example, if the number were 11, the following sequence of guesses could be used to find it: 1 (Higher), 2 (Higher), 4 (Higher), 8 (Higher), 16 (Lower), 12 (Lower), 10 (Higher). Now we know that the number must be 11 because it is higher than 10 and lower than 12.
One could also extend the method to include negative numbers; for example the following guesses could be used to find −13: 0, −1, −2, −4, −8, −16, −12, −14. Now we know that the number must be −13 because it is lower than −12 and higher than −14.
Word lists.
People typically use a mixture of the binary search and interpolative search algorithms when searching a telephone book, after the initial guess we exploit the fact that the entries are sorted and can rapidly find the required entry. For example, when searching for Smith, if Rogers and Thomas have been found, one can flip to a page about halfway between the previous guesses. If this shows Samson, it can be concluded that Smith is somewhere between the Samson and Thomas pages so these can be divided.
Applications to complexity theory.
Even if we do not know a fixed range the number "k" falls in, we can still determine its value by asking formula_3 simple yes/no questions of the form "Is "k" greater than "x"?" for some number "x". As a simple consequence of this, if you can answer the question "Is this integer property "k" greater than a given value?" in some amount of time then you can find the value of that property in the same amount of time with an added factor of formula_4. This is called a "reduction", and it is because of this kind of reduction that most complexity theorists concentrate on decision problems, algorithms that produce a simple yes/no answer.
For example, suppose we could answer "Does this "n" x "n" matrix have permanent larger than "k"?" in O("n") time. Then, by using binary search, we could find the (ceiling of the) permanent itself in O("n" log "p") time, where "p" is the value of the permanent. Notice that "p" is not the size of the input, but the "value" of the output; given a matrix whose maximum item (in absolute value) is "m", "p" is bounded by formula_5. Hence log "p" = O("n" log "n" + "n" log "m"). A binary search could find the permanent in O("n" log "n" + "n" log "m").
Algorithm.
Recursive.
A straightforward implementation of binary search is recursive. The initial call uses the indices of the entire array to be searched. The procedure then calculates an index midway between the two indices, determines which of the two subarrays to search, and then does a recursive call to search that subarray. Each of the calls is tail recursive, so a compiler need not make a new stack frame for each call. The variables codice_1 and codice_2 are the lowest and highest inclusive indices that are searched.
It is invoked with initial codice_1 and codice_2 values of codice_5 and codice_6 for a zero based array of length N.
The number type "int" shown in the code has an influence on how the midpoint calculation can be implemented correctly. With unlimited numbers, the midpoint can be calculated as codice_7. In practical programming, however, the calculation is often performed with numbers of a limited range, and then the intermediate result codice_8 might overflow. With limited numbers, the midpoint can be calculated correctly as codice_9.
Iterative.
The binary search algorithm can also be expressed iteratively with two index limits that progressively narrow the search range.
Deferred detection of equality.
The above iterative and recursive versions take three paths based on the key comparison: one path for less than, one path for greater than, and one path for equality. (There are two conditional branches.) The path for equality is taken only when the record is finally matched, so it is rarely taken. That branch path can be moved outside the search loop in the deferred test for equality version of the algorithm. The following algorithm uses only one conditional branch per iteration.
The deferred detection approach foregoes the possibility of early termination on discovery of a match, so the search will take about log("N") iterations. On average, a "successful" early termination search will not save many iterations. For large arrays that are a power of 2, the savings is about two iterations. Half the time, a match is found with one iteration left to go; one quarter the time with two iterations left, one eighth with three iterations, and so forth. The infinite series sum is 2.
The deferred detection algorithm has the advantage that if the keys are not unique, it returns the smallest index (the starting index) of the region where elements have the search key. The early termination version would return the first match it found, and that match might be anywhere in region of equal keys.
Performance.
With each test that fails to find a match at the probed position, the search is continued with one or other of the two sub-intervals, each at most half the size. More precisely, if the number of items, "N", is odd then both sub-intervals will contain ("N"−1)/2 elements, while if "N" is even then the two sub-intervals contain "N"/2−1 and "N"/2 elements.
If the original number of items is "N" then after the first iteration there will be at most "N"/2 items remaining, then at most "N"/4 items, at most "N"/8 items, and so on. In the worst case, when the value is not in the list, the algorithm must continue iterating until the span has been made empty; this will have taken at most ⌊log("N")+1⌋ iterations, where the ⌊ ⌋ notation denotes the floor function that rounds its argument down to an integer. This worst case analysis is tight: for any "N" there exists a query that takes exactly ⌊log("N")+1⌋ iterations. When compared to linear search, whose worst-case behaviour is "N" iterations, we see that binary search is substantially faster as "N" grows large. For example, to search a list of one million items takes as many as one million iterations with linear search, but never more than twenty iterations with binary search. However, a binary search can only be performed if the list is in sorted order.
Average performance.
 is the expected number of probes in an average successful search, and the worst case is , just one more probe. If the list is empty, no probes at all are made.
Thus binary search is a logarithmic algorithm and executes in time. In most cases it is considerably faster than a linear search. It can be implemented using iteration, or recursion. In some languages it is more elegantly expressed recursively; however, in some C-based languages tail recursion is not eliminated and the recursive version requires more stack space.
Binary search can interact poorly with the memory hierarchy (i.e. caching), because of its random-access nature. For in-memory searching, if the span to be searched is small, a linear search may have superior performance simply because it exhibits better locality of reference. For external searching, care must be taken or each of the first several probes will lead to a disk seek. A common method is to abandon binary searching for linear searching as soon as the size of the remaining span falls below a small value such as 8 or 16 or even more in recent computers. The exact value depends entirely on the machine running the algorithm.
Notice that for multiple searches "with a fixed value for ", then (with the appropriate regard for integer division), the first iteration always selects the middle element at , and the second always selects either or , and so on. Thus if the array's key values are in some sort of slow storage (on a disc file, in virtual memory, not in the CPU's on-chip memory), keeping those three keys in a local array for a special preliminary search will avoid accessing widely separated memory. Escalating to seven or fifteen such values will allow further levels at not much cost in storage. On the other hand, if the searches are frequent and not separated by much other activity, the computer's various storage control features will more or less automatically promote frequently accessed elements into faster storage.
When multiple binary searches are to be performed for the same key in related lists, fractional cascading can be used to speed up successive searches after the first one.
In theory binary search is usually faster than linear search, but in practice that may not hold true. For small arrays (say about 64 items or less), linear search may have better performance. For any size unsorted array, the cost of sorting the array may exceed the speed advantage of binary search when the array is only searched a few times because the time to sort the array is comparable to linear searches For example, if an unsorted array will only be searched once, it will be faster to just do a linear search rather than sorting the array and then doing a binary search.
Variations.
Exclusive or inclusive bounds.
The most significant differences are between the "exclusive" and "inclusive" forms of the bounds. In the "exclusive" bound form the span to be searched is "(L+1)" to "(R−1)", and this may seem clumsy when the span to be searched could be described in the "inclusive" form, as "L" to "R". Although the details differ the two forms are equivalent as can be seen by transforming one version into the other. The inclusive bound form can be attained by replacing all appearances of "L" by "(L−1)" and "R" by "(R+1)" then rearranging. Thus, the initialisation of "L" := 0 becomes ("L"−1) := 0 or "L" := 1, and "R" := "N"+1 becomes ("R"+1) := "N"+1 or "R" := "N". So far so good, but note now that the changes to "L" and "R" are no longer simply transferring the value of "p" to "L" or "R" as appropriate but now must be ("R"+1) := "p" or "R" := "p"−1, and ("L"−1) := "p" or "L" := "p"+1.
Thus, the gain of a simpler initialisation, done once, is lost by a more complex calculation, and which is done for every iteration. If that is not enough, the test for an empty span is more complex also, as compared to the simplicity of checking that the value of "p" is zero. Nevertheless, the inclusive bound form is found in many publications, such as Donald Knuth. "The Art of Computer Programming", Volume 3: "Sorting and Searching", Third Edition.
Another common variation uses inclusive bounds for the left bound, but exclusive bounds for the right bound. This is derived from the fact that the bounds in a language with zero-based arrays can be simply initialized to 0 and the size of the array, respectively. This mirrors the way array slices are represented in some programming languages.
Midpoint and width.
A different variation involves abandoning the "L" and "R" pointers and using a current position "p" and a width "w". At each iteration, the position "p" is adjusted and the width "w" is halved. Knuth states, "It is possible to do this, but only if extreme care is paid to the details."
Search domain.
There is no particular requirement that the array being searched has the bounds 1 to "N". It is possible to search a specified range, elements "first" to "last" instead of 1 to "N". All that is necessary is that the initialization of the bounds be "L := first−1" and "R := last+1", then all proceeds as before.
The elements of the list are not necessarily all unique. If one searches for a value that occurs multiple times in the list, the index returned will be of the first-encountered equal element, and this will not necessarily be that of the first, last, or middle element of the run of equal-key elements but will depend on the positions of the values. Modifying the list even in seemingly unrelated ways such as adding elements elsewhere in the list may change the result.
If the location of the first and/or last equal element needs to be determined, this can be done efficiently with a variant of the binary search algorithms which perform only one inequality test per iteration. See deferred detection of equality.
Noisy search.
Several algorithms closely related to or extending binary search exist. For instance, noisy binary search solves the same class of projects as regular binary search, with the added complexity that any given test can return a false value at random. (Usually, the number of such erroneous results are bounded in some way, either in the form of an average error rate, or in the total number of errors allowed per element in the search space.) Optimal algorithms for several classes of noisy binary search problems have been known since the late seventies, and more recently, optimal algorithms for noisy binary search in quantum computers (where several elements can be tested at the same time) have been discovered.
Exponential search.
An exponential search (also called a one-sided search) searches from a starting point within the array and either expects that the element being sought is nearby or the upper (lower) bound on the array is unknown. Starting with a step size of 1 and doubling with each step, the method looks for a number ≥ (≤) . Once the upper (lower) bound is found, then the method proceeds with a binary search. The complexity of the search is formula_6 if the sought element is in the "n"th array position. This depends only on and not on the size of the array.
Interpolated search.
An interpolated search tries to guess the location of the element formula_7 you're searching for, beginning by calculating a midpoint based on the lowest and highest value and assuming a fairly even distribution of values.
Implementation issues.
Although the basic idea of binary search is comparatively straightforward, the details can be surprisingly tricky… — Donald Knuth
When Jon Bentley assigned it as a problem in a course for professional programmers, he found that an astounding ninety percent failed to code a binary search correctly after several hours of working on it, and another study shows that accurate code for it is only found in five out of twenty textbooks. Furthermore, Bentley's own implementation of binary search, published in his 1986 book "Programming Pearls", contained an overflow error that remained undetected for over twenty years.
Arithmetic.
In a practical implementation, the variables used to represent the indices will often be of finite size, hence only capable of representing a finite range of values. For example, 32-bit unsigned integers can only hold values from 0 to 4294967295. 32-bit signed integers can only hold values from -2147483648 to 2147483647. If the binary search algorithm is to operate on large arrays, this has three implications:
Language support.
Many standard libraries provide a way to do a binary search:

</doc>
<doc id="4267" url="https://en.wikipedia.org/wiki?curid=4267" title="Belle and Sebastian">
Belle and Sebastian

Belle and Sebastian are a Scottish indie pop band formed in Glasgow in January 1996. Led by Stuart Murdoch, the band has released 9 albums to date. Though often praised by critics, Belle and Sebastian has enjoyed only limited commercial success.
After releasing a number of albums and EPs on Jeepster Records, they are now signed to Rough Trade Records in the United Kingdom and Matador Records in the United States.
History.
Formation and early years (1996–1998).
Belle and Sebastian were formed in Glasgow, Scotland in 1996 by Stuart Murdoch and Stuart David. Together, with Stow College music professor Alan Rankine, they recorded some demos; these demos were picked up by the college's Music Business course that produces and releases one single each year on the college's label, Electric Honey. As the band had a number of songs already and the label was extremely impressed with the demos, Belle and Sebastian were allowed to record a full-length album, which was titled "Tigermilk". Murdoch once described the band as a "product of botched capitalism". The band took their name from the television adaptation of the French novel "Belle et Sébastien".
"Tigermilk" was recorded in three days and originally only one thousand copies were pressed on vinyl. These original copies now sell for up to £400. The warm reception the album received inspired Murdoch and David to turn the band into a full-time project, recruiting Stevie Jackson (guitar and vocals), Isobel Campbell (cello/vocals), Chris Geddes (keys) and Richard Colburn (drums) to fill out the group.
After the success of the debut album, Belle and Sebastian were signed to Jeepster Records in August 1996 and "If You're Feeling Sinister", their second album, was released on 18 November. The album was named by "Spin" as one of the 100 greatest albums between 1985 and 2005, and is widely considered the band's masterpiece. Just before the recording of "Sinister", Sarah Martin (violin/vocals) joined the band. Following this a series of EPs were released in 1997. The first of these was "Dog on Wheels", which contained four demo tracks recorded before the real formation of the band. In fact, the only long-term band members to play on the songs were Murdoch, David, and Mick Cooke, who played trumpet on the EP but would not officially join the band until a few years later. It charted at No. 59 in the UK singles chart.
The "Lazy Line Painter Jane" EP followed in July. The track was recorded in the church where Murdoch lived and features vocals from Monica Queen. The EP narrowly missed out on the UK top 40, peaking at No. 41. The last of the 1997 EPs was October's "3.. 6.. 9 Seconds of Light". The EP was made Single of the Week in both the "NME" and "Melody Maker" and reached No. 32 in the charts, thus becoming the band's first top 40 single.
Critical acclaim (1998–2000).
The band released their third LP, "The Boy with the Arab Strap" in 1998, and it reached No. 12 in the UK charts. "Arab Strap" garnered positive reviews from "Rolling Stone" and the "Village Voice," among others; however, the album has its detractors, including "Pitchfork Media", who gave the album a particularly poor review, calling it a "parody" of their earlier work (Pitchfork has since removed the review from their website). During the recording of the album, long-time studio trumpet-player Mick Cooke was asked to join the band as a full member. The "This Is Just a Modern Rock Song" EP followed later that year.
In 1999 the band was awarded with Best Newcomer (for their third album) at the BRIT Awards, upsetting better-known acts such as Steps and 5ive. That same year, the band hosted their own festival, the Bowlie Weekender. "Tigermilk" was also given a full release by Jeepster before the band started work on their next LP. The result was "Fold Your Hands Child, You Walk Like a Peasant", which became the band's first top 10 album in the UK. A stand-alone single, "Legal Man", reached No. 15 and gave them their first appearance on Top of the Pops.
As the band's popularity and recognition was growing worldwide, their music began appearing in films and on television. The 2000 film "High Fidelity" mentions the band and features a clip from the song "Seymour Stein" from "The Boy with the Arab Strap". Also, the title track from "Arab Strap" was played over the end credits of the UK television series "Teachers," and the lyric "Color my life with the chaos of trouble" from the song was quoted by one of the characters in the 2009 film "(500) Days of Summer".
Line-up and label changes (2000–2005).
Stuart David soon left the band to concentrate on his side project, Looper, and his book writing, which included his "The Idle Thoughts of a Daydreamer". He was replaced by Bobby Kildea of V-Twin. The "Jonathan David" single, sung by Stevie Jackson, was released in June 2001 and was followed by "I'm Waking Up to Us" in November. "I'm Waking Up to Us" saw the band use an outside producer (Mike Hurst) for the first time. Most of 2002 was spent touring and recording a soundtrack album, "Storytelling" (for "Storytelling" by Todd Solondz). Campbell left the band in the spring of 2002, in the middle of the band's North American tour.
The band left Jeepster in 2002, signing a four-album deal with Rough Trade Records. Their first album for Rough Trade, "Dear Catastrophe Waitress", was released in 2003, and was produced by Trevor Horn. The album showed a markedly more "produced" sound compared to their first four LPs, as the band was making a concerted effort to produce more "radio-friendly" music. The album was warmly received and is credited with restoring the band's "indie cred". The album also marked the return of Murdoch as the group's primary songwriter, following the poorly received "Fold Your Hands Child, You Walk Like a Peasant" and "Storytelling", both of which were more collaborative than the band's early work. A documentary DVD, "Fans Only", was released by Jeepster in October 2003, featuring promotional videos, live clips and unreleased footage. A single from the album, "Step into My Office, Baby" followed in November 2003; it would be their first single taken from an album.
The Thin Lizzy-inspired "I'm a Cuckoo" was the second single from the album. It achieved their highest chart position yet, reaching No. 14 in the UK. The "Books" EP followed, a double A-side single led by "Wrapped Up in Books" from "Dear Catastrophe Waitress" and the new "Your Cover's Blown". This EP became the band's third top 20 UK release, and the band was nominated for both the Mercury Music Prize and an Ivor Novello Award. In January 2005, B&S was voted Scotland's greatest band in a poll by The List, beating Simple Minds, Idlewild, Travis, Franz Ferdinand, and The Proclaimers, among others.
Return to success (2005–2010).
In April 2005, members of the band visited Israel and the Palestinian territories with the UK charity War on Want; the group subsequently recorded a song inspired by the trip titled "The Eighth Station of the Cross Kebab House", which would first appear on the digital-download version of the charity album and would later have a physical release as a B-side on 2006's "Funny Little Frog" single. "Push Barman to Open Old Wounds", a compilation of the Jeepster singles and EPs, was released in May 2005 while the band were recording their seventh album in California. The result of the sessions was "The Life Pursuit", produced by Tony Hoffer. The album, originally intended to be a double album, became the band's highest-charting album upon its release in February 2006, peaking at No. 8 in the UK and No. 65 on the US "Billboard" 200. "Funny Little Frog", which preceded it, also proved to be their highest-charting single, debuting at No. 13.
On 6 July 2006, the band played a historic show with the Los Angeles Philharmonic at the Hollywood Bowl. The opening act at the 18,000 seat sell-out concert was The Shins. The members of the band see this as a landmark event, with Stevie Jackson saying, "This is the biggest thrill of my entire life". In October 2006, members of the band helped put together a CD collection of new songs for children titled "Colours Are Brighter", with the involvement of major bands such as Franz Ferdinand and The Flaming Lips.
On 18 November 2008 the band released "The BBC Sessions", which features songs from the period of 1996–2001 (including the last recordings featuring Isobel Campbell before she left the band), along with a second disc featuring a recording of a live performance in Belfast from Christmas 2001.
Recent years (2010–present).
On 17 July 2010, the band performed their first UK gig in almost four years to a crowd of around 30,000 at Latitude Festival in Henham Park, Southwold. They performed two new songs, "I Didn't See It Coming" and "I'm Not Living in the Real World".
Their eighth studio album, released in the UK and internationally on 25 September 2010, was titled "Belle and Sebastian Write About Love". The first single from the album, as well as the record's title track "Write About Love", was released in the US on 7 September 2010. "Write About Love" entered the UK albums chart in its first week of release, peaking at No. 8 as of 19 October 2010.
In December 2010 Belle and Sebastian curated the sequel to the "Bowlie Weekender" in the form of "Bowlie 2" presented by All Tomorrow's Parties.
In 2013, Pitchfork TV released an hour-long documentary in February, directed by RJ Bentler which focused on the band's 1996 album If You're Feeling Sinister, as well as the formation and early releases of the band. The documentary featured interviews with every member that was present on the album, as well as several archival photos and videos from the band's early days. The band compiled a second compilation album "The Third Eye Centre" which included the b-sides and rarities released after "Push Barman to Open Old Wounds", from the albums "Dear Catastrophe Waitress", "The Life Pursuit", and "Write About Love". In an interview at the end of 2013, Mick Cooke confirmed he had left the band on good terms.
The band received an 'Outstanding Contribution To Music Award' at the NME Awards 2014.
In 2014, the band returned to the studio, recording in Atlanta, Georgia for their ninth studio album, along with announcing tour dates for various festivals and concerts across the world during 2014. Their ninth album "Girls in Peacetime Want to Dance" was released on 19 January 2015.
Belle and Sebastian performed at the world famous Glastonbury Festival on Sunday 28 June 2015, on 'The Other Stage'.
Compilation albums
Live albums

</doc>
<doc id="4279" url="https://en.wikipedia.org/wiki?curid=4279" title="Broadcast domain">
Broadcast domain

A broadcast domain is a logical division of a computer network, in which all nodes can reach each other by broadcast at the data link layer. A broadcast domain can be within the same LAN segment or it can be bridged to other LAN segments.
In terms of current popular technologies: Any computer connected to the same Ethernet repeater or switch is a member of the same broadcast domain. Further, any computer connected to the same set of inter-connected switches/repeaters is a member of the same broadcast domain. Routers and other higher-layer devices form boundaries between broadcast domains.
This is as compared to a collision domain, which would be all nodes on the same set inter-connected repeaters, divided by switches and learning bridges. Collision domains are generally smaller than, and contained within, broadcast domains.
While some layer two network devices are able to divide the collision domains, broadcast domains are only divided by layer 3 network devices such as routers or layer 3 switches. Separating VLANs divides broadcast domains as well.
Further explanation.
The distinction between broadcast and collision domains comes about because simple Ethernet and similar systems use a shared transmission system. In simple Ethernet (without switches or bridges), data frames are transmitted to all other nodes on a network. Each receiving node checks the destination address of each frame, and simply ignores any frame not addressed to its own MAC.
Switches act as buffers, receiving and analyzing the frames from each connected network segment. Frames destined for nodes connected to the originating segment are not forwarded by the switch. Frames destined for a specific node on a different segment are sent only to that segment. Only broadcast frames are forwarded to all other segments. This reduces unnecessary traffic and collisions.
In such a switched network, transmitted frames may not be received by all other reachable nodes. Nominally, only broadcast frames will be received by all other nodes. Collisions are localized to the network segment they occur on. Thus, the broadcast domain is the entire inter-connected layer two network, and the segments connected to each switch/bridge port are each a collision domain.
Not all network systems or media feature broadcast/collision domains. For example, PPP links.
Broadcast domain control.
With a sufficiently sophisticated switch, it is possible to create a network in which the normal notion of a broadcast domain is strictly controlled. One implementation of this concept is termed a "private VLAN". Another implementation is possible with Linux and iptables. One helpful analogy is that by creating multiple VLANs, the number of broadcast domains increases, but the size of each broadcast domain decreases. This is because a virtual LAN (or VLAN) is technically a broadcast domain.
This is achieved by designating one or more "server" or "provider" nodes, either by MAC address or switch port. Broadcast frames are allowed to originate from these sources, and are sent to all other nodes. Broadcast frames from all other sources are directed only to the server/provider nodes. Traffic from other sources not destined to the server/provider nodes ("peer-to-peer" traffic) is blocked.
The result is a network based on a nominally shared transmission system; like Ethernet, but in which "client" nodes cannot communicate with each other, only with the server/provider. A common application is Internet providers. Allowing direct data link layer communication between customer nodes exposes the network to various security attacks, such as ARP spoofing. Controlling the broadcast domain in this fashion provides many of the advantages of a point-to-point network, using commodity broadcast-based hardware.

</doc>
<doc id="4282" url="https://en.wikipedia.org/wiki?curid=4282" title="Beechcraft">
Beechcraft

Beechcraft Corporation is an American manufacturer of general aviation and military aircraft, ranging from light single-engined aircraft to twin-engined turboprop transports, and military trainers. A brand of Textron Aviation since 2014, it has also been a division of Raytheon and later a brand of Hawker Beechcraft.
History.
Beech Aircraft Company was founded in Wichita, Kansas, in 1932 by Walter Beech and his wife Olive Ann Beech. The company began operations in an idle Cessna factory. With designer Ted Wells, they developed the first aircraft under the Beechcraft name, the classic Model 17 Staggerwing, which first flew in November 1932. Over 750 Staggerwings were built, with 270 manufactured for the United States Army Air Forces during World War II.
Beechcraft was not Beech's first company, as he had previously formed Travel Air in 1924 and the design numbers used at Beechcraft followed the sequence started at Travel Air, and were then continued at Curtiss-Wright, after Travel Air had been absorbed into the much larger company in 1929. Beech became President of the Curtiss-Wright's airplane division and VP of sales, but became dissatisfied with being so far removed from aircraft production and quit to form Beechcraft, using the original Travel Air facilities and employing many of the same people. Model numbers prior to 11/11000 were built under the Travel Air name, while Curtiss-Wright built the CW-12, 14, 15 and 16 as well as previous successful Travel Air models (mostly the model 4).
In 1942 Beech won its first Army-Navy "E" Award production award and became one of the elite five percent of war contracting firms in the country to win five straight awards for production efficiency, mostly for the production of the Beechcraft Model 18 which remains in widespread use worldwide. Beechcraft ranked 69th among United States corporations in the value of World War II military production contracts.
After the war, the Staggerwing was replaced by the revolutionary Beechcraft Bonanza with a distinctive V-tail. Perhaps the best known Beech aircraft, the single-engined Bonanza has been manufactured in various models since 1947. The Bonanza has had the longest production run of any airplane, past or present, in the world. Other important Beech aircraft are the King Air/Super King Air line of twin-engined turboprops, in production since 1964, the Baron, a twin-engined variant of the Bonanza, and the Beechcraft Model 18, originally a business transport and commuter airliner from the late 1930s through the 1960s, which remains in active service as a cargo transport.
In 1950, Olive Ann Beech was installed as president and CEO of the company, after the sudden death of her husband from a heart attack on 29 November of that year. She continued as CEO until Beech was purchased by Raytheon Company on 8 February 1980. Ted Wells had been replaced as Chief Engineer by Herbert Rawdon, who remained at the post until his retirement in the early 1960s.
In 1994, Raytheon merged Beechcraft with the Hawker product line it had acquired in 1993 from British Aerospace, forming Raytheon Aircraft Company. In 2002, the Beechcraft brand was revived to again designate the Wichita-produced aircraft. In 2006, Raytheon sold Raytheon Aircraft to Goldman Sachs creating Hawker Beechcraft. Since its inception Beechcraft has resided in Wichita, Kansas, also the home of chief competitor Cessna, the birthplace of Learjet and of Stearman, whose trainers were used in large numbers during WW2.
The entry into bankruptcy of Hawker Beechcraft on May 3, 2012 ended with its emergence on February 16, 2013 as a new entity, Beechcraft Corporation, with the Hawker Beechcraft name being retired. The new and much smaller company will produce the King Air line of aircraft as well as the T-6 and AT-6 military trainer/attack aircraft, the piston-powered single-engined Bonanza and twin-engined Baron aircraft. The jet line was discontinued, but the new company would continue to support the aircraft already produced with parts, plus engineering and airworthiness documentation.
By October 2013, the company, now financially turned around, was up for sale.
On December 26, 2013, Textron agreed to purchase Beechcraft, including the discontinued Hawker jet line, for $1.4 billion. The sale was expected to be concluded in the first half of 2014, pending government approval. Textron CEO Scott Donnelly indicated that Beechcraft and Cessna would be combined to form a new light aircraft manufacturing concern that will result in US$65M-$85M in annual savings over keeping the companies separate. Textron's initial plan is to keep both Beechcraft and Cessna as separate brands.
Facilities.
Beech Factory Airport house Beechcraft's head office, manufacturing facility and runway for test flights.

</doc>
<doc id="4283" url="https://en.wikipedia.org/wiki?curid=4283" title="Battle of Peleliu">
Battle of Peleliu

The Battle of Peleliu, codenamed Operation Stalemate II by the United States, was fought between the United States and the Empire of Japan in the Pacific Theater of World War II, from September to November 1944 on the island of Peleliu (in present-day Palau). U.S. Marines of the First Marine Division, and later soldiers of the U.S. Army's 81st Infantry Division, fought to capture an airstrip on the small coral island. This battle was part of a larger offensive campaign known as Operation Forager, which ran from June to November 1944 in the Pacific Theater of Operations.
Major General William Rupertus, (USMC commander of 1st Marine Division) predicted the island would be secured within four days. However, because of Japan's well-crafted fortifications and stiff resistance, the battle lasted more than two months. In the United States, this was a controversial battle because of the island's questionable strategic value and the high casualty rate, which exceeded that of all other amphibious operations during the Pacific War. The National Museum of the Marine Corps called it "the bitterest battle of the war for the Marines".
Background.
By 1944, American victories in the Southwest and Central Pacific had brought the war closer to Japan, with American bombers able to strike at the Japanese main islands from air bases secured during the Mariana Islands campaign (June—August 1944). There was disagreement among the U.S. Joint Chiefs over two proposed strategies to defeat the Japanese Empire. The strategy proposed by General Douglas MacArthur called for the recapture of the Philippines, followed by the capture of Okinawa, then an attack on the Japanese mainland. Admiral Chester Nimitz favored a more direct strategy of bypassing the Philippines, but seizing Okinawa and Taiwan as staging areas to an attack on the Japanese mainland, followed by the future invasion of Japan's southernmost islands. Both strategies included the invasion of Peleliu, but for different reasons.
The 1st Marine Division had already been chosen to make the assault. President Franklin D. Roosevelt traveled to Pearl Harbor to personally meet both commanders and hear their arguments. MacArthur's strategy was chosen. However, before MacArthur could retake the Philippines, the Palau Islands, specifically Peleliu and Angaur, were to be neutralized and an airfield built to protect MacArthur's right flank.
Preparations.
Japanese.
By 1944, Peleliu Island was occupied by about 11,000 Japanese of the 14th Infantry Division with Korean and Okinawan laborers. Colonel Kunio Nakagawa, commander of the division's 2nd Regiment, led the preparations for the island's defense.
After their losses in the Solomons, Gilberts, Marshalls and Marianas, the Imperial Army assembled a research team to develop new island-defense tactics. They chose to abandon the old strategy of stopping the enemy at the beach. The new tactics would only disrupt the landings at the water's edge and depend on an in-depth defense farther inland. Colonel Nakagawa used the rough terrain to his advantage, by constructing a system of heavily fortified bunkers, caves and underground positions all interlocked into a "honeycomb" system. The old "banzai charge" attack was also discontinued as being both wasteful of men and ineffective. These changes would force the Americans into a war of attrition requiring increasingly more resources.
Nakagawa's defenses were based at Peleliu's highest point, Umurbrogol Mountain, a collection of hills and steep ridges located at the center of Peleliu overlooking a large portion of the island, including the crucial airfield. The Umurbrogol contained some 500 limestone caves, interconnected by tunnels. Many of these were former mine shafts that were turned into defense positions. Engineers added sliding armored steel doors with multiple openings to serve both artillery and machine guns. Cave entrances were built slanted as a defense against grenade and flamethrower attacks. The caves and bunkers were connected to a vast system throughout central Peleliu, which allowed the Japanese to evacuate or reoccupy positions as needed, and to take advantage of shrinking interior lines.
The Japanese were well armed with , mortars and anti-aircraft cannons, and backed by a light tank unit and an anti-aircraft detachment.
The Japanese also used the beach terrain to their advantage. The northern end of the landing beaches faced a coral promontory that overlooked the beaches from a small peninsula, a spot later known to the Marines who assaulted it simply as "The Point". Holes were blasted into the ridge to accommodate a gun, and six 20-mm cannons. The positions were then sealed shut, leaving just a small firing slit to assault the beaches. Similar positions were crafted along the stretch of landing beaches.
The beaches were also filled with thousands of obstacles for the landing craft, principally mines and a large number of heavy artillery shells buried with the fuses exposed to explode when they were run over. A battalion was placed along the beach to defend against the landing, but they were meant to merely delay the inevitable American advance inland.
American.
Unlike the Japanese, who drastically altered their tactics for the upcoming battle, the American invasion plan was unchanged from that of previous amphibious landings, even after suffering 3,000 casualties and two months of delaying tactics against the entrenched Japanese defenders at the Battle of Biak. On Peleliu, American planners chose to land on the southwest beaches because of their proximity to the airfield on South Peleliu. The 1st Marine Regiment, commanded by Colonel Lewis B. Puller, was to land on the northern end of the beaches. The 5th Marine Regiment, under Colonel Harold D. Harris, would land in the center, and the 7th Marine Regiment, under Col. Herman H. Hanneken, would land at the southern end.
The division's artillery regiment, the 11th Marines, would land after the infantry regiments. The plan was for the 1st and 7th Regiments to push inland, guarding the 5th Regiment's left and right flank, and allowing them to capture the airfield located directly to the center of the landing beaches. The 5th Marines were to push to the eastern shore, cutting the island in half. The 1st Marines would push north into the Umurbrogol, while the 7th Marines would clear the southern end of the island. Only one battalion was left behind in reserve, with the Army's 81st Infantry Division available for support from Angaur, just south of Peleliu.
On September 4, the Marines shipped off from their station on Pavuvu, just north of Guadalcanal, a trip across the Pacific to Peleliu. The Navy's Underwater Demolition Team went in first to clear the beaches of obstacles, while U.S. Navy warships began their pre-invasion bombardment of Peleliu on September 12.
The battleships , , , and , heavy cruisers , , , and , and light cruisers , and , led by the command ship , subjected the tiny island, only in size, to a massive three-day bombardment, pausing only to permit air strikes from the three aircraft carriers, five light aircraft carriers, and eleven escort carriers with the attack force. A total of 519 rounds of shells, 1,845 rounds of shells and 1,793 bombs were dropped on the islands during this period.
The Americans believed the bombardment to be successful, as Rear Admiral Jesse Oldendorf claimed that the Navy had run out of targets. In reality, the majority of the Japanese positions were completely unharmed. Even the battalion left to defend the beaches was virtually unscathed. During the assault, the island's defenders exercised unusual firing discipline to avoid giving away their positions. The bombardment managed only to destroy Japan's aircraft on the island, as well as the buildings surrounding the airfield. The Japanese remained in their fortified positions, ready to attack the troops soon to be landing.
Battle.
Landing.
The Marines landed at 08:32 on September 15, the 1st Marines to the north on White Beach 1 and 2 and the 5th and 7th Marines to the center and south on Orange Beach 1, 2 and 3. As the other landing craft approached the beaches, they were caught in a crossfire when the Japanese opened the steel doors guarding their positions and fired artillery. The positions on the coral promontories guarding each flank attacked the Marines with 47-mm guns and 20-mm cannons. By 09:30, the Japanese had destroyed 60 LVTs and DUKWs.
The 1st Marines were quickly bogged down by heavy fire from the extreme left flank and a 30-foot-high coral ridge, "The Point". Colonel Chesty Puller narrowly escaped death when a dud high velocity artillery round struck his LVT. His communications section was destroyed on its way to the beach by a hit from a 47-mm round. The 7th Marines faced a cluttered Orange Beach 3, with natural and man-made obstacles, forcing the Amtracs to approach in column.
The 5th Marines made the most progress on the first day, aided by cover provided by coconut groves. They pushed toward the airfield, but were met with Nakagawa's first counterattack. His armored tank company raced across the airfield to push the Marines back, but was soon engaged by tanks, howitzers, naval guns and dive bombers. Nakagawa's tanks and escorting infantrymen were quickly destroyed.
At the end of the first day, the Americans held their stretch of landing beaches, but little else. Their biggest push in the south moved inland, but the 1st Marines to the north made very little progress because of the extremely thick resistance. The Marines had suffered 200 dead and 900 wounded. Rupertus, still unaware of his enemy's change of tactics, believed the Japanese would quickly crumble since their perimeter had been broken.
The airfield/South Peleliu.
On the second day, the 5th Marines moved to capture the airfield and push toward the eastern shore. They ran across the airfield, enduring heavy artillery fire from the highlands to the north, suffering heavy casualties in the process. After capturing the airfield, they rapidly advanced to the eastern end of Peleliu, leaving the island's southern defenders to be destroyed by the 7th Marines.
This area was hotly contested by the Japanese, who still occupied numerous pillboxes. Heat indices were around , and the Marines soon suffered high casualties from heat exhaustion. Further complicating the situation, the Marines' water was distributed in empty oil drums, contaminating the water with the oil residue. Still, by the eighth day the 5th and 7th Marines had accomplished their objectives, holding the airfield and the southern portion of the island, although the airfield remained under threat of sustained Japanese fire from the heights of Umurbrogol Mountain until the end of the battle.
American forces put the airfield to use on the third day. L-2 Grasshoppers from VMO-1 began aerial spotting missions for Marine artillery and naval gunfire support. On September 26 (D+11), Marine F4U Corsairs from VMF-114 landed on the airstrip. The Corsairs began dive-bombing missions across Peleliu, firing rockets into open cave entrances for the infantrymen, and dropping napalm; it was only the second time the latter weapon had been used in the Pacific. Napalm proved useful, burning away the vegetation hiding spider holes and usually killing their occupants.
The time from liftoff to the target area for the Corsairs based on Peleliu Airfield was very short, sometimes only 10 to 15 seconds. Consequently, there was almost no time for pilots to raise their aircraft undercarriage; most pilots did not bother and left them down during the strike. After the strike was completed and the payload dropped, the Corsair simply turned back into the landing pattern again.
The Point.
The fortress at the end of the southern landing beaches (a.k.a. “The Point”) continued to cause heavy casualties due to enfilading fire from heavy machine guns and anti-tank artillery across the landing beaches. Puller ordered Captain George P. Hunt, commander of K Company, 3rd Battalion, 1st Marines, to capture the position. He approached The Point short on supplies, having lost most of his machine guns while approaching the beaches. Hunt's second platoon was pinned down for nearly a day in an anti-tank trench between fortifications. The rest of his company was endangered when the Japanese cut a hole in their line, surrounding his company and leaving his right flank cut off.
However, a rifle platoon began knocking out the Japanese gun positions one by one. Using smoke grenades for cover, they swept through each hole, destroying the positions with rifle grenades and close-quarters combat. After knocking out the six machine gun positions, the Marines faced the 47 mm gun cave. A lieutenant blinded the 47 mm gunner with a smoke grenade, allowing Corporal Henry W. Hahn to launch a grenade through the cave's aperture. The grenade detonated the 47 mm's shells, forcing the cave's occupants out with their bodies lit aflame as well as their ammunition belts exploding around their waists. A fire team was positioned on the flank of the cave where the former occupants were shot down.
K Company had captured The Point, but Nakagawa counterattacked. The next 30 hours saw four major counterattacks against a sole company, critically low on supplies, out of water, and surrounded. The Marines soon had to resort to hand-to-hand combat and attrition warfare to fend off the Japanese attackers. By the time reinforcements arrived, the company had successfully repulsed all Japanese attacks, but had been reduced to 18 men, suffering 157 casualties during the battle for The Point. Hunt and Hahn were both awarded the Navy Cross for their actions.
Ngesebus Island.
The 5th Marines—after having secured the airfield—were sent to capture Ngesebus Island, just north of Peleliu. Ngesebus was occupied by many Japanese artillery positions, and was the site of an airfield still under construction. The tiny island was connected to Peleliu by a small causeway, but 5th Marines commander Harris opted instead to make a shore-to-shore amphibious landing, predicting the causeway to be an obvious target for the island's defenders.
Harris coordinated a pre-landing bombardment of the island on September 28, carried out by Army guns, naval guns, howitzers from the 11th Marines, strafing runs from VMF-114's Corsairs, and fire from the approaching LVTs. Unlike the Navy's bombardment of Peleliu, Harris' assault on Ngesebus successfully killed most of the Japanese defenders. The Marines still faced opposition in the ridges and caves, but the island fell quickly, with relatively light casualties for the 5th Marines. They had suffered 15 killed and 33 wounded, and inflicted 470 casualties on the Japanese.
Bloody Nose Ridge.
After capturing The Point, the 1st Marines moved north into the Umurbrogol pocket, named "Bloody Nose Ridge" by the Marines. Puller led his men in numerous assaults, but every one brought on severe casualties by the Japanese. The 1st Marines were trapped within the narrow paths between the ridges, with each ridge fortification supporting the other with deadly crossfire.
The Marines took increasingly high casualties as they slowly advanced through the ridges. The Japanese again showed unusual fire discipline, striking only when they could inflict maximum casualties. As casualties mounted, Japanese snipers began to take aim at stretcher bearers, knowing that if two stretcher bearers were injured or killed, more would have to return to replace them, and the snipers could steadily pick off more and more Marines. The Japanese infiltrated the American lines at night to attack the Marines in their foxholes. The Marines built two-man foxholes, so one could sleep while the other kept watch for infiltrators.
One particularly bloody battle on Bloody Nose came when the 1st Battalion, 1st Marines—under the command of Major Raymond Davis—attacked Hill 100. Over six days of fighting, the battalion suffered 71% casualties. Captain Everett Pope and his company penetrated deep into the ridges, leading his remaining 90 men to seize what he thought was Hill 100. It took a day's fighting to reach what he thought was the crest of the hill, which was in fact another ridge, occupied by more Japanese defenders.
Trapped at the base of the ridge, Pope set up a small defense perimeter, which was attacked relentlessly by the Japanese throughout the night. The Marines soon ran out of ammunition, and had to fight the attackers with knives and fists, even resorting to throwing coral rock and empty ammunition boxes at the Japanese. Pope and his men managed to hold out until dawn came, which brought on more deadly fire. When they evacuated the position, only nine men remained. Pope later received the Medal of Honor for the action. (Picture of the Peleliu Memorial dedicated on the 50th anniversary of the landing on Peleliu with Captain Pope's name)
The Japanese eventually inflicted 70% casualties on Puller's 1st Marines, or 1,749 men. After six days of fighting in the ridges of Umurbrogol, General Roy Geiger, commander of the III Amphibious Corps, sent elements of 81st Infantry Division to Peleliu to relieve the regiment. The 321st Regiment Combat Team landed on the western beaches of Peleliu—at the northern end of Umurbrogol mountain—on 23 September. The 321st and the 7th Marines encircled The Pocket by 24 Sept., D+9.
By October, the 7th Marines had suffered 46% casualties and General Geiger relieved them with the 5th Marines. Col. Harris adopted siege tactics, using bulldozers and flame-thrower tanks, pushing from the north. On 30 Oct., the 81st Inf. Div. took over command of Peleliu, taking another six weeks, with the same tactics, to reduce The Pocket.
On 24 November, Nakagawa proclaimed "Our sword is broken and we have run out of spears". He then burnt his regimental colors and performed ritual suicide. He was posthumously promoted to lieutenant general for his valor displayed on Peleliu. On 27 November, the island was declared secure, ending the 73-day-long battle.
A Japanese lieutenant with his 26 2nd Infantry soldiers and eight 45th Guard Force sailors held out in the caves in Peleliu until April 22, 1947 and surrendered after a Japanese admiral convinced them the war was over.
Aftermath.
The reduction of the Japanese pocket around Umurbrogol mountain has been called the most difficult fight that the U.S. military encountered in the entire war. The 1st Marine Division was severely mauled and it remained out of action until the invasion of Okinawa on 1 April 1945. In total, the 1st Marine Division suffered over 6,500 casualties during their month on Peleliu, over of their entire division. The 81st Infantry Division suffered nearly 3,300 casualties during their tenure on the island.
Postwar statisticians calculated that it took US forces over 1500 rounds of ammunition to kill each Japanese defender, and that during the course of the battle, the Americans expended 13.32 million rounds of 30-calibre, 1.52 million rounds of 45-calibre, 693,657 rounds of 50-calibre bullets, 118,262 hand grenades and approximately 150,000 mortar rounds.
The battle was controversial in the United States due to the island's lack of strategic value and the high casualty rate. The defenders lacked the means to interfere with potential US operations in the Philippines, and the airfield captured on Peleliu never played a key role in subsequent operations. The high casualty rate exceeded all other amphibious operations during the Pacific War.
Instead, the Ulithi Atoll in the Caroline Islands was used as a staging base for the invasion of Okinawa. In addition, few news reports were published about the battle because Rupertus' prediction of a "three days" victory motivated only six reporters to report from shore. The battle was also overshadowed by MacArthur's return to the Philippines and the Allies' push towards Germany in Europe.
The battles for Angaur and Peleliu showed Americans the pattern of future Japanese island defense which would be seen again at Iwo Jima and Okinawa. Naval bombardment prior to amphibious assault at Iwo Jima was only slightly more effective than at Peleliu, but at Okinawa the preliminary shelling was much improved. Frogmen performing underwater demolition at Iwo Jima confused the enemy by sweeping both coasts, but later alerted Japanese defenders to the exact assault beaches at Okinawa. American ground forces at Peleliu gained experience in assaulting heavily fortified positions such as they would find again at Okinawa.
On the recommendation of Admiral William F. Halsey, Jr., the planned occupation of Yap Island in the Caroline Islands was canceled. Halsey actually recommended that the landings on Peleliu and Angaur be canceled, too, and their Marines and soldiers be thrown into Leyte Island instead, but was overruled by Nimitz.
In popular culture.
The Battle of Peleliu is featured in many World War II themed video games including . The player takes the role of a US Marine forced to take Peleliu Airfield, repel counter-attacks, destroy machine-gun and mortar positions and eventually secure Japanese artillery emplacements. In flight-simulation game War Thunder, two teams of players clash to hold the southern and northern airfields. In multi-player shooter Red Orchestra 2: Rising Storm, a team of American troops attack the defensive Japanese team's control points.
The battle including footage and stills are featured in the fifth episode of Ken Burns' The War.
The battle features in Episodes 5, 6 and 7 of the TV mini-series "The Pacific".
Honors and recognitions.
Nakagawa was posthumously promoted to Lieutenant General for his heroism in defending the island.
The United States awarded the Medal of Honor to eight Marines, five posthumously (indicated by *):
The Peleliu Battlefield was listed on the U.S. National Register of Historic Places in 1985.

</doc>
<doc id="4284" url="https://en.wikipedia.org/wiki?curid=4284" title="Battle of Stalingrad">
Battle of Stalingrad

The Battle of Stalingrad (23 August 1942 – 2 February 1943) was a major battle on the Eastern Front of World War II in which Nazi Germany and its allies fought the Soviet Union for control of the city of Stalingrad (now Volgograd) in Southern Russia, on the eastern boundary of Europe.
Marked by constant close quarters combat and direct assaults on civilians by air raids, it is often regarded as one of the single largest (nearly 2.2 million personnel) and bloodiest (1.7–2 million wounded, killed or captured) battles in the history of warfare. The heavy losses inflicted on the German "Wehrmacht" make it arguably the most strategically decisive battle of the whole war. It was a turning point in the European theatre of World War II; German forces never regained the initiative in the East and withdrew a vast military force from the West to replace their losses.
The German offensive to capture Stalingrad began in late summer 1942, using the German 6th Army and elements of the 4th Panzer Army. The attack was supported by intensive "Luftwaffe" bombing that reduced much of the city to rubble. The fighting degenerated into house-to-house fighting, and both sides poured reinforcements into the city. By mid-November 1942, the Germans had pushed the Soviet defenders back at great cost into narrow zones generally along the west bank of the Volga River.
On 19 November 1942, the Red Army launched Operation Uranus, a two-pronged attack targeting the weaker Romanian and Hungarian forces protecting the German 6th Army's flanks. The Axis forces on the flanks were overrun and the 6th Army was cut off and surrounded in the Stalingrad area. Adolf Hitler ordered that the army stay in Stalingrad and make no attempt to break out; instead, attempts were made to supply the army by air and to break the encirclement from the outside. Heavy fighting continued for another two months. By the beginning of February 1943, the Axis forces in Stalingrad had exhausted their ammunition and food. The remaining elements of the 6th Army surrendered. The battle lasted five months, one week, and three days.
Historical background.
By the spring of 1942, despite the failure of Operation Barbarossa to decisively defeat the Soviet Union in a single campaign, the Germans had captured vast expanses of territory, including Ukraine, Belarus, and the Baltic republics. Elsewhere, the war had been progressing well: the U-Boat offensive in the Atlantic had been very successful and Rommel had just captured Tobruk. In the east, they had stabilized their front in a line running from Leningrad in the north to Rostov in the south. There were a number of salients, but these were not particularly threatening. Hitler was confident that he could master the Red Army after the winter of 1942, because even though Army Group Centre ("Heeresgruppe Mitte") had suffered heavy losses west of Moscow the previous winter, 65% of Army Group Centre's infantry had not been engaged and had been rested and re-equipped. Neither Army Group North nor Army Group South had been particularly hard pressed over the winter. Stalin was expecting the main thrust of the German summer attacks to be directed against Moscow again.
With the initial operations being very successful, the Germans decided that their summer campaign in 1942 would be directed at the southern parts of the Soviet Union. The initial objectives in the region around Stalingrad were the destruction of the industrial capacity of the city and the deployment of forces to block the Volga River. The river was a key route from the Caucasus and the Caspian Sea to central Russia. Its capture would disrupt commercial river traffic. The Germans cut the pipeline from the oilfields when they captured Rostov on 23 July. The capture of Stalingrad would make the delivery of Lend Lease supplies via the Persian Corridor much more difficult. Hitler proclaimed that after Stalingrad had been captured, all male civilians were to be killed and all women and children were to be deported because Stalingrad was dangerous with its communist inhabitants.
On 23 July 1942, Hitler personally rewrote the operational objectives for the 1942 campaign, greatly expanding them to include the occupation of the city of Stalingrad. Both sides began to attach propaganda value to the city based on it bearing the name of the leader of the Soviet Union. It was assumed that the fall of the city would also firmly secure the northern and western flanks of the German armies as they advanced on Baku, with the aim of securing these strategic petroleum resources for Germany. The expansion of objectives was a significant factor in Germany's failure at Stalingrad, caused by German overconfidence and an underestimation of Soviet reserves.
The Soviets realized that they were under tremendous constraints of time and resources and ordered that anyone strong enough to hold a rifle be sent to fight.
Prelude.
Army Group South was selected for a sprint forward through the southern Russian steppes into the Caucasus to capture the vital Soviet oil fields there. The planned summer offensive was code-named "Fall Blau" (Case Blue). It was to include the German 6th, 17th, 4th Panzer and 1st Panzer Armies. Army Group South had overrun the Ukrainian Soviet Socialist Republic in 1941. Poised in Eastern Ukraine, it was to spearhead the offensive.
Hitler intervened, however, ordering the Army Group to split in two. Army Group South (A), under the command of Wilhelm List, was to continue advancing south towards the Caucasus as planned with the 17th Army and First Panzer Army. Army Group South (B), including Friedrich Paulus's 6th Army and Hermann Hoth's 4th Panzer Army, was to move east towards the Volga and Stalingrad. Army Group B was commanded initially by Field Marshal Fedor von Bock and later by General Maximilian von Weichs.
The start of "Case Blue" had been planned for late May 1942. A number of German and Romanian units that were to take part in "Blau", however, were besieging Sevastopol on the Crimean Peninsula. Delays in ending the siege pushed back the start date for "Blau" several times, and the city did not fall until the end of June. A smaller action was taken in the meantime, pinching off a Soviet salient in the Second Battle of Kharkov, which resulted in the envelopment of a large Soviet force on 22 May.
"Blau" finally opened as Army Group South began its attack into southern Russia on 28 June 1942. The German offensive started well. Soviet forces offered little resistance in the vast empty steppes and started streaming eastward. Several attempts to re-establish a defensive line failed when German units outflanked them. Two major pockets were formed and destroyed: the first, northeast of Kharkov, on 2 July, and a second, around Millerovo, Rostov Oblast, a week later. Meanwhile, the Hungarian 2nd Army and the German 4th Panzer Army had launched an assault on Voronezh, capturing the city on 5 July.
The initial advance of the 6th Army was so successful that Hitler intervened and ordered the 4th Panzer Army to join Army Group South (A) to the south. A massive traffic jam resulted when the 4th Panzer and the 1st Panzer both required the few roads in the area. Both armies were stopped dead while they attempted to clear the resulting mess of thousands of vehicles. The delay was long, and it is thought that it cost the advance at least one week. With the advance now slowed, Hitler changed his mind and reassigned the 4th Panzer Army back to the attack on Stalingrad.
By the end of July, the Germans had pushed the Soviets across the Don River. At this point, the Don and Volga Rivers were only apart, and the Germans left their main supply depots west of the Don, which had important implications later in the course of the battle. The Germans began using the armies of their Italian, Hungarian and Romanian allies to guard their left (northern) flank. The Italians won several accolades in official German communiques. Sometimes they were held in little regard by the Germans, and were even accused of having low morale: in reality, the Italian divisions fought comparatively well, with the 3rd Mountain Infantry Division Ravenna and 5th Infantry Division Cosseria proving to have good morale, according to a German liaison officer and being forced to retreat only after a massive armoured attack in which German reinforcements had failed to arrive in time, according to a German historian. Indeed the Italians distinguished themselves in numerous battles, as in the battle of Nikolayevka.
The German 6th Army was only a few dozen kilometers from Stalingrad, and 4th Panzer Army, now to their south, turned northwards to help take the city. To the south, Army Group A was pushing far into the Caucasus, but their advance slowed as supply lines grew overextended. The two German army groups were not positioned to support one another due to the great distances involved.
After German intentions became clear in July 1942, Stalin appointed Marshal Andrey Yeryomenko as commander of the Southeastern Front on 1 August 1942. Yeryomenko and Commissar Nikita Khrushchev were tasked with planning the defense of Stalingrad. The eastern border of Stalingrad was the wide River Volga, and over the river, additional Soviet units were deployed. These units became the newly formed 62nd Army, which Yeryomenko placed under the command of Lieutenant General Vasiliy Chuikov on 11 September 1942. The situation was extremely dire. When asked how he interpreted his task, he responded "We will defend the city or die in the attempt." The 62nd Army's mission was to defend Stalingrad at all costs. Chuikov's generalship during the battle earned him one of his two Hero of the Soviet Union awards.
Attack on Stalingrad.
David Glantz indicated that 4 hard-fought battles – collectively known as the Kotluban Operations – north of Stalingrad, that the Soviets made their greatest stand, decided Germany's fate before the Nazis ever set foot in the city itself, and were a turning point in the war. Beginning in late August, continuing in September and into October, Soviets committed between 2 and 4 armies in hastily coordinated and poorly controlled attacks against the German's northern flank that resulted in more than 200,000 Red Army casualties, but that slowed the German assault.
On 23 August the 6th Army reached the outskirts of Stalingrad in pursuit of the 62nd and 64th Armies, which had fallen back into the city. Kleist later said after the war:
The Soviets had enough warning of the Germans' advance to ship grain, cattle, and railway cars across the Volga and out of harm's way but most civilian residents were not evacuated. This "harvest victory" left the city short of food even before the German attack began. Before the "Heer" reached the city itself, the "Luftwaffe" had rendered the River Volga, vital for bringing supplies into the city, unusable to Soviet shipping. Between 25 and 31 July, 32 Soviet ships were sunk, with another nine crippled.
The battle began with the heavy bombing of the city by "Generaloberst" Wolfram von Richthofen's "Luftflotte 4", which in the summer and autumn of 1942 was the most powerful single air formation in the world. Some 1,000 tons of bombs were dropped in 48 hours, more than in London at the height of the Blitz. Much of the city was quickly turned to rubble, although some factories continued production while workers joined in the fighting. The 369th (Croatian) Reinforced Infantry Regiment was the only non-German unit selected by the "Wehrmacht" to enter Stalingrad city during assault operations. It fought as part of the 100th Jäger Division.
Stalin rushed all available troops to the east bank of the Volga, some from as far away as Siberia. All the regular ferries were quickly destroyed by the Luftwaffe, which then targeted troop barges being towed slowly across the river by tugs. Many civilians were evacuated across the Volga. It has been said that Stalin prevented civilians from leaving the city in the belief that their presence would encourage greater resistance from the city's defenders. Civilians, including women and children, were put to work building trenchworks and protective fortifications. A massive German strategic bombing on 23 August caused a firestorm, killing thousands and turning Stalingrad into a vast landscape of rubble and burnt ruins. Ninety percent of the living space in the Voroshilovskiy area was destroyed. Between 23 and 26 August, Soviet reports indicate 955 people were killed and another 1,181 wounded as a result of the bombing. Casualties of 40,000 were greatly exaggerated, and after 25 August, the Soviets did not record any civilian and military casualties as a result of air raids.
The Soviet Air Force, the "Voyenno-Vozdushnye Sily" (VVS), was swept aside by the Luftwaffe. The VVS bases in the immediate area lost 201 aircraft between 23 and 31 August, and despite meager reinforcements of some 100 aircraft in August, it was left with just 192 serviceable aircraft, 57 of which were fighters. The Soviets continued to pour aerial reinforcements into the Stalingrad area in late September, but continued to suffer appalling losses; the "Luftwaffe" had complete control of the skies.
The burden of the initial defense of the city fell on the 1077th Anti-Aircraft Regiment, a unit made up mainly of young female volunteers who had no training for engaging ground targets. Despite this, and with no support available from other units, the AA gunners stayed at their posts and took on the advancing panzers. The German 16th Panzer Division reportedly had to fight the 1077th's gunners "shot for shot" until all 37 anti-aircraft guns were destroyed or overrun. The German 16th Panzer Division was shocked to find that, due to Soviet manpower shortages, it had been fighting female soldiers. In the early stages of the battle, the NKVD organized poorly armed "Workers' militias" composed of civilians not directly involved in war production for immediate use in the battle. The civilians were often sent into battle without rifles. Staff and students from the local technical university formed a "tank destroyer" unit. They assembled tanks from leftover parts at the tractor factory. These tanks, unpainted and lacking gunsights, were driven directly from the factory floor to the front line. They could only be aimed at point blank range through the gun barrel.
By the end of August, Army Group South (B) had finally reached the Volga, north of Stalingrad. Another advance to the river south of the city followed. By 1 September, the Soviets could only reinforce and supply their forces in Stalingrad by perilous crossings of the Volga under constant bombardment by artillery and aircraft.
On 5 September, the Soviet 24th and 66th Armies organized a massive attack against XIV Panzer Corps. The "Luftwaffe" helped repulse the offensive by heavily attacking Soviet artillery positions and defensive lines. The Soviets were forced to withdraw at midday after only a few hours. Of the 120 tanks the Soviets had committed, 30 were lost to air attack.
Soviet operations were constantly hampered by the "Luftwaffe". On 18 September, the Soviet 1st Guards and 24th Army launched an offensive against VIII Army Corps at Kotluban. "VIII. Fliegerkorps" dispatched wave after wave of Stuka dive-bombers to prevent a breakthrough. The offensive was repulsed. The Stukas claimed 41 of the 106 Soviet tanks knocked out that morning, while escorting Bf 109s destroyed 77 Soviet aircraft.
Amid the debris of the wrecked city, the Soviet 62nd and 64th Armies, which included the Soviet 13th Guards Rifle Division, anchored their defense lines with strongpoints in houses and factories.
Fighting within the ruined city was fierce and desperate. Lieutenant General Alexander Rodimtsev was in charge of the 13th Guards Rifle Division, and received one of two Heroes of the Soviet Union awarded during the battle for his actions. Stalin's Order No. 227 of 27 July 1942 decreed that all commanders who ordered unauthorized retreat would be subject to a military tribunal. However, it was the NKVD that ordered the regular army and lectured them, on the need to show some guts. Through brutal coercion for self-sacrifice, thousands of deserters and presumed malingerers were captured or executed to discipline the troops. At Stalingrad, it is estimated that 14,000 soldiers of the Red Army were executed in order to keep the formation. "Not a step back!" and "There is no land behind the Volga!" were the slogans. The Germans pushing forward into Stalingrad suffered heavy casualties.
Fighting in the city.
By 12 September, at the time of their retreat into the city, the Soviet 62nd Army had been reduced to 90 tanks, 700 mortars and just 20,000 personnel. The remaining tanks were used as immobile strongpoints within the city. The initial German attack attempted to take the city in a rush. One infantry division went after the Mamayev Kurgan, one attacked the central rail station and one attacked toward the central landing stage on the Volga.
Though initially successful, the German attacks stalled in the face of Soviet reinforcements brought in from across the Volga. The 13th Guards Rifle Division, assigned to counterattack at the Mamayev Kurgan and at Railway Station No. 1 suffered particularly heavy losses. Over 30 percent of its soldiers were killed in the first 24 hours, and just 320 out of the original 10,000 survived the entire battle. Both objectives were retaken, but only temporarily. The railway station changed hands 14 times in six hours. By the following evening, the 13th Guards Rifle Division had ceased to exist.
Combat raged for three days at the giant grain elevator in the south of the city. About fifty Red Army defenders, cut off from resupply, held the position for five days and fought off ten different assaults before running out of ammunition and water. Only forty dead Soviet fighters were found, though the Germans had thought there were many more due to the intensity of resistance. The Soviets burned large amounts of grain during their retreat in order to deny the enemy food. Paulus chose the grain elevator and silos as the symbol of Stalingrad for a patch he was having designed to commemorate the battle after a German victory.
German military doctrine was based on the principle of combined-arms teams and close cooperation between tanks, infantry, engineers, artillery and ground-attack aircraft. Some Soviet commanders adopted the tactic of always keeping their front-line positions as close to the Germans as physically possible; Chuikov called this "hugging" the Germans. This slowed the German advance and reduced the effectiveness of the German advantage in supporting fire.
The Red Army gradually adopted a strategy to hold for as long as possible all the ground in the city. Thus, they converted multi-floored apartment blocks, factories, warehouses, street corner residences and office buildings into a series of well defended strongpoints with small 5–10 man units. Manpower in the city was constantly refreshed by bringing additional troops over the Volga. When a position was lost, an immediate attempt was usually made to re-take it with fresh forces.
Bitter fighting raged for every ruin, street, factory, house, basement, and staircase. Even the sewers were the sites of firefights. The Germans, calling this unseen urban warfare "Rattenkrieg" ("Rat War"), bitterly joked about capturing the kitchen but still fighting for the living room and the bedroom. Buildings had to be cleared room by room through the bombed-out debris of residential neighborhoods, office blocks, basements and apartment high-rises. Some of the taller buildings, blasted into roofless shells by earlier German aerial bombardment, saw floor-by-floor, close quarters combat, with the Germans and Soviets on alternate levels, firing at each other through holes in the floors.
Fighting on and around Mamayev Kurgan, a prominent hill above the city, was particularly merciless; indeed, the position changed hands many times.
In another part of the city, a Soviet platoon under the command of Sergeant Yakov Pavlov fortified a four-story building that oversaw a square 300 meters from the river bank, later called "Pavlov's House". The soldiers surrounded it with minefields, set up machine-gun positions at the windows and breached the walls in the basement for better communications. The soldiers found about ten Soviet civilians hiding in the basement. They were not relieved, and not significantly reinforced, for two months. The building was labeled "Festung" ("Fortress") on German maps. Sgt. Pavlov was awarded the Hero of the Soviet Union for his actions.
The Germans made slow but steady progress through the city. Positions were taken individually, but the Germans were never able to capture the key crossing points along the river bank. The Germans used airpower, tanks and heavy artillery to clear the city with varying degrees of success. Toward the end of the battle, the gigantic railroad gun nicknamed "Dora" was brought into the area. The Soviets built up a large number of artillery batteries on the east bank of the Volga. This artillery was able to bombard the German positions or at least to provide counter-battery fire.
Snipers on both sides used the ruins to inflict casualties. The most famous Soviet sniper in Stalingrad was Vasily Zaytsev, with 225 confirmed kills during the battle. Targets were often soldiers bringing up food or water to forward positions. Artillery spotters were an especially prized target for snipers.
A significant historical debate concerns the degree of terror in the Red Army. The British historian Antony Beevor noted the "sinister" message from the Stalingrad Front's Political Department on 8 October 1942 that: "The defeatist mood is almost eliminated and the number of treasonous incidents is getting lower" as an example of the sort of coercion Red Army soldiers experienced under the Special Detachments (later to be renamed SMERSH). On the other hand, Beevor noted the often extraordinary bravery of the Soviet soldiers in a battle that was only comparable to Verdun, and argued that terror alone cannot explain such self-sacrifice. Richard Overy addresses the question of just how important the Red Army's coercive methods were to the Soviet war effort compared with other motivational factors such as hatred for the enemy. He argues that, though it is "easy to argue that from the summer of 1942 the Soviet army fought because it was forced to fight," to concentrate solely on coercion is nonetheless to "distort our view of the Soviet war effort." After conducting hundreds of interviews with Soviet veterans on the subject of terror on the Eastern Front – and specifically about Order No. 227 ("Not a step back!") at Stalingrad – Catherine Merridale notes that, seemingly paradoxically, "their response was frequently relief." Infantryman Lev Lvovich's explanation, for example, is typical for these interviews; as he recalls, "t was a necessary and important step. We all knew where we stood after we had heard it. And we all – it's true – felt better. Yes, we felt better."
Many women fought on the Soviet side, or were under fire. As General Chuikov acknowledged, "Remembering the defence of Stalingrad, I can't overlook the very important question ... about the role of women in war, in the rear, but also at the front. Equally with men they bore all the burdens of combat life and together with us men, they went all the way to Berlin." At the beginning of the battle there were 75,000 women and girls from the Stalingrad area who had finished military or medical training, and all of whom were to serve in the battle. Women staffed a great many of the anti-aircraft batteries that fought not only the Luftwaffe but German tanks. Soviet nurses not only treated wounded personnel under fire but were involved in the highly dangerous work of bringing wounded soldiers back to the hospitals under enemy fire. Many of the Soviet wireless and telephone operators were women who often suffered heavy casualties when their command posts came under fire. Though women were not usually trained as infantry, many Soviet women fought as machine gunners, mortar operators, and scouts. Women were also snipers at Stalingrad. Three air regiments at Stalingrad were entirely female. At least three women won the title Hero of the Soviet Union while driving tanks at Stalingrad.
For both Stalin and Hitler, Stalingrad became a matter of prestige far beyond its strategic significance. The Soviet command moved units from the Red Army strategic reserve in the Moscow area to the lower Volga, and transferred aircraft from the entire country to the Stalingrad region.
The strain on both military commanders was immense: Paulus developed an uncontrollable tic in his eye, which eventually afflicted the left side of his face, while Chuikov experienced an outbreak of eczema that required him to have his hands completely bandaged. Troops on both sides faced the constant strain of close-range combat.
Air attacks.
Determined to crush Soviet resistance, "Luftflotte" 4's "Stukawaffe" flew 900 individual sorties against Soviet positions at the "Dzerzhinskiy" Tractor Factory on 5 October. Several Soviet regiments were wiped out; the entire staff of the Soviet 339th Infantry Regiment was killed the following morning during an air raid.
In mid-October, the "Luftwaffe" intensified its efforts against remaining Red Army positions holding the west bank. "Luftflotte" 4 flew 2,000 sorties on 14 October and of bombs were dropped while German infantry surrounded the three factories. "Stukageschwader" 1, 2, and 77 had largely silenced Soviet artillery on the eastern bank of the Volga before turning their attention to the shipping that was once again trying to reinforce the narrowing Soviet pockets of resistance. The 62nd Army had been cut in two, and, due to intensive air attack on its supply ferries, was receiving much less material support. With the Soviets forced into a strip of land on the western bank of the Volga, over 1,208 "Stuka" missions were flown in an effort to eliminate them.
The "Luftwaffe" retained air superiority into November and Soviet daytime aerial resistance was nonexistent. However, the combination of constant air support operations on the German side and the Soviet surrender of the daytime skies began to affect the strategic balance in the air. After flying 20,000 individual sorties, the "Luftwaffe" original strength of 1,600 serviceable aircraft had fallen to 950. The "Kampfwaffe" (bomber force) had been hardest hit, having only 232 out of a force of 480 left. The "VVS" remained qualitatively inferior, but by the time of the Soviet counter-offensive, the "VVS" had reached numerical superiority.
The Soviet bomber force, the "Aviatsiya Dal'nego Deystviya" (Long Range Aviation; ADD), having taken crippling losses over the past 18 months, was restricted to flying at night. The Soviets flew 11,317 night sorties over Stalingrad and the Don-bend sector between 17 July and 19 November. These raids caused little damage and were of nuisance value only.
On 8 November, substantial units from "Luftflotte" 4 were withdrawn to combat the Allied landings in North Africa. The German air arm found itself spread thinly across Europe, struggling to maintain its strength in the other southern sectors of the Soviet-German front. The Soviets began receiving material assistance from the American government under the Lend-Lease program. During the last quarter of 1942, the U.S. sent the Soviet Union of explosives and of aviation gas.
As historian Chris Bellamy notes, the Germans paid a high strategic price for the aircraft sent into Stalingrad: the "Luftwaffe" was forced to divert much of its air strength away from the oil-rich Caucasus, which had been Hitler's original grand-strategic objective.
Germany reaches the Volga.
After three months of slow advance, the Germans finally reached the river banks, capturing 90% of the ruined city and splitting the remaining Soviet forces into two narrow pockets. Ice floes on the Volga now prevented boats and tugs from supplying the Soviet defenders. Nevertheless, the fighting, especially on the slopes of Mamayev Kurgan and inside the factory area in the northern part of the city, continued.
Soviet counter-offensives.
Recognizing that German troops were ill prepared for offensive operations during the winter of 1942, and that most of them were redeployed elsewhere on the southern sector of the Eastern Front, the Stavka decided to conduct a number of offensive operations between 19 November 1942 and 2 February 1943. These operations opened the Winter Campaign of 1942–1943 (19 November 1942 – 3 March 1943), which involved some 15 Armies operating on several fronts.
Weakness on the German flanks.
During the siege, the German and allied Italian, Hungarian, and Romanian armies protecting Army Group B's flanks had pressed their headquarters for support. The Hungarian 2nd Army was given the task of defending a section of the front north of Stalingrad between the Italian Army and Voronezh. This resulted in a very thin line, with some sectors where stretches were being defended by a single platoon. These forces were also lacking in effective anti-tank weapons.
Because of the total focus on the city, the Axis forces had neglected for months to consolidate their positions along the natural defensive line of the Don River. The Soviet forces were allowed to retain bridgeheads on the right bank from which offensive operations could be quickly launched. These bridgeheads in retrospect presented a serious threat to Army Group B.
Similarly, on the southern flank of the Stalingrad sector the front southwest of Kotelnikovo was held only by the Romanian 7th Army Corps, and beyond it, a single German division, the 16th Motorized Infantry.
Operation Uranus: the Soviet offensive.
In autumn, the Soviet generals Georgy Zhukov and Aleksandr Vasilevsky, responsible for strategic planning in the Stalingrad area, concentrated forces in the steppes to the north and south of the city. The northern flank was defended by Hungarian and Romanian units, often in open positions on the steppes. The natural line of defense, the Don River, had never been properly established by the German side. The armies in the area were also poorly equipped in terms of anti-tank weapons. The plan was to punch through the overstretched and weakly defended German flanks and surround the German forces in the Stalingrad region.
During the preparations for the attack, Marshal Zhukov personally visited the front and noticing the poor organization, insisted on a one-week delay in the start date of the planned attack. The operation was code-named "Uranus" and launched in conjunction with Operation Mars, which was directed at Army Group Center. The plan was similar to the one Zhukov had used to achieve victory at Khalkhin Gol three years before, where he had sprung a double envelopment and destroyed the 23rd Division of the Japanese army.
On 19 November 1942, the Red Army launched Operation Uranus. The attacking Soviet units under the command of Gen. Nikolay Vatutin consisted of three complete armies, the 1st Guards Army, 5th Tank Army, and 21st Army, including a total of 18 infantry divisions, eight tank brigades, two motorized brigades, six cavalry divisions and one anti-tank brigade. The preparations for the attack could be heard by the Romanians, who continued to push for reinforcements, only to be refused again. Thinly spread, deployed in exposed positions, outnumbered and poorly equipped, the Romanian 3rd Army, which held the northern flank of the German 6th Army, was overrun.
Behind the front lines, no preparations had been made to defend key points in the rear such as Kalach. The response by the "Wehrmacht" was both chaotic and indecisive. Poor weather prevented effective air action against the Soviet offensive.
On 20 November, a second Soviet offensive (two armies) was launched to the south of Stalingrad against points held by the Romanian 4th Army Corps. The Romanian forces, made up primarily of infantry, were overrun by large numbers of tanks. The Soviet forces raced west and met on 23 November at the town of Kalach, sealing the ring around Stalingrad. The link-up of the Soviet forces, not filmed at the time, was later re-enacted for a propaganda film which was shown worldwide.
Sixth Army surrounded.
About 265,000 German, Romanian, and Italian soldiers, the 369th (Croatian) Reinforced Infantry Regiment, and other volunteer subsidiary troops including some 40,000 Soviet volunteers fighting for the Germans (Beevor states that one quarter of the sixth army's frontline strength were HIWIs, as collaborationists recruited from the ranks of Soviet POWs were called) were surrounded. These Soviet HIWIs remained loyal, knowing the Soviet penalty for helping the Germans was summary execution. German strength in the pocket was about 210,000 according to strength breakdowns of the 20 field divisions (average size 9,000) and 100 battalion sized units of the Sixth Army on 19 November 1942. Inside the pocket (, literally "cauldron"), there were also around 10,000 Soviet civilians and several thousand Soviet soldiers the Germans had taken captive during the battle. Not all of the 6th Army was trapped; 50,000 soldiers were brushed aside outside the pocket. These belonged mostly to the other 2 divisions of the 6th Army between the Italian and Romanian Armies: the 62nd and 298th Infantry Divisions. Of the 210,000 Germans, 10,000 remained to fight on, 105,000 surrendered, 35,000 left by air and the remaining 60,000 died.
The Red Army units immediately formed two defensive fronts: a circumvallation facing inward and a contravallation facing outward. Field Marshal Erich von Manstein advised Hitler not to order the 6th Army to break out, stating that he could break through the Soviet lines and relieve the besieged 6th Army. The American historians Williamson Murray and Alan Millet wrote that it was Manstein's message to Hitler on 24 November advising him that the 6th Army should not break out, along with Göring's statements that the Luftwaffe could supply Stalingrad that "... sealed the fate of the Sixth Army." After 1945, Manstein claimed that he told Hitler that the 6th Army must break out. The American historian Gerhard Weinberg wrote that Manstein distorted his record on the matter. Manstein was tasked to conduct a relief operation, named Operation Winter Storm ("Unternehmen Wintergewitter") against Stalingrad, which he thought was feasible if the 6th Army was temporarily supplied through the air.
Adolf Hitler had declared in a public speech (in the Berlin Sportpalast) on 30 September 1942 that the German army would never leave the city. At a meeting shortly after the Soviet encirclement, German army chiefs pushed for an immediate breakout to a new line on the west of the Don, but Hitler was at his Bavarian retreat of Obersalzberg in Berchtesgaden with the head of the "Luftwaffe", Hermann Göring. When asked by Hitler, Göring replied, after being convinced by Hans Jeschonnek, that the Luftwaffe could supply the 6th Army with an "air bridge." This would allow the Germans in the city to fight on temporarily while a relief force was assembled. A similar plan had been used a year earlier at the Demyansk Pocket, albeit on a much smaller scale: a corps at Demyansk rather than an entire army.
The director of "Luftflotte" 4, Wolfram von Richthofen, tried to get this decision overturned. The forces under 6th Army were almost twice as large as a regular German army unit, plus there was also a corps of the 4th Panzer Army trapped in the pocket. The maximum they could deliver a day—based on the number of available aircraft and with only the airfield at Pitomnik to land at—was far less than the minimum needed. To supplement the limited number of Junkers Ju 52 transports, the Germans pressed other aircraft into the role, such as the Heinkel He 177 bomber (some bombers performed adequately—the Heinkel He 111 proved to be quite capable and was much faster than the Ju 52). General Richthofen informed Manstein on 27 November of the small transport capacity of the Luftwaffe and the impossibility of supplying 300 tons a day by air. Manstein now saw the enormous technical difficulties of a supply by air of these dimensions. The next day he made a six-page situation report to the general staff. Based on the information of the expert Richthofen, he declared that contrary to the example of the pocket of Demyansk the permanent supply by air would be impossible. If only a narrow link could be established to Sixth Army, he proposed that this should be used to pull it out from the encirclement, and said that the Luftwaffe should instead of supplies deliver only enough ammunition and fuel for a breakout attempt. He acknowledged the heavy moral sacrifice the giving up of Stalingrad means but this is made easier to bear by the conservation of the combat power of Sixth Army and the regaining of the initiative ..." He ignored the limited mobility of the army and the difficulties of disengaging the Soviets. Hitler reiterated that Sixth Army would stay at Stalingrad and that the air bridge would supply it until the encirclement was broken by a new German offensive.
The "Luftwaffe" was able to deliver an average of of supplies per day out of an air transport capacity of per day. The most successful day, 19 December, delivered of supplies in 154 flights.
In the early parts of the operation, fuel was shipped at a higher priority than food and ammunition because of a belief that there would be a breakout from the city. Transport aircraft also evacuated technical specialists and sick or wounded personnel from the besieged enclave. Sources differ on the number flown out: at least 25,000 to at most 35,000. Carell: 42,000, of which 5000 did not survive.
Initially, supply flights came in from the field at Tatsinskaya, called 'Tazi' by the German pilots. On 23 December, the Soviet 24th Tank Corps, commanded by Major-General Vasily Mikhaylovich Badanov, reached nearby Skassirskaya and in the early morning of 24 December, the tanks reached Tatsinskaya. Without any soldiers to defend the airfield, it was abandoned under heavy fire; in a little under an hour, 108 Ju 52s and 16 Ju 86s took off for Novocherkassk—leaving 72 Ju 52s and many other aircraft burning on the ground. A new base was established some from Stalingrad at Salsk, the additional distance would become another obstacle to the resupply efforts. Salsk was abandoned in turn by mid-January for a rough facility at Zverevo, near Shakhty. The field at Zverevo was attacked repeatedly on 18 January and a further 50 Ju 52s were destroyed. Winter weather conditions, technical failures, heavy Soviet anti-aircraft fire and fighter interceptions eventually led to the loss of 488 German aircraft.
In spite of the failure of the German offensive to reach 6th Army, the air supply operation continued under ever more difficult circumstances. The 6th Army slowly starved. Pilots were shocked to find the troops too exhausted and hungry to unload. Germans fought over the slightest scraps of bread. General Zeitzler, moved by their plight, began to limit himself to their slim rations at meal times. After a few weeks on such a diet, he had lost and had become so emaciated that Hitler, annoyed, personally ordered him to start eating regular meals again.
The toll on the "Transportgruppen" was heavy. 160 aircraft were destroyed and 328 were heavily damaged (beyond repair). Some 266 Junkers Ju 52s were destroyed; one-third of the fleet's strength on the Eastern Front. The He 111 "gruppen" lost 165 aircraft in transport operations. Other losses included 42 Ju 86s, 9 Fw 200 Condors, 5 He 177 bombers and 1 Ju 290. The "Luftwaffe" also lost close to 1,000 highly experienced bomber crew personnel. So heavy were the "Luftwaffe"s losses that four of "Luftflotte" 4's transport units (KGrzbV 700, KGrzbV 900, I./KGrzbV 1 and II./KGzbV 1) were "formally dissolved."
End of the battle.
Operation Winter Storm.
Soviet forces consolidated their positions around Stalingrad, and fierce fighting to shrink the pocket began. Operation Winter Storm ("Operation Wintergewitter"), the German attempt led by Erich von Manstein to relieve the trapped army from the south, was initially successful. By 19 December, the German Army had pushed to within 48 km (30 mi) of Sixth Army's positions. The starving encircled forces at Stalingrad made no attempt to break out or link up with the Manstein's advance. Some German officers requested that Paulus defy Hitler's orders to stand fast and instead attempt to break out of the Stalingrad pocket. Paulus refused. On 23 December, the attempt to relieve Stalingrad was abandoned and Manstein's forces switched over to the defensive to deal with new Soviet offensives.
Operation Little Saturn.
On 16 December, the Soviets launched Operation Little Saturn, which attempted to punch through the Axis army (mainly Italians) on the Don and take Rostov. The Germans set up a "mobile defense" of small units that were to hold towns until supporting armor arrived. From the Soviet bridgehead at Mamon, 15 divisions—supported by at least 100 tanks—attacked the Italian Cosseria and Ravenna Divisions, and although outnumbered 9 to 1, the Italians initially fought well, with the Germans praising the quality of the Italian defenders, but on 19 December, with the Italian lines disintegrating, ARMIR headquarters ordered the battered divisions to withdraw to new lines.
The fighting forced a total revaluation of the German situation. The attempt to break through to Stalingrad was abandoned and Army Group A was ordered to pull back from the Caucasus.
The 6th Army now was beyond all hope of German relief. While a motorised breakout might have been possible in the first few weeks, the 6th Army now had insufficient fuel and the German soldiers would have faced great difficulty breaking through the Soviet lines on foot in harsh winter conditions. But in its defensive position on the Volga, 6th Army continued to tie down a significant number of Soviet Armies.
Soviet victory.
The Germans inside the pocket retreated from the suburbs of Stalingrad to the city itself. The loss of the two airfields, at Pitomnik on 16 January 1943 and Gumrak on the night of 21/22 January, meant an end to air supplies and to the evacuation of the wounded. The third and last serviceable runway was at the Stalingradskaya flight school, which reportedly had the last landings and takeoffs on the night of 22–23 January. After daybreak on 23 January, there were no more reported landings except for intermittent air drops of ammunition and food until the end.
The Germans were now not only starving, but running out of ammunition. Nevertheless, they continued to resist, in part because they believed the Soviets would execute any who surrendered. In particular, the so-called "HiWis", Soviet citizens fighting for the Germans, had no illusions about their fate if captured. The Soviets were initially surprised by the number of Germans they had trapped, and had to reinforce their encircling troops. Bloody urban warfare began again in Stalingrad, but this time it was the Germans who were pushed back to the banks of the Volga. The Germans adopted a simple defense of fixing wire nets over all windows to protect themselves from grenades. The Soviets responded by fixing fish hooks to the grenades so they stuck to the nets when thrown.
The Germans had no usable tanks in the city, and those that still functioned could, at best, be used as makeshift pillboxes. The Soviets did not bother employing tanks in areas where the urban destruction restricted their mobility. A low-level Soviet envoy party (comprising Major Aleksandr Smyslov, Captain Nikolay Dyatlenko and a trumpeter) carried an offer to Paulus: if he surrendered within 24 hours, he would receive a guarantee of safety for all prisoners, medical care for the sick and wounded, prisoners being allowed to keep their personal belongings, "normal" food rations, and repatriation to any country they wished after the war; but Paulus—ordered not to surrender by Hitler—did not respond.
On 22 January Paulus requested that he be granted permission to surrender. Hitler rejected it on a point of honour. He telegraphed the 6th Army later that day, claiming that it had made a historic contribution to the greatest struggle in German history and that it should stand fast "to the last soldier and the last bullet." Hitler told Goebbels that the plight of the 6th Army was a "heroic drama of German history."
On 26 January 1943, the German forces inside Stalingrad were split into two pockets. A northern pocket centered on the tractor factory and a smaller southern pocket in the city center. The northern pocket was tactically commanded by General Walter Heitz while the southern pocket was commanded by Paulus.
On 30 January 1943, the 10th anniversary of Hitler's coming to power, Goebbels read out a proclamation that included the sentence: "The heroic struggle of our soldiers on the Volga should be a warning for everybody to do the utmost for the struggle for Germany's freedom and the future of our people, and thus in a wider sense for the maintenance of our entire continent." Hitler promoted Paulus to the rank of "Generalfeldmarschall". No German field marshal had ever surrendered, and the implication was clear: if Paulus surrendered, he would shame himself and would become the highest ranking German officer ever to be captured. Hitler believed that Paulus would either fight to the last man or commit suicide. Paulus, however, commented, "I have no intention of shooting myself for this Bohemian corporal."
The next day, the southern pocket in Stalingrad collapsed. Soviet forces reached the entrance to the German headquarters in the ruined GUM department store. General Schmidt negotiated a surrender of the headquarters while Paulus waited in another room. When interrogated by the Soviets, Paulus claimed that he had not surrendered. He said that he had been taken by surprise. He denied that he was the commander of the remaining northern pocket in Stalingrad and refused to issue an order in his name for them to surrender.
Four Soviet armies were deployed against the remaining northern pocket. At four in the morning on 2 February, General Strecker was informed that one of his own officers had gone to the Soviets to negotiate surrender terms. Seeing no point in continuing, he sent a radio message saying that his command had done its duty and fought to the last man. He then surrendered. Around 91,000 exhausted, ill, wounded, and starving prisoners were taken, including 3,000 Romanians (the survivors of the 20th Infantry Division, 1st Cavalry Division and "Col. Voicu" Detachment). The prisoners included 22 generals. Hitler was furious and confided that Paulus "could have freed himself from all sorrow and ascended into eternity and national immortality, but he prefers to go to Moscow."
Aftermath.
The German public was not officially told of the impending disaster until the end of January 1943, though positive media reports had stopped in the weeks before the announcement. Stalingrad marked the first time that the Nazi government publicly acknowledged a failure in its war effort; it was not only the first major setback for the German military, but a crushing defeat where German losses were almost equal to those of the Soviets was unprecedented. Prior losses of the Soviet Union were generally three times as high as the German ones. On 31 January, regular programming on German state radio was replaced by a broadcast of the somber Adagio movement from Anton Bruckner's Seventh Symphony, followed by the announcement of the defeat at Stalingrad.
On 18 February, Minister of Propaganda Joseph Goebbels gave the famous "Sportpalast" speech in Berlin, encouraging the Germans to accept a total war that would claim all resources and efforts from the entire population.
Based on Soviet records, over 10,000 soldiers continued to resist in isolated groups within the city for the next month. Some have presumed that they were motivated by a belief that fighting on was better than a slow death in Soviet captivity. The Israeli historian Omer Bartov claims they were motivated by National Socialism. He studied 11,237 letters sent by soldiers inside of Stalingrad between 20 December 1942 and 16 January 1943 to their families in Germany. Almost every letter expressed belief in Germany's ultimate victory and their willingness to fight and die at Stalingrad to achieve that victory. Bartov reported that a great many of the soldiers were well aware that they would not be able to escape from Stalingrad, but in their letters to their families boasted that they were proud to "sacrifice themselves for the Führer".
The remaining forces continued to resist, hiding in cellars and sewers, but by early March 1943, the remaining small and isolated pockets of resistance had surrendered. According to Soviet intelligence documents shown in the documentary, a remarkable NKVD report from March 1943 is available showing the tenacity of some of these German groups:
The operative report of the Don Front's staff issued on 5 February 1943, 22.00 said:
The condition of the troops that surrendered was pitiful. British war correspondent Alexander Werth describes the following scene in his "Russia at War" book, based on a first-hand account of his visit to Stalingrad during 3–5 February 1943:
Out of the nearly 91,000 German prisoners captured in Stalingrad, only about 5,000 ever returned. Already weakened by disease, starvation and lack of medical care during the encirclement, they were sent on foot marches to prisoner camps and later to labour camps all over the Soviet Union. Some 35,000 were eventually sent on transports, of which 17,000 did not survive. Overall, 75,000 POWs died within 3 months of capture. Most died of wounds, disease (particularly typhus), cold, overwork, mistreatment, and malnutrition. Some were kept in the city to help rebuild.
A handful of senior officers were taken to Moscow and used for propaganda purposes, and some of them joined the National Committee for a Free Germany. Some, including Paulus, signed anti-Hitler statements that were broadcast to German troops. Paulus testified for the prosecution during the Nuremberg Trials and assured families in Germany that those soldiers taken prisoner at Stalingrad were safe. He remained in the Soviet Union until 1952, then moved to Dresden in East Germany, where he spent the remainder of his days defending his actions at Stalingrad, and was quoted as saying that Communism was the best hope for postwar Europe. General Walther von Seydlitz-Kurzbach offered to raise an anti-Hitler army from the Stalingrad survivors, but the Soviets did not accept. It was not until 1955 that the last of the 5–6,000 survivors were repatriated (to West Germany) after a plea to the Politburo by Konrad Adenauer.
Significance.
Stalingrad has been described as the biggest defeat in the history of the German Army. It is often identified as "the" turning point on the Eastern Front, and in the war against Germany overall, and even the turning point of the whole Second World War. Before Stalingrad, the German forces had gone from victory to victory on the Eastern Front, with a limited setback in the winter of 1941–42. After Stalingrad, they won no decisive battles, even in summer. The Red Army had the initiative, and the Wehrmacht was in retreat. A year of German gains during Case Blue had been wiped out. Germany's Sixth Army had ceased to exist, and the forces of Germany's European allies, except Finland, had been shattered. In a speech on 9 November 1944, Hitler himself blamed Stalingrad for Germany's impending doom.
Stalingrad's significance has been downplayed by some historians, who point either to the Battle of Moscow or the Battle of Kursk as more strategically decisive. Others maintain that the destruction of an entire army (the largest killed, captured, wounded figures for Axis soldiers, nearly 1 million, during the war) and the frustration of Germany's grand strategy made the battle a watershed moment. At the time, however, the global significance of the battle was not in doubt. Writing in his diary on 1 January 1943, British General Alan Brooke, Chief of the Imperial General Staff, reflected on the change in the position from a year before:
At that point, the British had won the Battle of El Alamein in November 1942. However, there were only about 50,000 German soldiers at El Alamein in Egypt, while at Stalingrad 200,000 Germans had been lost.
Regardless of the strategic implications, there is little doubt that Stalingrad was a morale watershed. Germany's defeat shattered its reputation for invincibility and dealt a devastating blow to German morale. On 30 January 1943, the tenth anniversary of his coming to power, Hitler chose not to speak. Joseph Goebbels read the text of his speech for him on the radio. The speech contained an oblique reference to the battle, which suggested that Germany was now in a defensive war. The public mood was sullen, depressed, fearful, and war-weary. Germany was looking in the face of defeat.
The reverse was the case on the Soviet side. There was an overwhelming surge in confidence and belief in victory. A common saying was: "You cannot stop an army which has done Stalingrad." Stalin was feted as the hero of the hour and made a Marshal of the Soviet Union. In recognition of the determination of its defenders, Stalingrad was awarded the title Hero City in 1945. A colossal monument called The Motherland Calls was erected in 1967 on Mamayev Kurgan, the hill overlooking the city where bones and rusty metal splinters can still be found. The statue forms part of a war memorial complex which includes the ruins of the Grain Silo and Pavlov's House.
The news of the battle echoed round the world, with many people now believing that Hitler's defeat was inevitable. The Turkish Consul in Moscow predicted that "the lands which the Germans have destined for their living space will become their dying space". Britain's conservative "Daily Telegraph" proclaimed that the victory had saved European civilisation. The country celebrated "Red Army Day" on 23 February 1943. A ceremonial Sword of Stalingrad was forged by King George VI. After being put on public display in Britain, this was presented to Stalin by Winston Churchill at the Tehran conference later in 1943. Soviet propaganda spared no effort and wasted no time in capitalising on the triumph, impressing a global audience. The prestige of Stalin, the Soviet Union, and the worldwide Communist movement was immense, and their political position greatly enhanced.
During the defence of Stalingrad, the Red Army deployed five armies (28th, 51st, 57th, 62nd and 64th Armies) in and around the city and an additional nine armies in the encirclement counter offensive. The nine armies amassed for the counteroffensive were the 24th Army, 65th Army, 66th Army and 16th Air Army from the north as part of the Don Front offensive and 1st Guards Army, 5th Tank, 21st Army, 2nd Air Army and 17th Air Army from the south as part of the Southwestern Front.
More information.
Casualties.
The calculation of casualties depends on what scope is given to the battle of Stalingrad. The scope can vary from just the fighting within the city and suburbs itself to the inclusion of almost all fighting on the southern wing of the Soviet-German front from the spring of 1942 to the end of the fighting in the city in the winter of 1943. Different scholars have produced different estimates depending on their definition of the scope of the battle. The difference is comparing the city against the region.
The Axis suffered 850,000 total casualties (wounded, killed, captured) among all branches of the German armed forces and its allies; 400,000 Germans, 200,000 Romanians, 130,000 Italians, and 120,000 Hungarians were killed, wounded or captured.
On the materiel side, the Germans losses were 900 aircraft (including 274 transports and 165 bombers used as transports), 500 tanks, and 6,000 artillery pieces. According to a contemporary Soviet report, 5,762 artillery pieces; 1,312 mortars; 12,701 heavy machine guns; 156,987 rifles; 80,438 sub-machine guns; 10,722 trucks; 744 aircraft; 1,666 tanks; 261 other armored vehicles; 571 half-tracks; and 10,679 motorcycles were captured by the Soviets. An unknown amount of Hungarian, Italian, and Romanian material was lost.
The USSR, according to archival figures, suffered 1,129,619 total casualties; 478,741 personnel killed or missing, and 650,878 wounded or sick. On the material side, the USSR lost 4,341 tanks destroyed or damaged, 15,728 artillery pieces, and 2,769 combat aircraft.
Anywhere from 25,000 to 40,000 Soviet civilians died in Stalingrad and its suburbs during a single week of aerial bombing by "Luftflotte" 4 as the German 4th Panzer and 6th Armies approached the city; The total number of civilians killed in Stalingrad is unknown.
In all, the battle resulted in an estimated total of 1.7–2 million Axis and Soviet casualties.
Luftwaffe losses.
Aircraft losses of the Luftwaffe for the supply of the 6th Army at Stalingrad, and the recovery of wounded from 24 November 1942 to 31 January 1943:
The losses of transport planes were especially serious, as they destroyed the capacity for resupply of the trapped 6th Army. The destruction of 72 aircraft when the airfield at Tatsinskaya was overrun meant the loss of about 10% of the entire Luftwaffe transport fleet.
These losses amounted to about 50% of total aircraft committed. In addition, the Luftwaffe training program was stopped and sorties in other theaters of war were significantly reduced to save fuel for use at Stalingrad.
In popular culture.
The events of the Battle for Stalingrad have been covered in numerous media works of British, American, German, and Russian origin, for its significance as a turning point in the Second World War and for the loss of life associated with the battle.

</doc>
<doc id="4285" url="https://en.wikipedia.org/wiki?curid=4285" title="Bodhidharma">
Bodhidharma

Bodhidharma was a Buddhist monk who lived during the 5th or 6th century. He is traditionally credited as the transmitter of Chan Buddhism to China, and regarded as its first Chinese patriarch. According to Chinese legend, he also began the physical training of the monks of Shaolin Monastery that led to the creation of Shaolin Kung Fu.
Little contemporary biographical information on Bodhidharma is extant, and subsequent accounts became layered with legend. The principal Chinese sources vary on their account of Bodhidharma's origins, giving either an origin from India or Central Asia.
Throughout Buddhist art, Bodhidharma is depicted as an ill-tempered, profusely-bearded, wide-eyed non-Chinese person. He is referred as "The Blue-Eyed Barbarian" () in Chan texts.
Aside from the Chinese accounts, several popular traditions also exist regarding Bodhidharma's origins.
The accounts also differ on the date of his arrival, with one early account claiming that he arrived during the Liu Song dynasty (420–479) and later accounts dating his arrival to the Liang dynasty (502–557). Bodhidharma was primarily active in the territory of the Northern Wei (386–534). Modern scholarship dates him to about the early 5th century.
Bodhidharma's teachings and practice centered on meditation and the "Laṅkāvatāra Sūtra". The "Anthology of the Patriarchal Hall" (952) identifies Bodhidharma as the 28th Patriarch of Buddhism in an uninterrupted line that extends all the way back to the Gautama Buddha himself.
Biography.
Principal sources.
There are two known extant accounts written by contemporaries of Bodhidharma.
"The Record of the Buddhist Monasteries of Luoyang".
The earliest text mentioning Bodhidharma is "The Record of the Buddhist Monasteries of Luoyang" ( "Luòyáng Qiélánjì") which was compiled in 547 by Yáng Xuànzhī (楊衒之), a writer and translator of Mahayana sutras into Chinese. Yang gave the following account:
Tánlín – preface to the Two Entrances and Four Acts.
The second account was written by Tánlín (曇林; 506–574). Tánlín's brief biography of the "Dharma Master" is found in his preface to the "Long Scroll of the Treatise on the Two Entrances and Four Practices", a text traditionally attributed to Bodhidharma and the first text to identify him as South Indian:
Tánlín's account was the first to mention that Bodhidharma attracted disciples, specifically mentioning Dàoyù (道育) and Dazu Huike (慧可), the latter of whom would later figure very prominently in the Bodhidharma literature. Although Tánlín has traditionally been considered a disciple of Bodhidharma, it is more likely that he was a student of Huìkě.
"Chronicle of the "Laṅkāvatāra" Masters".
Tanlin's preface has also been preserved in Jingjue's (683-750) "Lengjie Shizi ji" "Chronicle of the "Laṅkāvatāra" Masters", which dates from 713-716./ca. 715 He writes, "The teacher of the Dharma, who came from South India in the Western Regions, the third son of a great Brahman king."
"Further Biographies of Eminent Monks".
In the 7th-century historical work "Further Biographies of Eminent Monks" (續高僧傳 "Xù gāosēng zhuàn"), Dàoxuān (道宣; 596-667) possibly drew on Tanlin's preface as a basic source, but made several significant additions:
Firstly, Dàoxuān adds more detail concerning Bodhidharma's origins, writing that he was of "South Indian Brahman stock" (南天竺婆羅門種 "nán tiānzhú póluómén zhŏng").
Secondly, more detail is provided concerning Bodhidharma's journeys. Tanlin's original is imprecise about Bodhidharma's travels, saying only that he "crossed distant mountains and seas" before arriving in Wei. Dàoxuān's account, however, implies "a specific itinerary": "He first arrived at Nan-yüeh during the Sung period. From there he turned north and came to the Kingdom of Wei" This implies that Bodhidharma had travelled to China by sea and that he had crossed over the Yangtze.
Thirdly, Dàoxuān suggests a date for Bodhidharma's arrival in China. He writes that Bodhidharma makes landfall in the time of the Song, thus making his arrival no later than the time of the Song's fall to the Southern Qi in 479.
Finally, Dàoxuān provides information concerning Bodhidharma's death. Bodhidharma, he writes, died at the banks of the Luo River, where he was interred by his disciple Dazu Huike, possibly in a cave. According to Dàoxuān's chronology, Bodhidharma's death must have occurred prior to 534, the date of the Northern Wei's fall, because Dazu Huike subsequently leaves Luoyang for Ye. Furthermore, citing the shore of the Luo River as the place of death might possibly suggest that Bodhidharma died in the mass executions at Heyin (河陰) in 528. Supporting this possibility is a report in the Chinese Buddhist canon stating that a Buddhist monk was among the victims at Héyīn.
Later accounts.
"Anthology of the Patriarchal Hall".
In the "Anthology of the Patriarchal Hall" (祖堂集 "Zǔtángjí") of 952, the elements of the traditional Bodhidharma story are in place. Bodhidharma is said to have been a disciple of Prajñātāra, thus establishing the latter as the 27th patriarch in India. After a three-year journey, Bodhidharma reached China in 527, during the Liang (as opposed to the Song in Dàoxuān's text). The "Anthology of the Patriarchal Hall" includes Bodhidharma's encounter with Emperor Wu of Liang, which was first recorded around 758 in the appendix to a text by Shenhui (神會), a disciple of Huineng.
Finally, as opposed to Daoxuan's figure of "over 150 years," the "Anthology of the Patriarchal Hall" states that Bodhidharma died at the age of 150. He was then buried on Mount Xionger (熊耳山 "Xióngĕr Shān") to the west of Luoyang. However, three years after the burial, in the Pamir Mountains, Sòngyún (宋雲)—an official of one of the later Wei kingdoms—encountered Bodhidharma, who claimed to be returning to India and was carrying a single sandal. Bodhidharma predicted the death of Songyun's ruler, a prediction which was borne out upon the latter's return. Bodhidharma's tomb was then opened, and only a single sandal was found inside.
Insofar as, according to the "Anthology of the Patriarchal Hall", Bodhidharma left the Liang court in 527 and relocated to Mount Song near Luoyang and the Shaolin Monastery, where he "faced a wall for nine years, not speaking for the entire time", his date of death can have been no earlier than 536. Moreover, his encounter with the Wei official indicates a date of death no later than 554, three years before the fall of the Western Wei.
Dàoyuán – Transmission of the Lamp.
Subsequent to the "Anthology of the Patriarchal Hall", the only dated addition to the biography of Bodhidharma is in the "Jingde Records of the Transmission of the Lamp" (景德傳燈錄 "Jĭngdé chuándēng lù", published 1004 CE), by Dàoyuán (道原), in which it is stated that Bodhidharma's original name had been Bodhitāra but was changed by his master Prajñātāra. The same account is given by the Japanese master Keizan's 13th century work of the same title.
Popular traditions.
Several contemporary popular traditions also exist regarding Bodhidharma's origins. An Indian tradition regards Bodhidharma to be the third son of a Pallava king from Kanchipuram. This is consistent with the Southeast Asian traditions which also describe Bodhidharma as a former South Indian prince who had awakened his kundalini and renounced royal life to become a monk. The Tibetan version similarly characterises him as a dark-skinned siddha from South India. Conversely, the Japanese tradition generally regards Bodhidharma as Persian.
Legends about Bodhidharma.
Several stories about Bodhidharma have become popular legends, which are still being used in the Ch'an, Seon and Zen-tradition.
Encounter with Emperor Xiāo Yǎn 蕭衍.
The "Anthology of the Patriarchal Hall" says that in 527, Bodhidharma visited Emperor Wu of Liang (Xiāo Yǎn 蕭衍, posthumous name Wǔdì 武帝), a fervent patron of Buddhism:
This encounter was included as the first kōan of the "Blue Cliff Record".
Nine years of wall-gazing.
Failing to make a favorable impression in South China, Bodhidharma is said to have travelled to the Shaolin Monastery. After either being refused entry or being ejected after a short time, he lived in a nearby cave, where he "faced a wall for nine years, not speaking for the entire time".
The biographical tradition is littered with apocryphal tales about Bodhidharma's life and circumstances. In one version of the story, he is said to have fallen asleep seven years into his nine years of wall-gazing. Becoming angry with himself, he cut off his eyelids to prevent it from happening again. According to the legend, as his eyelids hit the floor the first tea plants sprang up, and thereafter tea would provide a stimulant to help keep students of Chan awake during zazen.
The most popular account relates that Bodhidharma was admitted into the Shaolin temple after nine years in the cave and taught there for some time. However, other versions report that he "passed away, seated upright"; or that he disappeared, leaving behind the "Yijin Jing"; or that his legs atrophied after nine years of sitting, which is why Daruma dolls have no legs.
Huike cuts off his arm.
In one legend, Bodhidharma refused to resume teaching until his would-be student, Dazu Huike, who had kept vigil for weeks in the deep snow outside of the monastery, cut off his own left arm to demonstrate sincerity.
Transmission.
Skin, flesh, bone, marrow.
"Jǐngdé Records of the Transmission of the Lamp" (Jǐngdé chuándēng lù 景德传灯录) of Dàoyuán 道原, presented to the emperor in 1004, records that Bodhidharma wished to return to India and called together his disciples:
Bodhidharma passed on the symbolic robe and bowl of dharma succession to Dazu Huike and, some texts claim, a copy of the "Laṅkāvatāra Sūtra". Bodhidharma then either returned to India or died.
Bodhidharma at Shaolin.
Some Chinese accounts describe Bodhidharma as being disturbed by the poor physical shape of the Shaolin monks, after which he instructed them in techniques to maintain their physical condition as well as teaching meditation. He is said to have taught a series of external exercises called the Eighteen Arhat Hands and an internal practice called the Sinew Metamorphosis Classic. In addition, after his departure from the temple, two manuscripts by Bodhidharma were said to be discovered inside the temple: the "Yijin Jing" and the "Xisui Jing". Copies and translations of the "Yijin Jing" survive to the modern day. The "Xisui Jing" has been lost.
Travels in Southeast Asia.
According to Southeast Asian folklore, Bodhidharma travelled from Jambudvipa by sea to Palembang, Indonesia. Passing through Sumatra, Java, Bali, and Malaysia, he eventually entered China through Nanyue. In his travels through the region, Bodhidharma is said to have transmitted his knowledge of the Mahayana doctrine and the martial arts. Malay legend holds that he introduced forms to silat.
Vajrayana tradition links Bodhidharma with the 11th-century Dravidian monk Dampa Sangye who travelled extensively to Tibet and China spreading tantric teachings.
Appearance after his death.
Three years after Bodhidharma's death, Ambassador Sòngyún of northern Wei is said to have seen him walking while holding a shoe at the Pamir Heights. Sòngyún asked Bodhidharma where he was going, to which Bodhidharma replied "I am going home". When asked why he was holding his shoe, Bodhidharma answered "You will know when you reach Shaolin monastery. Don't mention that you saw me or you will meet with disaster". After arriving at the palace, Sòngyún told the emperor that he met Bodhidharma on the way. The emperor said Bodhidharma was already dead and buried and had Sòngyún arrested for lying. At Shaolin Monastery, the monks informed them that Bodhidharma was dead and had been buried in a hill behind the temple. The grave was exhumed and was found to contain a single shoe. The monks then said "Master has gone back home" and prostrated three times: "For nine years he had remained and nobody knew him; Carrying a shoe in hand he went home quietly, without ceremony."
Practice and teaching.
Bodhidharma is traditionally seen as introducing dhyana-practice in China.
Pointing directly to one's mind.
One of the fundamental Chán texts attributed to Bodhidharma is a four-line stanza whose first two verses echo the "Laṅkāvatāra Sūtra"s disdain for words and whose second two verses stress the importance of the insight into reality achieved through "self-realization":
The stanza, in fact, is not Bodhidharma's, but rather dates to the year 1108.
Wall-gazing.
Tanlin, in the preface to "Two Entrances and Four Acts", and Daoxuan, in the "Further Biographies of Eminent Monks", mention a practice of Bodhidharma's termed "wall-gazing" (壁觀 "bìguān"). Both Tanlin and Daoxuan associate this "wall-gazing" with "quieting h mind" ().
In the "Two Entrances and Four Acts", traditionally attributed to Bodhidharma, the term "wall-gazing" is given as follows: Daoxuan states, "The merits of Mahāyāna wall-gazing are the highest".
These are the first mentions in the historical record of what may be a type of meditation being ascribed to Bodhidharma.
Exactly what sort of practice Bodhidharma's "wall-gazing" was remains uncertain. Nearly all accounts have treated it either as an undefined variety of meditation, as Daoxuan and Dumoulin, or as a variety of seated meditation akin to the zazen () that later became a defining characteristic of Chan. The latter interpretation is particularly common among those working from a Chan standpoint.
There have also, however, been interpretations of "wall-gazing" as a non-meditative phenomenon.
The Laṅkāvatāra Sūtra.
There are early texts which explicitly associate Bodhidharma with the "Laṅkāvatāra Sūtra". Daoxuan, for example, in a late recension of his biography of Bodhidharma's successor Huike, has the sūtra as a basic and important element of the teachings passed down by Bodhidharma:
Another early text, the "Record of the Masters and Disciples of the Laṅkāvatāra Sūtra" () of Jìngjué (淨覺; 683–750), also mentions Bodhidharma in relation to this text. Jingjue's account also makes explicit mention of "sitting meditation" or zazen:
In other early texts, the school that would later become known as Chan Buddhism is sometimes referred to as the "Laṅkāvatāra school" (楞伽宗 "Léngqié zōng").
The "Laṅkāvatāra Sūtra", one of the Mahayana sutras, is a highly "difficult and obscure" text whose basic thrust is to emphasize "the inner enlightenment that does away with all duality and is raised above all distinctions". It is among the first and most important texts for East Asian Yogācāra.
One of the recurrent emphases in the "Laṅkāvatāra Sūtra" is a lack of reliance on words to effectively express reality:
In contrast to the ineffectiveness of words, the sūtra instead stresses the importance of the "self-realization" that is "attained by noble wisdom" and occurs "when one has an insight into reality as it is": "The truth is the state of self-realization and is beyond categories of discrimination". The sūtra goes on to outline the ultimate effects of an experience of self-realization:
Lineage.
Construction of lineages.
The idea of a patriarchal lineage in Ch'an dates back to the epitaph for Fărú (法如 638–689), a disciple of the 5th patriarch Hóngrĕn (弘忍 601–674). In the "Long Scroll of the Treatise on the Two Entrances and Four Practices" and the "Continued Biographies of Eminent Monks", Daoyu and Dazu Huike are the only explicitly identified disciples of Bodhidharma. The epitaph gives a line of descent identifying Bodhidharma as the first patriarch.
In the 6th century biographies of famous monks were collected. From this genre the typical Chan lineage was developed:
D. T. Suzuki contends that Chan's growth in popularity during the 7th and 8th centuries attracted criticism that it had "no authorized records of its direct transmission from the founder of Buddhism" and that Chan historians made Bodhidharma the 28th patriarch of Buddhism in response to such attacks.
Six patriarchs.
The earliest lineages described the lineage from Bodhidharma into the 5th to 7th generation of patriarchs. Various records of different authors are known, which give a variation of transmission lines:
Continuous lineage from Gautama Buddha.
Eventually these descriptions of the lineage evolved into a continuous lineage from Śākyamuni Buddha to Bodhidharma. The idea of a line of descent from Śākyamuni Buddha is the basis for the distinctive lineage tradition of Chan Buddhism.
According to the "Song of Enlightenment" (證道歌 "Zhèngdào gē") by Yǒngjiā Xuánjué (665-713), one of the chief disciples of Huìnéng, was Bodhidharma, the 28th Patriarch of Buddhism in a line of descent from Gautama Buddha via his disciple Mahākāśyapa:
The "Transmission of the Light" gives 28 patriarchs in this transmission:
Modern scholarship.
Bodhidharma has been the subject of critical scientific research, which has shed new light on the traditional stories about Bodhidharma.
Biography as a hagiographic process.
According to John McRae, Bodhidharma has been the subject of a hagiographic process which served the needs of Chan Buddhism. According to him it is not possible to write an accurate biography of Bodhidharma:
McRae's standpoint accords with Yanagida's standpoint: "Yanagida ascribes great historical value to the witness of the disciple T'an-lin, but at the same time acknowledges the presence of "many puzzles in the biography of Bodhidharma". Given the present state of the sources, he considers it impossible to compile a reliable account of Bodhidharma's life.
Several scholars have suggested that the composed image of Bodhidharma depended on the combination of supposed historical information on various historical figures over several centuries. Bodhidharma as a historical person may even never have actually existed.
Origins and place of birth.
Dumoulin comments on the three principal sources. The Persian heritage is doubtful, according to Dumoulin: "In the description of the Lo-yang temple, bodhidharma is called a Persian. Given the ambiguity of geographical references in writings of this period, such a statement should not be taken too seriously." Dumoulin considers Tan-lin's account of Bodhidharma being "the third son of a great Brahman king" to be a later addition, and finds the exact meaning of "South Indian Brahman stock" unclear: "And when Tao-hsuan speaks of origins from South Indian Brahman stock, it is not clear whether he is referring to roots in nobility or to India in general as the land of the Brahmans."
These Chinese sources lend themselves to make inferences about Bodhidharma's origins. "The third son of a Brahman king" has been speculated to mean "the third son of a Pallavine king". Based on a specific pronunciation of the Chinese characters 香至 as Kang-zhi, "meaning fragrance extreme", Tsutomu Kambe identifies 香至 to be Kanchipuram, an old capital town in the state Tamil Nadu. According to Tsutomu Kambe, "Kanchi means 'a radiant jewel' or 'a luxury belt with jewels', and puram means a town or a state in the sense of earlier times. Thus, it is understood that the '香至-Kingdom' corresponds to the old capital 'Kanchipuram'."
The Pakistani scholar Ahmad Hasan Dani speculated that according to popular accounts in Pakistan's northwest, Bodhidharma may be from the region around the Peshawar valley, or possibly around modern Afghanistan's eastern border with Pakistan.
Caste.
In the context of the Indian caste system the mention of "Brahman king" acquires a nuance. Broughton notes that "king" implies that Bodhidharma was of a member of the thondaiman caste, an shatriya caste of warriors and rulers. Brahman is, in western contexts, easily understood as Brahmana or Brahmin, which means "priest".
Name.
According to tradition Bodhidharma was given this name by his teacher known variously as Panyatara, Prajnatara, or Prajñādhara. His name prior to monkhood is said to be Jayavarman.
Bodhidharma is associated with several other names, and is also known by the name Bodhitara. Faure notes that:
Tibetan sources give his name as "Bodhidharmottāra" or "Dharmottara", that is, "Highest teaching (dharma) of enlightenment".
Abode in China.
Buswell dates Bodhidharma abode in China approximately at the early 5th century. Broughton dates Bodhidharma's presence in Luoyang to between 516 and 526, when the temple referred to—Yǒngníngsì (永寧寺), was at the height of its glory. Starting in 526, Yǒngníngsì suffered damage from a series of events, ultimately leading to its destruction in 534.
Shaolin boxing.
Traditionally Bodhidharma is credited as founder of the martial arts at the Shaolin Temple. However, martial arts historians have shown this legend stems from a 17th-century qigong manual known as the "Yijin Jing".
The authenticity of the "Yi Jin Jing" has been discredited by some historians including Tang Hao, Xu Zhen and Matsuda Ryuchi. This argument is summarized by modern historian Lin Boyuan in his "Zhongguo wushu shi":
The oldest available copy was published in 1827. The composition of the text itself has been dated to 1624. Even then, the association of Bodhidharma with martial arts only became widespread as a result of the 1904–1907 serialization of the novel "The Travels of Lao Ts'an" in "Illustrated Fiction Magazine":

</doc>
<doc id="4286" url="https://en.wikipedia.org/wiki?curid=4286" title="Biconditional introduction">
Biconditional introduction

In propositional logic, biconditional introduction is a valid rule of inference. It allows for one to infer a biconditional from two conditional statements. The rule makes it possible to introduce a biconditional statement into a logical proof. If formula_1 is true, and if formula_2 is true, then one may infer that formula_3 is true. For example, from the statements "if I'm breathing, then I'm alive" and "if I'm alive, then I'm breathing", it can be inferred that "I'm breathing if and only if I'm alive". Biconditional introduction is the converse of biconditional elimination. The rule can be stated formally as:
where the rule is that wherever instances of "formula_1" and "formula_2" appear on lines of a proof, "formula_3" can validly be placed on a subsequent line.
Formal notation.
The "biconditional introduction" rule may be written in sequent notation:
where formula_9 is a metalogical symbol meaning that formula_3 is a syntactic consequence when formula_1 and formula_2 are both in a proof;
or as the statement of a truth-functional tautology or theorem of propositional logic:
where formula_14, and formula_15 are propositions expressed in some formal system.

</doc>
<doc id="4287" url="https://en.wikipedia.org/wiki?curid=4287" title="Biconditional elimination">
Biconditional elimination

Biconditional elimination is the name of two valid rules of inference of propositional logic. It allows for one to infer a conditional from a biconditional. If formula_1 is true, then one may infer that formula_2 is true, and also that formula_3 is true. For example, if it's true that I'm breathing if and only if I'm alive, then it's true that if I'm breathing, I'm alive; likewise, it's true that if I'm alive, I'm breathing. The rules can be stated formally as:
and
where the rule is that wherever an instance of "formula_1" appears on a line of a proof, either "formula_2" or "formula_3" can be placed on a subsequent line;
Formal notation.
The "biconditional elimination" rule may be written in sequent notation:
and
where formula_11 is a metalogical symbol meaning that formula_2, in the first case, and formula_3 in the other are syntactic consequences of formula_1 in some logical system;
or as the statement of a truth-functional tautology or theorem of propositional logic:
where formula_17, and formula_18 are propositions expressed in some formal system.

</doc>
<doc id="4292" url="https://en.wikipedia.org/wiki?curid=4292" title="Base pair">
Base pair

A base pair (bp) is a unit consisting of two nucleobases bound to each other by hydrogen bonds. They form the building blocks of the DNA double helix, and contribute to the folded structure of both DNA and RNA. Dictated by specific hydrogen bonding patterns, Watson-Crick base pairs (guanine-cytosine and adenine-thymine) allow the DNA helix to maintain a regular helical structure that is subtly dependent on its nucleotide sequence. The complementary nature of this based-paired structure provides a backup copy of all genetic information encoded within double-stranded DNA. The regular structure and data redundancy provided by the DNA double helix make DNA well suited to the storage of genetic information, while base-pairing between DNA and incoming nucleotides provides the mechanism through which DNA polymerase replicates DNA, and RNA polymerase transcribes DNA into RNA. Many DNA-binding proteins can recognize specific base pairing patterns that identify particular regulatory regions of genes.
Intramolecular base pairs can occur within single-stranded nucleic acids. This is particularly important in RNA molecules (e.g., transfer RNA), where Watson-Crick base pairs (G-C and A-U) permit the formation of short double-stranded helices, and a wide variety of non-Watson-Crick interactions (e.g., G-U or A-A) allow RNAs to fold into a vast range of specific three-dimensional structures. In addition, base-pairing between transfer RNA (tRNA) and messenger RNA (mRNA) forms the basis for the molecular recognition events that result in the nucleotide sequence of mRNA becoming translated into the amino acid sequence of proteins.
The size of an individual gene or an organism's entire genome is often measured in base pairs because DNA is usually double-stranded. Hence, the number of total base pairs is equal to the number of nucleotides in one of the strands (with the exception of non-coding single-stranded regions of telomeres). The haploid human genome (23 chromosomes) is estimated to be about 3.2 billion bases long and to contain 20,000–25,000 distinct protein-coding genes. A kilobase (kb) is a unit of measurement in molecular biology equal to 1000 base pairs of DNA or RNA. The total amount of related DNA base pairs on Earth is estimated at 5.0 x 10, and weighs 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon).
Hydrogen bonding and stability.
Hydrogen bonding is the chemical interaction that underlies the base-pairing rules described above. Appropriate geometrical correspondence of hydrogen bond donors and acceptors allows only the "right" pairs to form stably. DNA with high GC-content is more stable than DNA with low GC-content, but, contrary to popular belief, the hydrogen bonds do not stabilize the DNA significantly, and stabilization is mainly due to stacking interactions.
The larger nucleobases, adenine and guanine, are members of a class of double-ringed chemical structures called purines; the smaller nucleobases, cytosine and thymine (and uracil), are members of a class of single-ringed chemical structures called pyrimidines. Purines are complementary only with pyrimidines: pyrimidine-pyrimidine pairings are energetically unfavorable because the molecules are too far apart for hydrogen bonding to be established; purine-purine pairings are energetically unfavorable because the molecules are too close, leading to overlap repulsion. Purine-pyrimidine base pairing of AT or GC or UA (in RNA) results in proper duplex structure. The only other purine-pyrimidine pairings would be AC and GT and UG (in RNA); these pairings are mismatches because the patterns of hydrogen donors and acceptors do not correspond. The GU pairing, with two hydrogen bonds, does occur fairly often in RNA (see wobble base pair).
Paired DNA and RNA molecules are comparatively stable at room temperature but the two nucleotide strands will separate above a melting point that is determined by the length of the molecules, the extent of mispairing (if any), and the GC content. Higher GC content results in higher melting temperatures; it is, therefore, unsurprising that the genomes of extremophile organisms such as "Thermus thermophilus" are particularly GC-rich. On the converse, regions of a genome that need to separate frequently — for example, the promoter regions for often-transcribed genes — are comparatively GC-poor (for example, see TATA box). GC content and melting temperature must also be taken into account when designing primers for PCR reactions.
Examples.
The following DNA sequences illustrate pair double-stranded patterns. By convention, the top strand is written from the 5' end to the 3' end; thus, the bottom strand is written 3' to 5'.
Base analogs and intercalators.
Chemical analogs of nucleotides can take the place of proper nucleotides and establish non-canonical base-pairing, leading to errors (mostly point mutations) in DNA replication and DNA transcription. This is due to their isosteric chemistry. One common mutagenic base analog is 5-bromouracil, which resembles thymine but can base-pair to guanine in its enol form.
Other chemicals, known as DNA intercalators, fit into the gap between adjacent bases on a single strand and induce frameshift mutations by "masquerading" as a base, causing the DNA replication machinery to skip or insert additional nucleotides at the intercalated site. Most intercalators are large polyaromatic compounds and are known or suspected carcinogens. Examples include ethidium bromide and acridine.
Unnatural base pair (UBP).
An unnatural base pair (UBP) is a designed subunit (or nucleobase) of DNA which is created in a laboratory and does not occur in nature. DNA sequences have been described which use newly created nucleobases to form a third base pair, in addition to the two base pairs found in nature, A-T (adenine - thymine) and G-C (guanine - cytosine). A few research groups have been searching for a third base pair for DNA, including teams led by Steven A. Benner, Philippe Marliere, Floyd Romesberg and Ichiro Hirao. Some new base pairs have been reported.
In 1989 Steven Benner, then at the Swiss Federal Institute of Technology in Zurich, and his team led with modified forms of cytosine and guanine into DNA molecules "in vitro". The nucleotides, which encoded RNA and proteins, were successfully replicated "in vitro". Since then, Benner's team has been trying to engineer cells that can make foreign bases from scratch, obviating the need for a feedstock.
In 2002, Ichiro Hirao’s group in Japan developed an unnatural base pair between 2-amino-8-(2-thienyl)purine (s) and pyridine-2-one (y) that functions in transcription and translation, for the site-specific incorporation of non-standard amino acids into proteins. In 2006, they created 7-(2-thienyl)imidazo,5-pyridine (Ds) and pyrrole-2-carbaldehyde (Pa) as a third base pair for replication and transcription. Afterward, Ds and 4--(6-aminohexanamido)-1-propyny-2-nitropyrrole (Px) was discovered as a high fidelity pair in PCR amplification. In 2013, they applied the Ds-Px pair to DNA aptamer generation by "in vitro" selection (SELEX) and demonstrated the genetic alphabet expansion significantly augment DNA aptamer affinities to target proteins.
In 2012, a group of American scientists led by Floyd Romesberg, a chemical biologist at the Scripps Research Institute in San Diego, California, published that his team designed an unnatural base pair (UBP). The two new artificial nucleotides or "Unnatural Base Pair" (UBP) were named d5SICS and dNaM. More technically, these artificial nucleotides bearing hydrophobic nucleobases, feature two fused aromatic rings that form a (d5SICS–dNaM) complex or base pair in DNA. His team designed a variety of "in vitro" or "test tube" templates containing the unnatural base pair and they confirmed that it was efficiently replicated with high fidelity in virtually all sequence contexts using the modern standard "in vitro" techniques, namely PCR amplification of DNA and PCR-based applications. Their results show that for PCR and PCR-based applications, the d5SICS–dNaM unnatural base pair is functionally equivalent to a natural base pair, and when combined with the other two natural base pairs used by all organisms, A–T and G–C, they provide a fully functional and expanded six-letter "genetic alphabet".
In 2014 the same team from the Scripps Research Institute reported that they synthesized a stretch of circular DNA known as a plasmid containing natural T-A and C-G base pairs along with the best-performing UBP Romesberg's laboratory had designed, and inserted it into cells of the common bacterium "E. coli" that successfully replicated the unnatural base pairs through multiple generations. The transfection did not hamper the growth of the "E. coli" cells, and showed no sign of losing its unnatural base pairs to its natural DNA repair mechanisms. This is the first known example of a living organism passing along an expanded genetic code to subsequent generations. Romesberg said he and his colleagues created 300 variants to refine the design of nucleotides that would be stable enough and would be replicated as easily as the natural ones when the cells divide. This was in part achieved by the addition of a supportive algal gene that expresses a nucleotide triphosphate transporter which efficiently imports the triphosphates of both d5SICSTP and dNaMTP into "E. coli" bacteria. Then, the natural bacterial replication pathways use them to accurately replicate a plasmid containing d5SICS–dNaM. Other researchers were surprised that the bacteria replicated these human-made DNA subunits.
The successful incorporation of a third base pair is a significant breakthrough toward the goal of greatly expanding the number of amino acids which can be encoded by DNA, from the existing 20 amino acids to a theoretically possible 172, thereby expanding the potential for living organisms to produce novel proteins. The artificial strings of DNA do not encode for anything yet, but scientists speculate they could be designed to manufacture new proteins which could have industrial or pharmaceutical uses. Experts said the synthetic DNA incorporating the unnatural base pair raises the possibility of life forms based on a different DNA code.
Length measurements.
The following abbreviations are commonly used to describe the length of a D/RNA molecule:
For case of single-stranded DNA/RNA units of nucleotides are used, abbreviated nt (or knt, Mnt, Gnt), as they are not paired.
For distinction between units of computer storage and bases kbp, Mbp, Gbp, etc. may be used for base pairs.
The centimorgan is also often used to imply distance along a chromosome, but the number of base pairs it corresponds to varies widely. In the Human genome, the centimorgan is about 1 million base pairs.

</doc>
<doc id="4293" url="https://en.wikipedia.org/wiki?curid=4293" title="Baltimore Ravens">
Baltimore Ravens

The Baltimore Ravens are a professional American football team based in Baltimore, Maryland. The Ravens are members of the North division of the American Football Conference (AFC) in the National Football League (NFL). The team plays its home games at M&T Bank Stadium and is headquartered in Owings Mills.
The Ravens were established in 1996, when Art Modell, who was then the owner of the Cleveland Browns, announced plans to relocate the franchise from Cleveland, Ohio to Baltimore. As part of a settlement between the league and the city of Cleveland, Modell was required to leave the Browns' heritage in Cleveland for a replacement team and replacement personnel that would take control in 1999 including the history and records of the Browns. In return, he was allowed to take his own personnel and team to Baltimore, where such personnel would then form an expansion team. The team's name was inspired by Edgar Allan Poe's poem "The Raven".
The Ravens have qualified for the NFL playoffs ten times since 2000, with two Super Bowl victories (Super Bowl XXXV and Super Bowl XLVII), two AFC Championship titles (2000 and 2012), four AFC North division titles (2003, 2006, 2011 and 2012), and are currently the only team in the NFL to hold a perfect record in multiple Super Bowl appearances. The Ravens organization has been led by general manager Ozzie Newsome since 1996, and has had three head coaches: Ted Marchibroda, Brian Billick, and John Harbaugh. With a record-breaking defensive unit in their 2000 season, the team established a reputation for relying on strong defensive play, led by players like middle linebacker Ray Lewis, who, until his retirement, was considered the "face of the franchise." The team is owned by Steve Bisciotti and valued at $1.5 billion, making the Ravens the 24th-most valuable sports franchise in the world.
History.
Background.
After the controversial relocation of the Colts to Indianapolis, several attempts were made to bring an NFL team back to Baltimore. In 1993, ahead of the 1995 league expansion, the city was considered a favorite, behind only St. Louis, to be granted one of two new franchises. League officials and team owners feared litigation due to conflicts between rival bidding groups if St. Louis was awarded a franchise, and in October Charlotte, North Carolina was the first city chosen. Several weeks later, Baltimore's bid for a franchise—dubbed the Baltimore Bombers, in honor of the locally produced Martin B-26 Marauder bomber—had three ownership groups in place and a state financial package which included a proposed $200 million, rent-free stadium and permission to charge up to $80 million in personal seat license fees. Baltimore, however, was unexpectedly passed over in favor of Jacksonville, Florida, despite Jacksonville's minor TV market status and that the city had withdrawn from contention in the summer, only to return with then-Commissioner Paul Tagliabue's urging. Although league officials denied that any city had been favored, it was reported that Taglibue and his longtime friend Washington Redskins owner Jack Kent Cooke had lobbied against Baltimore due to its proximity to Washington, D.C., and that Taglibue had used the initial committee voting system to prevent the entire league ownership from voting on Baltimore's bid. This led to public outrage and the "Baltimore Sun" describing Taglibue as having an "Anybody But Baltimore" policy. Maryland governor William Donald Schaefer said afterward that Taglibue had led him on, praising Baltimore and the proposed owners while working behind-the-scenes to oppose Baltimore's bid.
By May 1994, Baltimore Orioles owner Peter Angelos had gathered a new group of investors, including author Tom Clancy, to bid on teams whose owners had expressed interest in relocating. Angelos found a potential partner in Georgia Frontiere, who was open to moving the Los Angeles Rams to Baltimore. Jack Kent Cooke opposed the move, intending to build the Redskins' new stadium in Laurel, Maryland, close enough to Baltimore to cool outside interest in bringing in a new franchise. This led to heated arguments between Cooke and Angelos, who accused Cooke of being a "carpetbagger." The league eventually persuaded Rams team president John Shaw to relocate to St. Louis instead, leading to a league-wide rumor that Tagliabue was again steering interest away from Baltimore, a claim which Tagliabue denied. In response to anger in Baltimore, including Governor Schaefer's threat to announce over the loudspeakers Tagliabue's exact location in Camden Yards any time he attended a Baltimore Orioles game, Tagliabue remarked of Baltimore's financial package: "Maybe (Baltimore) can open another museum with that money." Following this, Angelos made an unsuccessful $200 million bid to bring the Tampa Bay Buccaneers to Baltimore.
Having failed to obtain a franchise via the expansion, the city, despite having "misgivings," turned to the possibility of obtaining the Cleveland Browns, whose owner Art Modell was financially struggling and at odds with the city of Cleveland over needed improvements to the team's stadium.
Relocation from Cleveland, Ohio.
Enticed by Baltimore's available funds for a first-class stadium, Modell announced on November 6, 1995 his intention to relocate the team from Cleveland to Baltimore the following year. The resulting controversy ended when representatives of Cleveland and the NFL reached a settlement on February 8, 1996. Tagliabue promised the city of Cleveland that an NFL team would be located in Cleveland, either through relocation or expansion, "no later than 1999". Additionally, the agreement stipulated that the Browns' name, colors, uniform design and franchise records would remain in Cleveland. The franchise history includes Browns club records and connections with Pro Football Hall of Fame players. Modell's Baltimore team, while retaining all current player contracts, would, for purposes of team history, appear as an expansion team, a "new franchise." Not all players, staff or front office would make the move to Baltimore, however.
After relocation, Modell hired Ted Marchibroda as the head coach for his new team in Baltimore. Marchibroda was already well known because of his work as head coach of the Baltimore Colts during the 1970s and the Indianapolis Colts during the early 1990s. Ozzie Newsome, the Browns' tight end for many seasons, joined Modell in Baltimore as director of football operations. He was later promoted to vice-president/general manager. A fan contest, drawing 33,288 voters, was then held to determine the team's name. The chosen name, "Ravens," alludes to the famous poem "The Raven" by Edgar Allan Poe, who spent the early part of his career in Baltimore.
The home stadium for the Ravens first two seasons was Baltimore's Memorial Stadium, home field of the Baltimore Colts and Baltimore Stallions years before. The Ravens moved to their own new stadium next to Camden Yards in 1998. Raven Stadium would subsequently wear the names PSI Net Stadium and then M&T Bank Stadium.
The Early Years and Ted Marchibroda Era (1996–1998).
1996.
In the 1996 NFL Draft, the Ravens, with two picks in the first round, drafted offensive tackle Jonathan Ogden at No. 4 overall and linebacker Ray Lewis at No. 26 overall.
The 1996 Ravens won their opening game against the Oakland Raiders, but finished the season 4–12 despite receiver Michael Jackson leading the league with 14 touchdown catches.
1997.
The 1997 Ravens started 3–1. Peter Boulware, a rookie defender from Florida State, recorded 11.5 sacks and was named AFC Defensive Rookie of the Year. The team finished 6–9–1. On October 26, the team made its first trip to Landover, Maryland to play their new regional rivals, the Washington Redskins, for the first time in the regular season, at the new Jack Kent Cooke Stadium (replacing the still-standing RFK Stadium in Washington, DC). The Ravens won the game 20–17.
1998.
Quarterback Vinny Testaverde left for the New York Jets before the 1998 season, and was replaced by former Indianapolis Colt Jim Harbaugh, and later Eric Zeier. Cornerback Rod Woodson joined the team after a successful stint with the Pittsburgh Steelers, and Priest Holmes started getting the first playing time of his career and ran for 1,000 yards.
The Ravens finished 1998 with a 6–10 record. On November 29, the Ravens welcomed the Colts back to Baltimore for the first time in 15 years. Amidst a shower of negative cheers towards the Colts, the Ravens, with Jim Harbaugh at quarterback, won 38–31.
Brian Billick Era and first Super Bowl victory (1999–2007).
1999.
Three consecutive losing seasons under Marchibroda led to a change in the head coach. Brian Billick took over as head coach in 1999. Billick had been offensive coordinator for the record-setting Minnesota Vikings the season before. Quarterback Tony Banks came to Baltimore from the St. Louis Rams and had the best season of his career with 17 touchdown passes and an 81.2 pass rating. He was joined by receiver Qadry Ismail, who posted a 1,000-yard season. The Ravens initially struggled with a record of 4–7 but managed to finish with an 8–8 record.
Due to continual financial hardships, the NFL directed Modell to initiate the sale of his franchise. On March 27, 2000, NFL owners approved the sale of 49% of the Ravens to Steve Bisciotti. In the deal, Bisciotti had an option to purchase the remaining 51% for $325 million in 2004 from Art Modell. On April 9, 2004 the NFL approved Steve Bisciotti's purchase of the majority stake in the club.
2000: Super Bowl XXXV champions.
The 2000 season saw the Ravens defense, led by defensive coordinator Marvin Lewis, develop into a rock-solid unit that emerged as one of the most formidable defenses in NFL history. The Ravens defense set a new NFL record in holding opposing teams to 165 total points; the feat eclipsed the mark set previously by the Chicago Bears of 187 points for a 16-game season. Linebacker Ray Lewis was named Defensive Player of the year and, with two of his defensive teammates, Sam Adams and Rod Woodson, made the Pro Bowl.
Baltimore's season started strong with a 3–1 record. Tony Banks began the 2000 season as the starting quarterback and was replaced by Trent Dilfer when the Ravens fell to 5–4, and failed to score an offensive touchdown the entire month of October. Coach Brian Billick announced that the change at quarterback would be for the rest of the season. The thousand yard rushing season by rookie running back Jamal Lewis combined with the stout Ravens defense kept Baltimore competitive in games even when the offense struggled. At one point in the season the team played five consecutive games without scoring an offensive touchdown but still managed two wins during that stretch. The team regrouped and won each of their last seven games, finishing 12–4. The Ravens had made the playoffs for the first time.
Since the divisional rival Tennessee Titans had a record of 13–3, Baltimore had to play in the wild card round. In their first ever playoff game, they dominated the Denver Broncos 21–3 in front of a then record-crowd of 69,638 at then called PSINet Stadium. In the divisional playoffs, they went on the road to Tennessee. Tied 10–10 in the fourth quarter, an Al Del Greco field goal attempt was blocked and returned for a touchdown by Anthony Mitchell, and a Ray Lewis interception return for a score put the game squarely in Baltimore's favor. The 24–10 win put the Ravens in the AFC Championship against the Oakland Raiders. Shannon Sharpe's 96-yard touchdown catch early in the second quarter, followed by an injury to Raiders quarterback Rich Gannon, highlighted the Ravens' 16–3 victory.
Baltimore then went to Tampa for Super Bowl XXXV where they met the New York Giants, cruising to a 34–7 win for their first championship in franchise history. The Ravens recorded four sacks, forced five turnovers, one of which was a Kerry Collins interception returned for a touchdown by Duane Starks. The Giants' only score was a Ron Dixon kickoff return for another touchdown (after Starks' interception return), making the 2000 Ravens the third Super Bowl team whose defense did not allow an opponent's offensive score; however, Baltimore immediately countered with a TD return by Jermaine Lewis. The Ravens became only the third wild-card team to win a Super Bowl championship. The interception return for a touchdown, followed by two kick return TDs, marked the quickest time in Super Bowl history that three touchdowns had been scored.
The title made the Ravens the fourth Baltimore-based pro football team to win a league championship. They were preceded by the NFL Baltimore Colts in 1958, 1959 and 1970, the USFL Stars in 1985 and the CFL Stallions in 1995.
2001.
In 2001, the Ravens attempted to defend its title with Elvis Grbac as its new starting quarterback, but a season-ending injury to Jamal Lewis on the first day of training camp and poor offensive performances stymied the team. After a 3–3 start, the Ravens defeated the Minnesota Vikings in the final week to clinch a wild card berth at 10–6. In the first round the Ravens showed flashes of their previous year with a 20–3 win over the Miami Dolphins, in which the team forced three turnovers and out-gained the Dolphins 347 yards to 151. In the divisional playoff the Ravens played the Pittsburgh Steelers. Three interceptions by Grbac ended the Ravens' season, as they lost 27–10.
2002.
Baltimore ran into salary cap problems entering the 2002 season and was forced to part with a number of impact players. In the NFL Draft, the team selected Ed Reed with the 24th overall pick. Reed would go on to become one of the best safeties in NFL history, making nine Pro Bowls until leaving the Ravens for the Houston Texans in 2013. Despite low expectations, the Ravens stayed somewhat competitive in 2002 until a losing streak in December eliminated any chances of a post-season berth. Their final record that year was 7–9.
2003.
The Ravens needed a quarterback but drafting after all the top quarterbacks were gone, used their 10th pick to select Arizona State defensive end Terrell Suggs. They then traded their 2003 2nd round pick and 2004 1st round pick to the New England Patriots for the 19th overall selection which they used to draft Cal quarterback Kyle Boller. The Patriots eventually used the Ravens' 2004 1st round selection to take defensive tackle Vince Wilfork.
The Ravens named Boller their starting QB just prior to the start of the 2003 season, but he was injured midway through the season and was replaced by Anthony Wright. The Ravens held a 5–5 record until, in a home game against the Seattle Seahawks, they wiped out a 41–24 gap in the final seven minutes of regulation, then won on a Matt Stover field goal in overtime for a 44–41 triumph. From there the Ravens won five of their last six games. With a 10–6 record, Baltimore won their first AFC North division title. Running back Jamal Lewis ran for 2,066 yards on the season, including a then NFL single-game record of 295 yards at home against the Cleveland Browns on September 14. In doing so, Lewis became only the fifth player to eclipse the 2,000-yard rushing mark in league history, with his single-season total placing second all-time; just 39 yards short of the NFL record held by Eric Dickerson. Their first playoff game, at home against the Tennessee Titans, went back and forth, with the Ravens being held to only 100 yards total rushing. The Ravens lost, though, by three, 20–17.
For his remarkable season, Jamal Lewis was named as the NFL Offensive Player of the Year, while linebacker Ray Lewis, with another stand-out year that included 6 interceptions, was also recognized as Defensive Player of the Year for the second time in his career. This marked the first time ever that teammates received these respective individual honors in the same season.
After the season, Art Modell officially transferred the remaining bulk of his ownership to Bisciotti, ending over 40 years of tenure as an NFL franchise majority owner. Modell still held an office at the Ravens' headquarters in Owings Mills, Maryland, and remained with the team as a consultant.
2004.
The Ravens did not make the playoffs in 2004 and finished the season with a record of 9–7 with Kyle Boller spending the season at QB. They did get good play from veteran corner Deion Sanders and third year safety Ed Reed, who won the NFL Defensive Player of the Year award. They were also the only team to defeat the 15–1 Pittsburgh Steelers in the regular season.
2005.
In the 2005 offseason the Ravens looked to augment their receiving corps (which was second-worst in the NFL in 2004) by signing Derrick Mason from the Titans and drafting star Oklahoma wide receiver Mark Clayton in the first round of the 2005 NFL Draft. However, the Ravens ended their season 6–10, but defeated the Green Bay Packers 48–3 on Monday Night Football and the Super Bowl champion Steelers.
2006.
The 2006 Baltimore Ravens season began with the team trying to improve on their 6–10 record of 2005. The Ravens, for the first time in franchise history, started 4–0, under the leadership of former Titans quarterback Steve McNair.
The Ravens lost two straight games mid-season on offensive troubles, prompting coach Billick to drop their offensive coordinator Jim Fassel in their week seven bye. After the bye, and with Billick calling the offense, Baltimore would record a five-game win streak before losing to the Cincinnati Bengals in week 13.
Still ranked second overall to first-place San Diego Chargers, the Ravens continued on. They defeated the Kansas City Chiefs, and held the defending Super Bowl champion Pittsburgh Steelers to only one touchdown at Heinz Field, allowing the Ravens to clinch the AFC North.
The Ravens ended the regular season with a franchise-best 13–3 record. Baltimore had secured the AFC North title, the No. 2 AFC playoff seed, and clinched a 1st-round bye by season's end. The Ravens were slated to face the Indianapolis Colts in the second round of the playoffs, in the first meeting of the two teams in the playoffs. Many Baltimore and Indianapolis fans saw this historic meeting as a sort of "Judgment Day" with the new team of Baltimore facing the old team of Baltimore (the former Baltimore Colts having left Baltimore under questionable circumstances in 1984). Both Indianapolis and Baltimore were held to scoring only field goals as the two defenses slugged it out all over M&T Bank Stadium. McNair threw two costly interceptions, including one at the 1-yard line. The eventual Super Bowl champion Colts won 15–6, ending Baltimore's season.
2007.
After a stellar 2006 season, the Ravens hoped to improve upon their 13–3 record but injuries and poor play plagued the team. The Ravens finished the 2007 season in the AFC North cellar with a disappointing 5–11 record. A humiliating 22–16 overtime loss to the previously winless Miami Dolphins on December 16 ultimately led to Billick's dismissal on New Year's Eve, one day after the end of the regular season. He was replaced by John Harbaugh, the special teams coach of the Philadelphia Eagles and the older brother of former Ravens quarterback Jim Harbaugh (1998).
Harbaugh/Flacco Era; second Super Bowl (2008–present).
2008: Arrival of Harbaugh, Flacco, and Ray Rice.
With rookies at head coach (John Harbaugh) and quarterback (Joe Flacco), the Ravens entered the 2008 campaign with lots of uncertainty. Their Week 2 contest at the Houston Texans was postponed until two months later because of Hurricane Ike, forcing the Ravens to play for what would eventually be eighteen straight weeks. With its record at 2–3 after consecutive losses to the Pittsburgh Steelers, Tennessee Titans and Indianapolis Colts, its triumph over the Dolphins in Week 7 was redemption for what had happened against the same opponent in the previous season. Eight victories in its last ten regular season matches enabled them to clinch the sixth seed in the AFC playoffs at an 11–5 record. Possibly the biggest win during that stretch came in Week 16 with a 33–24 humbling of the Dallas Cowboys in the final game at Texas Stadium. Willis McGahee's 77-yard touchdown run in the fourth quarter established a new stadium record which would last until Le'Ron McClain, on the very first offensive play of the Ravens' next possession, secured the victory with an 82-yarder.
On the strength of four interceptions, one resulting in an Ed Reed touchdown, the Ravens began its postseason run by winning a rematch over Miami 27–9 at Dolphin Stadium on January 4, 2009 in a wild-card game. Six days later, they advanced to the AFC Championship Game by avenging a Week 5 loss to the Titans 13–10 at LP Field on a Matt Stover field goal with 53 seconds left in regulation time. The Ravens fell one victory short of Super Bowl XLIII by losing to the Steelers 23–14 at Heinz Field on January 18, 2009.
2009.
With Jonathan Ogden retiring after the 2007 season and Matt Stover going into free agency, Baltimore's only remaining player from its first season was Ray Lewis. The Ravens held the 26th pick in the 2009 NFL draft but went up to the 23rd pick by trading its 26th pick and a 5th round pick to the New England Patriots. The Ravens selected Michael Oher (who later had a movie named "The Blind Side" made after his life during his early years) in the first round of the NFL Draft.
In the season opener, the offense continued its improvements from the year before as it scored 38 points and accounted for over 500 yards in a 38–24 victory over the Kansas City Chiefs. In week 2, the Ravens defeated the San Diego Chargers 31–26. Although the Ravens secondary struggled, Ray Lewis made a crucial tackle on fourth down of the Chargers' final drive to seal the Baltimore win. In week 3, the Ravens defeated the Cleveland Browns in a 34–3 blowout while celebrating Derrick Mason's 800th pass reception in his career.
In week 4, the Ravens lost in heartbreak fashion to the New England Patriots, 27–21, with their final drive ending with a dropped pass by Mark Clayton on 4th down within the 10-yard line with 28 seconds left on the clock. The following week, the Ravens hosted the Cincinnati Bengals, but lost with the Bengals' final drive resulting in a touchdown. The Ravens then played an away game against the Minnesota Vikings and suffered another heartbreaking loss, 33–31, putting them behind both the Bengals and the Steelers in the AFC North. The Ravens had rallied from 17 points down to the Vikings and managed to drive the ball down the field, but Steve Hauschka missed a 44-yard field goal as time expired on the clock. Joe Flacco made 28 out of 43 passing attempts and threw for a career high 385 yards, and Ray Rice ran for 117 yards. The very next week they hosted the Denver Broncos, who were undefeated (6–0). After Hauschka kicked a pair of field goals in the 1st and 2nd quarters, the Broncos kicked off at the start of the 3rd quarter and the Ravens immediately returned it for a touchdown, giving the Ravens a 13–0 lead. They finished the game victorious, crushing the Broncos 30–7, handing Denver its first loss of the season.
The following week, they looked to avenge the week 5 loss to the Bengals. However, they were out-played on both sides of the ball, suffered a crucial miss by Hauschka, and lost 17–7.
In week 10, the Ravens visited the Cleveland Browns on "Monday Night Football" and shut out their divisional rivals 16–0 despite a slow offensive start. Steve Hauschka missed a field goal and had an extra point blocked, costing the Ravens four points. This led coach John Harbaugh to release Hauschka and replace him with Billy Cundiff.
In week 11, the Ravens played their third undefeated opponent, the Colts, who were (9–0). They lost 17–15, failing to score a single touchdown. Cundiff went 5 for 6 on field goals, scoring the Ravens only points. Joe Flacco threw a late interception and after Ed Reed's fumbled attempt to lateral on a punt return, Peyton Manning kneeled to seal the Colts' seventh consecutive victory against Baltimore. With this loss, the Ravens record stood at 5–5, ranking third in the AFC North.
The Ravens then beat the Steelers, who were playing without quarterback Ben Roethlisberger with an overtime field goal on "Sunday Night Football". The next week, however, the Ravens lost to the Green Bay Packers on ESPN Monday Night Football.
The Ravens then crushed two opponents from the NFC North at home, beating the Detroit Lions 48–3 and the Chicago Bears 31–7. The Ravens improved to 8–6, second in the AFC North, and in line for the fifth seed. They looked ahead to their division rivals, the Steelers, who were coming off a dramatic last-second win against the Packers. A win would give the Ravens a chance to clinch a playoff spot and would knock the Steelers out of contention. But the Ravens, who committed 11 penalties and blew several chances to put additional points on Pittsburgh, lost 23–20. The Ravens still had a shot at the playoffs with a week 17 victory, and made it defeating the Oakland Raiders 21–13.
In the playoffs, they faced the Patriots in the wild card round. The Ravens beat the Patriots 33–14, aided by Ray Rice's 83-yard touchdown run on the first play from scrimmage, helping them to a 24–0 lead at the end of the first quarter. Advancing to the AFC divisional round, they next played Indianapolis. Two touchdowns late in the first half gave the Colts a 17–3 lead at halftime, and Baltimore miscues in the second half ensured the end of their season, by a 20–3 score.
2010.
During the 2009–2010 offseason, the Ravens made some key additions to their offense by acquiring WR Anquan Boldin from the Arizona Cardinals and free agent T. J. Houshmandzadeh, released after the preseason by the Seattle Seahawks. They also added Donté Stallworth, who most last played for the Cleveland Browns, but was suspended for the 2009 season, and signed back-up quarterback Mark Bulger who was released by the St. Louis Rams after the 1–15 2009 season. Stallworth broke his foot in the third preseason game and came back later in the season.
They also drafted tight ends Dennis Pitta, Ed Dickson, defensive tackle Terrence Cody, Arthur Jones, linebacker Sergio Kindle, wide receiver David Reed and offensive tackle Ramon Harewood in the 2010 NFL Draft. On July 25, Sergio Kindle suffered a head trauma after falling down two flights of stairs in a home in Austin, Texas and was lost for the season. The new additions accounted for a combined 37 starts.
The Ravens finished the season at 12–4 but with a marginally worse divisional record (Steelers 5–1 divisionally versus the Ravens' 4–2). They then went on to defeat the Kansas City Chiefs 30–7 in the wild card round of the playoffs, running back Ray Rice becoming the first Raven running back to have a receiving touchdown in a playoff game. The Ravens would then lose to the Steelers 31–24 in the divisional playoffs. Leading at halftime 21–7, the Ravens then turned the ball over three times in the third quarter, in which gave the Steelers 14 points. Baltimore's season ended with a potential touchdown drop by Anquan Boldin and, later, another drop by T. J. Houshmandzadeh on 4th down, surrendering the game 31–24.
2011.
After the 2011 NFL season labor dispute had ended, the Ravens had informed veterans Willis McGahee, Todd Heap, Kelly Gregg, and Derrick Mason that they would be cut in order to free up salary cap space. Following these cuts, the Ravens acquired fullback Vonta Leach, wide receiver Lee Evans, safety Bernard Pollard, and running back Ricky Williams. During the pre season, the Baltimore Ravens signed Left tackle Bryant McKinnie from the Minnesota Vikings. On top of that the Ravens signed Pro bowl center Andre Gurode from the Dallas Cowboys. With the new signings, there was a reshuffle within the Offensive line. The signing of McKinnie forced Michael Oher over to the Right Tackle position, which in turn allowed Marshall Yanda to revert to his natural Right Guard position. The Ravens finished their pre season 3–1, with a loss to the Philadelphia Eagles, and victories over the Washington Redskins, the Kansas City Chiefs, and the Atlanta Falcons.
2011 marked one of the most successful seasons in Baltimore Ravens franchise history. The Ravens started their campaign with a big 35–7 victory at home over their rivals the Pittsburgh Steelers. Other key victories included a 34–17 victory over the New York Jets in week three, a week six 29–14 victory over the Houston Texans, and a week nine win over the Steelers, this would be the first time since 2006 that the Ravens would sweep Pittsburgh in the AFC North division.
The Ravens went on and had a big win over the San Francisco 49ers in a week twelve Thursday night thanksgiving game. This was a game where Ravens coach John Harbaugh would face off against his brother Jim Harbaugh who had just taken over as head coach of the 49ers in the 2011 season. The Ravens would go on to end a 49ers win streak in a final score of 16–6. The Ravens recorded a franchise record of nine sacks on 49ers quarterback Alex Smith, three coming from Ravens linebacker Terrell Suggs.
The Ravens went into the final week of the regular season already assured of a play off place at 11–4, but were tied with the Steelers record wise, and so they had to beat the Cincinnati Bengals on the road, in order to clinch the AFC North division for the first time since 2006. The Ravens defeated the Bengals for the second time in the 2011 regular season by a score of 24–16. The victory sealed the AFC North crown, a season 12–4 record, and a first round bye in the playoffs, which in turn sent both the Steelers and the Bengals on road in the wildcard playoff games – which both rivals lost to the Denver Broncos, and the Texans, respectively.
The Ravens accomplished a number of significant achievements during the 2011 season, finishing 6–0 in the division, 6–0 against 2012 playoff teams, and 8–0 at home. The Ravens went into the 2012 NFL playoffs with high expectations, however there were underlying worries, with what many considered to be inconsistent performances throughout the season by starting quarterback Joe Flacco and the Ravens offense – which many put the blame on for the four questionable road losses that came against teams they were expected to beat, the Tennessee Titans, Jacksonville Jaguars, Seattle Seahawks, and San Diego Chargers.
The Texans would end up beating the Bengals in the wild card playoff round, which meant that they would face the Ravens in the divisional playoff game. The Ravens won the game 20–13 in a defensive struggle, Ed Reed would intercept a pass from the Texans rookie quarterback T. J. Yates in a fourth quarter offensive drive by the Texans, which ended up being the Texans last realistic shot at scoring to tie the game. Despite the victory over the Texans, a significant amount of sports media questioned the Ravens' offensive capability going into the AFC championship playoff round, after another proposed poor performance by the Ravens offense and Joe Flacco according to certain sports analysts.
The Ravens and New England Patriots played for a spot in the Super Bowl. After a close 3 quarters which saw the ball being turned over several times, Tom Brady leaped over the goal line on a 4th and 1 to put the Patriots ahead 23–20. After another couple of turnovers on both ends the Ravens marched down the field with under a minute to go. Joe Flacco threw a pass to Lee Evans with 38 seconds left, who appeared to catch it for the winning touchdown but the ball was knocked out by Patriots defensive back Sterling Moore, resulting in an incomplete pass. After Joe Flacco threw a dropped pass to Lee Evans, Billy Cundiff came out onto the field with 15 seconds to go to try and tie the game up with a 32-yard field goal. The kick went well to the left of the Patriots' goal post, and New England advanced to Super Bowl XLVI for a rematch with the New York Giants in Super Bowl XLII.
The inaugural NFL Honors ceremony was held on Super Bowl's eve. At this event Terrell Suggs was named the Defensive Player of the Year and Matt Birk was named Walter Payton Man of The Year.
2012: Ray Lewis' final season and 2nd Super Bowl.
Despite early injuries that led to struggles on the defensive side of the football, the Ravens jumped out to a 9–2 start thanks in part to a high powered no-huddle offense led by Joe Flacco. Needing just a single win to secure a second straight division title, the Ravens went on a three-game losing streak, highlighted by the firing of offensive coordinator Cam Cameron, promotion of quarterback coach Jim Caldwell to offensive coordinator, and a 34–17 loss at home to Peyton Manning and the Denver Broncos on December 16. Regardless, the Ravens clinched their fifth straight playoff berth after the Steelers lost to the Dallas Cowboys. Despite making the playoffs, this slump led some media outlets to questions the Ravens ability to win football games, but after making a statement with a decisive 33–14 victory over the Super Bowl champion New York Giants and securing back to back division titles for the first time in franchise history, they finished with a regular season record of 10–6 and a rare home playoff game in the Wildcard Round.
The 2012 season also saw long-serving linebacker Ray Lewis announce his retirement heading into the AFC Wildcard game against the Indianapolis Colts. Lewis tore his triceps midway through October in his 17th season with the Ravens after Baltimore selected him with the 26th overall pick in the 1996 NFL Draft. The unusual timing of the announcement was criticized by some, such as former New York Giant Amani Toomer who accused Lewis of being selfish. Nonetheless, many, to include team mate Terrell Suggs, considered the timing to as a "stroke of genius in regards to Lewis' strong motivational presence, and credited Lewis with providing the necessary inspiration for the team in what would prove to be a Super Bowl victory season. They defeated the Colts during the Wild Card round on January 6, 2013 at M&T Bank Stadium, with Ray Lewis celebrating his final game at home with his trademark dance both at the start of the game as he was introduced and on the final play of the game when he was brought back onto the field. With the defeat of the Colts in the Wildcard round, the Ravens were primed to face the Denver Broncos at Mile High Stadium in Denver in the AFC divisional round. Labeled as huge underdogs coming into the game, especially considering the previous thrashing that was dealt against them by the aforementioned Denver Broncos in a home loss at M&T Bank Stadium on December 16th, 2012. The Ravens, however, shocked the sports world as well as many analysts and fans of football alike, by defeating the Peyton Manning led Denver Broncos in the second overtime, 38–35, in a spectacular divisional round matchup on Saturday, January 12, 2013. It looked as though they had lost the game as they got the ball with just over a minute left on their own twenty-three-yard line, but a clutch 70-yard touchdown pass, known colloquially as "The Mile High Miracle", from Joe Flacco to Jacoby Jones sent the game to overtime. In overtime, an interception of Peyton Manning by Corey Graham (who had already returned one interception for a touchdown earlier in the game) put the Ravens in field position to kick the winning field goal. The win vaulted the Ravens to play for the AFC Conference Championship against the New England Patriots on Sunday, January 20, 2013; the Ravens won 28–13 after shutting out Tom Brady and the New England offense in the second half, completely dismantling any offensive attempt(s) thereafter. The win placed the Ravens in Super Bowl XLVII against the San Francisco 49ers on February 3, 2013. The Ravens opened the game in thrilling fashion as their opening drive of the game ended with a touchdown pass from Super Bowl MVP Joe Flacco to wide receiver Anquan Boldin. Flacco threw two second-quarter touchdown passes as Baltimore took a 21-6 lead into halftime. After halftime, the Ravens received the kickoff from the 49ers, and Jacoby Jones returned the kickoff for a record setting 108-yard touchdown. However, soon after the Jacoby Jones touchdown, a power-outage at the stadium led to a 34-minute stoppage in play due to inefficiencies in lighting, on-field visibility, and electrical equipment. After power was restored, the 49ers regained composure and came storming back, scoring 17 unanswered points. The 49ers had a final chance to take the lead late in the game, but a goal-line stand by the Ravens with less than two minutes remaining sealed the contest as the Baltimore Ravens won Super Bowl XLVII 34–31. Super Bowl XLVII has also been dubbed the "Harbaugh Bowl" since the 49ers were coached by Jim Harbaugh, the brother of Ravens coach John Harbaugh.
The Ravens returned to Baltimore to celebrate with their fans on Tuesday, February 5. A parade saw upwards of 300,000 people line the streets of downtown Baltimore while another 80,000 packed M&T Bank Stadium to cheer the team. Speeches by owner Steve Biscotti, Coach Harbaugh, Ray Lewis and Ed Reed were met by fans with standing ovations.
2013.
By virtue of winning Super Bowl XLVII, the Ravens were scheduled to host the Kickoff Game on Thursday, September 5, 2013; however, due to a scheduling conflict with the Baltimore Orioles (with whom they share a parking lot) and the NFL's refusal to move the game to a different night, the Baltimore Ravens were the first Super Bowl Champion in 10 years not to host the following year's Kickoff Game. The 2013 Kickoff Game was played at Sports Authority Field at Mile High when the Ravens visited the Denver Broncos.
After losing linebacker Ray Lewis and center Matt Birk to retirement, the Ravens' roster underwent significant changes throughout the 2013 offseason. Free agent linebacker Paul Kruger signed with the Cleveland Browns, linebacker Dannell Ellerbe signed with the Miami Dolphins, cornerback Cary Williams signed with the Philadelphia Eagles, and safety Ed Reed signed with the Houston Texans. Additionally, safety Bernard Pollard was released due to salary cap reasons and later signed with the Tennessee Titans. Wide receiver Anquan Boldin was traded to the San Francisco 49ers for a sixth-round draft pick. The Ravens made some offseason additions as well, signing defensive linemen Chris Canty and Marcus Spears as well as signing key free agent linebacker Elvis Dumervil and safety Michael Huff. As well as drafting first round pick Safety Matt Elam, second round pick Linebacker Arthur Brown, third round pick Defensive Tackle Brandon Williams, and fourth round pick "hybrid" Fullback Kyle Juszczyk among others. Unfortunately during training camp the Ravens lost another key player, tight-end Dennis Pitta for the entire season (who was primed for an even greater 2013 season). During a routine drill, he suffered a dislocated/fractured hip after colliding with another teammate (James Ihedigbo) at the back of the end zone. In response, during August, they re-signed wide receiver Brandon Stokley, whom they drafted in 1999, and signed former Colts and Tampa Bay Buccaneers tight end, Dallas Clark.
The season started out badly for the Ravens, as they were defeated by the Broncos in their season opener, thus becoming the second Super Bowl Champion to lose the NFL kickoff game. This meant that they held the first loss of the season, breaking a record for the most points allowed by the defense and ending a 75-game streak of holding a .500 or better win-loss ratio. They would rebound in Week 2 with a 14–6 victory over their division rival, the Cleveland Browns. The following week the Ravens took on the unbeaten Houston Texans at home; this time they would face a familiar face in safety Ed Reed, who played for them for 12 seasons. After a strong showing by the defense, which only allowed 9 points, the Ravens clobbered the Texans 30–9. In Week 4, the Ravens struggled against the Buffalo Bills as Joe Flacco threw a career high 5 interceptions. The Ravens fell to the Bills 23–20. At 2–2, the Ravens hit the road to Miami to take on the Dolphins. Although they faced a 13–6 deficit at halftime, the offense rallied to score 17 consecutive points giving the Ravens a 23–13 lead in the 4th quarter. However, Miami answered right back with 10 points of their own, tying the game at 23 with 8:03 left. Justin Tucker then kicked a 44-yard field goal with 1:42 left to give the Ravens a 26–23 lead. After the Dolphins comeback attempt stalled, Caleb Sturgis missed a field goal late, giving the Ravens the win. The Ravens then returned home to take on the Green Bay Packers. The offense struggled early, as the Ravens were shut out in the first half. Trailing 16-3 in the 4th quarter Joe Flacco found Jacoby Jones in the end zone for an 11-yard touchdown pass to reduce the deficit to 16-10. After another Packers field goal, the Ravens faced a 19-10 deficit with 4:17 remaining. Joe Flacco was able to find Dallas Clark in the end zone for an 18-yard touchdown pass, making it 19-17. However, the Packers were able to seal the victory by picking up first downs and running out the clock. The following week, the Ravens would fall to their rivals, the Pittsburgh Steelers 16-19 on a last second field goal by Steelers kicker Shaun Suisham. The following week, the Ravens would lose 18-24 to the Cleveland Browns for the first time under John Harbaugh. Facing the probable end to their season at that point, the Ravens would snap their three-game skid with a win against their first-place division rivals, the Cincinnati Bengals 20-17 in overtime. The following week, the Ravens were scheduled to face the Chicago Bears. The game was delayed for two hours due to a heavy storm. The Ravens would, however, fall to the Bears 20-23 in overtime. They would rebound the next week in a home game against the New York Jets. On Thanksgiving, the Ravens would defeat the Steelers 22-20 at home, avenging their Week 7 loss to them, and improving to 2-0 on Thanksgiving games. In Week 14 the Ravens would beat the Minnesota Vikings 26-29. In Week 15, the Ravens would beat the Detroit Lions at Ford Field 18-16 off Justin Tucker's 6 field goals, including a 61-yarder that proved to be the game-clinching score followed by a Matthew Stafford interception. In Week 16 the Ravens would lose at home to the New England Patriots 41-7 making it the second worst home loss in Ravens history. In Week 17 the Ravens faced the Cincinnati Bengals. The defense forced 4 interceptions on Andy Dalton but it wasn't enough for the offense to take advantage of. Baltimore tied the game 17-17 in the second half, but surrendered 17 unanswered points to the Bengals, thus ending their season at 8-8 and missing the playoffs for the first time since 2007.
2014.
On January 27, 2014, the Ravens hired former Houston Texans head coach Gary Kubiak to be their new offensive coordinator after Jim Caldwell accepted the new available head coaching job with the Detroit Lions. In the 2014 NFL Draft, the Ravens selected C.J. Mosley with the 17th overall pick. During the offseason, the Ravens signed former Carolina Panthers wide receiver Steve Smith to a 3-year contract. They also signed tight end Owen Daniels and running back Justin Forsett, as they played for Kubiak in Houston and would seemingly fit in Kubiak's new system. On February 15, 2014, star running back Ray Rice and his fiancée Janay Palmer were arrested and charged with assault after a physical altercation at Revel Casino in Atlantic City, New Jersey. Celebrity news website TMZ posted a video of Rice dragging Palmer's body out of an elevator after apparently knocking her out. For the incident, Rice was initially suspended for the first two games of the 2014 NFL season on July 25, 2014, which led to widespread criticism of the NFL.
In Week 1, on September 7, the Baltimore Ravens lost to the Cincinnati Bengals, 23-16. The next day, on September 8, 2014, TMZ released additional footage from an elevator camera showing Rice punching Palmer. The Baltimore Ravens terminated Rice's contract as a result, and was later indefinitely suspended by the NFL. Although starting out 0-1 for two straight seasons and having received unwanted media attention for the Ray Rice incident, on September 11, 2014, the Ravens rallied back and beat the Pittsburgh Steelers 26-6 on Thursday Night Football, to improve to 1-1 . On the Sunday after the Thursday night victory, the Ravens headed to Cleveland for a third straight division game with the Browns. In a tough fought game, the Baltimore Ravens won 23-21 with a game-winning 32-yard field goal by Justin Tucker as time expired. On September 28, the Ravens hosted the Carolina Panthers, and in a fabulous offensive performance, defeated the Carolina Panthers 38-10; with this victory the Ravens have defeated every visiting franchise at least once at home (and at M&T Bank Stadium). The very next week, the Ravens traveled to Indianapolis to play the Colts in Week 5. However, three turnovers and QB Joe Flacco being sacked four times resulted in a 20-13 loss, breaking the 3-2 Ravens win streak. In week 6, Joe Flacco led the Ravens to an impressive 48-17 victory over the Tampa Bay Buccaneers. Flacco threw 5 touchdown passes in the first 16:03 of the game, an NFL record, to put the contest out of reach early in the 2nd quarter. In Week 7, the Ravens hosted the Atlanta Falcons and won 29-7. The next week, the Ravens traveled to Cincinnati for a key divisional game. Both defenses forced key turnovers in a game that no passing touchdowns were thrown. In the waning seconds of the 4th quarter as Baltimore trailed, Flacco threw an 80-yard touchdown pass to Steve Smith that seemingly won the game. However, Smith was controversially called for offensive pass interference on Bengals safety George Iloka, who looked to have flopped, that negated the touchdown. The Ravens ended up losing 27-24 and being swept by the Bengals for the first time since 2009. The next week, the Ravens traveled to Pittsburgh for another key division match. Two unfortunate turnovers led to an offensive surge by the Steelers and Ben Roethlisberger throwing six touchdowns on an injury riddled Baltimore secondary, humbling the Ravens 43-23. Afterwards, the Ravens made some changes to their secondary, cutting cornerbacks Chykie Brown and Dominique Franks, and signing former Ravens cornerback Danny Gorrer. The Ravens hosted the Tennessee Titans in Week 10, winning 21-7, and going into their bye week 6-4. After the bye week, the Ravens traveled down to New Orleans for an interconference battle with the New Orleans Saints on Monday Night Football. In the third quarter, with the score tied 17-17, new Ravens free safety Will Hill stepped in front of a Drew Brees pass and returned it 44 yards for a touchdown. Running back Justin Forsett had a career night, running for 182 yards and 2 touchdowns, as the Ravens won 34-27, improving their record to 7-4 and a 4-0 sweep of the NFC south. In Week 13 against the San Diego Chargers, the Ravens loss in heartbreaking fashion 34-33, allowing the Chargers to score 21 points in the 4th quarter. The Ravens rebounded the next week against the Miami Dolphins winning 27-10, and the defense sacking Dolphins quarterback Ryan Tannehill six times. The defense was even more dominant against the Jacksonville Jaguars in a 20-12 win, sacking rookie quarterback Blake Bortles eight times. In Week 16, the Ravens traveled to Houston to take on the Texans. In one of Joe Flacco's worst performances, the offense sputtered against the Houston defense and Flacco threw three interceptions, including two inside the Ravens own 20 yard line, falling to the Texans 25-13. With their playoff chances and season hanging in the balance, the Ravens took on the Browns in Week 17 at home. After three quarters had gone by and down 10-3, Joe Flacco led the Ravens on a comeback scoring 17 unanswered points in the 4th quarter, winning 20-10. With the win, and the Kansas City Chiefs defeating the San Diego Chargers, the Ravens clinched their sixth playoff berth in seven seasons, and the first since winning Super Bowl XLVII.
In the wildcard playoff game, the Ravens won 30-17 against their divisional rivals, the Pittsburgh Steelers, at Heinz Field. The next in the Divisional round, the Ravens faced the New England Patriots. Despite a strong offensive effort and having a 14-point lead twice in the game, the Ravens were defeated by the Patriots 35-31, ending their season.
2015.
The 2015 season marked 20 seasons of the franchise's existence, competing in the NFL which the franchise have recognized with a special badge being worn on their uniforms during the 2015 NFL season.
After coming up just short against the Patriots in the playoffs, the Ravens were picked by some to win the AFC and even the Super Bowl. However, they lost key players like Joe Flacco, Justin Forsett, Terrell Suggs, Steve Smith Sr., and Eugene Monroe to season-ending injuries. Injuries and their inability to win close games early in the season led to the first losing season in the Harbaugh-Flacco era.
Rivalries.
Pittsburgh Steelers.
By far the team's biggest rival is the Pittsburgh Steelers. Pittsburgh and Baltimore are separated by a less-than-5-hour drive along Interstate 70. Both teams are known for their hard-hitting physical style of play. They play at least twice a year in the AFC North, and have met five times in the playoffs. Games between these two teams usually come down to the wire as most within the last 5 years have come down to 3 points or less.
The rivalry is considered one of the most significant and intense in the NFL today.
Indianapolis Colts.
Although the Steelers rivalry is based on mutual respect and antagonism for each other, the Ravens' rivalry with the Indianapolis Colts is fueled by the fans' animosity towards the organization, not contention between the players. This is due to the fact that the then-Colts owner, Robert Irsay, while publicly still in negotiations with the city for the stadium improvements that he was demanding, snuck the Colts out of Baltimore in the middle of the night to take them to Indianapolis. During Ravens home games the scoreboard lists the away team simply as "Away" or "Indy" rather than the team name that is traditionally used for the visiting opponent. The PA announcer will also refer to the Colts as the Indianapolis Professional Football Team; although on January 6, 2013 the scoreboard at the playoff game between the Baltimore Ravens and Indianapolis Colts at M&T Bank Stadium listed the away team as "Colts". The Indianapolis Colts hold an all-time 9–4 advantage over the Baltimore Ravens, including a 2–1 advantage in the playoffs.
Other AFC North Rivals.
The Ravens also have divisional rivalries with the Cleveland Browns and Cincinnati Bengals.
The reactivated Cleveland Browns and their fans maintain a hatred of Baltimore's team due to its move from Cleveland. The rivalry with the Browns has been very one-sided; Baltimore holds an advantage of 25-9 against Cleveland.
The rivalry with Cincinnati has been closer, standing at 20-19 in favor of the Ravens after the 2014 season.
New England Patriots.
The Ravens first met the New England Patriots in 1996, but the rivalry truly started in 2007 when the Ravens suffered a bitter 27–24 loss in the Patriots quest for perfection. The rivalry began to escalate in 2009 when the Patriots beat the Ravens 27–21 in a game that involved a confrontation between Patriots quarterback Tom Brady and Ravens linebacker Terrell Suggs. Both players would go on to take verbal shots at each other through the media after the game. The Ravens faced the Patriots in a 2009 AFC wild card playoff game and won 33–14; the Ravens ran the ball for more than 250 yards.
The Ravens faced the Patriots in Week 6 of the 2010 season; the Ravens ended up losing 23–20 in overtime; the game caused controversy due to a hit to the helmet of tight end Todd Heap by Patriots safety Brandon Meriweather.
The Ravens played the Patriots for the third consecutive season, in the 2011 AFC championship game in which the Ravens lost 23–20. The rivalry reached a new level of friction with this, the second career playoff game between the two clubs. The Ravens clawed to a 20–16 lead in the fourth quarter but Patriots quarterback Tom Brady dove into the end zone to make the score 23–20 with around 11 minutes remaining; this proved to be the winning touchdown. On the Ravens last possession of the game, quarterback Joe Flacco threw a pass to wide receiver Lee Evans in the corner of the end zone which looked to be the game winning touchdown, before a last second strip by Sterling Moore forced the ball from the hands of Evans, forcing the game to be decided on a last minute field goal by Ravens placekicker Billy Cundiff. With eleven seconds remaining on the clock, the kicker missed the 32-yard field goal attempt by a very wide margin, allowing the Patriots to kill the clock on their way to Super Bowl XLVI.
The Ravens' first regular-season win over the Patriots came on September 23, 2012. The game was emotional as receiver Torrey Smith was competing following the death of his brother in a motorcycle accident just the night before. Smith caught two touchdowns in a back and forth game; the Ravens erased a 13–0 deficit in the first half and led 14–13, but the Patriots scored at the end of the second quarter for a 20–14 lead. The lead changed twice in the third quarter and the Patriots led 30–21 in the fourth, but the Ravens scored on Smith's second touchdown catch. The Ravens were stopped on fourth down but the Patriots had to punt; in the final two minutes a pass interference penalty on Devin McCourty put the ball at the Patriots 7-yard line; new Ravens kicker Justin Tucker booted a 27-yard field goal on the final play; the ball sailed directly over the upright and was ruled good; the quality of officiating by replacement referees caused controversy as Bill Belichick angrily reached for one of the referees as they were leaving the field, leading to a $50,000 fine later that week.
The two teams met again on January 20, 2013 in the AFC Championship, where the Ravens won 28–13. The Patriots led at halftime, 13–7, but the Ravens' defense gave up no points in the second half. It was the first time ever that Tom Brady lost a game at home after leading at halftime, and the first time a road team beat the Patriots in the AFC Championship.
On December 22, 2013 the teams met again, this rematch of the AFC championship game was a mismatch from the outset. New England took a 17-0 lead early in the second quarter and never let up behind a defense that forced four turnovers and had four sacks. New England would go on to win the game 41-7.
On January 10, 2015, the two teams would meet in the Divisional Round of the playoffs. Unlike the previous meeting, the Ravens put up a strong offensive performance, leading by 14 points twice in the game. However, Tom Brady would bring the Patriots back by attacking the Ravens vulnerable secondary and taking a 35-31 lead late in the 4th quarter. Joe Flacco would drive to the Patriots side of the field with under two minutes to play in regulation. However a key interception by Flacco due to a misplay on the ball by Torrey Smith essentially sealed the game in the Patriots favor to send them to the AFC Championship.
Logo controversy.
The team's first helmet logo, used from 1996 through 1998, featured raven wings outspread from a shield displaying a letter "B" framed by the word "Ravens" overhead and a cross bottony underneath. The US Fourth Circuit Court of Appeals affirmed a jury verdict that the logo infringed on a copyright retained by Frederick E. Bouchat, an amateur artist and security guard in Maryland, but that he was entitled to only three dollars in damages from the NFL.
Bouchat had submitted his design to the Maryland Stadium Authority by fax after learning that Baltimore was to acquire an NFL team. He was not credited for the design when the logo was announced. Bouchat sued the team, claiming to be the designer of the emblem; representatives of the team asserted that the image had been designed independently. The court ruled in favor of Bouchat, noting that team owner Modell had access to Bouchat's work. Bouchat's fax had gone to John Moag, the Maryland Stadium Authority chairman, whose office was located in the same building as Modell's. Bouchat ultimately was not awarded monetary compensation in the damages phase of the case.
The "Baltimore Sun" ran a poll showing three designs for new helmet logos. Fans participating in the poll expressed a preference for a raven's head in profile over other designs. Art Modell announced that he would honor this preference but still wanted a letter "B" to appear somewhere in the design. The new Ravens logo featured a raven's head in profile with the letter superimposed. The secondary logo is a shield that honors Baltimore's history of heraldry. Alternating Calvert and Crossland emblems (seen also in the flag of Maryland and the flag of Baltimore) are interlocked with stylized letters "B" and "R".
Uniforms.
The design of the Ravens uniform has remained essentially unchanged since the team's inaugural season in 1996. Art Modell admitted to ESPN’s Roy Firestone that the Ravens’ colors, introduced in early 1996, were inspired by the Northwestern Wildcats 1995 dream season. Helmets are black with purple "talon" stripes rising from the facemask to the crown. Players normally wear purple jerseys at home and white jerseys on the road. In 1996 the team wore black pants with a single large white stripe for all games. At home games the combination of black pants with purple jersey made the Ravens the first NFL team to wear dark colors head to calf. A number of NFL teams have since donned the look, beginning with the all-black home uniform worn in three games by the 2001 New Orleans Saints.
In 1997 the Ravens opted for a more classic NFL look with white pants sporting stripes in purple and black. The white pants were worn with both home and road jerseys. The road uniform (white pants with white jerseys) was worn by the Ravens in the 2000 Super Bowl.
In the 2002 season the Ravens began the practice of wearing white jerseys for the home opener and, occasionally, other early games in the season that have a 1:00 kickoff. Since John Harbaugh became the head coach in 2008, the Ravens have also worn their white jerseys at home for preseason games.
In November 2004 the team introduced an alternate uniform design featuring black jerseys and solid black pants with black socks. The all-black uniform was first worn for a home game against the Cleveland Browns, entitled "Pitch Black" night, that resulted in a Ravens win. The uniform has since been worn for select prime-time national game broadcasts and other games of significance.
The Ravens began wearing black pants again with the white jersey in 2008. On December 7, 2008, during a Sunday Night Football game against the Washington Redskins, the Ravens introduced a new combination of black jersey with white pants. It was believed to be due to the fact that John Harbaugh doesn't like the "blackout" look. However, on December 19, 2010, the Ravens wore their black jerseys and black pants in a 30–24 victory over the New Orleans Saints.
On December 5, 2010, the Ravens reverted to the black pants with the purple jerseys versus the Pittsburgh Steelers during NBC's "Sunday Night Football" telecast. The Ravens lost to the Steelers 13–10. They wore the same look again for their game against the Cleveland Browns on December 24, 2011, and they won, 20–14. They wore this combination a third time against the Houston Texans on January 15, 2012 in the AFC Divisional playoff. They won 20–13. They would again wear this combination on January 6, 2013, during the AFC Wild Card playoff and what turned out to be Ray Lewis' final home game, where they defeated the Indianapolis Colts 24-9.
From their inaugural season until 2006, the Ravens wore white cleats with their uniforms; they switched to black cleats in 2007.
On December 20, 2015, the team unexpectedly debuted gold pants for the first time, wearing them with their regular purple jerseys against the Kansas City Chiefs. Although gold is an official accent color of the Ravens, the pants got an overwhelmingly negative response on social media by both Ravens fans and fans of other NFL teams, with some comparisons being made to the rival Pittsburgh Steelers' pants.
Marching band.
The team marching band is called Baltimore's Marching Ravens. They began as the Colts' marching band and have operated continuously from September 7, 1947 to the present. They helped campaign for football to return to Baltimore after the Colts moved. Because they stayed in Baltimore after the Colts left, the band is nicknamed "the band that would not die" and were the subject of an episode of ESPN's "30 for 30". The Washington Redskins are the only other NFL team that currently has a marching band.
Players of note.
Retired numbers.
The Ravens officially have no retired numbers. However, out of respect for Baltimore Colts quarterback Johnny Unitas, only quarterback Scott Mitchell has worn the number 19, which he did in his lone season in Baltimore in 1999. In addition, numbers 75, in honor of Jonathan Ogden, 52, in honor of Ray Lewis, and 20, in honor of Ed Reed, have not been issued since those players' retirements from football.
Ring of Honor.
The Ravens have a "Ring of Honor" which is on permanent display encircling the field of M&T Bank Stadium, including a sign with the names and dates of play viewable from the seats. The ring currently honors the following:
First round draft picks.
The Baltimore Ravens had their first draft in 1996, where they selected offensive lineman from UCLA and current NFL Hall of Famer, and 11-time Pro-Bowler Jonathan Ogden. Along with their pick in the next year's draft, this was the highest first-round draft pick that the Ravens have had. They also selected Ray Lewis with the 26th pick. In both 1996 and 2000, the Ravens had two first-round draft picks. However, in 2004 they had none. In their history, the Ravens have drafted 3 offensive linemen, 3 linebackers, 2 wide receivers, 2 cornerbacks, 2 quarterbacks, a running back, tight end, safety, and defensive tackle. The Ravens have 56 combined Pro-Bowl appearances from their first-round draft picks.
"All records as of December 30, 2014 per Pro-Football Reference.com"
Broadcast media.
The Ravens' flagship radio stations are WIYY (98 Rock) and WBAL 1090 AM, with Gerry Sandusky (WBAL-TV Sports Anchor since 1988) as the play-by-play announcer and analysts Stan White (Baltimore Colts LB 1972–1979) and Qadry Ismail (Baltimore Ravens WR 1999–2001). The Hearst-Argyle stations were in their first season of game coverage, replacing longtime stations WJFK/WQSR. As of the 2010 season, any Ravens preseason games not on national television are seen on WBAL-TV in Baltimore and on Mid-Atlantic Sports Network throughout the region. Sandusky, White and Ismail are also the television announcers. MASN also has extensive coverage of the team throughout the season, including postgame reports and the magazine show "Ravens Wired". "Ravens Wired", as well as "Ravens Report" and the regional preseason games, are produced by the Ravens in-house production department, RaveTV.
In terms of television broadcasting of regular season games, the Ravens' primary station is CBS O&O WJZ-TV, which began broadcasting the team's games in 1998, and has broadcast both of their Super Bowl victories. Interconference home games usually appear on WBFF-TV (Fox), and primetime games on WBAL-TV.
Affiliates
"Baltimore Ravens." News RSS. N.p., n.d. Web. January 16, 2013. <http://www.baltimoreRavens.com/>.

</doc>
