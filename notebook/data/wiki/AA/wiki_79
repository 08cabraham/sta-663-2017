<doc id="10819" url="https://en.wikipedia.org/wiki?curid=10819" title="Federal Reserve System">
Federal Reserve System

The Federal Reserve Systemalso known as the Federal Reserve or simply as the Fedis the central banking system of the United States. It was created on December 23, 1913, with the enactment of the Federal Reserve Act, largely in response to a series of financial panics, particularly a severe panic in 1907. Over time, the roles and responsibilities of the Federal Reserve System have expanded, and its structure has evolved. Events such as the Great Depression in the 1930s were major factors leading to changes in the system.
The U.S. Congress established three key objectives for monetary policy in the Federal Reserve Act: maximizing employment, stabilizing prices, and moderating long-term interest rates. The first two objectives are sometimes referred to as the Federal Reserve's dual mandate. Its duties have expanded over the years, and also include supervising and regulating banks, maintaining the stability of the financial system and providing financial services to depository institutions, the U.S. government, and foreign official institutions. The Fed conducts research into the economy and releases numerous publications, such as the Beige Book.
The Federal Reserve System's structure is composed of the presidentially appointed Board of Governors or Federal Reserve Board (FRB), partially presidentially appointed Federal Open Market Committee (FOMC), twelve regional Federal Reserve Banks located in major cities throughout the nation, numerous privately owned U.S. member banks, and various advisory councils. The federal government sets the salaries of the Board's seven governors. Nationally chartered commercial banks are required to hold stock in the Federal Reserve Bank of their region, which entitles them to elect some of their board members. The FOMC sets monetary policy and consists of all seven members of the Board of Governors and the twelve regional bank presidents, though only five bank presidents vote at any given time: the president of the New York Fed and four others who rotate through one-year terms. Thus, the Federal Reserve System has both private and public components to serve the interests of the public and private banks. The structure is considered unique among central banks. It is also unusual in that the United States Department of the Treasury, an entity outside of the central bank, creates the currency used. The Federal Reserve System considers itself "an independent central bank because its monetary policy decisions do not have to be approved by the President or anyone else in the executive or legislative branches of government, it does not receive funding appropriated by the Congress, and the terms of the members of the Board of Governors span multiple presidential and congressional terms."
The U.S. Government receives all the system's annual profits, after a statutory dividend of 6% on member banks' capital investment is paid, and an account surplus is maintained. In 2010, the Federal Reserve made a profit of $82 billion and transferred $79 billion to the U.S. Treasury.
Purpose.
The primary motivation for creating the Federal Reserve System was to address banking panics. Other purposes are stated in the Federal Reserve Act, such as "to furnish an elastic currency, to afford means of rediscounting commercial paper, to establish a more effective supervision of banking in the United States, and for other purposes". Before the founding of the Federal Reserve System, the United States underwent several financial crises. A particularly severe crisis in 1907 led Congress to enact the Federal Reserve Act in 1913. Today the Federal Reserve System has responsibilities in addition to ensuring the stability of the financial system.
Current functions of the Federal Reserve System include:
Addressing the problem of bank panics.
Banking institutions in the United States are required to hold reservesamounts of currency and deposits in other banksequal to only a fraction of the amount of the bank's deposit liabilities owed to customers. This practice is called fractional-reserve banking. As a result, banks usually invest the majority of the funds received from depositors. On rare occasions, too many of the bank's customers will withdraw their savings and the bank will need help from another institution to continue operating; this is called a bank run. Bank runs can lead to a multitude of social and economic problems. The Federal Reserve System was designed as an attempt to prevent or minimize the occurrence of bank runs, and possibly act as a lender of last resort when a bank run does occur. Many economists, following Milton Friedman, believe that the Federal Reserve inappropriately refused to lend money to small banks during the bank runs of 1929.
Check clearing system.
Because some banks refused to clear checks from certain others during times of economic uncertainty, a check-clearing system was created in the Federal Reserve System. It is briefly described in "The Federal Reserve SystemPurposes and Functions" as follows:
Lender of last resort.
In the United States, the Federal Reserve serves as the lender of last resort to those institutions that cannot obtain credit elsewhere and the collapse of which would have serious implications for the economy. It took over this role from the private sector "clearing houses" which operated during the Free Banking Era; whether public or private, the availability of liquidity was intended to prevent bank runs.
Fluctuations.
Through its discount window and credit operations, Reserve Banks provide liquidity to banks to meet short-term needs stemming from seasonal fluctuations in deposits or unexpected withdrawals. Longer term liquidity may also be provided in exceptional circumstances. The rate the Fed charges banks for these loans is called the discount rate (officially the primary credit rate).
By making these loans, the Fed serves as a buffer against unexpected day-to-day fluctuations in reserve demand and supply. This contributes to the effective functioning of the banking system, alleviates pressure in the reserves market and reduces the extent of unexpected movements in the interest rates. For example, on September 16, 2008, the Federal Reserve Board authorized an $85 billion loan to stave off the bankruptcy of international insurance giant American International Group (AIG).
Central bank.
In its role as the central bank of the United States, the Fed serves as a banker's bank and as the government's bank. As the banker's bank, it helps to assure the safety and efficiency of the payments system. As the government's bank, or fiscal agent, the Fed processes a variety of financial transactions involving trillions of dollars. Just as an individual might keep an account at a bank, the U.S. Treasury keeps a checking account with the Federal Reserve, through which incoming federal tax deposits and outgoing government payments are handled. As part of this service relationship, the Fed sells and redeems U.S. government securities such as savings bonds and Treasury bills, notes and bonds. It also issues the nation's coin and paper currency. The U.S. Treasury, through its Bureau of the Mint and Bureau of Engraving and Printing, actually produces the nation's cash supply and, in effect, sells the paper currency to the Federal Reserve Banks at manufacturing cost, and the coins at face value. The Federal Reserve Banks then distribute it to other financial institutions in various ways. During the Fiscal Year 2013, the Bureau of Engraving and Printing delivered 6.6 billion notes at an average cost of 5.0 cents per note.
Federal funds.
Federal funds are the reserve balances (also called Federal Reserve Deposits) that private banks keep at their local Federal Reserve Bank. These balances are the namesake reserves of the Federal Reserve System. The purpose of keeping funds at a Federal Reserve Bank is to have a mechanism for private banks to lend funds to one another. This market for funds plays an important role in the Federal Reserve System as it is what inspired the name of the system and it is what is used as the basis for monetary policy. Monetary policy is put into effect partly by influencing how much interest the private banks charge each other for the lending of these funds.
Federal reserve accounts contain federal reserve credit, which can be converted into federal reserve notes. Private banks maintain their bank reserves in federal reserve accounts.
Bank regulation.
The Federal Reserve regulates private banks. The system was designed out of a compromise between the competing philosophies of privatization and government regulation. In 2006 Donald L. Kohn, vice chairman of the Board of Governors, summarized the history of this compromise:
The balance between private interests and government can also be seen in the structure of the system. Private banks elect members of the board of directors at their regional Federal Reserve Bank while the members of the Board of Governors are selected by the President of the United States and confirmed by the Senate. Private banks give input to the government officials about their economic situation and government officials use this input in Federal Reserve policy decisions.
Government regulation and supervision.
The Federal Banking Agency Audit Act, enacted in 1978 as Public Law 95-320 and 31 U.S.C. section 714 establish that the Board of Governors of the Federal Reserve System and the Federal Reserve banks may be audited by the Government Accountability Office (GAO).
The GAO has authority to audit check-processing, currency storage and shipments, and some regulatory and bank examination functions, however there are restrictions to what the GAO may audit. Under the Federal Banking Agency Audit Act, 31 U.S.C. section 714(b), our audits of the Federal Reserve Board and Federal Reserve banks do not include (1) transactions for or with a foreign central bank or government, or non-private international financing organization; (2) deliberations, decisions, or actions on monetary policy matters; (3) transactions made under the direction of the Federal Open Market Committee; or (4) a part of a discussion or communication among or between members of the Board of Governors and officers and employees of the Federal Reserve System related to items (1), (2), or (3). See Federal Reserve System Audits: Restrictions on GAO's Access (GAO/T-GGD-94-44), statement of Charles A. Bowsher.
The Board of Governors in the Federal Reserve System has a number of supervisory and regulatory responsibilities in the U.S. banking system, but not complete responsibility. A general description of the types of regulation and supervision involved in the U.S. banking system is given by the Federal Reserve:
Regulatory and oversight responsibilities.
The board of directors of each Federal Reserve Bank District also has regulatory and supervisory responsibilities. If the board of directors of a district bank has judged that a member bank is performing or behaving poorly, it will report this to the Board of Governors. This policy is described in United States Code:
National payments system.
The Federal Reserve plays an important role in the U.S. payments system. The twelve Federal Reserve Banks provide banking services to depository institutions and to the federal government. For depository institutions, they maintain accounts and provide various payment services, including collecting checks, electronically transferring funds, and distributing and receiving currency and coin. For the federal government, the Reserve Banks act as fiscal agents, paying Treasury checks; processing electronic payments; and issuing, transferring, and redeeming U.S. government securities.
In the Depository Institutions Deregulation and Monetary Control Act of 1980, Congress reaffirmed that the Federal Reserve should promote an efficient nationwide payments system. The act subjects all depository institutions, not just member commercial banks, to reserve requirements and grants them equal access to Reserve Bank payment services. It also encourages competition between the Reserve Banks and private-sector providers of payment services by requiring the Reserve Banks to charge fees for certain payments services listed in the act and to recover the costs of providing these services over the long run.
The Federal Reserve plays a role in the nation's retail and wholesale payments systems by providing financial services to depository institutions. Retail payments are generally for relatively small-dollar amounts and often involve a depository institution's retail clientsindividuals and smaller businesses. The Reserve Banks' retail services include distributing currency and coin, collecting checks, and electronically transferring funds through the automated clearinghouse system. By contrast, wholesale payments are generally for large-dollar amounts and often involve a depository institution's large corporate customers or counterparties, including other financial institutions. The Reserve Banks' wholesale services include electronically transferring funds through the Fedwire Funds Service and transferring securities issued by the U.S. government, its agencies, and certain other entities through the Fedwire Securities Service. Because of the large amounts of funds that move through the Reserve Banks every day, the System has policies and procedures to limit the risk to the Reserve Banks from a depository institution's failure to make or settle its payments.
Structure.
The Federal Reserve System has a "unique structure that is both public and private" and is described as "independent within the government" rather than "independent of government". The System does not require public funding, and derives its authority and purpose from the Federal Reserve Act, which was passed by Congress in 1913 and is subject to Congressional modification or repeal. The four main components of the Federal Reserve System are (1) the Board of Governors, (2) the Federal Open Market Committee, (3) the twelve regional Federal Reserve Banks, and (4) the member banks throughout the country.
Board of Governors.
The seven-member Board of Governors is a federal agency. It is charged with the overseeing of the 12 District Reserve Banks and setting national monetary policy. It also supervises and regulates the U.S. banking system in general.
Governors are appointed by the President of the United States and confirmed by the Senate for staggered 14-year terms. One term begins every two years, on February 1 of even-numbered years, and members serving a full term cannot be renominated for a second term. "pon the expiration of their terms of office, members of the Board shall continue to serve until their successors are appointed and have qualified." The law provides for the removal of a member of the Board by the President "for cause". The Board is required to make an annual report of operations to the Speaker of the U.S. House of Representatives.
The Chair and Vice Chair of the Board of Governors are appointed by the President from among the sitting Governors. They both serve a four-year term and they can be renominated as many times as the President chooses, until their terms on the Board of Governors expire.
List of members of the Board of Governors.
The current members of the Board of Governors are as follows:
Nominations, confirmations and resignations.
In late December 2011, President Barack Obama nominated Jeremy C. Stein, a Harvard University finance professor and a Democrat, and Jerome H. Powell, formerly of Dillon Read, Bankers Trust and The Carlyle Group and a Republican. Both candidates also have Treasury Department experience in the Obama and George H.W. Bush administrations respectively.
"Obama administration officials a regrouped to identify Fed candidates after Peter Diamond, a Nobel Prize-winning economist, withdrew his nomination to the board in June 01 in the face of Republican opposition. Richard Clarida, a potential nominee who was a Treasury official under George W. Bush, pulled out of consideration in August 01", one account of the December nominations noted. The two other Obama nominees in 2011, Yellen and Raskin, were confirmed in September. One of the vacancies was created in 2011 with the resignation of Kevin Warsh, who took office in 2006 to fill the unexpired term ending January 31, 2018, and resigned his position effective March 31, 2011. In March 2012, U.S. Senator David Vitter (R, LA) said he would oppose Obama's Stein and Powell nominations, dampening near-term hopes for approval. However Senate leaders reached a deal, paving the way for affirmative votes on the two nominees in May 2012 and bringing the board to full strength for the first time since 2006 with Duke's service after term end. Later, on January 6, 2014, the United States Senate confirmed Yellen's nomination to be Chair of the Federal Reserve Board of Governors; she is slated to be the first woman to hold the position and will become Chair on February 1, 2014. Subsequently, President Obama nominated Stanley Fischer to replace Yellen as the Vice Chair.
In April 2014, Stein announced he was leaving to return to Harvard May 28 with four years remaining on his term. At the time of the announcement, the FOMC "already is down three members as it awaits the Senate confirmation of ... Fischer and Lael Brainard, and as residen Obama has yet to name a replacement for ... Duke. ... Powell is still serving as he awaits his confirmation for a second term."
Allan R. Landon, 65, former president and CEO of the Bank of Hawaii, was nominated in early 2015 by President Barack Obama to the Board.
In July 2015, President Obama nominated University of Michigan economist Kathryn M. Dominguez to fill the second vacancy on the Board. The Senate had not yet acted on Landon's confirmation by the time of the second nomination.
Federal Open Market Committee.
The Federal Open Market Committee (FOMC) consists of 12 members, seven from the Board of Governors and 5 of the regional Federal Reserve Bank presidents. The FOMC oversees open market operations, the principal tool of national monetary policy. These operations affect the amount of Federal Reserve balances available to depository institutions, thereby influencing overall monetary and credit conditions. The FOMC also directs operations undertaken by the Federal Reserve in foreign exchange markets. The president of the Federal Reserve Bank of New York is a permanent member of the FOMC; the presidents of the other banks rotate membership at two- and three-year intervals. All Regional Reserve Bank presidents contribute to the committee's assessment of the economy and of policy options, but only the five presidents who are then members of the FOMC vote on policy decisions. The FOMC determines its own internal organization and, by tradition, elects the Chair of the Board of Governors as its chair and the president of the Federal Reserve Bank of New York as its vice chair. It is informal policy within the FOMC for the Board of Governors and the New York Federal Reserve Bank president to vote with the Chair of the FOMC; anyone who is not an expert on monetary policy traditionally votes with the chair as well; and in any vote no more than two FOMC members can dissent. Formal meetings typically are held eight times each year in Washington, D.C. Nonvoting Reserve Bank presidents also participate in Committee deliberations and discussion. The FOMC generally meets eight times a year in telephone consultations and other meetings are held when needed.
Federal Advisory Council.
The Federal Advisory Council, composed of twelve representatives of the banking industry, advises the Board on all matters within its jurisdiction.
Federal Reserve Banks.
There are 12 Federal Reserve Banks and they are located in Boston, New York, Philadelphia, Cleveland, Richmond, Atlanta, Chicago, St. Louis, Minneapolis, Kansas City, Dallas, and San Francisco. Each reserve Bank is responsible for member banks located in its district. The size of each district was set based upon the population distribution of the United States when the Federal Reserve Act was passed. Each regional Bank has a president, who is the chief executive officer of their Bank. Each regional Reserve Bank's president is nominated by their Bank's board of directors, but the nomination is contingent upon approval by the Board of Governors. Presidents serve five-year terms and may be reappointed.
Each regional Bank's board consists of nine members. Members are broken down into three classes: A, B, and C. There are three board members in each class. Class A members are chosen by the regional Bank's shareholders, and are intended to represent member banks' interests. Member banks are divided into three categories: large, medium, and small. Each category elects one of the three class A board members. Class B board members are also nominated by the region's member banks, but class B board members are supposed to represent the interests of the public. Lastly, class C board members are nominated by the Board of Governors, and are also intended to represent the interests of the public.
A member bank is a private institution and owns stock in its regional Federal Reserve Bank. All nationally chartered banks hold stock in one of the Federal Reserve Banks. State chartered banks may choose to be members (and hold stock in their regional Federal Reserve bank), upon meeting certain standards. About 38% of U.S. banks are members of their regional Federal Reserve Bank. The amount of stock a member bank must own is equal to 3% of its combined capital and surplus. However, holding stock in a Federal Reserve bank is not like owning stock in a publicly traded company. These stocks cannot be sold or traded, and member banks do not control the Federal Reserve Bank as a result of owning this stock. The charter and organization of each Federal Reserve Bank is established by law and cannot be altered by the member banks. Member banks, do however, elect six of the nine members of the Federal Reserve Banks' boards of directors. From the profits of the Regional Bank of which it is a member, a member bank receives a dividend equal to 6% of their purchased stock. The remainder of the regional Federal Reserve Banks' profits is given over to the United States Treasury Department. In 2009, the Federal Reserve Banks distributed $1.4 billion in dividends to member banks and returned $47 billion to the U.S. Treasury.
Legal status of regional Federal Reserve Banks.
The Federal Reserve Banks have an intermediate legal status, with some features of private corporations and some features of public federal agencies. The United States has an interest in the Federal Reserve Banks as tax-exempt federally created instrumentalities whose profits belong to the federal government, but this interest is not proprietary. In "Lewis v. United States", the United States Court of Appeals for the Ninth Circuit stated that: "The Reserve Banks are not federal instrumentalities for purposes of the FTCA he [[Federal Tort Claims Act], but are independent, privately owned and locally controlled corporations." The opinion went on to say, however, that: "The Reserve Banks have properly been held to be federal instrumentalities for some purposes." Another relevant decision is "Scott v. Federal Reserve Bank of Kansas City", in which the distinction is made between Federal Reserve Banks, which are federally created instrumentalities, and the Board of Governors, which is a federal agency.
Regarding the structural relationship between the twelve Federal Reserve banks and the various commercial (member) banks, political science professor Michael D. Reagan has written that:
Member banks.
According to the website for the Federal Reserve Bank of Richmond, "ore than one-third of U.S. commercial banks are members of the Federal Reserve System. National banks must be members; state chartered banks may join by meeting certain requirements."
Accountability.
The GAO and an outside auditor regularly audit the Board of Governors, the Federal Reserve banks, and individual member banks. Audits do not cover "most of the Fed's monetary policy actions or decisions, including discount window lending (direct loans to financial institutions), open-market operations and any other transactions made under the direction of the Federal Open Market Committee" ...or may the GAO audi "dealings with foreign governments and other central banks." Various statutory changes, including the Federal Reserve Transparency Act, have been proposed to broaden the scope of the audits.
As of August 27, 2012, the Federal Reserve Board has been publishing unaudited financial reports for the Federal Reserve banks every quarter. This is an expansion of prior financial reporting practices. Greater transparency is offered with more frequent disclosure and more detail.
November 7, 2008, Bloomberg L.P. News brought a lawsuit against the Board of Governors of the Federal Reserve System to force the Board to reveal the identities of firms for which it has provided guarantees during the Late-2000s financial crisis. Bloomberg, L.P. won at the trial court and the Fed's appeals were rejected at both the United States Court of Appeals for the Second Circuit and the U.S. Supreme Court. The data was released on March 31, 2011.
Monetary policy.
The term "monetary policy" refers to the actions undertaken by a central bank, such as the Federal Reserve, to influence the availability and cost of money and credit to help promote national economic goals. What happens to money and credit affects interest rates (the cost of credit) and the performance of an economy. The Federal Reserve Act of 1913 gave the Federal Reserve authority to set monetary policy in the United States.
Interbank lending.
The Federal Reserve sets monetary policy by influencing the federal funds rate, which is the rate of interbank lending of excess reserves. The rate that banks charge each other for these loans is determined in the interbank market and the Federal Reserve influences this rate through the three "tools" of monetary policy described in the "Tools" section below. The federal funds rate is a short-term interest rate that the FOMC focuses on, which affects the longer-term interest rates throughout the economy. The Federal Reserve summarized its monetary policy in 2005:
Effects on the quantity of reserves that banks used to make loans influence the economy. Policy actions that add reserves to the banking system encourage lending at lower interest rates thus stimulating growth in money, credit, and the economy. Policy actions that absorb reserves work in the opposite direction. The Fed's task is to supply enough reserves to support an adequate amount of money and credit, avoiding the excesses that result in inflation and the shortages that stifle economic growth.
Tools.
There are three main tools of monetary policy that the Federal Reserve uses to influence the amount of reserves in private banks:
Federal funds rate and open market operations.
The Federal Reserve System implements monetary policy largely by targeting the federal funds rate. This is the interest rate that banks charge each other for overnight loans of federal funds, which are the reserves held by banks at the Fed. This rate is actually determined by the market and is not explicitly mandated by the Fed. The Fed therefore tries to align the effective federal funds rate with the targeted rate by adding or subtracting from the money supply through open market operations. The Federal Reserve System usually adjusts the federal funds rate target by 0.25% or 0.50% at a time.
Open market operations allow the Federal Reserve to increase or decrease the amount of money in the banking system as necessary to balance the Federal Reserve's dual mandates. Open market operations are done through the sale and purchase of United States Treasury security, sometimes called "Treasury bills" or more informally "T-bills" or "Treasuries". The Federal Reserve buys Treasury bills from its primary dealers. The purchase of these securities affects the federal funds rate, because primary dealers have accounts at depository institutions.
The Federal Reserve education website describes open market operations as follows:
Repurchase agreements.
To smooth temporary or cyclical changes in the money supply, the desk engages in repurchase agreements (repos) with its primary dealers. Repos are essentially secured, short-term lending by the Fed. On the day of the transaction, the Fed deposits money in a primary dealer's reserve account, and receives the promised securities as collateral. When the transaction matures, the process unwinds: the Fed returns the collateral and charges the primary dealer's reserve account for the principal and accrued interest. The term of the repo (the time between settlement and maturity) can vary from 1 day (called an overnight repo) to 65 days.
Discount rate.
The Federal Reserve System also directly sets the "discount rate", which is the interest rate for "discount window lending", overnight loans that member banks borrow directly from the Fed. This rate is generally set at a rate close to 100 basis points above the target federal funds rate. The idea is to encourage banks to seek alternative funding before using the "discount rate" option. The equivalent operation by the European Central Bank is referred to as the "marginal lending facility".
Both the discount rate and the federal funds rate influence the prime rate, which is usually about 3 percent higher than the federal funds rate.
Reserve requirements.
Another instrument of monetary policy adjustment employed by the Federal Reserve System is the fractional reserve requirement, also known as the required reserve ratio. The required reserve ratio sets the balance that the Federal Reserve System requires a depository institution to hold in the Federal Reserve Banks, which depository institutions trade in the federal funds market discussed above. The required reserve ratio is set by the Board of Governors of the Federal Reserve System. The reserve requirements have changed over time and some history of these changes is published by the Federal Reserve.
As a response to the financial crisis of 2008, the Federal Reserve now makes interest payments on depository institutions' required and excess reserve balances. The payment of interest on excess reserves gives the central bank greater opportunity to address credit market conditions while maintaining the federal funds rate close to the target rate set by the FOMC.
New facilities.
In order to address problems related to the subprime mortgage crisis and United States housing bubble, several new tools have been created. The first new tool, called the Term Auction Facility, was added on December 12, 2007. It was first announced as a temporary tool but there have been suggestions that this new tool may remain in place for a prolonged period of time. Creation of the second new tool, called the Term Securities Lending Facility, was announced on March 11, 2008. The main difference between these two facilities is that the Term Auction Facility is used to inject cash into the banking system whereas the Term Securities Lending Facility is used to inject treasury securities into the banking system. Creation of the third tool, called the Primary Dealer Credit Facility (PDCF), was announced on March 16, 2008. The PDCF was a fundamental change in Federal Reserve policy because now the Fed is able to lend directly to primary dealers, which was previously against Fed policy. The differences between these three new facilities is described by the Federal Reserve:
Some measures taken by the Federal Reserve to address this mortgage crisis have not been used since The Great Depression. The Federal Reserve gives a brief summary of these new facilities:
A fourth facility, the Term Deposit Facility, was announced December 9, 2009, and approved April 30, 2010, with an effective date of June 4, 2010. The Term Deposit Facility allows Reserve Banks to offer term deposits to institutions that are eligible to receive earnings on their balances at Reserve Banks. Term deposits are intended to facilitate the implementation of monetary policy by providing a tool by which the Federal Reserve can manage the aggregate quantity of reserve balances held by depository institutions. Funds placed in term deposits are removed from the accounts of participating institutions for the life of the term deposit and thus drain reserve balances from the banking system.
Term auction facility.
The Term Auction Facility is a program in which the Federal Reserve auctions term funds to depository institutions. The creation of this facility was announced by the Federal Reserve on December 12, 2007, and was done in conjunction with the Bank of Canada, the Bank of England, the European Central Bank, and the Swiss National Bank to address elevated pressures in short-term funding markets. The reason it was created is that banks were not lending funds to one another and banks in need of funds were refusing to go to the discount window. Banks were not lending money to each other because there was a fear that the loans would not be paid back. Banks refused to go to the discount window because it is usually associated with the stigma of bank failure.
It is also described in the "Term Auction Facility FAQ"
Term securities lending facility.
The Term Securities Lending Facility is a 28-day facility that will offer Treasury general collateral to the Federal Reserve Bank of New York's primary dealers in exchange for other program-eligible collateral. It is intended to promote liquidity in the financing markets for Treasury and other collateral and thus to foster the functioning of financial markets more generally. Like the Term Auction Facility, the TSLF was done in conjunction with the Bank of Canada, the Bank of England, the European Central Bank, and the Swiss National Bank. The resource allows dealers to switch debt that is less liquid for U.S. government securities that are easily tradable. It is anticipated by Federal Reserve officials that the primary dealers, which include Goldman Sachs Group. Inc., J.P. Morgan Chase, and Morgan Stanley, will lend the Treasuries on to other firms in return for cash. That will help the dealers finance their balance sheets. The currency swap lines with the European Central Bank and Swiss National Bank were increased.
Primary dealer credit facility.
The Primary Dealer Credit Facility (PDCF) is an overnight loan facility that will provide funding to primary dealers in exchange for a specified range of eligible collateral and is intended to foster the functioning of financial markets more generally. This new facility marks a fundamental change in Federal Reserve policy because now primary dealers can borrow directly from the Fed when this previously was not permitted.
Interest on reserves.
, the Federal Reserve banks will pay interest on reserve balances (required and excess) held by depository institutions. The rate is set at the lowest federal funds rate during the reserve maintenance period of an institution, less 75bp. As of October 23, 2008, the Fed has lowered the spread to a mere 35 bp.
Term deposit facility.
The Term Deposit Facility is a program through which the Federal Reserve Banks will offer interest-bearing term deposits to eligible institutions. By removing "excess deposits" from participating banks, the overall level of reserves available for lending is reduced, which should result in increased market interest rates, acting as a brake on economic activity and inflation. The Federal Reserve has stated that:
The Federal Reserve initially authorized up to five "small-value offerings are designed to ensure the effectiveness of TDF operations and to provide eligible institutions with an opportunity to gain familiarity with term deposit procedures." After three of the offering auctions were successfully completed, it was announced that small-value auctions would continue on an ongoing basis.
The Term Deposit Facility is essentially a tool available to reverse the efforts that have been employed to provide liquidity to the financial markets and to reduce the amount of capital available to the economy. As stated in Bloomberg News:
Chairman Ben S. Bernanke, testifying before House Committee on Financial Services, described the Term Deposit Facility and other facilities to Congress in the following terms:
Asset Backed Commercial Paper Money Market Mutual Fund Liquidity Facility.
The Asset Backed Commercial Paper Money Market Mutual Fund Liquidity Facility (ABCPMMMFLF) was also called the AMLF. The Facility began operations on September 22, 2008, and was closed on February 1, 2010.
All U.S. depository institutions, bank holding companies (parent companies or U.S. broker-dealer affiliates), or U.S. branches and agencies of foreign banks were eligible to borrow under this facility pursuant to the discretion of the FRBB.
Collateral eligible for pledge under the Facility was required to meet the following criteria:
Commercial Paper Funding Facility.
On October 7, 2008, the Federal Reserve further expanded the collateral it will loan against to include commercial paper using the new Commercial Paper Funding Facility (CPFF). The action made the Fed a crucial source of credit for non-financial businesses in addition to commercial banks and investment firms. Fed officials said they'll buy as much of the debt as necessary to get the market functioning again. They refused to say how much that might be, but they noted that around $1.3 trillion worth of commercial paper would qualify. There was $1.61 trillion in outstanding commercial paper, seasonally adjusted, on the market as of October 1, 2008, according to the most recent data from the Fed. That was down from $1.70 trillion in the previous week. Since the summer of 2007, the market has shrunk from more than $2.2 trillion. This program lent out a total $738 billion before it was closed. Forty-five out of 81 of the companies participating in this program were foreign firms. Research shows that Troubled Asset Relief Program (TARP) recipients were twice as likely to participate in the program than other commercial paper issuers who did not take advantage of the TARP bailout. The Fed incurred no losses from the CPFF.
Quantitative policy.
A little-used tool of the Federal Reserve is the "quantitative policy". With that the Federal Reserve actually buys back corporate bonds and mortgage backed securities held by banks or other financial institutions. This in effect puts money back into the financial institutions and allows them to make loans and conduct normal business. The Federal Reserve Board used this policy in the early 1990s when the U.S. economy experienced the savings and loan crisis.
The bursting of the United States housing bubble prompted the Fed to buy mortgage-backed securities for the first time in November 2008. Over six weeks, a total of $1.25 trillion were purchased in order to stabilize the housing market, about one-fifth of all U.S. government-backed mortgages.
History.
Central banking in the United States, 1791-1813.
The first attempt at a national currency was during the American Revolutionary War. In 1775 the Continental Congress, as well as the states, began issuing paper currency, calling the bills "Continentals". The Continentals were backed only by future tax revenue, and were used to help finance the Revolutionary War. Overprinting, as well as British counterfeiting, caused the value of the Continental to diminish quickly. This experience with paper money led the United States to strip the power to issue Bills of Credit (paper money) from a draft of the new Constitution on August 16, 1787, as well as banning such issuance by the various states, and limiting the states' ability to make anything but gold or silver coin legal tender on August 28.
In 1791, the government granted the First Bank of the United States a charter to operate as the U.S. central bank until 1811. The First Bank of the United States came to an end under President Madison because Congress refused to renew its charter. The Second Bank of the United States was established in 1816, and lost its authority to be the central bank of the U.S. twenty years later under President Jackson when its charter expired. Both banks were based upon the Bank of England. Ultimately, a third national bank, known as the Federal Reserve, was established in 1913 and still exists to this day.
First Central Bank, 1791 and Second Central Bank, 1816.
The first U.S. institution with central banking responsibilities was the First Bank of the United States, chartered by Congress and signed into law by President George Washington on February 25, 1791, at the urging of Alexander Hamilton. This was done despite strong opposition from Thomas Jefferson and James Madison, among numerous others. The charter was for twenty years and expired in 1811 under President Madison, because Congress refused to renew it.
In 1816, however, Madison revived it in the form of the Second Bank of the United States. Years later, early renewal of the bank's charter became the primary issue in the reelection of President Andrew Jackson. After Jackson, who was opposed to the central bank, was reelected, he pulled the government's funds out of the bank. Nicholas Biddle, President of the Second Bank of the United States, responded by contracting the money supply to pressure Jackson to renew the bank's charter forcing the country into a recession, which the bank blamed on Jackson's policies. Jackson was the only President to completely pay off the debt. The bank's charter was not renewed in 1836.
From 1837 to 1862, in the Free Banking Era there was no formal central bank.
From 1846 to 1921 an Independent Treasury System ruled.
From 1863 to 1913, a system of national banks was instituted by the 1863 National Banking Act during which series of bank panics, in 1873, 1893, and 1907 occurred
Creation of Third Central Bank, 1907-1913.
The main motivation for the third central banking system came from the Panic of 1907, which caused renewed demands for banking and currency reform. During the last quarter of the 19th century and the beginning of the 20th century the United States economy went through a series of financial panics. According to many economists, the previous national banking system had two main weaknesses: an inelastic currency and a lack of liquidity. In 1908, Congress enacted the Aldrich–Vreeland Act, which provided for an emergency currency and established the National Monetary Commission to study banking and currency reform. The National Monetary Commission returned with recommendations which were repeatedly rejected by Congress. A revision crafted during a secret meeting on Jekyll Island by Senator Aldrich and representatives of the nation's top finance and industrial groups later became the basis of the Federal Reserve Act. The House voted on December 22, 1913, with 298 yeas to 60 nays, and the Senate voted 4325 on December 23, 1913. President Woodrow Wilson signed the bill later that day.
Federal Reserve Act, 1913.
The head of the bipartisan National Monetary Commission was financial expert and Senate Republican leader Nelson Aldrich. Aldrich set up two commissions – one to study the American monetary system in depth and the other, headed by Aldrich himself, to study the European central banking systems and report on them. Aldrich went to Europe opposed to centralized banking, but after viewing Germany's monetary system he came away believing that a centralized bank was better than the government-issued bond system that he had previously supported.
In early November 1910, Aldrich met with five well known members of the New York banking community to devise a central banking bill. Paul Warburg, an attendee of the meeting and longtime advocate of central banking in the U.S., later wrote that Aldrich was "bewildered at all that he had absorbed abroad and he was faced with the difficult task of writing a highly technical bill while being harassed by the daily grind of his parliamentary duties". After ten days of deliberation, the bill, which would later be referred to as the "Aldrich Plan", was agreed upon. It had several key components, including a central bank with a Washington-based headquarters and fifteen branches located throughout the U.S. in geographically strategic locations, and a uniform elastic currency based on gold and commercial paper. Aldrich believed a central banking system with no political involvement was best, but was convinced by Warburg that a plan with no public control was not politically feasible. The compromise involved representation of the public sector on the Board of Directors.
Aldrich's bill met much opposition from politicians. Critics charged Aldrich of being biased due to his close ties to wealthy bankers such as J. P. Morgan and John D. Rockefeller, Jr., Aldrich's son-in-law. Most Republicans favored the Aldrich Plan, but it lacked enough support in Congress to pass because rural and western states viewed it as favoring the "eastern establishment". In contrast, progressive Democrats favored a reserve system owned and operated by the government; they believed that public ownership of the central bank would end Wall Street's control of the American currency supply. Conservative Democrats fought for a privately owned, yet decentralized, reserve system, which would still be free of Wall Street's control.
The original Aldrich Plan was dealt a fatal blow in 1912, when Democrats won the White House and Congress. Nonetheless, President Woodrow Wilson believed that the Aldrich plan would suffice with a few modifications. The plan became the basis for the Federal Reserve Act, which was proposed by Senator Robert Owen in May 1913. The primary difference between the two bills was the transfer of control of the Board of Directors (called the Federal Open Market Committee in the Federal Reserve Act) to the government. The bill passed Congress on December 23, 1913, on a mostly partisan basis, with most Democrats voting "yea" and most Republicans voting "nay".
Federal Reserve era, 1913-present.
Key laws affecting the Federal Reserve have been:
Measurement of economic variables.
The Federal Reserve records and publishes large amounts of data. A few websites where data is published are at the Board of Governors Economic Data and Research page, the Board of Governors statistical releases and historical data page, and at the St. Louis Fed's FRED (Federal Reserve Economic Data) page. The Federal Open Market Committee (FOMC) examines many economic indicators prior to determining monetary policy.
Some criticism involves economic data compiled by the Fed. The Fed sponsors much of the monetary economics research in the U.S., and Lawrence H. White objects that this makes it less likely for researchers to publish findings challenging the status quo.
Net worth of households and nonprofit organizations.
The net worth of households and nonprofit organizations in the United States is published by the Federal Reserve in a report titled "Flow of Funds". At the end of the third quarter of fiscal year 2012, this value was $64.8 trillion. At the end of the first quarter of fiscal year 2014, this value was $95.5 trillion.
Money supply.
The most common measures are named M0 (narrowest), M1, M2, and M3. In the United States they are defined by the Federal Reserve as follows:
The Federal Reserve stopped publishing M3 statistics in March 2006, saying that the data cost a lot to collect but did not provide significantly useful information. The other three money supply measures continue to be provided in detail.
Personal consumption expenditures price index.
The Personal consumption expenditures price index, also referred to as simply the PCE price index, is used as one measure of the value of money. It is a United States-wide indicator of the average increase in prices for all domestic personal consumption. Using a variety of data including United States Consumer Price Index and U.S. Producer Price Index prices, it is derived from the largest component of the Gross Domestic Product in the BEA's National Income and Product Accounts, personal consumption expenditures.
One of the Fed's main roles is to maintain price stability, which means that the Fed's ability to keep a low inflation rate is a long-term measure of their success. Although the Fed is not required to maintain inflation within a specific range, their long run target for the growth of the PCE price index is between 1.5 and 2 percent. There has been debate among policy makers as to whether the Federal Reserve should have a specific inflation targeting policy.
Inflation and the economy.
Most mainstream economists favor a low, steady rate of inflation. Low (as opposed to zero or negative) inflation may reduce the severity of economic recessions by enabling the labor market to adjust more quickly in a downturn, and reduce the risk that a liquidity trap prevents monetary policy from stabilizing the economy. The task of keeping the rate of inflation low and stable is usually given to monetary authorities.
Unemployment rate.
One of the stated goals of monetary policy is maximum employment. The unemployment rate statistics are collected by the Bureau of Labor Statistics, and like the PCE price index are used as a barometer of the nation's economic health.
Budget.
The Federal Reserve is self-funded. The vast majority (90%+) of Fed revenues come from open market operations, specifically the interest on the portfolio of Treasury securities as well as "capital gains/losses" that may arise from the buying/selling of the securities and their derivatives as part of Open Market Operations. The balance of revenues come from sales of financial services (check and electronic payment processing) and discount window loans. The Board of Governors (Federal Reserve Board) creates a budget report once per year for Congress. There are two reports with budget information. The one that lists the complete balance statements with income and expenses as well as the net profit or loss is the large report simply titled, "Annual Report". It also includes data about employment throughout the system. The other report, which explains in more detail the expenses of the different aspects of the whole system, is called "Annual Report: Budget Review". These are comprehensive reports with many details and can be found at the Board of Governors' website under the section "Reports to Congress"
Net worth.
Balance sheet.
One of the keys to understanding the Federal Reserve is the Federal Reserve balance sheet (or balance statement). In accordance with Section 11 of the Federal Reserve Act, the Board of Governors of the Federal Reserve System publishes once each week the "Consolidated Statement of Condition of All Federal Reserve Banks" showing the condition of each Federal Reserve bank and a consolidated statement for all Federal Reserve banks. The Board of Governors requires that excess earnings of the Reserve Banks be transferred to the Treasury as interest on Federal Reserve notes.
Below is the balance sheet as of July 6, 2011 (in billions of dollars):
NOTE: The Fed balance sheet shown in this article has assets, liabilities and net equity that do not add up correctly. The Fed balance sheet is missing the item "Reserve Balances with Federal Reserve Banks" which would make the figures balance.
In addition, the balance sheet also indicates which assets are held as collateral against Federal Reserve Notes.
Criticism.
The Federal Reserve System has faced various criticisms since its inception in 1913. Critique of the organization and system has come from sources such as writers, journalists, economists, and financial institutions as well as politicians and various government employees. Criticisms include doubt of efficacy due to what is seen by some as poor historical performance and populist concerns about the debasement of the value of the dollar.

</doc>
<doc id="10821" url="https://en.wikipedia.org/wiki?curid=10821" title="Francium">
Francium

Francium is a chemical element with symbol Fr and atomic number 87. It used to be known as eka-caesium and actinium K. It is the second-least electronegative element, behind only caesium. Francium is a highly radioactive metal that decays into astatine, radium, and radon. As an alkali metal, it has one valence electron.
Bulk francium has never been viewed. Because of the general appearance of the other elements in its periodic table column, it is assumed that francium would appear as a highly reflective metal, if enough could be collected together to be viewed as a bulk solid or liquid. Obtaining such a sample is highly improbable, since the extreme heat of decay (the half-life of its longest-lived isotope is only 22 minutes) would immediately vaporize any viewable quantity of the element.
Francium was discovered by Marguerite Perey in France (from which the element takes its name) in 1939. It was the last element first discovered in nature, rather than by synthesis. Outside the laboratory, francium is extremely rare, with trace amounts found in uranium and thorium ores, where the isotope francium-223 continually forms and decays. As little as 20–30 g (one ounce) exists at any given time throughout the Earth's crust; the other isotopes (except for francium-221) are entirely synthetic. The largest amount produced in the laboratory was a cluster of more than 300,000 atoms.
Characteristics.
Francium is the most unstable of the naturally occurring elements: its most stable isotope, francium-223, has a half-life of only 22 minutes. In contrast, astatine, the second-least stable naturally occurring element, has a half-life of 8.5 hours. All isotopes of francium decay into astatine, radium, or radon. Francium is also less stable than all synthetic elements up to element 105.
Francium is an alkali metal whose chemical properties mostly resemble those of caesium. A heavy element with a single valence electron, it has the highest equivalent weight of any element. Liquid francium—if created—should have a surface tension of 0.05092 N/m at its melting point. Francium's melting point was calculated to be around 27 °C (80 °F, 300 K). The melting point is uncertain because of the element's extreme rarity and radioactivity. Thus, the estimated boiling point value of 677 °C (1250 °F, 950 K) is also uncertain.
Linus Pauling estimated the electronegativity of francium at 0.7 on the Pauling scale, the same as caesium; the value for caesium has since been refined to 0.79, but there are no experimental data to allow a refinement of the value for francium. Francium has a slightly higher ionization energy than caesium, 392.811(4) kJ/mol as opposed to 375.7041(2) kJ/mol for caesium, as would be expected from relativistic effects, and this would imply that caesium is the less electronegative of the two. Francium should also have a higher electron affinity than caesium and the Fr ion should be more polarizable than the Cs ion. The CsFr molecule is predicted to have francium at the negative end of the dipole, unlike all known heterodiatomic alkali metal molecules. Francium superoxide (FrO) is expected to have a more covalent character than its lighter congeners; this is attributed to the 6p electrons in francium being more involved in the francium–oxygen bonding.
Francium coprecipitates with several caesium salts, such as caesium perchlorate, which results in small amounts of francium perchlorate. This coprecipitation can be used to isolate francium, by adapting the radiocaesium coprecipitation method of Glendenin and Nelson. It will additionally coprecipitate with many other caesium salts, including the iodate, the picrate, the tartrate (also rubidium tartrate), the chloroplatinate, and the silicotungstate. It also coprecipitates with silicotungstic acid, and with perchloric acid, without another alkali metal as a carrier, which provides other methods of separation. Nearly all francium salts are water-soluble.
Isotopes.
There are 34 known isotopes of francium ranging in atomic mass from 199 to 232. Francium has seven metastable nuclear isomers. Francium-223 and francium-221 are the only isotopes that occur in nature, though the former is far more common.
Francium-223 is the most stable isotope, with a half-life of 21.8 minutes, and it is highly unlikely that an isotope of francium with a longer half-life will ever be discovered or synthesized. Francium-223 is the fifth product of the actinium decay series as the daughter isotope of actinium-227. Francium-223 then decays into radium-223 by beta decay (1149 keV decay energy), with a minor (0.006%) alpha decay path to astatine-219 (5.4 MeV decay energy).
Francium-221 has a half-life of 4.8 minutes. It is the ninth product of the neptunium decay series as a daughter isotope of actinium-225. Francium-221 then decays into astatine-217 by alpha decay (6.457 MeV decay energy).
The least stable ground state isotope is francium-215, with a half-life of 0.12 μs. (9.54 MeV alpha decay to astatine-211): Its metastable isomer, francium-215m, is less stable still, with a half-life of only 3.5 ns.
Applications.
Due to its instability and rarity, there are no commercial applications for francium. It has been used for research purposes in the fields of chemistry
and of atomic structure. Its use as a potential diagnostic aid for various cancers has also been explored, but this application has been deemed impractical.
Francium's ability to be synthesized, trapped, and cooled, along with its relatively simple atomic structure have made it the subject of specialized spectroscopy experiments. These experiments have led to more specific information regarding energy levels and the coupling constants between subatomic particles. Studies on the light emitted by laser-trapped francium-210 ions have provided accurate data on transitions between atomic energy levels which are fairly similar to those predicted by quantum theory.
History.
As early as 1870, chemists thought that there should be an alkali metal beyond caesium, with an atomic number of 87. It was then referred to by the provisional name "eka-caesium". Research teams attempted to locate and isolate this missing element, and at least four false claims were made that the element had been found before an authentic discovery was made.
Erroneous and incomplete discoveries.
Soviet chemist D. K. Dobroserdov was the first scientist to claim to have found eka-caesium, or francium. In 1925, he observed weak radioactivity in a sample of potassium, another alkali metal, and incorrectly concluded that eka-caesium was contaminating the sample (the radioactivity from the sample was from the naturally occurring potassium radioisotope, potassium-40). He then published a thesis on his predictions of the properties of eka-caesium, in which he named the element "russium" after his home country. Shortly thereafter, Dobroserdov began to focus on his teaching career at the Polytechnic Institute of Odessa, and he did not pursue the element further.
The following year, English chemists Gerald J. F. Druce and Frederick H. Loring analyzed X-ray photographs of manganese(II) sulfate. They observed spectral lines which they presumed to be of eka-caesium. They announced their discovery of element 87 and proposed the name "alkalinium", as it would be the heaviest alkali metal.
In 1930, Fred Allison of the Alabama Polytechnic Institute claimed to have discovered element 87 when analyzing pollucite and lepidolite using his magneto-optical machine. Allison requested that it be named "virginium" after his home state of Virginia, along with the symbols Vi and Vm. In 1934, H.G. MacPherson of UC Berkeley disproved the effectiveness of Allison's device and the validity of this false discovery.
In 1936, Romanian physicist Horia Hulubei and his French colleague Yvette Cauchois also analyzed pollucite, this time using their high-resolution X-ray apparatus. They observed several weak emission lines, which they presumed to be those of element 87. Hulubei and Cauchois reported their discovery and proposed the name "moldavium", along with the symbol Ml, after Moldavia, the Romanian province where Hulubei was born. In 1937, Hulubei's work was criticized by American physicist F. H. Hirsh Jr., who rejected Hulubei's research methods. Hirsh was certain that eka-caesium would not be found in nature, and that Hulubei had instead observed mercury or bismuth X-ray lines. Hulubei insisted that his X-ray apparatus and methods were too accurate to make such a mistake. Because of this, Jean Baptiste Perrin, Nobel Prize winner and Hulubei's mentor, endorsed moldavium as the true eka-caesium over Marguerite Perey's recently discovered francium. Perey took pains to be accurate and detailed in her criticism of Hulubei's work, and finally she was credited as the sole discoverer of element 87. All other previous purported discoveries of element 87 were ruled out due to francium's very limited half-life.
Perey's analysis.
Eka-caesium was discovered in 1939 by Marguerite Perey of the Curie Institute in Paris, when she purified a sample of actinium-227 which had been reported to have a decay energy of 220 keV. Perey noticed decay particles with an energy level below 80 keV. Perey thought this decay activity might have been caused by a previously unidentified decay product, one which was separated during purification, but emerged again out of the pure actinium-227. Various tests eliminated the possibility of the unknown element being thorium, radium, lead, bismuth, or thallium. The new product exhibited chemical properties of an alkali metal (such as coprecipitating with caesium salts), which led Perey to believe that it was element 87, caused by the alpha decay of actinium-227. Perey then attempted to determine the proportion of beta decay to alpha decay in actinium-227. Her first test put the alpha branching at 0.6%, a figure which she later revised to 1%.
Perey named the new isotope "actinium-K" (now referred to as francium-223) and in 1946, she proposed the name "catium" for her newly discovered element, as she believed it to be the most electropositive cation of the elements. Irène Joliot-Curie, one of Perey's supervisors, opposed the name due to its connotation of "cat" rather than "cation". Perey then suggested "francium", after France. This name was officially adopted by the International Union of Pure and Applied Chemistry in 1949, becoming the second element after gallium to be named after France. It was assigned the symbol Fa, but this abbreviation was revised to the current Fr shortly thereafter. Francium was the last element discovered in nature, rather than synthesized, following rhenium in 1925. Further research into francium's structure was carried out by, among others, Sylvain Lieberman and his team at CERN in the 1970s and 1980s.
Occurrence.
Natural.
Fr is the result of the alpha decay of Ac and can be found in trace amounts in uranium and thorium minerals. In a given sample of uranium, there is estimated to be only one francium atom for every 1 × 10 uranium atoms. It is also calculated that there is at most 30 g of francium in the Earth's crust at any given time.
Synthesis.
Francium can be synthesized in the nuclear reaction:
This process, developed by Stony Brook Physics, yields francium isotopes with masses of 209, 210, and 211, which are then isolated by the magneto-optical trap (MOT). The production rate of a particular isotope depends on the energy of the oxygen beam. An O beam from the Stony Brook LINAC creates Fr in the gold target with the nuclear reaction Au + O → Fr + 5n. The production required some time to develop and understand. It was critical to operate the gold target very close to its melting point and to make sure that its surface was very clean. The nuclear reaction embeds the francium atoms deep in the gold target, and they must be removed efficiently. The atoms quickly diffuse to the surface of the gold target and are released as ions, but this does not happen every time. The francium ions are guided by electrostatic lenses until they land in a surface of hot yttrium and become neutral again. The francium is then injected into a glass bulb. A magnetic field and laser beams cool and confine the atoms. Although the atoms remain in the trap for only about 20 seconds before escaping (or decaying), a steady stream of fresh atoms replaces those lost, keeping the number of trapped atoms roughly constant for minutes or longer. Initially, about 1000 francium atoms were trapped in the experiment. This was gradually improved and the setup is capable of trapping over 300,000 neutral atoms of francium a time. These are neutral metallic atoms in a gaseous unconsolidated state. Enough francium is trapped that a video camera can capture the light given off by the atoms as they fluoresce. The atoms appear as a glowing sphere about 1 millimeter in diameter. This was the first time that anyone had ever seen francium. The researchers can now make extremely sensitive measurements of the light emitted and absorbed by the trapped atoms, providing the first experimental results on various transitions between atomic energy levels in francium. Initial measurements show very good agreement between experimental values and calculations based on quantum theory. Other synthesis methods include bombarding radium with neutrons, and bombarding thorium with protons, deuterons, or helium ions. Francium has not been synthesized in amounts large enough to weigh.

</doc>
<doc id="10822" url="https://en.wikipedia.org/wiki?curid=10822" title="Fermium">
Fermium

Fermium is a synthetic element with symbol Fm and atomic number 100. It is a member of the actinide series. It is the heaviest element that can be formed by neutron bombardment of lighter elements, and hence the last element that can be prepared in macroscopic quantities, although pure fermium metal has not yet been prepared. A total of 19 isotopes are known, with Fm being the longest-lived with a half-life of 100.5 days.
It was discovered in the debris of the first hydrogen bomb explosion in 1952, and named after Enrico Fermi, one of the pioneers of nuclear physics. Its chemistry is typical for the late actinides, with a preponderance of the +3 oxidation state but also an accessible +2 oxidation state. Owing to the small amounts of produced fermium and all of its isotopes having relatively short half-lives, there are currently no uses for it outside of basic scientific research.
Discovery.
Fermium was first discovered in the fallout from the 'Ivy Mike' nuclear test (1 November 1952), the first successful test of a hydrogen bomb. Initial examination of the debris from the explosion had shown the production of a new isotope of plutonium, : this could only have formed by the absorption of six neutrons by a uranium-238 nucleus followed by two β decays. At the time, the absorption of neutrons by a heavy nucleus was thought to be a rare process, but the identification of raised the possibility that still more neutrons could have been absorbed by the uranium nuclei, leading to new elements.
Element 99 (einsteinium) was quickly discovered on filter papers which had been flown through the cloud from the explosion (the same sampling technique that had been used to discover ). It was then identified in December 1952 by Albert Ghiorso and co-workers at the University of California at Berkeley. They discovered the isotope Es (half-life 20.5 days) that was made by the capture of 15 neutrons by uranium-238 nuclei – which then underwent seven successive beta decays:
Some U atoms, however, could capture another amount of neutrons (most likely, 16 or 17).
The discovery of fermium (Z = 100) required more material, as the yield was expected to be at least an order of magnitude lower than that of element 99, and so contaminated coral from the Enewetak atoll (where the test had taken place) was shipped to the University of California Radiation Laboratory in Berkeley, California, for processing and analysis. About two months after the test, a new component was isolated emitting high-energy α-particles (7.1 MeV) with a half-life of about a day. With such a short half-life, it could only arise from the β decay of an isotope of einsteinium, and so had to be an isotope of the new element 100: it was quickly identified as Fm (t</sub> = 20.07(7) hours).
The discovery of the new elements, and the new data on neutron capture, was initially kept secret on the orders of the U.S. military until 1955 due to Cold War tensions. Nevertheless, the Berkeley team were able to prepare elements 99 and 100 by civilian means, through the neutron bombardment of plutonium-239, and published this work in 1954 with the disclaimer that it was not the first studies that had been carried out on the elements. The 'Ivy Mike' studies were declassified and published in 1955.
The Berkeley team had been worried that another group might discover lighter isotopes of element 100 through ion bombardment techniques before they could publish their classified research, and this proved to be the case. A group at the Nobel Institute for Physics in Stockholm independently discovered the element, producing an isotope later confirmed to be Fm (t = 30 minutes) by bombarding a target with oxygen-16 ions, and published their work in May 1954. Nevertheless, the priority of the Berkeley team was generally recognized, and with it the prerogative to name the new element in honour of the recently deceased Enrico Fermi, the developer of the first artificial self-sustained nuclear reactor.
Isotopes.
There are 19 isotopes of fermium listed in N 2003, with atomic weights of 242 to 260, of which Fm is the longest-lived with a half-life of 100.5 days. Fm has a half-life of 3 days, while Fm of 5.3 h, Fm of 25.4 h, Fm of 3.2 h, Fm of 20.1 h, and Fm of 2.6 hours. All the remaining ones have half-lives ranging from 30 minutes to less than a millisecond.
The neutron-capture product of fermium-257, Fm, undergoes spontaneous fission with a half-life of just 370(14) microseconds; Fm and Fm are also unstable with respect to spontaneous fission (t = 1.5(3) s and 4 ms respectively). This means that neutron capture cannot be used to create nuclides with a mass number greater than 257, unless carried out in a nuclear explosion. As Fm is an α-emitter, decaying to Cf, and no fermium isotopes undergo beta minus decay (which would produce isotopes of the next element, mendelevium), fermium is also the last element that can be prepared by a neutron-capture process.
Production.
Fermium is produced by the bombardment of lighter actinides with neutrons in a nuclear reactor. Fermium-257 is the heaviest isotope that is obtained via neutron capture, and can only be produced in nanogram quantities. The major source is the 85 MW High Flux Isotope Reactor (HFIR) at the Oak Ridge National Laboratory in Tennessee, USA, which is dedicated to the production of transcurium ("Z" > 96) elements. In a "typical processing campaign" at Oak Ridge, tens of grams of curium are irradiated to produce decigram quantities of californium, milligram quantities of berkelium and einsteinium and picogram quantities of fermium. However, nanogram and microgram quantities of fermium can be prepared for specific experiments. The quantities of fermium produced in 20–200 kiloton thermonuclear explosions is believed to be of the order of milligrams, although it is mixed in with a huge quantity of debris; 40 picograms of Fm was recovered from 10 kilograms of debris from the 'Hutch' test (16 July 1969).
After production, the fermium must be separated from other actinides and from lanthanoid fission products. This is usually achieved by ion exchange chromatography, with the standard process using a cation exchanger such as Dowex 50 or T eluted with a solution of ammonium α-hydroxyisobutyrate. Smaller cations form more stable complexes with the α-hydroxyisobutyrate anion, and so are preferentially eluted from the column. A rapid fractional crystallization method has also been described.
Although the most stable isotope of fermium is Fm, with a half-life of 100.5 days, most studies are conducted on Fm (t = 20.07(7) hours) as this isotope can be easily isolated as required as the decay product of Es (t = 39.8(12) days).
Synthesis in nuclear explosions.
The analysis of the debris at the 10-megaton "Ivy Mike" nuclear test was a part of long-term project, one of the goals of which was studying the efficiency of production of transuranium elements in high-power nuclear explosions. The motivation for these experiments was as follows: synthesis of such elements from uranium requires multiple neutron capture. The probability of such events increases with the neutron flux, and nuclear explosions are the most powerful neutron sources, providing densities of the order 10 neutrons/cm within a microsecond, i.e. about 10 neutrons/(cm·s). In comparison, the flux of the HFIR reactor is 5 neutrons/(cm·s). A dedicated laboratory was set up right at Enewetak Atoll for preliminary analysis of debris, as some isotopes could have decayed by the time the debris samples reached the U.S. The laboratory was receiving samples for analysis, as soon as possible, from airplanes equipped with paper filters which flew over the atoll after the tests. Whereas it was hoped to discover new chemical elements heavier than fermium, those were not found after a series of megaton explosions conducted between 1954 and 1956 at the atoll.
The atmospheric results were supplemented by the underground test data accumulated in the 1960s at the Nevada Test Site, as it was hoped that powerful explosions conducted in confined space might result in improved yields and heavier isotopes. Apart from traditional uranium charges, combinations of uranium with americium and thorium have been tried, as well as a mixed plutonium-neptunium charge. They were less successful in terms of yield that was attributed to stronger losses of heavy isotopes due to enhanced fission rates in heavy-element charges. Isolation of the products was found to be rather problematic, as the explosions were spreading debris through melting and vaporizing rocks under the great depth of 300–600 meters, and drilling to such depth in order to extract the products was both slow and inefficient in terms of collected volumes.
Among the nine underground tests, which were carried between 1962 and 1969 and codenamed Anacostia (5.2 kilotons, 1962), Kennebec (<5 kilotons, 1963), Par (38, kilotons, 1964), Barbel (<20 kilotons, 1964), Tweed (<20 kilotons, 1965), Cyclamen (13 kilotons, 1966), Kankakee (20-200 kilotons, 1966), Vulcan (25 kilotons, 1966) and Hutch (20-200 kilotons, 1969), the last one was most powerful and had the highest yield of transuranium elements. In the dependence on the atomic mass number, the yield showed a saw-tooth behavior with the lower values for odd isotopes, due to their higher fission rates. The major practical problem of the entire proposal was however collecting the radioactive debris dispersed by the powerful blast. Aircraft filters adsorbed only about 4 of the total amount and collection of tons of corals at Enewetak Atoll increased this fraction by only two orders of magnitude. Extraction of about 500 kilograms of underground rocks 60 days after the Hutch explosion recovered only about 10 of the total charge. The amount of transuranium elements in this 500-kg batch was only 30 times higher than in a 0.4 kg rock picked up 7 days after the test. This observation demonstrated the highly nonlinear dependence of the transuranium elements yield on the amount of retrieved radioactive rock. In order to accelerate sample collection after explosion, shafts were drilled at the site not after but before the test, so that explosion would expel radioactive material from the epicenter, through the shafts, to collecting volumes near the surface. This method was tried in the Anacostia and Kennebec tests and instantly provided hundreds kilograms of material, but with actinide concentration 3 times lower than in samples obtained after drilling; whereas such method could have been efficient in scientific studies of short-lived isotopes, it could not improve the overall collection efficiency of the produced actinides.
Although no new elements (apart from einsteinium and fermium) could be detected in the nuclear test debris, and the total yields of transuranium elements were disappointingly low, these tests did provide significantly higher amounts of rare heavy isotopes than previously available in laboratories. So 6 atoms of Fm could be recovered after the Hutch detonation. They were then used in the studies of thermal-neutron induced fission of Fm and in discovery of a new fermium isotope Fm. Also, the rare Cm isotope was synthesized in large quantities, which is very difficult to produce in nuclear reactors from its progenitor Cm – the half-life of Cm (64 minutes) is much too short for months-long reactor irradiations, but is very "long" on the explosion timescale.
Natural occurrence.
Because of the short half-life of all isotopes of fermium, any primordial fermium, that is fermium that could be present on the Earth during its formation, has decayed by now. Synthesis of fermium from naturally occurring actinides uranium and thorium in the Earth crust requires multiple neutron capture, which is an extremely unlikely event. Therefore, most fermium is produced on Earth in scientific laboratories, high-power nuclear reactors, or in nuclear weapons tests, and is present only within a few months from the time of the synthesis. The transuranic elements from americium to fermium did occur naturally in the natural nuclear fission reactor at Oklo, but no longer do so.
Chemistry.
The chemistry of fermium has only been studied in solution using tracer techniques, and no solid compounds have been prepared. Under normal conditions, fermium exists in solution as the Fm ion, which has a hydration number of 16.9 and an acid dissociation constant of 1.6 (p"K" = 3.8). Fm forms complexes with a wide variety of organic ligands with hard donor atoms such as oxygen, and these complexes are usually more stable than those of the preceding actinides. It also forms anionic complexes with ligands such as chloride or nitrate and, again, these complexes appear to be more stable than those formed by einsteinium or californium. It is believed that the bonding in the complexes of the later actinides is mostly ionic in character: the Fm ion is expected to be smaller than the preceding An ions because of the higher effective nuclear charge of fermium, and hence fermium would be expected to form shorter and stronger metal–ligand bonds.
Fermium(III) can be fairly easily reduced to fermium(II), for example with samarium(II) chloride, with which fermium coprecipitates. The electrode potential has been estimated to be similar to that of the ytterbium(III)/(II) couple, or about −1.15 V with respect to the standard hydrogen electrode, a value which agrees with theoretical calculations. The Fm/Fm couple has an electrode potential of −2.37(10) V based on polarographic measurements.
Toxicity.
Although few people come in contact with fermium, the International Commission on Radiological Protection has set annual exposure limits for the two most stable isotopes. For fermium-253, the ingestion limit was set at 10 Becquerels (1 Bq is equivalent to one decay per second), and the inhalation limit at 10 Bq; for fermium-257, at 10 Bq and 4000 Bq respectively.

</doc>
<doc id="10823" url="https://en.wikipedia.org/wiki?curid=10823" title="Frédéric Chopin">
Frédéric Chopin

Frédéric François Chopin (; ; 1 March 181017 October 1849), born Fryderyk Franciszek Chopin, was a Polish and French (by birth of father and citizenship) composer and a virtuoso pianist of the Romantic era, who wrote primarily for the solo piano. He gained and has maintained renown worldwide as one of the leading musicians of his era, whose "poetic genius was based on a professional technique that was without equal in his generation." Chopin was born in what was then the Duchy of Warsaw, and grew up in Warsaw, which after 1815 became part of Congress Poland. A child prodigy, he completed his musical education and composed his earlier works in Warsaw before leaving Poland at the age of 20, less than a month before the outbreak of the November 1830 Uprising. 
At the age of 21 he settled in Paris. Thereafter, during the last 18 years of his life, he gave only some 30 public performances, preferring the more intimate atmosphere of the salon. He supported himself by selling his compositions and teaching piano, for which he was in high demand. Chopin formed a friendship with Franz Liszt and was admired by many of his musical contemporaries, including Robert Schumann. In 1835 he obtained French citizenship. After a failed engagement to Maria Wodzińska, from 1837 to 1847 he maintained an often troubled relationship with the French writer George Sand. A brief and unhappy visit to Majorca with Sand in 1838–39 was one of his most productive periods of composition. In his last years, he was financially supported by his admirer Jane Stirling, who also arranged for him to visit Scotland in 1848. Through most of his life, Chopin suffered from poor health. He died in Paris in 1849, probably of tuberculosis.
All of Chopin's compositions include the piano. Most are for solo piano, though he also wrote two piano concertos, a few chamber pieces, and some songs to Polish lyrics. His keyboard style is highly individual and often technically demanding; his own performances were noted for their nuance and sensitivity. Chopin invented the concept of instrumental ballade. His major piano works also include mazurkas, waltzes, nocturnes, polonaises, études, impromptus, scherzos, preludes and sonatas, some published only after his death. Influences on his compositional style include Polish folk music, the classical tradition of J. S. Bach, Mozart and Schubert, the music of all of whom he admired, as well as the Paris salons where he was a frequent guest. His innovations in style, musical form, and harmony, and his association of music with nationalism, were influential throughout and after the late Romantic period.
In his native Poland, in France, where he composed most of his works, and beyond, Chopin's music, his status as one of music's earliest superstars, his association (if only indirect) with political insurrection, his love life and his early death have made him, in the public consciousness, a leading symbol of the Romantic era. His works remain popular, and he has been the subject of numerous films and biographies of varying degrees of historical accuracy.
Life.
Childhood.
Fryderyk Chopin was born in Żelazowa Wola, 46 kilometres () west of Warsaw, in what was then the Duchy of Warsaw, a Polish state established by Napoleon. The parish baptismal record gives his birthday as 22 February 1810, and cites his given names in the Latin form "Fridericus Franciscus" (in Polish, he was "Fryderyk Franciszek"). However, the composer and his family used the birthdate 1 March, which is now generally accepted as the correct date.
Fryderyk's father, Nicolas Chopin, was a Frenchman from Lorraine who had emigrated to Poland in 1787 at the age of sixteen. Nicolas tutored children of the Polish aristocracy, and in 1806 married Justyna Krzyżanowska, a poor relative of the Skarbeks, one of the families for whom he worked. Fryderyk was baptized on Easter Sunday, 23 April 1810, in the same church where his parents had married, in Brochów. His eighteen-year-old godfather, for whom he was named, was Fryderyk Skarbek, a pupil of Nicolas Chopin. Fryderyk was the couple's second child and only son; he had an elder sister, Ludwika (1807–55), and two younger sisters, Izabela (1811–81) and Emilia (1812–27). Nicolas was devoted to his adopted homeland, and insisted on the use of the Polish language in the household.
In October 1810, six months after Fryderyk's birth, the family moved to Warsaw, where his father acquired a post teaching French at the Warsaw Lyceum, then housed in the Saxon Palace. Fryderyk lived with his family in the Palace grounds. The father played the flute and violin; the mother played the piano and gave lessons to boys in the boarding house that the Chopins kept. Chopin was of slight build, and even in early childhood was prone to illnesses.
Fryderyk may have had some piano instruction from his mother, but his first professional music tutor, from 1816 to 1821, was the Czech pianist Wojciech Żywny. His elder sister Ludwika also took lessons from Żywny, and occasionally played duets with her brother. It quickly became apparent that he was a child prodigy. By the age of seven Fryderyk had begun giving public concerts, and in 1817 he composed two polonaises, in G minor and B-flat major. His next work, a polonaise in A-flat major of 1821, dedicated to Żywny, is his earliest surviving musical manuscript.
In 1817 the Saxon Palace was requisitioned by Warsaw's Russian governor for military use, and the Warsaw Lyceum was reestablished in the Kazimierz Palace (today the rectorate of Warsaw University). Fryderyk and his family moved to a building, which still survives, adjacent to the Kazimierz Palace. During this period, Fryderyk was sometimes invited to the Belweder Palace as playmate to the son of the ruler of Russian Poland, Grand Duke Constantine; he played the piano for the Duke and composed a march for him. Julian Ursyn Niemcewicz, in his dramatic eclogue, "Nasze Przebiegi" ("Our Discourses", 1818), attested to "little Chopin's" popularity.
Education.
From September 1823 to 1826 Chopin attended the Warsaw Lyceum, where he received organ lessons from the Czech musician Wilhelm Würfel during his first year. In the autumn of 1826 he began a three-year course under the Silesian composer Józef Elsner at the Warsaw Conservatory, studying music theory, figured bass and composition. Throughout this period he continued to compose and to give recitals in concerts and salons in Warsaw. He was engaged by the inventors of a mechanical organ, the "eolomelodicon", and on this instrument in May 1825 he performed his own improvisation and part of a concerto by Moscheles. The success of this concert led to an invitation to give a similar recital on the instrument before Tsar Alexander I, who was visiting Warsaw; the Tsar presented him with a diamond ring. At a subsequent eolomelodicon concert on 10 June 1825, Chopin performed his Rondo Op. 1. This was the first of his works to be commercially published and earned him his first mention in the foreign press, when the Leipzig "Allgemeine Musikalische Zeitung" praised his "wealth of musical ideas".
During 1824–28 Chopin spent his vacations away from Warsaw, at a number of locales. In 1824 and 1825, at Szafarnia, he was a guest of Dominik Dziewanowski, the father of a schoolmate. Here for the first time he encountered Polish rural folk music. His letters home from Szafarnia (to which he gave the title "The Szafarnia Courier"), written in a very modern and lively Polish, amused his family with their spoofing of the Warsaw newspapers and demonstrated the youngster's literary gift.
In 1827, soon after the death of Chopin's youngest sister Emilia, the family moved from the Warsaw University building, adjacent to the Kazimierz Palace, to lodgings just across the street from the university, in the south annex of the Krasiński Palace on Krakowskie Przedmieście, where Chopin lived until he left Warsaw in 1830. Here his parents continued running their boarding house for male students; the Chopin Family Parlour ("Salonik Chopinów") became a museum in the 20th century. In 1829 the artist Ambroży Mieroszewski executed a set of portraits of Chopin family members, including the first known portrait of the composer.
Four boarders at his parents' apartments became Chopin's intimates: Tytus Woyciechowski, Jan Nepomucen Białobłocki, Jan Matuszyński and Julian Fontana; the latter two would become part of his Paris milieu. He was friendly with members of Warsaw's young artistic and intellectual world, including Fontana, Józef Bohdan Zaleski and Stefan Witwicki. He was also attracted to the singing student Konstancja Gładkowska. In letters to Woyciechowski, he indicated which of his works, and even which of their passages, were influenced by his fascination with her; his letter of 15 May 1830 revealed that the slow movement ("Larghetto") of his Piano Concerto No. 1 (in E minor) was secretly dedicated to her – "It should be like dreaming in beautiful springtime – by moonlight." His final Conservatory report (July 1829) read: "Chopin F., third-year student, exceptional talent, musical genius."
Travel and domestic success.
In September 1828 Chopin, while still a student, visited Berlin with a family friend, zoologist Feliks Jarocki, enjoying operas directed by Gaspare Spontini and attending concerts by Carl Friedrich Zelter, Felix Mendelssohn and other celebrities. On an 1829 return trip to Berlin, he was a guest of Prince Antoni Radziwiłł, governor of the Grand Duchy of Posen—himself an accomplished composer and aspiring cellist. For the prince and his pianist daughter Wanda, he composed his Introduction and Polonaise brillante in C major for cello and piano, Op. 3.
Back in Warsaw that year, Chopin heard Niccolò Paganini play the violin, and composed a set of variations, "Souvenir de Paganini". It may have been this experience which encouraged him to commence writing his first Études, (1829–32), exploring the capacities of his own instrument. On 11 August, three weeks after completing his studies at the Warsaw Conservatory, he made his debut in Vienna. He gave two piano concerts and received many favourable reviews—in addition to some commenting (in Chopin's own words) that he was "too delicate for those accustomed to the piano-bashing of local artists". In one of these concerts, he premiered his Variations on "Là ci darem la mano", Op. 2 (variations on an aria from Mozart's opera "Don Giovanni") for piano and orchestra. He returned to Warsaw in September 1829, where he premiered his Piano Concerto No. 2 in F minor, Op. 21 on 17 March 1830.
Chopin's successes as a composer and performer opened the door to western Europe for him, and on 2 November 1830, he set out, in the words of Zdzisław Jachimecki, "into the wide world, with no very clearly defined aim, forever." With Woyciechowski, he headed for Austria, intending to go on to Italy. Later that month, in Warsaw, the November 1830 Uprising broke out, and Woyciechowski returned to Poland to enlist. Chopin, now alone in Vienna, was nostalgic for his homeland, and wrote to a friend, "I curse the moment of my departure." When in September 1831 he learned, while travelling from Vienna to Paris, that the uprising had been crushed, he expressed his anguish in the pages of his private journal: "Oh God! ... You are there, and yet you do not take vengeance!" Jachimecki ascribes to these events the composer's maturing "into an inspired national bard who intuited the past, present and future of his native Poland."
Paris.
Chopin arrived in Paris in late September 1831; he would never return to Poland, thus becoming one of many expatriates of the Polish Great Emigration. In France he used the French versions of his given names, and after receiving French citizenship in 1835, he travelled on a French passport. However, Chopin remained close to his fellow Poles in exile as friends and confidants and he never felt fully comfortable speaking French. Chopin's biographer Adam Zamoyski writes that he never considered himself to be French, despite his father's French origins, and always saw himself as a Pole.
In Paris, Chopin encountered artists and other distinguished figures, and found many opportunities to exercise his talents and achieve celebrity. During his years in Paris he was to become acquainted with, among many others, Hector Berlioz, Franz Liszt, Ferdinand Hiller, Heinrich Heine, Eugène Delacroix, and Alfred de Vigny. Chopin was also acquainted with the poet Adam Mickiewicz, principal of the Polish Literary Society, some of whose verses he set as songs.
Two Polish friends in Paris were also to play important roles in Chopin's life there. His fellow student at the Warsaw Conservatory, Julian Fontana, had originally tried unsuccessfully to establish himself in England; Albert Grzymała, who in Paris became a wealthy financier and society figure, often acted as Chopin's adviser and "gradually began to fill the role of elder brother in i life." Fontana was to become, in the words of Michałowski and Samson, Chopin's "general factotum and copyist".
At the end of 1831, Chopin received the first major endorsement from an outstanding contemporary when Robert Schumann, reviewing the Op. 2 Variations in the "Allgemeine musikalische Zeitung" (his first published article on music), declared: "Hats off, gentlemen! A genius." On 26 February 1832 Chopin gave a debut Paris concert at the Salle Pleyel which drew universal admiration. The critic François-Joseph Fétis wrote in the "Revue et gazette musicale": "Here is a young man who ... taking no model, has found, if not a complete renewal of piano music, ... an abundance of original ideas of a kind to be found nowhere else ..." After this concert, Chopin realized that his essentially intimate keyboard technique was not optimal for large concert spaces. Later that year he was introduced to the wealthy Rothschild banking family, whose patronage also opened doors for him to other private salons (social gatherings of the aristocracy and artistic and literary elite). By the end of 1832 Chopin had established himself among the Parisian musical elite, and had earned the respect of his peers such as Hiller, Liszt, and Berlioz. He no longer depended financially upon his father, and in the winter of 1832 he began earning a handsome income from publishing his works and teaching piano to affluent students from all over Europe. This freed him from the strains of public concert-giving, which he disliked.
Chopin seldom performed publicly in Paris. In later years he generally gave a single annual concert at the Salle Pleyel, a venue that seated three hundred. He played more frequently at salons, but preferred playing at his own Paris apartment for small groups of friends. The musicologist Arthur Hedley has observed that "As a pianist Chopin was unique in acquiring a reputation of the highest order on the basis of a minimum of public appearances—few more than thirty in the course of his lifetime." The list of musicians who took part in some of his concerts provides an indication of the richness of Parisian artistic life during this period. Examples include a concert on 23 March 1833, in which Chopin, Liszt and Hiller performed (on pianos) a concerto by J.S. Bach for three keyboards; and, on 3 March 1838, a concert in which Chopin, his pupil Adolphe Gutmann, Charles-Valentin Alkan, and Alkan's teacher Joseph Zimmermann performed Alkan's arrangement, for eight hands, of two movements from Beethoven's 7th symphony. Chopin was also involved in the composition of Liszt's "Hexameron"; he wrote the sixth (and final) variation on Bellini's theme. Chopin's music soon found success with publishers, and in 1833 he contracted with Maurice Schlesinger, who arranged for it to be published not only in France but, through his family connections, also in Germany and England.
In the spring of 1834, Chopin attended the Lower Rhenish Music Festival in Aix-la-Chapelle with Hiller, and it was there that Chopin met Felix Mendelssohn. After the festival, the three visited Düsseldorf, where Mendelssohn had been appointed musical director. They spent what Mendelssohn described as "a very agreeable day", playing and discussing music at his piano, and met Friedrich Wilhelm Schadow, director of the Academy of Art, and some of his eminent pupils such as Lessing, Bendemann, Hildebrandt and Sohn. In 1835 Chopin went to Carlsbad, where he spent time with his parents; it was the last time he would see them. On his way back to Paris, he met old friends from Warsaw, the Wodzińskis. He had made the acquaintance of their daughter Maria in Poland five years earlier, when she was eleven. This meeting prompted him to stay for two weeks in Dresden, when he had previously intended to return to Paris via Leipzig. The sixteen-year-old girl's portrait of the composer is considered, along with Delacroix's, as among Chopin's best likenesses. In October he finally reached Leipzig, where he met Schumann, Clara Wieck and Felix Mendelssohn, who organised for him a performance of his own oratorio "St. Paul", and who considered him "a perfect musician". In July 1836 Chopin travelled to Marienbad and Dresden to be with the Wodziński family, and in September he proposed to Maria, whose mother Countess Wodzińska approved in principle. Chopin went on to Leipzig, where he presented Schumann with his G minor Ballade. At the end of 1836 he sent Maria an album in which his sister Ludwika had inscribed seven of his songs, and his 1835 Nocturne in C-sharp minor, Op. 27, No. 1. The anodyne thanks he received from Maria proved to be the last letter he was to have from her.
Franz Liszt.
Although it is not known exactly when Chopin first met Liszt after arriving in Paris, on 12 December 1831 he mentioned in a letter to his friend Woyciechowski that "I have met Rossini, Cherubini, Baillot, etc.—also Kalkbrenner. You would not believe how curious I was about Herz, Liszt, Hiller, etc." Liszt was in attendance at Chopin's Parisian debut on 26 February 1832 at the Salle Pleyel, which led him to remark: "The most vigorous applause seemed not to suffice to our enthusiasm in the presence of this talented musician, who revealed a new phase of poetic sentiment combined with such happy innovation in the form of his art."
The two became friends, and for many years lived in close proximity in Paris, Chopin at 38 Rue de la Chaussée-d'Antin, and Liszt at the Hôtel de France on the Rue Lafitte, a few blocks away. They performed together on seven occasions between 1833 and 1841. The first, on 2 April 1833, was at a benefit concert organized by Hector Berlioz for his bankrupt Shakespearean actress wife Harriet Smithson, during which they played George Onslow's "Sonata in F minor" for piano duet. Later joint appearances included a benefit concert for the Benevolent Association of Polish Ladies in Paris. Their last appearance together in public was for a charity concert conducted for the Beethoven Memorial in Bonn, held at the Salle Pleyel and the Paris Conservatory on 25 and 26 April 1841.
Although the two displayed great respect and admiration for each other, their friendship was uneasy and had some qualities of a love-hate relationship. Harold C. Schonberg believes that Chopin displayed a "tinge of jealousy and spite" towards Liszt's virtuosity on the piano, and others have also argued that he had become enchanted with Liszt's theatricality, showmanship and success. Liszt was the dedicatee of Chopin's Op. 10 Études, and his performance of them prompted the composer to write to Hiller, "I should like to rob him of the way he plays my studies." However, Chopin expressed annoyance in 1843 when Liszt performed one of his nocturnes with the addition of numerous intricate embellishments, at which Chopin remarked that he should play the music as written or not play it at all, forcing an apology. Most biographers of Chopin state that after this the two had little to do with each other, although in his letters dated as late as 1848 he still referred to him as "my friend Liszt". Some commentators point to events in the two men's romantic lives which led to a rift between them; there are claims that Liszt had displayed jealousy of his mistress Marie d'Agoult's obsession with Chopin, while others believe that Chopin had become concerned about Liszt's growing relationship with George Sand.
George Sand.
In 1836, at a party hosted by Marie d'Agoult, Chopin met the French author George Sand (born mantin Aurore ucil Dupin). Short (under five feet, or 152 cm), dark, big-eyed and a cigar smoker, she initially repelled Chopin, who remarked, "What an unattractive person "la Sand" is. Is she really a woman?" However, by early 1837 Maria Wodzińska's mother had made it clear to Chopin in correspondence that a marriage with her daughter was unlikely to proceed. It is thought that she was influenced by his poor health and possibly also by rumours about his associations with women such as d'Agoult and Sand. Chopin finally placed the letters from Maria and her mother in a package on which he wrote, in Polish, "My tragedy". Sand, in a letter to Grzymała of June 1838, admitted strong feelings for the composer and debated whether to abandon a current affair in order to begin a relationship with Chopin; she asked Grzymała to assess Chopin's relationship with Maria Wodzińska, without realising that the affair, at least from Maria's side, was over.
In June 1837 Chopin visited London incognito in the company of the piano manufacturer Camille Pleyel where he played at a musical soirée at the house of English piano maker James Broadwood. On his return to Paris, his association with Sand began in earnest, and by the end of June 1838 they had become lovers. Sand, who was six years older than the composer, and who had had a series of lovers, wrote at this time: "I must say I was confused and amazed at the effect this little creature had on me ... I have still not recovered from my astonishment, and if I were a proud person I should be feeling humiliated at having been carried away ..." The two spent a miserable winter on Majorca (8 November 1838 to 13 February 1839), where, together with Sand's two children, they had journeyed in the hope of improving the health of Chopin and that of Sand's 15-year-old son Maurice, and also to escape the threats of Sand's former lover Félicien Mallefille. After discovering that the couple were not married, the deeply traditional Catholic people of Majorca became inhospitable, making accommodation difficult to find. This compelled the group to take lodgings in a former Carthusian monastery in Valldemossa, which gave little shelter from the cold winter weather.
On 3 December, Chopin complained about his bad health and the incompetence of the doctors in Majorca: "Three doctors have visited me ... The first said I was dead; the second said I was dying; and the third said I was about to die." He also had problems having his Pleyel piano sent to him. It finally arrived from Paris in December. Chopin wrote to Pleyel in January 1839: "I am sending you my Preludes Op. 28. I finished them on your little piano, which arrived in the best possible condition in spite of the sea, the bad weather and the Palma customs." Chopin was also able to undertake work on his Ballade No. 2, Op. 38; two Polonaises, Op. 40; and the Scherzo No. 3, Op. 39.
Although this period had been productive, the bad weather had such a detrimental effect on Chopin's health that Sand determined to leave the island. To avoid further customs duties, Sand sold the piano to a local French couple, the Canuts. The group traveled first to Barcelona, then to Marseilles, where they stayed for a few months while Chopin convalesced. In May 1839 they headed for the summer to Sand's estate at Nohant, where they spent most summers until 1846. In autumn they returned to Paris, where Chopin's apartment at 5 rue Tronchet was close to Sand's rented accommodation at the rue Pigalle. He frequently visited Sand in the evenings, but both retained some independence. In 1842 he and Sand moved to the Square d'Orléans, living in adjacent buildings.
At the funeral of the tenor Adolphe Nourrit in Paris in 1839, Chopin made a rare appearance at the organ, playing a transcription of Franz Schubert's "lied" "Die Gestirne". On 26 July 1840 Chopin and Sand were present at the dress rehearsal of Berlioz's "Grande symphonie funèbre et triomphale", composed to commemorate the tenth anniversary of the July Revolution. Chopin was reportedly unimpressed with the composition.
During the summers at Nohant, particularly in the years 1839–43, Chopin found quiet, productive days during which he composed many works, including his Polonaise in A-flat major, Op. 53. Among the visitors to Nohant were Delacroix and the mezzo-soprano Pauline Viardot, whom Chopin had advised on piano technique and composition. Delacroix gives an account of staying at Nohant in a letter of 7 June 1842:
The hosts could not be more pleasant in entertaining me. When we are not all together at dinner, lunch, playing billiards, or walking, each of us stays in his room, reading or lounging around on a couch. Sometimes, through the window which opens on the garden, a gust of music wafts up from Chopin at work. All this mingles with the songs of nightingales and the fragrance of roses.
Decline.
From 1842 onwards, Chopin showed signs of serious illness. After a solo recital in Paris on 21 February 1842, he wrote to Grzymała: "I have to lie in bed all day long, my mouth and tonsils are aching so much." He was forced by illness to decline a written invitation from Alkan to participate in a repeat performance of the Beethoven Seventh Symphony arrangement at Erard's on 1 March 1843. Late in 1844, Charles Hallé visited Chopin and found him "hardly able to move, bent like a half-opened penknife and evidently in great pain", although his spirits returned when he started to play the piano for his visitor. Chopin's health continued to deteriorate, particularly from this time onwards. Modern research suggests that apart from any other illnesses, he may also have suffered from temporal lobe epilepsy.
Chopin's relations with Sand were soured in 1846 by problems involving her daughter Solange and Solange's fiancé, the young fortune-hunting sculptor Auguste Clésinger. The composer frequently took Solange's side in quarrels with her mother; he also faced jealousy from Sand's son Maurice. Chopin was utterly indifferent to Sand's radical political pursuits, while Sand looked on his society friends with disdain. As the composer's illness progressed, Sand had become less of a lover and more of a nurse to Chopin, whom she called her "third child". In letters to third parties, she vented her impatience, referring to him as a "child," a "little angel", a "sufferer" and a "beloved little corpse." In 1847 Sand published her novel "Lucrezia Floriani", whose main characters—a rich actress and a prince in weak health—could be interpreted as Sand and Chopin; the story was uncomplimentary to Chopin, who could not have missed the allusions as he helped Sand correct the printer's galleys. In 1847 he did not visit Nohant, and he quietly ended their ten-year relationship following an angry correspondence which, in Sand's words, made "a strange conclusion to nine years of exclusive friendship." The two would never meet again.
Chopin's output as a composer throughout this period declined in quantity year by year. Whereas in 1841 he had written a dozen works, only six were written in 1842 and six shorter pieces in 1843. In 1844 he wrote only the Op. 58 sonata. 1845 saw the completion of three mazurkas (Op. 59). Although these works were more refined than many of his earlier compositions, Zamoyski opines that "his powers of concentration were failing and his inspiration was beset by anguish, both emotional and intellectual."
Tour of England and Scotland.
Chopin's public popularity as a virtuoso began to wane, as did the number of his pupils, and this, together with the political strife and instability of the time, caused him to struggle financially. In February 1848, with the cellist Auguste Franchomme, he gave his last Paris concert, which included three movements of the Cello Sonata Op. 65.
In April, during the Revolution of 1848 in Paris, he left for London, where he performed at several concerts and at numerous receptions in great houses. This tour was suggested to him by his Scottish pupil Jane Stirling and her elder sister. Stirling also made all the logistical arrangements and provided much of the necessary funding.
In London Chopin took lodgings at Dover Street, where the firm of Broadwood provided him with a grand piano. At his first engagement, on 15 May at Stafford House, the audience included Queen Victoria and Prince Albert. The Prince, who was himself a talented musician, moved close to the keyboard to view Chopin's technique. Broadwood also arranged concerts for him; among those attending were Thackeray and the singer Jenny Lind. Chopin was also sought after for piano lessons, for which he charged the high fee of one guinea (£1.05 in present British currency) per hour, and for private recitals for which the fee was 20 guineas. At a concert on 7 July he shared the platform with Viardot, who sang arrangements of some of his mazurkas to Spanish texts.
In late summer he was invited by Jane Stirling to visit Scotland, where he stayed at Calder House near Edinburgh and at Johnstone Castle in Renfrewshire, both owned by members of Stirling's family. She clearly had a notion of going beyond mere friendship, and Chopin was obliged to make it clear to her that this could not be so. He wrote at this time to Grzymała "My Scottish ladies are kind, but such bores", and responding to a rumour about his involvement, answered that he was "closer to the grave than the nuptial bed." He gave a public concert in Glasgow on 27 September, and another in Edinburgh, at the Hopetoun Rooms on Queen Street (now Erskine House) on 4 October. In late October 1848, while staying at 10 Warriston Crescent in Edinburgh with the Polish physician Adam Łyszczyński, he wrote out his last will and testament—"a kind of disposition to be made of my stuff in the future, if I should drop dead somewhere", he wrote to Grzymała.
Chopin made his last public appearance on a concert platform at London's Guildhall on 16 November 1848, when, in a final patriotic gesture, he played for the benefit of Polish refugees. By this time he was very seriously ill, weighing under 99 pounds (i.e. less than 45 kg), and his doctors were aware that his sickness was at a terminal stage.
At the end of November, Chopin returned to Paris. He passed the winter in unremitting illness, but gave occasional lessons and was visited by friends, including Delacroix and Franchomme. Occasionally he played, or accompanied the singing of Delfina Potocka, for his friends. During the summer of 1849, his friends found him an apartment in Chaillot, out of the centre of the city, for which the rent was secretly subsidised by an admirer, Princess Obreskoff. Here in June 1849 he was visited by Jenny Lind.
Death and funeral.
With his health further deteriorating, Chopin desired to have a family member with him. In June 1849 his sister Ludwika came to Paris with her husband and daughter, and in September, supported by a loan from Jane Stirling, he took an apartment at Place Vendôme 12. After 15 October, when his condition took a marked turn for the worse, only a handful of his closest friends remained with him, although Viardot remarked sardonically that "all the grand Parisian ladies considered it "de rigueur" to faint in his room."
Some of his friends provided music at his request; among them, Potocka sang and Franchomme played the cello. Chopin requested that his body be opened after death (for fear of being buried alive) and his heart returned to Warsaw where it rests at the Church of the Holy Cross. He also bequeathed his unfinished notes on a piano tuition method, "Projet de méthode", to Alkan for completion. On 17 October, after midnight, the physician leaned over him and asked whether he was suffering greatly. "No longer", he replied. He died a few minutes before two o'clock in the morning. Those present at the deathbed appear to have included his sister Ludwika, Princess Marcelina Czartoryska, Sand's daughter Solange, and his close friend Thomas Albrecht. Later that morning, Solange's husband Clésinger made Chopin's death mask and a cast of his left hand.
Chopin's disease and the cause of his death have since been a matter of discussion. His death certificate gave the cause as tuberculosis, and his physician, Jean Cruveilhier, was then the leading French authority on this disease. Other possibilities have been advanced including cystic fibrosis, cirrhosis and alpha 1-antitrypsin deficiency. However, the attribution of tuberculosis as principal cause of death has not been disproved. Permission for DNA testing, which could put the matter to rest, has been denied by the Polish government.
The funeral, held at the Church of the Madeleine in Paris, was delayed almost two weeks, until 30 October. Entrance was restricted to ticket holders as many people were expected to attend. Over 3,000 people arrived without invitations, from as far as London, Berlin and Vienna, and were excluded.
Mozart's Requiem was sung at the funeral; the soloists were the soprano Jeanne-Anais Castellan, the mezzo-soprano Pauline Viardot, the tenor Alexis Dupont, and the bass Luigi Lablache; Chopin's Preludes No. 4 in E minor and No. 6 in B minor were also played. The organist at the funeral was Louis Lefébure-Wély. The funeral procession to Père Lachaise Cemetery, which included Chopin's sister Ludwika, was led by the aged Prince Adam Czartoryski. The pallbearers included Delacroix, Franchomme, and Camille Pleyel. At the graveside, the "Funeral March" from Chopin's Piano Sonata No. 2 was played, in Reber's instrumentation.
Chopin's tombstone, featuring the muse of music, Euterpe, weeping over a broken lyre, was designed and sculpted by Clésinger. The expenses of the funeral and monument, amounting to 5,000 francs, were covered by Jane Stirling, who also paid for the return of the composer's sister Ludwika to Warsaw. Ludwika took Chopin's heart in an urn, preserved in alcohol, back to Poland in 1850. She also took a collection of two hundred letters from Sand to Chopin; after 1851 these were returned to Sand, who seems to have destroyed them.
Music.
Overview.
Over 230 works of Chopin survive; some compositions from early childhood have been lost. All his known works involve the piano, and only a few range beyond solo piano music, as either piano concertos, songs or chamber music.
Chopin was educated in the tradition of Beethoven, Haydn, Mozart and Clementi; he used Clementi's piano method with his own students. He was also influenced by Hummel's development of virtuoso, yet Mozartian, piano technique. He cited Bach and Mozart as the two most important composers in shaping his musical outlook. Chopin's early works are in the style of the "brilliant" keyboard pieces of his era as exemplified by the works of Ignaz Moscheles, Friedrich Kalkbrenner, and others. Less direct in the earlier period are the influences of Polish folk music and of Italian opera. Much of what became his typical style of ornamentation (for example, his "fioriture") is taken from singing. His melodic lines were increasingly reminiscent of the modes and features of the music of his native country, such as drones.
Chopin took the new salon genre of the nocturne, invented by the Irish composer John Field, to a deeper level of sophistication. He was the first to write ballades and scherzi as individual concert pieces. He essentially established a new genre with his own set of free-standing preludes (Op. 28, published 1839). He exploited the poetic potential of the concept of the concert étude, already being developed in the 1820s and 1830s by Liszt, Clementi and Moscheles, in his two sets of studies (Op. 10 published in 1833, Op. 25 in 1837).
Chopin also endowed popular dance forms with a greater range of melody and expression. Chopin's mazurkas, while originating in the traditional Polish dance (the "mazurek"), differed from the traditional variety in that they were written for the concert hall rather than the dance hall; "it was Chopin who put the mazurka on the European musical map." The series of seven polonaises published in his lifetime (another nine were published posthumously), beginning with the Op. 26 pair (published 1836), set a new standard for music in the form. His waltzes were also written specifically for the salon recital rather than the ballroom and are frequently at rather faster tempos than their dance-floor equivalents.
Titles, opus numbers and editions.
Some of Chopin's well-known pieces have acquired descriptive titles, such as the "Revolutionary" Étude (Op. 10, No. 12), and the "Minute Waltz" (Op. 64, No. 1). However, with the exception of his "Funeral March", the composer never named an instrumental work beyond genre and number, leaving all potential extramusical associations to the listener; the names by which many of his pieces are known were invented by others. There is no evidence to suggest that the "Revolutionary" Étude was written with the failed Polish uprising against Russia in mind; it merely appeared at that time. The "Funeral March", the third movement of his Sonata No. 2 (Op. 35), the one case where he did give a title, was written before the rest of the sonata, but no specific event or death is known to have inspired it.
The last opus number that Chopin himself used was 65, allocated to the Cello Sonata in G minor. He expressed a deathbed wish that all his unpublished manuscripts be destroyed. At the request of the composer's mother and sisters, however, his musical executor Julian Fontana selected 23 unpublished piano pieces and grouped them into eight further opus numbers (Opp. 66–73), published in 1855. In 1857, 17 Polish songs that Chopin wrote at various stages of his life were collected and published as Op. 74, though their order within the opus did not reflect the order of composition.
Works published since 1857 have received alternative catalogue designations instead of opus numbers. The present standard musicological reference for Chopin's works is the Kobylańska Catalogue (usually represented by the initials 'KK'), named for its compiler, the Polish musicologist Krystyna Kobylańska.
Chopin's original publishers included Maurice Schlesinger and Camille Pleyel. His works soon began to appear in popular 19th-century piano anthologies. The first collected edition was by Breitkopf & Härtel (1878–1902). Among modern scholarly editions of Chopin's works are the version under the name of Paderewski published between 1937 and 1966 and the more recent Polish "National Edition", edited by Jan Ekier, both of which contain detailed explanations and discussions regarding choices and sources.
Form and harmony.
Improvisation stands at the centre of Chopin's creative processes. However, this does not imply impulsive rambling: Nicholas Temperley writes that "improvisation is designed for an audience, and its starting-point is that audience's expectations, which include the current conventions of musical form." The works for piano and orchestra, including the two concertos, are held by Temperley to be "merely vehicles for brilliant piano playing ... formally longwinded and extremely conservative". After the piano concertos (which are both early, dating from 1830), Chopin made no attempts at large-scale multi-movement forms, save for his late sonatas for piano and for cello; "instead he achieved near-perfection in pieces of simple general design but subtle and complex cell-structure." Rosen suggests that an important aspect of Chopin's individuality is his flexible handling of the four-bar phrase as a structural unit.
J. Barrie Jones suggests that "amongst the works that Chopin intended for concert use, the four ballades and four scherzos stand supreme", and adds that "the Barcarolle Op. 60 stands apart as an example of Chopin's rich harmonic palette coupled with an Italianate warmth of melody." Temperley opines that these works, which contain "immense variety of mood, thematic material and structural detail", are based on an extended "departure and return" form; "the more the middle section is extended, and the further it departs in key, mood and theme, from the opening idea, the more important and dramatic is the reprise when it at last comes."
Chopin's mazurkas and waltzes are all in straightforward ternary or episodic form, sometimes with a coda. The mazurkas often show more folk features than many of his other works, sometimes including modal scales and harmonies and the use of drone basses. However, some also show unusual sophistication, for example Op. 63 No. 3, which includes a canon at one beat's distance, a great rarity in music.
Chopin's polonaises show a marked advance on those of his Polish predecessors in the form (who included his teachers Zywny and Elsner). As with the traditional polonaise, Chopin's works are in triple time and typically display a martial rhythm in their melodies, accompaniments and cadences. Unlike most of their precursors, they also require a formidable playing technique.
The 21 nocturnes are more structured, and of greater emotional depth, than those of Field (whom Chopin met in 1833). Many of the Chopin nocturnes have middle sections marked by agitated expression (and often making very difficult demands on the performer) which heightens their dramatic character.
Chopin's études are largely in straightforward ternary form. He used them to teach his own technique of piano playing—for instance playing double thirds (Op. 25, No. 6), playing in octaves (Op. 25, No. 10), and playing repeated notes (Op. 10, No.  7).
The preludes, many of which are very brief (some consisting of simple statements and developments of a single theme or figure), were described by Schumann as "the beginnings of studies". Inspired by J.S. Bach's "The Well-Tempered Clavier", Chopin's preludes move up the circle of fifths (rather than Bach's chromatic scale sequence) to create a prelude in each major and minor tonality. The preludes were perhaps not intended to be played as a group, and may even have been used by him and later pianists as generic preludes to others of his pieces, or even to music by other composers, as Kenneth Hamilton suggests: he has noted a recording by Ferruccio Busoni of 1922, in which the Prelude Op. 28 No. 7 is followed by the Étude Op. 10 No. 5.
The two mature piano sonatas (No. 2, Op. 35, written in 1839 and No. 3, Op. 58, written in 1844) are in four movements. In Op. 35, Chopin was able to combine within a formal large musical structure many elements of his virtuosic piano technique—"a kind of dialogue between the public pianism of the brilliant style and the German sonata principle". The last movement, a brief (75-bar) perpetuum mobile in which the hands play in unmodified octave unison throughout, was found shocking and unmusical by contemporaries, including Schumann. The Op. 58 sonata is closer to the German tradition, including many passages of complex counterpoint, "worthy of Brahms" according to the music historians Kornel Michałowski and Jim Samson.
Chopin's harmonic innovations may have arisen partly from his keyboard improvisation technique. Temperley says that in his works "novel harmonic effects frequently result from the combination of ordinary appoggiaturas or passing notes with melodic figures of accompaniment", and cadences are delayed by the use of chords outside the home key (neapolitan sixths and diminished sevenths), or by sudden shifts to remote keys. Chord progressions sometimes anticipate the shifting tonality of later composers such as Claude Debussy, as does Chopin's use of modal harmony.
Technique and performance style.
In 1841, Léon Escudier wrote of a recital given by Chopin that year, "One may say that Chopin is the creator of a school of piano and a school of composition. In truth, nothing equals the lightness, the sweetness with which the composer preludes on the piano; moreover nothing may be compared to his works full of originality, distinction and grace." Chopin refused to conform to a standard method of playing and believed that there was no set technique for playing well. His style was based extensively on his use of very independent finger technique. In his "Projet de méthode" he wrote: "Everything is a matter of knowing good fingering ... we need no less to use the rest of the hand, the wrist, the forearm and the upper arm." He further stated: "One needs only to study a certain position of the hand in relation to the keys to obtain with ease the most beautiful quality of sound, to know how to play short notes and long notes, and o attai unlimited dexterity." The consequences of this approach to technique in Chopin's music include the frequent use of the entire range of the keyboard, passages in double octaves and other chord groupings, swiftly repeated notes, the use of grace notes, and the use of contrasting rhythms (four against three, for example) between the hands.
Jonathan Bellman writes that modern concert performance style—set in the "conservatory" tradition of late 19th- and 20th-century music schools, and suitable for large auditoria or recordings—militates against what is known of Chopin's more intimate performance technique. The composer himself said to a pupil that "concerts are never real music, you have to give up the idea of hearing in them all the most beautiful things of art." Contemporary accounts indicate that in performance, Chopin avoided rigid procedures sometimes incorrectly attributed to him, such as "always crescendo to a high note", but that he was concerned with expressive phrasing, rhythmic consistency and sensitive colouring. Berlioz wrote in 1853 that Chopin "has created a kind of chromatic embroidery ... whose effect is so strange and piquant as to be impossible to describe ... virtually nobody but Chopin himself can play this music and give it this unusual turn". Hiller wrote that "What in the hands of others was elegant embellishment, in his hands became a colourful wreath of flowers."
Chopin's music is frequently played with "rubato", "the practice in performance of disregarding strict time, 'robbing' some note-values for expressive effect". There are differing opinions as to how much, and what type, of "rubato" is appropriate for his works. Charles Rosen comments that "most of the written-out indications of rubato in Chopin are to be found in his mazurkas ... It is probable that Chopin used the older form of rubato so important to Mozart ... her the melody note in the right hand is delayed until after the note in the bass ... An allied form of this rubato is the arpeggiation of the chords thereby delaying the melody note; according to Chopin's pupil, Karol Mikuli, Chopin was firmly opposed to this practice."
Friederike Müller, a pupil of Chopin, wrote: "i playing was always noble and beautiful; his tones sang, whether in full "forte" or softest "piano". He took infinite pains to teach his pupils this "legato", "cantabile" style of playing. His most severe criticism was 'He—or she—does not know how to join two notes together.' He also demanded the strictest adherence to rhythm. He hated all lingering and dragging, misplaced "rubatos", as well as exaggerated "ritardandos" ... and it is precisely in this respect that people make such terrible errors in playing his works."
Polish heritage.
With his mazurkas and polonaises, Chopin has been credited with introducing to music a new sense of nationalism. Schumann, in his 1836 review of the piano concertos, highlighted the composer's strong feelings for his native Poland, writing that "Now that the Poles are in deep mourning fter the failure of the November 1830 risin, their appeal to us artists is even stronger ... If the mighty autocrat in the north .e. [[Nicholas I of Russia] could know that in Chopin's works, in the simple strains of his mazurkas, there lurks a dangerous enemy, he would place a ban on his music. Chopin's works are cannon buried in flowers!" The biography of Chopin published in 1863 under the name of Franz Liszt (but probably written by Carolyne zu Sayn-Wittgenstein) claims that Chopin "must be ranked first among the first musicians ... individualizing in themselves the poetic sense of an entire nation."
Some modern commentators have argued against exaggerating Chopin's primacy as a "nationalist" or "patriotic" composer. George Golos refers to earlier "nationalist" composers in Central Europe, including Poland's Michał Kleofas Ogiński and Franciszek Lessel, who utilised polonaise and mazurka forms. Barbara Milewski suggests that Chopin's experience of Polish music came more from "urbanised" Warsaw versions than from folk music, and that attempts (by Jachimecki and others) to demonstrate genuine folk music in his works are without basis. Richard Taruskin impugns Schumann's attitude toward Chopin's works as patronizing and comments that Chopin "felt his Polish patriotism deeply and sincerely" but consciously modelled his works on the tradition of Bach, Beethoven, Schubert and Field.
A reconciliation of these views is suggested by William Atwood: "Undoubtedly hopin' use of traditional musical forms like the polonaise and mazurka roused nationalistic sentiments and a sense of cohesiveness amongst those Poles scattered across Europe and the New World ... While some sought solace in he, others found them a source of strength in their continuing struggle for freedom. Although Chopin's music undoubtedly came to him intuitively rather than through any conscious patriotic design, it served all the same to symbolize the will of the Polish people ..."
Classical pianist Yuja Wang describes Chopin's music as "national, yet universal."
Reception and influence.
Jones comments that "Chopin's unique position as a composer, despite the fact that virtually everything he wrote was for the piano, has rarely been questioned." He also notes that Chopin was fortunate to arrive in Paris in 1831—"the artistic environment, the publishers who were willing to print his music, the wealthy and aristocratic who paid what Chopin asked for their lessons"—and these factors, as well as his musical genius, also fuelled his contemporary and later reputation. While his illness and his love-affairs conform to some of the stereotypes of romanticism, the rarity of his public recitals (as opposed to performances at fashionable Paris soirées) led Arthur Hutchings to suggest that "his lack of Byronic flamboyance n his aristocratic reclusiveness make him exceptional" among his romantic contemporaries, such as Liszt and Henri Herz.
Chopin's qualities as a pianist and composer were recognized by many of his fellow musicians. Schumann named a piece for him in his suite "Carnaval", and Chopin later dedicated his Ballade No. 2 in F major to Schumann. Elements of Chopin's music can be traced in many of Liszt's later works. Liszt later transcribed for piano six of Chopin's Polish songs. A less fraught friendship was with Alkan, with whom he discussed elements of folk music, and who was deeply affected by Chopin's death.
Two of Chopin's long-standing pupils, Karol Mikuli (1821–1897) and Georges Mathias, were themselves piano teachers and passed on details of his playing to their own students, some of whom (such as Raoul Koczalski) were to make recordings of his music. Other pianists and composers influenced by Chopin's style include Louis Moreau Gottschalk, Édouard Wolff (1816–1880) and Pierre Zimmermann. Debussy dedicated his own 1915 piano Études to the memory of Chopin; he frequently played Chopin's music during his studies at the Paris Conservatoire, and undertook the editing of Chopin's piano music for the publisher Jacques Durand.
Polish composers of the following generation included virtuosi such as Moritz Moszkowski, but, in the opinion of J. Barrie Jones, his "one worthy successor" among his compatriots was Karol Szymanowski (1882–1937). Edvard Grieg, Antonín Dvořák, Isaac Albéniz, Pyotr Ilyich Tchaikovsky and Sergei Rachmaninoff, among others, are regarded by critics as having been influenced by Chopin's use of national modes and idioms. Alexander Scriabin was devoted to the music of Chopin, and his early published works include nineteen mazurkas, as well as numerous études and preludes; his teacher Nikolai Zverev drilled him in Chopin's works to improve his virtuosity as a performer. In the 20th century, composers who paid homage to (or in some cases parodied) the music of Chopin included George Crumb, Bohuslav Martinů, Darius Milhaud,
Igor Stravinsky and Heitor Villa-Lobos.
Chopin's music was used in the 1909 ballet "Chopiniana", choreographed by Michel Fokine and orchestrated by Alexander Glazunov. Sergei Diaghilev commissioned additional orchestrations—from Stravinsky, Anatoly Lyadov, Sergei Taneyev and Nikolai Tcherepnin—for later productions, which used the title "Les Sylphides".
Chopin's music remains very popular and is regularly performed, recorded and broadcast worldwide. The world's oldest monographic music competition, the International Chopin Piano Competition, founded in 1927, is held every five years in Warsaw. The Fryderyk Chopin Institute of Poland lists on its website over eighty societies world-wide devoted to the composer and his music. The Institute site also lists nearly 1,500 performances of Chopin works on YouTube as of January 2014.
Recordings.
The British Library notes that "Chopin's works have been recorded by all the great pianists of the recording era." The earliest recording was an 1895 performance by Paul Pabst of the Nocturne in E major Op. 62 No. 2. The British Library site makes available a number of historic recordings, including some by Alfred Cortot, Ignaz Friedman, Vladimir Horowitz, Benno Moiseiwitsch, Paderewski, Arthur Rubinstein, Xaver Scharwenka and many others. A select discography of recordings of Chopin works by pianists representing the various pedagogic traditions stemming from Chopin is given by Methuen-Campbell in his work tracing the lineage and character of those traditions.
Numerous recordings of Chopin's works are available. On the occasion of the composer's bicentenary, the critics of "The New York Times" recommended performances by the following contemporary pianists (among many others): Martha Argerich, Vladimir Ashkenazy, Emanuel Ax, Evgeny Kissin, Murray Perahia, Maurizio Pollini and Krystian Zimerman. The Warsaw Chopin Society organizes the "Grand prix du disque de F. Chopin" for notable Chopin recordings, held every five years.
In literature, stage, film and television.
Chopin has figured extensively in Polish literature, both in serious critical studies of his life and music and in fictional treatments. The earliest manifestation was probably an 1830 sonnet on Chopin by Leon Ulrich. French writers on Chopin (apart from Sand) have included Marcel Proust and André Gide; and he has also featured in works of Gottfried Benn and Boris Pasternak. There are numerous biographies of Chopin in English (see bibliography for some of these).
Possibly the first venture into fictional treatments of Chopin's life was a fanciful operatic version of some of its events. "Chopin" was written by Giacomo Orefice and produced in Milan in 1901. All the music is derived from that of Chopin.
Chopin's life and his relations with George Sand have been fictionalized in numerous films. The 1945 biographical film "A Song to Remember" earned Cornel Wilde an Academy Award nomination as Best Actor for his portrayal of the composer. Other film treatments have included: "La valse de l'adieu" (France, 1928) by Henry Roussel, with Pierre Blanchar as Chopin; "Impromptu" (1991), starring Hugh Grant as Chopin; "La note bleue" (1991); and "" (2002).
Chopin's life was covered in a BBC TV documentary "Chopin – The Women Behind The Music" (2010), and in a 2010 documentary realised by Angelo Bozzolini and Roberto Prosseda for Italian television.
Chopin's last hours are depicted in the Bandai Namco Entertainment title Eternal Sonata (2007) in a much stylized way.
References.
Notes
Citations
Bibliography
Music scores

</doc>
<doc id="10825" url="https://en.wikipedia.org/wiki?curid=10825" title="Free Democratic Party (Germany)">
Free Democratic Party (Germany)

The Free Democratic Party (, FDP) is a liberal and classical liberal political party in Germany. The FDP is led by Christian Lindner.
The FDP was founded in 1948 by members of the former liberal political parties existing in Germany before World War II, the German Democratic Party and the German People's Party. For most of the Federal Republic's history, it has held the balance of power in the Bundestag. It was a junior coalition partner to either the CDU/CSU (1949–56, 1961–66, 1982–98, and 2009–13) or the Social Democratic Party of Germany (1969–82). However, in the 2013 federal election the FDP failed to win any directly elected seats in the Bundestag, and came up short of the 5 percent threshold to qualify for list representation. It was thus shut out of the Bundestag for the first time in its history.
The FDP, which strongly supports human rights, civil liberties, and internationalism, has shifted from the centre to the centre-right over time. Since the 1980s, the party has firmly pushed economic liberalism, and has aligned itself closely to the promotion of free markets and privatisation. It is a member of the Liberal International and Alliance of Liberals and Democrats for Europe (ALDE).
Currently the FDP is represented in six state parliaments and in the European Parliament.
History.
Soon after World War II, the Soviet Union forced the creation of political parties. In July 1945 William Kulice and Eugen Schiffer called for the establishment of a pan-German Party, whose constitution the Allies hesitantly approved only in the Soviet occupation zone as the Liberal Democratic Party of Germany. In September 1945, citizens in Hamburg established the Party of Free Democrats (PFD) as a bourgeois Left Party and the first Liberal Party in the Western zones. In the first state elections in Hamburg in October 1946 the party won 18.2 percent of the vote. The FDP secured between 7.8 and 29.9 percent of the 1946 vote in Greater Berlin (East) and Saxony, the only states in Soviet-occupied territories that held free parliamentary elections. However, it had to support the policies of the Socialist Unity Party of Germany (SED) and join the National Front of the GDR as a "bloc party".Following the FDP's success, liberal parties were founded across the states. The FDP won Hesse's 1950 state election with 31.8 percent, the best result in its history, through appealing to East Germans displaced by the war by including them on their ticket.
Founding of the party.
The Democratic Party of Germany (DPD) was established in Rothenburg ob der Tauber on 17 March 1947 as a pan-German Party. Its leaders were Theodor Heuss and Wilhelm Külz. However, the project failed as a result of disputes over Külz's political direction.
The Free Democratic Party was established on 11–12 December 1948 in Heppenheim, in Hesse, as an association of all 13 regional liberal party organizations in the three Western zones of occupation. The proposed name, Liberal Democratic Party (LDP), was rejected by the delegates, who voted 64 to 25 in favour of the name Free Democratic Party (FDP).
The party's first chairman was Theodor Heuss; his deputy was Franz Blücher. The place for the party's foundation was chosen deliberately: it was at the Heppenheim Assembly that the moderate liberals had met in October 1847 before the March Revolution. 
Some regard the "Heppenheim Assembly", which was held at the "Halber Mond" (Half Moon) Hotel on 10 October 1847, as a meeting of leading liberals that was the beginning of the German Revolution of 1848-49.
Up to the 1950s, several of the FDP's regional organizations were to the right of the CDU/CSU, which initially had ideas of some sort of Christian socialism, and even former office-holders of the Third Reich were courted with national, patriotic values.
The FDP was founded on 11 December 1948 through the merger of nine regional liberal parties formed in 1945 from the remnants of the pre-1933 German People's Party (DVP) and the German Democratic Party (DDP), which had been active in the Weimar Republic. The FDP's first Chairman, Theodor Heuss, was formerly a member of the DDP and after the war of the Democratic People's Party (DVP).
1949–1969: The reconstruction of Germany.
In the first elections to the Bundestag on 14 August 1949, the FDP won a vote share of 11.9 percent (with 12 direct mandates, particularly in Baden-Württemberg and Hesse), and thus obtained 52 of 402 seats. In September of the same year the FDP chairman Theodor Heuss was elected the first President of the Federal Republic of Germany. In his 1954 re-election, he received the best election result to date of a President with 871 of 1018 votes (85.6 percent) of the Federal Assembly. Adenauer was also elected on the proposal of the new German President with an extremely narrow majority as the first Chancellor. The FDP participated with the CDU/CSU and the DP in Adenauer's coalition cabinet: they had three ministers: Franz Blücher (Vice-Chancellor), Thomas Dehler (Justice) and Eberhard Wildermuth (housing).
On the most important economic, social and German national issues, the FDP agreed with their coalition partners, the CDU/CSU. However, the FDP recommended to the bourgeois voters a secular party that refused the religious schools and the accused the opposition parties of clericalization. The FDP said they were known also as a consistent representative of the market economy, while the CDU was then dominated nominally from the Ahlen Programme, which allowed a Third Way between capitalism and socialism. Ludwig Erhard, the "father" of the social market economy, had his followers in the early years of the Federal Republic in the Union rather than in the FDP.
The FDP voted in parliament at the end of 1950 against the CDU- and SPD- introduced de-nazification process. At their party conference in Munich in 1951 they demanded the release of all "so-called war criminals" and welcomed the establishment of the "Association of German soldiers" of former Wehrmacht and SS members, to advance the integration of the nationalist forces in democracy. The 1953 Naumann-Affair, named after Werner Naumann, identifies old Nazis trying to infiltrate the party, which had many right-wing and nationalist members in Hesse, North Rhine-Westphalia and Lower Saxony. After the British occupation authorities had arrested seven prominent members of the Naumann circle, the FDP federal board installed a commission of inquiry, chaired by Thomas Dehler, which particularly sharply criticized the situation in the North Rhine-Westphalian FDP. In the following years, the right wing lost power, and the extreme right increasingly sought areas of activity outside the FDP. In the 1953 federal election the FDP received 9.5 percent of the party votes, 10.8 percent of the primary vote (with 14 direct mandates, particularly in Hamburg, Lower Saxony, Hesse, Württemberg and Bavaria) and 48 of 487 seats.
In the second term of the Bundestag, the South German Liberal democrats gained influence in the party. Thomas Dehler, a representative of a more left-liberal course took over as party and parliamentary leader. The former Minister of Justice Dehler, who in 1933 suffered persecution by the Nazis, was known for his rhetorical focus. Generally the various regional associations were independent and translated so different from country to country accents in liberal politics. After the FDP had left in early 1956, the coalition with the CDU in North Rhine-Westphalia and made with SPD and center a new state government, were a total of 16 members of parliament, including the four federal ministers from the FDP and founded the short-lived Free People's Party, which then up was involved to the end of the legislature instead of FDP in the Federal Government. The FDP first took it to the opposition.
Only one of the smaller post-war parties, the FDP survived despite many problems. In 1957 federal elections they still reached 7.7 percent of the vote to 1990 and their last direct mandate with which they had held 41 of 497 seats in the Bundestag. However, they still remained in opposition, because the Union won an absolute majority. In the following example, the FDP sat for a nuclear-free zone in Central Europe.
Even before the election Dehler was assigned as party chairman. At the federal party in Berlin at the end January 1957 relieved him Reinhold Maier. Dehler's role as Group Chairman took over after the election of the national set very Erich Mende. Mende was also chairman of the party.
In the 1961 federal elections, it achieved 12.8 percent nationwide, the best result until then, and the FDP entered a coalition with the CDU again. Although it was committed before the election to continuing to sit in any case in a government together with Adenauer, Chancellor Adenauer was again, however, to withdraw under the proviso, after two years. These events led to the FDP being nicknamed the "Umfallerpartei" (pushover party).
In the Spiegel Affair, the FDP withdrew their ministers from the federal government. Although the coalition was renewed again under Adenauer in 1962, the FDP withdrew again on the condition in October 1963. This occurred even under the new Chancellor, Ludwig Erhard. This was for Erich Mende turn the occasion to go into the cabinet: he took the rather unimportant Federal Ministry for All-German Affairs.
In the 1965 federal elections the FDP gained 9.5 percent. The coalition with the CDU in 1966 broke on the subject of tax increases and it was followed by a grand coalition between the CDU and the SPD. The opposition also pioneered a course change to: The former foreign policy and the attitude to the eastern territories were discussed. The new chairman elected delegates in 1968 Walter Scheel, a European-oriented liberals, although it came from the national liberal camp, but with Willi Weyer and Hans-Dietrich Genscher led the new center of the party. This center strove to make the FDP coalition support both major parties. Here, the Liberals approached to by their reorientation in East Germany and politics especially of the SPD.
1969–1982: Social changes and crises.
On 21 October 1969 began the period after the election of a Social Liberal coalition with the SPD and the German Chancellor Willy Brandt. Walter Scheel was he who initiated the foreign policy reversal. Despite a very small majority he and Willy Brandt sat by the controversial New Ostpolitik. This policy was within the FDP quite controversial, especially since the entry into the Federal Government defeats in state elections in North Rhine-Westphalia, Lower Saxony and Saarland on 14 June 1970 followed. In Hanover and Saarbrücken, the party left the parliament.
After the federal party congress in Bonn, just a week later supported the policy of the party leadership and Scheel had confirmed in office, founded by Siegfried party rights Zoglmann 11 July 1970 a "non-partisan" organization called the National-Liberal action on the Hohensyburgstraße - to fall with the goal of ending the left-liberal course of the party and Scheel. However, this was not. Zoglmann supported in October 1970 a disapproval resolution of opposition to Treasury Secretary Alexander Möller, Erich Mende, Heinz Starke, and did the same. A little later all three declared their withdrawal from the FDP; Mende and Strong joined the CDU in, Zoglmann later founded the German Union, which does not make it past the status of a splinter party.
The foreign policy and the socio-political changes were made in 1971 by the Freiburg theses, which were as Rowohlt Paperback sold more than 100,000 times, on a theoretical basis, the FDP is committed to "social liberalism" and social reforms. Walter Scheel was first foreign minister and vice chancellor, 1974, he was then second-liberal President and paving the way for inner-party the previous interior minister Hans-Dietrich Genscher free.
From 1969 to 1974 supported the FDP Chancellor Willy Brandt, then she ruled at the Helmut Schmidt page. Already at the end of the 70s did not seem to be sufficient for a coalition the similarities between FDP and SPD, but the CDU / CSU chancellor candidate of Franz Josef Strauss in 1980 left the two parties go again together in the federal election. The FDP, however, saw more and more the differences to the SPD, especially in economic policy. The position on the question of NATO Double-Track Decision Chancellor Schmidt's own SPD had not behind. Also contradictions within the FDP were always greater
1982–1998: joined Kohl government, with economic transition and reunification.
In all federal election campaigns since the 1980s, the party sided with the CDU and CSU, the main conservative parties in Germany. An exception to the party policy was made in the 2002 campaign, in which it adopted a position of "equidistance" to the CDU and SPD. Following German reunification in 1990, the FDP merged with the Association of Free Democrats, a grouping of liberals from East Germany and the Liberal Democratic Party of Germany.
On 1 October 1982, FDP elected together with the CDU / CSU parliamentary group of the CDU party chairman Helmut Kohl as the new Chancellor (→ turn (West Germany)). The coalition change had severe internal conflicts result, the FDP then lost about 20 percent of its 86,500 members, as reflected in the general election in 1983 reflected (drop from 10.6 percent to 7.0 percent). The members went mostly to the SPD, the Greens and newly formed splinter parties, such as the left-liberal party Liberal Democrats (LD) across. Under the exiting members was also the former FDP General Secretary and later EU Commissioner Günter Verheugen. At the party convention in November 1982, the Schleswig-Holstein state chairman Uwe Ronneburger challenged Hans-Dietrich Genscher as party chairman. Ronneburger received 186 of the votes - about 40 percent - and was just narrowly defeated by Genscher.
Young FDP members who did not agree with the politics of the FDP youth organization Young Democrats, had founded in 1980 the Young Liberals (JuLis). For a time there were two youth organizations side by side until the JuLis penetrated due to the turn and the new official youth organization of the FDP were. The Young Democrats split from the FDP and were left a party independent youth organization.
In the time of reunification, the FDP's objective of a special economic zone in the former East Germany, but could not prevail against the CDU / CSU, as this would prevent any loss of votes in the five new federal states in the general election in 1990.
During the political upheavals of 1989/1990 originated in the GDR new liberal parties like the FDP East Germany or the German Forum Party. They formed the Liberal Democratic Party, who had previously acted as a block party on the side of the SED and with Manfred Gerlach also the last Council of State of the GDR presented, the Alliance of Free Democrats, (BFD). Within the FDP came in the following years to considerable internal discussions about dealing with the former block party. Even before the reunification of Germany united on a joint congress in Hanover, West German FDP with the parties to the BFD and the former block party NDPD to the first all-German party. Both party factions brought the FDP a great, albeit short-lived, increase in membership. In the first all-German Bundestag elections, the CDU/CSU/FDP center-right coalition was confirmed, the FDP received 11.0 percent of the valid votes (79 seats) and won (in Halle (Saale)) the first direct mandate since 1957.
During the 1990s, the FDP won between 6.2 and 11 percent of the vote in Bundestag elections. It last participated in the federal government by representing the junior partner in the government of Chancellor Helmut Kohl of the CDU.
In 1998, CDU/CSU and FDP coalition lost the federal election, ended the FDP for near than 30 years in government coalition. They went back into opposition until 2009, a new center-right coalition government formed.
2005 federal election.
In the 2005 general election the party won 9.8 percent of the vote and 61 federal deputies, an unpredicted improvement from prior opinion polls. It is believed that this was partly due to tactical voting by CDU and Christian Social Union of Bavaria (CSU) alliance supporters who hoped for stronger market-oriented economic reforms than the CDU/CSU alliance called for. However, because the CDU did worse than predicted, the FDP and the CDU/CSU alliance were unable to form a coalition government. At other times, for example after the 2002 federal election, a coalition between the FDP and CDU/CSU was impossible primarily because of the weak results of the FDP.
The CDU/CSU parties had achieved the 3rd worst performance in German postwar history with only 35.2 percent of the votes. Therefore, the FDP wasn't able to form a coalition with its preferred partners, the CDU/CSU parties. As a result, the party was considered as a potential member of two other political coalitions, following the election. One possibility was a partnership between the FDP, the Social Democratic Party of Germany (SPD) and the Alliance 90/The Greens, known as a "traffic light coalition", named after the colors of the three parties. This coalition was ruled out, because the FDP considered the Social Democrats and the Greens insufficiently committed to market-oriented economic reform. The other possibility was a CDU-FDP-Green coalition, known as a "Jamaica coalition" because of the colours of the three parties. This coalition wasn't concluded either, since the Greens ruled out participation in any coalition with the CDU/CSU. Instead, the CDU formed a Grand coalition with the SPD, and the FDP entered the opposition. FDP leader Guido Westerwelle became the unofficial leader of the opposition by virtue of the FDP's position as the largest opposition party in the Bundestag.
In the 2009 European parliament elections, the FDP received 11% of the national vote (2,888,084 votes in total) and returned 12 MEPs.
2009 federal election.
In the national vote on 27 September 2009 the FDP increased its share of the vote by 4.8 percentage points to 14.6%, an all-time record so far. This percentage was enough to offset a decline in the CDU/CSU's vote compared to 2005, to create a CDU-FDP governing coalition in the Bundestag with a 53% majority of seats. On election night, party leader Westerwelle said his party would work to ensure that civil liberties were respected and that Germany got an "equitable tax system and better education opportunities."
The party also made gains in the two state elections held at the same time, acquiring sufficient seats for a CDU-FDP coalition in the northernmost state, Schleswig-Holstein and gaining enough votes in left leaning Brandenburg to clear the 5% hurdle to enter that state's parliament.
However, after reaching its best ever election result in 2009, the FDP's support collapsed. The party’s policy pledges were put on hold by Merkel as the recession of 2009 unfolded and with the onset of the European debt crisis in 2010. By the end of 2010, the party's support had dropped to as low as 5%. The FDP retained their seats in the state elections in North Rhine-Westphalia, which was held six months after the federal election, but out of the seven state elections that have been held since 2009, the FDP have lost all their seats in five of them due to failing to cross the 5% threshold.
Support for the party further eroded amid infighting and an internal rebellion over euro-area bailouts during the debt crisis. Westerwelle stepped down as party leader in 2011 after the party was wiped out in Saxony-Anhalt, Rhineland-Palatinate and lost half its seats in Baden-Württemberg. He was replaced on 13 May 2011 by Philipp Rösler. The change in leadership failed to revive the FDP's fortunes and in the next series of state elections the party lost all its seats in Bremen, Mecklenburg-Vorpommern and Berlin. In Berlin the party lost nearly 3/4 of the support they had in the previous election. In March 2012, they also lost all their seats in Saarland. However, this was averted in the Schleswig-Holstein state elections, when they achieved 8% of the vote, which was a severe loss of seats but still over the 5% threshold. In the snap elections in North Rhine-Westphalia a week later, the FDP not only crossed the threshold, but also increased its share of the votes to 2 percentage points higher than the previous state election. This was attributed to the local leadership of Christian Lindner.
2013 federal election.
In the federal elections on 22 September 2013 the FDP came up just short of the 5% threshold. It failed to win any directly elected seats either; it has only won directly elected seats at only one election since 1953, and hasn't won any directly elected seats since 1990. As a result, the FDP will be out of the Bundestag for the first time since 1949.
2014 European election.
In the 2014 European parliament elections, the FDP received 3.36% of the national vote (986,253 votes in total) and returned 3 MEPs.
Ideology.
The FDP adheres to a classical liberal ideology, advocating liberalism in both the economic sphere and social sphere. The current guidelines of the FDP are enshrined in the "Principles of Wiesbaden". A key objective of the FDP is the "strengthening of freedom and individual responsibility".
Throughout its history, the FDP's policies have shifted between emphasis on social liberalism and economic liberalism. Since the 1980s, the FDP has maintained a consistent pro-business stance. The FDP supports strong competition laws and a minimum standard of welfare protection for every citizen. In addition, the FDP endorses changes to social welfare and health care systems with laws that would require every employed citizen to invest in a private social security account. The party supports a bracket income tax system, as opposed to the current 'linear' system, and, in the long term, a flat tax. The FDP aims for the introduction of a citizen's dividend, which collects all the tax-financed social welfare and social security funds of the state.
The FDP supports gay rights; former party leader Guido Westerwelle is openly gay. Yet the party's group in parliament voted against an oppositional motion for gay marriage, in order not to threaten the coalition with the Christian Democrats.
The FDP describes itself as a pro-European party, although the minority national-liberal faction is Eurosceptic. The FDP wants a politically integrated EU with a Common Foreign and Security Policy, but supported a referendum on the Treaty of Lisbon. The FDP advocates the accession of Turkey to the EU, although this would require Turkey to fulfil all criteria.
Election results.
Federal Parliament ("Bundestag").
Below are charts of the results that the Free Democratic Party has secured in each election to the federal Bundestag. Timelines showing the number of seats and percentage of party list votes won are on the right.

</doc>
<doc id="10826" url="https://en.wikipedia.org/wiki?curid=10826" title="Fax">
Fax

Fax (short for facsimile), sometimes called telecopying or telefax, is the telephonic transmission of scanned printed material (both text and images), normally to a telephonyoce number connected to a printer or other output device. The original document is scanned with a fax machine (or a telecopier), which processes the contents (text or images) as a single fixed graphic image, converting it into a bitmap, and then transmitting it through the telephone system in the form of audio-frequency tones. The receiving fax machine interprets the tones and reconstructs the image, printing a paper copy. Early systems used direct conversions of image darkness to audio tone in a continuous or analog manner. Since the 1980s, most machines modulate the transmitted audio frequencies using a digital representation of the page which is compressed to quickly transmit areas which are all-white or all-black.
History.
Wire transmission.
Scottish inventor Alexander Bain worked on chemical mechanical fax type devices and in 1846 was able to reproduce graphic signs in laboratory experiments. He received patent 9745 on May 27, 1843 for his "Electric Printing Telegraph." Frederick Bakewell made several improvements on Bain's design and demonstrated a telefax machine. The Pantelegraph was invented by the Italian physicist Giovanni Caselli. He introduced the first commercial telefax service between Paris and Lyon in 1865, some 11 years before the invention of the telephone.
In 1881, English inventor Shelford Bidwell constructed the "scanning phototelegraph" that was the first telefax machine to scan any two-dimensional original, not requiring manual plotting or drawing. Around 1900, German physicist Arthur Korn invented the "", widespread in continental Europe especially, since a widely noticed transmission of a wanted-person photograph from Paris to London in 1908, used until the wider distribution of the radiofax. Its main competitors were the "Bélinographe" by Édouard Belin first, then since the 1930s the "Hellschreiber", invented in 1929 by German inventor Rudolf Hell, a pioneer in mechanical image scanning and transmission.
The 1888 invention of the telautograph by Elisha Grey marked a further development in fax technology, allowing users to send signatures over long distances, thus allowing the verification of identification or ownership over long distances.
On May 19, 1924, scientists of the AT&T Corporation "by a new process of transmitting pictures by electricity" sent 15 photographs by telephone from Cleveland to New York City, such photos suitable for newspaper reproduction. Previously, photographs had been sent over the radio using this process.
The Western Union "Deskfax" fax machine, announced in 1948, was a compact machine that fit comfortably on a desktop, using special spark printer paper.
Wireless transmission.
As a designer for the Radio Corporation of America (RCA), in 1924, Richard H. Ranger invented the wireless photoradiogram, or transoceanic radio facsimile, the forerunner of today’s "fax" machines. A photograph of President Calvin Coolidge sent from New York to London on November 29, 1924 became the first photo picture reproduced by transoceanic radio facsimile. Commercial use of Ranger’s product began two years later. Also in 1924, Herbert E. Ives of AT&T Corporation transmitted and reconstructed the first color facsimile, using color separations. Around 1952 or so, Finch Facsimile, a highly developed machine, was described in detail in a book; it was never manufactured in quantity.
By the late 1940s, radiofax receivers were sufficiently miniaturized to be fitted beneath the dashboard of Western Union's "Telecar" telegram delivery vehicles.
In the 1960s, the United States Army transmitted the first photograph via satellite facsimile to Puerto Rico from the Deal Test Site using the Courier satellite.
Radio fax is still in limited use today for transmitting weather charts and information to ships at sea.
Telephone transmission.
In 1964, Xerox Corporation introduced (and patented) what many consider to be the first commercialized version of the modern fax machine, under the name (LDX) or Long Distance Xerography. This model was superseded two years later with a unit that would truly set the standard for fax machines for years to come. Up until this point facsimile machines were very expensive and hard to operate. In 1966, Xerox released the Magnafax Telecopier, a smaller, 46-pound facsimile machine. This unit was far easier to operate and could be connected to any standard telephone line. This machine was capable of transmitting a letter-sized document in about six minutes. The first sub-minute, digital fax machine was developed by Dacom, which built on digital data compression technology originally developed at Lockheed for satellite communication.
By the late 1970s, many companies around the world (especially Japan), entered the fax market. Very shortly after a new wave of more compact, faster and efficient fax machines would hit the market. Xerox continued to refine the fax machine for years after their ground-breaking first machine. In later years it would be combined with copier equipment to create the hybrid machines we have today that copy, scan and fax. Some of the lesser known capabilities of the Xerox fax technologies included their Ethernet enabled Fax Services on their 8000 workstations in the early 1980s.
Prior to the introduction of the ubiquitous fax machine, one of the first being the Exxon Qwip in the mid-1970s, facsimile machines worked by optical scanning of a document or drawing spinning on a drum. The reflected light, varying in intensity according to the light and dark areas of the document, was focused on a photocell so that the current in a circuit varied with the amount of light. This current was used to control a tone generator (a modulator), the current determining the frequency of the tone produced. This audio tone was then transmitted using an acoustic coupler (a speaker, in this case) attached to the microphone of a common telephone handset. At the receiving end, a handset’s speaker was attached to an acoustic coupler (a microphone), and a demodulator converted the varying tone into a variable current that controlled the mechanical movement of a pen or pencil to reproduce the image on a blank sheet of paper on an identical drum rotating at the same rate.
Computer facsimile interface.
In 1985, Dr. Hank Magnuski, founder of GammaLink, produced the first computer fax board, called GammaFax.
Fax in the 21st century.
Although businesses usually maintain some kind of fax capability, the technology has faced increasing competition from Internet-based alternatives. In some countries, because electronic signatures on contracts are not yet recognized by law, while faxed contracts with copies of signatures are, fax machines enjoy continuing support in business. In Japan, faxes are still used extensively for cultural and graphemic reasons and are available for sending to both domestic and international recipients from over 81% of all convenience stores nationwide. Convenience-store fax machines commonly print the slightly re-sized content of the sent fax in the electronic confirmation-slip, in A4 paper size.
In many corporate environments, freestanding fax machines have been replaced by fax servers and other computerized systems capable of receiving and storing incoming faxes electronically, and then routing them to users on paper or via an email (which may be secured). Such systems have the advantage of reducing costs by eliminating unnecessary printouts and reducing the number of inbound analog phone lines needed by an office.
The once ubiquitous fax machine has also begun to disappear from the small office and home office environments. Remotely hosted fax-server services are widely available from VoIP and e-mail providers allowing users to send and receive faxes using their existing e-mail accounts without the need for any hardware or dedicated fax lines. Personal computers have also long been able to handle incoming and outgoing faxes using analogue modems or ISDN, eliminating the need for a stand-alone fax machine. These solutions are often ideally suited for users who only very occasionally need to use fax services. There are 17 million fax machines in the US, about one every 4.47 square miles.
Capabilities.
There are several indicators of fax capabilities: Group, class, data transmission rate, and conformance with ITU-T (formerly CCITT) recommendations. Since the 1968 Carterphone decision, most fax machines have been designed to connect to standard PSTN lines and telephone numbers.
Group.
Analog.
Group 1 and 2 faxes are sent in the same manner as a frame of analog television, with each scanned line transmitted as a continuous analog signal. Horizontal resolution depended upon the quality of the scanner, transmission line, and the printer. Analog fax machines are obsolete and no longer manufactured. ITU-T Recommendations T.2 and T.3 were withdrawn as obsolete in July 1996.
Digital.
A major breakthrough in the development of the modern facsimile system was the result of digital technology, where the analog signal from scanners was digitized and then compressed, resulting in the ability to transmit high rates of data across standard phone lines. The first digital fax machine was the Dacom Rapidfax first sold in late 1960s, which incorporated digital data compression technology developed by Lockheed for transmission of images from satellites.
Group 3 and 4 faxes are digital formats, and take advantage of digital compression methods to greatly reduce transmission times.
Fax Over IP (FoIP) can transmit and receive pre-digitized documents at near realtime speeds using ITU-T recommendation T.38 to send digitised images over an IP network using JPEG compression. T.38 is designed to work with VoIP services and often supported by analog telephone adapters used by legacy fax machines that need to connect through a VoIP service. Scanned documents are limited to the amount of time the user takes to load the document in a scanner and for the device to process a digital file. The resolution can vary from as little as 150 DPI to 9600 DPI or more. This type of faxing is not related to the e-mail to fax service that still uses fax modems at least one way.
Class.
Computer modems are often designated by a particular fax class, which indicates how much processing is offloaded from the computer's CPU to the fax modem.
Data transmission rate.
Several different telephone line modulation techniques are used by fax machines. They are negotiated during the fax-modem handshake, and the fax devices will use the highest data rate that both fax devices support, usually a minimum of 14.4 kbit/s for Group 3 fax.
Note that "Super Group 3" faxes use V.34bis modulation that allows a data rate of up to 33.6 kbit/s.
Compression.
As well as specifying the resolution (and allowable physical size of the image being faxed), the ITU-T T.4 recommendation specifies two compression methods for decreasing the amount of data that needs to be transmitted between the fax machines to transfer the image. The two methods defined in T.4 are:
An additional method is specified in T.6:
Later, other compression techniques were added as options to ITU-T recommendation T.30, such as the more efficient JBIG (T.82, T.85) for bi-level content, and JPEG (T.81), T.43, MRC (T.44), and T.45 for grayscale, palette, and colour content. Fax machines can negotiate at the start of the T.30 session to use the best technique implemented on both sides.
Modified Huffman.
Modified Huffman (MH), specified in T.4 as the one-dimensional coding scheme, is a codebook-based run-length encoding scheme optimised to efficiently compress whitespace. As most faxes consist mostly of white space, this minimises the transmission time of most faxes. Each line scanned is compressed independently of its predecessor and successor.
Modified READ.
Modified READ (MR), specified as an optional two-dimensional coding scheme in T.4, encodes the first scanned line using MH. The next line is compared to the first, the differences determined, and then the differences are encoded and transmitted. This is effective as most lines differ little from their predecessor. This is not continued to the end of the fax transmission, but only for a limited number of lines until the process is reset and a new 'first line' encoded with MH is produced. This limited number of lines is to prevent errors propagating throughout the whole fax, as the standard does not provide for error-correction. MR is an optional facility, and some fax machines do not use MR in order to minimise the amount of computation required by the machine. The limited number of lines is two for 'Standard' resolution faxes, and four for 'Fine' resolution faxes.
Modified Modified READ.
The ITU-T T.6 recommendation adds a further compression type of Modified Modified READ (MMR), which simply allows for a greater number of lines to be coded by MR than in T.4. This is because T.6 makes the assumption that the transmission is over a circuit with a low number of line errors such as digital ISDN. In this case, there is no maximum number of lines for which the differences are encoded.
JBIG.
In 1999, ITU-T recommendation T.30 added JBIG (ITU-T T.82) as another lossless bi-level compression algorithm, or more precisely a "fax profile" subset of JBIG (ITU-T T.85). JBIG-compressed pages result in 20% to 50% faster transmission than MMR-compressed pages, and up to 30-times faster transmission if the page includes halftone images.
JBIG performs adaptive compression, that is both the encoder and decoder collect statistical information about the transmitted image from the pixels transmitted so far, in order to predict the probability for each next pixel being either black or white. For each new pixel, JBIG looks at ten nearby, previously transmitted pixels. It counts, how often in the past the next pixel has been black or white in the same neighborhood, and estimates from that the probability distribution of the next pixel. This is fed into an arithmetic coder, which adds only a small fraction of a bit to the output sequence if the more probable pixel is then encountered.
The ITU-T T.85 "fax profile" constrains some optional features of the full JBIG standard, such that codecs do not have to keep data about more than the last three pixel rows of an image in memory at any time. This allows the streaming of "endless" images, where the height of the image may not be known until the last row is transmitted.
ITU-T T.30 allows fax machines to negotiate one of two options of the T.85 "fax profile":
Matsushita Whiteline Skip.
A proprietary compression scheme employed on Panasonic fax machines is Matsushita Whiteline Skip (MWS). It can be overlaid on the other compression schemes, but is operative only when two Panasonic machines are communicating with one another. This system detects the blank scanned areas between lines of text, and then compresses several blank scan lines into the data space of a single character. (JBIG implements a similar technique called "typical prediction", if header flag TPBON is set to 1.)
Typical characteristics.
Group 3 fax machines transfer one or a few printed or handwritten pages per minute in black-and-white (bitonal) at a resolution of 204×98 (normal) or 204×196 (fine) dots per square inch. The transfer rate is 14.4 kbit/s or higher for modems and some fax machines, but fax machines support speeds beginning with 2400 bit/s and typically operate at 9600 bit/s. The transferred image formats are called ITU-T (formerly CCITT) fax group 3 or 4. Group 3 faxes have the suffix codice_1 and the MIME type image/g3fax.
The most basic fax mode transfers black and white colors only. The original page is scanned in a resolution of 1728 pixels/line and 1145 lines/page (for A4). The resulting raw data is compressed using a modified Huffman code optimized for written text, achieving average compression factors of around 20. Typically a page needs 10 s for transmission, instead of about 3 minutes for the same uncompressed raw data of 1728×1145 bits at a speed of 9600 bit/s. The compression method uses a Huffman codebook for run lengths of black and white runs in a single scanned line, and it can also use the fact that two adjacent scanlines are usually quite similar, saving bandwidth by encoding only the differences.
Fax classes denote the way fax programs interact with fax hardware. Available classes include Class 1, Class 2, Class 2.0 and 2.1, and Intel CAS. Many modems support at least class 1 and often either Class 2 or Class 2.0. Which is preferable to use depends on factors such as hardware, software, modem firmware, and expected use.
Printing process.
Fax machines from the 1970s to the 1990s often used direct thermal printers with rolls of thermal paper as their printing technology, but since the mid-1990s there has been a transition towards plain-paper faxes:- thermal transfer printers, inkjet printers and laser printers.
One of the advantages of inkjet printing is that inkjets can affordably print in color; therefore, many of the inkjet-based fax machines claim to have color fax capability. There is a standard called ITU-T30e (formally ITU-T Recommendation T.30 Annex E ) for faxing in color; unfortunately, it is not widely supported, so many of the color fax machines can only fax in color to machines from the same manufacturer.
Stroke speed.
Stroke speed in facsimile systems is the rate at which a fixed line perpendicular to the direction of scanning is crossed in one direction by a scanning or recording spot. Stroke speed is usually expressed as a number of strokes per minute. When the fax system scans in both directions, the stroke speed is twice this number. In most conventional 20th century mechanical systems, the stroke speed is equivalent to drum speed.
Fax paper.
As a precaution, thermal fax paper is typically not accepted in archives or as documentary evidence in some courts of law unless photocopied. This is because the image-forming coating is eradicable and brittle, and it tends to detach from the medium after a long time in storage.
Internet fax.
One popular alternative is to subscribe to an Internet fax service, allowing users to send and receive faxes from their personal computers using an existing email account. No software, fax server or fax machine is needed. Faxes are received as attached TIFF or PDF files, or in proprietary formats that require the use of the service provider's software. Faxes can be sent or retrieved from anywhere at any time that a user can get Internet access. Some services offer secure faxing to comply with stringent HIPAA and Gramm–Leach–Bliley Act requirements to keep medical information and financial information private and secure. Utilizing a fax service provider does not require paper, a dedicated fax line, or consumable resources.
Another alternative to a physical fax machine is to make use of computer software which allows people to send and receive faxes using their own computers, utilizing fax servers and unified messaging. A virtual (email) fax can be printed out and then signed and scanned back to computer before being emailed. Also the sender can attach a digital signature to the document file.
With the surging popularity of mobile phones, virtual fax machines can now be downloaded as applications for Android and iOS. These applications make use of the phone's internal camera to scan fax documents for upload or they can import from various cloud services.

</doc>
<doc id="10827" url="https://en.wikipedia.org/wiki?curid=10827" title="Film crew">
Film crew

A movie crew is a group of people hired by a production company for the purpose of producing a film or motion picture. The crew is distinguished from the "cast" as the "cast" are understood to be the actors who appear in front of the camera or provide voices for characters in the film. The "crew" is also separate from the "producers" as the "producers" are the ones who own a portion of either the film company or the film's intellectual property rights. A film crew is divided into different departments, each of which specializes in a specific aspect of the production. Film crew positions have evolved over the years, spurred by technological change, but many traditional jobs date from the early 20th century and are common across jurisdictions and film-making cultures.
Motion picture projects have three discrete stages: development, production and distribution. Within the production stage there are also three clearly defined sequential phases — pre-production, principal photography and post-production — and many film crew positions are associated with only one or two of the phases. Distinctions are also made between above-the-line personnel (such as the director, the screenwriter and the producers) who begin their involvement during the project's development stage, and the below-the-line "technical" crew involved only with the production stage.
A study of the 100 top-grossing films of each year between 1994 and 2013 found that there were an average of 588 crew credits per film, however, profitable independent films have been made with crews of less than a dozen. 
Television crew positions are derived from those of film crew.
Director.
The director is considered to be a separate entity, not within the film crew's departmental structure.
Production.
"Production" is generally not considered a department as such, but rather as a series of functional groups. These include the film's producers and executive producers such as the production manager, the production coordinator, and their assistants; the various assistant directors; the accounting staff; and sometimes the locations manager and their assistants. 
Additional production credits.
Since the turn of the 21st century, several additional professionals are now routinely listed in the production credits on most major motion pictures.
Camera & lighting.
Grip.
Grips are trained lighting and rigging technicians. Their main responsibility is to work closely with the electrical department to put in the non-electrical components of lighting set-ups required for a shot, such as flags, overheads, and bounces. On the sound stage, they move and adjust major set pieces when something needs to be moved to get a camera into position. In the US and Canada they may belong to the International Alliance of Theatrical Stage Employees.
Art department.
The art department in a major feature film can often number hundreds of people. Usually it is considered to include several sub-departments: the art department proper, with its art director, set designers and draftsmen; set decoration, under the set decorator; props, under the props master; construction, headed by the construction coordinator; scenic, headed by the key scenic artist; and special effects.
Art.
Within the overall art department is a sub-department, also called the art department—which can be confusing. This consists of the people who design the sets and create the graphic art.
Hair and make-up.
Some actors or actresses have personal makeup artists or hair stylists.
Special effects.
This department oversees the mechanical effects—also called practical or physical effects—that create optical illusions during live-action shooting. It is not to be confused with the Visual effects department, which adds photographic effects during filming to be altered later during video editing in the post-production process.
Post-production.
Visual effects.
Visual effects commonly refers to post-production alterations of the film's images. The on set VFX crew works to prepare shots and plates for future visual effects. This may include adding tracking markers, taking and asking for reference plates and helping the Director understand the limitations and ease of certain shots that will effect the future post production. A VFX crew can also work alongside the Special effects department for any on-set optical effects that need physical representation during filming (on camera.)
Animation.
Animation film crews have many of the same roles and departments as live-action films (including directing, production, editing, camera, sound, and so on), but nearly all on-set departments (lighting, electrical, grip, sets, props, costume, hair, makeup, special effects, and stunts) were traditionally replaced with a single animation department made up of various types of animators (character, effects, in-betweeners, cleanup, and so on). In traditional animation, the nature of the medium meant that "everything" was literally flattened into the drawn lines and solid colors that became the characters, making nearly all live-action positions irrelevant. Because animation has traditionally been so labor-intensive and thus expensive, animation films normally have a separate story department in which storyboard artists painstakingly develop scenes to make sure they make sense before they are actually animated.
However, since the turn of the 21st century, modern 3D computer graphics and computer animation have made possible a level of rich detail never seen before. Many animated films now have specialized artists and animators who act as the virtual equivalent of lighting technicians, grips, costume designers, props masters, set decorators, set dressers, and cinematographers. They make artistic decisions strongly similar to those of their live-action counterparts, but implement them in a virtual space that exists only in software rather than on a physical set. There have been major breakthroughs in the simulation of hair since 2005, meaning that hairstylists have been called in since then to consult on a few animation projects.

</doc>
<doc id="10828" url="https://en.wikipedia.org/wiki?curid=10828" title="Fear">
Fear

Fear is a feeling induced by perceived danger or threat that occurs in certain types of organisms, which causes a change in metabolic and organ functions and ultimately a change in behavior, such as fleeing, hiding or freezing from perceived traumatic events. Fear in human beings may occur in response to a specific stimulus occurring in the present, or in anticipation or expectation of a future threat perceived as a risk to body or life. The fear response arises from the perception of danger leading to confrontation with or escape from/avoiding the threat (also known as the fight-or-flight response), which in extreme cases of fear (horror and terror) can be a freeze response or paralysis. 
In humans and animals, fear is modulated by the process of cognition and learning. Thus fear is judged as rational or appropriate and irrational or inappropriate. An irrational fear is called a phobia.
Psychologists such as John B. Watson, Robert Plutchik, and Paul Ekman have suggested that there is only a small set of basic or innate emotions and that fear is one of them. This hypothesized set includes such emotions as acute stress reaction, anger, angst, anxiety, fright, horror, joy, panic and sadness. Fear is closely related to, but should be distinguished from, the emotion anxiety, which occurs as the result of threats that are perceived to be uncontrollable or unavoidable. The fear response serves survival by generating appropriate behavioral responses, so it has been preserved throughout evolution.
Etymology.
The noun "fear" stems from the Middle English words "feer", "fere" and "fer", the Old English "fǣr" for "calamity" or "danger" (and its verb "fǣran", "frighten", but also "revere") and is related to the Proto-Germanic "fērą", "danger", the Proto-Indo-European "*per", "to attempt, try, research, risk". In German the word for "danger" is "Gefahr", in Dutch "gevaar", in Swedish "fara", in Albanian "frikë", and in Latin "perīculum", which is the root for the term in the Romance languages.
As a noun "fear" can be used in three ways with different meanings: In the uncountable form fear is a strong, uncontrollable and unpleasant emotion caused by actual or perceived danger, e.g. "He was struck by fear on seeing the snake." In the countable form, and when used with the indefinite article, a "fear" means a phobia, a sense of fear induced by something or someone, e.g. "Not everybody has the same fears; I have a fear of ants." In an uncountable form it can also mean extreme veneration or awe, as toward a supreme being or deity.
Types.
Top 10 types in the U.S..
In a 2005 Gallup poll (U.S.A.), a national sample of adolescents between the ages of 13 and 17 were asked what they feared the most. The question was open-ended and participants were able to say whatever they wanted. The top ten fears were, in order: terrorist attacks, spiders, death, being a failure, war, criminal or gang violence, being alone, the future, and nuclear war.
In an estimate of what people fear the most, book author Bill Tancer analyzed the most frequent online queries that involved the phrase, "fear of..." following the assumption that people tend to seek information on the issues that concern them the most. His top ten list of fears published 2008 consisted of flying, heights, clowns, intimacy, death, rejection, people, snakes, failure, and driving.
Common phobias.
According to surveys , some of the most common fears are of demons and ghosts, the existence of evil powers, cockroaches, spiders, snakes, heights, water, enclosed spaces, tunnels, bridges, needles, social rejection, failure, examinations and public speaking.
A person with arachnophobia may panic or feel uneasy around a spider even though most are harmless. Sometimes, even an object resembling a spider can trigger a panic attack in an arachnophobic individual, which is called automatonophobia. 
One of the most common phobias in humans is the glossophobia or the fear of public speaking. People may be comfortable speaking inside a room, but when it becomes public speaking, fear enters in the form of suspicion over whether the words uttered are correct or incorrect, because there are many to judge them. Another common fear can be fear of pain, or of someone damaging a person. Fear of pain in a plausible situation brings flinching, or cringing.
Fear of death.
Death anxiety is multidimensional; it covers "fears related to one's own death, the death of others, fear of the unknown after death, fear of obliteration, and fear of the dying process, which includes fear of a slow death and a painful death".
The Yale philosopher Shelly Kagan examined fear of death in a 2007 Yale open course by examining the following questions: Is fear of death a reasonable appropriate response? What conditions are required and what are appropriate conditions for feeling fear of death? What is meant by fear, and how much fear is appropriate? According to Kagan for fear in general to make sense, three conditions should be met: the object of fear needs to be "something bad", there needs to be a non-negligible chance that the bad state of affairs will happen, and there needs to be some uncertainty about the bad state of affairs. The amount of fear should be appropriate to the size of "the bad". If the 3 conditions aren't met, fear is an inappropriate emotion. He argues, that death does not meet the first two criteria, even if death is a "deprivation of good things" and even if one believes in a painful afterlife. Because death is certain, it also does not meet the third criteria, but he grants that the unpredictability of when one dies "may" be cause to a sense of fear.
In a 2003 study of 167 women and 121 men, aged 65–87, low self-efficacy predicted fear of the unknown after death and fear of dying for women and men better than demographics, social support, and physical health. Fear of death was measured by a "Multidimensional Fear of Death Scale" which included the 8 subscales Fear of Dying, Fear of the Dead, Fear of Being Destroyed, Fear for Significant Others, Fear of the Unknown, Fear of Conscious Death, Fear for the Body After Death, and Fear of Premature Death. In hierarchical multiple regression analysis the most potent predictors of death fears were low "spiritual health efficacy", defined as beliefs relating to one's perceived ability to generate spiritually based faith and inner strength, and low "instrumental efficacy", defined as beliefs relating to one's perceived ability to manage activities of daily living.
Psychologists have tested the hypothesis that fear of death motivates religious commitment, and assurances about an afterlife alleviate the fear and empirical research on this topic has been equivocal. Religiosity can be related to fear of death when the afterlife is portrayed as time of punishment. "Intrinsic religiosity", as opposed to mere "formal religious involvement" has been found to be negatively correlated with death anxiety. In a 1976 study people of various Christian denominations those most firm in their faith, attending religious services weekly were the least afraid of dying. The survey found a negative correlation between fear of death and "religious concern".
In a 2006 study of white, Christian men and women the hypothesis was tested that traditional, church-centered religiousness and de-institutionalized spiritual seeking are ways of approaching fear of death in old age. Both religiousness and spirituality were related to positive psychosocial functioning, but only church-centered religiousness protected subjects against the fear of death.
Fear of the unknown.
Fear of the unknown or irrational fear is caused by negative thinking which arises from anxiety. Many people are scared of the "unknown". The irrational fear can branch out to many areas such as the hereafter, the next ten years, or even tomorrow. In these cases specialists use False Evidence Appearing Real as a definition. Being scared makes people to anticipate and aggravate of what may lie ahead rather than plan and evaluate. E.g. Continuation of scholarly education, most educators perceive this as a risk that may cause them fear and stress and they would rather teach things they've been taught than go and do research. This can lead to habits such as laziness and procrastination. The ambiguity of a situations that tend to be uncertain and unpredictable can cause anxiety, other psychological and physical problems in some populations; especially those who engage it constantly. E.g. War-ridden or Conflict places, Terrorism, Abuse ...etc. Poor parenting that instills fear can also debilitate children's psyche development or personality. E.g. Parents tell their children not to talk to strangers in order to protect them. In school they would be motivated to not show fear in talking with strangers, but to be assertive and also aware of the risks and the environment that it takes place. Ambiguous and mixed messages like this can affect their self-esteem and self-confidence. Researcher's say talking to strangers isn't something to be thwarted but allowed in a parent's presence if required. Developing a sense of equanimity to handle various situations is often advocated as an antidote to irrational fear and essential skill by a number of ancient philosophies.
Causes.
People develop specific fears as a result of learning. This has been studied in psychology as fear conditioning, beginning with John B. Watson's Little Albert experiment in 1920, which was inspired after observing a child with an irrational fear of dogs. In this study, an 11-month-old boy was conditioned to fear a white rat in the laboratory. The fear became generalized to include other white, furry objects, such as a rabbit, dog, and even a ball of cotton.
Fear can be learned by experiencing or watching a frightening traumatic accident. For example, if a child falls into a well and struggles to get out, he or she may develop a fear of wells, heights (acrophobia), enclosed spaces (claustrophobia), or water (aquaphobia). There are studies looking at areas of the brain that are affected in relation to fear. When looking at these areas (such as the amygdala), it was proposed that a person learns to fear regardless of whether they themselves have experienced trauma, or if they have observed the fear in others. In a study completed by Andreas Olsson, Katherine I. Nearing and Elizabeth A. Phelps the amygdala were affected both when subjects observed someone else being submitted to an aversive event, knowing that the same treatment awaited themselves, and when subjects were subsequently placed in a fear-provoking situation. This suggests that fear can develop in both conditions, not just simply from personal history.
Fear is affected by cultural and historical context. For example, in the early 20th century, many Americans feared polio, a disease that cripples the body part it affects, leaving that body part immobilized for the rest of one's life. There are consistent cross-cultural differences in how people respond to fear. Display rules affect how likely people are to show the facial expression of fear and other emotions.
Although many fears are learned, the capacity to fear is part of human nature. Many studies have found that certain fears (e.g. animals, heights) are much more common than others (e.g. flowers, clouds). These fears are also easier to induce in the laboratory. This phenomenon is known as preparedness. Because early humans that were quick to fear dangerous situations were more likely to survive and reproduce, preparedness is theorized to be a genetic effect that is the result of natural selection .
From an evolutionary psychology perspective, different fears may be different adaptations that have been useful in our evolutionary past. They may have developed during different time periods. Some fears, such as fear of heights, may be common to all mammals and developed during the mesozoic period. Other fears, such as fear of snakes, may be common to all simians and developed during the cenozoic time period. Still others, such as fear of mice and insects, may be unique to humans and developed during the paleolithic and neolithic time periods (when mice and insects become important carriers of infectious diseases and harmful for crops and stored foods).
Fear is high only if the observed risk and seriousness both are high, and is low, if risk or seriousness is low.
Symptoms and signs.
Many physiological changes in the body are associated with fear, summarized as the fight-or-flight response. An inborn response for coping with danger, it works by accelerating the breathing rate (hyperventilation), heart rate, constriction of the peripheral blood vessels leading to blushing and vasodilation of the central vessels (pooling), increasing muscle tension including the muscles attached to each hair follicle to contract and causing "goose bumps", or more clinically, piloerection (making a cold person warmer or a frightened animal look more impressive), sweating, increased blood glucose (hyperglycemia), increased serum calcium, increase in white blood cells called neutrophilic leukocytes, alertness leading to sleep disturbance and "butterflies in the stomach" (dyspepsia). This primitive mechanism may help an organism survive by either running away or fighting the danger. With the series of physiological changes, the consciousness realizes an emotion of fear.
In animals.
Often laboratory studies with rats are conducted to examine the acquisition and extinction of conditioned fear responses. In 2004, researchers conditioned rats ("rattus norvegicus") to fear a certain stimulus, through electric shock. The researchers were able to then cause an extinction of this conditioned fear, to a point that no medications or drugs were able to further aid in the extinction process. However the rats did show signs of avoidance learning, not fear, but simply avoiding the area that brought pain to the tests rats. The avoidance learning of rats is seen as a conditioned response, and therefore the behavior can be unconditioned, as supported by the earlier research. 
Species-specific defense reactions (SSDRs) or avoidance learning in nature is the specific tendency to avoid certain threats or stimuli, it is how animals survive in the wild. Humans and animals both share these species-specific defense reactions, such as the flight, fight, which also include pseudo-aggression, fake or intimidating aggression, freeze response to threats, which is controlled by the sympathetic nervous system. These SSDRs are learned very quickly through social interactions between others of the same species, other species, and interaction with the environment. These acquired sets of reactions or responses are not easily forgotten. The animal that survives is the animal that already knows what to fear and how to avoid this threat. An example in humans is the reaction to the sight of a snake, many jump backwards before cognitively realizing what they are jumping away from, and in some cases it is a stick rather than a snake.
As with many functions of the brain, there are various regions of the brain involved in deciphering fear in humans and other nonhuman species. The amygdala communicates both directions between the prefrontal cortex, hypothalamus, the sensory cortex, the hippocampus, thalamus, septum, and the brainstem. The amygdala plays an important role in SSDR, such as the ventral amygdalofugal, which is essential for associative learning, and SSDRs are learned through interaction with the environment and others of the same species. An emotional response is created only after the signals have been relayed between the different regions of the brain, and activating the sympathetic nervous systems; which controls the flight, fight, freeze, fright, and faint response. Often a damaged amygdala can cause impairment in the recognition of fear (like the human case of patient S.M.). This impairment can cause different species to lack the sensation of fear, and often can become overly confident, confronting larger peers, or walking up to predatory creatures.
Robert C. Bolles (1970), a researcher at University of Washington, wanted to understand species-specific defense reactions and avoidance learning among animals, but found that the theories of avoidance learning and the tools that were used to measure this tendency were out of touch with the natural world. He theorized the species-specific defense reaction (SSDR). There are three forms of SSDRs: flight, fight (pseudo-aggression), or freeze. Even domesticated animals have SSDRs, and in those moments it is seen that animals revert to atavistic standards and become "wild" again. Dr. Bolles states that responses are often dependent on the reinforcement of a safety signal, and not the aversive conditioned stimuli. This safety signal can be a source of feedback or even stimulus change. Intrinsic feedback or information coming from within, muscle twitches, increased heart rate, is seen to be more important in SSRDs than extrinsic feedback, stimuli that comes from the external environment. Dr. Bolles found that most creatures have some intrinsic set of fears, to help assure survival of the species. Rats will run away from any shocking event, and pigeons will flap their wings harder when threatened, the wing flapping in pigeons and the scattered running of rats are considered a species-specific defense reaction or behavior. Bolles believed that SSDR are conditioned through pavlovian conditioning, and not operant conditioning; SSDR arise from the association between the environmental stimuli and adverse events. Michael S. Fanselow conducted an experiment, to test some specific defense reactions, he observed that rats in two different shock situations responded differently, on based on instinct or defensive topography, rather than contextual information.
Species specific defense responses are created out of fear, and are essential for survival. Rats that lack the gene stathmin show no avoidance learning, or a lack of fear, and will often walk directly up to cats and be eaten. Animals use these SSDR to continue living, to help increase their chance of fitness, by surviving long enough to procreate. Humans and animals alike have created fear to know what should avoided, and this fear can be learned through association with others in the community, or learned through personal experience with a creature, species, or situations that should be avoided. SSDRs are an evolutionary adaptation that has been seen in many species throughout the world including rats, chimpanzees, prairie dogs, and even humans, an adaptation created to help individual creatures survive in a hostile world.
The brain structure that is the center of most neurobiological events associated with fear is the amygdala, located behind the pituitary gland. The amygdala is part of a circuitry of fear learning. It is essential for proper adaptation to stress and specific modulation of emotional learning memory. In the presence of a threatening stimulus, the amygdala generates the secretion of hormones that influence fear and aggression. Once response to the stimulus in the form of fear or aggression commences, the amygdala may elicit the release of hormones into the body to put the person into a state of alertness, in which they are ready to move, run, fight, etc. This defensive response is generally referred to in physiology as the fight-or-flight response regulated by the hypothalamus, part of the limbic system. Once the person is in safe mode, meaning that there are no longer any potential threats surrounding them, the amygdala will send this information to the medial prefrontal cortex (mPFC) where it is stored for similar future situations, which is known as memory consolidation.
Neurocircuit in mammals.
Some of the hormones involved during the state of fight-or-flight include epinephrine, which regulates heart rate and metabolism as well as dilating blood vessels and air passages, norepinephrine increasing heart rate, blood flow to skeletal muscles and the release of glucose from energy stores. and cortisol which increases blood sugar, increases circulating neutrophilic leukocytes, calcium amongst other things.
After a situation which incites fear occurs, the amygdala and hippocampus record the event through synaptic plasticity. The stimulation to the hippocampus will cause the individual to remember many details surrounding the situation. Plasticity and memory formation in the amygdala are generated by activation of the neurons in the region. Experimental data supports the notion that synaptic plasticity of the neurons leading to the lateral amygdala occurs with fear conditioning. In some cases, this forms permanent fear responses such as post-traumatic stress disorder (PTSD) or a phobia. MRI and fMRI scans have shown that the amygdala in individuals diagnosed with such disorders including bipolar or panic disorder is larger and wired for a higher level of fear.
Pathogens can suppress amygdala activity. Rats infected with the toxoplasmosis parasite become less fearful of cats, sometimes even seeking out their urine-marked areas. This behavior often leads to them being eaten by cats. The parasite then reproduces within the body of the cat. There is evidence that the parasite concentrates itself in the amygdala of infected rats. In a separate experiment, rats with lesions in the amygdala did not express fear or anxiety towards unwanted stimuli. These rats pulled on levers supplying food that sometimes sent out electrical shocks. While they learned to avoid pressing on them, they did not distance themselves from these shock-inducing levers.
Several brain structures other than the amygdala have also been observed to be activated when individuals are presented with fearful vs. neutral faces, namely the occipitocerebellar regions including the fusiform gyrus and the inferior parietal / superior temporal gyri. Interestingly, fearful eyes, brows and mouth seem to separately reproduce these brain responses. Scientist from Zurich studies show that the hormone oxytocin related to stress and sex reduces activity in your brain fear center.
Pheromones and why fear can be contagious.
In threatening situations insects, aquatic organisms, birds, reptiles, and mammals emit odorant substances, initially called alarm substances, which are chemical signals now called alarm pheromones ("Schreckstoff" in German). This is to defend themselves and at the same time to inform members of the same species of danger and leads to observable behavior change like freezing, defensive behavior, or dispersion depending on circumstances and species. For example, stressed rats release odorant cues that cause other rats to move away from the source of the signal. Pheromones are synthesized, emitted and perceived by all living organisms studied to date, with the exception of viruses and prions: i.e. in bacteria, prokaryotes, plants, plankton, parasites, insects, invertebrates and vertebrates (aquatic organisms, birds, reptiles, and mammals).
After the discovery of pheromones in 1959, alarm pheromones were first described in 1968 in ants and earthworms, and 4 years later also found in mammals, both mice and rats. Over the next two decades identification and characterization of these pheromones proceeded in all manner of insects and sea animals, including fish, but it was not until 1990 that more insight into mammalian alarm pheromones was gleaned.
Early on, in 1985, a link between odors released by stressed rats and pain perception was discovered: unstressed rats exposed to these odors developed opioid-mediated analgesia. In 1997, researchers found bees became less responsive to pain after they had been stimulated with isoamyl acetate, a chemical smelling of banana, and a component of bee alarm pheromone. The experiment also showed that the bees' fear-induced pain tolerance was mediated by an endorphine.
By using the forced swimming test in rats as a model of fear-induction, the first mammalian "alarm substance" was found.
In 1991, this "alarm substance" was shown to fulfill criteria for pheromones: well-defined behavioral effect, species specificity, minimal influence of experience and control for nonspecific arousal. Rat activity testing with alarm pheromone and their preference/avoidance for odors from cylinders containing the pheromone showed, that the pheromone had very low volatility.
In 1993 a connection between alarm chemosignals in mice and their immune response was found.
Pheromone production in mice was found to be associated with or mediated by the pituitary gland in 1994.
It was not until 2011 that a link between severe pain, neuroinflammation and alarm pheromones release in rats was found: real time RT-PCR analysis of rat brain tissues indicated that shocking the footpad of a rat increased its production of proinflammatory cytokines in deep brain structures, namely of IL-1β, heteronuclear Corticotropin-releasing hormone and c-fos mRNA expressions in both the paraventricular nucleus and the bed nucleus of the stria terminalis, and it increased stress hormone levels in plasma (corticosterone).
In 2004, it was demonstrated that rats’ alarm pheromones had different effects on the “recipient“ rat (the rat perceiving the pheromone) depending which body region they were released from: Pheromone production from the face modified behavior in the recipient rat, e.g. caused sniffing or movement, whereas pheromone secreted from the rat's anal area induced autonomic nervous system stress responses, like an increase in core body temperature. Further experiments showed that when a rat perceived alarm pheromones, it increased its defensive and risk assessment behavior. and its acoustic startle reflex was enhanced.
The neurocircuit for how rats perceive alarm pheromones was shown to be related to hypothalamus, brainstem, and amygdala, all of which are evolutionary ancient structures deep inside or in the case of the brainstem underneath the brain away from the cortex, and involved in the Fight-or-flight response, as is the case in humans.
Alarm pheromone-induced anxiety in rats has been used to evaluate the degree to which anxiolytics can alleviate anxiety in humans. For this the change in the acoustic startle reflex of rats with alarm pheromone-induced anxiety (i.e. reduction of defensiveness) has been measured. Pretreatment of rats with one of five anxiolytics used in clinical medicine was able to reduce their anxiety: namely midazolam, phenelzine (a nonselective monoamine oxidase (MAO) inhibitor), propranolol, a nonselective beta blocker, clonidine, an alpha 2 adrenergic agonist or CP-154,526, a corticotropin-releasing hormone antagonist.
Faulty development of odor discrimination impairs the perception of pheromones and pheromone-related behavior, like aggressive behavior and mating in male rats: The enzyme Mitogen-activated protein kinase 7 (MAPK7) has been implicated in regulating the development of the olfactory bulb and odor discrimination and it is highly expressed in developing rat brains, but absent in most regions of adult rat brains. conditional deletion of the MAPK7gene in mouse neural stem cells impairs several pheromone-mediated behaviors, including aggression and mating in male mice. These behavior impairments were not caused by a reduction in the level of testosterone, by physical immobility, by heightened fear or anxiety or by depression. Using mouse urine as a natural pheromone-containing solution, it has been shown that the impairment was associated with defective detection of related pheromones, and with changes in their inborn preference for pheromones related to sexual and reproductive activities.
Lastly, alleviation of an acute fear response because a friendly peer (or in biological language: an affiliative conspecific) tends and befriends is called "social buffering". The term is in analogy to the 1985 "buffering" hypothesis in psychology, where social support has been proven to mitigate the negative health effects of alarm pheromone mediated distress. The role of a "social pheromone" is suggested by the recent discovery that olfactory signals are responsible in mediating the "social buffering" in male rats. "Social buffering" was also observed to mitigate the conditioned fear responses of honeybees. A bee colony exposed to an environment of high threat of predation did not show increased aggression and aggressive-like gene expression patterns in individual bees, but decreased aggression. That the bees did not simply habituate to threats is suggested by the fact that the disturbed colonies also decreased their foraging.
Biologists have proposed in 2012 that fear pheromones evolved as molecules of "keystone significance", a term coined in analogy to keystone species. Pheromones may determine species compositions, and affect rates of energy and material exchange in an ecological community. Thus pheromones generate structure in a trophic web and play critical roles in maintaining natural systems.
Fear pheromones in humans.
Evidence of chemosensory alarm signals in humans has emerged slowly: Although alarm pheromones have not been physically isolated and their chemical structure has not been identified in man so far, there is evidence for their presence. Androstadienone, for example, a steroidal, endogenous odorant, is a pheromone candidate found in human sweat, axillary hair and plasma. The closely related compound androstenone is involved in communicating dominance, aggression or competition; sex hormone influences on androstenone perception in humans showed high testosterone level related to heightened androstenone sensitivity in men, a high testosterone level related to unhappiness in response to androstenone in men, and a high estradiol level related to disliking of androstenone in women.
A German study from 2006 showed when anxiety-induced versus exercise-induced human sweat from a dozen people was pooled and offered to seven study participants, of five able to olfactorily distinguish exercise-induced sweat from room air, three could also distinguish exercise-induced sweat from anxiety induced sweat. The acoustic startle reflex response to a sound when sensing anxiety sweat was larger than when sensing exercise-induced sweat, as measured by electromyograph analysis of the orbital muscle, which is responsible for the eyeblink component. This showed for the first time that fear chemosignals can modulate the startle reflex in humans without emotional mediation; fear chemosignals primed the recipient's "defensive behavior" prior to the subjects' conscious attention on the acoustic startle reflex level.
In analogy to the social buffering of rats and honeybees in response to chemosignals, induction of empathy by "smelling anxiety" of another person has been found in humans.
A study from 2013 provided brain imaging evidence that human responses to fear chemosignals may be gender-specific. Researchers collected alarm-induced sweat and exercise-induced sweat from donors extracted it, pooled it and presented it to 16 unrelated people undergoing functional brain MRI. While stress-induced sweat from males produced a comparably strong emotional response in both females and males, stress-induced sweat from females produced a markedly stronger arousal in women than in men. Statistical tests pinpointed this gender-specificity to the right amygdala and strongest in the superficial nuclei. Since no significant differences were found in the olfactory bulb, the response to female fear-induced signals is likely based on processing the meaning, i.e. on the emotional level, rather than the strength of chemosensory cues from each gender, i.e. the perceptual level.
An approach-avoidance task was set up where volunteers seeing either an angry or a happy cartoon face on a computer screen pushed away or pulled toward them a joystick as fast as possible. Volunteers smelling anandrostadienone, masked with clove oil scent responded faster, especially to angry faces, than those smelling clove oil only, which was interpreted as anandrostadienone-related activation of the fear system. A potential mechanism of action is, that androstadienone alters the "emotional face processing". Androstadienone is known to influence activity of the fusiform gyrus which is relevant for face recognition.
In culture.
Death.
The fear of the end and its existence is in other words the fear of death. The fear of death ritualized the lives of our ancestors. These rituals were designed to reduce that fear; they helped collect the cultural ideas that we now have in the present. These rituals also helped preserve the cultural ideas. The results and methods of human existence had been changing at the same time that social formation was changing. One can say that the formation of communities happened because people lived in fear. The result of this fear forced people to unite to fight dangers together rather than fight alone.
Religion.
Religions are filled with different fears that humans have had throughout many centuries. The fears aren't just metaphysical (including the problems of life and death) but are also moral. Death is seen as a boundary to another world. That world would always be different depending on how each individual lived their lives. The origins of this intangible fear are not found in the present world. In a sense we can assume that fear was a big influence on things such as morality. This assumption, however, flies in the face of concepts such as Moral Absolutism and Moral Universalism – which would hold that our morals are rooted in either the divine or natural laws of the universe, and would not be generated by any human feeling, thought or emotion.
There is another fear in the Bible that has a different meaning; the fear of God. Fear is used to express a Filial or a slavish passion. In believers the fear of god is "holy awe" or "reverence" of a particular god and the laws of its associated religion.
Manipulation.
Fear may be politically and culturally manipulated to persuade citizenry of ideas which would otherwise be widely rejected or dissuade citizenry from ideas which would otherwise be wildly supported. In contexts of disasters, nation-states manage the fear not only to provide their citizens with an explanation about the event or blaming some minorities, but also to adjust their previous beliefs. The manipulation of fear is done by means of symbolic instruments as terror movies and the administration ideologies that lead to nationalism. After a disaster, the fear is re-channeled in a climate of euphoria based on patriotism. The fear and evilness are inextricably intertwined.
Mirroring fears.
Fear is found in mythology and folklore, and portrayed in books and movies. 
The Story of the Youth Who Went Forth to Learn What Fear Was is a German fairy tale dealing with the topic of not knowing fear.
For example, many stories include characters who fear the antagonist of the plot. One of the important characteristics of historical and mythical heroes across cultures is to be fearless in the face of big and often lethal enemies.
Overcoming.
Pharmaceutical.
A drug treatment for fear conditioning and phobias via the amygdala is the use of glucocorticoids. In one study, glucocorticoid receptors in the central nucleus of the amygdala were disrupted in order to better understand the mechanisms of fear and fear conditioning. The glucocorticoid receptors were inhibited using lentiviral vectors containing Cre-recombinase injected into mice. Results showed that disruption of the glucocorticoid receptors prevented conditioned fear behavior. The mice were subjected to auditory cues which caused them to freeze normally. However, a reduction of freezing was observed in the mice that had inhibited glucocorticoid receptors.
Psychology.
Cognitive behavioral therapy has been successful in helping people overcome fear. Because fear is more complex than just forgetting or deleting memories, an active and successful approach involves people repeatedly confronting their fears. By confronting their fears in a safe manner a person can suppress the fear-triggering memory or stimulus. Known as ‘exposure therapy’, this practice can help cure up to 90% of people, with specific phobias.
Inability to experience.
People who have damage to the amygdala, such as from Urbach–Wiethe disease, are unable to experience fear. This is not debilitating, but a lack of fear can allow someone to get into a dangerous situation they otherwise would have avoided.

</doc>
<doc id="10830" url="https://en.wikipedia.org/wiki?curid=10830" title="Football team">
Football team

A football team is the collective name given to a group of players selected together in the various team sports known as football. 
Such teams could be selected to play in a match against an opposing team, to represent a football club, group, state or nation, an All-star team or even selected as a hypothetical team (such as a Dream Team or Team of the Century) and never play an actual match. 
There are several varieties of football, notably Association football, Gridiron football, Australian rules football, Gaelic football, rugby league, and rugby union. The number of players selected for each team within these varieties and their associated codes can vary substantially. In some, use of the word "team" is sometimes limited to those who play on the field in a match and does not always include other players who may take part as replacements or emergency players. "Football squad" may be used to be inclusive of these support and reserve players.
The term football club is the most commonly used for a sports club which is an organised or incorporated body with a president, committee and a set of rules responsible for ensuring the continued playing existence of one or more teams which are selected for regular competition play (and which may participate in several different divisions or leagues). The oldest football clubs date back to the early 19th century. The words team and club are sometimes used interchangeably by supporters, although they typically refer to the team within the club playing in the highest division or competition.
Variation of player numbers among football codes.
The number of players that take part in the sport simultaneously, thus forming the team are:
References.
For more about football 
https://www.facebook.com/latestfootballtrolls/?fref=ts

</doc>
<doc id="10831" url="https://en.wikipedia.org/wiki?curid=10831" title="F">
F

F (named "ef" ) is the sixth letter in the modern English alphabet and the ISO basic Latin alphabet.
History.
The origin of 'F' is the Semitic letter "vâv" (or "waw") that represented a sound like or . Graphically it originally probably depicted either a hook or a club. It may have been based on a comparable Egyptian hieroglyph such as (transliterated as ḥ(dj)): T3
The Phoenician form of the letter was adopted into Greek as a vowel, "upsilon" (which resembled its descendant 'Y' but was also the ancestor of the Roman letters 'U', 'V', and 'W'); and, with another form, as a consonant, "digamma", which indicated the pronunciation , as in Phoenician. Latin 'F,' despite being pronounced differently, is ultimately descended from digamma and closely resembles it in form.
After sound changes eliminated from spoken Greek, "digamma" was used only as a numeral. However, the Greek alphabet also gave rise to other alphabets, and some of these retained letters descended from digamma. In the Etruscan alphabet, 'F' probably represented , as in Greek, and the Etruscans formed the digraph 'FH' to represent . (At the time these letters were borrowed, there was no Greek letter that represented /f/: the Greek letter phi 'Φ' then represented an aspirated voiceless bilabial plosive , although in Modern Greek it has come to represent .) When the Romans adopted the alphabet, they used 'V' (from Greek "upsilon") not only for the vowel , but also for the corresponding semivowel , leaving 'F' available for . And so out of the various "vav" variants in the Mediterranean world, the letter F entered the Roman alphabet attached to a sound which its antecedents in Greek and Etruscan did not have. The Roman alphabet forms the basis of the alphabet used today for English and many other languages.
The lowercase ' f ' is not related to the visually similar long s, ' ſ ' (or medial s). The use of the "long s" largely died out by the beginning of the 19th century, mostly to prevent confusion with ' f ' when using a short mid-bar (see more at: S).
Use in writing systems.
English.
In the English writing system is used to represent the sound , the voiceless labiodental fricative. It is commonly doubled at the end of words. Exceptionally, it represents the voiced labiodental fricative in the common word "of".
Other languages.
In the writing systems of other languages, commonly represents , or .
Other systems.
The International Phonetic Alphabet uses to represent the voiceless labiodental fricative.
Other uses.
In English-language online slang, "F" (with the pronunciation spelling "eff") is used as an initialism for "fuck". (e.g. "F U", meaning "fuck you"). The "F-word" refers to the word "fuck" itself.
In school grading "F" stands for Fail.

</doc>
<doc id="10834" url="https://en.wikipedia.org/wiki?curid=10834" title="Food preservation">
Food preservation

Food preservation involves preventing the growth of bacteria, fungi (such as yeasts), or other micro-organisms (although some methods work by introducing benign bacteria or fungi to the food), as well as retarding the oxidation of fats that cause rancidity. Food preservation may also include processes that inhibit visual deterioration, such as the enzymatic browning reaction in apples after they are cut during food preparation.
Many processes designed to preserve food will involve a number of food preservation methods. Preserving fruit by turning it into jam, for example, involves boiling (to reduce the fruit’s moisture content and to kill bacteria, etc.), sugaring (to prevent their re-growth) and sealing within an airtight jar (to prevent recontamination). Some traditional methods of preserving food have been shown to have a lower energy input and carbon footprint, when compared to modern methods. However, some methods of food preservation are known to create carcinogens, and in 2015, the International Agency for Research on Cancer of the World Health Organization classified processed meat, i.e. meat that has undergone salting, curing, fermenting, and smoking, as "carcinogenic to humans".
Maintaining or creating nutritional value, texture and flavor is an important aspect of food preservation, although, historically, some methods drastically altered the character of the food being preserved. In many cases these changes have come to be seen as desirable qualities – cheese, yogurt and pickled onions being common examples. 
Traditional techniques.
New techniques of food preservation became available to the home chef from the dawn of agriculture until the Industrial Revolution.
Drying.
Drying is one of the oldest techniques used to hamper the decomposition of food products. As early as 12,000 B.C., Middle Eastern and Oriental cultures were drying foods using the power of the sun. Vegetables and fruit are naturally dried by the sun and wind, but "still houses" were built in areas that did not have enough sunlight to dry things. A fire would be built inside the building to provide the heat to dry the various fruits, vegetables, and herbs.
Cooling.
Cooling preserves foods by slowing down the growth and reproduction of micro-organisms and the action of enzymes that cause food to rot. The introduction of commercial and domestic refrigerators drastically improved the diets of many in the Western world by allowing foods such as fresh fruit, salads and dairy products to be stored safely for longer periods, particularly during warm weather.
Freezing.
Freezing is also one of the most commonly used processes, both commercially and domestically, for preserving a very wide range of foods, including prepared foods that would not have required freezing in their unprepared state. For example, potato waffles are stored in the freezer, but potatoes themselves require only a cool dark place to ensure many months' storage. Cold stores provide large-volume, long-term storage for strategic food stocks held in case of national emergency in many countries.
Heating.
Heating to temperatures which are sufficient to kill microorganisms inside the food is a method used with perpetual stews. Milk is also boiled before storing to kill many microorganisms.
Salting.
Salting or curing draws moisture from the meat through a process of osmosis. Meat is cured with salt or sugar, or a combination of the two. Nitrates and nitrites are also often used to cure meat and contribute the characteristic pink color, as well as inhibition of "Clostridium botulinum".
It was a main method of preservation in medieval times and around the 1700s.
Sugaring.
The earliest cultures have used sugar as a preservative, and it was commonplace to store fruit in honey. Similar to pickled foods, sugar cane was brought to Europe through the trade routes. In northern climates without sufficient sun to dry foods, preserves are made by heating the fruit with sugar. "Sugar tends to draw water from the microbes (plasmolysis). This process leaves the microbial cells dehydrated, thus killing them. In this way, the food will remain safe from microbial spoilage." Sugar is used to preserve fruits, either in an anti-microbial syrup with fruit such as apples, pears, peaches, apricots and plums, or in crystallized form where the preserved material is cooked in sugar to the point of crystallization and the resultant product is then stored dry. This method is used for the skins of citrus fruit (candied peel), angelica and ginger.
Also sugaring can be used in jam jellies.
Smoking.
Smoking is used to lengthen the shelf life of perishable food items. This effect is achieved by exposing the food to smoke from burning plant materials such as wood. Smoke deposits a number of pyrolysis products onto the food, including the phenols syringol, guaiacol and catechol. These compounds aid in the drying and preservation of meats and other foods. Most commonly subjected to this method of food preservation are meats and fish that have undergone curing. Fruits and vegetables like paprika, cheeses, spices, and ingredients for making drinks such as malt and tea leaves are also smoked, but mainly for cooking or flavoring them. It is one of the oldest food preservation methods, which probably arose after the development of cooking with fire.
Pickling.
Pickling is a method of preserving food in an edible anti-microbial liquid. Pickling can be broadly classified into two categories: chemical pickling and fermentation pickling.
In chemical pickling, the food is placed in an edible liquid that inhibits or kills bacteria and other micro-organisms. Typical pickling agents include brine (high in salt), vinegar, alcohol, and vegetable oil, especially olive oil but also many other oils. Many chemical pickling processes also involve heating or boiling so that the food being preserved becomes saturated with the pickling agent. Common chemically pickled foods include cucumbers, peppers, corned beef, herring, and eggs, as well as mixed vegetables such as piccalilli.
In fermentation pickling, the food itself produces the preservation agent, typically by a process that produces lactic acid. Fermented pickles include sauerkraut, nukazuke, kimchi, surströmming, and curtido. Some pickled cucumbers are also fermented.
Lye.
Sodium hydroxide (lye) makes food too alkaline for bacterial growth. Lye will saponify fats in the food, which will change its flavor and texture. Lutefisk uses lye in its preparation, as do some olive recipes. Modern recipes for century eggs also call for lye.
Canning.
Canning involves cooking food, sealing it in sterile cans or jars, and boiling the containers to kill or weaken any remaining bacteria as a form of sterilization. It was invented by the French confectioner Nicolas Appert. By 1806, this process was used by the French Navy to preserve meat, fruit, vegetables, and even milk. Although Appert had discovered a new way of preservation, it wasn't understood until 1864 when Louis Pasteur found the relationship between microorganisms, food spoilage, and illness.
Foods have varying degrees of natural protection against spoilage and may require that the final step occur in a pressure cooker. High-acid fruits like strawberries require no preservatives to can and only a short boiling cycle, whereas marginal vegetables such as carrots require longer boiling and addition of other acidic elements. Low-acid foods, such as vegetables and meats, require pressure canning. Food preserved by canning or bottling is at immediate risk of spoilage once the can or bottle has been opened.
Lack of quality control in the canning process may allow ingress of water or micro-organisms. Most such failures are rapidly detected as decomposition within the can causes gas production and the can will swell or burst. However, there have been examples of poor manufacture (underprocessing) and poor hygiene allowing contamination of canned food by the obligate anaerobe "Clostridium botulinum", which produces an acute toxin within the food, leading to severe illness or death. This organism produces no gas or obvious taste and remains undetected by taste or smell. Its toxin is denatured by cooking, however. Cooked mushrooms, handled poorly and then canned, can support the growth of Staphylococcus aureus, which produces a toxin that is not destroyed by canning or subsequent reheating.
Jellying.
Food may be preserved by cooking in a material that solidifies to form a gel. Such materials include gelatin, agar, maize flour, and arrowroot flour. Some foods naturally form a protein gel when cooked, such as eels and elvers, and sipunculid worms, which are a delicacy in Xiamen, in the Fujian province of the People's Republic of China. Jellied eels are a delicacy in the East End of London, where they are eaten with mashed potatoes. Potted meats in aspic (a gel made from gelatine and clarified meat broth) were a common way of serving meat off-cuts in the UK until the 1950s. Many jugged meats are also jellied.
A traditional British way of preserving meat (particularly shrimp) is by setting it in a pot and sealing it with a layer of fat. Also common is potted chicken liver; compare pâté.
Jugging.
Meat can be preserved by jugging. Jugging is the process of stewing the meat (commonly game or fish) in a covered earthenware jug or casserole. The animal to be jugged is usually cut into pieces, placed into a tightly-sealed jug with brine or gravy, and stewed. Red wine and/or the animal's own blood is sometimes added to the cooking liquid. Jugging was a popular method of preserving meat up until the middle of the 20th century.
Burial.
Burial of food can preserve it due to a variety of factors: lack of light, lack of oxygen, cool temperatures, pH level, or desiccants in the soil. Burial may be combined with other methods such as salting or fermentation. Most foods can be preserved in soil that is very dry and salty (thus a desiccant) such as sand, or soil that is frozen.
Many root vegetables are very resistant to spoilage and require no other preservation than storage in cool dark conditions, for example by burial in the ground, such as in a storage clamp. Century eggs are created by placing eggs in alkaline mud (or other alkaline substance), resulting in their "inorganic" fermentation through raised pH instead of spoiling. The fermentation preserves them and breaks down some of the complex, less flavorful proteins and fats into simpler, more flavorful ones. Cabbage was traditionally buried in the fall in northern farms in the U.S. for preservation. Some methods keep it crispy while other methods produce sauerkraut. A similar process is used in the traditional production of kimchi. Sometimes meat is buried under conditions that cause preservation. If buried on hot coals or ashes, the heat can kill pathogens, the dry ash can desiccate, and the earth can block oxygen and further contamination. If buried where the earth is very cold, the earth acts like a refrigerator.
In Orissa, India, it is practical to store rice by burying it underground. This method helps to store for three to six months during the dry season.
Curing.
The earliest form of curing was dehydration. To accelerate this process, salt is usually added. In the culinary world, it was common to choose raw salts from various sources (rock salt, sea salt, etc.). More modern "examples of salts that are used as preservatives include sodium chloride (NaCl), sodium nitrate (NaNO) and sodium nitrite (NaNO). Even at mild concentrations (up to 2%), sodium chloride, found in many food products, is capable of neutralizing the antimicrobial character of natural compounds."
Fermentation.
Some foods, such as many cheeses, wines, and beers, use specific micro-organisms that combat spoilage from other less-benign organisms. These micro-organisms keep pathogens in check by creating an environment toxic for themselves and other micro-organisms by producing acid or alcohol. Methods of fermentation include, but are not limited to, starter micro-organisms, salt, hops, controlled (usually cool) temperatures and controlled (usually low) levels of oxygen. These methods are used to create the specific controlled conditions that will support the desirable organisms that produce food fit for human consumption.
Fermentation is the microbial conversion of starch and sugars into alcohol. Not only can fermentation produce alcohol, but it can also be a valuable preservation technique. Fermentation can also make foods more nutritious and palatable. For example, drinking water in the Middle Ages was dangerous because it often contained pathogens that could spread disease. When the water is made into beer, the resulting alcohol kills any bacteria in the water that could make people sick. Additionally, the water now has the nutrients from the barley and other ingredients, and the microorganisms can also produce vitamins as they ferment.
Industrial/modern techniques.
Techniques of food preservation were developed in research laboratories for commercial applications.
Pasteurization.
Pasteurization is a process for preservation of liquid food. It was originally applied to combat the souring of young local wines. Today, the process is mainly applied to dairy products. In this method, milk is heated at about 70 °C for 15 to 30 seconds to kill the bacteria present in it and cooling it quickly to 10 °C to prevent the remaining bacteria from growing. The milk is then stored in sterilized bottles or pouches in cold places. This method was invented by Louis Pasteur, a French chemist, in 1862.
Vacuum packing.
Vacuum-packing stores food in a vacuum environment, usually in an air-tight bag or bottle. The vacuum environment strips bacteria of oxygen needed for survival. Vacuum-packing is commonly used for storing nuts to reduce loss of flavor from oxidization. A major drawback to vacuum packaging, at the consumer level, is that vacuum sealing can deform contents and rob certain foods, such as cheese, of its flavor.
Artificial food additives.
Preservative food additives can be "antimicrobial", which inhibit the growth of bacteria or fungi, including mold, or "antioxidant", such as oxygen absorbers, which inhibit the oxidation of food constituents. Common antimicrobial preservatives include calcium propionate, sodium nitrate, sodium nitrite, sulfites (sulfur dioxide, sodium bisulfite, potassium hydrogen sulfite, etc.) and disodium EDTA. Antioxidants include BHA and BHT. Other preservatives include formaldehyde (usually in solution), glutaraldehyde (kills insects), ethanol, and methylchloroisothiazolinone.
Irradiation.
Irradiation of food is the exposure of food to ionizing radiation. The two types of ionizing radiation used are beta particles (high-energy electrons) and gamma rays (emitted from radioactive sources as cobalt-60 or cesium-137). Treatment effects include killing bacteria, molds, and insect pests, reducing the ripening and spoiling of fruits, and at higher doses inducing sterility. The technology may be compared to pasteurization; it is sometimes called "cold pasteurization", as the product is not heated.
The irradiation process is not directly related to nuclear energy, but does use radioactive isotopes produced in nuclear reactors. Cobalt-60, for example does not occur naturally and can only be produced through neutron bombardment of cobalt-59. Ionizing radiation at high energy levels is hazardous to life (hence its usefulness in sterilisation); for this reason, irradiation facilities have a heavily shielded irradiation room where the process takes place. Radiation safety procedures are used to ensure that neither the workers in such facilities nor the environment receives any radiation dose above administrative limits. Irradiated food does not and cannot become radioactive, and national and international expert bodies have declared food irradiation as wholesome. However, the wholesomeness of consuming such food is disputed by opponents and consumer organizations. National and international expert bodies have declared food irradiation as "wholesome"; organizations of the United Nations, such as the World Health Organization and Food and Agriculture Organization, endorse food irradiation. International legislation on whether food may be irradiated or not varies worldwide from no regulation to full banning. Irradiation may allow lower-quality or contaminated foods to be rendered marketable.
Approximately 500,000 tons of food items are irradiated per year worldwide in over 40 countries. These are mainly spices and condiments with an increasing segment of fresh fruit irradiated for fruit fly quarantine.
Pulsed electric field electroporation.
Pulsed electric field (PEF) electroporation is a method for processing cells by means of brief pulses of a strong electric field. PEF holds potential as a type of low-temperature alternative pasteurization process for sterilizing food products. In PEF processing, a substance is placed between two electrodes, then the pulsed electric field is applied. The electric field enlarges the pores of the cell membranes, which kills the cells and releases their contents. PEF for food processing is a developing technology still being researched. There have been limited industrial applications of PEF processing for the pasteurization of fruit juices. To date, several PEF treated juices are available on the market in Europe. Furthermore, for several years a juice pasteurization application in the US has used PEF.
For cell disintegration purposes especially potato processors show great interest in PEF technology as an efficient alternative for their preheaters. Potato applications are already operational in the US and Canada. There are also commercial PEF potato applications in various countries in Europe, as well as in Australia, India and China.
Modified atmosphere.
Modifying atmosphere is a way to preserve food by operating on the atmosphere around it. Salad crops that are notoriously difficult to preserve are now being packaged in sealed bags with an atmosphere modified to reduce the oxygen (O) concentration and increase the carbon dioxide (CO) concentration. There is concern that, although salad vegetables retain their appearance and texture in such conditions, this method of preservation may not retain nutrients, especially vitamins.
There are two methods for preserving grains with carbon dioxide. One method is placing a block of dry ice in the bottom and filling the can with the grain. Another method is purging the container from the bottom by gaseous carbon dioxide from a cylinder or bulk supply vessel.
Carbon dioxide prevents insects and, depending on concentration, mold and oxidation from damaging the grain. Grain stored in this way can remain edible for approximately five years.
Nitrogen gas (N) at concentrations of 98% or higher is also used effectively to kill insects in the grain through hypoxia. However, carbon dioxide has an advantage in this respect, as it kills organisms through hypercarbia and hypoxia (depending on concentration), but it requires concentrations of above 35%, or so. This makes carbon dioxide preferable for fumigation in situations where a hermetic seal cannot be maintained.
Controlled Atmospheric Storage (CA): "CA storage is a non-chemical process. Oxygen levels in the sealed rooms are reduced, usually by the infusion of nitrogen gas, from the approximate 21 percent in the air we breathe to 1 percent or 2 percent. Temperatures are kept at a constant . Humidity is maintained at 95 percent and carbon dioxide levels are also controlled. Exact conditions in the rooms are set according to the apple variety. Researchers develop specific regimens for each variety to achieve the best quality. Computers help keep conditions constant."
"Eastern Washington, where most of Washington’s apples are grown, has enough warehouse storage for 181 million boxes of fruit, according to a report done in 1997 by managers for the Washington State Department of Agriculture Plant Services Division. The storage capacity study shows that 67 percent of that space —enough for 121,008,000 boxes of apples — is CA storage."
Air-tight storage of grains (sometimes called hermetic storage) relies on the respiration of grain, insects, and fungi that can modify the enclosed atmosphere sufficiently to control insect pests. This is a method of great antiquity, as well as having modern equivalents. The success of the method relies on having the correct mix of sealing, grain moisture, and temperature.
A patented process uses fuel cells to exhaust and automatically maintain the exhaustion of oxygen in a shipping container, containing, for example, fresh fish.
Nonthermal plasma.
This process subjects the surface of food to a "flame" of ionized gas molecules, such as helium or nitrogen. This causes micro-organisms to die off on the surface.
High-pressure food preservation.
High-pressure food preservation or pascalization refers to the use of a food preservation technique that makes use of high pressure. "Pressed inside a vessel exerting or more, food can be processed so that it retains its fresh appearance, flavor, texture and nutrients while disabling harmful microorganisms and slowing spoilage." By 2005, the process was being used for products ranging from orange juice to guacamole to deli meats and widely sold.
Biopreservation.
Biopreservation is the use of natural or controlled microbiota or antimicrobials as a way of preserving food and extending its shelf life. Beneficial bacteria or the fermentation products produced by these bacteria are used in biopreservation to control spoilage and render pathogens inactive in food. It is a benign ecological approach which is gaining increasing attention.
Of special interest are lactic acid bacteria (LAB). Lactic acid bacteria have antagonistic properties that make them particularly useful as biopreservatives. When LABs compete for nutrients, their metabolites often include active antimicrobials such as lactic acid, acetic acid, hydrogen peroxide, and peptide bacteriocins. Some LABs produce the antimicrobial nisin, which is a particularly effective preservative.
These days, LAB bacteriocins are used as an integral part of hurdle technology. Using them in combination with other preservative techniques can effectively control spoilage bacteria and other pathogens, and can inhibit the activities of a wide spectrum of organisms, including inherently resistant Gram-negative bacteria.
Hurdle technology.
Hurdle technology is a method of ensuring that pathogens in food products can be eliminated or controlled by combining more than one approach. These approaches can be thought of as "hurdles" the pathogen has to overcome if it is to remain active in the food. The right combination of hurdles can ensure all pathogens are eliminated or rendered harmless in the final product.
Hurdle technology has been defined by Leistner (2000) as an intelligent combination of hurdles that secures the microbial safety and stability as well as the organoleptic and nutritional quality and the economic viability of food products. The organoleptic quality of the food refers to its sensory properties, that is its look, taste, smell, and texture.
Examples of hurdles in a food system are high temperature during processing, low temperature during storage, increasing the acidity, lowering the water activity or redox potential, and the presence of preservatives or biopreservatives. According to the type of pathogens and how risky they are, the intensity of the hurdles can be adjusted individually to meet consumer preferences in an economical way, without sacrificing the safety of the product.

</doc>
<doc id="10835" url="https://en.wikipedia.org/wiki?curid=10835" title="Frequency modulation">
Frequency modulation

In telecommunications and signal processing, frequency modulation (FM) is the encoding of information in a carrier wave by varying the instantaneous frequency of the wave. This contrasts with amplitude modulation, in which the amplitude of the carrier wave varies, while the frequency remains constant.
In analog frequency modulation, such as FM radio broadcasting of an audio signal representing voice or music, the instantaneous frequency deviation, the difference between the frequency of the carrier and its center frequency, is proportional to the modulating signal.
Digital data can be encoded and transmitted via FM by shifting the carrier's frequency among a predefined set of frequencies representing digits - for example one frequency can represent a binary 1 and a second can represent binary 0. This modulation technique is known as frequency-shift keying (FSK). FSK is widely used in modems and fax modems, and can also be used to send Morse code. Radioteletype also uses FSK.
Frequency modulation is widely used for FM radio broadcasting. It is also used in telemetry, radar, seismic prospecting, and monitoring newborns for seizures via EEG, two-way radio systems, music synthesis, magnetic tape-recording systems and some video-transmission systems. In radio transmission, an advantage of frequency modulation is that it has a larger signal-to-noise ratio and therefore rejects radio frequency interference better than an equal power amplitude modulation (AM) signal. For this reason, most music is broadcast over FM radio.
Frequency modulation has a close relationship with phase modulation; phase modulation is often used as an intermediate step to achieve frequency modulation. Mathematically both of these are considered a special case of quadrature amplitude modulation (QAM).
Theory.
If the information to be transmitted (i.e., the baseband signal) is formula_1 and the sinusoidal carrier is formula_2, where "f" is the carrier's base frequency, and "A" is the carrier's amplitude, the modulator combines the carrier with the baseband data signal to get the transmitted signal:
In this equation, formula_6 is the "instantaneous frequency" of the oscillator and formula_7 is the "frequency deviation", which represents the maximum shift away from "f" in one direction, assuming "x"("t") is limited to the range ±1.
While most of the energy of the signal is contained within "f" ± "f", it can be shown by Fourier analysis that a wider range of frequencies is required to precisely represent an FM signal. The frequency spectrum of an actual FM signal has components extending infinitely, although their amplitude decreases and higher-order components are often neglected in practical design problems.
Sinusoidal baseband signal.
Mathematically, a baseband modulated signal may be approximated by a sinusoidal continuous wave signal with a frequency "f".This method is also named as Single-tone Modulation.The integral of such a signal is:
In this case, the expression for y(t) above simplifies to:
where the amplitude formula_10 of the modulating sinusoid is represented by the peak deviation formula_7 (see frequency deviation).
The harmonic distribution of a sine wave carrier modulated by such a sinusoidal signal can be represented with Bessel functions; this provides the basis for a mathematical understanding of frequency modulation in the frequency domain.
Modulation index.
As in other modulation systems,the modulation index indicates by how much the modulated variable varies around its unmodulated level. It relates to variations in the carrier frequency:
where formula_13 is the highest frequency component present in the modulating signal "x"("t"), and formula_14 is the peak frequency-deviation—i.e. the maximum deviation of the "instantaneous frequency" from the carrier frequency. For a sine wave modulation, the modulation index is seen to be the ratio of the peak frequency deviation of the carrier wave to the frequency of the modulating sine wave.
If formula_15, the modulation is called narrowband FM, and its bandwidth is approximately formula_16.Sometimes modulation index h<0.3 rad is considered as Narrowband FM otherwise Wideband FM.
For digital modulation systems, for example Binary Frequency Shift Keying (BFSK), where a binary signal modulates the carrier, the modulation index is given by:
where formula_18 is the symbol period, and formula_19 is used as the highest frequency of the modulating binary waveform by convention, even though it would be more accurate to say it is the highest "fundamental" of the modulating binary waveform. In the case of digital modulation, the carrier formula_20 is never transmitted. Rather, one of two frequencies is transmitted, either formula_21 or formula_22, depending on the binary state 0 or 1 of the modulation signal.
If formula_23, the modulation is called "wideband FM" and its bandwidth is approximately formula_24. While wideband FM uses more bandwidth, it can improve the signal-to-noise ratio significantly; for example, doubling the value of formula_14, while keeping formula_26 constant, results in an eight-fold improvement in the signal-to-noise ratio. (Compare this with Chirp spread spectrum, which uses extremely wide frequency deviations to achieve processing gains comparable to traditional, better-known spread-spectrum modes).
With a tone-modulated FM wave, if the modulation frequency is held constant and the modulation index is increased, the (non-negligible) bandwidth of the FM signal increases but the spacing between spectra remains the same; some spectral components decrease in strength as others increase. If the frequency deviation is held constant and the modulation frequency increased, the spacing between spectra increases.
Frequency modulation can be classified as narrowband if the change in the carrier frequency is about the same as the signal frequency, or as wideband if the change in the carrier frequency is much higher (modulation index >1) than the signal frequency.
Bessel functions.
For the case of a carrier modulated by a single sine wave, the resulting frequency spectrum can be calculated using Bessel functions of the first kind, as a function of the sideband number and the modulation index. The carrier and sideband amplitudes are illustrated for different modulation indices of FM signals. For particular values of the modulation index, the carrier amplitude becomes zero and all the signal power is in the sidebands.
Since the sidebands are on both sides of the carrier, their count is doubled, and then multiplied by the modulating frequency to find the bandwidth. For example, 3 kHz deviation modulated by a 2.2 kHz audio tone produces a modulation index of 1.36. Suppose that we limit ourselves to only those sidebands that have a relative amplitude of at least 0.01. Then, examining the chart shows this modulation index will produce three sidebands. These three sidebands, when doubled, gives us (6 * 2.2 kHz) or a 13.2 kHz required bandwidth.
Carson's rule.
A rule of thumb, "Carson's rule" states that nearly all (~98 percent) of the power of a frequency-modulated signal lies within a bandwidth formula_27 of:
where formula_29, as defined above, is the peak deviation of the instantaneous frequency formula_30 from the center carrier frequency formula_20 andformula_13 is the highest frequency in the modulating signal.
Condition for application of Carson's rule is only sinusoidal signals.
Noise reduction.
A major advantage of FM in a communications circuit, compared for example with AM, is the possibility of improved Signal-to-noise ratio (SNR). Compared with an optimum AM scheme, FM typically has poorer SNR below a certain signal level called the noise threshold, but above a higher level – the full improvement or full quieting threshold – the SNR is much improved over AM. The improvement depends on modulation level and deviation. For typical voice communications channels, improvements are typically 5-15 dB. FM broadcasting using wider deviation can achieve even greater improvements. Additional techniques, such as pre-emphasis of higher audio frequencies with corresponding de-emphasis in the receiver, are generally used to improve overall SNR in FM circuits. Since FM signals have constant amplitude, FM receivers normally have limiters that remove AM noise, further improving SNR.
Implementation.
Modulation.
FM signals can be generated using either direct or indirect frequency modulation:
Demodulation.
Many FM detector circuits exist. A common method for recovering the information signal is through a Foster-Seeley discriminator. A phase-locked loop can be used as an FM demodulator. "Slope detection" demodulates an FM signal by using a tuned circuit which has its resonant frequency slightly offset from the carrier. As the frequency rises and falls the tuned circuit provides a changing amplitude of response, converting FM to AM. AM receivers may detect some FM transmissions by this means, although it does not provide an efficient means of detection for FM broadcasts.
Applications.
Magnetic tape storage.
FM is also used at intermediate frequencies by analog VCR systems (including VHS) to record the luminance (black and white) portions of the video signal. Commonly, the chrominance component is recorded as a conventional AM signal, using the higher-frequency FM signal as bias. FM is the only feasible method of recording the luminance ("black and white") component of video to (and retrieving video from) magnetic tape without distortion; video signals have a large range of frequency components – from a few hertz to several megahertz, too wide for equalizers to work with due to electronic noise below −60 dB. FM also keeps the tape at saturation level, acting as a form of noise reduction; a limiter can mask variations in playback output, and the FM capture effect removes print-through and pre-echo. A continuous pilot-tone, if added to the signal – as was done on V2000 and many Hi-band formats – can keep mechanical jitter under control and assist timebase correction.
These FM systems are unusual, in that they have a ratio of carrier to maximum modulation frequency of less than two; contrast this with FM audio broadcasting, where the ratio is around 10,000. Consider, for example, a 6-MHz carrier modulated at a 3.5-MHz rate; by Bessel analysis, the first sidebands are on 9.5 and 2.5 MHz and the second sidebands are on 13 MHz and −1 MHz. The result is a reversed-phase sideband on +1 MHz; on demodulation, this results in unwanted output at 6−1 = 5 MHz. The system must be designed so that this unwanted output is reduced to an acceptable level.
Sound.
FM is also used at audio frequencies to synthesize sound. This technique, known as FM synthesis, was popularized by early digital synthesizers and became a standard feature in several generations of personal computer sound cards.
Radio.
Edwin Howard Armstrong (1890–1954) was an American electrical engineer who invented wideband frequency modulation (FM) radio.
He patented the regenerative circuit in 1914, the superheterodyne receiver in 1918 and the super-regenerative circuit in 1922. Armstrong presented his paper, "A Method of Reducing Disturbances in Radio Signaling by a System of Frequency Modulation", (which first described FM radio) before the New York section of the Institute of Radio Engineers on November 6, 1935. The paper was published in 1936.
As the name implies, wideband FM (WFM) requires a wider signal bandwidth than amplitude modulation by an equivalent modulating signal; this also makes the signal more robust against noise and interference. Frequency modulation is also more robust against signal-amplitude-fading phenomena. As a result, FM was chosen as the modulation standard for high frequency, high fidelity radio transmission, hence the term "FM radio" (although for many years the BBC called it "VHF radio" because commercial FM broadcasting uses part of the VHF band—the FM broadcast band). FM receivers employ a special detector for FM signals and exhibit a phenomenon known as the "capture effect", in which the tuner "captures" the stronger of two stations on the same frequency while rejecting the other (compare this with a similar situation on an AM receiver, where both stations can be heard simultaneously). However, frequency drift or a lack of selectivity may cause one station to be overtaken by another on an adjacent channel. Frequency drift was a problem in early (or inexpensive) receivers; inadequate selectivity may affect any tuner.
An FM signal can also be used to carry a stereo signal; this is done with multiplexing and demultiplexing before and after the FM process. The FM modulation and demodulation process is identical in stereo and monaural processes. A high-efficiency radio-frequency switching amplifier can be used to transmit FM signals (and other constant-amplitude signals). For a given signal strength (measured at the receiver antenna), switching amplifiers use less battery power and typically cost less than a linear amplifier. This gives FM another advantage over other modulation methods requiring linear amplifiers, such as AM and QAM.
FM is commonly used at VHF radio frequencies for high-fidelity broadcasts of music and speech. Analog TV sound is also broadcast using FM. Narrowband FM is used for voice communications in commercial and amateur radio settings. In broadcast services, where audio fidelity is important, wideband FM is generally used. In two-way radio, narrowband FM (NBFM) is used to conserve bandwidth for land mobile, marine mobile and other radio services.

</doc>
<doc id="10837" url="https://en.wikipedia.org/wiki?curid=10837" title="Faith and rationality">
Faith and rationality

Faith and rationality are two ideologies that exist in varying degrees of conflict or compatibility. Rationality is based on reason or facts. Faith is belief in inspiration, revelation, or authority. The word "faith" usually refers to a belief that is held with lack of, in spite of or against reason or evidence, while another position holds that it can refer to belief based upon a degree of evidential warrant.
Although the words "faith" and "belief" are sometimes erroneously conflated and used as synonyms, "faith" properly refers to a particular type (or subset) of "belief," as defined above.
Broadly speaking, there are two categories of views regarding the relationship between faith and rationality:
The Catholic Church also has taught that faith and reason can and must work together, in the Papal encyclical letter issued by Pope John Paul II, "Fides et Ratio" (" Faith and Reason").
Relationship between faith and reason.
From at least the days of the Greek Philosophers, the relationship between faith and reason has been hotly debated. Plato argued that knowledge is simply memory of the eternal. Aristotle set down rules by which knowledge could be discovered by reason.
Rationalists point out that many people hold irrational beliefs, for many reasons. There may be evolutionary causes for irrational beliefs — irrational beliefs may increase our ability to survive and reproduce. Or, according to Pascal's Wager, it may be to our advantage to have faith, because faith may promise infinite rewards, while the rewards of reason are seen by many as finite. One more reason for irrational beliefs can perhaps be explained by operant conditioning. For example, in one study by B. F. Skinner in 1948, pigeons were awarded grain at regular time intervals regardless of their behaviour. The result was that each of pigeons developed their own idiosyncratic response which had become associated with the consequence of receiving grain.
Believers in faith — for example those who believe salvation is possible through faith alone — frequently suggest that everyone holds beliefs arrived at by faith, not reason. The belief that the universe is a sensible place and that our minds allow us to arrive at correct conclusions about it, is a belief we hold through faith. Rationalists contend that this is arrived at because they have observed the world being consistent and sensible, not because they have faith that it is.
Beliefs held "by faith" may be seen existing in a number of relationships to rationality:
Views of the Roman Catholic Church.
St. Thomas Aquinas, the most important doctor of the Catholic Church, was the first to write a full treatment of the relationship, differences, and similarities between faith—an intellectual assent—and reason, predominately in his "Summa Theologica", "De Veritate", and "Summa contra Gentiles".
The Council of Trent's catechism—the "Roman Catechism", written during the Catholic Church's Counter-Reformation to combat Protestantism and Martin Luther's antimetaphysical tendencies.
"Dei Filius" was a dogmatic constitution of the First Vatican Council on the Roman Catholic faith. It was adopted unanimously on 24 April 1870 and was influenced by the philosophical conceptions of Johann Baptist Franzelin, who had written a great deal on the topic of faith and rationality.
Because the Roman Catholic Church does not disparage reason, but rather affirms its veracity and utility, there have been many Catholic scientists over the ages.
Twentieth-century Thomist philosopher Étienne Gilson wrote about faith and reason in his 1922 book "Le Thomisme". His contemporary Jacques Maritain wrote about it in his "The Degrees of Knowledge".
"Fides et Ratio" is an encyclical promulgated by Pope John Paul II on 14 September 1998. It deals with the relationship between faith and reason.
Pope Benedict XVI's 12 September 2006 Regensburg Lecture was about faith and reason.
Lutheran epistemology.
Some have asserted that Martin Luther taught that faith and reason were antithetical in the sense that questions of faith could not be illuminated by reason. Contemporary Lutheran scholarship however has found a different reality in Luther. Luther rather seeks to separate faith and reason in order to honor the separate spheres of knowledge that each understand. Bernhard Lohse for example has demonstrated in his classic work "Fides Und Ratio" that Luther ultimately sought to put the two together. More recently Hans-Peter Großhans has demonstrated that Luther's work on Bibilical Criticism stresses the need for external coherence in right exegetical method. This means that for Luther it is more important that the Bible be reasonable according to the reality outside of the scriptures than that the Bible make sense to itself, that it has internal coherence. The right tool for understanding the world outside of the Bible for Luther is none other than Reason which for Luther denoted science, philosophy, history and empirical observation. Here a differing picture is presented of a Luther who deeply valued both faith and reason, and held them in dialectical partnership. Luther's concern thus in separating them is honoring their different epistemological spheres.
Reformed epistemology.
Faith as underlying rationality.
The view that faith underlies all rationality holds that rationality is dependent on faith for its coherence. Under this view, there is no way to comprehensively "prove" that we are actually seeing what we appear to be seeing, that what we remember actually happened, or that the laws of logic and mathematics are actually real. Instead, all beliefs depend for their coherence on "faith" in our senses, memory, and reason, because the foundations of rationalism cannot be proven by evidence or reason. Rationally, you can not prove anything you see is real, but you can prove that you yourself are real, and rationalist belief would be that you can believe that the world is consistent until something demonstrates inconsistency. This differs from faith based belief, where you believe that your world view is consistent no matter what inconsistencies the world has with your beliefs.
Rationalist point of view.
In this view, there are many beliefs that are held by faith alone, that rational thought would force the mind to reject. As an example, many people believe in the Biblical story of Noah's flood: that the entire Earth was covered by water for forty days. But objected that most plants cannot survive being covered by water for that length of time, a boat of that magnitude could not have been built by wood, and there would be no way for two of every animal to survive on that ship and migrate back to their place of origin. (such as penguins), Although Christian apologists offer answers to these and such issues, under the premise that such responses are insufficient, one must choose between accepting the story on faith and rejecting reason, or rejecting the story by reason and thus rejecting faith.
Within the rationalist point of view, there remains the possibility of multiple rational explanations. For example, considering the biblical story of Noah's flood, one making rational determinations about the probability of the events does so via interpretation of modern evidence. Two observers of the story may provide different plausible explanations for the life of plants, construction of the boat, species living at the time, and migration following the flood. Some see this as meaning that a person is not strictly bound to choose between faith and reason.
Evangelical views.
American biblical scholar Archibald Thomas Robertson stated that the Greek word "pistis" used for faith in the New Testament (over two hundred forty times), and rendered "assurance" in Acts 17:31 (KJV), is "an old verb to furnish, used regularly by Demosthenes for bringing forward evidence." Likewise Tom Price (Oxford Centre for Christian Apologetics) affirms that when the New Testament talks about faith positively it only uses words derived from the Greek root isti which means "to be persuaded."
In contrast to faith meaning blind trust, in the absence of evidence, even in the teeth of evidence, Alister McGrath quotes Oxford Anglican theologian W. H. Griffith-Thomas, (1861-1924), who states faith is "not blind, but intelligent" and "commences with the conviction of the mind based on adequate evidence...", which McGrath sees as "a good and reliable definition, synthesizing the core elements of the characteristic Christian understanding of faith."
Alvin Plantinga upholds that faith may be the result of evidence testifying to the reliability of the source of truth claims, but although it may involve this, he sees faith as being the result of hearing the truth of the gospel with the internal persuasion by the Holy Spirit moving and enabling him to believe. "Christian belief is produced in the believer by the internal instigation of the Holy Spirit, endorsing the teachings of Scripture, which is itself divinely inspired by the Holy Spirit. The result of the work of the Holy Spirit is faith."
Jewish philosophy.
The 14th Century Jewish philosopher Levi ben Gerson tried to reconcile faith and reason. He wrote, "The Torah cannot prevent us from considering to be true that which our reason urges us to believe." His contemporary Hasdai ben Abraham Crescas argued the contrary view, that reason is weak and faith strong, and that only through faith can we discover the fundamental truth that God is love, that through faith alone can we endure the suffering that is the common lot of God's chosen people.

</doc>
<doc id="10839" url="https://en.wikipedia.org/wiki?curid=10839" title="List of film institutes">
List of film institutes

Some notable institutions celebrating film, including both national film institutes and independent and non-profit organizations. For the purposes of this list, institutions that do not have their own article on Wikipedia are not considered notable.

</doc>
<doc id="10841" url="https://en.wikipedia.org/wiki?curid=10841" title="Forth">
Forth

Forth may refer to:
FORTH may stand for:

</doc>
<doc id="10842" url="https://en.wikipedia.org/wiki?curid=10842" title="F wave">
F wave

In neuroscience, an F wave is the second of two voltage changes observed after electrical stimulation is applied to the skin surface above the distal region of a nerve. F waves are often used to measure nerve conduction velocity, and are particularly useful for evaluating conduction problems in the proximal region of nerves (i.e., portions of nerves near the spinal cord).
It's called F wave because it was initially recorded in the foot muscles.
Overview.
In a typical F wave study, a strong electrical stimulus (supramaximal stimulation) is applied to the skin surface above the distal portion of a nerve so that the impulse travels both distally (towards the muscle fiber) and proximally (back to the motor neurons of the spinal cord). (These directions are also known as orthodromic and antidromic, respectively.) When the "orthodromic" stimulus reaches the muscle fiber, it elicits a strong M-response indicative of muscle contraction. When the "antidromic" stimulus reaches the motor neuron cell bodies, a small portion of the motor neurons backfire and orthodromic wave travels back down the nerve towards the muscle. This reflected stimulus evokes small proportion of the muscle fibers causing a small, second CMAP called the F wave.
Because a different population of anterior horn cells is stimulated with each stimulation, each F wave have a slightly different shape, amplitude and latency.
Properties.
F wave properties include:
F wave measurements.
Several measurements can be done on the F responses, including minimal and maximal latencies, and F wave persistence.
The minimal F wave latency is typically 25-32 ms in the upper extremities, and 45-56 ms in the lower extremities.
F wave persistence is the number of F waves obtained per the number of stimulations, which is normally 80-100% (or above 50%).

</doc>
<doc id="10843" url="https://en.wikipedia.org/wiki?curid=10843" title="Fruit">
Fruit

In botany, a fruit is the seed-bearing structure in angiosperms formed from the ovary after flowering.
Fruits are the means by which angiosperms disseminate seeds. Edible fruits, in particular, have propagated with the movements of humans and animals in a symbiotic relationship as a means for seed dispersal and nutrition; in fact, humans and many animals have become dependent on fruits as a source of food. Accordingly, fruits account for a substantial fraction of the world's agricultural output, and some (such as the apple and the pomegranate) have acquired extensive cultural and symbolic meanings.
In common language usage, "fruit" normally means the fleshy seed-associated structures of a plant that are sweet or sour, and edible in the raw state, such as apples, bananas, grapes, lemons, oranges, and strawberries. On the other hand, in botanical usage, "fruit" includes many structures that are not commonly called "fruits", such as bean pods, corn kernels, tomatoes, and wheat grains. The section of a fungus that produces spores is also called a fruiting body.
Botanic fruit and culinary fruit.
Many common terms for seeds and fruit do not correspond to the botanical classifications. In culinary terminology, a "fruit" is usually any sweet-tasting plant part, especially a botanical fruit; a "nut" is any hard, oily, and shelled plant product; and a "vegetable" is any savory or less sweet plant product. However, in botany, a "fruit" is the ripened ovary or carpel that contains seeds, a "nut" is a type of fruit and not a seed, and a "seed" is a ripened ovule.
Examples of culinary "vegetables" and nuts that are botanically fruit include corn, cucurbits (e.g., cucumber, pumpkin, and squash), eggplant, legumes (beans, peanuts, and peas), sweet pepper, and tomato. In addition, some spices, such as allspice and chili pepper, are fruits, botanically speaking. In contrast, rhubarb is often referred to as a fruit, because it is used to make sweet desserts such as pies, though only the petiole (leaf stalk) of the rhubarb plant is edible, and edible gymnosperm seeds are often given fruit names, e.g., ginkgo nuts and pine nuts.
Botanically, a cereal grain, such as corn, rice, or wheat, is also a kind of fruit, termed a caryopsis. However, the fruit wall is very thin and is fused to the seed coat, so almost all of the edible grain is actually a seed.
Fruit structure.
The outer, often edible layer, is the "pericarp", formed from the ovary and surrounding the seeds, although in some species other tissues contribute to or form the edible portion. The pericarp may be described in three layers from outer to inner, the "epicarp", "mesocarp" and "endocarp".
Fruit that bears a prominent pointed terminal projection is said to be "beaked".
Fruit development.
A fruit results from maturation of one or more flowers, and the gynoecium of the flower(s) forms all or part of the fruit.
Inside the ovary/ovaries are one or more ovules where the megagametophyte contains the egg cell. After double fertilization, these ovules will become seeds. The ovules are fertilized in a process that starts with pollination, which involves the movement of pollen from the stamens to the stigma of flowers. After pollination, a tube grows from the pollen through the stigma into the ovary to the ovule and two sperm are transferred from the pollen to the megagametophyte. Within the megagametophyte one of the two sperm unites with the egg, forming a zygote, and the second sperm enters the central cell forming the endosperm mother cell, which completes the double fertilization process. Later the zygote will give rise to the embryo of the seed, and the endosperm mother cell will give rise to endosperm, a nutritive tissue used by the embryo.
As the ovules develop into seeds, the ovary begins to ripen and the ovary wall, the "pericarp", may become fleshy (as in berries or drupes), or form a hard outer covering (as in nuts). In some multiseeded fruits, the extent to which the flesh develops is proportional to the number of fertilized ovules. The pericarp is often differentiated into two or three distinct layers called the "exocarp" (outer layer, also called epicarp), "mesocarp" (middle layer), and "endocarp" (inner layer). In some fruits, especially simple fruits derived from an inferior ovary, other parts of the flower (such as the floral tube, including the petals, sepals, and stamens), fuse with the ovary and ripen with it. In other cases, the sepals, petals and/or stamens and style of the flower fall off. When such other floral parts are a significant part of the fruit, it is called an "accessory fruit". Since other parts of the flower may contribute to the structure of the fruit, it is important to study flower structure to understand how a particular fruit forms.
There are three general modes of fruit development:
Plant scientists have grouped fruits into three main groups, simple fruits, aggregate fruits, and composite or multiple fruits. The groupings are not evolutionarily relevant, since many diverse plant taxa may be in the same group, but reflect how the flower organs are arranged and how the fruits develop.
Simple fruit.
Simple fruits can be either dry or fleshy, and result from the ripening of a simple or compound ovary in a flower with only one pistil. Dry fruits may be either dehiscent (they open to discharge seeds), or indehiscent (they do not open to discharge seeds). Types of dry, simple fruits, and examples of each, include:
Fruits in which part or all of the "pericarp" (fruit wall) is fleshy at maturity are "simple fleshy fruits". Types of simple, fleshy, fruits (with examples) include:
An aggregate fruit, or "etaerio", develops from a single flower with numerous simple pistils.
The pome fruits of the family Rosaceae, (including apples, pears, rosehips, and saskatoon berry) are a syncarpous fleshy fruit, a simple fruit, developing from a half-inferior ovary.
Schizocarp fruits form from a syncarpous ovary and do not really dehisce, but rather split into segments with one or more seeds; they include a number of different forms from a wide range of families. Carrot seed is an example.
Aggregate fruit.
Aggregate fruits form from single flowers that have multiple carpels which are not joined together, i.e. each pistil contains one carpel. Each pistil forms a fruitlet, and collectively the fruitlets are called an etaerio. Four types of aggregate fruits include etaerios of achenes, follicles, drupelets, and berries. Ranunculaceae species, including "Clematis" and "Ranunculus" have an etaerio of achenes, "Calotropis" has an etaerio of follicles, and "Rubus" species like raspberry, have an etaerio of drupelets. "Annona" have an etaerio of berries.
The raspberry, whose pistils are termed "drupelets" because each is like a small drupe attached to the receptacle. In some bramble fruits (such as blackberry) the receptacle is elongated and part of the ripe fruit, making the blackberry an "aggregate-accessory" fruit. The strawberry is also an aggregate-accessory fruit, only one in which the seeds are contained in achenes. In all these examples, the fruit develops from a single flower with numerous pistils.
Multiple fruits.
A multiple fruit is one formed from a cluster of flowers (called an "inflorescence"). Each flower produces a fruit, but these mature into a single mass. Examples are the pineapple, fig, mulberry, osage-orange, and breadfruit.
In the photograph on the right, stages of flowering and fruit development in the noni or Indian mulberry ("Morinda citrifolia") can be observed on a single branch. First an inflorescence of white flowers called a head is produced. After fertilization, each flower develops into a drupe, and as the drupes expand, they become "connate" (merge) into a "multiple fleshy fruit" called a "syncarp".
Berries.
Berries are another type of fleshy fruit; they are simple fruit created from a single ovary. The ovary may be compound, with several carpels. Types include (examples follow in the table below):
Accessory fruit.
Some or all of the edible part of accessory fruit is not generated by the ovary. Accessory fruit can be simple, aggregate, or multiple, i.e., they can include one or more pistils and other parts from the same flower, or the pistils and other parts of many flowers.
Seedless fruits.
Seedlessness is an important feature of some fruits of commerce. Commercial cultivars of bananas and pineapples are examples of seedless fruits. Some cultivars of citrus fruits (especially grapefruit, mandarin oranges, navel oranges), satsumas, table grapes, and watermelons are valued for their seedlessness. In some species, seedlessness is the result of "parthenocarpy", where fruits set without fertilization. Parthenocarpic fruit set may or may not require pollination, but most seedless citrus fruits require a stimulus from pollination to produce fruit.
Seedless bananas and grapes are triploids, and seedlessness results from the abortion of the embryonic plant that is produced by fertilization, a phenomenon known as "stenospermocarpy", which requires normal pollination and fertilization.
Seed dissemination.
Variations in fruit structures largely depend on their seeds' mode of dispersal. This dispersal can be achieved by animals, explosive dehiscence, water, or wind.
Some fruits have coats covered with spikes or hooked burrs, either to prevent themselves from being eaten by animals, or to stick to the feathers, hairs, or legs of animals, using them as dispersal agents. Examples include cocklebur and unicorn plant.
The sweet flesh of many fruits is "deliberately" appealing to animals, so that the seeds held within are eaten and "unwittingly" carried away and deposited (i.e., defecated) at a distance from the parent. Likewise, the nutritious, oily kernels of nuts are appealing to rodents (such as squirrels), which hoard them in the soil to avoid starving during the winter, thus giving those seeds that remain uneaten the chance to germinate and grow into a new plant away from their parent.
Other fruits are elongated and flattened out naturally, and so become thin, like wings or helicopter blades, e.g., elm, maple, and tuliptree. This is an evolutionary mechanism to increase dispersal distance away from the parent, via wind. Other wind-dispersed fruit have tiny "parachutes", e.g., dandelion, milkweed, salsify.
Coconut fruits can float thousands of miles in the ocean to spread seeds. Some other fruits that can disperse via water are nipa palm and screw pine.
Some fruits fling seeds substantial distances (up to 100 m in sandbox tree) via explosive dehiscence or other mechanisms, e.g., impatiens and squirting cucumber.
Uses.
Many hundreds of fruits, including fleshy fruits (like apple, kiwifruit, mango,peach, pear, and watermelon) are commercially valuable as human food, eaten both fresh and as jams, marmalade and other preserves. Fruits are also used in manufactured foods (e.g., cakes, cookies, ice cream, muffins, or yogurt) or beverages, such as fruit juices (e.g., apple juice, grape juice, or orange juice) or alcoholic beverages (e.g., brandy, fruit beer, or wine), Fruits are also used for gift giving, e.g., in the form of Fruit Baskets and Fruit Bouquets.
Many "vegetables" in culinary "parlance" are botanical fruits, including bell pepper, cucumber, eggplant, green bean, okra, pumpkin, squash, tomato, and zucchini. Olive fruit is pressed for olive oil. Spices like allspice, black pepper, paprika, and vanilla are derived from berries.
Nutritional value.
Fresh fruits are generally high in fiber, vitamin C, and water.
Regular consumption of fruit is generally associated with reduced risks of several diseases and functional declines associated with aging.
Nonfood uses.
Because fruits have been such a major part of the human diet, various cultures have developed many different uses for fruits they do not depend on for food. For example: 
Safety.
For food safety, the CDC recommends proper fruit handling and preparation to reduce the risk of food contamination and foodborne illness. Fresh fruits and vegetables should be carefully selected; at the store, they should not be damaged or bruised; and pre-cut pieces should be refrigerated or surrounded by ice. 
All fruits and vegetables should be rinsed before eating. This recommendation also applies to produce with rinds or skins that are not eaten. It should be done just before preparing or eating to avoid premature spoilage. 
Fruits and vegetables should be kept separate from raw foods like meat, poultry, and seafood, as well as from utensils that have come in contact with raw foods. Fruits and vegetables that are not going to be cooked should be thrown away if they have touched raw meat, poultry, seafood, or eggs. 
All cut, peeled, or cooked fruits and vegetables should be refrigerated within two hours. After a certain time, harmful bacteria may grow on them and increase the risk of foodborne illness.
Allergies.
Fruit allergies make up about 10 percent of all food related allergies
Storage.
All fruits benefit from proper post harvest care, and in many fruits, the plant hormone ethylene causes ripening. Therefore, maintaining most fruits in an efficient cold chain is optimal for post harvest storage, with the aim of extending and ensuring shelf life.

</doc>
<doc id="10844" url="https://en.wikipedia.org/wiki?curid=10844" title="French materialism">
French materialism

French materialism is the name given to a handful of French 18th-century philosophers during the Age of Enlightenment, many of them clustered around the salon of Baron d'Holbach. Although there are important differences between them, all of them were materialists who believed that the world was made up of a single substance, matter, the motions and properties of which could be used to explain all phenomena. 
Prominent French materialists of the 18th century include:

</doc>
<doc id="10845" url="https://en.wikipedia.org/wiki?curid=10845" title="February">
February

February ( or or ) is the second month of the year in the Julian and Gregorian calendars. It is the shortest month and the only month with fewer than 30 days. The month has 28 days in common years or 29 days in leap years, with the quadrennial 29th day being called the "leap day." 
February is the third month of meteorological winter in the Northern Hemisphere. In the Southern Hemisphere, February is the last month of summer (the seasonal equivalent of August in the Northern Hemisphere, in meteorological reckoning).
History.
The Roman month "Februarius" was named after the Latin term "februum", which means "purification", via the purification ritual "Februa" held on February 15 (full moon) in the old lunar Roman calendar. January and February were the last two months to be added to the Roman calendar, since the Romans originally considered winter a monthless period. They were added by Numa Pompilius about 713 BC. February remained the last month of the calendar year until the time of the decemvirs (c. 450 BC), when it became the second month. At certain intervals February was truncated to 23 or 24 days, and a 27-day intercalary month, Intercalaris, was inserted immediately after February to realign the year with the seasons.
Under the reforms that instituted the Julian calendar, Intercalaris was abolished, leap years occurred regularly every fourth year, and in leap years February gained a 29th day. Thereafter, it remained the second month of the calendar year, meaning the order that months are displayed (January, February, March, ..., December) within a year-at-a-glance calendar. Even during the Middle Ages, when the numbered Anno Domini year began on March 25 or December 25, the second month was February whenever all twelve months were displayed in order. The Gregorian calendar reforms made slight changes to the system for determining which years were leap years and thus contained a 29-day February.
Historical names for February include the Old English terms Solmonath (mud month) and Kale-monath (named for cabbage) as well as Charlemagne's designation Hornung. In Finnish, the month is called "helmikuu", meaning "month of the pearl"; when snow melts on tree branches, it forms droplets, and as these freeze again, they are like pearls of ice. In Polish and Ukrainian, respectively, the month is called "luty" or "лютий", meaning the month of ice or hard frost. In Macedonian the month is "sechko" (сечко), meaning month of cutting oo. In Czech, it is called "únor", meaning month of submerging f river ic. Croatians call the month "veljača", whose meaning is unknown but may come from the word for "greater," a possible reference to the days increasing in length.
In Slovene, February is traditionally called "svečan", related to icicles or Candlemas. This name originates from "sičan", written as "svičan" in the "New Carniolan Almanac" from 1775 and changed to its final form by Franc Metelko in his "New Almanac" from 1824. The name was also spelled "sečan", meaning "the month of cutting down of trees". In 1848, a proposal was put forward in "Kmetijske in rokodelske novice" by the Slovene Society of Ljubljana to call this month "talnik" (related to ice melting), but it did not stick. The idea was proposed by the priest and patriot Blaž Potočnik. Another name of February in Slovene was "vesnar", after the mythological character Vesna.
Pronunciation.
February may be pronounced either as ( or or ). Many people pronounce it as ( rather than ), as if it were spelled "Feb-u-ary". This comes about by analogy with "January" (which ends in "-uary" but not "-ruary"), as well as by a dissimilation effect whereby having two "r"s close to each other causes one to change for ease of pronunciation.
Patterns.
February starts on the same day of the week as both March and November in common years, and as August in leap years. February ends on the same day of the week as October every year and on the same day of the week as January in common years only. February starts on the same day of the week as June of the previous year in all years. February ends on the same day of the week as May of the previous year in common years and August and November of the previous year in leap years. February ends on the same day of the week as July of the following year in years immediately before common years and April and December of the following year in years immediately before leap years. February starts on the same day of the week as May of the following year in leap years and years immediately before leap years. In leap years, it is the only month that ends on the same weekday it began.
Having only 28 days in common years, it is the only month of the year that can pass without a single full moon. This last happened in 1999 and will next happen in 2018.
February is also the only month of the calendar that once every six years and twice every 11 years consecutively, either back into the past or forward into the future, will have four full 7-day weeks. In countries that start their week on a Monday, it occurs as part of a common year starting on Friday, in which February 1st is a Monday and the 28th is a Sunday, this was observed in 2010 and can be traced back 11 years to 1999, 6 years back to 1993, 11 years back to 1982, 11 years back to 1971 and 6 years back to 1965, and will be observed in 2021. In countries that start their week on a Sunday, it occurs in a common year starting on Thursday, with the next occurrence in 2026, and previous occurrences in 2015 (11 years earlier than 2026), 2009 (6 years earlier than 2015), 1998 (11 years earlier than 2009) and 1987 (11 years earlier than 1998). This works unless the pattern is broken by a skipped leap year, but no leap year has been skipped since 1900 and no others will be skipped until 2100.
Observances.
"This list does not necessarily imply either official status nor general observance."
Monday closest to January 29 - February 1
First Monday - February 1
First Week of February (first Monday, ending on Sunday) - February 1-7
First Friday - February 5
First Saturday - February 6
First Sunday - February 7
Second Monday - February 8
Second Day of the second week - February 8
Second Tuesday - February 9
Second Saturday - February 13
Second Sunday - February 14
Third Monday' - February 15
Third Thursday - February 18
Third Friday - February 19
Week of February 22 - February 21-27
Last Tuesday - February 23
Last Friday - February 26
Last Saturday - February 27
Last day of February - February 29

</doc>
<doc id="10846" url="https://en.wikipedia.org/wiki?curid=10846" title="February 1">
February 1


</doc>
<doc id="10847" url="https://en.wikipedia.org/wiki?curid=10847" title="First Lady of the United States">
First Lady of the United States

The First Lady of the United States (FLOTUS), is an unofficial title and position traditionally held by the wife of the president, concurrent with his term of office. Historically, if a president is not married, or if the president's wife is unable to act as First Lady, the president usually asks a female relative or friend to act as White House hostess.
The position of the First Lady is unofficial and carries no official duties. Nonetheless, first ladies have traditionally held this highly visible position in U.S. government. The role of the First Lady has evolved over the centuries. She is, first and foremost, the hostess of the White House. She organizes and attends official ceremonies and functions of state either along with, or in place of, the president.
Current First Ladies.
The current First Lady is Michelle Obama. At present, there are five living former first ladies: Rosalynn Carter, wife of Jimmy Carter; Nancy Reagan, widow of Ronald Reagan; Barbara Bush, wife of George H. W. Bush; Hillary Rodham Clinton, wife of Bill Clinton; and Laura Bush, wife of George W. Bush.
Origins of the title.
The use of the title "First Lady" to describe the spouse or hostess of an executive began in the United States. In the early days of the republic, there was not a generally accepted title for the wife of the president. Many early first ladies expressed their own preference for how they were addressed, including the use of such titles as "Lady", "Mrs. President", and "Mrs. Presidentress"; Martha Washington was often referred to as "Lady Washington." One of the earliest uses of the term "First Lady" was applied to her in an 1838 newspaper article that appeared in the St. Johnsbury (VT) Caledonian, the author, "Mrs. Sigourney", discussing how Martha Washington had not changed, even after her husband George became president, wrote that "The first lady of the nation still preserved the habits of early life. Indulging in no indolence, she left the pillow at dawn, and after breakfast, retired to her chamber for an hour for the study of the scriptures and devotion".
Dolley Madison was reportedly referred to as "First Lady" in 1849 at her funeral in a eulogy delivered by President Zachary Taylor; however, no written record of this eulogy exists, nor did any of the newspapers of her day refer to her by that title. Sometime after 1849, the title began being used in Washington, D.C., social circles. One of the earliest known written examples comes from the November 3, 1863, diary entry of William Howard Russell, in which he referred to gossip about "the First Lady in the Land," referring to Mary Todd Lincoln. The title first gained nationwide recognition in 1877, when newspaper journalist Mary C. Ames referred to Lucy Webb Hayes as "the First Lady of the Land" while reporting on the inauguration of Rutherford B. Hayes. The frequent reporting on Lucy Hayes' activities helped spread use of the title outside Washington. A popular 1911 comedic play about Dolley Madison by playwright Charles Nirdlinger, titled "The First Lady in the Land", popularized the title further. By the 1930s it was in wide use. Use of the title later spread from the United States to other nations.
When Edith Wilson took control of her husband's schedule in 1919 after he had a debilitating stroke, one Republican senator labeled her "the Presidentress who had fulfilled the dream of the suffragettes by changing her title from First Lady to Acting First Man."
The wife of the :Vice President of the United States is sometimes referred to as the Second Lady of the United States, but this title is much less common.
Several women who were not presidents' wives have served as First Lady, as when the president was a bachelor or widower, or when the wife of the president was unable to fulfill the duties of the First Lady herself. In these cases, the position has been filled by a female relative or friend of the president, such as Martha Jefferson Randolph during Jefferson's presidency, Emily Donelson and Sarah Yorke Jackson during Jackson's, Mary Elizabeth (Taylor) Bliss during Taylor's, Mary Harrison McKee during Benjamin Harrison's presidency, upon her mother's death, and Harriet Lane during Buchanan's.
Role.
Burns identifies four successive main themes of the First Ladyship: as public woman (1900–1929); as political celebrity (1932–1961); as political activist (1964–1977); and as political interloper (1980–2001).
The position of the First Lady is not an elected one and carries no official duties. Nonetheless, first ladies have held a highly visible position in U.S. government. The role of the First Lady has evolved over the centuries. She is, first and foremost, the hostess of the White House. She organizes and attends official ceremonies and functions of state either along with, or in place of, the president.
Both Martha Washington and Abigail Adams gained fame from the Revolutionary War and were treated as if they were "ladies" of the British royal court. Dolley Madison popularized the First Ladyship by engaging in efforts to assist orphans and women, by dressing in elegant fashions and attracting newspaper coverage, and by risking her life to save iconic treasures during the War of 1812. Madison set the standard for the ladyship and her actions were the model for nearly every First Lady until Eleanor Roosevelt in the 1930s. Plagued by a paralytic illness, President Franklin D. Roosevelt was not free to travel around the country, so Mrs. Roosevelt assumed this role. She authored a weekly newspaper column and hosted a radio show. Jacqueline Kennedy led an effort to redecorate and restore the White House while she was First Lady.
Over the course of the 20th century it became increasingly common for first ladies to select specific causes to promote, usually ones that are not politically divisive. It is common for the First Lady to hire a staff to support these activities. Lady Bird Johnson pioneered environmental protection and beautification; Pat Nixon encouraged volunteerism and traveled extensively abroad; Betty Ford supported women's rights; Rosalynn Carter aided those with mental disabilities; Nancy Reagan founded the Just Say No drug awareness campaign; Barbara Bush promoted literacy; Hillary Clinton sought to reform the healthcare system in the U.S.; and Laura Bush supported women's rights groups and encouraged childhood literacy. Michelle Obama has become identified with supporting military families and tackling childhood obesity.
Clinton was elected a U.S. Senator from New York in 2001 and was the Secretary of State in the Obama administration from 2009 to 2013. Many first ladies, including Jacqueline Kennedy, Nancy Reagan, and Michelle Obama have been significant fashion trendsetters. There is a strong tradition against the First Lady holding outside employment while serving as White House hostess. However, some first ladies have exercised a degree of political influence by virtue of being an important adviser to the president. During Hillary Clinton's campaign for election to the U.S. Senate, the couple's daughter, Chelsea, took over much of the First Lady's role.
Office of the First Lady.
The Office of the First Lady of the United States is accountable to the First Lady for her to carry out her duties as hostess of the White House, and is also in charge of all social and ceremonial events of the White House. The First Lady has her own staff that includes a chief of staff, press secretary, White House Social Secretary, Chief Floral Designer, etc. The Office of the First Lady is an entity of the White House Office, a branch of the Executive Office of the President. When First Lady Hillary Clinton decided to pursue a run for Senator of New York, she set aside her duties as first lady and moved to Chappaqua, New York to establish state residency. She resumed her duties as First Lady after winning her senatorial campaign, and retained her duties as both first lady and U.S. Senator for the seventeen-day overlap before Bill Clinton's term came to an end.
Exhibitions and collections.
Established in 1912, the First Ladies Collection has been one of the most popular attractions at the Smithsonian Institution. The original exhibition opened in 1914 and was one of the first at the Smithsonian to prominently feature women. Originally focused largely on fashion, the exhibition now delves deeper into the contributions of first ladies to the presidency and American society. In 2008, "First Ladies at the Smithsonian" opened at the National Museum of American History as part of its reopening year celebration. That exhibition served as a bridge to the museum's expanded exhibition on first ladies' history that opened on November 19, 2011. "The First Ladies" explores the unofficial but important position of first lady and the ways that different women have shaped the role to make their own contributions to the presidential administrations and the nation. The exhibition features 26 dresses and more than 160 other objects, ranging from those of Martha Washington to Michelle Obama, and includes White House china, personal possessions and other objects from the Smithsonian's unique collection of first ladies' materials.
First Lady and fashion.
Some first ladies have garnered attention for their dress and style. Jacqueline Kennedy, for instance, became a global fashion icon: her style was copied by commercial manufacturers and imitated by many young women, and she was named to the International Best Dressed List Hall of Fame in 1965. Michelle Obama has also received significant attention for her fashion choices: style writer Robin Givhan praised her in "The Daily Beast", arguing that the First Lady's style has helped to enhance the public image of the office.

</doc>
<doc id="10852" url="https://en.wikipedia.org/wiki?curid=10852" title="Frank Herbert">
Frank Herbert

Frank Patrick Herbert, Jr. (October 8, 1920 – February 11, 1986) was an American science fiction writer best known for the novel "Dune" and its five sequels. Though he became famous for science fiction, he was also a newspaper journalist, photographer, short story writer, book reviewer, ecological consultant and lecturer.
The "Dune" saga, set in the distant future and taking place over millennia, deals with complex themes such as human survival and evolution, ecology, and the intersection of religion, politics and power. "Dune" itself is the best-selling science fiction novel of all time and the series is widely considered to be among the classics of the genre.
Biography.
Early life.
Frank Herbert was born on October 8, 1920, in Tacoma, Washington, to Frank Patrick Herbert, Sr. and Eileen (McCarthy) Herbert. Because of a poor home environment, he ran away from home in 1938 to live with an aunt and uncle in Salem, Oregon. He enrolled in high school at Salem High School (now North Salem High School), where he graduated the next year. In 1939 he lied about his age to get his first newspaper job at the "Glendale Star". Herbert then returned to Salem in 1940 where he worked for the "Oregon Statesman" newspaper (now "Statesman Journal") in a variety of positions, including photographer.
He served in the U.S. Navy's Seabees for six months as a photographer during World War II, then he was given a medical discharge. He married Flora Parkinson in San Pedro, California in 1940. They had a daughter, Penny (b. February 16, 1942), but divorced in 1945.
After the war Herbert attended the University of Washington, where he met Beverly Ann Stuart at a creative writing class in 1946. They were the only students who had sold any work for publication; Herbert had sold two pulp adventure stories to magazines, the first to "Esquire" in 1945, and Stuart had sold a story to "Modern Romance" magazine. They married in Seattle, Washington on June 20, 1946 and had two sons, Brian Patrick Herbert (b. June 29, 1947, Seattle, Washington) and Bruce Calvin Herbert (b. June 26, 1951, Santa Rosa, California d. June 15, 1993, San Rafael, California, a professional photographer and gay rights activist).
In 1949 Herbert and his wife moved to California to work on the "Santa Rosa Press-Democrat". Here they befriended the psychologists Ralph and Irene Slattery. The Slatterys introduced Herbert to the work of several thinkers who would influence his writing, including Freud, Jung, Jaspers and Heidegger; they also familiarized Herbert with Zen Buddhism.
Herbert did not graduate from the university; according to his son Brian, he wanted to study only what interested him and so did not complete the required curriculum. He returned to journalism and worked at the "Seattle Star" and the "Oregon Statesman". He was a writer and editor for the "San Francisco Examiner's" "California Living" magazine for a decade.
In a 1973 interview, Herbert stated that he had been reading science fiction "about ten years" before he
began writing in the genre, and he listed his favorite authors as H. G. Wells, Robert A. Heinlein, Poul Anderson and Jack Vance.
Herbert's first science fiction story, "Looking for Something", was published in the April 1952 issue of "Startling Stories", then a monthly edited by Samuel Mines. Three more of his stories appeared in 1954 issues of "Astounding Science Fiction" and "Amazing Stories". His career as a novelist began in 1955 with the serial publication of "Under Pressure" in "Astounding" from November 1955; afterward it was issued as a book by Doubleday, "The Dragon in the Sea". The story explored sanity and madness in the environment of a 21st-century submarine and predicted worldwide conflicts over oil consumption and production. It was a critical success but not a major commercial one. During this time Herbert also worked as a speechwriter for Republican senator Guy Cordon.
"Dune".
Herbert began researching "Dune" in 1959. He was able to devote himself wholeheartedly to his writing career because his wife returned to work full-time as an advertising writer for department stores, becoming the breadwinner during the 1960s. He later told Willis E. McNelly that the novel originated when he was supposed to do a magazine article on sand dunes in the Oregon Dunes near Florence, Oregon. He became too involved and ended up with far more raw material than needed for an article. The article was never written, but instead planted the seed that led to "Dune".
"Dune" took six years of research and writing to complete and it was much longer than commercial science fiction of the time was supposed to run. "Analog" (the renamed "Astounding", still edited by John W. Campbell) published it in two parts comprising eight installments, "Dune World" from December 1963 and "Prophet of Dune" in 1965. It was then rejected by nearly twenty book publishers. One editor prophetically wrote, "I might be making the mistake of the decade, but ..."
Sterling E. Lanier, an editor of Chilton Book Company (known mainly for its auto-repair manuals) had read the Dune serials and offered a $7,500 advance plus future royalties for the rights to publish them as a hardcover book. Herbert rewrote much of his text. "Dune" was soon a critical success. It won the Nebula Award for Best Novel in 1965 and shared the Hugo Award in 1966 with "...And Call Me Conrad" by Roger Zelazny. "Dune" was the first major ecological science fiction novel, embracing a multitude of sweeping, inter-related themes and multiple character viewpoints, a method that ran through all Herbert's mature work.
"Dune" was not immediately a bestseller. By 1968 Herbert had made $20,000 from it, far more than most science fiction novels of the time were generating, but not enough to let him take up full-time writing. However, the publication of "Dune" did open doors for him. He was the "Seattle Post-Intelligencer's" education writer from 1969 to 1972 and lecturer in general studies and interdisciplinary studies at the University of Washington (1970–1972). He worked in Vietnam and Pakistan as social and ecological consultant in 1972. In 1973 he was director-photographer of the television show "The Tillers".
By 1972, Herbert retired from newspaper writing and became a full-time fiction writer. During the 1970s and 1980s, Herbert enjoyed considerable commercial success as an author. He divided his time between homes in Hawaii and Washington's Olympic Peninsula; his home in Port Townsend on the peninsula was intended to be an "ecological demonstration project". During this time he wrote numerous books and pushed ecological and philosophical ideas. He continued his "Dune" saga, following it with "Dune Messiah", "Children of Dune", and "God Emperor of Dune". Other highlights were "The Dosadi Experiment", "The Godmakers", "The White Plague" and the books he wrote in partnership with Bill Ransom: "The Jesus Incident", "The Lazarus Effect", and "The Ascension Factor" which were sequels to "". He also helped launch the career of Terry Brooks with a very positive review of Brooks' first novel, "The Sword of Shannara", in 1977.
Success, family changes, and death.
Herbert's change in fortune was shadowed by tragedy. In 1974, Beverly underwent an operation for cancer. She lived ten more years, but her health was adversely affected by the surgery. During this period, Herbert was the featured speaker at the Octocon II science fiction convention at the El Rancho Tropicana in Santa Rosa, California in October 1978; in 1979, he met anthropologist James Funaro with whom he conceived the Contact Conference. Beverly Herbert died on February 7, 1984, the same year that "Heretics of Dune" was published; in his afterword to 1985's "", Frank Herbert wrote a eulogy for her.
In 1983, British heavy metal band Iron Maiden requested permission from Herbert's publisher to name a song on their album "Piece of Mind" after "Dune", but were told that the author had a strong distaste for their style of music. They instead titled the song "To Tame a Land".
1984 was a tumultuous year in Herbert's life. During this same year of his wife's death, his career took off with the release of David Lynch's film version of "Dune". Despite high expectations, a big-budget production design and an A-list cast, the movie drew mostly poor reviews in the United States. However, despite a disappointing response in the USA, the film was a critical and commercial success in Europe and Japan.
After Beverly's death, Herbert married Theresa Shackleford in 1985, the year he published "Chapterhouse: Dune", which tied up many of the saga's story threads. This would be Herbert's final single work (the anthology "Eye" was published that year, and "Man of Two Worlds" was published in 1986). He died of a massive pulmonary embolism while recovering from surgery for pancreatic cancer on February 11, 1986 in Madison, Wisconsin age 65. He was raised a Catholic but adopted Zen Buddhism as an adult.
Criticism of government.
Herbert was a critic of the Soviet Union and shared many views with controversial Republican senator, Joseph McCarthy, of whom Herbert was also a distant relative and referred to as "Cousin Joe." Herbert was, however, appalled to learn of McCarthy's blacklisting of suspected Communists from working in certain careers and believed that he was endangering essential freedoms of citizens of the United States. Herbert believed that governments lie to protect themselves and that, following the infamous Watergate scandal, President Richard Nixon had unwittingly taught an important lesson in not trusting government.
Ideas and themes.
Frank Herbert used his science fiction novels to explore complex ideas involving philosophy, religion, psychology, politics and ecology, which have caused many of his readers to take an interest in these areas. The underlying thrust of his work was a fascination with the question of human survival and evolution. Herbert has attracted a sometimes fanatical fan base, many of whom have tried to read everything he wrote, fiction or non-fiction, and see Herbert as something of an authority on the subject matters of his books. Indeed, such was the devotion of some of his readers that Herbert was at times asked if he was founding a cult, something he was very much against.
There are a number of key themes in Herbert's work:
Frank Herbert carefully refrained from offering his readers formulaic answers to many of the questions he explored.
Status and influence on science fiction.
"Dune" and the "Dune" saga constitute one of the world's best-selling science fiction series and novels; "Dune" in particular has received widespread critical acclaim, winning the Nebula Award in 1965 and sharing the Hugo Award in 1966, and is frequently considered one of the best science fiction novels ever, if not the best. "Locus" subscribers voted it the all-time best SF novel in 1975, again in 1987, and the best "before 1990" in 1998. According to contemporary Robert A. Heinlein, Herbert's opus was "powerful, convincing, and most ingenious."
"Dune" is considered a landmark novel for a number of reasons:
Herbert wrote more than twenty novels after "Dune" that are regarded as being of variable quality. Books like "The Green Brain", "The Santaroga Barrier" seemed to hark back to the days before "Dune", when a good technological idea was all that was needed to drive a sci-fi novel. And some fans of the "Dune" saga are critical of the follow-up novels as being subpar.
Herbert never again equalled the critical acclaim he received for "Dune". Neither his sequels to "Dune" nor any of his other books won a Hugo or Nebula Award, although almost all of them were "New York Times" Best Sellers. Some felt that "Children of Dune" was almost too literary and too dark to get the recognition it may have deserved; others felt that "The Dosadi Experiment" lacked an epic quality that fans had come to expect.
Largely overlooked because of the concentration on "Dune" was Herbert's 1973 novel, "Hellstrom's Hive", with its minutely worked-out depiction of a human society modeled on social insects, which could be counted a major utopia/dystopia.
Malcolm Edwards in the "Encyclopedia of Science Fiction" wrote:
Much of Herbert's work makes difficult reading. His ideas were genuinely developed concepts, not merely decorative notions, but they were sometimes embodied in excessively complicated plots and articulated in prose which did not always match the level of thinking ... His best novels, however, were the work of a speculative intellect with few rivals in modern science fiction. 
The Science Fiction Hall of Fame inducted Herbert in 2006.
California State University, Fullerton's Pollack Library has several of Herbert's draft manuscripts of "Dune" and other works, with the author's notes, in their Frank Herbert Archives.
Bibliography.
Posthumously published works.
Beginning in 2012, Herbert's estate and WordFire Press have released four previously unpublished novels in e-book and paperback formats: "High-Opp" (2012), "Angels' Fall" (2013), "A Game of Authors" (2013), and "A Thorn in the Bush" (2014).
In recent years, Frank Herbert's son Brian Herbert and author Kevin J. Anderson have added to the "Dune" franchise, using notes left behind by Frank Herbert and discovered over a decade after his death. Brian Herbert and Anderson have written two prequel trilogies ("Prelude to Dune" and "Legends of Dune") exploring the history of the "Dune" universe before the events within "Dune", as well as two post-"Chapterhouse Dune" novels that complete the original series ("Hunters of Dune" and "Sandworms of Dune") based on Frank Herbert's own "Dune 7" outline.

</doc>
<doc id="10853" url="https://en.wikipedia.org/wiki?curid=10853" title="Fictional language">
Fictional language

Fictional languages are constructed languages created as part of a fictional setting, for example in books or movies. Fictional languages are intended to be the languages of a fictional world and are often designed with the intent of giving more depth and an appearance of plausibility to the fictional worlds with which they are associated, and to have their characters communicate in a fashion which is both alien and dislocated.
Some of these languages, e.g., in worlds of fantasy fiction, alternate universes, Earth's future, or alternate history, are presented as distorted versions or dialects of modern English or other natural language, while others are independently designed conlangs.
Purpose.
Fictional languages are separated from artistic languages by both purpose and relative completion: a fictional language often has the least amount of grammar and vocabulary possible, and rarely extends beyond the absolutely necessary. At the same time, some others have developed languages in detail for their own sake, such as J. R. R. Tolkien's Quenya and Sindarin, Star Trek's Klingon language and Avatar's Na'vi language which exist as functioning, usable languages. Here "fictional" can be a misnomer.
By analogy with the word "conlang", the term "conworld" is used to describe these fictional worlds, inhabited by fictional constructed cultures. The conworld influences vocabulary (what words the language will have for flora and fauna, articles of clothing, objects of technology, religious concepts, names of places and tribes, etc.), as well as influencing other factors such as pronouns, or how their cultures view the break-off points between colors or the gender and age of family members.
Professional fictional languages.
Professional fictional languages are those languages created for use in books, movies, television shows, video games, comics, toys, and musical albums (prominent examples of works featuring fictional languages include the Middle-earth and Star Trek universes and the game Myst).
Alien languages.
A notable subgenre of fictional languages are alien languages, the ones that are used or might be used by putative extraterrestrial life forms. Alien languages are subject of both science fiction and scientific research.
Perhaps the most fully developed fictional alien language is the Klingon language of the Star Trek universe - a fully developed constructed language.
The problem of alien language has confronted generations of science fiction writers; some have created fictional languages for their characters to use, while others have circumvented the problem through translation devices or other fantastic technology.
Although this field remains largely confined to science fiction, the possibility of intelligent extraterrestrial life makes the question of alien language a credible topic for scientific and philosophical speculation.
While many cases an alien language is but an element of fictional reality, in a number of science fiction works the core of the plot are linguistic and psychological problems of communication between various alien races.
Internet-based fictional languages.
Internet-based fictional languages are hosted along with their "conworlds" on the Internet, and based at these sites, becoming known to the world through the visitors to these sites; Verdurian, the language of Mark Rosenfelder's Verduria on the planet of Almea, is a flagship Internet-based fictional language. Many other fictional languages and their associated conworlds are created privately by their inventor, known only to the inventor and perhaps a few friends. In this context the term "professional" (used for the first category) as opposed to "amateur" (used for the second and third) refers only to the professionalism of the used medium, and not to the professionalism of the language itself or its creator. In fact, most professional languages are the work of non-linguists, while many amateur languages were in fact created by linguists, and in general the latter are better developed. 

</doc>
<doc id="10854" url="https://en.wikipedia.org/wiki?curid=10854" title="Formula One">
Formula One

Formula One (also Formula 1 or F1) is the highest class of single-seat auto racing that is sanctioned by the Fédération Internationale de l'Automobile (FIA). The FIA Formula One World Championship has been the premier form of racing since the inaugural season in 1950, although other Formula One races were regularly held until 1983. The "formula", designated in the name, refers to a set of rules, to which all participants' cars must conform. The F1 season consists of a series of races, known as "Grands Prix" (from French, originally meaning great prizes), held throughout the world on purpose-built F1 circuits and public roads.
The results of each race are evaluated using a points system to determine two annual World Championships, one for drivers, one for constructors. The racing drivers are required to be holders of valid Super Licences, the highest class of racing licence issued by the FIA. The races are required to be held on tracks graded 1 (formerly A), the highest grade a track can receive by the FIA. Most events are held in rural locations on purpose-built tracks, but there are several events in city centres throughout the world, with the Monaco Grand Prix being the most obvious and famous example.
Formula One cars are the fastest road course racing cars in the world, owing to very high cornering speeds achieved through the generation of large amounts of aerodynamic downforce. Formula One cars race at speeds of up to with engines currently limited in performance to a maximum of 15,000 RPM. The cars are capable of lateral acceleration in excess of five g in corners. The performance of the cars is very dependent on electronicsalthough traction control and other driving aids have been banned since 2008and on aerodynamics, suspension and tyres. The formula has radically evolved and changed through the history of the sport.
While Europe is the sport's traditional base, and hosts about half of each year's races, the sport's scope has expanded significantly and an increasing number of Grands Prix are held on other continents. F1 had a total global television audience of 425 million people during the course of the 2014 season. Grand Prix racing began in 1906 and became the most popular type internationally in the second half of the twentieth century. The Formula One Group is the legal holder of the commercial rights.
With the cost of designing and building mid-tier cars being of the order of $120 million, Formula One's economic effect and creation of jobs is significant, and its financial and political battles are widely reported. Its high profile and popularity have created a major merchandising environment, which has resulted in great investments from sponsors and budgets in the hundreds of millions for the constructors. Since 2000 the sport's spiraling expenditures and the distribution of prize money which favors established top teams have forced complaints from smaller teams and led several teams to bankruptcy.
History.
The Formula One series originated with the European Grand Prix Motor Racing ("q.v." for pre-1947 history) of the 1920s and 1930s. The formula is a set of rules which all participants' cars must meet. Formula One was a new formula agreed upon after World War II during 1946, with the first non-championship races being held that year. A number of Grand Prix racing organisations had laid out rules for a world championship before the war, but due to the suspension of racing during the conflict, the World Drivers' Championship was not formalised until 1947. The first world championship race was held at Silverstone, United Kingdom in 1950. A championship for constructors followed in 1958. National championships existed in South Africa and the UK in the 1960s and 1970s. Non-championship Formula One events were held for many years, but due to the increasing cost of competition, the last of these occurred in 1983.
Return of racing.
The first World Championship for Drivers was won by Italian Giuseppe Farina in his Alfa Romeo in 1950, barely defeating his Argentine teammate Juan Manuel Fangio. However Fangio won the title in 1951, 1954, 1955, 1956 and 1957 (His record of five World Championship titles stood for 45 years until German driver Michael Schumacher took his sixth title in 2003), his streak interrupted (after an injury) by two-time champion Alberto Ascari of Ferrari. Although the UK's Stirling Moss was able to compete regularly, he was never able to win the world championship, and is now widely considered to be the greatest driver never to have won the title. Fangio, however, is remembered for dominating Formula One's first decade and has long been considered the "Grand Master" of Formula One.
This period featured teams managed by road car manufacturers Alfa Romeo, Ferrari, Mercedes-Benz, and Maserati; all of whom had competed before the war. The first seasons were run using pre-war cars like Alfa's 158. They were front-engined, with narrow tyres and 1.5-litre supercharged or 4.5-litre normally aspirated engines. The 1952 and 1953 world championships were run to Formula Two regulations, for smaller, less powerful cars, due to concerns over the paucity of Formula One cars available. When a new Formula One, for engines limited to 2.5 litres, was reinstated to the world championship for 1954, Mercedes-Benz introduced the advanced W196, which featured innovations such as desmodromic valves and fuel injection as well as enclosed streamlined bodywork. Mercedes drivers won the championship for two years, before the team withdrew from all motorsport in the wake of the 1955 Le Mans disaster.
The "Garagistes".
The first major technological development, Bugatti's re-introduction of mid-engined cars (following Ferdinand Porsche's pioneering Auto Unions of the 1930s), occurred with the Type 251, which was unsuccessful. Australian Jack Brabham, world champion during 1959, 1960, and 1966, soon proved the mid-engined design's superiority. By 1961, all regular competitors had switched to mid-engined cars. The Ferguson P99, a four-wheel drive design, was the last front-engined F1 car to enter a world championship race. It was entered in the 1961 British Grand Prix, the only front-engined car to compete that year.
The first British World Champion was Mike Hawthorn, who drove a Ferrari to the title during the 1958 season. However, when Colin Chapman entered F1 as a chassis designer and later founder of Team Lotus, British racing green came to dominate the field for the next decade. Including Brabham, Jim Clark, Jackie Stewart, John Surtees, Graham Hill, and Denny Hulme, British teams and Commonwealth drivers won twelve world championships between 1962 and 1973.
During 1962, Lotus introduced a car with an aluminium-sheet monocoque chassis instead of the traditional space-frame design. This proved to be the greatest technological breakthrough since the introduction of mid-engined cars. During 1968, Lotus painted Imperial Tobacco livery on their cars, thus introducing sponsorship to the sport.
Aerodynamic downforce slowly gained importance in car design from the appearance of aerofoils during the late 1960s. During the late 1970s, Lotus introduced ground-effect aerodynamics (previously used on Jim Hall's Chaparral 2J during 1970) that provided enormous downforce and greatly increased cornering speeds. So great were the aerodynamic forces pressing the cars to the track (up to five times the car's weight), extremely stiff springs were needed to maintain a constant ride height, leaving the suspension virtually solid, depending entirely on the tyres for any small amount of cushioning of the car and driver from irregularities of the road surface.
Big business.
Beginning in the 1970s, Bernie Ecclestone rearranged the management of Formula One's commercial rights; he is widely credited with transforming the sport into the multibillion-dollar business it now is. When Ecclestone bought the Brabham team during 1971 he gained a seat on the Formula One Constructors' Association and during 1978 he became its president. Previously, the circuit owners controlled the income of the teams and negotiated with each individually, however Ecclestone persuaded the teams to "hunt as a pack" through FOCA. He offered Formula One to circuit owners as a package which they could take or leave. In return for the package almost all that was required was to surrender trackside advertising.
The formation of the Fédération Internationale du Sport Automobile (FISA) during 1979 set off the FISA–FOCA controversy, during which FISA and its president Jean-Marie Balestre disputed repeatedly with FOCA over television revenues and technical regulations. "The Guardian" said of FOCA that Ecclestone and Max Mosley "used it to wage a guerrilla war with a very long-term aim in view". FOCA threatened to establish a rival series, boycotted a Grand Prix and FISA withdrew its sanction from races. The result was the 1981 Concorde Agreement, which guaranteed technical stability, as teams were to be given reasonable notice of new regulations. Although FISA asserted its right to the TV revenues, it handed the administration of those rights to FOCA.
FISA imposed a ban on ground-effect aerodynamics during 1983. By then, however, turbocharged engines, which Renault had pioneered in 1977, were producing over and were essential to be competitive. By 1986, a BMW turbocharged engine achieved a flash reading of 5.5 bar pressure, estimated to be over in qualifying for the Italian Grand Prix. The next year power in race trim reached around , with boost pressure limited to only 4.0 bar. These cars were the most powerful open-wheel circuit racing cars ever. To reduce engine power output and thus speeds, the FIA limited fuel tank capacity in 1984 and boost pressures in 1988 before banning turbocharged engines completely in 1989.
The development of electronic driver aids began during the 1980s. Lotus began to develop a system of active suspension which first appeared during 1982 on the 91. By 1987, this system had been perfected and was driven to victory by Ayrton Senna in the Monaco Grand Prix that year. In the early 1990s other teams followed suit and semi-automatic gearboxes and traction control were a natural progression. The FIA, due to complaints that technology was determining the outcome of races more than driver skill, banned many such aids for 1994. This resulted in cars that were previously dependent on electronic aids becoming very "twitchy" and difficult to drive (particularly the Williams FW16). Many observers felt the ban on driver aids was in name only as they "proved difficult to police effectively".
The teams signed a second Concorde Agreement during 1992 and a third in 1997, which expired on the last day of 2007.
On the track, the McLaren and Williams teams dominated the 1980s and 1990s, with Brabham also being competitive during the early part of the 1980s, winning two Drivers' Championships with Nelson Piquet. Powered by Porsche, Honda, and Mercedes-Benz, McLaren won sixteen championships (seven constructors' and nine drivers') in that period, while Williams used engines from Ford, Honda, and Renault to also win sixteen titles (nine constructors' and seven drivers'). The rivalry between racers Ayrton Senna and Alain Prost became F1's central focus during 1988, and continued until Prost retired at the end of 1993. Senna died at the 1994 San Marino Grand Prix after crashing into a wall on the exit of the notorious curve Tamburello, having taken over Prost's lead drive at Williams that year. The FIA worked to improve the sport's safety standards since that weekend, during which Roland Ratzenberger also lost his life in an accident during Saturday qualifying. No driver had died of injuries sustained on the track at the wheel of a Formula One car for 20 years, until the 2014 Japanese Grand Prix where Jules Bianchi collided with a recovery vehicle after aquaplaning off the circuit. Since 1994, three track marshals have lost their lives, one at the 2000 Italian Grand Prix, the second at the 2001 Australian Grand Prix and the third at the 2013 Canadian Grand Prix.
Since the deaths of Senna and Ratzenberger, the FIA has used safety as a reason to impose rule changes which otherwise, under the Concorde Agreement, would have had to be agreed upon by all the teams — most notably the changes introduced for 1998. This so-called 'narrow track' era resulted in cars with smaller rear tyres, a narrower track overall, and the introduction of grooved tyres to reduce mechanical grip. There were to be four grooves on the front (three in the first year) and rear that ran through the entire circumference of the tyre. The objective was to reduce cornering speeds and to produce racing similar to rainy conditions by enforcing a smaller contact patch between tyre and track. This, according to the FIA, was to promote driver skill and provide a better spectacle.
Results have been mixed as the lack of mechanical grip has resulted in the more ingenious designers clawing back the deficit with aerodynamic grip — pushing more force onto the tyres through wings and aerodynamic devices which in turn has resulted in less overtaking as these devices tend to make the wake behind the car 'dirty' (turbulent), preventing other cars from following closely due to their dependence on 'clean' air to make the car stick to the track. The grooved tyres also had the unfortunate side effect of initially being of a harder compound to be able to hold the grooved tread blocks, which resulted in spectacular accidents in times of aerodynamic grip failure as the harder compound could not grip the track as well.
Drivers from McLaren, Williams, Renault (formerly Benetton), and Ferrari, dubbed the "Big Four", won every World Championship from 1984 to 2008 and the teams themselves won every Constructors' Championship from 1979 to 2008. Due to the technological advances of the 1990s, the cost of competing in Formula One increased dramatically. This increased financial burdens, combined with the dominance of four teams (largely funded by big car manufacturers such as Mercedes-Benz), caused the poorer independent teams to struggle not only to remain competitive, but to stay in business, and forced several teams to withdraw. Since 1990, twenty-eight teams have withdrawn from Formula One. This has prompted former Jordan owner Eddie Jordan to say that the days of competitive privateers are over.
Manufacturers' return.
Michael Schumacher and Ferrari won five consecutive Drivers' Championships (2000–2004) and six consecutive constructors' championships (1999–2004). Schumacher set many new records, including those for Grand Prix wins (91), wins in a season (thirteen of eighteen), and most Drivers' Championships (seven). Schumacher's championship streak ended on 25 September 2005 when Renault driver Fernando Alonso became Formula One's youngest champion at that time. During 2006, Renault and Alonso won both titles again. Schumacher retired at the end of 2006 after sixteen years in Formula One, but came out of retirement for the 2010 season, racing for the newly formed Mercedes works team for three seasons.
During this period the championship rules were changed frequently by the FIA with the intention of improving the on-track action and cutting costs. Team orders, legal since the championship started during 1950, were banned during 2002 after several incidents in which teams openly manipulated race results, generating negative publicity, most famously by Ferrari at the 2002 Austrian Grand Prix. Other changes included the qualifying format, the points scoring system, the technical regulations, and rules specifying how long engines and tyres must last. A 'tyre war' between suppliers Michelin and Bridgestone saw lap times fall, although at the 2005 United States Grand Prix at Indianapolis seven out of ten teams did not race when their Michelin tyres were deemed unsafe for use, leading to Bridgestone becoming the sole tyre supplier to Formula One for the 2007 season. During 2006, Max Mosley outlined a 'green' future for Formula One, in which efficient use of energy would become an important factor.
Since 1983, Formula One had been dominated by specialist race teams like Williams, McLaren, and Benetton, using engines supplied by large car manufacturers like Mercedes-Benz, Honda, Renault, and Ford. Starting in 2000, with Ford's creation of the largely unsuccessful Jaguar team, new manufacturer-owned teams entered Formula One for the first time since the departure of Alfa Romeo and Renault at the end of 1985. By 2006, the manufacturer teams–Renault, BMW, Toyota, Honda, and Ferrari–dominated the championship, taking five of the first six places in the constructors' championship. The sole exception was McLaren, which at the time was part-owned by Mercedes Benz. Through the Grand Prix Manufacturers Association (GPMA) they negotiated a larger share of Formula One's commercial profit and a greater say in the running of the sport.
Manufacturers' decline and return of the privateers.
In 2008 and 2009, Honda, BMW, and Toyota all withdrew from Formula One racing within the space of a year, blaming the economic recession. This resulted in the end of manufacturer dominance within the sport. The Honda F1 team went through a management buyout to become Brawn GP with the notable F1 designer Ross Brawn and Nick Fry running and owning the majority of the organisation. Brawn GP went through a painful size reduction, laying off hundreds of employees, but eventually won the year's world championships with Jenson Button and Rubens Barrichello. BMW F1 was bought out by the original founder of the team Peter Sauber. The Lotus F1 Team are another, formerly manufacturer-owned team that has reverted to "privateer" ownership; with the buy-out of the Renault F1 Team, by Genii Capital investors in recent years. A link with their previous owners still survived however; with their car continuing to be powered by a Renault Power Unit until 2014.
McLaren also announced that it was to reacquire the shares in its team from Mercedes Benz (McLaren's partnership with Mercedes was reported to have started to sour with the McLaren Mercedes SLR road car project and tough F1 championships which included McLaren being found guilty of spying on Ferrari). Hence, during the 2010 season Mercedes Benz re-entered the sport as a manufacturer after its purchase of Brawn GP, and split with McLaren after 15 seasons with the team. This leaves Mercedes, McLaren, and Ferrari as the only car manufacturers in the sport, although both McLaren and Ferrari began as racing teams rather than manufacturers.
AT&T Williams confirmed towards the end of 2009 their new engine deal with Cosworth, who also supplied the wave of new teams Virgin Racing, Hispania Racing F1, and the newly formed Lotus Racing team. The exit of car manufacturers has also paved the way for teams representing their countries, with some having the funding by their respective national governments (such as Lotus being funded by Malaysia, Lotus Cars being owned by Proton, a Malaysian manufacturer, and Lotus Racing being run by Tony Fernandes, a Malaysian business man known for his Asian low-cost airline). Williams later rejoined with Renault in 2012, rekindling a partnership that dates back to the early to mid-1990s. However, the partnership was short lived and as of the 2014 F1 season, Williams compete with the Mercedes Power Unit.
A rule shake-up in 2014 meant Mercedes emerged as the dominant force, with Lewis Hamilton winning the championship closely followed by his main rival and team-mate, Nico Rosberg – the team winning 16 out of the 19 races that season (all other victories coming from Daniel Ricciardo of Red Bull). 2014 also saw a financial crisis which resulted in the backmarker Marussia and Caterham teams being put into administration, alongside the uncertain futures of Force India and Sauber. Marussia returned under the Manor name in 2015, a season in which Ferrari were the only challengers to Mercedes - with Vettel taking victory in the three Grands Prix Mercedes didn't win.
Political disputes.
FISA–FOCA war.
The battle for control of Formula One was contested between the Fédération Internationale du Sport Automobile (FISA), at the time an autonomous subcommittee of the FIA, and FOCA (the Formula One Constructors' Association).
The beginnings of the dispute are numerous, and many of the underlying reasons may be lost in history. The teams (excepting Ferrari and the other major manufacturers – Renault and Alfa Romeo in particular) were of the opinion that their rights and ability to compete against the larger and better funded teams were being negatively affected by a perceived bias on the part of the controlling organisation (FISA) toward the major manufacturers.
In addition, the battle revolved around the commercial aspects of the sport (the FOCA teams were unhappy with the disbursement of proceeds from the races) and the technical regulations which, in FOCA's opinion, tended to be malleable according to the nature of the transgressor more than the nature of the transgression.
The war culminated in a FOCA boycott of the 1982 San Marino Grand Prix months later. In theory, all FOCA teams were supposed to boycott the Grand Prix as a sign of solidarity and complaint at the handling of the regulations and financial compensation (and extreme opposition to the accession of Balestre to the position of FISA president: both Colin Chapman of Lotus and Frank Williams of Williams stated clearly that they would not continue in Formula One with Balestre as its governor). In practice, several of the FOCA teams backed out of the boycott, citing "sponsor obligations". Notable among these were the Tyrrell and Toleman teams.
FIA–FOTA dispute.
During the 2009 season of Formula One, the sport was gripped in a governance crisis. The FIA President Max Mosley proposed numerous cost cutting measures for the following season, including an optional budget cap for the teams; teams electing to take the budget cap would be granted greater technical freedom, adjustable front and rear wings and an engine not subject to a rev limiter. The Formula One Teams Association (FOTA) believed that allowing some teams to have such technical freedom would have created a 'two-tier' championship, and thus requested urgent talks with the FIA. However, talks broke down and FOTA teams announced, with the exception of Williams and Force India, that 'they had no choice' but to form a breakaway championship series.
On 24 June, an agreement was reached between Formula One's governing body and the teams to prevent a breakaway series. It was agreed teams must cut spending to the level of the early 1990s within two years; exact figures were not specified, and Max Mosley agreed he would not stand for re-election to the FIA presidency in October. Following further disagreements after Max Mosley suggested he would stand for re-election, FOTA made it clear that breakaway plans were still being pursued. On 8 July, FOTA issued a press release stating they had been informed they were not entered for the 2010 season, and an FIA press release said the FOTA representatives had walked out of the meeting. On 1 August, it was announced FIA and FOTA had signed a new Concorde Agreement, bringing an end to the crisis and securing the sport's future until 2012.
Outside the World Championship.
The terms "Formula One race" and "World Championship race" are effectively synonymous; since 1984, every Formula One race has counted towards an official FIA World Championship, and every World Championship race has been held to Formula One regulations. In the earlier history of Formula One, many races took place outside the world championship, and local championships run to Formula One regulations also occurred. These events often took place on circuits that were not suitable for the World Championship, and featured local cars and drivers as well as those competing in the Championship.
European non-championship racing.
In the early years of Formula One, before the world championship was established, there were around twenty races held from late Spring to early Autumn in Europe, although not all of these were considered significant. Most competitive cars came from Italy, particularly Alfa Romeo. After the start of the world championship, these non-championship races continued. In the 1950s and 1960s, there were many Formula One races which did not count for the World Championship; in a total of twenty-two Formula One races were held, of which only six counted towards the World Championship. In 1952 and 1953, when the world championship was run for Formula Two cars, non-championship events were the only Formula One races that took place.
Some races, particularly in the UK, including the Race of Champions, Oulton Park International Gold Cup and the International Trophy, were attended by the majority of the world championship contenders. Other smaller events were regularly held in locations not part of the championship, such as the Syracuse and Danish Grands Prix, although these only attracted a small amount of the championship teams and relied on private entries and lower Formula cars to make up the grid. These became less common through the 1970s and 1983 saw the last non-championship Formula One race; the 1983 Race of Champions at Brands Hatch, won by reigning World Champion Keke Rosberg in a Williams-Cosworth in a close fight with American Danny Sullivan.
South African Formula One championship.
South Africa's flourishing domestic Formula One championship ran from 1960 through to 1975. The frontrunning cars in the series were recently retired from the world championship although there was also a healthy selection of locally built or modified machines. Frontrunning drivers from the series usually contested their local World Championship Grand Prix, as well as occasional European events, although they had little success at that level.
British Formula One Series.
The DFV helped make the UK domestic Formula One series possible between 1978 and 1980. As in South Africa a decade before, second hand cars from manufacturers like Lotus and Fittipaldi Automotive were the order of the day, although some, such as the March 781, were built specifically for the series. In 1980, the series saw South African Desiré Wilson become the only woman to win a Formula One race when she triumphed at Brands Hatch in a Wolf WR3.
Racing and strategy.
A Formula One Grand Prix event spans a weekend. It begins with two free practice sessions on Friday (except in Monaco, where Friday practices are moved to Thursday), and one free practice on Saturday. Additional drivers (commonly known as third drivers) are allowed to run on Fridays, but only two cars may be used per team, requiring a race driver to give up his seat. A qualifying session is held after the last free practice session. This session determines the starting order for the race on Sunday.
Qualifying.
For much of the sport's history, qualifying sessions differed little from practice sessions; drivers would have one or more sessions in which to set their fastest time, with the grid order determined by each driver's best single lap, with the fastest on pole position. Grids were generally limited to 26 cars – if the race had more entries qualification would also decide which drivers would start the race. During the early 1990s, the number of entries was so high that the worst-performing teams had to enter a pre-qualifying session, with the fastest cars allowed through to the main qualifying session. The qualifying format began to change in the late 1990s, with the FIA experimenting with limiting the number of laps, determining the aggregate time over two sessions, and allowing each driver only one qualifying lap.
The current qualifying system was adopted in the 2006 season. Known as "knock-out" qualifying, it is split into three periods, known as Q1, Q2 and Q3. In each period, drivers run qualifying laps to attempt to advance to the next period, with the slowest drivers being "knocked out" at the end of the period and their grid positions set, based on their best lap times. Drivers are allowed as many laps as they wish within each period. After each period, all times are reset, and only a driver's fastest lap in that period (barring infractions) counts. Any timed lap started before the end of that period may be completed, and will count toward that driver's placement. The number of cars eliminated in each period is dependent on the total number of cars entered into the championship. Currently, with 20 cars, Q1 runs for 18 minutes, and eliminates the slowest five drivers. During this period, any driver whose best lap time exceeds 107% of the fastest time in Q1, will not be allowed to start the race without permission from the stewards. This rule does not affect drivers in Q2 or Q3. In Q2, the 15 remaining drivers have 15 minutes to set one of the ten fastest times and proceed to the next period. Finally, Q3 lasts 12 minutes and sees the remaining ten drivers decide the first ten grid positions.
Drivers may run any tyre compound throughout the qualifying session, and each car taking part in the final period receives an extra set of the 'option' (softer) tyre. All drivers must start the race on the tyre used Q2, unless the weather requires the use of wet-weather tyres. Any penalties that affect grid position are applied at the end of qualifying. Grid penalties can be applied for driving infractions in the previous or current Grand Prix, or for changing a gearbox or engine component. If a car fails scrutineering, the driver will be excluded from qualifying, but will be allowed to start the race from the back of the grid at the race steward's discretion.
Race.
The race begins with a warm-up lap, after which the cars assemble on the starting grid in the order they qualified. This lap is often referred to as the formation lap, as the cars lap in formation with no overtaking (although a driver who makes a mistake may regain lost ground provided he has not fallen to the back of the field). The warm-up lap allows drivers to check the condition of the track and their car, gives the tyres a chance to warm up to increase traction, and also gives the pit crews time to clear themselves and their equipment from the grid.
Once all the cars have formed on the grid, a light system above the track indicates the start of the race: five red lights are illuminated at intervals of one second; they are all then extinguished simultaneously after an unspecified time (typically less than 3 seconds) to signal the start of the race. The start procedure may be abandoned if a driver stalls on the grid, signalled by raising his arm. If this happens the procedure restarts: a new formation lap begins with the offending car removed from the grid. The race may also be restarted in the event of a serious accident or dangerous conditions, with the original start voided. The race may be started from behind the Safety Car if officials feel a racing start would be excessively dangerous, such as extremely heavy rainfall. There is no formation lap when races start behind the Safety Car.
Under normal circumstances the winner of the race is the first driver to cross the finish line having completed a set number of laps, which added together should give a distance of approximately Monaco]. Race officials may end the race early (putting out a red flag) due to unsafe conditions such as extreme rainfall, and it must finish within two hours, although races are only likely to last this long in the case of extreme weather or if the safety car is deployed during the race. Drivers may overtake one another for position over the course of the race and are 'Classified' in the order they finished 90% of the race distance. If a leader comes across a back marker (slower car) who has completed fewer laps, the back marker is shown a blue flag telling him he is obliged to allow the leader to overtake him. The slower car is said to be 'lapped' and, once the leader finishes the race, is classified as finishing the race 'one lap down'. A driver can be lapped numerous times, by any car in front of him. A driver who fails to finish a race, through mechanical problems, accident, or any other reason is said to have retired from the race and is 'Not Classified' in the results. However, if the driver has completed more than 90% of the race distance, he will be classified.
Throughout the race drivers may make pit stops to change tyres and repair damage (from 1994 to 2009 inclusive they could also refuel). Different teams and drivers employ different pit stop strategies in order to maximise their car's potential. Two tyre compounds, with different durability and adhesion characteristics, are available to drivers. Over the course of a race, drivers must use both. One compound will have a performance advantage over the other, and choosing when to use which compound is a key tactical decision to make. The prime and option tyres have different colours on their sidewalls; this allows spectators to understand the strategies. Under wet conditions drivers may switch to one of two specialised wet weather tyres with additional grooves (one "intermediate", for mild wet conditions, such as after recent rain, one "full wet", for racing in or immediately after rain). A driver must make at least one stop to use both tyre compounds; up to three stops are typically made, although further stops may be necessary to fix damage or if weather conditions change. If rain tyres are used, drivers are no longer obliged to use both types of dry tyres.
The format of the race has changed little through Formula One's history. The main changes have revolved around what is allowed at pit stops. In the early days of Grand Prix racing, a driver would be allowed to continue a race in his teammate's car should his develop a problem—in the modern era cars are so carefully fitted to drivers that this has become impossible. In recent years, the emphasis has been on changing refuelling and tyre change regulations. From the 2010 season, refuelling—which was reintroduced in 1994—has not been allowed, to encourage less tactical racing following safety concerns. The rule requiring both compounds of tyre to be used during the race was introduced in 2007, again to encourage racing on the track. The safety car is another relatively recent innovation that reduced the need to deploy the red flag, allowing races to be completed on time for a growing international live television audience.
Points system.
Various systems for awarding championship points have been used since 1950. The current system, in place since 2010, awards the top ten cars points in the Drivers and Constructors Championships, with the winner receiving 25 points. If both a team's cars finish in the points, they both receive Constructors Championship points. The total number of points won at each race are added up, and the driver and constructor with the most points at the end of the season are World Champions. A driver can switch teams during the season and, for the Drivers Championship, keep all points gained at the previous team.
A driver must be classified to receive points. In order to be classified, a driver need not finish the race, but complete at least 90% of the winner's race distance. Therefore, it is possible for a driver to receive points even if they retired before the end of the race.
In the event that less than 75% of the race laps are completed by the winner, only half of the points listed in the table are awarded to the drivers and constructors. This has happened on only five occasions in the history of the championship, and it has decided the championship winner on one occasion. The last occurrence was at the 2009 Malaysian Grand Prix when the race was called off after 31 laps due to torrential rain.
Constructors.
Since 1981, Formula One teams have been required to build the chassis in which they compete, and consequently the terms "team" and "constructor" became more or less interchangeable. This requirement distinguishes the sport from series such as the IndyCar Series which allows teams to purchase chassis, and "spec series" such as GP2, which require all cars be kept to an identical specification. It also effectively prohibits privateers, which were common even in Formula One well into the 1970s.
The sport's debut season, 1950, saw eighteen teams compete, but due to high costs many dropped out quickly. In fact, such was the scarcity of competitive cars for much of the first decade of Formula One that Formula Two cars were admitted to fill the grids. Ferrari is the oldest Formula One team, the only still-active team which competed in 1950.
Early manufacturer involvement came in the form of a "factory team" or "works team" (that is, one owned and staffed by a major car company), such as those of Alfa Romeo, Ferrari, or Renault. After having virtually disappeared by the early 1980s, factory teams made a comeback in the 1990s and 2000s and formed up to half the grid with Ferrari, Jaguar, BMW, Renault, Toyota, and Honda either setting up their own teams or buying out existing ones. Mercedes-Benz owned 40% of the McLaren team and manufactures the team's engines. Factory teams make up the top competitive teams; in 2008 wholly owned factory teams took four of the top five positions in the Constructors' Championship, and McLaren the other. Ferrari holds the record for having won the most Constructors' Championships (sixteen). However, by the end of the 2000s factory teams were once again on the decline with only Ferrari, Mercedes-Benz and Renault lodging entries to the 2010 championship.
Companies such as Climax, Repco, Cosworth, Hart, Judd and Supertec, which had no direct team affiliation, often sold engines to teams that could not afford to manufacture them. In the early years, independently owned Formula One teams sometimes also built their engines, though this became less common with the increased involvement of major car manufacturers such as BMW, Ferrari, Honda, Mercedes-Benz, Renault, and Toyota, whose large budgets rendered privately built engines less competitive. Cosworth was the last independent engine supplier. Beginning in 2007, the manufacturers' deep pockets and engineering ability took over, eliminating the last of the independent engine manufacturers. It is estimated the major teams spend between €100 and €200 million ($125–$225 million) "per" year" per" manufacturer on engines alone.
In the 2007 season, for the first time since the 1981 rule, two teams used chassis built by other teams. Super Aguri started the season using a modified Honda Racing RA106 chassis (used by Honda the previous year), while Scuderia Toro Rosso used the same chassis used by the parent Red Bull Racing team, which was formally designed by a separate subsidiary. The usage of these loopholes was ended for 2010 with the publication of new technical regulations, which require each constructor to own the intellectual property rights to their chassis, which prevents a team using a chassis owned by another Formula One constructor. The regulations continue to allow a team to subcontract the design and construction of the chassis to a third-party, an option used by the HRT team in 2010.
Although teams rarely disclose information about their budgets, it is estimated they range from US$66 million to US$400 million each.
Entering a new team in the Formula One World Championship requires a £25 million (about US$47 million) up-front payment to the FIA, which is then repaid to the team over the course of the season. As a consequence, constructors desiring to enter Formula One often prefer to buy an existing team: B.A.R.'s purchase of Tyrrell and Midland's purchase of Jordan allowed both of these teams to sidestep the large deposit and secure the benefits the team already had, such as TV revenue.
Drivers.
Every team in Formula One must run two cars in every session in a Grand Prix weekend, and every team may use up to four drivers in a season. A team may also run two additional drivers in Free Practice sessions, which are often used to test potential new drivers for a career as a Formula One driver or gain experienced drivers to evaluate the car. Most modern drivers are contracted for at least the duration of a season, with driver changes taking place in between seasons, in comparison to early years where drivers often competed at an ad hoc basis from race to race. Each competitor must be in the possession of a FIA Super Licence to compete in a Grand Prix, which is issued to drivers who have met the criteria of success in junior motorsport categories and having achieved of running in a Formula One car. Drivers may also be issued a Super License by the World Motor Sport Council if they fail to meet the criteria. Teams also contract test and reserve drivers, to stand in for regular drivers when necessary and develop the team's car; although with the reduction on testing the reserve drivers' role mainly takes places on a simulator, such as rFactor Pro, which is used by most of the F1 Teams. Although most drivers earn their seat on ability, commercial considerations also come into play with teams having to satisfy sponsors and financial demands.
Each driver chooses an unassigned number from 2–99 (excluding 17) upon entering Formula One, and keeps that number during their time in the series. The number one is reserved for the reigning driver's champion, who retains their previous number and may choose to use it instead of the number one. At the onset of the championship, numbers were allocated by race organisers on an ad-hoc basis from race to race, and competitors did not have a permanent number throughout the season. Permanent numbers were introduced in , when teams were allocated numbers in ascending order based upon the constructors standings. The teams would hold those numbers from season to season with the exception of the team with the world drivers champion, which would swap its numbers with the one and two of the previous champion's team. New entrants were allocated spare numbers, with the exception of the number 13 which had been unused since . As teams kept their numbers for long periods of time car numbers became associated with a team, such as Ferrari's 27 and 28. A different system was used from to . At the start of each season, the current drivers champion was designated number one, his team-mate number two, and the rest of the teams assigned ascending numbers according to previous season's constructors' championship order.
A total of 32 separate drivers have won the world championship, with Michael Schumacher holding the record for most championships with seven, as well as holding the race wins and pole position records. Juan Manuel Fangio has won the next most, with five championships won during the 1950s, as well as having won the greatest percentage of wins, with 24 out of 52 entries. Jochen Rindt is the only posthumous World Champion, after his points total was not overhauled despite his fatal accident at the 1970 Italian Grand Prix. Drivers from the United Kingdom have been the most successful in the sport, with 14 championships from 10 drivers, and 214 wins from 19.
Feeder series.
Most F1 drivers start in kart racing competitions, and then come up through traditional European single seater series like Formula Ford and Formula Renault to Formula 3, and finally the GP2 Series. GP2 started in 2005, replacing Formula 3000, which itself had replaced Formula Two as the last major stepping-stone into F1. Most champions from this level graduate into F1, but 2006 GP2 champion Lewis Hamilton became the first F2, F3000 or GP2 champion to win the Formula One driver's title in 2008. Drivers are not required to have competed at this level before entering Formula One. British F3 has supplied many F1 drivers, with champions including Nigel Mansell, Ayrton Senna and Mika Häkkinen having moved straight from that series to Formula One. More rarely a driver may be picked from an even lower level, as was the case with 2007 World Champion Kimi Räikkönen, who went straight from Formula Renault to F1.
American Championship Car Racing has also contributed to the Formula One grid with mixed results. CART Champions Mario Andretti and Jacques Villeneuve became F1 World Champions, while Juan Pablo Montoya won seven races in F1. Other CART (also known as ChampCar) Champions, like Michael Andretti and Alessandro Zanardi won no races in F1. Other drivers have taken different paths to F1; Damon Hill raced motorbikes, and Michael Schumacher raced in sports cars, albeit after climbing through the junior single seater ranks. Former F1 driver Paul di Resta raced in DTM until he was signed with Force India in 2011. To race, however, the driver must hold an FIA Super Licence–ensuring that the driver has the requisite skills, and will not therefore be a danger to others. Some drivers have not had the license when first signed to a F1 team; Räikkönen received the license despite having only 23 car races to his credit.
Beyond F1.
Most F1 drivers retire in their mid to late 30s; however, many keep racing in disciplines which are less physically demanding. The German touring car championship, the DTM, is a popular category involving ex-drivers such as two-time champion Mika Häkkinen and F1 race winners Jean Alesi, David Coulthard and Ralf Schumacher. In recent years, it has become common for former drivers who have had shorter careers to take up factory seats driving LMP1 cars in the FIA World Endurance Championship, with notable drivers including Mark Webber, Allan McNish, Anthony Davidson, Alexander Wurz, and Sébastien Buemi. Some F1 drivers have left to race in the United States—Nigel Mansell and Emerson Fittipaldi duelled for the 1993 CART title, Rubens Barrichello moved to IndyCar in 2012, while Jacques Villeneuve, Juan Pablo Montoya, Nelson Piquet, Jr. and Scott Speed moved to NASCAR. Some drivers, such as Vitantonio Liuzzi, Narain Karthikeyan and Jos Verstappen went on to race in the A1 Grand Prix series. During its existence from 2008 to 2011, Superleague Formula attracted ex-Formula One drivers like Sébastien Bourdais, Antônio Pizzonia and Giorgio Pantano. A series for former Formula One drivers, called Grand Prix Masters, ran briefly in 2005 and 2006. Others, like Jackie Stewart, Gerhard Berger and Alain Prost, returned to F1 as team owners while their former competitors have become colour commentators for TV coverage such as James Hunt (BBC), Martin Brundle (BBC, ITV and Sky), David Hobbs (NBC), Alan Jones (BBC, Nine Network and Ten Network) David Coulthard (BBC), Luciano Burti for Globo (Brazil), and Jean Alesi for Italian national network RAI. Others, such as Damon Hill and Jackie Stewart, take active roles in running motorsport in their own countries. Carlos Reutemann became a politician and served as governor of his native state in Argentina.
Grands Prix.
The number of Grands Prix held in a season has varied over the years. The inaugural world championship season comprised only seven races, while the season contained nineteen races. Although throughout the first decades of the world championship there were no more than eleven Grands Prix a season, a large number of non-championship Formula One events also took place. The number of Grands Prix increased to an average of sixteen/seventeen by the late 1970s; simultaneously non-championship events ended by 1983. More Grands Prix began to be held in the 2000s, and recent seasons have seen an average of 19 races. In the calendar peaked at twenty events, which remains the highest number of world championship races in one season.
Six of the original seven races took place in Europe; the only non-European race that counted towards the World Championship in 1950 was the Indianapolis 500, which was held to different regulations and later replaced by the United States Grand Prix. The F1 championship gradually expanded to other non-European countries. Argentina hosted the first South American Grand Prix in 1953, and Morocco hosted the first African World Championship race in . Asia (Japan in ) and Oceania (Australia in ) followed, and the first race in the Middle East was held in . The nineteen races of the 2014 Formula One season are spread over every populated continent except for Africa, with ten Grands Prix held outside Europe.
Some of the Grands Prix, such as the oldest recognised event the French Grand Prix, pre-date the formation of the World Championship and were incorporated into the championship as Formula One races in 1950. The British and Italian Grands Prix are the only events to have been held every Formula One season; other long-running races include the Belgian, German and currently defunct French Grands Prix. The Monaco Grand Prix, first held in 1929 and run continuously since 1955, is widely considered to be one of the most important and prestigious automobile races in the world.
Traditionally each nation has hosted a single Grand Prix, which carries the name of the country. If a single country hosts multiple Grands Prix in a year they receive different names. In European countries the second event has often been titled the European Grand Prix, or named after a neighbouring state without a race. The United States has held six separate Grands Prix, including the Indianapolis 500, with the additional events named after the host city. Grands Prix are not always held at the same circuit each year, and may switch locations due to the suitability of the track or the financial status of the race organisers. The German Grand Prix currently alternates between the Nürburgring and Hockenheimring circuits, and others such as the American and French races have switched venues throughout their history.
All Grands Prix have traditionally been run during the day, until the inaugural hosted the first Formula One night race, which was followed in 2009 by the day–night Abu Dhabi Grand Prix. Along with holding races at night, other Grands Prix in Asia have had their start times adjusted to benefit the European television audience.
Recent additions to the calendar include the Singapore Grand Prix which, in September 2008, hosted the first night race ever held in Formula One, the Abu Dhabi Grand Prix, which hosted the first day-to-night race in November 2009, the Korean Grand Prix, first held in October 2010 and the Indian Grand Prix, first held in October 2011. The United States Grand Prix held its first race in Austin, Texas at the new Circuit of the Americas in 2012. The first F1 Russian Grand Prix was held in 2014 at the new Sochi circuit, that runs around a venue used for the 2014 Winter Olympics.
Circuits.
A typical circuit usually features a stretch of straight road on which the starting grid is situated. The "pit lane", where the drivers stop for tyres and minor repairs (such as changing the car's nose due to front wing damage) during the race, retirements from the race, and where the teams work on the cars before the race, is normally located next to the starting grid. The layout of the rest of the circuit varies widely, although in most cases the circuit runs in a clockwise direction. Those few circuits that run anticlockwise (and therefore have predominantly left-handed corners) can cause drivers neck problems due to the enormous lateral forces generated by F1 cars pulling their heads in the opposite direction to normal.
Most of the circuits currently in use are specially constructed for competition. The current street circuits are Monaco, Melbourne, Montreal, Singapore and Sochi, although races in other urban locations come and go (Las Vegas and Detroit, for example) and proposals for such races are often discussed—most recently New Jersey. Several circuits have been completely laid out on public roads in the past, such as Valencia in Spain, though Monaco is the only one that remains. The glamour and history of the Monaco race are the primary reasons why the circuit is still in use, even though it does not to meet the strict safety requirements imposed on other tracks. Three-time World champion Nelson Piquet famously described racing in Monaco as "like riding a bicycle around your living room".
Circuit design to protect the safety of drivers is becoming increasingly sophisticated, as exemplified by the new Bahrain International Circuit, added in and designed—like most of F1's new circuits—by Hermann Tilke. Several of the new circuits in F1, especially those designed by Tilke, have been criticised as lacking the "flow" of such classics as Spa-Francorchamps and Imola. His redesign of the Hockenheim circuit in Germany for example, while providing more capacity for grandstands and eliminating extremely long and dangerous straights, has been frowned upon by many who argue that part of the character of the Hockenheim circuits was the long and blinding straights into dark forest sections. These newer circuits, however, are generally agreed to meet the safety standards of modern Formula One better than the older ones.
The newest additions to the calendar are the Sochi Autodrom and the Autodromo Hermanos Rodriguez, which is returning after 23 years on the sidelines.
A single race requires hotel rooms to accommodate at least 5000 visitors.
Cars and technology.
Modern Formula One cars are mid-engined open cockpit, open wheel single-seaters. The chassis is made largely of carbon-fibre composites, rendering it light but extremely stiff and strong. The whole car, including engine, fluids and driver, weighs only – the minimum weight set by the regulations. If the construction of the car is lighter than the minimum, it can be ballasted up to add the necessary weight. The race teams take advantage of this by placing this ballast at the extreme bottom of the chassis, thereby locating the centre of gravity as low as possible in order to improve handling and weight transfer.
The cornering speed of Formula One cars is largely determined by the aerodynamic downforce that they generate, which pushes the car down onto the track. This is provided by "wings" mounted at the front and rear of the vehicle, and by ground effect created by low air pressure under the flat bottom of the car. The aerodynamic design of the cars is very heavily constrained to limit performance and the current generation of cars sport a large number of small winglets, "barge boards", and turning vanes designed to closely control the flow of the air over, under, and around the car.
The other major factor controlling the cornering speed of the cars is the design of the tyres. From to , the tyres in Formula One were not "slicks" (tyres with no tread pattern) as in most other circuit racing series. Instead, each tyre had four large circumferential grooves on its surface designed to limit the cornering speed of the cars. Slick tyres returned to Formula One in the season. Suspension is double wishbone or multilink front and rear, with pushrod operated springs and dampers on the chassis – one exception being that of the 2009 specification Red Bull Racing car (RB5) which used pullrod suspension at the rear, the first car to do so since the Minardi PS01 in 2001. Ferrari used a pullrod suspension at both the front and rear in their 2012 car. Both Ferrari (F138) and McLaren (MP4-28) of the 2013 season used a pullrod suspension at both the front and the rear.
Carbon-carbon disc brakes are used for reduced weight and increased frictional performance. These provide a very high level of braking performance and are usually the element which provokes the greatest reaction from drivers new to the formula.
Starting with the 2014 Formula 1 season the engines have changed from a 2.4-litre naturally aspirated V8 to turbocharged 1.6 litre V6 "power-units". These get a significant amount of their power from electric motors. In addition they include a lot of energy recovery technology. Engines run on unleaded fuel closely resembling publicly available petrol. The oil which lubricates and protects the engine from overheating is very similar in viscosity to water. The 2006 generation of engines spun up to 20,000 RPM and produced up to . For , engines were restricted to 19,000 rpm with limited development areas allowed, following the engine specification freeze from the end of . For the 2009 Formula One season the engines were further restricted to 18,000 rpm.
A wide variety of technologies—including active suspension and ground effect aerodynamics —are banned under the current regulations. Despite this the current generation of cars can reach speeds in excess of at some circuits. The highest straight line speed recorded during a Grand Prix was , set by Juan Pablo Montoya during the 2005 Italian Grand Prix. A Honda Formula One car, running with minimum downforce on a runway in the Mojave desert achieved a top speed of in 2006. According to Honda, the car fully met the FIA Formula One regulations. Even with the limitations on aerodynamics, at aerodynamically generated downforce is equal to the weight of the car, and the oft-repeated claim that Formula One cars create enough downforce to "drive on the ceiling", while possible in principle, has never been put to the test. Downforce of 2.5 times the car's weight can be achieved at full speed. The downforce means that the cars can achieve a lateral force with a magnitude of up to 3.5 times that of the force of gravity (3.5g) in cornering. Consequently, the driver's head is pulled sideways with a force equivalent to the weight of 20 kg in corners. Such high lateral forces are enough to make breathing difficult and the drivers need supreme concentration and fitness to maintain their focus for the one to two hours that it takes to complete the race. A high-performance road car like the Ferrari Enzo only achieves around 1g. 
As of 2015, each team may have no more than two cars available for use at any time. Each driver may use no more than four engines during a championship season unless he drives for more than one team. If more engines are used, he drops ten places on the starting grid of the event at which an additional engine is used. The only exception is where the engine is provided by a manufacturer or supplier taking part in its first championship season, in which case up to five may be used by a driver. Each driver may use no more than one gearbox for six consecutive events; every unscheduled gearbox change requires the driver to drop five places on the grid unless he failed to finish the previous race due to reasons beyond the team's control.
Revenue and profits.
In March 2007, "F1 Racing" published its annual estimates of spending by Formula One teams. The total spending of all eleven teams in 2006 was estimated at $2.9 billion US. This was broken down as follows: Toyota $418.5 million, Ferrari $406.5 m, McLaren $402 m, Honda $380.5 m, BMW Sauber $355 m, Renault $324 m, Red Bull $252 m, Williams $195.5 m, Midland F1/Spyker-MF1 $120 m, Toro Rosso $75 m, and Super Aguri $57 million.
Costs vary greatly from team to team. Honda, Toyota, McLaren-Mercedes, and Ferrari are estimated to have spent approximately $200 million on engines in 2006, Renault spent approximately $125 million and Cosworth's 2006 V8 was developed for $15 million. In contrast to the 2006 season on which these figures are based, the 2007 sporting regulations ban all performance related engine development.
Formula One teams pay entry fees of $500,000, plus $5,000 per point scored the previous year or $6,000 per point for the winner of the constructors' championship. Formula One drivers pay a Superlicense fee of $10,000 plus $1,000 per point.
There have been controversies with the way profits are shared amongst the teams. The smaller teams have complained that the profits are unevenly shared favoring established top teams. In September 2015, Force India and Sauber officially lodged a complaint with the European Union against Formula One questioning the governance and stating that the system of dividing revenues and determining the rules is unfair and unlawful.
The cost of building a brand new permanent circuit can be up to hundreds of millions of dollars, while the cost of converting a public road, such as Albert Park, into a temporary circuit is much less. Permanent circuits, however, can generate revenue all year round from leasing the track for private races and other races, such as MotoGP. The Shanghai International Circuit cost over $300 million and the Istanbul Park circuit cost $150 million to build.
Formula One drivers earn the highest salary of any drivers in auto racing. The highest paid driver in 2010 was Fernando Alonso, who received $40 million in salary from Ferrari—a record for any driver. The very top Formula One drivers get paid more than IndyCar or NASCAR drivers. Most top IndyCar drivers are paid around a tenth of their Formula One counterparts.
Future.
The expense of Formula One has seen the FIA and the Formula One Commission attempt to create new regulations to lower the costs for a team to compete in the sport. Cost-saving proposals have included allowing customer cars, either by teams purchasing a car from another constructor, or the series supplying a basic chassis and engine to some teams at a low cost. Allowing teams to share more car components such as the monocoque and safety components is also under consideration. The FIA also continually researches new ways to increase safety in the sport, which includes introducing new regulations and accident procedures.
In the interest of making the sport truer to its role as a World Championship, Bernie Ecclestone has initiated and organised a number of Grands Prix in new countries. Proposals to hold future races are regularly made by both new locations and countries and circuits that have previously hosted a Formula One Grand Prix. One new race is currently planned for 2016, the Baku European Grand Prix in Azerbaijan.
An enhanced rule package for 2017 is currently being considered by Formula One. The introduction of 1000 bhp engines, achieved by reducing limitations on the current 1.6L turbocharged V6 power units, is currently planned in order to increase the difficulty of driving a Formula One car. Increasing the width of the cars to 2000mm, which was last seen in , is another proposal set to be introduced. A tender for a new tyre supplier contract, currently held by Pirelli, will be held before the 2017 season. The new contract may see the introduction of lower profile tyres, with the wheel size potentially increasing from 13-inch to 18 or 21 inches.
Media coverage.
Formula One can be seen live or tape delayed in almost every country and territory around the world and attracts one of the largest global television audiences. The 2008 season attracted a global audience of 600 million people per race. It is a massive television event; the cumulative television audience was calculated to be 54 billion for the 2001 season, broadcast to 200 territories.
During the early 2000s, Formula One Group created a number of trademarks, an official logo, and an official website for the sport in an attempt to give it a corporate identity. Ecclestone experimented with a digital television package (known colloquially as Bernievision) which was launched at the 1996 German Grand Prix in cooperation with German digital television service "DF1", 30 years after the first GP colour TV broadcast, the 1967 German Grand Prix. This service offered the viewer several simultaneous feeds (such as super signal, on board, top of field, backfield, highlights, pit lane, timing) which were produced with cameras, technical equipment and staff different from those used for the conventional coverage. It was introduced in many countries over the years, but was shut down after the 2002 season for financial reasons.
TV stations all take what is known as the "World Feed", either produced by the FOM (Formula One Management) or occasionally, the "host broadcaster". The only station that originally differed from this
was "Premiere"—a German channel which offers all sessions live and interactive, with features such as the onboard channel. This service was more widely available around Europe until the end of 2002, when the cost of a whole different feed for the digital interactive services was thought too much. This was in large part because of the failure of the "F1 Digital +" Channel launched through Sky in the United Kingdom. Prices were too high for viewers, considering they could watch both the qualifying and the races themselves free on ITV.
However, upon the commencement of its coverage for the 2009 season, the BBC reintroduced complementary features such as the "red button" in-car camera angles, multiple soundtracks (broadcast commentary, CBBC commentary for children, or ambient sound only) and a rolling highlights package. Different combinations of these features are available across the various digital platforms (Freeview, Freesat, Sky, Virgin Media cable and the BBC F1 web site) prior to, during, and after the race weekend. Not all services are available across all the various platforms due to technical constraints. The BBC also broadcasts a post-race programme called "F1 Forum" on the digital terrestrial platforms' "red button" interactive services.
An announcement made on 12 January 2011, on the official Formula 1 website, announced that F1 would adopt the HD format for the 2011 season offering a world feed at a data rate of 42 Megabits/second (MPEG-2). The BBC subsequently announced later that day that their 2011 F1 coverage would be broadcast in HD which has been made immediately possible due to SIS LIVE, the provider of the BBC's F1 outside broadcast coverage, having already upgraded their technical facilities to HD as of the 2010 Belgian Grand Prix.
It was announced on 29 July 2011 that Sky Sports and the BBC would team up to show the races in F1 in 2012. In March 2012, Sky launched a channel dedicated to F1, with an HD counterpart. Sky Sports F1 covered all races live without commercial interruption as well as live practice and qualifying sessions, along with F1 programming, including interviews, archive action and magazine shows. The deal secured Formula 1 on Sky up to 2018. The BBC in 2012 featured live coverage of half of the races in the season: China, Spain, Monaco, Europe, Britain, Belgium, Singapore, Korea, Abu Dhabi, and Brazil. The BBC also showed live coverage of practice and qualifying sessions from those races. For the races that the BBC did not show live, "extended highlights" of the race were available a few hours after the live broadcast.
BBC ended their joint television contract after the 2015 season, transferring their rights to Channel 4 until the end of 2018 season. Sky Sports F1 coverage wiil remain unaffected and BBC Radio 5 Live and 5 Live Sports Extra will be extended until the 2021 season.
Formula One has an extensive web following, with most major TV companies covering it such as the BBC. The official Formula One website (formula1.com) has a live timing Java applet that can be used during the race to keep up with the leaderboard in real time. Recently an official application has been made available in the iTunes App Store that allows iPhone / iPod Touch users to see a real time feed of driver positions, timing and commentary. The same official application has been available for Android phones and tablets since 2011.
To accommodate fans who were unable to video the races on live television, Formula One Management's in-house production team began producing exclusive race edits synchronized to music from some of the world's top artists.
Distinction between Formula One and World Championship races.
Currently the terms "Formula One race" and "World Championship race" are effectively synonymous; since 1984, every Formula One race has counted towards the World Championship, and every World Championship race has been to Formula One regulations. But the two terms are not interchangeable.
The distinction is most relevant when considering career summaries and "all time lists". For example, in the List of Formula One drivers, Clemente Biondetti is shown with 1 race against his name. Biondetti actually competed in "four" Formula One races in 1950, but only one of these counted for the World Championship. Similarly, several Indy 500 winners technically won their first World Championship race, though most record books choose to ignore this and instead only record regular participants.

</doc>
<doc id="10855" url="https://en.wikipedia.org/wiki?curid=10855" title="Franco Baresi">
Franco Baresi

Franco Baresi (; born 8 May 1960 in Travagliato, Italy) is an Italian football youth team coach and a former player and manager. He played as a sweeper and as a central defender, and spent his entire 20-year career with Serie A club A.C. Milan, captaining the club for 15 seasons. He is considered one of the greatest defenders of all time, and was ranked 19th in World Soccer's list of the 100 greatest players of the twentieth century. He won the Champions League 3 times, as well as 6 Serie A titles, 4 Supercoppa Italiana titles, 3 European Super Cups and 2 Intercontinental Cups.
With Italy, he won the 1982 FIFA World Cup. He also played in the 1990 FIFA World Cup where he was named in the FIFA World Cup All-Star Team, finishing third in the competition. At the 1994 FIFA World Cup he was named Italy's captain and was an integral part of the team that reached the final, although he would miss a penalty in the resulting shoot-out, as Brazil lifted the trophy. Baresi also represented Italy at two UEFA European Championships, in 1980 and 1988, and at the 1984 Olympics, reaching the semi-finals on each occasion.
The younger brother of former footballer Giuseppe Baresi, after joining the Milan senior team as a youngster, Franco Baresi was initially nicknamed "Piscinin", Milanese for "Little one"; due to his skill and success, he was later known as "Kaiser Franz", a reference to fellow legendary sweeper Franz Beckenbauer. In 1999, he was voted Milan's Player of the Century. After his final season at Milan in 1997, the club retired Baresi's shirt number 6. He was named by Pelé one of the 125 Greatest Living Footballers at the FIFA centenary awards ceremony in 2004. Baresi was inducted into the Italian Football Hall of Fame in 2013.
Club career.
Originally a Milan youth product, Baresi went on to spend his entire twenty year professional career with Milan, making his Serie A debut at the age of 17, during the 1977–78 season on 23 April 1978. He had initially been rejected by Inter, who chose his brother Giuseppe instead, while Milan signed Franco Baresi. The following season, he was made a member of the starting eleven, playing as a sweeper or as a centreback, winning the 1978–79 Serie A title, Milan's tenth overall, playing alongside Fabio Capello, and also Gianni Rivera, in what would be his last season at the club. This success was soon followed by a dark period in the club's history, when Milan was relegated to Serie B twice during the early 1980s. Milan were relegated in 1980 for being involved in the match fixing scandal of 1980, and once again after finishing third-last in the 1981–82 season, after having just returned to Serie A the previous season, after winning the 1980–81 Serie B title. Despite being a member of the Euro 1980 Italy squad that had finished fourth, and the 1982 World Cup winning team, Baresi elected to stay with Milan, winning the Serie B title for the second time during the 1982–83 season, and bringing Milan back to Serie A. After Aldo Maldera and Fulvio Collovati left the club in 1982, Baresi was appointed Milan's captain, at the age of 22, and would hold this position for much of his time at the club, becoming a symbol and a leader for the team. During this temporary bleak period for Milan, Baresi did manage to win a Mitropa Cup in 1982, and reached the Coppa Italia final during 1984–85 season, although the team failed to dominate in Serie A.
During the end of the 1980s and the first half of the 1990s, Baresi was at the heart of a formidable all-Italian defence alongside Paolo Maldini, Alessandro Costacurta, Mauro Tassotti, and later, Christian Panucci, under managers Arrigo Sacchi and Fabio Capello, a defence which is regarded by many as the greatest of all time. When the attacking Dutch trio of Marco van Basten, Ruud Gullit and Frank Rijkaard arrived at the club in the late 1980s, Milan began a period of domestic and international domination, and between 1987 and 1996, at the height of the club's success, the Milan squad contained many Italian and international stars, such as Roberto Donadoni, Carlo Ancelotti, Marco van Basten, Ruud Gullit, Frank Rijkaard, and later, Demetrio Albertini, Dejan Savićević, Zvonimir Boban, Marcel Desailly, George Weah, Jean-Pierre Papin, Brian Laudrup, and Roberto Baggio. Under Sacchi, Milan won the Serie A title in 1987–88, with Baresi helping Milan to concede only 14 goals. This title was immediately followed by an Italian Supercup in 1988 the next season, and back to back European Cups in 1988–89 and 1989–90. Baresi was also runner-up to team mate Van Basten for the Ballon d'Or in 1989, finishing ahead of his other team mate Frank Rijkaard, and was named Serie A Footballer of the Year in 1989–90. Milan also reached the Coppa Italia final during the 1989–90 season.
Baresi went on to win four more Serie A titles with Milan under Fabio Capello, including three consecutive titles, during the 1991–92, 1992–93 and the 1993–94 seasons. Baresi helped Milan win the 1991–92 title undefeated, helping Milan to go unbeaten for an Italian record of 58 matches. Milan also scored a record 74 goals that season. During the 1993–94 season, Baresi helped Milan concede a mere 15 goals in Serie A, helping the club to finish the season with the best defence. Baresi also won three consecutive Italian Supercups under Capello, in 1992, 1993, and 1994. Milan also reached three consecutive UEFA Champions League Finals during the 1992–93, 1993–94 and 1994–95 seasons, losing out to Marseille in the 1992–93 tournament, and Ajax in the 1994–95 tournament. Baresi won the third European Cup (UEFA Champions League) of his career in 1993–94 where, Milan defeated Johan Cruyff's 'Dream Team' FC Barcelona 4–0 in the final. Baresi also managed to win the 1994 European Supercup, although Milan were defeated in the 1994 Intercontinental Cup, the 1993 UEFA Super Cup and the 1993 Intercontinental Cup. Under Capello, Milan and Baresi were able to capture another Serie A title during 1995–96 season, Baresi's sixth title in total.
Baresi retired at the end of the 1996–97 Serie A season, at the age of 37. In his 20 seasons with Milan, he won six Serie A titles, three Champions League titles (reaching five finals in total), two Intercontinental Cups (four finals in total), four European Supercups (five finals in total), four Italian Supercups (five finals in total), two Serie B titles, and a Mitropa Cup. He scored 31 goals for Milan, 21 of which were on penalties, and, despite being a defender, he was the top scorer of the Coppa Italia during the 1989–90 season, the only trophy which he failed to win with Milan, reaching the final twice during his career. His final goal for Milan was scored in a 2–1 win against Padova on 27 August 1995. In his honour, Milan retired his number 6 shirt, which he had worn throughout his career. The captain's armband, which he had worn for 15 seasons, was handed over to Paolo Maldini. Milan organised a celebration match in his honour, which was played on 28 October 1997 at the San Siro stadium, featuring many footballing stars.
International career.
At the age of 20, whilst still playing in the Italy Under-21 side, Baresi was named in Italy's 22-man squad for the 1980 European Championship along with his older brother Giuseppe, by manager Enzo Bearzot. The tournament was held on home soil, and Italy went on to finish fourth, although, unlike his brother, Franco Baresi did not play a single match in the tournament. Euro 1980 would be the only time that the two brothers were on the Italy squad together at a major tournament. At the age of 22, Baresi was named in Italy's squad for the 1982 FIFA World Cup. The "Azzurri" won their third World Cup, beating West Germany in the final, but Baresi, once again, was not selected to play a match throughout the tournament. Baresi was also a member of the Italy squad that took part in the 1984 Olympics. Italy finished in fourth place after a semi-final defeat to Brazil, and losing the bronze medal match to Yugoslavia. Baresi scored a goal against the USA during the group stage. He won his first senior international cap in a 1984 UEFA European Championship qualifying match against Romania in Florence, on 14 December 1982, which ended 0–0. Italy, however, ultimately failed to qualify for the final tournament.
Baresi was not included in Italy's squad for the 1986 World Cup by coach Enzo Bearzot, who saw him as being more of a midfielder than a defender (although his brother Giuseppe was selected as a defender for the World Cup, as well as Roberto Tricella). He returned to the team for the 1988 European Championships, playing as a sweeper, where Italy reached the semi-finals under Azeglio Vicini, becoming an undisputed first team member, playing in every match. He made his first appearance in a World Cup finals match in the 1990 tournament, which was held on home soil, and he played in every match as one of the starting centrebacks, as Italy finished in third-place, after being eliminated by defending champions Argentina in a penalty shootout in the semi-finals. Baresi helped the Italian defence to keep five consecutive clean sheets, only conceding 2 goals, and going unbeaten for a World Cup record of 518 minutes, until they were beaten by an Argentinian equaliser in the semi-final. His performances earned him a spot on the 1990 World Cup Team of the tournament.
After replacing Giuseppe Bergomi as captain for the 1994 World Cup under his former manager at Milan Arrigo Sacchi, Baresi sustained an injury to his meniscus in Italy's second group match (a 1–0 win against Norway) and missed most of the tournament. He returned to the squad 25 days later, in record time for the final, with a dominant defensive performance, helping Italy to keep a clean sheet against the Brazilians, despite the key defensive absences of his Milan team mates Costacurta and Tassotti. After a 0–0 deadlock following extra time, the match went to a penalty shootout, and Baresi subsequently missed his penalty, suffering from severe cramps and fatigue. Following misses by Massaro and Baggio, Italy were defeated by Brazil in the penalty shootout. Following the World Cup defeat, Baresi made one more appearance for Italy, in an away Euro 1996 qualifying match against Slovenia, on 7 September 1994, which ended in a 1–1 draw. Baresi subsequently retired from the national side at the age of 34, and passed on the captain's armband to his Milan team-mate Maldini. Baresi amassed 81 caps for Italy, scoring one goal in a friendly win against the USSR, and he is one of seven players to have achieved the rare feat of winning Gold, Silver and Bronze World Cup medals during his international career.
Style of play.
Baresi is regarded as one of the greatest defenders of all time. He played his entire twenty year career with Milan, becoming a club legend. At Milan, he formed one of the most formidable defensive units of all time, alongside Maldini, Costacurta, Tassotti, Galli, and later Panucci. He was a complete and consistent defender, who combined power with elegance, and he was gifted with outstanding physical and mental attributes, such as pace, strength, tenacity, concentration and stamina, which made him effective in the air.
Although Baresi was capable of playing anywhere along the backline, he primarily excelled as a centreback and as sweeper, where he combined his defensive attributes, and his ability to read the game, with his excellent vision, technique, and ball distribution skills. His passing range, technical ability, and ball control allowed him to advance forward into the midfield to start attacking plays from the back, enabling him to function as a secondary playmaker for his team, and also play as a defensive or central midfielder when necessary. Despite being a defender, he was also an accurate penalty kick taker. Baresi was known for being a strong and accurate tackler, who was very good at winning back possession, and at anticipating and intercepting plays, due to his acute tactical intelligence, marking ability, and positional sense. He was also known for his professionalism, his longevity, his outstanding leadership, his commanding presence on the pitch, and his organisational skills throughout his career, captaining both Milan and the Italian national side.
Coaching career.
On 1 June 2002, Baresi was officially appointed as director of football at Fulham, but tensions between Baresi and then Fulham manager Jean Tigana led to resignation from the club in August.
He was appointed head coach of Milan's "Primavera" Under-20 squad. In 2006, he was moved by the club to coach the "Berretti" Under-19 squad, with his former fellow Filippo Galli replacing him at the helm of the Primavera squad. He retired from coaching and was replaced by Roberto Bertuzzo.
Personal life.
Franco Baresi is the younger brother of Internazionale legendary defender Giuseppe Baresi. Interestingly, as youngsters, both players had tryouts for Inter, but Franco was rejected, and purchased by local rivals Milan; as he was the younger player, he was initially known as "Baresi 2". Due to Franco's eventual great success and popularity throughout his career, however, which even surpassed that of his older brother's, Giuseppe later became known as "the other Baresi", despite also achieving notable success.
Media.
Baresi is featured in the EA Sports football video game series "FIFA 14"'s Classic XI – a multi-national all-star team, along with compatriots Bruno Conti, Gianni Rivera, and Giacinto Facchetti. He was named in the Ultimate Team Legends in "FIFA 15".
Career statistics.
Club.
"*European competitions include the UEFA Champions League, UEFA Cup, and UEFA Super Cup"
 

</doc>
<doc id="10857" url="https://en.wikipedia.org/wiki?curid=10857" title="Stage (stratigraphy)">
Stage (stratigraphy)

In chronostratigraphy, a stage is a succession of rock strata laid down in a single age on the geologic timescale, which usually represents millions of years of deposition. A given stage of rock and the corresponding age of time will by convention have the same name, and the same boundaries.
Rock series are divided into stages, just as geological epochs are divided into ages. Stages can be divided into smaller stratigraphic units called chronozones. (See chart at right for full terminology hierarchy.) Stages may also be divided into substages or indeed grouped as superstages.
The term faunal stage is sometimes used, referring to the fact that the same fauna (animals) are found throughout the layer (by definition).
Defining.
Stages are primarily defined by a consistent set of fossils (biostratigraphy) or a consistent magnetic polarity (see paleomagnetism) in the rock. Usually one or more index fossils that are common, found worldwide, easily recognized, and limited to a single, or at most a few, stages are used to define the stage's bottom. 
Thus, for example in the local North American subdivision, a paleontologist finding fragments of the trilobite "Olenellus" would identify the beds as being from the Waucoban Stage whereas fragments of a later trilobite such as "Elrathia" would identify the stage as Albertan. 
Stages were important in the 19th and early 20th centuries as they were the major tool available for dating and correlating rock units prior to the development of seismology and radioactive dating in the second half of the 20th Century. Microscopic analysis of the rock (petrology) is also sometimes useful in confirming that a given segment of rock is from a particular age.
Originally, faunal stages were only defined regionally; however as additional stratigraphic and geochonologic tools, were developed, stages were defined over broader and broader areas. More recently, the adjective "faunal" has been dropped as regional and global correlations of rock sequences have become relatively certain and there is less need for faunal labels to define the age of formations. A tendency developed to use European and, to a lesser extent, Asian, stage names for the same time period worldwide, even though the faunas in other regions often had little in common with the stage as originally defined.
International standardization.
Boundaries and names are established by the International Commission on Stratigraphy (ICS) of the International Union of Geological Sciences. As of 2008, the ICS is nearly finished a task begun in 1974, subdividing the Phanerozoic eonothem into internationally accepted stages using two types of benchmark. For younger stages, a Global Boundary Stratotype Section and Point (GSSP), a physical outcrop clearly demonstrates the boundary. For older stages, a Global Standard Stratigraphic Age (GSSA) is an absolute date. The benchmarks will give a much greater certainty that results can be compared with confidence in the date determinations, and such results will have farther scope than any evaluation based solely on local knowledge and conditions. 
In many regions local subdivisions and classification criteria are still used along with the newer internationally coordinated uniform system, but once the research establishes a more complete international system, it is expected that local systems will be abandoned.
Stages and lithostratigraphy.
Stages can include many lithostratigraphic units (for example formations, beds, members, etc.) of differing rock types that were being laid down in different environments at the same time. In the same way, a lithostratigraphic unit can include a number of stages or parts of them.

</doc>
<doc id="10858" url="https://en.wikipedia.org/wiki?curid=10858" title="Franz Kafka">
Franz Kafka

Franz Kafka (3 July 1883 – 3 June 1924) was a German-language writer of novels and short stories who is widely regarded as one of the major figures of 20th-century literature. His work, which fuses elements of realism and the fantastic, typically features isolated protagonists faced by bizarre or surrealistic predicaments and incomprehensible social-bureaucratic powers, and has been interpreted as exploring themes of alienation, existential anxiety, guilt, and absurdity. His best known works include "" ("The Metamorphosis"), ' ("The Trial"), and ' ("The Castle"). The term "Kafkaesque" has entered the English language to describe situations like those in his writing.
Kafka was born into a middle-class, German-speaking Jewish family in Prague, the capital of the Kingdom of Bohemia, then part of the Austro-Hungarian Empire. He trained as a lawyer, and after completing his legal education he was employed with an insurance company, forcing him to relegate writing to his spare time. Over the course of his life, Kafka wrote hundreds of letters to family and close friends, including his father, with whom he had a strained and formal relationship. He died in 1924 at the age of 40 from tuberculosis.
Few of Kafka's works were published during his lifetime: the story collections ' ("Contemplation") and ' ("A Country Doctor"), and individual stories (such as "") were published in literary magazines but received little public attention. Kafka's unfinished works, including his novels ', ' and ' (also known as ', "The Man Who Disappeared"), were ordered by Kafka to be destroyed by his friend Max Brod, who nonetheless ignored his friend's direction and published them after Kafka's death.
Life.
Family.
Kafka was born near the Old Town Square in Prague, then part of the Austro-Hungarian Empire. His family were middle-class Ashkenazi Jews. His father, Hermann Kafka (1852–1931), was the fourth child of Jakob Kafka, a ' or ritual slaughterer in Osek, a Czech village with a large Jewish population located near Strakonice in southern Bohemia. Hermann brought the Kafka family to Prague. After working as a travelling sales representative, he eventually became a fancy goods and clothing retailer who employed up to 15 people and used the image of a jackdaw (' in Czech, pronounced and colloquially written as "kafka") as his business logo. Kafka's mother, Julie (1856–1934), was the daughter of Jakob Löwy, a prosperous retail merchant in Poděbrady, and was better educated than her husband.
Kafka's parents probably spoke a variety of German influenced by Yiddish that was sometimes pejoratively called Mauscheldeutsch, but, as the German language was considered the vehicle of social mobility, they probably encouraged their children to speak High German. Hermann and Julie had six children, of whom Franz was the eldest. Franz's two brothers, Georg and Heinrich, died in infancy before Franz was seven; his three sisters were Gabriele ("Ellie") (1889–1944), Valerie ("Valli") (1890–1942) and Ottilie ("Ottla") (1892–1943). They all died during the Holocaust of World War II. Valli was deported to the Łódź Ghetto in Poland in 1942, but that is the last documentation of her.
Hermann is described by the biographer Stanley Corngold as a "huge, selfish, overbearing businessman" and by Franz Kafka as "a true Kafka in strength, health, appetite, loudness of voice, eloquence, self-satisfaction, worldly dominance, endurance, presence of mind, n knowledge of human nature". On business days, both parents were absent from the home, with Julie Kafka working as many as 12 hours each day helping to manage the family business. Consequently, Kafka's childhood was somewhat lonely, and the children were reared largely by a series of governesses and servants. Kafka's troubled relationship with his father is evident in his "" ("Letter to His Father") of more than 100 pages, in which he complains of being profoundly affected by his father's authoritarian and demanding character; his mother, in contrast, was quiet and shy. The dominating figure of Kafka's father had a significant influence on Kafka's writing.
The Kafka family had a servant girl living with them in a cramped apartment. Franz's room was often cold. In November 1913 the family moved into a bigger apartment, although Ellie and Valli had married and moved out of the first apartment. In early August 1914, just after World War I began, the sisters did not know where their husbands were in the military and moved back in with the family in this larger apartment. Both Ellie and Valli also had children. Franz at age 31 moved into Valli's former apartment, quiet by contrast, and lived by himself for the first time.
Education.
From 1889 to 1893, Kafka attended the ' German boys' elementary school at the ' (meat market), now known as Masná Street. His Jewish education ended with his Bar Mitzvah celebration at the age of 13. Kafka never enjoyed attending the synagogue and went with his father only on four high holidays a year.
After leaving elementary school in 1893, Kafka was admitted to the rigorous classics-oriented state gymnasium, "", an academic secondary school at Old Town Square, within the Kinský Palace. German was the language of instruction, but Kafka also spoke and wrote in Czech. He studied the latter at the gymnasium for eight years, achieving good grades. Although Kafka received compliments for his Czech, he never considered himself fluent in Czech, though he spoke German with a Czech accent. He completed his Matura exams in 1901.
Admitted to the ' of Prague in 1901, Kafka began studying chemistry, but switched to law after two weeks. Although this field did not excite him, it offered a range of career possibilities which pleased his father. In addition, law required a longer course of study, giving Kafka time to take classes in German studies and art history. He also joined a student club, ' (Reading and Lecture Hall of the German students), which organized literary events, readings and other activities. Among Kafka's friends were the journalist Felix Weltsch, who studied philosophy, the actor Yitzchak Lowy who came from an orthodox Hasidic Warsaw family, and the writers Oskar Baum and Franz Werfel.
At the end of his first year of studies, Kafka met Max Brod, a fellow law student who became a close friend for life. Brod soon noticed that, although Kafka was shy and seldom spoke, what he said was usually profound. Kafka was an avid reader throughout his life; together he and Brod read Plato's "Protagoras" in the original Greek, on Brod's initiative, and Flaubert's ' and ' ("The Temptation of Saint Anthony") in French, at his own suggestion. Kafka considered Fyodor Dostoyevsky, Flaubert, Nikolai Gogol, Franz Grillparzer, and Heinrich von Kleist to be his "true blood brothers". Besides these, he took an interest in Czech literature and was also very fond of the works of Goethe. Kafka was awarded the degree of Doctor of Law on 18 July 1906 and performed an obligatory year of unpaid service as law clerk for the civil and criminal courts.
Employment.
On 1 November 1907, Kafka was hired at the ', an Italian insurance company, where he worked for nearly a year. His correspondence during that period indicates that he was unhappy with a working time schedule—from 08:00 until 18:00—making it extremely difficult to concentrate on writing, which was assuming increasing importance to him. On 15 July 1908, he resigned. Two weeks later he found employment more amenable to writing when he joined the Worker's Accident Insurance Institute for the Kingdom of Bohemia. The job involved investigating and assessing compensation for personal injury to industrial workers; accidents such as lost fingers or limbs were commonplace at this time. The management professor Peter Drucker credits Kafka with developing the first civilian hard hat while employed at the Worker's Accident Insurance Institute, but this is not supported by any document from his employer. His father often referred to his son's job as an insurance officer as a ', literally "bread job", a job done only to pay the bills; Kafka often claimed to despise it. Kafka was rapidly promoted and his duties included processing and investigating compensation claims, writing reports, and handling appeals from businessmen who thought their firms had been placed in too high a risk category, which cost them more in insurance premiums. He would compile and compose the annual report on the insurance institute for the several years he worked there. The reports were received well by his superiors. Kafka usually got off work at 2 p.m., so that he had time to spend on his literary work, to which he was committed. Kafka's father also expected him to help out at and take over the family fancy goods store. In his later years, Kafka's illness often prevented him from working at the insurance bureau and at his writing. Years later, Brod coined the term "" ("The Close Prague Circle") to describe the group of writers, which included Kafka, Felix Weltsch and him.
In late 1911, Elli's husband Karl Hermann and Kafka became partners in the first asbestos factory in Prague, known as Prager Asbestwerke Hermann & Co., having used dowry money from Hermann Kafka. Kafka showed a positive attitude at first, dedicating much of his free time to the business, but he later resented the encroachment of this work on his writing time. During that period, he also found interest and entertainment in the performances of Yiddish theatre. After seeing a Yiddish theater troupe perform in October 1911, for the next six months Kafka "immersed himself in Yiddish language and in Yiddish literature". This interest also served as a starting point for his growing exploration of Judaism. It was at about this time that Kafka became a vegetarian. Around 1915 Kafka received his draft notice for military service in World WarI, but his employers at the insurance institute arranged for a deferment because his work was considered essential government service. Later he attempted to join the military but was prevented from doing so by medical problems associated with tuberculosis, with which he was diagnosed in 1917. In 1918 the Worker's Accident Insurance Institute put Kafka on a pension due to his illness, for which there was no cure at the time, and he spent most of the rest of his life in sanatoriums.
Private life.
Kafka was never married. According to Brod, Kafka was "tortured" by sexual desire and Kafka's biographer Reiner Stach states that his life was full of "incessant womanising" and that he was filled with a fear of "sexual failure". He visited brothels for most of his adult life and was interested in pornography. In addition, he had close relationships with several women during his life. On 13 August 1912, Kafka met Felice Bauer, a relative of Brod, who worked in Berlin as a representative of a dictaphone company. A week after the meeting at Brod's home, Kafka wrote in his diary:
Shortly after this, Kafka wrote the story "" ("The Judgment") in only one night and worked in a productive period on ' ("The Man Who Disappeared") and "Die Verwandlung" ("The Metamorphosis"). Kafka and Felice Bauer communicated mostly through letters over the next five years, met occasionally, and were engaged twice. Kafka's extant letters to her were published as ' ("Letters to Felice"); her letters do not survive. According to biographers Stach and James Hawes, around 1920 Kafka was engaged a third time, to Julie Wohryzek, a poor and uneducated hotel chambermaid. Although the two rented a flat and set a wedding date, the marriage never took place. During this time Kafka began a draft of the "Letter to His Father", who objected to Julie because of her Zionist beliefs. Before the date of the intended marriage, he took up with yet another woman. While he needed women and sex in his life, he had low self-confidence, felt sex was dirty, and was shy—especially about his body.
Stach and Brod state that during the time that Kafka knew Felice Bauer, he had an affair with a friend of hers, Margarethe "Grete" Bloch, a Jewish woman from Berlin. Brod says that Bloch gave birth to Kafka's son, although Kafka never knew about the child. The boy, whose name is not known, was born in 1914 or 1915 and died in Munich in 1921. However, Kafka's biographer Peter-André Alt claims that, while Bloch had a son, Kafka was not the father as the pair were never intimate. Stach states that Bloch had a son, but there is not solid proof but contradictory evidence that Kafka was the father.
Kafka was diagnosed with tuberculosis in August 1917 and moved for a few months to the Bohemian village of Zürau (Siřem in the Czech language), where his sister Ottla worked on the farm of her brother-in-law Hermann. He felt comfortable there and later described this time as perhaps the best time in his life, probably because he had no responsibilities. He kept diaries and ' (octavo). From the notes in these books, Kafka extracted 109 numbered pieces of text on "Zettel", single pieces of paper in no given order. They were later published as ' (The Zürau Aphorisms or Reflections on Sin, Hope, Suffering, and the True Way).
In 1920 Kafka began an intense relationship with Milena Jesenská, a Czech journalist and writer. His letters to her were later published as '. During a vacation in July 1923 to Graal-Müritz on the Baltic Sea, Kafka met Dora Diamant, a 25-year-old kindergarten teacher from an orthodox Jewish family. Kafka, hoping to escape the influence of his family to concentrate on his writing, moved briefly to Berlin and lived with Diamant. She became his lover and caused him to become interested in the Talmud. He worked on four stories, which he prepared to be published as ' ("A Hunger Artist").
Personality.
Kafka feared that people would find him mentally and physically repulsive. However, those who met him found him to possess a quiet and cool demeanor, obvious intelligence, and a dry sense of humour; they also found him boyishly handsome, although of austere appearance. Brod compared Kafka to Heinrich von Kleist, noting that both writers had the ability to describe a situation realistically with precise details. Brod thought Kafka was one of the most entertaining people he had met; Kafka enjoyed sharing humour with his friends, but also helped them in difficult situations with good advice. According to Brod, he was a passionate reciter, who was able to phrase his speaking as if it were music. Brod felt that two of Kafka's most distinguishing traits were "absolute truthfulness" (') and "precise conscientiousness" ('). He explored details, the inconspicuous, in depth and with such love and precision that things surfaced that were unforeseen, seemingly strange, but absolutely true ("").
Although Kafka showed little interest in exercise as a child, he later showed interest in games and physical activity, as a good rider, swimmer, and rower. On weekends he and his friends embarked on long hikes, often planned by Kafka himself. His other interests included alternative medicine, modern education systems such as Montessori, and technical novelties such as airplanes and film. Writing was important to Kafka; he considered it a "form of prayer". He was highly sensitive to noise and preferred quiet when writing.
Pérez-Álvarez has claimed that Kafka may have possessed a schizoid personality disorder. His style, it is claimed, not only in "Die Verwandlung" ("The Metamorphosis"), but in various other writings, appears to show low to medium-level schizoid traits, which explain much of his work. His anguish can be seen in this diary entry from 21 June 1913:
and in Zürau Aphorism number 50:
Though Kafka never married, he held marriage and children in high esteem. He had several girlfriends. He may have suffered from an eating disorder. Doctor Manfred M. Fichter of the Psychiatric Clinic, University of Munich, presented "evidence for the hypothesis that the writer Franz Kafka had suffered from an atypical anorexia nervosa", and that Kafka was not just lonely and depressed but also "occasionally suicidal". In his 1995 book "Franz Kafka, the Jewish Patient", Sander Gilman investigated "why a Jew might have been considered 'hypochondriac' or 'homosexual' and how Kafka incorporates aspects of these ways of understanding the Jewish male into his own self-image and writing". Kafka considered committing suicide at least once, in late 1912.
Political views.
Prior to World War I, Kafka attended several meetings of the Klub Mladých, a Czech anarchist, anti-militarist, and anti-clerical organization. Hugo Bergmann, who attended the same elementary and high schools as Kafka, fell out with Kafka during their last academic year (1900–1901) because "afka' socialism and my Zionism were much too strident". "Franz became a socialist, I became a Zionist in 1898. The synthesis of Zionism and socialism did not yet exist". Bergmann claims that Kafka wore a red carnation to school to show his support for socialism. In one diary entry, Kafka made reference to the influential anarchist philosopher Prince Peter Kropotkin: "Don't forget Kropotkin!"
During the communist era, the legacy of Kafka's work for Eastern bloc socialism was hotly debated. Opinions ranged from the notion that he satirised the bureaucratic bungling of a crumbling Austria-Hungarian Empire, to the belief that he embodied the rise of socialism. A further key point was Marx's theory of alienation. While the orthodox position was that Kafka's depictions of alienation were no longer relevant for a society that had supposedly eliminated alienation, a 1963 conference held in Liblice, Czechoslovakia, on the eightieth anniversary of his birth, reassessed the importance of Kafka's portrayal of bureaucracy. Whether or not Kafka was a political writer is still an issue of debate.
Judaism and Zionism.
Kafka grew up in Prague as a German-speaking Jew. He was deeply fascinated by the Jews of Eastern Europe, who he thought possessed an intensity of spiritual life that was absent from Jews in the West. His diary is full of references to Yiddish writers. Yet he was at times alienated from Judaism and Jewish life: "What have I in common with Jews? I have hardly anything in common with myself and should stand very quietly in a corner, content that I can breathe". In his adolescent years, Kafka had declared himself an atheist.
Hawes suggests that Kafka, though very aware of his own Jewishness, did not incorporate it into his work, which, according to Hawes, lacks Jewish characters, scenes or themes. In the opinion of literary critic Harold Bloom, although Kafka was uneasy with his Jewish heritage, he was the quintessential Jewish writer. Lothar Kahn is likewise unequivocal: "The presence of Jewishness in Kafka's ' is no longer subject to doubt". Pavel Eisner, one of Kafka's first translators, interprets ' ("The Trial") as the embodiment of the "triple dimension of Jewish existence in Prague... his protagonist Josef K. is (symbolically) arrested by a German (Rabensteiner), a Czech (Kullich) and a Jew (Kaminer). He stands for the 'guiltless guilt' that imbues the Jew in the modern world, although there is no evidence that he himself is a Jew".
In his essay "Sadness in Palestine?!", Dan Miron explores Kafka's connection to Zionism: "It seems that those who claim that there was such a connection and that Zionism played a central role in his life and literary work, and those who deny the connection altogether or dismiss its importance, are both wrong. The truth lies in some very elusive place between these two simplistic poles". Kafka considered moving to Palestine with Felice Bauer, and later with Dora Diamant. He studied Hebrew while living in Berlin, hiring a friend of Brod's from Palestine, Pua Bat-Tovim, to tutor him and attending Rabbi Julius Grünthal's and Rabbi Julius Guttmann's classes in the Berlin "" (College for the Study of Judaism).
Livia Rothkirchen calls Kafka the "symbolic figure of his era". His contemporaries included numerous Jewish writers (Czech, German and national Jews) who were sensitive to German, Czech, Austrian and Jewish culture. According to Rothkirchen, "This situation lent their writings a broad cosmopolitan outlook and a quality of exaltation bordering on transcendental metaphysical contemplation. An illustrious example is Franz Kafka".
Towards the end of his life Kafka sent a postcard to his friend Hugo Bergman in Tel Aviv, announcing his intention to emigrate to Palestine. Bergman refused to host Kafka because he had young children and was afraid that Kafka would infect them with tuberculosis.
Death.
Kafka's laryngeal tuberculosis worsened and in March 1924 he returned from Berlin to Prague, where members of his family, principally his sister Ottla, took care of him. He went to Dr. Hoffmann's sanatorium in Kierling near Vienna for treatment on 10 April, and died there on 3 June 1924. The cause of death seemed to be starvation: the condition of Kafka's throat made eating too painful for him, and since parenteral nutrition had not yet been developed, there was no way to feed him. Kafka was editing "A Hunger Artist" on his deathbed, a story whose composition he had begun before his throat closed to the point that he could not take any nourishment. His body was brought back to Prague where he was buried on 11 June 1924, in the New Jewish Cemetery in Prague-Žižkov. Kafka was unknown during his own lifetime, but he did not consider fame important. He became famous soon after his death.
Works.
All of Kafka's published works, except some letters he wrote in Czech to Milena Jesenská, were written in German. What little was published during his lifetime attracted scant public attention.
Kafka finished none of his full-length novels and burned around 90 percent of his work, much of it during the period he lived in Berlin with Diamant, who helped him burn the drafts. In his early years as a writer, he was influenced by von Kleist, whose work he described in a letter to Bauer as frightening, and whom he considered closer than his own family.
Stories.
Kafka's earliest published works were eight stories which appeared in 1908 in the first issue of the literary journal "Hyperion" under the title "" ("Contemplation"). He wrote the story "" ("Description of a Struggle") in 1904; he showed it to Brod in 1905 who advised him to continue writing and convinced him to submit it to "Hyperion". Kafka published a fragment in 1908 and two sections in the spring of 1909, all in Munich.
In a creative outburst on the night of 22 September 1912, Kafka wrote the story "Das Urteil" ("The Judgment", literally: "The Verdict") and dedicated it to Felice Bauer. Brod noted the similarity in names of the main character and his fictional fiancée, Georg Bendemann and Frieda Brandenfeld, to Franz Kafka and Felice Bauer. The story is often considered Kafka's breakthrough work. It deals with the troubled relationship of a son and his dominant father, facing a new situation after the son's engagement. Kafka later described writing it as "a complete opening of body and soul", a story that "evolved as a true birth, covered with filth and slime". The story was first published in Leipzig in 1912 and dedicated "to Miss Felice Bauer", and in subsequent editions "for F."
In 1912, Kafka wrote "Die Verwandlung" ("The Metamorphosis", or "The Transformation"), published in 1915 in Leipzig. The story begins with a travelling salesman waking to find himself transformed into a ', a monstrous vermin, ' being a general term for unwanted and unclean animals. Critics regard the work as one of the seminal works of fiction of the 20th century. The story "In der Strafkolonie" ("In the Penal Colony"), dealing with an elaborate torture and execution device, was written in October 1914, revised in 1918, and published in Leipzig during October 1919. The story "Ein Hungerkünstler" ("A Hunger Artist"), published in the periodical "" in 1924, describes a victimized protagonist who experiences a decline in the appreciation of his strange craft of starving himself for extended periods. His last story, "Josefine, die Sängerin oder Das Volk der Mäuse" ("Josephine the Singer, or the Mouse Folk"), also deals with the relationship between an artist and his audience.
Novels.
He began his first novel in 1912; its first chapter is the story "Der Heizer" ("The Stoker"). Kafka called the work, which remained unfinished, "" ("The Man Who Disappeared" or "The Missing Man"), but when Brod published it after Kafka's death he named it "Amerika". The inspiration for the novel was the time spent in the audience of Yiddish theatre the previous year, bringing him to a new awareness of his heritage, which led to the thought that an innate appreciation for one's heritage lives deep within each person. More explicitly humorous and slightly more realistic than most of Kafka's works, the novel shares the motif of an oppressive and intangible system putting the protagonist repeatedly in bizarre situations. It uses many details of experiences of his relatives who had emigrated to America and is the only work for which Kafka considered an optimistic ending.
During 1914, Kafka began the novel "" ("The Trial"), the story of a man arrested and prosecuted by a remote, inaccessible authority, with the nature of his crime revealed neither to him nor to the reader. Kafka did not complete the novel, although he finished the final chapter. According to Nobel Prize winner and Kafka scholar Elias Canetti, Felice is central to the plot of "Der Process" and Kafka said it was "her story". Canetti titled his book on Kafka's letters to Felice "Kafka's Other Trial", in recognition of the relationship between the letters and the novel. Michiko Kakutani notes in a review for "The New York Times" that Kafka's letters have the "earmarks of his fiction: the same nervous attention to minute particulars; the same paranoid awareness of shifting balances of power; the same atmosphere of emotional suffocation—combined, surprisingly enough, with moments of boyish ardor and delight."
According to his diary, Kafka was already planning his novel ' (The Castle), by 11 June 1914; however, he did not begin writing it until 27 January 1922. The protagonist is the ' (land surveyor) named K., who struggles for unknown reasons to gain access to the mysterious authorities of a castle who govern the village. Kafka's intent was that the castle's authorities notify K. on his deathbed that his "legal claim to live in the village was not valid, yet, taking certain auxiliary circumstances into account, he was to be permitted to live and work there". Dark and at times surreal, the novel is focused on alienation, bureaucracy, the seemingly endless frustrations of man's attempts to stand against the system, and the futile and hopeless pursuit of an unobtainable goal. Hartmut M. Rastalsky noted in his thesis: "Like dreams, his texts combine precise "realistic" detail with absurdity, careful observation and reasoning on the part of the protagonists with inexplicable obliviousness and carelessness."
Publishing history.
Kafka's stories were initially published in literary periodicals. His first eight were printed in 1908 in the first issue of the bi-monthly "Hyperion". Franz Blei published two dialogues in 1909 which became part of "Beschreibung eines Kampfes" ("Description of a Struggle"). A fragment of the story "Die Aeroplane in Brescia" ("The Aeroplanes at Brescia"), written on a trip to Italy with Brod, appeared in the daily "Bohemia" on 28 September 1909. On 27 March 1910, several stories that later became part of the book ' were published in the Easter edition of "Bohemia". In Leipzig during 1913, Brod and publisher Kurt Wolff included "" ("The Judgment. A Story by Franz Kafka.") in their literary yearbook for the art poetry "Arkadia". The story "" ("Before the Law") was published in the 1915 New Year's edition of the independent Jewish weekly '; it was reprinted in 1919 as part of the story collection ' ("A Country Doctor") and became part of the novel '. Other stories were published in various publications, including Martin Buber's "Der Jude", the paper ', and the periodicals ', "Genius", and "Prager Presse".
Kafka's first published book, ' ("Contemplation", or "Meditation"), was a collection of 18stories written between 1904 and 1912. On a summer trip to Weimar, Brod initiated a meeting between Kafka and Kurt Wolff; Wolff published ' in the at the end of 1912 (with the year given as 1913). Kafka dedicated it to Brod, ", and added in the personal copy given to his friend " ("As it is already printed here, for my dearest Max").
Kafka's story "Die Verwandlung" ("The Metamorphosis") was first printed in the October 1915 issue of ', a monthly edition of expressionist literature, edited by René Schickele. Another story collection, ' ("A Country Doctor"), was published by Kurt Wolff in 1919, dedicated to Kafka's father. Kafka prepared a final collection of four stories for print, ' "(A Hunger Artist)", which appeared in 1924 after his death, in '. On 20 April 1924, the "" published Kafka's essay on Adalbert Stifter.
Max Brod.
Kafka left his work, both published and unpublished, to his friend and literary executor Max Brod with explicit instructions that it should be destroyed on Kafka's death; Kafka wrote: "Dearest Max, my last request: Everything I leave behind me... in the way of diaries, manuscripts, letters (my own and others'), sketches, and so on, to be burned unread". Brod ignored this request and published the novels and collected works between 1925 and 1935. He took many papers, which remain unpublished, with him in suitcases to Palestine when he fled there in 1939. Kafka's last lover, Dora Diamant (later, Dymant-Lask), also ignored his wishes, secretly keeping 20notebooks and 35letters. These were confiscated by the Gestapo in 1933, but scholars continue to search for them.
As Brod published the bulk of the writings in his possession, Kafka's work began to attract wider attention and critical acclaim. Brod found it difficult to arrange Kafka's notebooks in chronological order. One problem was that Kafka often began writing in different parts of the book; sometimes in the middle, sometimes working backwards from the end. Brod finished many of Kafka's incomplete works for publication. For example, Kafka left ' with unnumbered and incomplete chapters and ' with incomplete sentences and ambiguous content; Brod rearranged chapters, copy edited the text, and changed the punctuation. ' appeared in 1925 in '. Kurt Wolff published two other novels, ' in 1926 and "Amerika" in 1927. In 1931, Brod edited a collection of prose and unpublished stories as ' "(The Great Wall of China)", including the story of the same name. The book appeared in the "". Brod's sets are usually called the "Definitive Editions".
Modern editions.
In 1961, Malcolm Pasley acquired most of Kafka's original handwritten work for the Oxford Bodleian Library. The text for ' was later purchased through auction and is stored at the German Literary Archives in Marbach am Neckar, Germany. Subsequently, Pasley headed a team (including Gerhard Neumann, Jost Schillemeit and Jürgen Born) which reconstructed the German novels; republished them. Pasley was the editor for ', published in 1982, and "", published in 1990. Jost Schillemeit was the editor of " published in 1983. These are called the "Critical Editions" or the "Fischer Editions".
Unpublished papers.
When Brod died in 1968, he left Kafka's unpublished papers, which are believed to number in the thousands, to his secretary Esther Hoffe. She released or sold some, but left most to her daughters, Eva and Ruth, who also refused to release the papers. A court battle began in 2008 between the sisters and the National Library of Israel, which claimed these works became the property of the nation of Israel when Brod emigrated to British Palestine in 1939. Esther Hoffe sold the original manuscript of "" for US$2 million in 1988 to the German Literary Archive Museum of Modern Literature in Marbach am Neckar. Only Eva was still alive as of 2012. A ruling by a Tel Aviv family court in 2010 held that the papers must be released and a few were, including a previously unknown story, but the legal battle continued. The Hoffes claim the papers are their personal property, while the National Library argues they are "cultural assets belonging to the Jewish people". The National Library also suggests that Brod bequeathed the papers to them in his will. The Tel Aviv Family Court ruled in October 2012 that the papers were the property of the National Library.
Critical interpretations.
The poet W. H. Auden called Kafka "the Dante of the twentieth century"; the novelist Vladimir Nabokov placed him among the greatest writers of the 20th century. Gabriel García Márquez noted the reading of Kafka's "The Metamorphosis" showed him "that it was possible to write in a different way". A prominent theme of Kafka's work, first established in the short story "Das Urteil", is father-son conflict: the guilt induced in the son is resolved through suffering and atonement. Other prominent themes and archetypes include alienation, physical and psychological brutality, characters on a terrifying quest, and mystical transformation.
Kafka's style has been compared to that of Kleist as early as 1916, in a review of "Die Verwandlung" and "Der Heizer" by Oscar Walzel in "Berliner Beiträge". The nature of Kafka's prose allows for varied interpretations and critics have placed his writing into a variety of literary schools. Marxists, for example, have sharply disagreed over how to interpret Kafka's works. Some accused him of distorting reality whereas others claimed he was critiquing capitalism. The hopelessness and absurdity common to his works are seen as emblematic of existentialism. Some of Kafka's books are influenced by the expressionist movement, though the majority of his literary output was associated with the experimental modernist genre. Kafka also touches on the theme of human conflict with bureaucracy. William Burrows claims that such work is centred on the concepts of struggle, pain, solitude, and the need for relationships. Others, such as Thomas Mann, see Kafka's work as allegorical: a quest, metaphysical in nature, for God.
According to Gilles Deleuze and Félix Guattari, the themes of alienation and persecution, although present in Kafka's work, have been over-emphasised by critics. They argue Kafka's work is more deliberate and subversive—and more joyful—than may first appear. They point out that reading his work while focusing on the futility of his characters' struggles reveals Kafka's play of humour; he is not necessarily commenting on his own problems, but rather pointing out how people tend to invent problems. In his work, Kafka often created malevolent, absurd worlds. Kafka read drafts of his works to his friends, typically concentrating on his humorous prose. The writer Milan Kundera suggests that Kafka's surrealist humour may have been an inversion of Dostoyevsky's presentation of characters who are punished for a crime. In Kafka's work a character is punished although a crime has not been committed. Kundera believes that Kafka's inspirations for his characteristic situations came both from growing up in a patriarchal family and living in a totalitarian state.
Attempts have been made to identify the influence of Kafka's legal background and the role of law in his fiction. Most interpretations identify aspects of law and legality as important in his work, in which the legal system is often oppressive. The law in Kafka's works, rather than being representative of any particular legal or political entity, is usually interpreted to represent a collection of anonymous, incomprehensible forces. These are hidden from the individual but control the lives of the people, who are innocent victims of systems beyond their control. Critics who support this absurdist interpretation cite instances where Kafka describes himself in conflict with an absurd universe, such as the following entry from his diary:
However, James Hawes argues many of Kafka's descriptions of the legal proceedings in ""—metaphysical, absurd, bewildering and nightmarish as they might appear, are based on accurate and informed descriptions of German and Austrian criminal proceedings of the time, which were inquisitorial rather than adversarial. Although he worked in insurance, as a trained lawyer Kafka was "keenly aware of the legal debates of his day". In an early 21st-century publication that uses Kafka's office writings as its point of departure, Pothik Ghosh states that with Kafka, law "has no meaning outside its fact of being a pure force of domination and determination".
Translations.
The earliest English translations of Kafka's works were by Edwin and Willa Muir, who in 1930 translated the first German edition of '. This was published as "The Castle" by Secker & Warburg in England and Alfred A. Knopf in the United States. A 1941 edition, including a homage by Thomas Mann, spurred a surge in Kafka's popularity in the United States the late 1940s. The Muirs translated all shorter works that Kafka had seen fit to print; they were published by Schocken Books in 1948 as ', including additionally "The First Long Train Journey", written by Kafka and Brod, Kafka's "A Novel about Youth", a review of Felix Sternheim's "Die Geschichte des jungen Oswald", his essay on Kleist's "Anecdotes", his review of the literary magazine "Hyperion", and an epilogue by Brod.
Later editions, notably those of 1954 ("Dearest Father. Stories and Other Writings"), included text, translated by Eithne Wilkins and Ernst Kaiser, which had been deleted by earlier publishers. Known as "Definitive Editions", they include translations of "The Trial, Definitive", "The Castle, Definitive", and other writings. These translations are generally accepted to have a number of biases and are considered to be dated in interpretation. Published in 1961 by Schocken Books, "Parables and Paradoxes" presented in a bilingual edition by Nahum N. Glatzer selected writings, drawn from notebooks, diaries, letters, short fictional works and the novel "Der Process".
New translations were completed and published based on the recompiled German text of Pasley and Schillemeit"The Castle, Critical" by Mark Harman (Schocken Books, 1998), "The Trial, Critical" by Breon Mitchell (Schocken Books, 1998), and "Amerika: The Man Who Disappeared" by Michael Hofmann (New Directions Publishing, 2004).
Translation problems to English.
Kafka often made extensive use of a characteristic particular to the German language which permits long sentences that sometimes can span an entire page. Kafka's sentences then deliver an unexpected impact just before the full stop—this being the finalizing meaning and focus. This is due to the construction of subordinate clauses in German which require that the verb be positioned at the end of the sentence. Such constructions are difficult to duplicate in English, so it is up to the translator to provide the reader with the same (or at least equivalent) effect found in the original text. German's more flexible word order and syntactical differences provide for multiple ways in which the same German writing can be translated into English. An example is the first sentence of Kafka's "The Metamorphosis", which is crucial to the setting and understanding of the entire story:
Another virtually insurmountable problem facing translators is how to deal with the author's intentional use of ambiguous idioms and words that have several meanings which result in phrasing difficult to precisely translate. One such instance is found in the first sentence of "The Metamorphosis". English translators often render the word ' as "insect"; in Middle German, however, ' literally means "an animal unclean for sacrifice"; in today's German it means vermin. It is sometimes used colloquially to mean "bug" —a very general term, unlike the scientific "insect". Kafka had no intention of labeling Gregor, the protagonist of the story, as any specific thing, but instead wanted to convey Gregor's disgust at his transformation. Another example is Kafka's use of the German noun ' in the final sentence of "Das Urteil". Literally, ' means intercourse and, as in English, can have either a sexual or non-sexual meaning; in addition, it is used to mean transport or traffic. The sentence can be translated as: "At that moment an unending stream of traffic crossed over the bridge". The double meaning of "Verkehr" is given added weight by Kafka's confession to Brod that when he wrote that final line, he was thinking of "a violent ejaculation".
Legacy.
Literary and cultural influence.
Unlike many famous writers, Kafka is rarely quoted by others. Instead, he is noted more for his visions and perspective. Shimon Sandbank, a professor and writer, identifies Kafka as having influenced Jorge Luis Borges, Albert Camus, Eugène Ionesco, J. M. Coetzee and Jean-Paul Sartre. A "Financial Times" literary critic credits Kafka with influencing José Saramago, and Al Silverman, a writer and editor, states that J. D. Salinger loved to read Kafka's works. In 1999 a committee of 99 authors, scholars, and literary critics ranked ' and ' the second and ninth most significant German-language novels of the 20th century. Shimon Sandbank, a literary critic, argues that despite Kafka's pervasiveness, his enigmatic style has yet to be emulated. Neil Christian Pages, a professor of German Studies and Comparative Literature at Binghamton University who specialises in Kafka's works, says Kafka's influence transcends literature and literary scholarship; it impacts visual arts, music, and popular culture. Harry Steinhauer, a professor of German and Jewish literature, says that Kafka "has made a more powerful impact on literate society than any other writer of the twentieth century". Brod said that the 20th century will one day be known as the "century of Kafka".
Michel-André Bossy writes that Kafka created a rigidly inflexible and sterile bureaucratic universe. Kafka wrote in an aloof manner full of legal and scientific terms. Yet his serious universe also had insightful humour, all highlighting the "irrationality at the roots of a supposedly rational world". His characters are trapped, confused, full of guilt, frustrated, and lacking understanding of their surreal world. Much of the post-Kafka fiction, especially science fiction, follow the themes and precepts of Kafka's universe. This can be seen in the works of authors such as George Orwell and Ray Bradbury.
The following are examples of works across a range of literary, musical, and dramatic genres which demonstrate the extent of cultural influence:
"Kafkaesque".
Kafka's writing has inspired the term "Kafkaesque", used to describe concepts and situations reminiscent of his work, particularly "" ("The Trial") and "Die Verwandlung" ("The Metamorphosis"). Examples include instances in which bureaucracies overpower people, often in a surreal, nightmarish milieu which evokes feelings of senselessness, disorientation, and helplessness. Characters in a Kafkaesque setting often lack a clear course of action to escape a labyrinthine situation. Kafkaesque elements often appear in existential works, but the term has transcended the literary realm to apply to real-life occurrences and situations that are incomprehensibly complex, bizarre, or illogical.
Numerous films and television works have been described as Kafkaesque, and the style is particularly prominent in dystopian science fiction. Works in this genre that have been thus described include Patrick Bokanowski's 1982 film "The Angel", Terry Gilliam's 1985 film "Brazil", and the 1998 science fiction film noir, "Dark City". Films from other genres which have been similarly described include "The Tenant" (1976) and "Barton Fink" (1991). The television series "The Prisoner" and "The Twilight Zone" are also frequently described as Kafkaesque.
However, with common usage, the term has become so ubiquitous that Kafka scholars note it's often misused. More accurately then, according to author Ben Marcus, paraphrased in "What it Means to be Kafkaesque" by Joe Fassler in "The Atlantic", "Kafka’s quintessential qualities are affecting use of language, a setting that straddles fantasy and reality, and a sense of striving even in the face of bleakness—hopelessly and full of hope." 
Commemoration.
The Franz Kafka Museum in Prague is dedicated to Kafka and his work. A major component of the museum is an exhibit "The City of K. Franz Kafka and Prague", which was first shown in Barcelona in 1999, moved to the Jewish Museum in New York City, and was finally established in 2005 in Prague in Malá Strana (Lesser Town), along the Moldau. The museum calls its display of original photos and documents "Město K. Franz Kafka a Praha" (City K. Kafka and Prague) and aims to immerse the visitor into the world in which Kafka lived and about which he wrote.
The Franz Kafka Prize is an annual literary award of the Franz Kafka Society and the City of Prague established in 2001. It recognizes the merits of literature as "humanistic character and contribution to cultural, national, language and religious tolerance, its existential, timeless character, its generally human validity, and its ability to hand over a testimony about our times". The selection committee and recipients come from all over the world, but are limited to living authors who have had at least one work published in the Czech language. The recipient receives $10,000, a diploma, and a bronze statuette at a presentation in Prague's Old Town Hall on the Czech State Holiday in late October.
San Diego State University (SDSU) operates the "Kafka Project", which began in 1998 as the official international search for Kafka's last writings.
References.
Bibliography.
"Journals"
"Newspapers"
"Online sources"
Further reading.
"Journals"

</doc>
<doc id="10859" url="https://en.wikipedia.org/wiki?curid=10859" title="Fields Medal">
Fields Medal

The Fields Medal is a prize awarded to two, three, or four mathematicians under 40 years of age at the International Congress of the International Mathematical Union (IMU), a meeting that takes place every four years. The Fields Medal is sometimes viewed as the highest honor a mathematician can receive. The Fields Medal and the Abel Prize have often been described as the "mathematician's Nobel Prize" (but different at least for the age restriction).
The prize comes with a monetary award, which since 2006 has been C$15,000 (in Canadian dollars). The colloquial name is in honour of Canadian mathematician John Charles Fields. Fields was instrumental in establishing the award, designing the medal itself, and funding the monetary component.
The medal was first awarded in 1936 to Finnish mathematician Lars Ahlfors and American mathematician Jesse Douglas, and it has been awarded every four years since 1950. Its purpose is to give recognition and support to younger mathematical researchers who have made major contributions.
In 2014 Maryam Mirzakhani became the first woman as well as the first Iranian, and Artur Avila became the first mathematician from Latin America to be awarded a Fields Medal.
Conditions of the award.
The Fields Medal is often described as the "Nobel Prize of Mathematics" and for a long time was regarded as the most prestigious award in the field of mathematics. However, in contrast to the Nobel Prize, the Fields Medal is awarded only every four years. The Fields Medal also has an age limit: a recipient must be under age 40 on 1 January of the year in which the medal is awarded. This is similar to restrictions applicable to the Clark Medal in economics. The under-40 rule is based on Fields' desire that "while it was in recognition of work already done, it was at the same time intended to be an encouragement for further achievement on the part of the recipients and a stimulus to renewed effort on the part of others."
The monetary award is much lower than the 8,000,000 Swedish kronor (roughly 1,400,000 Canadian dollars) given with each Nobel prize as of 2014. Other major awards in mathematics, such as the Abel Prize and the Chern Medal, have larger monetary prizes, comparable to the Nobel.
Landmarks.
In 1954, Jean-Pierre Serre became the youngest winner of the Fields Medal, at 27. He still retains this distinction.
In 1966, Alexander Grothendieck boycotted the ICM, held in Moscow, to protest Soviet military actions taking place in Eastern Europe. Léon Motchane, founder and director of the Institut des Hautes Études Scientifiques attended and accepted Grothendieck's Fields Medal on his behalf.
In 1970, Sergei Novikov, because of restrictions placed on him by the Soviet government, was unable to travel to the congress in Nice to receive his medal.
In 1978, Grigory Margulis, because of restrictions placed on him by the Soviet government, was unable to travel to the congress in Helsinki to receive his medal. The award was accepted on his behalf by Jacques Tits, who said in his address: "I cannot but express my deep disappointment — no doubt shared by many people here — in the absence of Margulis from this ceremony. In view of the symbolic meaning of this city of Helsinki, I had indeed grounds to hope that I would have a chance at last to meet a mathematician whom I know only through his work and for whom I have the greatest respect and admiration."
In 1982, the congress was due to be held in Warsaw but had to be rescheduled to the next year, because of martial law introduced in Poland on 13 Dec 1981. The awards were announced at the ninth General Assembly of the IMU earlier in the year and awarded at the 1983 Warsaw congress.
In 1990, Edward Witten became the first and so far only physicist to win this award.
In 1998, at the ICM, Andrew Wiles was presented by the chair of the Fields Medal Committee, Yuri I. Manin, with the first-ever IMU silver plaque in recognition of his proof of Fermat's Last Theorem. Don Zagier referred to the plaque as a "quantized Fields Medal". Accounts of this award frequently make reference that at the time of the award Wiles was over the age limit for the Fields medal. Although Wiles was slightly over the age limit in 1994, he was thought to be a favorite to win the medal; however, a gap (later resolved by Taylor and Wiles) in the proof was found in 1993.
In 2006, Grigori Perelman, who proved the Poincaré conjecture, refused his Fields Medal and did not attend the congress.
In 2014, Maryam Mirzakhani became the first woman as well as the first Iranian, Artur Avila the first South American and Manjul Bhargava the first person of Indian origins to win the Fields Medal.
By university affiliation.
Fields Medalists by university affiliation at the time of being awarded. 
The medal.
The medal was designed by Canadian sculptor R. Tait McKenzie.
Translation: "Mathematicians gathered from the entire world have awarded nderstood 'this prize for outstanding writings."
In the background, there is the representation of Archimedes' tomb, with the carving illustrating his theorem on the sphere and the cylinder, behind a branch. (This is the mathematical result of which Archimedes was reportedly most proud: Given a sphere and a circumscribed cylinder of the same height and diameter, the ratio between their volumes is equal to ⅔.)
The rim bears the name of the prizewinner.

</doc>
<doc id="10861" url="https://en.wikipedia.org/wiki?curid=10861" title="The Trial">
The Trial

The Trial (original German title: , later , and ) is a novel written by Franz Kafka from 1914 to 1915 and published in 1925. One of his best-known works, it tells the story of a man arrested and prosecuted by a remote, inaccessible authority, with the nature of his crime revealed neither to him nor to the reader. Heavily influenced by Dostoyevsky's "Crime and Punishment" and "The Brothers Karamazov", Kafka even went so far as to call Dostoevsky a blood relative. Like Kafka's other novels, "The Trial" was never completed, although it does include a chapter which brings the story to an end.
After Kafka's death in 1924 his friend and literary executor Max Brod edited the text for publication by Verlag Die Schmiede. The original manuscript is held at the Museum of Modern Literature, Marbach am Neckar, Germany. The first English language translation, by Willa and Edwin Muir, was published in 1937. In 1999, the book was listed in "Le Monde"'s 100 Books of the Century and as No. 2 of the Best German Novels of the Twentieth Century.
Plot.
On his thirtieth birthday, the chief financial officer of a bank, Josef K., is unexpectedly arrested by two unidentified agents from an unspecified agency for an unspecified crime. The agents' boss later arrives and holds a mini-tribunal in the room of K.'s neighbor, Fräulein Bürstner. K. is not taken away, however, but left "free" and told to await instructions from the Committee of Affairs. He goes to work, and that night apologizes to Fräulein Bürstner for the intrusion into her room. At the end of the conversation he suddenly kisses her.
K. receives a phone call summoning him to court, and the coming Sunday is arranged as the date. No time is set, but the address is given to him. The address turns out to be a huge tenement building. K. has to explore to find the court, which turns out to be in the attic. The room is airless, shabby and crowded, and although he has no idea what he is charged with, or what authorizes the process, K. makes a long speech denigrating the whole process, including the agents who arrested him; during this speech an attendant's wife and a man engage in sexual activities. K. then returns home.
K. later goes to visit the court again, although he has not been summoned, and finds that it is not in session. He instead talks with the attendant's wife, who attempts to seduce him into taking her away, and who gives him more information about the process and offers to help him. K. later goes with the attendant to a higher level of the attic where the shabby and airless offices of the court are housed.
K. returns home to find Fräulein Montag, a lodger from another room, moving in with Fräulein Bürstner. He suspects that this is to prevent him from pursuing his affair with the latter woman. Yet another lodger, Captain Lanz, appears to be in league with Montag.
Later, in a store room at his own bank, K. discovers the two agents who arrested him being whipped by a flogger for asking K. for bribes and as a result of complaints K. made at court. K. tries to argue with the flogger, saying that the men need not be whipped, but the flogger cannot be swayed. The next day he returns to the store room and is shocked to find everything as he had found it the day before, including the whipper and the two agents.
K. is visited by his uncle, who was K.'s guardian. The uncle seems distressed by K.'s predicament. At first sympathetic, he becomes concerned that K. is underestimating the seriousness of the case. The uncle introduces K. to a lawyer, who is attended by Leni, a nurse, who K.'s uncle suspects is the advocate's mistress. During the discussion it becomes clear how different this process is from regular legal proceedings: guilt is assumed, the bureaucracy running it is vast with many levels, and everything is secret, from the charge, to the rules of the court, to the authority behind the courts – even the identity of the judges at the higher levels. The attorney tells him that he can prepare a brief for K., but since the charge is unknown and the rules are unknown, it is difficult work. It also never may be read, but is still very important. The lawyer says that his most important task is to deal with powerful court officials behind the scenes. As they talk, the lawyer reveals that the Chief Clerk of the Court has been sitting hidden in the darkness of a corner. The Chief Clerk emerges to join the conversation, but K. is called away by Leni, who takes him to the next room, where she offers to help him and seduces him. They have a sexual encounter. Afterwards K. meets his uncle outside, who is angry, claiming that K.'s lack of respect has hurt K.'s case.
K. visits the lawyer several times. The lawyer tells him incessantly how dire his situation is and tells many stories of other hopeless clients and of his behind-the-scenes efforts on behalf of these clients, and brags about his many connections. The brief is never complete. K.'s work at the bank deteriorates as he is consumed with worry about his case.
K. is surprised by one of his bank clients, who tells K. that he is aware that K. is dealing with a trial. The client learned of K.'s case from Titorelli, a painter, who has dealings with the court and told the client about K.'s case. The client advises K. to go to Titorelli for advice. Titorelli lives in the attic of a tenement in a suburb on the opposite side of town from the court that K. visited. Three teenage girls taunt K. on the steps and tease him sexually. Titorelli turns out to be an official painter of portraits for the court (an inherited position), and has a deep understanding of the process. K. learns that, to Titorelli's knowledge, not a single defendant has ever been acquitted. He sets out K.'s options and offers to help K. with either. The options are: obtain a provisional verdict of innocence from the lower court, which can be overturned at any time by higher levels of the court, which would lead to re-initiation of the process; or curry favor with the lower judges to keep the process moving at a glacial pace. Titorelli has K. leave through a small back door, as the girls are blocking the door through which K. entered. To K.'s shock, the door opens into another warren of the court's offices – again shabby and airless.
K. decides to take control of matters himself and visits his lawyer with the intention of dismissing him. At the lawyer's office he meets a downtrodden individual, Block, a client who offers K. some insight from a client's perspective. Block's case has continued for five years and he has gone from being a successful businessman to being almost bankrupt and is virtually enslaved by his dependence on the lawyer and Leni, with whom he appears to be sexually involved. The lawyer mocks Block in front of K. for his dog-like subservience. This experience further poisons K.'s opinion of his lawyer. (This chapter was left unfinished by the author.)
K. is asked by the bank to show an Italian client around local places of cultural interest, but the Italian client, short of time, asks K. to take him only to the cathedral, setting a time to meet there. When the client does not show up, K. explores the cathedral, which is empty except for an old woman and a church official. K. notices a priest who seems to be preparing to give a sermon from a small second pulpit, and K. begins to leave, lest it begin and K. be compelled to stay for its entirety. Instead of giving a sermon, the priest calls out K.'s name. K. approaches the pulpit and the priest berates him for his attitude toward the trial and for seeking help, especially from women. K. asks him to come down and the two men walk inside the cathedral. The priest works for the court as a chaplain and tells K. a fable (which was published earlier as "Before the Law") that is meant to explain his situation. K. and the priest discuss the parable. The priest tells K. that the parable is an ancient text of the court, and many generations of court officials have interpreted it differently.
On the eve of K.'s thirty-first birthday, two men arrive at his apartment. He has been waiting for them, and he offers little resistance – indeed the two men take direction from K. as they walk through town. K. leads them to a quarry where the two men place K's head on a discarded block. One of the men produces a double-edged butcher knife, and as the two men pass it back and forth between them, the narrator tells us that "K. knew then precisely, that it would have been his duty to take the knife...and thrust it into himself." He does not take the knife. One of the men holds his shoulder and pulls him up and the other man stabs him in the heart and twists the knife twice. K.'s last words are: "Like a dog!".
Characters.
Josef K. – The tale's protagonist.
Fräulein Bürstner – A boarder in the same house as Josef K. She lets him kiss her one night, but then rebuffs his advances. K. briefly catches sight of her, or someone who looks similar to her, in the final pages of the novel.
Fräulein Montag – Friend of Fräulein Bürstner, she talks to K. about ending his relationship with Fräulein Bürstner after his arrest. She claims she can bring him insight, because she is an objective third party.
Willem and Franz – Officers who arrest K. one morning but refuse to disclose the crime he is said to have committed.
Inspector – Man who conducts a proceeding at Josef K.'s boardinghouse to inform K. officially that he is under arrest.
Rabinsteiner, Kullich and Kaminer – Junior bank employees who attend the proceeding at the boardinghouse.
Frau Grubach – The proprietress of the lodging house in which K. lives. She holds K. in high esteem, despite his arrest.
Woman in the Court – In her house happens the first judgment of K. She claims help from K. because she doesn't want to be abused by the magistrates.
Student – Deformed man who acts under orders of the instruction judge. Will be a powerful man in the future.
Instruction Judge – First Judge of K. In his trial, he confuses K. with a Wall Painter.
Uncle Karl – K.'s impetuous uncle from the country, formerly his guardian. Upon learning about the trial, Karl insists that K. hire Herr Huld, the lawyer.
Herr Huld, the Lawyer – K.'s pompous and pretentious advocate who provides precious little in the way of action and far too much in the way of anecdote.
Leni – Herr Huld's nurse, she has feelings for Josef K. and soon becomes his lover. She shows him her webbed hand, yet another reference to the motif of the hand throughout the book. Apparently, she finds accused men extremely attractive—the fact of their indictment makes them irresistible to her.
Albert – Office director at the court and a friend of Huld.
Flogger – Man who punishes Franz and Willem in the Bank after K's. complaints against the two agents in his first Judgement.
Vice-President – K.'s unctuous rival at the Bank, only too willing to catch K. in a compromising situation. He repeatedly takes advantage of K.'s preoccupation with the trial to advance his own ambitions.
President – Manager of the Bank. A sickly figure, whose position the Vice-President is trying to assume. Gets on well with K., inviting him to various engagements.
Rudi Block, the Merchant – Block is another accused man and client of Huld. His case is five years old, and he is but a shadow of the prosperous grain dealer he once was. All his time, energy, and resources are now devoted to his case, to the point of detriment to his own life. Although he has hired five additional lawyers on the side, he is completely and pathetically subservient to Huld.
Manufacturer – Person who hears about K.'s case and advises him to see a painter who knows how the court system works.
Titorelli, the Painter – Titorelli inherited the position of Court Painter from his father. He knows a great deal about the comings and goings of the Court's lowest level. He offers to help K., and manages to unload a few identical landscape paintings on the accused man.
Priest – Prison chaplain whom K. encounters in a church. The priest advises K. that his case is going badly and tells him to accept his fate.
Doorkeeper and Farmer – The characters of the Chaplain's Tale.
Legality metaphors.
In a study based on Kafka's office writings, Reza Banakar points out that many of Kafka’s descriptions of law and legality are often treated as metaphors for things other than law, but also are worthy of examination as a particular concept of law and legality which operates paradoxically as an integral part of the human condition under modernity. Josef K. and his inexplicable experience of the law in "The Trial" were, for example, influenced by an actual legal case in which Kafka was involved.
References.
Notes
Bibliography

</doc>
<doc id="10862" url="https://en.wikipedia.org/wiki?curid=10862" title="The Metamorphosis">
The Metamorphosis

The Metamorphosis (, also sometimes translated as The Transformation) is a novella by Franz Kafka, first published in 1915. It has been called one of the seminal works of fiction of the 20th century and is studied in colleges and universities across the Western world. 
The story begins with a traveling salesman, Gregor Samsa, waking to find himself transformed (metamorphosed) into a large, monstrous insect-like creature. The cause of Gregor's transformation is never revealed, and Kafka himself never gave an explanation. The rest of Kafka's novella deals with Gregor's attempts to adjust to his new condition as he deals with being burdensome to his parents and sister, who are repelled by the horrible, verminous creature Gregor has become.
Plot.
Part I.
One day, Gregor Samsa, a traveling game master, wakes up to find himself transformed into a giant insect (the most common translation of the German description "ungeheures Ungeziefer", literally "monstrous vermin"). He reflects on how dreary life as a traveling salesman is. As he looks at the wall clock, he notices that he has overslept and missed his train for work. He ponders the consequences of this delay. Gregor becomes annoyed at how his boss never accepts excuses or explanations from any of his employees no matter how hard-working they are, displaying an apparent lack of trusting abilities. Gregor's mother knocks on the door, and he answers her. She is concerned for Gregor because he is late for work, which is unorthodox for him. Gregor answers his mother and realizes that his voice has changed, but his answer is short, so his mother does not notice. His sister, Grete, to whom he was very close, then whispers through the door and begs him to open it. He tries to get out of bed but is incapable of moving his body. While trying to move, he finds that his office manager, the chief clerk, has shown up to check on him. He finally rocks his body to the floor and calls out that he will open the door shortly.
Offended by Gregor's delayed response in opening the door, the clerk warns him of the consequences of missing work. He adds that Gregor's recent performance has been unsatisfactory. Gregor disagrees and tells him that he will open the door shortly. Nobody on the other side of the door has understood a single word he had uttered (Gregor is unaware that his voice has also transformed), and they conclude that he is seriously ill. Finally, Gregor manages to unlock and open the door with his mouth. He apologizes to the office manager for the delay. Horrified by Gregor's appearance, his mother faints, and the manager bolts out of the apartment. Gregor tries to catch up with him, but his father drives him back into the bedroom with a cane and a rolled newspaper. Gregor injures himself squeezing back through the doorway, and his father slams the door shut. Gregor, exhausted, falls asleep.
Part II.
Gregor awakens and sees that someone has put milk and bread in his room. Initially excited, he quickly discovers that he has no taste for milk, once one of his favorites. He settles himself under a couch. The next morning, his sister comes in, sees that he has not touched the milk, and replaces it with rotting food scraps, which Gregor happily eats. This begins a routine in which his sister feeds him and cleans up while he hides under the couch, afraid that his appearance will frighten her. Gregor spends his time listening through the wall to his family members talking. They often discuss the difficult financial situation they find themselves in now that Gregor can't provide for them. Gregor had plans of sending Grete to the conservatory to pursue violin lessons, something everyone else – including Grete – considered a dream. His incapability of providing for his family, coupled with his speechlessness, reduces his thought process greatly. Gregor also learns that his mother wants to visit him, but his sister and father will not let her.
Gregor grows more comfortable with his changed body. He begins climbing the walls and ceiling for amusement. Discovering Gregor's new pastime, Grete decides to remove some of the furniture to give Gregor more space. She and her mother begin taking furniture away, but Gregor finds their actions deeply distressing. He tries to save a picture on the wall of a woman wearing a fur hat, fur scarf, and fur muff. Gregor's mother sees him hanging on the wall and passes out. Greta calls out to Gregor – the first time anyone has spoken directly to him since his transformation. Gregor runs out of the room and into the kitchen. He encounters his father, who has just returned home from work. The father throws apples at Gregor, and one of them sinks into a sensitive spot in his back and remains lodged there, paralyzing his movements for a month and damaging him permanently. Gregor manages to get back into his bedroom but is severely injured.
Part III.
One evening, the cleaning lady leaves Gregor's door open while three boarders, whom the family has taken on for additional income, lounge about the living room. Grete has been asked to play the violin for them, and Gregor – who usually took care to avoid crossing paths with anyone in the flat – in the midst of his depression and resultant detachment, creeps out of his bedroom to listen. The boarders, who initially seemed interested in Grete, grow bored with her performance, but Gregor is transfixed by it. One of the boarders spots Gregor, and the rest become alarmed. Gregor's father tries to shove the boarders back into their rooms, but the three men protest and announce that they will move out immediately without paying rent because of the disgusting conditions in the apartment.
Grete, who has by now become tired of taking care of Gregor and is realizing the burden his existence puts on each one in the family, tells her parents they must get rid of Gregor, or they will all be ruined. Her father agrees, wishing Gregor could understand them and would leave of his own accord. Gregor does, in fact, understand and slowly moves back to the bedroom. There, determined to rid his family of his presence, Gregor dies.
Upon discovering Gregor is dead, the family feels a great sense of relief. The father kicks out the boarders and decides to fire the cleaning lady, who has disposed of Gregor's body. The family takes a trolley ride out to the countryside, during which they consider their finances. They decide to move to a smaller apartment to further save money, an act they were unable to carry out in Gregor's presence. During this short trip, Mr. and Mrs. Samsa realize that, in spite of going through hardships which have brought an amount of paleness to her face, Grete appears to have grown up into a pretty and well-figured lady, which leads her parents to think about finding her a husband.
Characters.
Gregor Samsa.
Gregor is the main character of the story. He works as a traveling salesman in order to provide money for his sister and parents. He wakes up one morning finding himself transformed into an insect. After the metamorphosis, Gregor becomes unable to work and is confined to his room for the remainder of the story. This prompts his family to begin working once again.
The name "Gregor Samsa" appears to derive partly from literary works Kafka had read. The hero of "The Story of Young Renate Fuchs", by German-Jewish novelist Jakob Wassermann (1873–1934), is a certain Gregor Samsa. The Viennese author Leopold von Sacher-Masoch, whose sexual imagination gave rise to the idea of masochism, is also an influence. Sacher-Masoch (note the letters Sa-Mas) wrote "Venus in Furs" (1870), a novel whose hero assumes the name Gregor at one point. A "Venus in furs" literally recurs in "The Metamorphosis" in the picture that Gregor Samsa has hung on his bedroom wall. The name Samsa is similar to "Kafka" in its play of vowels and consonants: "Five letters in each word. The S in the word Samsa has the same position as the K in the word Kafka. The A is in the second and fifth positions in both words."
Grete Samsa.
Grete is Gregor's younger sister, who becomes his caretaker after his metamorphosis. Initially Grete and Gregor have a close relationship, but this quickly fades. While Grete initially volunteers to feed him and clean his room, she grows increasingly impatient with the burden and begins to leave his room in disarray out of spite. She plays the violin and dreams of going to the conservatory, a dream Gregor had intended to make happen; Gregor had planned on making the announcement on Christmas Eve. To help provide an income for the family after Gregor's transformation, she starts working as a salesgirl.
Mr. Samsa.
Mr. Samsa is Gregor's father. After the metamorphosis, he is forced to return to work in order to support the family financially. His attitude towards his son is harsh; he regards the transformed Gregor with disgust and possibly even fear.
Mrs. Samsa.
Mrs. Samsa is Grete and Gregor's mother. She is initially shocked at Gregor's transformation; however, she wants to enter his room. This proves too much for her, thus giving rise to a conflict between her maternal impulse and sympathy, and her fear and revulsion at Gregor's new form.
Translation.
Kafka's sentences often deliver an unexpected impact just before the period – that being the finalizing meaning and focus. This is achieved from the construction of sentences in the original German, where the verbs of subordinate clauses are put at the end. For example, in the opening sentence, it is the final word, "verwandelt", that indicates transformation:
These constructions are not directly replicable in English, so it is up to the translator to provide the reader with the effect of the original text.
English translators have often sought to render the word "Ungeziefer" as "insect", but this is not strictly accurate. In Middle High German, "Ungeziefer" literally means "unclean animal not suitable for sacrifice" and is sometimes used colloquially to mean "bug" – a very general term, unlike the scientific sounding "insect". Kafka had no intention of labeling Gregor as any specific thing, but instead wanted to convey Gregor's disgust at his transformation. The phrasing used by Joachim Neugroschel is: "Gregor Samsa found himself, in his bed, transformed into a monstrous vermin", whereas David Wyllie says" "transformed in his bed into a horrible vermin".
However, in Kafka's letter to his publisher of 25 October 1915, in which he discusses his concern about the cover illustration for the first edition, he uses the term "Insekt", saying: "The insect itself is not to be drawn. It is not even to be seen from a distance."
"Ungeziefer" has sometimes been translated as "cockroach", "dung beetle", "beetle", and other highly specific terms. The term "dung beetle" or "Mistkäfer" is, in fact, used by the cleaning lady near the end of the story, but it is not used in the narration. "Ungeziefer" also denotes a sense of separation between himself and his environment: he is unclean and must therefore be secluded.
Vladimir Nabokov, who was a lepidopterist as well as writer and literary critic, insisted that Gregor was not a cockroach, but a beetle with wings under his shell, and capable of flight. Nabokov left a sketch annotated, "just over three feet long", on the opening page of his (heavily corrected) English teaching copy. In his accompanying lecture notes, Nabokov discusses the type of insect Gregor has been transformed into, concluding that Gregor "is not, technically, a dung beetle. He is merely a big beetle".
Adaptations to other media.
Film.
There are many film versions of the story, mostly short films, including:
External links.
Online editions
Commentary

</doc>
<doc id="10865" url="https://en.wikipedia.org/wiki?curid=10865" title="FSF">
FSF

FSF may refer to:

</doc>
<doc id="10868" url="https://en.wikipedia.org/wiki?curid=10868" title="Francisco Goya">
Francisco Goya

Francisco José de Goya y Lucientes (; (30 March 1746 – 16 April 1828) was a Spanish romantic painter and printmaker. He is considered the most important Spanish artist of late 18th and early 19th centuries and throughout his long career was a commentator and chronicler of his era. Immensely successful in his lifetime, Goya is often referred to as both the last of the Old Masters and the first of the moderns.
He was born to a modest family in 1746 in the village of Fuendetodos in Aragon. He studied painting from age 14 under José Luzán y Martinez and moved to Madrid to study with Anton Raphael Mengs. He married Josefa Bayeu in 1775; the couple's life together was characterised by an almost constant series of pregnancies and miscarriages. He became a court painter to the Spanish Crown in 1786 and the early portion of his career is marked by portraits commissioned by the Spanish aristocracy and royalty, as well as the Rococo style tapestry cartoons designed for the royal palace.
Goya was a guarded man and although letters and writings survive, we know comparatively little about his thoughts. He suffered a severe and undiagnosed illness in 1793 which left him completely deaf. After 1793 his work became progressively darker and pessimistic. His later easel and mural paintings, prints and drawings appear to reflect a bleak outlook on personal, social and political levels, and contrast with his social climbing. He was appointed "Director of the Royal Academy" in 1795, the year Manuel Godoy made an unfavorable treaty with France. In 1799 Goya became "Primer Pintor de Càmara", the then highest rank for a Spanish court painter. In the late 1790s, commissioned by Godoy, he completed his "La maja desnuda", a remarkably daring nude for the time and clearly indebted to Diego Velázquez. In 1801 he painted "Charles IV of Spain and His Family". In 1807 Napoleon led the French army into Spain.
He remained in Madrid during the Peninsular War, which seems to have affected him deeply. Although he did not vocalise his thoughts in public, they can be inferred from his "Disasters of War" series of prints (although published 35 years after his death) and his 1814 paintings "The Second of May 1808" and "The Third of May 1808". Other works from his mid period include the "Caprichos" and "Los Disparates" etching series, and a wide variety of paintings concerned with insanity, mental asylums, witches, fantastical creatures and religious and political corruption, all of which suggest that he feared for both his country's fate and his own mental and physical health. His output culminates with the so-called "Black Paintings" of 1819-1823, applied on oil on the plaster walls of his house the "Quinta del Sordo" ("house of the deaf man") where, disillusioned by domestic political and social developments he lived in near isolation. Goya eventually abandoned Spain in 1824 to retire to the French city of Bordeaux, accompanied by his much younger maid and companion, Leocadia Weiss, who may or may not have been his lover. There he completed his "La Tauromaquia" series and a number of canvases. Following a stroke which left him paralysed on his right side, and suffering failing eyesight and poor access to painting materials, he died and was buried on 16 April 1828 aged 82. His body was later re-interred in Spain.
Early years (1746-1771).
Francisco Goya was born in Fuendetodos, Aragón, Spain, on 30 March 1746 to José Benito de Goya y Franque and Gracia de Lucientes y Salvador. The family had moved that year from the city of Zaragoza, but there is no record why; likely José was commissioned to work there. They were lower middle-class, José was the son of a notary and of Basque origin, earning his living as a gilder, specialising in religious and decorative craftwork. He oversaw the gilding and most of the ornamentation during the rebuilding of the Basilica of Our Lady of the Pillar ("Santa Maria del Pilar"), the principal cathedral of Zaragoza. Francisco was their fourth child, following his sister Rita (b. 1737), brother Tomás (b. 1739) (who was to follow in his father's trade) and second sister Jacinta (b. 1743). There were two later sons, Mariano (b. 1750) and Camilo (b. 1753).
His mother's family had pretensions of nobility and the house, a modest brick cottage, was owned by her family and, perhaps fancifully, bore their crest. About 1749 José and Gracia bought a home in Zaragoza and were able to return to live in the city. Although there are no surviving records it is thought that Goya may have attended the Escuelas Pías de San Antón, which offered free schooling. His education seems to have been adequate but not enlightening; he had reading, writing and numeracy, and some knowledge of the classics. According to art Critic Robert Hughes the artist "seems to have taken no more interest than a carpenter in philosophical or theological matters, and his views on painting ... were very down to earth: Goya was no theoretician. At school he formed a close and lifelong friendship with fellow pupil Martin Zapater; the 131 letters Goya wrote to him from 1775 until Zapater's death in 1801 give valuable insight into Goya's early years at the court in Madrid.
Visit to Italy.
At age 14 Goya studied under the painter José Luzán, in Luzán's workshop he copied stamps for 4 years until he decided to work on his own, as he wrote later on "paint from my invention". He moved to Madrid where he studied with Anton Raphael Mengs, a painter who was popular with Spanish royalty. He clashed with his master, and his examinations were unsatisfactory. Goya submitted entries for the Real Academia de Bellas Artes de San Fernando in 1763 and 1766, but was denied entrance.
Italy at the time was the cultural capital of Europe and held all the prototypes of classical antiquity, while Spain lacked a coherent artistic direction, with all of its significant visual achievements in the past. Having failed to earn a scholarship, Goya relocated at his own expense to Rome in the old tradition of European artists stretching back to at least to Albrecht Dürer. He was an unknown at the time and so the records are scant and uncertain. Early biographers have him travelling to Rome with a gang of bullfighters, where he worked as a street acrobat, or for a Russian diplomat, or fell in love with beautiful young nun whom he plotted to abduct from her convent. What is more certain is two surviving mythological painting completed during the visit, a "Sacrifice to Vesta" and a "Sacrifice to Pan", both dated 1771.
In 1771 he won second prize in a painting competition organized by the City of Parma. That year he returned to Saragossa and painted parts of the cupolas of the Basilica of the Pillar (including "Adoration of the Name of God"), a cycle of frescoes in the monastic church of the Charterhouse of Aula Dei, and the frescoes of the Sobradiel Palace. He studied with the Aragónese artist Francisco Bayeu y Subías and his painting began to show signs of the delicate tonalities for which he became famous.
Goya befriended Francisco Bayeu, and married Bayeu's sister Josefa (he nicknamed her "Pepa") on 25 July 1773 and they had their first child, Antonio Juan Ramon Carlos, on 29 August 1774.
Madrid (1775-1789).
The marriage and Francisco Bayeu's 1765 membership of the Real Academia de Bellas Artes de San Fernando and directorship of the tapestry works from 1777 helped Goya earn a commission for a series of tapestry cartoons for the Royal Tapestry Factory. Over five years he designed some 42 patterns, many of which were used to decorate and insulate the stone walls of El Escorial and the Palacio Real del Pardo, the residences of the Spanish monarchs. While designing tapestries was neither prestigious nor well paid, his cartoons are mostly popularist in a rococo style, and Goya used them to bring himself to wider attention.
The cartoons were not his only royal commissions, and were accompanied by a series of engravings, mostly copies after old masters such as Marcantonio Raimondi and Velázquez. Goya had a complicated relationship to the latter artist; while many of his contemporaries saw folly in Goya's attempts to copy and emulate him, he had access to a wide range of the long dead painter's works that had been contained in the royal collection. Nonetheless, etching was to become a medium that the young artist was to master, a medium that was to reveal both the true depths of his imagination and his political beliefs. His c 1779 etching of "The Garrotted Man" ("El agarrotado") was the largest work he had produced to date, and an obvious foreboding of his later "Disasters of War" series.
Goya was beset by illness and his condition was used against him by the contemporary art scene, which looked jealously upon any artist seen to be rising in stature. Some of the larger cartoons, such as "The Wedding", were more than 8 by 10 feet, and had proved a drain on his physical strength. Ever resourceful, Goya turned this misfortune around, claiming that his illness had allowed him the insight to produce works that were more personal and informal. However, he found the format limiting, it did not allow him capture complex colour shift or texture, and was unsuited to the impasto and glazing techniques he was by then applying to his painted works. The tapestries seems as comments on human types, fashion and fads.
Other works from the period include a canvas for the altar of the Church of San Francisco El Grande in Madrid, which led to his appointment as a member of the Royal Academy of Fine Art.
Court painter.
In 1783, the Count of Floridablanca, a favorite of Charles III of Spain, commissioned Goya to paint his portrait. He also became friends with Crown Prince Don Luis, and spent two summers with him, painting portraits of both the Infante and his family. During the 1780s, his circle of patrons grew to include the Duke and Duchess of Osuna, the King and other notable people of the kingdom whom he painted. In 1786, Goya was given a salaried position as painter to Charles III.
In 1789 he was appointed court painter to Charles IV. The following year he became First Court Painter with a salary of 50,000 reales and an allowance of 500 ducats for a coach. He painted the King and the Queen, royal family pictures, a portrait of the Spanish Prime Minister Manuel de Godoy and many other nobles. His portraits are notable for their disinclination to flatter, and in the case of "Charles IV of Spain and His Family", the lack of visual diplomacy is remarkable. Modern interpreters view the portrait as satirical; it is thought to reveal the corruption behind the rule of Charles IV. Under his reign his wife Louisa was thought to have had the real power, and thus Goya placed her at the center of the group portrait. From the back left of the painting one can see the artist himself looking out at the viewer, and the painting behind the family depicts Lot and his daughters, thus once again echoing the underlying message of corruption and decay.
Goya received commissions from the highest ranks of the Spanish nobility, including Pedro Téllez-Girón, 9th Duke of Osuna and his wife María Josefa Pimentel, 12th Countess-Duchess of Benavente, María del Pilar de Silva, 13th Duchess of Alba (universally known simply as the "Duchess of Alba"), and her husband José María Álvarez de Toledo, 15th Duke of Medina Sidonia, and María Ana de Pontejos y Sandoval, Marchioness of Pontejos. In 1801 he painted Godoy in a commission to commemorate the victory in the brief War of the Oranges against Portugal. The two were friends, even if Goya's "1801 portrait" is usually seen as satire. Yet even after Godoy's fall from grace the politician referred to the artist in warm terms. Godoy saw himself as instrumental in the publication of the Caprichos and is widely believed to have commissioned "La maja desnuda".
Mid period (1793-1799).
"La Maja Desnuda" ("La maja desnuda") was "the first totally profane life-size female nude in Western art" without pretense to allegorical or mythological meaning. The identity of the "Majas" is uncertain. The most popularly cited models are the Duchess of Alba, with whom Goya was sometimes thought to have had an affair, and Pepita Tudó, mistress of Manuel de Godoy. Neither theory has been verified, and it remains as likely that the paintings represent an idealized composite. The paintings were never publicly exhibited during Goya's lifetime and were owned by Godoy. In 1808 all Godoy's property was seized by Ferdinand VII after his fall from power and exile, and in 1813 the Inquisition confiscated both works as 'obscene', returning them in 1836 to the Academy of Fine Arts of San Fernando.
In 1798 he painted luminous and airy scenes for the pendentives and cupola of the Real Ermita (Chapel) of San Antonio de la Florida in Madrid. Many of these depict miracles of Saint Anthony of Padua set in the midst of contemporary Madrid.
At some time between late 1792 and early 1793 a undiagnosed illness left Goya deaf. He became withdrawn and introspective while the direction and tone of his work changed. He began the series of aquatinted etchings, published in 1799 as the "Caprichos"—completed in parallel with the more official commissions of portraits and religious paintings. In 1799 Goya published a 80 "Caprichos" prints depicting what he described as "the innumerable foibles and follies to be found in any civilized society, and from the common prejudices and deceitful practices which custom, ignorance, or self-interest have made usual". The visions in these prints are partly explained by the caption "The sleep of reason produces monsters". Yet these are not solely bleak in nature and demonstrate the artist's sharp satirical wit, particularly evident in etchings such as "Hunting for Teeth".
Goya's physical and mental breakdown seems to have happened a few weeks after the French declaration of war on Spain. A contemporary reported, "The noises in his head and deafness aren’t improving, yet his vision is much better and he is back in control of his balance." These symptoms may indicate a prolonged viral encephalitis, or possibly a series of miniature strokes resulting from high blood pressure and which affected the hearing and balance centers of the brain. The triad of tinnitus, episodes of imbalance, and progressive deafness are also typical of Ménière's disease. It is even possible that Goya suffered from cumulative lead poisoning, as he used massive amounts of lead white in his paintings, both as a canvas primer and as a primary color.
Other postmortem diagnostic assessments point toward paranoid dementia due to an unknown brain trauma (perhaps resulting from the unknown illness which he reported). If this is the case, from here on we see an insidious assault on his faculties manifesting as paranoid features in his paintings, and culminating in his black paintings, especially "Saturn Devouring His Sons". Yet Goya could transform his personal demons into horrific and fantastic imagery that speaks universally, and allows his audience to find its own catharsis in these images.
Peninsular War (1808–1814).
French forces invaded Spain in 1808, leading to the Peninsular War of 1808–1814. The extent of Goya's involvement with the court of the "Intruder king", Joseph I, the brother of Napoleon Bonaparte, is not known; he painted works for French patrons and sympathisers, but kept neutral during the fighting. After the restoration of the Spanish king, Ferdinand VII, in 1814, Goya denied any involvement with the French. When his wife Josefa died in 1812, he was mentally and emotionally processing the war by painting "The Second of May 1808" and "The Third of May 1808", and preparing the series of prints later known as "The Disasters of War" ("Los desastres de la guerra"). Ferdinand VII returned to Spain in 1814 but relations with Goya were not cordial. He painted portraits of the king for a variety of organizations, but not for the king himself
During a period of convalescence during 1793–1794, Goya completed a set of eleven small pictures painted on tin. Known as "Fantasy and Invention", they mark a significant change in the tone and subject matter of his art. They draw from dark and dramatic realms of fantasy nightmare. "Yard with Lunatics" is a horrifying, imaginary vision of loneliness, fear and social alienation. The condemnation of brutality towards prisoners (whether criminal or insane) is a subject that Goya assayed in later works that focused on the degradation of the human figure. It was one of the first of Goya's mid-1790s cabinet paintings, in which his earlier search for ideal beauty gave way to an examination of the relationship between naturalism and fantasy that would preoccupy him for the rest of his career. He was undergoing a nervous breakdown and entering prolonged physical illness, and admitted that the series was created to reflect his own self-doubt, anxiety and fear that he himself was going mad. Goya wrote that the works served "to occupy my imagination, tormented as it is by contemplation of my sufferings." The series, he said, consisted of pictures which "normally find no place in commissioned works."
Although he did not make known his intention when creating the aquatint plates of the "The Disasters of War" in the 1810s, art historians view them as a visual protest against the violence of the 1808 Dos de Mayo Uprising, the subsequent Peninsular War and the setbacks to the liberal cause following the restoration of the Bourbon monarchy in 1814. The scenes are singularly disturbing, sometimes macabre in their depiction of battlefield horror, and represent an outraged conscience in the face of death and destruction. They were not published until 1863, 35 years after his death. It is likely that only then was it considered politically safe to distribute a sequence of artworks criticising both the French and restored Bourbons.
The first 47 plates in the series focus on incidents from the war and show the consequences of the conflict on individual soldiers and civilians. The middle series (plates 48 to 64) record the effects of the famine that hit Madrid in 1811–12, before the city was liberated from the French. The final 17 reflect the bitter disappointment of liberals when the restored Bourbon monarchy, encouraged by the Catholic hierarchy, rejected the Spanish Constitution of 1812 and opposed both state and religious reform. Since their first publication, Goya's scenes of atrocities, starvation, degradation and humiliation have been described as the "prodigious flowering of rage".
Goya's works from 1814 to 1819 are mostly commissioned portraits, but also include the altarpiece of Santa Justa and Santa Rufina for the Cathedral of Seville, the print series of "La Tauromaquia" depicting scenes from bullfighting, and probably the etchings of "Los Disparates".
Quinta del Sordo (1819-1822).
The historical record of Goya's later life is relatively scant; no accounts of his thoughts from this time survive. He deliberately suppressed a number of his works from this period. Tormented by a dread of old age and fear of madness, the latter possibly from anxiety caused by an undiagnosed illness that left him deaf from the early 1790s. Goya had been a successful and royally placed artist, but withdrew from public life during his final years. From the late 1810s he lived in near-solitude outside Madrid in a farmhouse converted into a studio. The house had become known as "La Quinta del Sordo" (The House of the Deaf Man), after the nearest farmhouse had coincidentally also belonged to a deaf man.
Art historians assume Goya felt alienated from the social and political trends that followed the 1814 restoration of the Bourbon monarchy, and that he viewed these developments as reactionary means of social control. In his unpublished art he seems to have railed against what he saw as a tactical retreat into Medievalism. It is thought that he had hoped for political and religious reform, but like many liberals became disillusioned when the restored Bourbon monarchy and Catholic hierarchy rejected the Spanish Constitution of 1812.
At the age of 75, alone and in mental and physical despair, he completed the work as one of his 14 "Black Paintings", all of which were executed in oil directly onto the plaster walls of his house. Goya did not intend for the paintings to be exhibited, did not write of them, and likely never spoke of them. It was not until around 1874, some 50 years after his death, that they were taken down and transferred to a canvas support. Many of the works were significantly altered during the restoration, and in the words of Arthur Lubow what remain are "at best a crude facsimile of what Goya painted." The effects of time on the murals, coupled with the inevitable damage caused by the delicate operation of mounting the crumbling plaster on canvas, meant that most of the murals suffered extensive damage and loss of paint. Today they are on permanent display at the Museo del Prado, Madrid.
Bordeaux (October 1824-1828).
Leocadia Weiss (née Zorrilla, b. 1790) the artist's maid, younger by 35 years, and a distant relative, lived with and cared for Goya after Bayeu's death. She stayed with him in his Quinta del Sordo villa until 1824 with her daughter Rosario. Leocadia was probably similar in features to Goya's first wife Josefa Bayeu, to the point that one of his well known portraits bears the cautious title of "Josefa Bayeu (or Leocadia Weiss)".
Not much is known about her beyond her fiery temperament. She was likely related to the Goicoechea family, a wealthy dynasty into which the artist's son, the feckless Javier, had married. It is believed she held liberal political views and was unafraid of expressing them, a fact met with disapproval by Goya's family. It is known that Leocadia had an unhappy marriage with a jeweler, Isideo Weiss, but was separated from him since 1811. Her husband cited "illicit conduct" during the divorce proceedings. She had two children before the marriage dissolved, and bore a third, Rosario, in 1814 when she was 26. Isideo was not the father, and it has often been speculated—although with little firm evidence—that the child belonged to Goya. There has been much speculation that Goya and Weiss were romantically linked, however, it is more likely the affection between them was sentimental.
Leocadia was left nothing in Goya's will; mistresses were often omitted in such circumstances, but it is also likely that he did not want to dwell on his mortality by thinking about or revising his will. She wrote to a number of Goya's friends to complain of her exclusion but many of her friends were Goya's also and by then were old men or had died, and did not reply. Largely destitute she moved into rented accommodation and passed on her copy of the "Caprichos" for free.

</doc>
<doc id="10869" url="https://en.wikipedia.org/wiki?curid=10869" title="Frequentist probability">
Frequentist probability

Frequentist probability or frequentism is a standard interpretation of probability; it defines an event's probability as the limit of its relative frequency in a large number of trials. This interpretation supports the statistical needs of experimental scientists and pollsters; probabilities can be found (in principle) by a repeatable objective process (and are thus ideally devoid of opinion). It does not support all needs; gamblers typically require estimates of the odds without experiments.
The development of the frequentist account was motivated by the problems and paradoxes of the previously dominant viewpoint, the classical interpretation. In the classical interpretation, probability was defined in terms of the principle of indifference, based on the natural symmetry of a problem, so, "e.g." the probabilities of dice games arise from the natural symmetric 6-sidedness of the cube. This classical interpretation stumbled at any statistical problem that has no natural symmetry for reasoning.
Definition.
In the frequentist interpretation, probabilities are discussed only when dealing with well-defined random experiments (or random samples). The set of all possible outcomes of a random experiment is called the sample space of the experiment. An event is defined as a particular subset of the sample space to be considered. For any given event, only one of two possibilities may hold: it occurs or it does not. The relative frequency of occurrence of an event, observed in a number of repetitions of the experiment, is a measure of the probability of that event. This is the core conception of probability in the frequentist interpretation.
Thus, if formula_1 is the total number of trials and formula_2 is the number of trials where the event formula_3 occurred, the probability formula_4 of the event occurring will be approximated by the relative frequency as follows:
Clearly, as the number of trials is increased, one might expect the relative frequency to become a better approximation of a "true frequency".
A claim of the frequentist approach is that in the "long run," as the number of trials approaches infinity, the relative frequency will converge "exactly" to the true probability:
Scope.
The frequentist interpretation is a philosophical approach to the definition and use of probabilities; it is one of several such approaches. It does not claim to capture all connotations of the concept 'probable' in colloquial speech of natural languages.
As an interpretation, it is not in conflict with the mathematical axiomatization of probability theory; rather, it provides guidance for how to apply mathematical probability theory to real-world situations. It offers distinct guidance in the construction and design of practical experiments, especially when contrasted with the Bayesian interpretation. As to whether this guidance is useful, or is apt to mis-interpretation, has been a source of controversy. Particularly when the frequency interpretation of probability is mistakenly assumed to be the only possible basis for frequentist inference. So, for example, a list of mis-interpretations of the meaning of p-values accompanies the article on p-values; controversies are detailed in the article on statistical hypothesis testing. The Jeffreys–Lindley paradox shows how different interpretations, applied to the same data set, can lead to different conclusions about the 'statistical significance' of a result.
As William Feller noted:
Feller's comment was criticism of Laplace, who published a solution to the sunrise problem using an alternative probability interpretation. Despite Laplace's explicit and immediate disclaimer in the source, based on expertise in astronomy as well as probability, two centuries of criticism have followed.
History.
The frequentist view may have been foreshadowed by Aristotle, in "Rhetoric", when he wrote:
Poisson clearly distinguished between objective and subjective probabilities in 1837. Soon thereafter a flurry of nearly simultaneous publications by Mill, Ellis ("On the Foundations of the Theory of Probabilities" and "Remarks on the Fundamental Principles of the Theory of Probabilities"), Cournot ("Exposition de la théorie des chances et des probabilités") and Fries introduced the frequentist view. Venn provided a thorough exposition ("The Logic of Chance: An Essay on the Foundations and Province of the Theory of Probability" (published editions in 1866, 1876, 1888)) two decades later. These were further supported by the publications of Boole and Bertrand. By the end of the 19th century the frequentist interpretation was well established and perhaps dominant in the sciences. The following generation established the tools of classical inferential statistics (significance testing, hypothesis testing and confidence intervals) all based on frequentist probability.
Alternatively, Jacob Bernoulli (AKA James or Jacques) understood the concept of frequentist probability and published a critical proof (the weak law of large numbers) posthumously in "1713". He is also credited with some appreciation for subjective probability (prior to and without Bayes theorem). Gauss and Laplace used frequentist (and other) probability in derivations of the least squares method a century later, a generation before Poisson. Laplace considered the probabilities of testimonies, tables of mortality, judgments of tribunals, etc. which are unlikely candidates for classical probability. In this view, Poisson's contribution was his sharp criticism of the alternative "inverse" (subjective, Bayesian) probability interpretation. Any criticism by Gauss and Laplace was muted and implicit. (Their later derivations did not use inverse probability.)
Major contributors to "classical" statistics in the early 20th century included Fisher, Neyman and Pearson. Fisher contributed to most of statistics and made significance testing the core of experimental science; Neyman formulated confidence intervals and contributed heavily to sampling theory; Neyman and Pearson paired in the creation of hypothesis testing. All valued objectivity, so the best interpretation of probability available to them was frequentist. All were suspicious of "inverse probability" (the available alternative) with prior probabilities chosen by the using the principle of indifference. Fisher said, "...the theory of inverse probability is founded upon an error, eferring to Bayes theore and must be wholly rejected." (from his Statistical Methods for Research Workers). While Neyman was a pure frequentist, Fisher's views of probability were unique; Both had nuanced view of probability. von Mises offered a combination of mathematical and philosophical support for frequentism in the era.
Etymology.
According to the "Oxford English Dictionary", the term 'frequentist' was first used by M. G. Kendall in 1949, to contrast with Bayesians, whom he called "non-frequentists". He observed
"The Frequency Theory of Probability" was used a generation earlier as a chapter title in Keynes (1921).
The historical sequence: probability concepts were introduced and much of probability mathematics derived (prior to the 20th century), classical statistical inference methods were developed, the mathematical foundations of probability were solidified and current terminology was introduced (all in the 20th century). The primary historical sources in probability and statistics did not use the current terminology of classical, subjective (Bayesian) and frequentist probability.
Alternative views.
Probability theory is a branch of mathematics. While its roots reach centuries into the past, it reached maturity with the axioms of Andrey Kolmogorov in 1933. The theory focuses on the valid operations on probability values rather than on the initial assignment of values; the mathematics is largely independent of any interpretation of probability.
Applications and interpretations of probability are considered by philosophy, the sciences and statistics. All are interested in the extraction of knowledge from observations—inductive reasoning. There are a variety of competing interpretations; All have problems. Major interpretations include classical probability, subjective probability and frequency interpretations.
The frequentist interpretation does resolve difficulties with the classical interpretation, such as any problem where the natural symmetry of outcomes is not known. It does not address other issues, such as the dutch book. Propensity probability is an alternative physicalist approach.

</doc>
<doc id="10870" url="https://en.wikipedia.org/wiki?curid=10870" title="List of French-language poets">
List of French-language poets

List of poets who have written in the French language:

</doc>
<doc id="10871" url="https://en.wikipedia.org/wiki?curid=10871" title="FM-2030">
FM-2030

FM-2030 (October 15, 1930 – July 8, 2000) was an author, teacher, transhumanist philosopher, futurist, consultant and athlete. FM-2030 was born Fereidoun M. Esfandiary ().
He became notable as a transhumanist with the book "Are You a Transhuman?: Monitoring and Stimulating Your Personal Rate of Growth in a Rapidly Changing World", published in 1989. In addition, he wrote a number of works of fiction under his original name F.M. Esfandiary.
Early life and education.
The son of an Iranian diplomat, he travelled widely as a child, living in 17 countries by age 11; then, as a young man, he represented Iran as a basketball player at the 1948 Olympic Games in London and served on the United Nations Conciliation Commission for Palestine from 1952 to 1954.
Name change.
In the mid-1970s F.M. Esfandiary legally changed his name to FM-2030 for two main reasons. Firstly, to reflect the hope and belief that he would live to celebrate his 100th birthday in 2030; secondly, and more importantly, to break free of the widespread practice of naming conventions that he saw as rooted in a collectivist mentality, and existing only as a relic of humankind's tribalistic past. He viewed traditional names as almost always stamping a label of collective identity—varying from gender to nationality—on the individual, thereby existing as prima facie elements of thought processes in the human cultural fabric, that tended to degenerate into stereotyping, factionalism, and discrimination. In his own words, "Conventional names define a person's past: ancestry, ethnicity, nationality, religion. I am not who I was ten years ago and certainly not who I will be in twenty years. . The name 2030 reflects my conviction that the years around 2030 will be a magical time. In 2030 we will be ageless and everyone will have an excellent chance to live forever. 2030 is a dream and a goal."
Personal life.
He was a lifelong vegetarian and said he would not eat anything that had a mother. FM-2030 once said, "I am a 21st century person who was accidentally launched in the 20th. I have a deep nostalgia for the future." He taught at The New School, UCLA, and Florida International University. He worked as a corporate consultant for Lockheed and J.C. Penney.
Death.
On July 8, 2000, FM-2030 died from pancreatic cancer and was placed in cryonic suspension at the Alcor Life Extension Foundation in Scottsdale, Arizona, where his body remains today. He did not yet have remote standby arrangements, so no Alcor team member was present at his death, but FM-2030 was the first person to be vitrified, rather than simply frozen as previous cryonics patients had been. FM-2030 was survived by his four sisters and brother.

</doc>
<doc id="10874" url="https://en.wikipedia.org/wiki?curid=10874" title="West Flemish">
West Flemish

West Flemish (, ) is a dialect of the Dutch language spoken in western Belgium and adjoining parts of the Netherlands and France.
West Flemish is spoken by about a million people in the Belgian province of West Flanders, and a further 120,000 in the neighbouring Dutch coastal district of Zeelandic Flanders (and another 220,000 if Zealandic is included), and 10,000 in the northern part of the French "département" of Nord. Some of the main cities where West Flemish is widely spoken are Bruges, Kortrijk, Ostend, Roeselare, and Ypres. The dialects of the rest of the Dutch province of Zeeland, Zeelandic, are often included in West Flemish; these are part of a dialect continuum which proceeds further north into Hollandic.
West Flemish is listed as a "vulnerable" language in UNESCO's online Red Book of Endangered Languages.
Phonology.
West Flemish phonology differs a lot from the standard Dutch phonology.

</doc>
<doc id="10875" url="https://en.wikipedia.org/wiki?curid=10875" title="Fritz Leiber">
Fritz Leiber

Fritz Reuter Leiber, Jr. (December 24, 1910 – September 5, 1992) was an American writer of fantasy, horror, and science fiction. He was also a poet, actor in theater and films, playwright and chess expert. With writers such as Robert E. Howard and Michael Moorcock, Leiber can be regarded as one of the fathers of sword and sorcery fantasy, having in fact created the term. Moreover, he excelled in all fields of speculative fiction, writing award-winning work in fantasy, horror, and science fiction.
Life.
Leiber was born December 24, 1910, in Chicago, Illinois, to the actors Fritz Leiber, Sr., and Virginia Bronson Leiber and, for a time, he seemed inclined to follow in his parents' footsteps. (Theater and actors are prominently featured in his fiction.) He spent 1928 touring with his parents' Shakespeare company before studying philosophy at the University of Chicago, where he graduated with honors (1928–32). In 1932, he studied at the Anglican-affiliated General Theological Seminary and worked for a time as a lay preacher. In 1934, he toured with his parents' acting company, Fritz Leiber & Co. Six short stories in the 2010 collection "Strange Wonders: A Collection of Rare Fritz Leiber Works" carry 1934 and 1935 dates.
As well as being an actor and lay preacher, Leiber worked variously as a college teacher of drama, and a staff writer for an encyclopedia, and he tried free-lancing sporadically. He introduced Fafhrd and the Gray Mouser in the August 1939 number of "Unknown" magazine, edited by John W. Campbell.
He married Jonquil Stephens on January 16, 1936, and their son, Justin Leiber, was born in 1938. During World War II, he decided that the struggle against fascism was more important than his long-held pacifist convictions, and he accepted a job in aircraft production. After working on the staff of "Science Digest" for a dozen years, he wrote what Poul Anderson called "a lot of the best science fiction and fantasy in the business". Eventually, he moved from Chicago to southern California and started writing full-time.
In 1941 the family moved back to California, and Leiber began working as a speech and drama instructor at Occidental College in Los Angeles. After 1947, a year which marked his first hardcover publication with his collection "Night's Black Agents" (Arkham House), the family moved back to Chicago, where Leiber became editor of "Science Digest". In 1958 they returned to Los Angeles, where Leiber became a full-time writer.
Jonquil's death in 1969 precipitated a move to San Francisco, and three years as a drunk, but he returned to his original form with a fantasy novel set in modern-day San Francisco, "Our Lady of Darkness", which is about a writer of weird tales who must deal with the death of his wife and his recovery from alcoholism. Leiber seems to have suffered periods of penury in the 1970s; Harlan Ellison has written of his anger at finding that the much-awarded Leiber had to write his novels on a manual typewriter that was propped up over the sink in his apartment, and Marc Laidlaw wrote that, when visiting Leiber as a fan in 1976, he "was shocked to find him occupying one small room of a seedy San Francisco residence hotel, its squalor relieved mainly by walls of books". But other reports suggest that Leiber preferred to live simply in the city, spending his money on dining, movies and travel. In the last years of his life, royalty checks from TSR, the makers of "Dungeons and Dragons", who had licensed the mythos of the "Fafhrd and Gray Mouser" series, were enough in themselves to ensure that he lived comfortably.
In 1992, the last year of his life, Leiber married his second wife, Margo Skinner, a journalist and poet with whom he had been friends for many years. 
Leiber's death occurred a few weeks after a physical collapse while traveling from a science fiction convention in London, Ontario, with Skinner. The cause of his death was given as "organic brain disease".
He wrote a 100-page-plus autobiography, "Not Much Disorder and Not So Early Sex", which can be found in "The Ghost Light" (1984).
Leiber's own literary criticism, including several essays on Lovecraft, was collected in the volume "Fafhrd and Me" (1990).
Theater.
As the child of two Shakespearean actors—Fritz, Sr. and Virginia (née Bronson)—Leiber was fascinated with the stage, describing itinerant Shakespearean companies in stories like "No Great Magic" and "Four Ghosts in Hamlet," and creating an actor/producer protagonist for his novel "A Specter is Haunting Texas".
Although his "Change War" novel, "The Big Time", is about a war between two factions, the "Snakes" and the "Spiders", changing and rechanging history throughout the universe, all the action takes place in a small bubble of isolated space-time about the size of a theatrical stage, with only a handful of characters. Judith Merril (in the July 1969 issue of "The Magazine of Fantasy & Science Fiction") remarks on Leiber's acting skills when the writer won a science fiction convention costume ball. Leiber's costume consisted of a cardboard military collar over turned-up jacket lapels, cardboard insignia, an armband, and a spider pencilled large in black on his forehead, thus turning him into an officer of the Spiders, one of the combatants in his Change War stories. "The only other component," Merril writes, "was the Leiber instinct for theatre."
Films.
He can be seen as an actor in several movies.
He appears briefly with his father, the actor Fritz Leiber, Sr. in two films: the wedding-feast scene of Garbo's film "Camille" (1936) and in Warner Bros.' "The Great Garrick" (1937). Fritz Leiber, Jr. had a small uncredited speaking part in the "The Hunchback of Notre Dame" (1939) in which his father Fritz Leiber, Sr. had a credited part.
Due to the similarity of the names of the father and the son, some filmographies incorrectly attribute to Fritz, Jr roles which were in fact played by his father, Fritz Leiber, Sr. Fritz, Sr. was the evil Inquisitor in the Errol Flynn adventure film "The Sea Hawk" (1940) and had played in many other movies from 1917 onwards until the late 1950s. It is to be noted that it is the elder Leiber, not the younger, who appears in the Vincent Price vehicle "The Web" (1947) and in Charlie Chaplin's "Monsieur Verdoux" (1947).
In the cult horror film "Equinox" (1970) (a.k.a. "The Beast"; video title), directed by Jack Woods, Fritz, Jr. has a cameo appearance as the geologist Dr Watermann. In the edited second version of the movie Leiber has no spoken dialogue in the film but features in a few scenes. The original version of the movie has a longer appearance by Leiber recounting the ancient book and a brief speaking role, all of which was cut from the re-release of the film.
He also appears in the 1979 Schick Sunn Classics documentary "The Bermuda Triangle", based on the book by Charles Berlitz, as Chavez. Director:
Richard Friedenberg; Writers: Stephen Lord; Stars: Brad Crandall, Donald Albee and Lin Berlitz.
Surprisingly, Leiber's acting talents were not utilised for any of the movie versions of his novel "Conjure Wife" or for other screen adaptations of his work (see § Screen adaptations).
Writing career.
Leiber was heavily influenced by H. P. Lovecraft and Robert Graves in the first two decades of his career. Beginning in the late 1950s, he was increasingly influenced by the works of Carl Jung, particularly by the concepts of the anima and the shadow. From the mid-1960s onwards, he began incorporating elements of Joseph Campbell's "The Hero with a Thousand Faces". These concepts are often openly mentioned in his stories, especially the anima, which becomes a method of exploring his fascination with, but estrangement from, the female.
Leiber liked cats, which feature prominently in many of his stories. Tigerishka, for example, is a cat-like alien who is sexually attractive to the human protagonist yet repelled by human customs in the novel "The Wanderer". Leiber's "Gummitch" stories feature a kitten with an I.Q. of 160, just waiting for his ritual cup of coffee so that he can become human, too.
His first stories were inspired by H. P. Lovecraft's Cthulhu Mythos and it seems that a letter of encouragement from Lovecraft during 1936 spurred his decision to pursue a literary career. Leiber later wrote several essays on Lovecraft such as "A Literary Copernicus" which formed key moments in the serious critical appreciation of Lovecraft's life and work.
Leiber's first professional sale was "Two Sought Adventure" ("Unknown", August 1939), which introduced his most famous characters, Fafhrd and the Gray Mouser. His work as a writer earned much praise but little money, a problem exacerbated by bouts of alcoholism and an addiction to downers. In 1943, he sold his first novels, "Conjure Wife" to "Unknown" and "Gather, Darkness" to "Astounding". From 1945–56 Leiber was associate editor of "Science Digest".
1947 marked the publication of his first book, "Night's Black Agents", a short story collection containing seven stories grouped as 'Modern Horrors', one as a 'Transition', and two grouped as 'Ancient Adventures': "The Sunken Land" and "Adept's Gambit", which are both stories of Fafhrd and the Gray Mouser.
Book publication of the science fiction novel "Gather, Darkness" followed in 1950. It deals with a futuristic world that follows the Second Atomic Age which is ruled by scientists, until in the throes of a new Dark Age, the witches revolt.
In 1951 Leiber was Guest of Honour at the World Science Fiction Convention in New Orleans. Further novels followed during the 1950s, and in 1958 "The Big Time" won the Hugo Award for Best Novel.
Leiber published many further books in the 1960s. His novel "The Wanderer" (1964) won the Hugo Award for Best Novel. In the novel, an artificial planet, quickly nicknamed the Wanderer, materializes from hyperspace within earth's orbit. The Wanderer's gravitational field captures the moon and shatters it into something like one of Saturn's rings. On earth, the Wanderer's gravity well triggers massive earthquakes, tsunamis, and tidal phenomena. The multi-threaded plot follows the exploits of a large ensemble cast as they struggle to survive the global disaster.
Leiber was awarded three further Hugos for Best Novella/Novellette: for "Gonna Roll the Bones" (1967), (which also won the Nebula Award in the same category); "Ship of Shadows" (1969) and "Ill Met in Lankhmar" (1970).
"Our Lady of Darkness" (1977)— originally serialized in short form in "The Magazine of Fantasy & Science Fiction" under the title "The Pale Brown Thing" (1977)— featured cities as the breeding grounds for new types of elementals called paramentals, summonable by the dark art of megapolisomancy, with such activities centering on the Transamerica Pyramid. Its main characters include Franz Westen, Jaime Donaldus Byers, and the magician Thibault de Castries. "Our Lady of Darkness" won the World Fantasy Award.
Leiber also did the 1966 novelization of the Clair Huffaker screenplay of "Tarzan and the Valley of Gold".
Many of Leiber's most-acclaimed works are short stories, especially in the horror genre. Owing to such stories as "The Smoke Ghost," "The Girl With the Hungry Eyes" and "You're All Alone" (a.k.a. "The Sinful Ones"), he is widely regarded as one of the forerunners of the modern urban horror story. (Ramsey Campbell cites him as his single biggest influence.) Leiber also challenged the conventions of science fiction through reflexive narratives such as "A Bad Day For Sales" (first published in "Galaxy Science Fiction", July 1953), in which the protagonist, Robie, "America’s only genuine mobile salesrobot," references the title character of Isaac Asimov’s idealistic robot story, "Robbie." Questioning Isaac Asimov’sThree Laws of Robotics, Leiber imagines the futility of automatons in a post-apocalyptic New York City. In his later years, Leiber returned to short story horror in such works as "Horrible Imaginings", "Black Has Its Charms" and the award-winning "The Button Moulder."
The short parallel worlds story "Catch That Zeppelin!" (1975) added yet another Nebula and Hugo award to his collection. This story shows a plausible alternate reality that is much better than our own, whereas the typical parallel universe story depicts a world that is much worse than our own. "Belsen Express" (1975) won him another World Fantasy Award. Both stories reflect Leiber's uneasy fascination with Nazism—an uneasiness compounded by his mixed feelings about his German ancestry and his philosophical pacifism during World War II.
Leiber was named the second Gandalf Grand Master of Fantasy by participants in the 1975 World Science Fiction Convention (Worldcon), after the posthumous inaugural award to J. R. R. Tolkien. Next year he won the World Fantasy Award for Life Achievement. He was Guest of Honor at the 1979 Worldcon in Brighton, England (1979). The Science Fiction Writers of America made him its fifth SFWA Grand Master in 1981; the Horror Writers Association made him an inaugural winner of the Bram Stoker Award for Lifetime Achievement in 1988 (named in 1987); and the Science Fiction and Fantasy Hall of Fame inducted him in 2001, its sixth class of two deceased and two living writers.
Leiber was a founding member of the Swordsmen and Sorcerers' Guild of America, a loose-knit group of Heroic fantasy authors founded in the 1960s, led by Lin Carter, with entry by fantasy credentials alone. Some works by SAGA members were published in Lin Carter's "Flashing Swords!" anthologies. Leiber himself is credited with inventing the term sword and sorcery for the particular subgenre of epic fantasy exemplified by his Fafhrd and Grey Mouser stories.
In an appreciation in the July 1969 "Special Fritz Leiber Issue" of "The Magazine of Fantasy & Science Fiction", Judith Merril writes of Leiber's connection with his readers: "That this kind of "personal" response...is shared by thousands of other readers, has been made clear on several occasions." The November 1959 issue of "Fantastic", for instance: Leiber had just come out of one of his recurrent dry spells, and editor Cele Lalli bought up all his new material until there was enough ive storie to fill an issue; the magazine came out with a big black headline across its cover — "Leiber Is Back!"
Fafhrd and the Gray Mouser.
His legacy appears to have been consolidated by the most famous of his creations, the "Fafhrd and the Gray Mouser" stories, written over a span of 50 years. The first of them, "Two Sought Adventure", appeared in "Unknown", August 1939. They are concerned with an unlikely pair of heroes found in and around the city of Lankhmar. Fafhrd was based on Leiber himself and the Mouser on his friend Harry Otto Fischer, and the two characters were created in a series of letters exchanged by the two in the mid-1930s. These stories were among the progenitors of many of the tropes of the sword and sorcery genre. They are also notable among sword and sorcery stories in that, over the course of the stories, his two heroes mature, take on more responsibilities, and eventually settle down into marriage.
Some Fafhrd and Mouser stories were recognized by annual genre awards: "Scylla's Daughter" (1961) was "Short Story" Hugo finalist and "Ill Met in Lankhmar" (1970) won the "Best Novella" Hugo and Nebula Awards. Fittingly, Leiber's last major work, "The Knight and Knave of Swords" (1991) brought the series to a satisfactory close while leaving room for possible sequels. In the last year of his life, Leiber was considering allowing the series to be continued by other writers, but his sudden death made this more difficult. One new Fafhrd and the Mouser novel, "Swords Against the Shadowland", by Robin Wayne Bailey, did appear in 1998, and according to the author's web site, a second volume is in the works.
The stories were influential in shaping the genre and were influential on other works. Joanna Russ' stories about thief-assassin Alyx (collected in 1976 in "The Adventures of Alyx") were in part inspired by Fafhrd and the Gray Mouser, and Alyx in fact made guest appearances in two of Leiber's stories. Numerous writers have paid homage to the stories. For instance, Terry Pratchett's city of Ankh-Morpork bears something more than a passing resemblance to Lankhmar (acknowledged by Pratchett by the placing of the swordsman-thief "The Weasel" and his giant barbarian comrade "Bravd" in the opening scenes of the first Discworld novel). More recently, playing off the visit of Fafhrd and the Grey Mouser to our world in "Adept's Gambit" (set in second century B.C. Tyre), Steven Saylor's short story "Ill Seen in Tyre" takes his Roma Sub Rosa series hero Gordianus to the city of Tyre a hundred years later, where the two visitors from Nehwon are remembered as local legends.
Fischer and Leiber contributed to the original game design of the wargame "Lankhmar"—published in 1976 by TSR.
Selected works.
Screen adaptations.
"Conjure Wife" has been made into feature films three times under other titles:
A new film adaptation of "Conjure Wife" was announced in 2008, to be filmed by US director Billy Ray. It is slated to be a United Artists/Studio Canal co-production.
"The Girl With the Hungry Eyes" was filmed under that title by Kastenbaum Films in 1995. "The Girl with the Hungry Eyes". Director: Jon Jacobs; Writers:Christina Fulton (additional story and dialogue), Jon Jacobs; Stars:Christina Fulton, Isaac Turner and Leon Herbert. (This film is not to be confused with the 1967 William Rotsler film "The Girl with the Hungry Eyes" which is entirely unrelated to Leiber's story).
Two Leiber stories were filmed for TV for Rod Serling's "Night Gallery". These were "The Girl with the Hungry Eyes" (1970) (adapted by Robert M. Young and directed by John Badham); and "The Dead Man" (adapted and directed by Douglas Heyes).
Quotations.
"Consider the age in which we live. It wants magicians…. A scientist tells people the truth. When times are good—that is, when the truth offers no threat—people don't mind.… A magician, on the other hand, tells people what they wish were true—that perpetual motion works, that cancer can be cured by colored lights, that a psychosis is no worse than a head cold, that they'll live forever. In good times magicians are laughed at. They're a luxury of the spoiled wealthy few. But in bad times people sell their souls for magic cures and buy perpetual-motion machines to power their war rockets."
"Everyone knows Newton as the great scientist. Few remember that he spent half his life muddling with alchemy, looking for the philosopher's stone. That was the pebble by the seashore he really wanted to find."
"If you could sum up all you felt about life and crystallize it in one master insight, you would have said it all and you would be dead.
A bibliography of Leiber's work is "Fritz Leiber: A Bibliography 1934–1979" by Chris Morgan (Birmingham, UK: Morgenstern, 1979). It is fairly definitive as to the date of publication but Leiber's work badly needs an updated comprehensive bibliography.
An essay examining Leiber's literary relationship with H. P. Lovecraft ("Passing the Torch: H.P. Lovecraft and Fritz Leiber") appears in S. T. Joshi's "The Evolution of the Weird Tale" (2004).

</doc>
<doc id="10878" url="https://en.wikipedia.org/wiki?curid=10878" title="Flanders">
Flanders

Flanders (Dutch: "Vlaanderen" , ) today normally refers to the Dutch-speaking northern portion of Belgium. It is one of the communities, regions and language areas of Belgium. The demonym associated with Flanders is Fleming, while the corresponding adjective is Flemish. The official capital of Flanders is Brussels, although Brussels itself has an independent regional government, and the government of Flanders only oversees some cultural aspects of Brussels life.
Historically, the name referred to the County of Flanders, which around 1000 CE stretched from the Strait of Dover to the Scheldt estuary. The only parts of historical Flanders that lay within modern-day Flanders are the provinces West Flanders and East Flanders. Nevertheless, during the 19th and 20th centuries it became increasingly commonplace to use the term "Flanders" to refer to the entire Dutch-speaking part of Belgium, stretching all the way to the River Maas. In accordance with late 20th century Belgian state reforms the area was made into two political entities: the "Flemish Community" () and the "Flemish Region" (). These entities were merged, although geographically the Flemish Community, which has a broader cultural mandate, covers Brussels, whereas the Flemish Region does not.
Flanders has figured prominently in European history. During the late Middle Ages, cities such as Ghent, Bruges, Antwerp and Brussels made it one of the richest and most urbanized parts of Europe, weaving the wool of neighbouring lands into cloth for both domestic use and export. As a consequence, a very sophisticated culture developed, with impressive achievements in the arts and architecture, rivaling those of northern Italy. Belgium was one of the centres of the 19th century industrial revolution but Flanders was at first overtaken by French-speaking Wallonia. In the second half of the 20th century, however, Flanders' economy modernised rapidly, and today Flanders is more wealthy than its southern counterpart.
Geographically, Flanders is generally flat, and has a small section of coast on the North Sea. Much of Flanders is agriculturally fertile and densely populated, with a population density of almost 500 people per square kilometer (1,200 per square mile). It borders France to the west, the Netherlands to the north and east, and Wallonia to the south. The Brussels Capital Region is an enclave within the Flemish Region. Flanders has exclaves of its own: Voeren in the east is between Wallonia and the Netherlands and Baarle-Hertog in the north consists of 22 exclaves surrounded by the Netherlands.
Terminology.
In Belgium.
The term "Flanders" has several main meanings:
Dutch-speaking part of Belgium.
The significance of the County of Flanders and its counts eroded through time, but the designation remained in a very broad sense. In the Early modern period, the term Flanders was associated with the southern part of the Low Countries: the Southern Netherlands. During the 19th and 20th centuries, it became increasingly commonplace to refer to the Dutch-speaking part of Belgium as "Flanders". The linguistic limit between French and Dutch was recorded in the early '60's, from Kortrijk to Maastricht. Now, Flanders extends over the northern part of Belgium, including Belgian Limburg (corresponding closely to the medieval County of Loon), and the Belgian parts of the medieval Duchy of Brabant.
The ambiguity between this wider area and that of the County (or the Belgian parts thereof), still remains. In most present-day contexts however, in general the term Flanders is taken to refer to either the political, social, cultural, and linguistic community (and the corresponding official institution, the Flemish Community), or the geographical area, one of the three institutional regions in Belgium, namely the Flemish Region.
In the history of art and other fields, the adjectives Flemish and Netherlandish are commonly used to designate all the artistic production in this area before about 1580, after which it refers specifically to the southern Netherlands. For example, the term "Flemish Primitives", now outdated in English but used in French, Flemish and other languages, is a synonym for "Early Netherlandish painting", and it is not uncommon to see Mosan art categorized as Flemish art. In music the "Franco-Flemish School" is also known as the "Dutch School".
Within this Dutch-speaking part of Belgium is commonplace, French has never ceased to be spoken by some citizens and Jewish groups have been speaking Yiddish in Antwerp for centuries. Today, Flanders' minority residents include 170 nationalities — the largest groups speaking French, English, Berber, Turkish, Arabic, Spanish, Italian and Polish.
History.
Early history.
The area, roughly encompassing the later geographical meanings of Flanders, was considered to be in the northern and less economically developed part of Gallia Belgica, the most northeastern continental province of the Roman Empire at its height. Linguistically, the tribes in this area were under Celtic influence in the south, and Germanic influence in the east, but there is disagreement about what language was spoken locally, which may even have been an intermediate "Nordwestblock" language related to both. By the first century BC Germanic languages had become prevalent. In the future county of Flanders, the main Belgic tribe in Roman times was the Menapii, but also on the coast were the Marsacii and Morini.
Historical Flanders.
Created in the year 862 as a feudal fief in West Francia, the County of Flanders was divided when its western districts fell under French rule in the late 12th century. The remaining parts of Flanders came under the rule of the counts of neighbouring Hainaut in 1191. The entire area passed in 1384 to the dukes of Burgundy, in to the Habsburg dynasty, and in 1556 to the kings of Spain. The western districts of Flanders came finally under French rule under successive treaties of 1659 (Artois), 1668, and 1678.
During the late Middle Ages Flanders' trading towns (notably Ghent, Bruges and Ypres) made it one of the richest and most urbanized parts of Europe, weaving the wool of neighbouring lands into cloth for both domestic use and export. As a consequence, a very sophisticated culture developed, with impressive achievements in the arts and architecture, rivaling those of northern Italy. Ghent, Bruges, Ypres and the Franc of Bruges formed the Four Members, a form of parliament that exercised considerable power in Flanders.
Increasingly powerful from the 12th century, the territory's autonomous urban communes were instrumental in defeating a French attempt at annexation (1300–1302), finally defeating the French in the Battle of the Golden Spurs (11 July 1302), near Kortrijk. Two years later, the uprising was defeated and Flanders remained part of the French Crown. Flemish prosperity waned in the following century, however, owing to widespread European population decline following the Black Death of 1348, the disruption of trade during the Anglo-French Hundred Years' War (1337–1453), and increased English cloth production. Flemish weavers had gone over to Worstead and North Walsham in Norfolk in the 12th century and established the woolen industry.
In 1500, Charles V was born in Ghent. He inherited the Seventeen Provinces (1506), Spain (1516) with its colonies and in 1519 was elected Holy Roman Emperor. The Pragmatic Sanction of 1549, issued by Charles V, established the Low Countries as the Seventeen Provinces (or Spanish Netherlands in its broad sense) as an entity separate from the Holy Roman Empire and from France. In 1556 Charles V abdicated due to ill health (he suffered from crippling gout). Spain and the Seventeen Provinces went to his son, king Philip II of Spain.
Low Countries.
Over the first half of the 16th century Antwerp grew to become the second-largest European city north of the Alps by 1560. Antwerp was the richest city in Europe at this time. According to Luc-Normand Tellier "It is estimated that the port of Antwerp was earning the Spanish crown seven times more revenues than the Americas."
Meanwhile, Protestantism had reached the Low Countries. Among the wealthy traders of Antwerp, the Lutheran beliefs of the German Hanseatic traders found appeal, perhaps partly for economic reasons. The spread of Protestantism in this city was aided by the presence of an Augustinian cloister (founded 1514) in the St. Andries quarter. Luther, an Augustinian himself, had taught some of the monks, and his works were in print by 1518. The first Lutheran martyrs came from Antwerp. The Reformation resulted in consecutive but overlapping waves of reform: a Lutheran, followed by a militant Anabaptist, then a Mennonite, and finally a Calvinistic movement. These movements existed independently of each other.
Philip II, a devout Catholic and self-proclaimed protector of the Counter-Reformation, suppressed Calvinism in Flanders, Brabant and Holland (what is now approximately Belgian Limburg was part of the Bishopric of Liège and was Catholic "de facto"). In 1566, the wave of iconoclasm known as the "Beeldenstorm" was a prelude to religious war between Catholics and Protestants, especially the Anabaptists. The "Beeldenstorm" started in what is now French Flanders, with open-air sermons () that spread through the Low Countries, first to Antwerp and Ghent, and from there further east and north. In total it lasted not even a month.
Subsequently, Philip II sent the Duke of Alba to the Provinces to repress the revolt. Alba recaptured the southern part of the Provinces, who signed the Union of Atrecht, which meant that they would accept the Spanish government on condition of more freedom. But the northern part of the provinces signed the Union of Utrecht and settled in 1581 the Republic of the Seven United Netherlands. Spanish troops quickly started fighting the rebels, but before the revolt could be completely defeated, a war between England and Spain had broken out, forcing Philip's Spanish troops to halt their advance. Meanwhile, the Spanish armies had already conquered the important trading cities of Bruges and Ghent. Antwerp, which was then the most important port in the world, also had to be conquered. On 17 August 1585, Antwerp fell. This ended the Eighty Years' War for the (from now on) Southern Netherlands. The United Provinces (the Northern Netherlands) fought on until 1648 – the Peace of Westphalia.
While Spain was at war with England, the rebels from the north, strengthened by refugees from the south, started a campaign to reclaim areas lost to Philip II's Spanish troops. They managed to conquer a considerable part of Brabant (the later Noord-Brabant of the Netherlands), and the south bank of the Scheldt estuary (Zeeuws-Vlaanderen), before being stopped by Spanish troops. The front line at the end of this war stabilized and became the current border between present-day Belgium and the Netherlands. The Dutch (as they later became known) had managed to reclaim enough of Spanish-controlled Flanders to close off the river Scheldt, effectively cutting Antwerp off from its trade routes.
First the fall of Antwerp to the Spanish and later also the closing of the Scheldt were causes of a considerable emigration of Antverpians. Many of the Calvinist merchants of Antwerp and also of other Flemish cities left Flanders and emigrated to the north. A large number of them settled in Amsterdam, which was at the time a smaller port, of significance only in the Baltic trade. In the following years Amsterdam was rapidly transformed into one of the world's most important ports. Because of the contribution of the Flemish exiles to this transformation, the exodus is sometimes described as "creating a new Antwerp"".
Flanders and Brabant, due to these events, went into a period of relative decline from the time of the Thirty Years War. In the Northern Netherlands however, the mass emigration from Flanders and Brabant became an important driving force behind the Dutch Golden Age.
Although arts remained at a relatively impressive level for another century with Peter Paul Rubens (1577–1640) and Anthony van Dyck, Flanders experienced a loss of its former economic and intellectual power under Spanish, Austrian, and French rule, with heavy taxation and rigid imperial political control compounding the effects of industrial stagnation and Spanish-Dutch and Franco-Austrian conflict. The Southern Netherlands suffered severely under the War of the Spanish Succession, but under the reign of empress Maria-Theresia these lands economically flourished again. Influenced by the Enlightenment, the Austrian emperor Joseph II was the first sovereign who has been in the Southern Netherlands since king Philip II of Spain left them in 1559.
In 1794 the French Republican Army started using Antwerp as the northernmost naval port of France, which country officially annexed Flanders the following year as the "départements" of Lys, Escaut, Deux-Nèthes, Meuse-Inférieure and Dyle. Obligatory (French) army service for all men aged 16–25 was one of the main reasons for the people's uprising against the French in 1798, known as the "Boerenkrijg" ("Peasants' War"), with the heaviest fighting in the Campine area.
After the defeat of Napoleon Bonaparte at the 1815 Battle of Waterloo in Waterloo, Brabant, sovereignty over the Austrian Netherlands – Belgium minus the East Cantons and Luxembourg – was given by the Congress of Vienna (1815) to the United Netherlands (Dutch: "Verenigde Nederlanden"), the state that briefly existed under Sovereign Prince William I of Orange Nassau, the latter King William I of the United Kingdom of the Netherlands, after the French Empire was driven out of the Dutch territories. The United Kingdom of the Netherlands was born. The Protestant King of the Netherlands, William I rapidly started the industrialisation of the southern parts of the Kingdom. The political system that was set up however, slowly but surely failed to forge a true union between the northern and the southern parts of the Kingdom. The southern bourgeoisie mainly was Roman Catholic, in contrast to the mainly Protestant north; large parts of the southern bourgeoisie also primarily spoke French rather than Dutch.
In 1815 the Dutch Senate was reinstated (Dutch: "Eerste Kamer der Staaten Generaal"). The nobility, mainly coming from the south, became more and more estranged from their northern colleagues. Resentment grew both between the Roman Catholics from the south and the Protestants from the north and among the powerful liberal bourgeoisie from the south and their more moderate colleagues from the north. On 25 August 1830 (after the showing of the opera 'La Muette de Portici' of Daniel Auber in Brussels) the Belgian Revolution sparked off and became a fact. On 4 October 1830, the Provisional Government (Dutch: "Voorlopig Bewind") proclaimed the independence, which was later confirmed by the National Congress that issued a new Liberal Constitution and declared the new state a Constitutional Monarchy, under the House of Saxe-Coburg. Flanders now became part of the Kingdom of Belgium, which was recognized by the major European Powers on 20 January 1831. The de facto dissidence was finally recognized by the United Kingdom of the Netherlands on 19 April 1839.
Kingdom of Belgium.
In 1830, the Belgian Revolution led to the splitting up of the two countries. Belgium was confirmed as an independent state by the Treaty of London of 1839, but deprived of the eastern half of Limburg (now Dutch Limburg), and the Eastern half of Luxembourg (now the Grand-Duchy of Luxembourg). Sovereignty over Zeeuws Vlaanderen, south of the Westerscheldt river delta, was left with the Kingdom of the Netherlands, which was allowed to levy a toll on all traffic to Antwerp harbour until 1863.
The Belgian Revolution was not well supported in Flanders and even on 4 October 1830, when the Belgian independence was eventually declared, Flemish authorities refused to take orders from the new Belgian government in Brussels. Only after Flanders was subdued with the aid of a large French military force one month later, under the leadership of the Count de Pontécoulant, did Flanders become a true part of Belgium.
The French-speaking bourgeoisie showed very little respect for the Dutch-speaking part of the population. French became the only official language in Belgium and all secondary and higher education in the Dutch language was abolished. 
In 1834, all people even remotely suspected of being "Flemish minded" or calling for the reunification of the Netherlands were prosecuted and their houses looted and burnt. Flanders, until then a very prosperous European region, was not considered worthwhile for investment and scholarship. A study in 1918 demonstrated that in the first 88 years of its existence, 80% of the Belgian GNP was invested in Wallonia. This led to a widespread poverty in Flanders, forcing roughly 300.000 Flemish to emigrate to Wallonia to start working there in the heavy industry.
All of these events led to a silent uprising in Flanders against the French-speaking domination. But it was not until 1878 that Dutch was allowed to be used for official purposes in Flanders (see language legislation in Belgium), although French remained the only official language in Belgium.
In 1873, Dutch became the official language in public secondary schools. In 1898 Dutch and French were declared equal languages in laws and Royal orders. In 1930 the first Flemish university was opened.
The first official translation of the Belgian constitution in Dutch was not published until 1967.
Flanders (and Belgium as a whole) saw some of the greatest loss of life on the Western Front of the First World War, in particular from the three battles of Ypres.
Flemish feeling of identity and consciousness grew through the events and experiences of war. The occupying German authorities took several Flemish-friendly measures. More importantly, the experiences of many Dutch-speaking soldiers on the front led by French-speaking officers catalysed Flemish emancipation. The French-speaking officers often gave orders in French only, followed by "et pour les Flamands, la même chose!", meaning "and for the Flemish, the same thing!" (which did not help the Flemish conscripts, who were mostly uneducated farmers and workers unable to have understood what had been said in French).
The resulting suffering is still remembered by Flemish organizations during the yearly Yser pilgrimage in Diksmuide at the monument of the Yser Tower.
During the interbellum and World War II, several right-wing fascist and/or national-socialistic parties emerged in Belgium, the Flemish ones being energized by the anti-Flemish discrimination of the Wallonians. Since these parties were promised more rights for the Flemings by the German government during World War II, many of them collaborated with the Nazi regime. After the war, collaborators (or people who were "Zwart", "Black" during the war) were prosecuted and punished, among them many Flemish Nationalists whose main political goal had been the emancipation of Flanders. As a result, up until this day Flemish Nationalism is often associated with right-wing and sometimes fascist ideologies.
After World War II, the differences between Dutch-speaking and French-speaking Belgians became clear in a number of conflicts, such as the Royal Question, the question whether King Leopold III should return (which most Flemings supported but not the Walloons) and the use of Dutch in the Catholic University of Leuven. As a result, several state reforms took place in the second half of the 20th century, which transformed the unitary Belgium into a federal state with communities, regions and language areas. This resulted also in the establishment of a Flemish Parliament and Government. During the 1970s, all major political parties split into a Flemish and French-speaking party.
Several Flemish parties still advocate for more Flemish autonomy, some even for Flemish independence (see Partition of Belgium), whereas the French-speakers would like to keep the current state as it is. Recent governments (such as Verhofstadt I Government) have transferred certain federal competences to the regional governments.
On 13 December 2006, a spoof news broadcast by the Belgian Francophone public broadcasting station RTBF declared that Flanders had decided to declare independence from Belgium.
The 2007 federal elections showed more support for Flemish autonomy, marking the start of the 2007–2011 Belgian political crisis. All the political parties that advocated a significant increase of Flemish autonomy gained votes as well as seats in the Belgian federal parliament. This was especially the case for Christian Democratic and Flemish and New Flemish Alliance (N-VA) (who had participated on a shared electoral list). The trend continued during the 2009 regional elections, where CD&V and N-VA were the clear winners in Flanders, and N-VA became even the largest party in Flanders and Belgium during the 2010 federal elections, followed by the longest-ever government formation after which the Di Rupo I Government was formed excluding N-VA. Eight parties agreed on a sixth state reform which aim to solve the disputes between Flemings and French-speakers. The 2012 provincial and municipal elections however continued the trend of N-VA becoming the biggest party in Flanders.
These victories for the advocates of much more Flemish autonomy are very much in parallel with opinion polls that show a structural increase in popular support for their agenda. Since 2006, certain polls have started showing a majority in favour of Flemish independence. Those polls are not yet representative, but they point to a significant long-term trend.
Government and politics.
Both the Flemish Community and the Flemish Region are constitutional institutions of the Kingdom of Belgium, exercising certain powers within their jurisdiction, granted following a series of state reforms. In practice, the Flemish Community and Region together form a single body, with its own parliament and government, as the Community legally absorbed the competences of the Region. The parliament is a directly elected legislative body composed of 124 representatives. The government consists of up to a maximum of eleven members and is presided by a Minister-President, currently Geert Bourgeois (New Flemish Alliance) leading a coalition of his party (N-VA) with Christen-Democratisch en Vlaams (CD&V) and Open Vlaamse Liberalen en Democraten (Open VLD).
The area of the Flemish Community is represented on the maps above, including the area of the Brussels-Capital Region (hatched on the relevant map). Roughly, the Flemish Community exercises competences originally oriented towards the individuals of the Community's language: culture (including audiovisual media), education, and the use of the language. Extensions to personal matters less directly associated with language comprise sports, health policy (curative and preventive medicine), and assistance to individuals (protection of youth, social welfare, aid to families, immigrant assistance services, etc.)
The area of the Flemish Region is represented on the maps above. It has a population of more than 6 million (excluding the Dutch-speaking community in the Brussels Region, grey on the map for it is not a part of the Flemish Region). Roughly, the Flemish Region is responsible for territorial issues in a broad sense, including economy, employment, agriculture, water policy, housing, public works, energy, transport, the environment, town and country planning, nature conservation, credit, and foreign trade. It supervises the provinces, municipalities, and intercommunal utility companies.
The number of Dutch-speaking Flemish people in the Capital Region is estimated to be between 11% and 15% (official figures do not exist as there is no language census and no official subnationality). According to a survey conducted by the Université catholique de Louvain in Louvain-la-Neuve and published in June 2006, 51% of respondents from Brussels claimed to be bilingual, even if they do not have Dutch as their first language. They are governed by the Brussels Region for economics affairs and by the Flemish Community for educational and cultural issues.
As mentioned above, Flemish institutions such as the Flemish Parliament and Government, represent the Flemish Community and the Flemish Region. The region and the community thus "de facto" share the same parliament and the same government. All these institutions are based in Brussels. Nevertheless, both types of subdivisions (the Community and the Region) still exist legally and the distinction between both is important for the people living in Brussels. Members of the Flemish Parliament who were elected in the Brussels Region cannot vote on affairs belonging to the competences of the Flemish Region.
The official language for all Flemish institutions is Dutch. French enjoys a limited official recognition in a dozen municipalities along the borders with French-speaking Wallonia, and a large recognition in the bilingual Brussels Region. French is widely known in Flanders, with 59% claiming to know French according to a survey conducted by the Université catholique de Louvain in Louvain-la-Neuve and published in June 2006.
Politics.
Historically, the political parties reflected the pillarisation ("verzuiling") in Flemish society. The traditional political parties of the three pillars are Christian-Democratic and Flemish (CD&V), the Open Flemish Liberals and Democrats (Open Vld) and the Socialist Party – Differently (sp.a).
However, during the last half century, many new political parties were founded in Flanders. One of the first was the nationalist People's Union, of which the right nationalist Flemish Block (now Flemish Interest) split off, and which later dissolved into the now-defunct Spirit or Social Liberal Party, moderate nationalism rather left of the spectrum, on the one hand, and the New Flemish Alliance (N-VA), more conservative but independentist, on the other hand. Other parties are the leftist alternative/ecological Green party; the short-lived anarchistic libertarian spark ROSSEM and more recently the conservative-right liberal List Dedecker, founded by Jean-Marie Dedecker, and the socialist Workers' Party.
Particularly the Flemish Block/Flemish Interest has seen electoral success roughly around the turn of the century, and the New Flemish Alliance during the last few elections, becoming even the largest party in the 2010 federal elections.
Flemish nation.
For some Flemings, Flanders is more than just a geographical area or the federal institutions (Flemish Community and Region). Supporters of the Flemish Movement even call it a nation and pursue Flemish independence, but most people (approximately 75%) living in Flanders say they are proud to be Belgian and opposed to the dissolution of Belgium. 20% is even "very proud", while some 25% are not proud and 8% is "very not proud". Mostly students claim to be proud of their nationality, with 90% of them staying so. Of the people older than 55, only 31% claim to be proud of being a Belgian. All in all, 22% of Flemings directly state they want to secede. Particular opposition to secession comes from women, people employed in services, the highest social classes and people from big families. Strongest of all opposing the notion are housekeepers - both housewives and house husbands.
In 2012, the Flemish government drafted a "Charter for Flanders" ("Handvest voor Vlaanderen") of which the first article says "Vlaanderen is een deelstaat van de federale Staat België en maakt deel uit van de Europese Unie." ("Flanders is a component state of the federal State of Belgium and is part of the European Union"). Though interpreted by many Flemish nationalists as a statement, this phrase is merely a quotation from the Belgian constitution and has no further legal value whatsoever.
Geography.
Flanders shares its borders with Wallonia in the south, Brussels being an enclave within the Flemish Region. The rest of the border is shared with the Netherlands (Zeelandic Flanders, North Brabant and Limburg) in the north and east, and with France (French Flanders) and the North Sea in the west. Voeren is an exclave of Flanders between Wallonia and the Netherlands, while Baarle-Hertog in Flanders forms a complicated series of enclaves and exclaves with Baarle-Nassau in the Netherlands. Germany, although bordering Wallonia and close to Voeren in Limburg, does not share a border with Flanders. The German-speaking Community of Belgium, also close to Voeren, does not border Flanders either. (The commune of Plombières, majority French speaking, lies between them.)
Flanders is a highly urbanised area, lying completely within the Blue Banana. Antwerp, Ghent, Bruges and Leuven are the largest cities of the Flemish Region. Antwerp has a population of more than 500,000 citizens and is the largest city, Ghent has a population of 250,000 citizens, followed by Bruges with 120,000 citizens and Leuven counts almost 100,000 citizens. Brussels is a part of Flanders as far as community matters are concerned, but does not belong to the Flemish Region.
Flanders has two main geographical regions: the coastal Yser basin plain in the north-west and a central plain. The first consists mainly of sand dunes and clayey alluvial soils in the polders. Polders are areas of land, close to or below sea level that have been reclaimed from the sea, from which they are protected by dikes or, a little further inland, by fields that have been drained with canals. With similar soils along the lowermost Scheldt basin starts the central plain, a smooth, slowly rising fertile area irrigated by many waterways that reaches an average height of about five metres (16.4 ft) above sea level with wide valleys of its rivers upstream as well as the Campine region to the east having sandy soils at altitudes around thirty metres Near its southern edges close to Wallonia one can find slightly rougher land richer of calcium with low hills reaching up to and small valleys, and at the eastern border with the Netherlands, in the Meuse basin, there are marl caves ("mergelgrotten"). Its exclave around Voeren between the Dutch border and the Walloon province of Liège attains a maximum altitude of above sea level.
Administrative divisions.
The present-day Flemish Region covers and is divided into five provinces, 22 arrondissements and 308 cities or municipalities.
The province of Flemish Brabant is the most recent one, being formed in 1995 after the splitting of the province of Brabant.
Most municipalities are made up of several former municipalities, now called "deelgemeenten". The largest municipality (both in terms of population and area) is Antwerp, having more than half a million inhabitants. Its nine "deelgemeenten" have a special status and are called districts, which have an elected council and a college. While any municipality with more than 100,000 inhabitants can establish districts, only Antwerp did this so far. The smallest municipality (also both in terms of population and area) is Herstappe (Limburg).
The Flemish Community covers both the Flemish Region and, together with the French Community, the Brussels-Capital Region. Brussels, an enclave within the province of Flemish Brabant, is not divided into any province nor is it part of any. It coincides with the Arrondissement of Brussels-Capital and includes 19 municipalities.
The Flemish Government has its own local institutions in the Brussels-Capital Region, being the "Vlaamse Gemeenschapscommissie" (VGC), and its municipal antennae ("Gemeenschapscentra", community centres for the Flemish community in Brussels). These institutions are independent from the educational, cultural and social institutions that depend directly on the Flemish Government. They exert, among others, all those cultural competences that outside Brussels fall under the provinces.
Climate.
The climate is maritime temperate, with significant precipitation in all seasons (Köppen climate classification: "Cfb"; the average temperature is in January, and in July; the average precipitation is 65 millimetres (2.6 in) in January, and 78 millimetres (3.1 in) in July).
Economy.
Total GDP of the Flemish Region in 2004 was €165,847 million (Eurostat figures). Per capita GDP at purchasing power parity was 23% above the EU average. Flemish productivity per capita is about 13% higher than that in Wallonia, and wages are about 7% higher than in Wallonia.
Flanders was one of the first continental European areas to undergo the Industrial Revolution, in the 19th century. Initially, the modernization relied heavily on food processing and textile. However, by the 1840s the textile industry of Flanders was in severe crisis and there was famine in Flanders (1846–50). After World War II, Antwerp and Ghent experienced a fast expansion of the chemical and petroleum industries. Flanders also attracted a large majority of foreign investments in Belgium. The 1973 and 1979 oil crises sent the economy into a recession. The steel industry remained in relatively good shape. In the 1980s and 90s, the economic centre of Belgium continued to shift further to Flanders and is now concentrated in the populous Flemish Diamond area. Nowadays, the Flemish economy is mainly service-oriented.
Belgium is a founding member of the European Coal and Steel Community in 1951, which evolved into the present-day European Union. In 1999, the euro, the single European currency, was introduced in Flanders. It replaced the Belgian franc in 2002.
The Flemish economy is strongly export-oriented, in particular of high value-added goods. The main imports are food products, machinery, rough diamonds, petroleum and petroleum products, chemicals, clothing and accessories, and textiles. The main exports are automobiles, food and food products, iron and steel, finished diamonds, textiles, plastics, petroleum products, and non-ferrous metals. Since 1922, Belgium and Luxembourg have been a single trade market within a customs and currency union—the Belgium–Luxembourg Economic Union. Its main trading partners are Germany, the Netherlands, France, the United Kingdom, Italy, the United States, and Spain.
Antwerp is the number one diamond market in the world, diamond exports account for roughly 1/10 of Belgian exports. The Antwerp-based BASF plant is the largest BASF-base outside Germany, and accounts on its own for about 2% of Belgian exports. Other industrial and service activities in Antwerp include car manufacturing, telecommunications, photographic products.
Flanders is home to several science and technology institutes, such as IMEC, VITO, Flanders DC and Flanders DRIVE.
Infrastructure.
Flanders has developed an extensive transportation infrastructure of ports, canals, railways and highways. The Port of Antwerp is the second-largest in Europe, after Rotterdam. Other, much smaller ports are Ghent, Bruges-Zeebrugge and Ostend, the last two being located at the Belgian coast.
Whereas railways are managed by the federal National Railway Company of Belgium, other public transport (De Lijn) and roads are managed by the Flemish region.
The main airport is Brussels Airport, the only other civilian airport with scheduled services in Flanders is Antwerp International Airport, but there are two other ones with cargo or charter flights: Ostend-Bruges International Airport and Kortrijk-Wevelgem International Airport, both in West Flanders.
Demographics.
The highest population density is found in the area circumscribed by the Brussels-Antwerp-Ghent-Leuven agglomerations that surround Mechelen and is known as the Flemish Diamond, in other important urban centres as Bruges and Kortrijk to the west, and notable centres Turnhout and Hasselt to the east. On 1 January 2015, the Flemish Region had a population of 6,444,127 and about 15% of the 1,175,173 people in the Brussels Region are also considered Flemish.
Religion.
The (Belgian) "laicist", or secularist, constitution provides for freedom of religion, and the various governments in general respect this right in practice. Since independence, Catholicism, counterbalanced by strong freethought movements, has had an important role in Belgium's politics, since the 20th century in Flanders mainly via the Christian trade union ACV and the Christian Democratic and Flemish party (CD&V). According to the "2001 Survey and Study of Religion", about 47 percent of the Belgian population identify themselves as belonging to the Catholic Church, while Islam is the second-largest religion at 3.5 percent. A 2006 inquiry in Flanders, considered more religious than Wallonia, showed that 55% considered themselves religious, and 36% believed that God created the world.
Jews [[History of the Jews in Belgium|have been present]] in Flanders for a long time, in particular [[History of the Jews in Antwerp|in Antwerp]]. More recently, Muslims have immigrated to Flanders, now forming the largest minority religion with about 3.9% in the Flemish Region and 25% in Brussels. The largest Muslim group is the Moroccans, while the second largest is the Turks.
[[File:Castle Arenberg, Katholieke Universiteit Leuven adj.jpg|thumb|right|[[Arenberg Château]], part of the [[Katholieke Universiteit Leuven]], the oldest university in Belgium and the Low Countries.]]
Education.
Education is compulsory from the ages of six to 18, but most [[Flemish people|Flemings]] continue to study until around 23. Among the [[Organisation for Economic Co-operation and Development]] countries in 1999, Flanders had the third-highest proportion of 18- to 21-year-olds enrolled in [[postsecondary education]]. Flanders also scores very high in international comparative studies on education. Its secondary school students consistently rank among the top three for mathematics and science. However, the success is not evenly spread: ethnic minority youth score consistently lower, and the difference is larger than in most comparable countries.
Mirroring the historical political conflicts between the freethought and Catholic segments of the population, the Flemish educational system is split into a secular branch controlled by the communities, the provinces, or the municipalities, and a [[subsidy|subsidised]] religious—mostly Catholic—branch. For the subsidised schools, the main costs such as the teacher's wages and building maintenance completely borne by the Flemish government. Subsidised schools are also free to determine their own teaching and examination methods, but in exchange, they must be able to proof that certain minimal terms are achieved by keeping records of the given lessons and exams. It should however be noted that—at least for the [[Catholic school]]s—the religious authorities have very limited power over these schools, neither do the schools have a lot of power on their own. Instead, the Catholic schools are a member of the Catholic umbrella organisation [[VSKO]]. The VSKO determines most practicalities for schools, like the advised schedules per study field. However, there's freedom of education in Flanders, which doesn't only mean that every pupil can choose his/her preferred school, but also that every organisation can found a school, and even be subsidised when abiding the different rules. This resulted also in some smaller school systems follow 'methodical pedagogies' (e.g. [[Waldorf education|Steiner]], [[Montessori]], or [[Freinet]]) or serve the Jewish and Protestant minorities.
During the school year 2003–2004, 68.30% of the total population of children between the ages of six and 18 went to subsidized private schools (both religious schools or 'methodical pedagogies' schools).
The big freedom given to schools results in a constant competition to be the "best" school. The schools get certain reputations amongst parents and employers. So it's important for schools to be the best school since the subsidies depend on the number of pupils. This competition has been pinpointed as one of the main reasons for the high overall quality of the Flemish education. However, the importance of a school's reputation also makes schools more eager to expel pupils that don't perform well. Resulting in the ethnic differences and the well-known waterfall system: pupils start high in the perceived hierarchy, and then drop towards more professional oriented directions or "easier" schools when they can't handle the pressure any longer.
Healthcare.
Healthcare is a federal matter, but the [[Flemish Government]] is responsible for care, health education and [[preventive care]].
Culture.
At first sight, "Flemish culture" is defined by its [[language]] and its gourmandic mentality, as compared to the more Calvinistic Dutch culture. [[Dutch paintings|Dutch]] and [[Flemish painting]]s enjoyed more equal international admiration.
Language and literature.
[[File:Guidogezelle.jpg|thumb|Statue of [[Guido Gezelle|Gezelle]] in [[Bruges]], by sculptor [[Jules Lagae]]]]
The standard language in Flanders is [[Dutch language|Dutch]]; spelling and grammar are regulated by a single authority, the [[Dutch Language Union]] ("Nederlandse Taalunie"), comprising a committee of ministers of the Flemish and Dutch governments, their advisory council of appointed experts, a controlling commission of 22 parliamentarians, and a secretariate. The term [[Flemish]] can be applied to the Dutch spoken in Flanders; it shows many regional and local variations.
The biggest difference between Belgian Dutch and Dutch used in the Netherlands is in the pronunciation of words. Dutch is typically described as being "sharper", while Flemish is "softer". In Flemish, there are also less vowels pronounced as [[diphthongs]]. When it comes to spelling, Flemish language purists historically avoided to write words using a French spelling, or search for specific translations of words derived from French. While the Dutch prefer to stick with French spelling, as it differentiates Dutch more from the neighbouring German. For example, the Dutch word "punaise" (English: "[[Drawing pin]]") is derived directly from the French language. Flemish language purists have lobbied to accept the word "duimspijker" (literally: "thumb spike") as official Dutch, though the Dutch Language Union never accepted it as official Dutch. Other proposals by purists were sometimes accepted, and sometimes reverted again in later spelling revisions. As language purists were quite often professionally involved in language (f.e. as a teacher), these unofficial purist translations are found more often in Flemish texts.
Literature in non-[[standardized dialect]]s of the current area of Flanders originated with [[Hendrik van Veldeke]]'s "Eneas Romance", the first courtly romance in a [[Germanic language]] (12th century). With a writer of [[Hendrik Conscience]]'s stature, [[Flemish literature]] rose ahead of French literature in Belgium's early history. [[Guido Gezelle]] not only explicitly referred to his writings as Flemish but actually used it in many of his poems, and strongly defended it:
Original 
<poem>
"Gij zegt dat ’t vlaamsch te niet zal gaan:"
"’t en zal!"
"dat ’t waalsch gezwets zal boven slaan:'
"’t en zal!"
"Dat hopen, dat begeren wij:"
"dat zeggen en dat zweren wij:"
"zoo lange als wij ons weren, wij:"
"’t en zal, ’t en zal,"
"’t en zal!"
</poem>
Translation . For explanations, continue along
"It shan't!"
"This we hope, for this we hanker:"
"this we say and this we vow:"
"as long as we fight back, we:"
"It shan't, It shan't,"
"It shan't!"
</poem>
The distinction between [[Dutch literature|Dutch]] and Flemish literature, often perceived politically, is also made on intrinsic grounds by some experts such as Kris Humbeeck, professor of Literature at the [[University of Antwerp]]. Nevertheless, nearly all [[Dutch language|Dutch-language]] literature read (and appreciated to varying degrees) in Flanders is the same as that in the Netherlands.
Influential Flemish writers include [[Ernest Claes]], [[Stijn Streuvels]] and [[Felix Timmermans]]. Their novels mostly describe rural life in Flanders in the 19th century and at beginning of the 20th. Widely read by the older generations, they are considered somewhat old-fashioned by present-day critics. Some famous Flemish writers of the early 20th century wrote in French, including Nobel Prize winners (1911) [[Maurice Maeterlinck]] and [[Emile Verhaeren]]. They were followed by a younger generation, including [[Paul van Ostaijen]] and [[Gaston Burssens]], who "[[Activism|activated]]" the [[Flemish Movement]]. Still widely read and translated into other languages (including English) are the novels of authors such as [[Willem Elsschot]], [[Louis Paul Boon]] and [[Hugo Claus]]. The recent crop of writers includes the novelists [[Tom Lanoye]] and [[Herman Brusselmans]], and poets such as the married couple [[Herman de Coninck]] and [[Kristien Hemmerechts]].
Other languages in Flanders.
At the creation of the Belgian state, French was the only official language. French was during a long period used as a [[second language]] in Flanders and, like elsewhere in Europe, commonly spoken among the aristocracy. There is still a French-speaking minority in Flanders, especially in the [[municipalities with language facilities]], along the language border and the [[municipalities with language facilities#Rim municipalities|Brussels periphery]] (Vlaamse Rand), though many of them are French-speakers that migrated to Flanders in recent decades. French is the primary language in the officially bilingual [[Brussels Capital Region]], (see [[Francization of Brussels]]). In [[French Flanders]], French is now the native language of the majority of the population and the only official language. Historically it was a Dutch-speaking region and there is still a minority of Dutch-speakers living there.
Many Flemings are also able to speak French, children in Flanders generally get their first French lessons in the 5th primary year (normally around 10 years). But the current lack of French outside the educational context makes it hard to maintain a decent level of French. As such, the proficiency of French is declining. Flemish pupils are also obligated to follow English lessons as their third language. Normally from the second secondary year (around 14 years old), but the ubiquity of English in movies, music, IT and even advertisements makes it easier to learn and maintain the English language. This makes the Flemish people very proficient in English (only Sweden and Malta have a better knowledge of English as a second language).
Media.
The public radio and television broadcaster in Flanders is [[Vlaamse Radio- en Televisieomroeporganisatie|VRT]], which operates the TV channels [[één]], [[Canvas (Belgium)|Canvas]], [[Ketnet]], [[OP12]] and (together with the Netherlands) [[BVN]]. Flemish provinces each have up to two TV channels as well. Commercial television broadcasters include [[vtm]] and [[Vier]] (VT4). Popular TV series are for example "[[Thuis]]" and "[[F.C. De Kampioenen]]".
The five most successful Flemish films were "[[Loft (2008 film)|Loft]]" (2008; 1,186,071 visitors), "[[Koko Flanel]]" (1990; 1,082,000 tickets sold), "[[Hector (film)|Hector]]" (1987; 933,000 tickets sold), "[[Daens (film)|Daens]]" (1993; 848,000 tickets sold) and "[[The Alzheimer Case|De Zaak Alzheimer]]" (2003; 750,000 tickets sold). The first and last ones were directed by [[Erik Van Looy]], and an American remake is being made of both of them, respectively "[[The Loft (2012 film)|The Loft]]" (2012) and "[[The Memory of a Killer]]". The other three ones were directed by [[Stijn Coninx]].
Newspapers are grouped under three main publishers: [[De Persgroep]] with [[Het Laatste Nieuws]], the most popular newspaper in Flanders, [[De Morgen]] and [[De Tijd]]. Then [[Corelio]] with [[De Gentenaar]], the oldest extant Flemish newspaper, [[Het Nieuwsblad]] and [[De Standaard]]. Lastly, [[Concentra]] publishes [[Gazet van Antwerpen]] and [[Het Belang van Limburg]].
Magazines include [[Knack (magazine)|Knack]] and [[HUMO]].
Sports.
[[File:Kim Clijsters 2006.jpg|thumb|Kim Clijsters was [[WTA Awards|WTA Player of the Year]] in 2005 and 2010]]
[[Association football]] (soccer) is one of the most popular sports in both parts of Belgium, together with cycling, tennis, swimming and judo.
In cycling, the [[Tour of Flanders]] is considered one of the five "[[Classic cycle races|Monuments]]". Other "[[Flanders Classics]]" races include "[[Dwars door Vlaanderen]]" and [[Gent–Wevelgem]]. [[Eddy Merckx]] is regarded as one of the greatest cyclists of all time, with five victories in the [[Tour de France]] and numerous other cycling records. His hour speed record (set in 1972) stood for 12 years.
[[Jean-Marie Pfaff]], a former Belgian goalkeeper, is considered one of the greatest in the history of football (soccer).
[[Kim Clijsters]] (as well as the French-speaking Belgian [[Justine Henin]]) was [[WTA Awards|Player of the Year]] twice in the [[Women's Tennis Association]] as she was ranked the number one female tennis player.
[[Kim Gevaert]] and [[Tia Hellebaut]] are notable [[track and field]] stars from Flanders.
The [[1920 Summer Olympics]] were held in Antwerp. [[Jacques Rogge]] has been president of the [[International Olympic Committee]] since 2001.
The Flemish government agency for sports is [[Bloso]].
Music.
Flanders is known for its [[music festival]]s, like the annual [[Rock Werchter]], [[Tomorrowland (festival)|Tomorrowland]] and [[Pukkelpop]]. The [[Gentse Feesten]] are another very large yearly event.
The best-selling Flemish group or artist is the (Flemish-Dutch) group [[2 Unlimited]], followed by (Italian-born) [[Rocco Granata]], [[Technotronic]], [[Helmut Lotti]] and [[Vaya Con Dios (band)|Vaya Con Dios]].
The weekly charts of best-selling singles is the [[Ultratop 50]]. "Kvraagetaan" by the [[Fixkes]] holds the current record for longest time at #1 on the chart.
References.
[[Category:Flanders| ]]
[[Category:Regions of Belgium]]
[[Category:Autonomous regions]]
[[Category:Divided regions]]
[[Category:Germanic countries and territories]]

</doc>
<doc id="10879" url="https://en.wikipedia.org/wiki?curid=10879" title="Freud (disambiguation)">
Freud (disambiguation)

Sigmund Freud (1856–1939) was the inventor of psychoanalysis, psychosexual stages, and personality theory of Ego, Superego and Id.
Freud may also refer to:
People with the surname.
The Freud family:

</doc>
<doc id="10880" url="https://en.wikipedia.org/wiki?curid=10880" title="Plurality voting system">
Plurality voting system

A plurality voting system is a voting system in which each voter is allowed to vote for only one candidate, and the candidate who polls more votes (plurality) than any other candidate is elected. In a system based on single-member districts, it may be called first-past-the-post, single-choice voting, simple plurality or relative/simple majority. In a system based on multi-member districts, it may be referred to as winner-takes-all or bloc voting. The system is often used to elect members of a legislative assembly or executive officers. It is the most common form of the system, used in Canada, the lower house (Lok Sabha) in India, the United Kingdom, and most elections in the United States.
Plural voting is distinguished from a majority voting system, in which, to win, a candidate must receive an absolute majority of votes — i.e., more votes than all other candidates combined. Both systems may use single-member or multi-member constituencies, in what is referred to as an exhaustive counting system where one member is elected at a time and the process repeated until the number of vacancies is filled.
In some countries such as France (as well as in some jurisdictions of the United States, such as Louisiana and Georgia) a "two-ballot" or "runoff election" plurality system is used. This may require two rounds of voting. If on the first round no candidate receives over 50% of the votes, then a second round takes place, consisting of the two highest-voted candidates in the first round. This ensures that the winner gains a majority of votes in the second round. Alternatively, all candidates above a certain threshold in the first round may compete in the second round. If there are more than two candidates standing, then a plurality vote may decide the result.
In political science, the use of the plurality voting system with multiple, single-winner constituencies to elect a multi-member body is often referred to as single-member district plurality or SMDP. This combination is also variously referred to as winner-takes-all to contrast it with proportional representation systems. This term is sometimes also used to refer to elections for multiple winners in a particular constituency using bloc voting.
Plurality/Majority systems in the broader family of voting systems.
Most experts group electoral systems into 3 general categories:
Voting.
Plurality voting is used for local and/or national elections in 43 of the 193 countries of the United Nations. Plurality voting is particularly prevalent in the United Kingdom and former British colonies, including the United States, Canada and India.
In single winner plurality voting, each voter is allowed to vote for only one candidate, and the winner of the election is whichever candidate represents a plurality of voters, that is, whoever received the largest number of votes. This makes the plurality voting system among the simplest of all voting systems for voters and vote counting officials. (However the drawing of district boundary lines can be very contentious in this system.)
In an election for a legislative body, each voter in a given geographically-defined electoral district votes for one candidate from a list of candidates competing to represent that district. Under the plurality system, the winner of the election then becomes the representative of the entire electoral district, and serves with representatives of other electoral districts.
In an election for a single seat, such as president in a presidential system, the same style of ballot is used and the candidate who receives the largest number of votes represents the entire population.
In the two-round voting system, usually the two highest polling candidates in the first ballot progress to the second round Run-off ballot.
In a multiple member plurality election the counting of the ballot uses an exhaustive iteration process using the same ballot papers to elect one person each iteration for each vacant position.
Ballot types.
Generally plurality ballots (single-mark ballots) can be categorized into two forms. The simplest form is a blank ballot where the name of a candidate is written in by hand. A more structured ballot will list all the candidates and allow a mark to be made next to the name of a single candidate; however a structured ballot can also include space for a write-in candidate.
Examples of plurality voting.
General elections in the United Kingdom.
The United Kingdom, like the United States and Canada, uses single-member districts as the base for national elections. Each electoral district (constituency) chooses one member of parliament, i.e. the candidate that gets the most votes, whether he gets 50% of the votes cast or not ("first past the post"). In 1992, for example, a Liberal Democrat in Scotland won with just 26% of the votes. This system of single-member districts with plurality winners tends to produce two large political parties. (In countries with proportional representation there is not such a great incentive to vote for a large party, and that contributes to multi-party systems.)
Scotland, Wales and Northern Ireland use the first past the post system for general elections in the UK, but use versions of proportional representation for local elections and European elections.
The countries that inherited the British majoritarian system tend toward two large parties: one left, the other right, such as the U.S. Democrats and Republicans. Canada is an exception with three major political parties consisting of the New Democratic Party which is to the left, the Conservative Party which is to the far right and the Liberal Party which is slightly off center to the left. A fourth party that no longer has major party status is the separatist Bloc Québécois party which is territorial and concentrated in Quebec . New Zealand used the British system, and it too yielded two large parties. It also left many New Zealanders unhappy, because other viewpoints were ignored, so its parliament in 1993 adopted a new electoral law, modelled on Germany's system of proportional representation (PR) with a partial selection by constituencies. New Zealand soon developed a more complex party system.
After the 2015 Elections in the United Kingdom, there were calls from UKIP to change to proportional representation after receiving 3,881,129 votes but only 1 MP. The Green Party was similarly under-represented. This contrasted greatly with the SNP in Scotland who only received 1,454,436 votes but won 56 seats due to more concentrated support.
Example.
If each voter in each city naively selects one city on the ballot (Memphis voters select Memphis, Nashville voters select Nashville, and so on), then Memphis will be selected, as it has the most votes (42%). Note that this system does not require that the winner have a majority but only a plurality. Memphis wins because it has the most votes, even though 58% of the voters in this example preferred Memphis least. Notice that this problem does not hold anymore in the two-round system, in which Nashville would have won. (In practice, with FPTP, many voters in Chattanooga and Knoxville are likely to vote tactically for Nashville: see below.)
Disadvantages.
Tactical voting.
To a much greater extent than many other electoral methods, plurality electoral systems encourage tactical voting techniques, like "compromising". Voters are pressured to vote for one of the two candidates they predict are most likely to win, even if their true preference is neither, because a vote for any other candidate will likely be wasted and have no impact on the final result.
In the Tennessee example, if all the voters for Chattanooga and Knoxville had instead voted for Nashville, then Nashville would have won (with 58% of the vote); this would only have been the 3rd choice for those voters, but voting for their respective 1st choices (their own cities) actually results in their 4th choice (Memphis) being elected.
The difficulty is sometimes summed up, in an extreme form, as "All votes for anyone other than the second place are votes for the winner", because by voting for other candidates, they have denied those votes to the second place candidate who could have won had they received them. It is often claimed by United States Democrats that Democrat Al Gore lost the 2000 Presidential Election to Republican George W. Bush because some voters on the left voted for Ralph Nader of the Green Party, who exit polls indicated would have preferred Gore at 45% to Bush at 27%, with the rest not voting in Nader's absence.
Such a mentality is reflected by elections in Puerto Rico and its three principal voter groups: the Independentistas (pro-independence), the Populares (pro-commonwealth), and the Estadistas (pro-statehood). Historically, there has been a tendency for Independentista voters to elect Popular candidates and policies. This phenomenon is responsible for some Popular victories, even though the Estadistas have the most voters on the island. It is so widely recognised that the Puerto Ricans sometimes call the Independentistas who vote for the Populares "melons", because the fruit is green on the outside but red on the inside (in reference to the party colors).
Because voters have to predict in advance who the top two candidates will be, this can cause significant perturbation to the system:
Proponents of other single-winner voting systems argue that their proposals would reduce the need for tactical voting and reduce the spoiler effect. Examples include the commonly used two-round system of runoffs and instant runoff voting, along with less tested systems such as approval voting and Condorcet methods.
Fewer political parties.
Duverger's law is a theory that constituencies that use first-past-the-post systems will have a two-party system, given enough time.
First-past-the-post tends to reduce the number of political parties to a greater extent than most other methods do, making it more likely that a single party will hold a majority of legislative seats. (In the United Kingdom, 21 out of 24 General Elections since 1922 have produced a single-party majority government.)
FPTP's tendency toward fewer parties and more frequent one-party rules can also produce government that may not consider as wide a range of perspectives and concerns. It is entirely possible that a voter finds all major parties to have similar views on issues and that a voter does not have a meaningful way of expressing a dissenting opinion through his vote.
As fewer choices are offered to voters, voters may vote for a candidate although they disagree with him, because they disagree even more with his opponents. Consequently, candidates will less closely reflect the viewpoints of those who vote for them.
Furthermore, one-party rule is more likely to lead to radical changes in government policy even though the changes are favoured only by a plurality or a bare majority of the voters, whereas a multi-party system usually require greater consensus in order to make dramatic changes in policy.
Wasted votes.
Wasted votes are votes cast for losing candidates or votes cast for winning candidates in excess of the number required for victory. For example, in the UK General Election of 2005, 52% of votes were cast for losing candidates and 18% were excess votes—a total of 70% wasted votes. This is perhaps the most fundamental criticism of FPTP, that a large majority of votes may play no part in determining the outcome. Alternative electoral systems attempt to ensure that almost all votes are effective in influencing the result and the number of wasted votes is consequently minimised.
Gerrymandering.
Because FPTP permits a high level of wasted vote, an election under FPTP is easily gerrymandered. Through gerrymandering, constituencies are deliberately designed to unfairly increase the number of seats won by one party at the expense of another.
In brief, suppose that governing party G wishes to reduce the seats that will be won by opposition party O in the next election. It creates a number of constituencies in each of which O has an overwhelming majority of votes. O will win these seats, but a large number of its voters will waste their votes. Then the rest of the constituencies are designed with small majorities for G. Few G votes are wasted, and G will win a large number of seats by small margins. As a result of the gerrymander, O's seats have cost it more votes than G's seats.
Manipulation charges.
The presence of spoilers often gives rise to suspicions that manipulation of the slate has taken place. The spoiler may have received incentives to run. A spoiler may also drop out at the last moment, inducing charges that such an act was intended from the beginning.
Spoiler effect.
The spoiler effect is the effect of vote splitting between candidates or ballot questions with similar ideologies. One spoiler candidate's presence in the election draws votes from a major candidate with similar politics thereby causing a strong opponent of both or several to win. Smaller parties can disproportionately change the outcome of an FPTP election by swinging what is called the 50-50% balance of two party systems, by creating a faction within one or both ends of the political spectrum which shifts the winner of the election from an absolute majority outcome to a simple majority outcome favouring the previously less favoured party. In comparison, for electoral systems using proportional representation small groups win only their proportional share of representation.
Issues specific to particular countries.
Solomon Islands.
In August 2008, Sir Peter Kenilorea commented on what he perceived as the flaws of a first-past-the-post electoral system in the Solomon Islands:
Advantages.
Preservation of "one person, one vote" principle.
The arguments for a plurality voting system rely on the preservation of the "one person, one vote" principle (also "one man, one vote", or OMOV, or more recently "one member, one vote"), as cited by the Supreme Court of the United States, wherein each voter is only able to cast one vote in a given election, where that vote can only go to one candidate. Plurality voting systems elect the candidate who is preferred first by the largest number of voters, although this need not be an absolute majority. Other voting systems, such as instant-runoff voting, party-list proportional representation or single transferable vote also preserve OMOV, but it is not as obvious that they do so, because they rely on lower voter preference to enable a candidate to earn either an absolute majority (single member district) or a quota (multi-member district), respectively.
Moderation.
Some other voting systems can end up giving a greater chance of victory to a candidate perceived as having extreme views. Under the first-past-the-post system, voters are often afraid of "wasting" their vote on a candidate unlikely to win, so they vote for the candidate they perceive as the least bad candidate who has a chance to win. Advocates of plurality voting suggest that this results in most serious candidates having to present a fairly moderate or centrist position. This is debated by advocates of other systems, who argue that ranked voting systems or cardinal voting systems, by getting more information from voters, allow a more rigorous definition of the word "moderate" and can be designed to explicitly favor candidates fitting that description.
Advantages compared to proportional representation.
Plurality is often conflated with single-winner voting systems in general, in order to contrast it with proportional representation. In this context, it shares advantages, such as local accountability, with other single-winner systems.
International examples.
The United Kingdom continues to use the first-past-the-post electoral system for general elections, and for local government elections in England and Wales. Changes to the UK system have been proposed, and alternatives were examined by the Jenkins Commission in the late 1990s. After the formation of a new coalition government in 2010, it was announced as part of the coalition agreement that a referendum would be held on switching to the alternative vote system. However the alternative vote system was rejected 2-1 by British voters in a referendum held on 5 May 2011.
Canada also uses FPTP for national and provincial elections. In May 2005 the Canadian province of British Columbia had a referendum on abolishing single-member district plurality in favour of multi-member districts with the Single Transferable Vote system after the Citizens' Assembly on Electoral Reform made a recommendation for the reform. The referendum obtained 57% of the vote, but failed to meet the 60% requirement for passing. An October 2007 referendum in the Canadian province of Ontario on adopting a Mixed Member Proportional system, also requiring 60% approval, failed with only 36.9% voting in favour.
Northern Ireland, Scotland, Wales, the Republic of Ireland, Australia and New Zealand are notable examples of countries within the UK, or with previous links to it, that use non-FPTP electoral systems (Northern Ireland, Scotland and Wales use FPTP in United Kingdom general elections, however).
Nations which have undergone democratic reforms since 1990 but have not adopted the FPTP system include South Africa, almost all of the former Eastern bloc nations, Russia, Afghanistan and Iraq.
List of countries.
Countries that use a plurality voting system to elect the lower or only house of their legislature include:

</doc>
<doc id="10881" url="https://en.wikipedia.org/wiki?curid=10881" title="Fetish">
Fetish

Fetish may refer to:

</doc>
<doc id="10882" url="https://en.wikipedia.org/wiki?curid=10882" title="February 14">
February 14


</doc>
<doc id="10883" url="https://en.wikipedia.org/wiki?curid=10883" title="Free trade area">
Free trade area

A free-trade area is the region encompassing a trade bloc whose member countries have signed a free trade agreement (FTA). Such agreements involve cooperation between at least two countries to reduce trade barriers import quotas and tariffs and to increase trade of goods and services with each other.
If people are also free to move between the countries, in addition to FTA, it would also be considered an open border. It can be considered the second stage of economic integration.
Description.
Unlike a customs union (the third stage of economic integration), members of a free-trade area do not have a common external tariff, which means they have different quotas and customs, as well as other policies with respect to non-members. To avoid tariff evasion (through re-exportation) the countries use the system of certification of origin most commonly called rules of origin, where there is a requirement for the minimum extent of local material inputs and local transformations adding value to the goods. Only goods that meet these minimum requirements are entitled to the special treatment envisioned by the free trade area provisions.
Cumulation is the relationship between different FTAs regarding the rules of origin sometimes different FTAs supplement each other, in other cases there is no cross-cumulation between the FTAs. A free-trade area is a result of a free-trade agreement (a form of trade pact) between two or more countries. Free-trade areas and agreements (FTAs) are cascadable to some degree if some countries sign agreements to form a free-trade area and choose to negotiate together (either as a trade bloc or as a forum of individual members of their FTA) another free-trade agreement with another country (or countries) then the new FTA will consist of the old FTA plus the new country (or countries).
Within an industrialized country there are usually few if any significant barriers to the easy exchange of goods and services between parts of that country. For example, there are usually no trade tariffs or import quotas; there are usually no delays as goods pass from one part of the country to another (other than those that distance imposes); there are usually no differences of taxation and regulation. Between countries, on the other hand, many of these barriers to the easy exchange of goods often do occur. It is commonplace for there to be import duties of one kind or another (as goods enter a country) and the levels of sales tax and regulation often vary by country..
The aim of a free-trade area is to reduce barriers to exchange so that trade can grow as a result of specialisation, division of labour, and most importantly via comparative advantage. The theory of comparative advantage argues that in an unrestricted marketplace (in equilibrium) each source of production will tend to specialize in that activity where it has comparative (rather than absolute) advantage. The theory argues that the net result will be an increase in income and ultimately wealth and well-being for everyone in the free-trade area. But the theory refers only to aggregate wealth and says nothing about the distribution of wealth; in fact there may be significant losers, in particular among the recently protected industries with a comparative disadvantage. In principle, the overall gains from trade could be used to compensate for the effects of reduced trade barriers by appropriate inter-party transfers.
Every customs union, trade common market, economic union, customs and monetary union and economic and monetary union also has a free-trade area, but these are each listed in their own articles only.

</doc>
<doc id="10885" url="https://en.wikipedia.org/wiki?curid=10885" title="French fries">
French fries

French fries (American English), chips, fries, finger chips, or French-fried potatoes are batons of deep-fried potato. In the United States and most of Canada, the term "fries" refers to any elongated pieces of fried potatoes, while in the United Kingdom, Australia, South Africa, Ireland and New Zealand, long, thinly cut elongated strips of fried potatoes are sometimes called "shoestring" to distinguish them from the more thickly cut strips called "chips".
French fries are served hot, either soft or crispy, and generally eaten as part of lunch or dinner, or on their own as a snack, and they commonly appear on the menus of fast food restaurants. French fries are generally salted and are often served with ketchup; in many countries they are topped instead with other condiments or toppings, including vinegar, mayonnaise, or other local specialties. Fries can also be topped more elaborately, as in the dishes of poutine and chili cheese fries. Sometimes, fries are made with sweet potatoes instead of potatoes, are baked instead of fried, or are cut into unusual shapes.
Etymology.
Thomas Jefferson had "potatoes served in the French manner" at a White House dinner in 1802.
The expression "French Fried Potatoes" first occurs in print in English in the 1856 work "Cookery for Maids of All Work" by E Warren:
French Fried Potatoes. – Cut new potatoes in thin slices, put them in boiling fat, and a little salt; fry both sides of a light golden brown colour; drain.
In the early 20th century, the term "French fried" was being used in the sense of "deep-fried", for other foods such as onion rings or chicken.
It is unlikely that "French fried" refers to "frenching" in the sense of "julienning", which is not attested until after "French fried potatoes". Previously, frenching referred only to trimming meat off the shanks of chops.
Culinary origin.
Belgium.
It is claimed that fries originated in Belgium, and that the ongoing dispute between the French and Belgians about where they were invented is highly contentious, with both countries claiming ownership. From the Belgian standpoint the popularity of the term "French fries" is explained as a "French gastronomic hegemony" into which the cuisine of Belgium was assimilated because of a lack of understanding coupled with a shared language and geographic proximity between the two countries.
Belgian journalist Jo Gérard claims that a 1781 family manuscript recounts that potatoes were deep-fried prior to 1680 in the Meuse valley, in what was then the Spanish Netherlands (present-day Belgium): "The inhabitants of Namur, Andenne, and Dinant had the custom of fishing in the Meuse for small fish and frying, especially among the poor, but when the river was frozen and fishing became hazardous, they cut potatoes in the form of small fish and put them in a fryer like those here."
Gérard has not produced the manuscript that supports this claim, which, even if true, is unrelated to the later history of the French fry, as the potato did not arrive in the region until around 1735. Also, given 18th century economic conditions: "It is absolutely unthinkable that a peasant could have dedicated large quantities of fat for cooking potatoes. At most they were sautéed in a pan...".
Some people believe that the term "French" was introduced when British and American soldiers arrived in Belgium during World War I and consequently tasted Belgian fries. They supposedly called them "French", as it was the local language and the official language of the Belgian Army at that time, believing themselves to be in France. At that time, the term "French fries" was growing popular. But in fact the term was already used in America as early as 1899, in an item in "Good Housekeeping" which specifically references "Kitchen Economy in France": "The perfection of French fries is due chiefly to the fact that plenty of fat is used".
""Pommes frites", "frites" (French), or "frieten" (Dutch) became the national snack and a substantial part of several national dishes, such as Moules-frites or Steak-frites.
France.
In France and other French-speaking countries, fried potatoes are formally "pommes de terre frites", but more commonly "pommes frites", "patates frites", or simply "frites". The word "aiguillettes"" or "allumettes" is used when the French fries are very small and thin.
One enduring origin story holds that French fries were invented by street vendors on the Pont Neuf bridge in Paris in 1789, just before the outbreak of the French revolution. However, a reference exists in France from 1775 to "a few pieces of fried potato" and to "fried potatoes".
Eating potatoes was promoted in France by Parmentier, but he did not mention fried potatoes in particular.
Many Americans attribute the dish to France and offer as evidence a notation by U.S. President Thomas Jefferson. "Pommes de terre frites à cru, en petites tranches" ("Potatoes deep-fried while raw, in small slices") in a manuscript in Thomas Jefferson's hand (circa 1801–1809) and the recipe almost certainly comes from his French chef, Honoré Julien.
In addition, from 1813 on, recipes for what can be described as French fries occur in popular American cookbooks. By the late 1850s, one of these uses the term "French fried potatoes".
"Frites" are the main ingredient in the Canadian dish of Québécois descent known in both Canadian English and French as poutine, consisting of fried potatoes covered with cheese curds and gravy, a dish with a growing number of variations.
Scandinavia.
In Denmark, Sweden and Norway they are called "Pommes Frites" (potatoes, fried). They are the most common form of potatoes when served together with breaded plaice (or certain other low fat fishes). When served with fish, represents remoulade and a good lemon slice the typical Danish version of fish and chips. Remoulade is a yellowish often fat sauce which mostly is based on mayonnaise and pickles usually made of minced cauliflower and cabbage.
Pommes Frites are also served in entire Scandinavia as a stand-alone dish (then together with ketchup, remoulade or hamburger dressing). Fried sausage (same kind as for Hot Dogs), hamburgers or schnitzels may be the meat portion of a dish which includes french fries.
Some actual restaurants (as contrast to "fast-food") can serve french fries. Then usually to an entrecote or other beef together with bearnaise. Better restaurants tend to avoid serving french fries, with the possible exception of fish'n chips.
Spain.
In Spain, fried potatoes are called "patatas fritas" or "papas fritas". Another common form, in which the potatoes are cut into irregular shapes and seasoned with a spicy tomato sauce, is called "patatas bravas".
Some speculate that the dish may have been invented in Spain, the first European country in which the potato appeared via the New World colonies, and assume the first appearance to have been as an accompaniment to fish dishes in Galicia, from which it spread to the rest of the country and further to the Spanish Netherlands, which became Belgium more than a century later.
Professor Paul Ilegems, curator of the Frietmuseum in Bruges, Belgium, believes that Saint Teresa of Ávila fried the first French fries, referring also to the tradition of frying in Mediterranean cuisine.
Subsequent history.
The J. R. Simplot Company is credited with successfully commercializing French fries in frozen form during the 1940s. Subsequently, in 1967, Ray Kroc of McDonald's contracted the Simplot company to supply them with frozen fries, replacing fresh-cut potatoes.
In 2004, 29% of the United States' potato crop were used to make frozen fries – 90% consumed by the food services sector and 10% by retail. It is estimated that 80% of households in the UK buy frozen fries each year.
By country.
Belgium and the Netherlands.
Fries are very popular in Belgium, where they are known as "frieten" (in Dutch) or "frites" (in French), and the Netherlands, where they are known as "patat" in the north and, in the south, "friet". In Belgium, fries are sold in shops called "friteries" (French), "frietkot"/"frituur" (Dutch), or "Fritüre"/"Frittüre" (German). They are served with a large variety of Belgian sauces and eaten either on their own or with other snacks such as fricandelle or burgers. Traditionally, fries are served in a "cornet de frites" (French), "frietzak"/"fritzak" (Dutch), or "Frittentüte" (German), a white cardboard cone, then wrapped in paper, with a spoonful of sauce (mayonnaise and many others) on top. They may also be served with other traditional fast-food items, such as "frikandel"/"fricadelle", fishsticks "gehaktbal"/"boulet" (meatballs) or "kroket"/croquette. In the Netherlands, fries are sold at snack bars, often served with sauce Fritessaus or curry ketchup..
Friteries and other fast-food establishments tend to offer a number of different sauces for the fries and meats. In addition to ketchup and mayonnaise, popular options include: aioli, sauce andalouse, sauce Americaine, "Bicky" Dressing (Gele Bicky-sauce), curry mayonnaise, mammoet-sauce, peanut sauce, samurai-sauce, sauce "Pickles", pepper-sauce, tartar sauce, zigeuner sauce, and À la zingara. These sauces are generally also available in supermarkets. In addition to this, hot sauces are sometimes offered by friteries, including hollandaise sauce, sauce provençale, Béarnaise sauce, or a splash carbonade flamande stew from a constantly simmering pot, in the spirit of British "chips and gravy".
France.
In France, the thick-cut fries are called "Pommes Pont-Neuf" or simply "pommes frites", about 10 mm; thinner variants are "pommes allumettes" (matchstick potatoes), ±7 mm, and "pommes paille" (potato straws), 3–4 mm (roughly 0.4, 0.3 and 0.15 inch respectively). The two-bath technique is standard (Bocuse). "Pommes gaufrettes" are waffle fries.
Germany, Austria, Switzerland.
French fries migrated to the German-speaking countries during the 19th century. In Germany, where they are usually known by the French words "pommes frites", or (derived from the French words, but pronounced as German words) only "Pommes" or "Fritten", they are often served with mayonnaise, and are a popular walking snack offered by "Schnellimbiss" ("quick bite") kiosks. Since the advent of "Currywurst" in the 1950s, a paper tray of sausage (bratwurst or bockwurst) anointed with curry ketchup and laced with additional curry powder, and a side of french fries, has become an immensely popular fast-food meal.
United Kingdom and Ireland.
The usual deep-fried potatoes in the United Kingdom are called "chips", and are between 10 and 15 mm (0.4 to 0.6 inches) wide. They are occasionally made from unpeeled potatoes. Chips are often less crisp than the continental European French fry, owing to their relatively high water content. British chips are not potato chips, which are called 
"crisps" in Britain.
Like other deep-fried potatoes, they are cooked twice, once at a relatively low temperature (blanching) to cook the potato, and then at a higher temperature to crisp the surface, making them crunchy on the outside and fluffier on the inside.
In the UK, chips are part of the popular fast food dish fish and chips.
The first chips fried in the UK were sold by a Mrs 'Granny' Duce, in one of the West Riding towns beginning in 1854. A blue plaque in Oldham marks the origin of the fish and chip shop and fast food industries in Britain. In Scotland, chips were first sold in Dundee, "...in the 1870s, that glory of British gastronomy – the chip – was first sold by Belgian immigrant Edward De Gernier in the city's Greenmarket".
United States.
Although French fries were already a popular dish in most British commonwealth countries, the thin style of French fries has been popularized worldwide in part by the large American fast-food chains, such as McDonald's, Burger King, and Wendy's.
Pre-made French fries have been available for home cooking since the 1960s, usually having been pre-fried (or sometimes baked), frozen and placed in a sealed plastic bag.
Some later varieties of French fries are battered and breaded, and many fast-food chains in the U.S. dust the potatoes with kashi, dextrin, and other flavor coatings for crispier fries with particular tastes. Results with batterings and breadings, followed by microwaving, have not achieved widespread critical acceptance. Oven frying delivers a dish different from deep-fried potatoes.
Variants.
There are several variants of French fries. They include:
Accompaniments.
Fries tend to be served with a variety of accompaniments, such as salt and vinegar (malt, balsamic or white), pepper, grated cheese, melted cheese, mushy peas, heated curry sauce, curry ketchup (mildly spiced mix of the former), hot sauce, relish, mustard, mayonnaise, bearnaise sauce, tartar sauce, chili, tzatziki, feta cheese, garlic sauce, fry sauce, butter, sour cream, ranch dressing, barbecue sauce, gravy, honey, aioli, brown sauce, ketchup, lemon juice, piccalilli, pickled cucumber, pickled gherkins, pickled onions or pickled eggs.
Health aspects.
French fries contain primarily carbohydrates from the potato (mostly in the form of starch) and fat absorbed during the deep-frying process. For example: a large serving of French fries at McDonald's in the United States is (154 grams); nearly all of the 500 calories per serving derive from the 63 g of carbohydrates and the 25 g of fat; a serving also contains 6 g of protein, plus 350 mg of sodium.
Frying French fries in beef tallow, lard, or other animal fats adds saturated fat to the diet. Replacing animal fats with tropical vegetable oils, such as palm oil, simply substitutes one saturated fat for another. Replacing animal fats with partially hydrogenated oil reduces cholesterol but adds trans fat, which has been shown to both raise LDL cholesterol and lower HDL cholesterol. Canola/Rapeseed oil, or sunflower-seed oil are also used, as are mixes of vegetable oils, but beef tallow is generally more popular, especially amongst fast-food outlets that use communal oil baths. Accordingly, many restaurants now advertise their use of unsaturated oils; for example, both Five Guys and Chick-fil-A advertise that their fries are prepared with peanut oil.
French fries contain some of the highest levels of acrylamides of any foodstuff, and concerns have been raised about the impact of acrylamides on human health. According to the American Cancer Society it is not clear, , whether acrylamide consumption affects people's risk of getting cancer.
Legal issues.
In June 2004, the United States Department of Agriculture, with the advisement of a federal district judge from Beaumont, Texas, classified batter-coated French fries as a vegetable under the "Perishable Agricultural Commodities Act". This was primarily for trade reasons. French fries do not meet the standard to be listed as a processed food. This classification, referred to as the "French fry rule", was upheld in the United States Court of Appeals for the Fifth Circuit case "Fleming Companies, Inc. v. USDA".
In the United States in 2002, the McDonald's Corporation agreed to donate to Hindu and other groups to settle lawsuits filed against the chain for mislabeling French fries and hash browns as vegetarian, because their French fries and hash browns were found to contain beef extract added during production.

</doc>
<doc id="10886" url="https://en.wikipedia.org/wiki?curid=10886" title="Field hockey">
Field hockey

Field hockey, or simply hockey, is a team sport of the hockey family. The earliest origins of the sport date back to the Middle Ages in England, Scotland and the Netherlands. The game can be played on a grass field or a turf field as well as an indoor board surface. Each team plays with eleven players including the goalie. Players use sticks made out of wood or fiber glass to hit a round, hard, rubber like ball. The length of the stick depends on the player's individual height. Only one side of the stick is allowed to be used. Goalies have a different kind of stick. Theirs has another curve on the end of the stick. The uniform consists of shin-guards, cleats, skirts (usually referred to as kilts) or shorts, a mouth guard and a jersey. At the turn of the 21st century, the game is played globally, with particular popularity throughout western Europe, the Indian subcontinent, and Australasia as well as the American South and Northeast (such as Mississippi, Alabama, and Florida, Kentucky, Massachusetts, and Pennsylvania) as well as Southern Africa. Hockey is the national sport of Pakistan, and is sometimes assumed to be India's national sport as well, although officially India does not have a national sport. The term "field hockey" is used primarily in Canada and the United States where ice hockey is more popular.
During play, goal keepers are the only players who are allowed to touch the ball with any part of their body (the player's hand is considered 'part of the stick'), with this only applying within the "shooting circle" (also known as the "D", or "shooting arc", or just the circle), while field players play the ball with the flat side of their stick. The team that scores the most goals by the end of the match wins. If the score is tied at the end of the game, either a draw is declared or the game goes into extra time or a penalty shootout, depending on the competition's format. There are many variations to overtime play that depend on the league and tournament play. In college play, a seven-aside overtime period consists of a 10-minute periods with seven players for each team. If a tie still remains the game enters a one-on-one competition where each team chooses 5 players to dribble from the 25 yard line down to the circle against the opposing goalie. The player has 8 seconds to score on the goalie keeping it in bounds. The play ends after a goal is scored, the ball goes out of bounds, or time expires.
The governing body of hockey is the International Hockey Federation (IHF), with men and women being represented internationally in competitions including the Olympic Games, World Cup, World League, Champions Trophy and Junior World Cup, with many countries running extensive junior, senior, and masters' club competitions. The FIH is also responsible for organising the Hockey Rules Board and developing the rules for the sport.
A popular variant of field hockey is indoor field hockey, which differs in a number of respects while embodying the primary principles of hockey. Indoor hockey is a 5-a-side variant, with a field which is reduced to approximately . With many of the rules remaining the same, including obstruction and feet, there are several key variations – Players may not raise the ball unless shooting on goal, players may not hit the ball (instead utilising pushes to transfer the ball), and the sidelines are replaced with solid barriers which the ball will rebound off.
History.
There is a depiction of a hockey-like game from 510 BC in Ancient Greece when the game may have been called "Κερητίζειν" ("kerētízein") because it was played with a horn ("κέρας" in Greek) and a ball-like object. In East Asia, a similar game was entertained, using a carved wooden stick and ball prior to 300 BC. In Inner Mongolia, China, the Daur people have been playing Beikou a game with some similarities to field hockey for about 1,000 years. The word 'hockey' itself was recorded in 1363 when Edward III of England issued the proclamation: "Moreover we ordain that you prohibit under penalty of imprisonment all and sundry from such stone, wood and iron throwing; handball, football, or hockey; coursing and cock-fighting, or other such idle games."
The modern game grew from English public schools in the early 19th century. The first club was in 1849 at Blackheath in south-east London, but the modern rules grew out of a version played by Middlesex cricket clubs for winter sport. Teddington Hockey Club formed the modern game by introducing the striking circle and changing the ball to a sphere from a rubber cube. The Hockey Association was founded in 1886. The first international took place in 1895 (Ireland 3, Wales 0) and the International Rules Board was founded in 1900.
Field hockey was played at the Summer Olympics in 1908 and 1920. It was dropped in 1924, leading to the foundation of the Fédération Internationale de Hockey sur Gazon (FIH) as an international governing body by seven continental European nations, and hockey was reinstated in 1928. Men's hockey united under the FIH in 1970.
The two oldest trophies are the Irish Senior Cup, which dates back to 1894, and the Irish Junior Cup, a 2nd XI only competition instituted in 1895.
In India, the Beighton Cup and the Aga Khan tournament commenced within ten years. Entering the Olympics in 1928, India won all five games without conceding a goal and won from 1932 until 1956 and then in 1964 and 1980. Pakistan won in 1960, 1968 and 1984.
In the early 1970s, artificial turf began to be used. Synthetic pitches changed most aspects of field hockey, gaining speed. New tactics and techniques such as the Indian dribble developed, followed by new rules to take account. Indian history says that similar game like hockey was played in 17th century in Punjab state of India under name 'Khido Khundi' khido is a woolen ball and khundi is stick. The switch to synthetic surfaces ended Indian and Pakistani domination because artificial turf was too expensive—in comparison to the wealthier European countries—and since the 1970s Australia, the Netherlands and Germany have dominated at the Olympics.
Women's field hockey was first played at British universities and schools. The first club, the Molesey Ladies, was founded in 1887. The first national association was the Irish Ladies Hockey Union in 1894, and though rebuffed by the Hockey Association, women's field hockey grew rapidly around the world. This led to the International Federation of Women's Hockey Associations (IFWHA) in 1927, though this did not include many continental European countries where women played as sections of men's associations and were affiliated to the FIH. The IFWHA held conferences every three years, and tournaments associated with these were the primary IFWHA competitions. These tournaments were non-competitive until 1975.
By the early 1970s, there were 22 associations with women's sections in the FIH and 36 associations in the IFWHA. Discussions started about a common rule book. The FIH introduced competitive tournaments in 1974, forcing the acceptance of the principle of competitive field hockey by the IFWHA in 1973. It took until 1982 for the two bodies to merge, but this allowed the introduction of women's field hockey to the Olympic games from 1980 where, as in the men's game, The Netherlands, Germany, and Australia have been consistently strong. Argentina has emerged as a team to be reckoned with since 2000, winning the world championship in 2002 and 2010 and medals at the last three Olympics.
Outside North America, participation is now fairly evenly balanced between men and women. For example, in England, England Hockey reports that as of the 2008–09 season there were 2488 registered men's teams, 1969 women's teams, 1042 boys' teams, 966 girls' teams and 274 mixed teams. In 2006 the Irish Hockey Association reported that the gender split among its players was approximately 65% female and 35% male. In its 2008 census, Hockey Australia reported 40,534 male club players and 41,542 female. However, in the United States of America, there are few field hockey clubs, most play taking place between high school or college sides, almost entirely of females. The strength of college field hockey reflects the impact of Title IX which mandated that colleges should fund men's and women's sports programmes comparably.
The game's roots in the English public girls' school mean that the game is associated in the UK with active or overachieving middle class and upper class women. For example, in "Nineteen Eighty-Four", George Orwell's novel set in a totalitarian London, main character Winston Smith initially dislikes Julia, the woman he comes to love, because of "the atmosphere of hockey-fields and cold baths and community hikes and general clean-mindedness which she managed to carry about with her."
Field of play.
Most hockey field dimensions were originally fixed using whole numbers of imperial measures. Nevertheless, metric measurements are now the official dimensions as laid down by the International Hockey Federation (FIH) in the "Rules of Hockey". The pitch is a rectangular field. At each end is a goal high and wide, as well as lines across the field from each end-line (generally referred to as the 23-metre lines or the 25-yard lines) and in the center of the field. A spot in diameter, called the penalty spot or stroke mark, is placed with its centre from the centre of each goal. The shooting circle is from the base line.
Playing Surface.
Historically the game developed on natural grass turf. In the early 1970s, "synthetic grass" fields began to be used for hockey, with the first Olympic Games on this surface being held at the 1976 Montreal edition. Synthetic pitches are now mandatory for all international tournaments and for most national competitions. While hockey is still played on traditional grass fields at some local levels and lesser national divisions, it has been replaced by synthetic surfaces almost everywhere in the western world. There are three main types of artificial hockey surface:
Since the 1970s, sand-based pitches have been favoured as they dramatically speed up the game. However, in recent years there has been a massive increase in the number of "water-based" artificial turfs. Water-based synthetic turfs enable the ball to be transferred more quickly than on sand-based surfaces. It is this characteristic that has made them the surface of choice for international and national league competitions. Water-based surfaces are also less abrasive than sand-based surfaces and reduce the level of injury to players when they come into contact with the surface. The FIH are now proposing that new surfaces being laid should be of a hybrid variety which require less watering. This is due to the negative ecological effects of the high water requirements of water-based synthetic fields. It has also been stated that the decision to make artificial surfaces mandatory greatly favoured more affluent countries who could afford these new pitches.
Rules and play.
The game is played between two teams of whom eleven are permitted to be on the pitch at any one time. The remaining players may be substituted in any combination. There is an unlimited amount of times a team can sub in and out. Substitutions are permitted at any point in the game, apart from between the award and end of a penalty corner; two exceptions to this rule is for injury or suspension of the defending goalkeeper, which is not allowed when playing with a field keep, or a player can exit the field, but you must wait until after the inserter touches the ball to put somebody back in.
Players are permitted to play the ball with the flat of the 'face side' and with the edges of the head and handle of the field hockey stick with the exception that, for reasons of safety, the ball may not be struck 'hard' with a forehand edge stroke, because of the difficulty of controlling the height and direction of the ball from that stroke.
The flat side is always on the "natural" side for a right-handed person swinging the stick at the ball from right to left. Left-handed sticks are rare, but available; however they are pointless as the rules forbid their use in a game. To make a strike at the ball with a left to right swing the player must present the flat of the 'face' of the stick to the ball by 'reversing' the stick head, i.e. by turning the handle through approximately 180°(while a reverse edge hit would turn the stick head through approximately 90° from the position of an upright forehand stoke with the 'face' of the stick head).
Edge hitting of the ball underwent a two-year "experimental period", twice the usual length of an "experimental trial" and is still a matter of some controversy within the sport. Ric Charlesworth, the former Australian coach, has been a strong critic of the unrestricted use of the reverse edge hit. The 'hard' forehand edge hit was banned after similar concerns were expressed about the ability of players to direct the ball accurately, but the reverse edge hit does appear to be more predictable and controllable than its counterpart. This type is hit is now more commonly referred to as the "forehand sweep" where the ball is hit with the flat side or "natural" side of the stick and not the rounded edge. 
Other rules include; no foot to ball contact, no use of hands, no obstructing other players, no high back swing, and no third party. If a player is dribbling the ball and either loses control and kicks the ball or another player interferes that player is not permitted to gain control and continue dribbling. The rules do not allow the person who kicked the ball to gain advantage from the kick, so the ball will automatically be passed on to the opposing team. Conversely, if no advantage is gained from kicking the ball, play should continue. Players may not obstruct another's chance of hitting the ball in any way. No shoving/using your body/stick to prevent advancement in the other team. Penalty for this is the opposing team receives the ball and if the problem continues, the player can be carded. While a player is taking a free hit or starting a corner the back swing of their hit cannot be too high for this is considered dangerous. Finally there may not be three players touching the ball at one time. Two players from opposing teams can battle for the ball, however if another player interferes it is considered third party and the ball automatically goes to the team who only had one player involved in the third party.
Positions.
" hockey positions are discussed, notions of fluidity are very common. Each team can be fielded with a maximum of 11 players and will typically arrange themselves into forwards, midfielders, and defensive players (fullbacks) with players frequently moving between theses lines with the flow of play. Each team may also play with:"
"* a goalkeeper who wears a different color shirt and full protective equipment comprising at least headgear, leg guards and kickers; this player is referred to in the rules as a goalkeeper; or"
"* a field player with goalkeeping privileges wearing a different color shirt and who may wear protective headgear (but not leg guards and kickers or other goalkeeping protective equipment) when inside their defending 23m area; they must wear protective headgear when defending a penalty corner or stroke; this player is referred to in the rules as a player with goalkeeping privileges; or"
"* Only field players; no player has goalkeeping privileges or wears a different color shirt; no player may wear protective headgear except a face mask when defending a penalty corner or stroke."
Formations.
As hockey has a very dynamic style of play, it is difficult to simplify positions to the static formations which are common in association football. Although positions will typically be categorized as either fullback, halfback, midfield/inner or striker, it is important for players to have an understanding of every position on the field. For example, it is not uncommon to see a halfback overlap and end up in either attacking position, with the midfield and strikers being responsible for re-adjusting to fill the space they left. Movement between lines like this is particularly common across all positions.
This fluid Australian culture of hockey has been responsible for developing an international trend towards players occupying spaces on the field, not having assigned positions. Although they may have particular spaces on the field which they are more comfortable and effective as players, they are responsible for occupying the space nearest them. This fluid approach to hockey and player movement, has made it easy for teams to transition between formations such as; "3 at the back", "2 centre halves", "5 at the back" and more.
Goal keepers.
When the ball is inside the circle they are defending and they have their stick in their hand, goalkeepers wearing full protective equipment are permitted to use their stick, feet, kickers or leg guards to propel the ball and to use their stick, feet, kickers, leg guards or any other part of their body to stop the ball or deﬂect it in any direction including over the back-line. Similarly, field players are permitted to use their stick. They are not allowed to use their feet and legs to propel the ball, stop the ball or deflect it in any direction including over the back-line. However, neither goalkeepers, or players with goalkeeping privileges are permitted to conduct themselves in a manner which is dangerous to other players by taking advantage of the protective equipment they wear.
Neither goalkeepers or players with goalkeeping privileges may lie on the ball, however, they are permitted to use arms, hands and any other part of their body to push the ball away. Lying on the ball deliberately will result into a penalty stroke, whereas if an umpire deems a goalkeeper has lay on the ball accidentally (e.g. it gets stuck in their protective equipment), a penalty corner is awarded.
"* The action above is permitted only as part of a goal saving action or to move the ball away from the possibility of a goal scoring action by opponents. It does not permit a goalkeeper or player with goalkeeping privileges to propel the ball forcefully with arms, hands or body so that it travels a long distance"
When the ball is outside the circle they are defending, goalkeepers or players with goalkeeping privileges are only permitted to play the ball with their stick. Further, a goalkeeper, or player with goalkeeping privileges whom is wearing a helmet must not take part in the match outside the 23m area they are defending, except when taking a penalty stroke. A goalkeeper must wear protective headgear at all times, except when taking a penalty stroke.
General play.
For the purposes of the rules, all players on the team in possession of the ball are attackers, and those on the team without the ball are defenders, yet throughout the game being played you are always"defending" your goal and "attacking" the opposite goal.
The match is officiated by two field umpires. Traditionally each umpire generally controls half of the field, divided roughly diagonally. These umpires are often assisted by a technical bench including a timekeeper and record keeper.
Prior to the start of the game, a coin is tossed and the winning captain can choose a starting end or whether to start with the ball. The game consists of two halves of 35 minutes with a 5-minute break at half time before changing ends. At the start of each period, as well as after goals are scored, play is started with a pass from the centre of the field. All players must start in their defensive half (apart from the player making the pass), but the ball may be played in any direction along the floor. Each team starts with the ball in one half, and the team that conceded the goal has possession for the restart. Teams trade sides at halftime.
Field players may only play the ball with the face of the stick. If the back side of the stick is used, it is a penalty and the other team will get the ball back. Tackling is permitted as long as the tackler does not make contact with the attacker or the other persons stick before playing the ball (contact after the tackle may also be penalized if the tackle was made from a position where contact was inevitable). Further, the player with the ball may not deliberately use his body to push a defender out of the way.
Field players may not play the ball with their feet, but if the ball accidentally hits the feet, and the player gains no benefit from the contact, then the contact is not penalized. Although there has been a change in the wording of this rule from 1 January 2007, the current FIH umpires' briefing instructs umpires not to change the way they interpret this rule.
Obstruction typically occurs in three circumstances – when a defender comes between the player with possession and the ball in order to prevent them tackling; when a defender's stick comes between the attacker's stick and the ball or makes contact with the attacker's stick or body; and also when blocking the opposition's attempt to tackle a teammate with the ball (called "third party obstruction").
When the ball passes completely over the sidelines (on the sideline is still in), it is returned to play with a sideline hit, taken by a member of the team whose players were not the last to touch the ball before crossing the sideline. The ball must be placed on the sideline, with the hit taken from as near the place the ball went out of play as possible. If it crosses the back line after last touched by an attacker, a hit. A 15 m hit is also awarded for offenses committed by the attacking side within 15 m of the end of the pitch they are attacking.
Set plays.
Set plays are often utilized for specific situations such as a penalty corner or free hit. For instance, many teams have penalty corner variations that they can use to beat the defensive team. The coach may have plays that sends the ball between two defenders and let the player attack the opposing teams goal. There are no set plays unless your team has them.
Free hits.
Free hits are awarded when offences are committed outside the scoring circles (the term 'free hit' is standard usage but the ball need not be hit). The ball may be hit, pushed or lifted in any direction by the team offended against. The ball can be lifted from a free hit but not by hitting, you must flick or scoop to lift from a free hit. (In previous rules versions hits in the area outside the circle in open play have been permitted but lifting one direction from a free hit prohibited). Opponents must move from the ball when a free hit is awarded. A free hit must be taken from within playing distance of the place of the offence for which it was awarded and the ball must be stationary when the free-hit is taken.
As mentioned above, a 15 m hit is awarded if an attacking player commits a foul forward of that line, or if the ball passes over the back line off an attacker. These free hits are taken in line with where the foul was committed (taking a line parallel with the sideline between where the offence was committed, or the ball went out of play). When an attacking free hit is awarded within 5 m of the circle everyone including the person taking the penalty must be five metres from the circle and everyone apart from the person taking the free hit must be five metres away from the ball. When taking an attacking free hit the ball may not be hit straight into the circle if you are within your attacking 23 metre area (25 yard area). It must travel 5 metres before going in.
2009 experimental changes.
In February 2009 the FIH introduced, as a "Mandatory Experiment" for international competition, an updated version of free hit rule. The changes allows a player taking a free hit to pass the ball to themselves. Importantly, this is not a "play on" situation, but to the untrained eye it may appear to be. The player must play the ball any distance in two separate motions, before continuing as if it were a play-on situation. They may raise an aerial or overhead immediately as the second action, or any other stroke permitted by the rules of field hockey. At the high school level, this is called a self-pass and was adopted in Pennsylvania in 2010 as a legal technique for putting the ball in play.
Also, all players (from both teams) must be at least 5 m from any free hit awarded to the attack within the 23 m area. Additionally, no free hits to the attack are permitted within 5m of the circle, so if a free hit is awarded inside this area it must be dragged back outside this zone. The ball may not travel directly into the circle from a free hit to the attack within the 23 m area without first being touched by another player or being dribbled at least 5 m by a player making a "self-pass". These experimental rules apply to all free hit situations, including sideline and corner hits. National Associations may also choose to introduce these rules for their domestic competitions.
Corner.
A corner is awarded if the ball goes over the back line after last being touched by a defender, provided they do not play it over the back line deliberately, in which case a penalty corner is awarded. Corners are played by the attacking team and involve a free hit on the sideline 5 m from the corner of the field closest to where the ball went out of play, this rule, however, was changed in 2015. The ball is taken up to the 23 metre line, in line with where it went out, the rest of the rules for a long corner stayed the same. These restarts are also known as long corners (as opposed to short corner which is an alternative name for the penalty corner). The defense must wait until the offender passes the ball in. The offender has to pull the ball out of the circle before trying to make a goal.
Penalty corner.
The short or penalty corner is awarded: 
Short corners begin with five defenders (usually including the keeper) positioned behind the back line and at least 10 yards from the nearest goal post. All other players in the defending team must be beyond the centre line, that is not in their 'own' half of the pitch, until the ball is in play. Attacking players begin the play standing outside the scoring circle, except for one attacker who starts the corner by playing the ball from a mark 10 m either side of the goal (the circle has a 14.63 m radius). This player puts the ball into play by pushing or hitting the ball to the other attackers outside the circle; the ball must pass outside the circle and then put back into the circle before the attackers may make a shot at the goal from which a goal can be scored. FIH rules do not forbid a shot at goal before the ball leaves the circle after being 'inserted', nor is a shot at the goal from outside the circle prohibited, but a goal cannot be scored at all if the ball has not gone out of the circle and cannot be scored from a shot from outside the circle if it is not again played by an attacking player before it enters the goal.
For safety reasons, the first shot of a penalty corner must not exceed 460 mm high (the height of the "backboard" of the goal) at the point it crosses the goal line if it is hit. However, if the ball is deemed to be below backboard height, the ball can be subsequently deflected above this height by another player (defender or attacker), providing that this deflection does not lead to danger. Note that the "Slap" stroke (a sweeping motion towards the ball, where the stick is kept on or close to the ground when striking the ball) is classed as a hit, and so the first shot at goal must be below backboard height for this type of shot also.
If the first shot at goal in a short corner situation is a push, flick or scoop, in particular the "drag flick" (which has become popular at international and national league standards), the shot is permitted to rise above the height of the backboard, as long as the shot is not deemed dangerous to any opponent. This form of shooting was developed because it is not height restricted in the same way as the first hit shot at the goal and players with good technique are able to drag-flick with as much power as many others can hit a ball.
Penalty stroke.
A penalty stroke is awarded when a defender commits a foul in the circle (accidental or otherwise) that prevents a probable goal or commits a deliberate foul in the circle or if defenders repeatedly run from the back line too early at a penalty corner. The penalty stroke is taken by a single attacker in the circle, against the goalkeeper, from a spot 6.4 m from goal. The ball is played only once at goal by the attacker using a push, flick or scoop stroke. If the shot is saved, play is restarted with a 15 m hit to the defenders. When a goal is scored, play is restarted in the normal way.
Dangerous play and raised balls.
According to the current Rules of Hockey 2013 issued by the FIH there are only two criteria for a dangerously played ball. The first is legitimate evasive action by an opponent (what constitutes legitimate evasive action is an umpiring judgment). The second is specific to the rule concerning a shot at goal at a penalty corner but is generally, if somewhat inconsistently, applied throughout the game and in all parts of the pitch: it is that a ball lifted above knee height and at an opponent who is within 5m of the ball is certainly dangerous.
The velocity of the ball is not mentioned in the rules concerning a dangerously played ball. A ball that hits a player above the knee may on some occasions not be penalized, this is in the umpire's discretion. A jab tackle for example, might accidentally lift the ball above knee height into an opponent from close range but at such low velocity as not to be, in the opinion of the umpire, dangerous play. In the same way a high velocity hit at very close range into an opponent, but below knee height, could be considered to be dangerous or reckless play in the view of the umpire, especially when safer alternatives are open to the striker of the ball.
A ball that has been lifted high so that it will fall among close opponents may be deemed to be potentially dangerous and play may be stopped for that reason. A lifted ball that is falling to a player in clear space may be made potentially dangerous by the actions of an opponent closing to within 5m of the receiver before the ball has been controlled to ground – a rule which is often only loosely applied; the distance allowed is often only what might be described as playing distance, 2–3 m, and opponents tend to be permitted to close on the ball as soon as the receiver plays it: these unofficial variations are often based on the umpire's perception of the skill of the players i.e. on the level of the game, in order to maintain game flow, which umpires are in general in both Rules and Briefing instructed to do, by not penalising when it is unnecessary to do so, this is also a matter in the umpire's discretion.
The term "falling ball" is important in what may be termed encroaching offences. It is generally only considered an offence to encroach on an opponent receiving a lifted ball that has been lifted to above head height (although the height is not specified in rule) and is falling. So, for example, a lifted shot at the goal which is still rising as it crosses the goal line (or would have been rising as it crossed the goal line) can be legitimately followed up by any of the attacking team looking for a rebound.
In general even potentially dangerous play is not penalised if an opponent is not disadvantage by it or, obviously, not injured by it so that he cannot continue. A personal penalty, that is a caution or a suspension, rather than a team penalty, such as a free ball or a penalty corner, may be (many would say should be or even must be, but again this is in the umpire's discretion) issued to the guilty party after an advantage allowed by the umpire has been played out in any situation where an offence has occurred, including dangerous play (but once advantage has been allowed the umpire cannot then call play back and award a team penalty).
It is not an offence to lift the ball over an opponent's stick (or body on the ground), provided that it is done with consideration for the safety of the opponent and not dangerously. For example, a skillful attacker may lift the ball over a defenders stick or prone body and run past them, however if the attacker lifts the ball into or at the defender's body, this would almost certainly be regarded as dangerous.
It is not against the rules to bounce the ball on the stick and even to run with it while doing so, as long as that does not lead to a potentially dangerous conflict with an opponent who is attempting to make a tackle. For example, two players trying to play at the ball in the air at the same time, would probably be considered a dangerous situation and it is likely that the player who first put the ball up or who was so 'carrying' it would be penalised.
Dangerous play rules also apply to the usage of the stick when approaching the ball, making a stroke at it (replacing what was at one time referred to as the "sticks" rule, which once forbade the raising of any part of the stick above the shoulder during any play. This last restriction has been removed but the stick should still not be used in a way that endangers an opponent) or attempting to tackle, (fouls relating to tripping, impeding and obstruction). The use of the stick to strike an opponent will usually be much more severely dealt with by the umpires than offences such as barging, impeding and obstruction with the body, although these are also dealt with firmly, especially when these fouls are intentional: field hockey is a non-contact sport.
Players may not play or attempt to play at the ball above their shoulders unless trying to save a shot that could go into the goal, in which case they are permitted to stop the ball or deflect it safely away. A swing, as in a hit, at a high shot at the goal (or even wide of the goal) will probably be considered dangerous play if at opponents within 5 m and such a stroke would be contrary to rule in these circumstances anyway.
Within the English National League it is now a legal action to take a ball above shoulder height if completed using a controlled action.
Warnings and suspensions.
Hockey uses a three-tier penalty card system of warnings and suspensions:
In addition to their colours, field hockey penalty cards are often shaped differently, so they can be recognized easily. Green cards are normally triangular, yellow cards rectangular and red cards circular.
Unlike football, a player may receive more than one green or yellow card. However, they cannot receive the same card for the same offence (for example two yellows for dangerous play), and the second must always be a more serious card. In the case of a second yellow card for a different breach of the rules (for example a yellow for deliberate foot, and a second later in the game for dangerous play) the temporary suspension would be expected to be of considerably longer duration than the first. However, local playing conditions may mandate that cards are awarded only progressively, and not allow any second awards.
Referees may also advance a free-hit by up to 10 m for dissent or other misconduct after a penalty has been awarded; or, if the free-hit would have been in the attacking 23 m area, upgrade the penalty to a penalty corner.
Scoring.
The teams' object is to play the ball into their attacking circle and, from there, hit, push or flick the ball into the goal, scoring a goal. The team with more goals after 70 minutes wins the game. The playing time may be shortened, particularly when younger players are involved, or for some tournament play.
Tie breaking.
In many competitions (such as regular club competition, or in pool games in FIH international tournaments such as the Olympics or the World Cup), a tied result stands and the overall competition standings are adjusted accordingly. Since March 2013, when tie-breaking is required, the official FIH Tournament Regulations mandate to no longer have extra time and go directly into a penalty shoot-out when a classification match ends in a tie. However, many associations follow the previous procedure consisting of two periods of 7.5 minutes of "golden goal" extra time during which the game ends as soon as one team scores.
Rule change procedure.
The FIH implemented a two-year rules cycle with the 2007–08 edition of the rules, with the intention that the rules be reviewed on a biennial basis. The 2009 rulebook was officially released in early March 2009 (effective 1 May 2009), however the FIH published the major changes in February. The current rule book is effective from 1 January 2015.
The FIH has adopted a policy of including major changes to the rules as "Mandatory Experiments", showing that they must be played at international level, but are treated as experimental and will be reviewed before the next rulebook is published and either changed, approved as permanent rules, or deleted.
Recent examples of such experiments include a fixed 2-minute suspension for a green card and a (limited) ability to request video umpiring decisions.
Local rules.
There are sometimes minor variations in rules from competition to competition; for instance, the duration of matches is often varied for junior competitions or for carnivals. Different national associations also have slightly differing rules on player equipment.
The new Euro Hockey League has made major alterations to the rules to aid television viewers, such as splitting the game into four quarters, and to try to improve player behaviour, such as a two-minute suspension for green cards—the latter was also used in the 2010 World Cup. In the United States, the NCAA has its own rules for inter-collegiate competitions; high school associations similarly play to different rules, usually using the rules published by the National Federation of State High School Associations (NFHS). This article assumes FIH rules unless otherwise stated. USA Field Hockey produces an annual summary of the differences.
In the United States, the games at the junior high level consist of two 25-minute halves, while the high school level consists of two 30-minute halves. Many private American schools play 25-minute halves, and some have adopted FIH rules rather than NFHS rules. Players are required to wear mouth guards and shin guards in order to play the game. Also, there is a newer rule requiring certain types of sticks be used. In recent years, the NFHS rules have moved closer to FIH, but in 2011 a new rule requiring protective eyewear was introduced for the 2011 Fall season. The 'cage style' goggles favored by US high school lacrosse and permitted in high school field hockey is not permitted under FIH rules.
Equipment.
Field hockey stick.
Each player carries a "stick" that normally measures between 80–95 cm (31–38") long; shorter or longer sticks are available. Sticks were traditionally made of wood, but are now often made also with fibreglass, kevlar or carbon fibre composites. Metal is forbidden from use in field hockey sticks, due to the risk of injury from sharp edges if the stick were to break. The stick has a rounded handle, has a J-shaped hook at the bottom, and is flattened on the left side (when looking down the handle with the hook facing upwards). All sticks are right handed. Left handed sticks are not permitted.
There was traditionally a slight curve (called the bow, or rake) from the top to bottom of the face side of the stick and another on the 'heel' edge to the top of the handle (usually made according to the angle at which the handle part was inserted into the splice of the head part of the stick), which assisted in the positioning of the stick head in relation to the ball and made striking the ball easier and more accurate.
The hook at the bottom of the stick was only recently the tight curve (Indian style) that we have nowadays. The older 'English' sticks had a longer bend, making it very hard to use the stick on the reverse. For this reason players now use the tight curved sticks.
The handle makes up the about the top third of the stick. It is wrapped in a grip similar to that used on tennis racket. The grip may be made of a variety of materials, including chamois leather, which many players think improves grip in the wet.
It was recently discovered that increasing the depth of the face bow made it easier to get high speeds from the dragflick and made the stroke easier to execute. At first, after this feature was introduced, the Hockey Rules Board placed a limit of 50 mm on the maximum depth of bow over the length of the stick but experience quickly demonstrated this to be excessive. New rules now limit this curve to under 25 mm so as to limit the power with which the ball can be flicked.
Field hockey ball.
Standard field hockey balls are hard spherical balls, made of plastic (sometimes over a cork core), and are usually white, although they can be any colour as long as they contrast with the playing surface. The balls have a circumference of and a mass of . The ball is often covered with indentations to reduce aquaplaning that can cause an inconsistent ball speed on wet surfaces.
Goalkeeping equipment.
The 2007 rulebook has seen major changes regarding goalkeepers. A fully equipped goalkeeper must wear a helmet, leg guards and kickers. Usually the field hockey goalkeepers must wear extensive additional protective equipment including chest guards, padded shorts, heavily padded hand protectors, groin protectors, neck guards, arm guards, and like all players, they must carry a stick. A goalie may not cross the 23 m line, the sole exception to this being if the goalkeeper is to take a penalty stroke at the other end of the field, when the clock is stopped. The goalkeeper can also remove their helmet for this action. However, if the goalkeeper elects to wear only a helmet (and a different colored shirt), they may cross the 23 m line if they have removed their helmet (and placed it safely off the field of play). If play returns to the circle without them having opportunity to replace the helmet, this player still has "goalkeeping privileges", that is, they are not limited to using their stick to play the ball whilst it is in the circle, and the helmet must be worn whilst defending penalty corners and penalty strokes but the best thing to do would be to wear it at all times. While goaltenders are allowed to use their feet and hands to clear the ball, they too are only allowed to use one side of their stick. Slide tackling is permitted as long as it is with the intention of clearing the ball, not aimed at a player.
It is now also even possible for teams to have a full eleven outfield players and no goalkeeper at all. No player may wear a helmet or other goalkeeping equipment, neither will any player be able to play the ball with any other part of the body than with their stick. This may be used to offer a tactical advantage, or to allow for play to commence if no goalkeeper or kit is available.
Tactics.
The basic tactic in field hockey, as in association football and many other team games, is to outnumber the opponent in a particular area of the field at a moment in time. When in possession of the ball this temporary numerical superiority can be used to pass the ball around opponents so that they cannot effect a tackle because they cannot get within playing reach of the ball and to further use this numerical advantage to gain time and create clear space for making scoring shots on the opponent's goal. When not in possession of the ball numerical superiority is used to isolate and channel an opponent in possession and 'mark out' any passing options so that an interception or a tackle may be made to gain possession. Highly skillful players can sometimes get the better of more than one opponent and retain the ball and successfully pass or shoot but this tends to use more energy than quick early passing.
Every player has a role depending on their relationship to the ball if the team communicates throughout the play of the game. There will be players on the ball (offensively - ball carriers; defensively - pressure, support players, and movement players.
The main methods by which the ball is moved around the field by players are a) passing b) pushing the ball and running with it controlled to the front or right of the body and c)"dribbling"; where the player controls the ball with the stick and moves in various directions with it to elude opponents. To make a pass the ball may be propelled with a pushing stroke, where the player uses their wrists to push the stick head through the ball while the stick head is in contact with it; the "flick" or "scoop", similar to the push but with an additional arm and leg and rotational actions to lift the ball off the ground; and the "hit", where a swing at ball is taken and contact with it is often made very forcefully, causing the ball to be propelled at velocities in excess of . In order to produce a powerful hit, usually for travel over long distances or shooting at the goal, the stick is raised higher and swung with maximum power at the ball, a stroke sometimes known as a "drive".
Tackles are made by placing the stick into the path of the ball or playing the stick head or shaft directly at the ball. To increase the effectiveness of the tackle, players will often place the entire stick close to the ground horizontally, thus representing a wider barrier. To avoid the tackle, the ball carrier will either pass the ball to a teammate using any of the push, flick, or hit strokes, or attempt to maneuver or "drag" the ball around the tackle, trying to deceive the tackler.
In recent years, the penalty corner has gained importance as a goal scoring opportunity. Particularly with the technical development of the drag flick. Tactics at penalty corners to set up time for a shot with a drag flick or a hit shot at the goal involve various complex plays, including multiple passes before a deflections towards the goal is made but the most common method of shooting is the direct flick or hit at the goal.
At the highest level, field hockey is a fast-moving, highly skilled sport, with players using fast moves with the stick, quick accurate passing, and hard hits, in attempts to keep possession and move the ball towards the goal. Tackling with physical contact and otherwise physically obstructing players is not permitted, Some of the tactics used resemble football (soccer), but with greater ball speed.
With the 2009 changes to the rules regarding free hits in the attacking 23m area, the common tactic of hitting the ball hard into the circle was forbidden. Although at higher levels this was considered tactically risky and low-percentage at creating scoring opportunities, it was used with some effect to 'win' penalty corners by forcing the ball onto a defender's foot or to deflect high (and dangerously) off a defender's stick. The FIH felt it was a dangerous practice that could easily lead to raised deflections and injuries in the circle, which is often crowded at a free-hit situation, and outlawed it.
International competition.
The biggest two field hockey tournaments are the Olympic Games tournament, and the Hockey World Cup, which is also held every 4 years. Apart from this, there is the Champions Trophy held each year for the six top-ranked teams. Field hockey has also been played at the Commonwealth Games since 1998. Amongst the men, India lead in Olympic competition, having won 8 golds (6 successive in row). Amongst the women, Australia and Netherlands have 3 Olympic golds while Netherlands has clinched the World Cup 6 times. The Sultan Azlan Shah Hockey Tournament and Sultan Ibrahim Ismail Hockey Tournament is for the junior team but both tournaments are held annually in Malaysia, is becoming a prominent field hockey tournament where teams from around the world participate to win the cup.
Pakistan dominated men's hockey until the early 1980s, winning three of the first five world cups, but have become less prominent with the ascendancy of the Netherlands, Germany, New Zealand, Australia and Spain since the late 1980s, as grass playing surfaces were replaced with artificial turf (which conferred increased importance on athleticism). Other notable men's nations include Argentina, England (who combine with other British "Home Nations" to form the Great Britain side at Olympic events) and South Korea. Despite their recent drop in international rankings, Pakistan still holds the record of four World Cup wins.
Netherlands, Australia and Argentina are the most successful national teams among women. The Netherlands was the predominant women's team before field hockey was added to Olympic events. In the early 1990s, Australia emerged as the strongest women's country although retirement of a number of players weakened the team. Argentina improved its play on the 2000s, heading IFH rankings in 2003, 2010 and 2013. Other prominent women's teams are China, South Korea, Germany and South Africa.
This is a list of the major International field hockey tournaments, in chronological order. Tournaments included are:
Although invitational or not open to all countries, the following are also considered international tournaments:
Variants.
Hockey 5s.
As the name suggests, Hockey 5s is a hockey variant which features five players on each team (which must include a goalkeeper). The field of play is 55 m long and 41.70 m wide—this is approximately half the size of a regular pitch. Few additional markings are needed as there is no penalty circle nor penalty corners; shots can be taken from anywhere on the pitch. Penalty strokes are replaced by a "challenge" which is like the one-on-one method used in a penalty shoot-out. The duration of the match is three 12-minute periods with an interval of two minutes between periods. The rules are simpler and it is intended that the game is faster, creating more shots on goal with less play in midfield, and more attractive to spectators.
An Asian qualification tournament for two places at the 2014 Youth Olympic Games was the first time an FIH event used the Hockey 5s format. Hockey 5s was also used for the Youth Olympic hockey tournament, and at the Pacific Games in 2015.

</doc>
<doc id="10887" url="https://en.wikipedia.org/wiki?curid=10887" title="Finagle's law">
Finagle's law

Finagle's Law of Dynamic Negatives, also known as Melody's law or Finagle's corollary to Murphy's law, is usually rendered:
The term "Finagle's Law" was first used by John W. Campbell, Jr., the influential editor of "Astounding Science Fiction" (later "Analog"). He used it frequently in his editorials for many years in the 1940s to 1960s but it never came into general usage the way Murphy's Law has.
Variants.
One variant (known as O'Toole's Corollary of Finagle's Law) favored among hackers is a takeoff on the second law of thermodynamics (also known as entropy):
In the "Star Trek" episode "The Ultimate Computer", Dr. McCoy refers to an alcoholic drink known as the "Finagle's Folly," apparently a reference to "Finagle's Law." In Season 2 episode "Amok Time" (written by Theodore Sturgeon, 1967), Captain Kirk tells Spock, "As one of Finagle's Laws puts it: 'Any home port the ship makes will be somebody else's, not mine.'"
The term "Finagle's law" was popularized by science fiction author Larry Niven in several stories depicting a frontier culture of asteroid miners; this "Belter" culture professed a religion or running joke involving the worship of the dread god Finagle and his mad prophet Murphy.
"Finagle's Law" can also be the related belief "Inanimate objects are out to get us", also known as Resistentialism.
Similar to Finagle's Law is the verbless phrase of the German novelist Friedrich Theodor Vischer: "die Tücke des Objekts" (the perfidy of inanimate objects).
A related concept, the "Finagle factor", is an "ad hoc" multiplicative or additive term in an equation which can only be justified by the fact that it gives more correct results. Also known as Finagle's variable constant, it is sometimes defined as the right answer divided by your answer.

</doc>
<doc id="10890" url="https://en.wikipedia.org/wiki?curid=10890" title="Fundamental interaction">
Fundamental interaction

Fundamental interactions, also known as fundamental forces, are the interactions in physical systems that do not appear to be reducible to more basic interactions. There are four conventionally accepted fundamental interactions—gravitational, electromagnetic, strong nuclear, and weak nuclear. Each one is understood as the dynamics of a "field". The gravitational force is modelled as a continuous classical field. The other three are each modelled as discrete quantum fields, and exhibit a measurable unit or "elementary particle".
The two nuclear interactions produce strong forces at minuscule, subatomic distances. The strong nuclear interaction is responsible for the binding of atomic nuclei. The weak nuclear interaction also acts on the nucleus, mediating radioactive decay. Electromagnetism and gravity produce significant forces at macroscopic scales where the effects can be seen directly in every day life. Electrical and magnetic fields tend to cancel each other out when large collections of objects are considered, so over the largest distances (on the scale of planets and galaxies), gravity tends to be the dominant force.
Theoretical physicists working beyond the Standard Model seek to quantize the gravitational field toward predictions that particle physicists can experimentally confirm, thus yielding acceptance to a theory of quantum gravity (QG) (Phenomena suitable to model as a fifth force—perhaps an added gravitational effect—remain widely disputed.) Other theorists seek to unite the electroweak and strong fields within a Grand Unified Theory (GUT). While all four fundamental interactions are widely thought to align on a highly minuscule scale, particle accelerators cannot produce the massive energy levels required to experimentally probe at that Planck scale (which would experimentally confirm such theories.) Yet some theories, such as the string theory, seek both QG and GUT within one framework, unifying all four fundamental interactions along with mass generation within a theory of everything (ToE).
General relativity.
In his 1687 theory, Isaac Newton postulated space as an infinite and unalterable physical structure existing before, within, and around all objects while their states and relations unfold at a constant pace everywhere, thus absolute space and time. Inferring that all objects bearing mass approach at a constant rate, but collide by impact proportional to their masses, Newton inferred that matter exhibits an attractive force. His law of universal gravitation mathematically stated it to span the entire universe instantly (despite absolute time), or, if not actually a force, to be instant interaction among all objects (despite absolute space.) As conventionally interpreted, Newton's theory of motion modelled a "central force" without a communicating medium. Thus Newton's theory violated the first principle of mechanical philosophy, as stated by Descartes, "No action at a distance". Conversely, during the 1820s, when explaining magnetism, Michael Faraday inferred a "field" filling space and transmitting that force. Faraday conjectured that ultimately, all forces unified into one.
In the early 1870s, James Clerk Maxwell unified electricity and magnetism as effects of an electromagnetic field whose third consequence was light, travelling at constant speed in a vacuum. The electromagnetic field theory contradicted predictions of Newton's theory of motion, unless physical states of the luminiferous aether—presumed to fill all space whether within matter or in a vacuum and to manifest the electromagnetic field—aligned all phenomena and thereby held valid the Newtonian principle relativity or invariance. Disfavouring hypotheses at unobservables, Albert Einstein discarded the aether, and aligned electrodynamics with relativity by denying absolute space and time, and stating relative space and time. The two phenomena altered in the vicinity of an object measured to be in motion—length contraction and time dilation for the object experienced to be in relative motion—Einstein's principle special relativity, published in 1905.
Special relativity was accepted as a theory too. It rendered Newton's theory of motion apparently untenable, especially since Newtonian physics postulated an object's mass to be constant. A consequence of special relativity is mass being a variant form of energy, condensed into an object. By the equivalence principle, published by Einstein in 1907, gravitation is indistinguishable from acceleration, perhaps two phenomena sharing a mechanism. That year, Hermann Minkowski modelled special relativity to a unification of space and time, 4D spacetime. Stretching the three spatial dimensions onto the single dimension of time's arrow, Einstein arrived at the general theory of relativity in 1915. Einstein interpreted space as a substance, "Einstein-aether", whose physical properties receive motion from an object and transmit it to other objects while modulating events unfolding. Equivalent to energy, mass contracts space, which dilates time—events unfold more slowly—establishing local tension. The object relieves it in the likeness of a free fall at light speed along the pathway of least resistance, a straight line's equivalent on the curved surface of 4D spacetime, a pathway termed "worldline".
Einstein abolished "action at a distance" by theorizing a gravitational field—4D spacetime—that waves while transmitting motion across the universe at light speed. All objects always travel at light speed in 4D spacetime. At zero relative speed, an object is observed to travel none through space, but age most rapidly. That is, an object at relative rest in 3D space exhibits its constant energy to an observer by exhibiting top speed along 1D time flow. Conversely, at highest relative speed, an object traverses 3D space at light speed, yet is ageless, none of its constant energy available to internal motion as flow along 1D time. Whereas Newtonian inertia is an idealized case of an object either keeping rest or holding constant velocity by its hypothetical existence in a universe otherwise devoid of matter, Einsteinian inertia is indistinguishable from an object experiencing no acceleration by existing in a gravitational field possibly full of matter distributed uniformly. Conversely, even massless energy manifests gravitation—which is acceleration—on local objects by "curving" the surface of 4D spacetime. Physicists renounced belief that motion must be mediated by a "force".
Standard Model.
The electromagnetic, strong, and weak interactions associate with elementary particles, whose behaviours are modelled in quantum mechanics (QM). For predictive success with QM's probabilistic outcomes, particle physics conventionally models QM events across a field set to special relativity, altogether relativistic quantum field theory (QFT). Force particles, called gauge bosons—"force carriers" or "messenger particles" of underlying fields—interact with matter particles, called fermions. Everyday matter is atoms, composed of three fermion types: up-quarks and down-quarks constituting, as well as electrons orbiting, the atom's nucleus. Atoms interact, form molecules, and manifest further properties through electromagnetic interactions among their electrons absorbing and emitting photons, the electromagnetic field's force carrier, which if unimpeded traverse potentially infinite distance. Electromagnetism's QFT is quantum electrodynamics (QED).
The electromagnetic interaction was modelled with the weak interaction, whose force carriers are W and Z bosons, traversing the minuscule distance, in electroweak theory (EWT). Electroweak interaction would operate at such high temperatures as soon after the presumed Big Bang, but, as the early universe cooled, split into electromagnetic and weak interactions. The strong interaction, whose force carrier is the gluon, traversing minuscule distance among quarks, is modeled in quantum chromodynamics (QCD). EWT, QCD, and the Higgs mechanism, whereby the Higgs field manifests Higgs bosons that interact with some quantum particles and thereby endow those particles with mass comprise particle physics' Standard Model (SM). Predictions are usually made using calculational approximation methods, although such perturbation theory is inadequate to model some experimental observations (for instance bound states and solitons.) Still, physicists widely accept the Standard Model as science's most experimentally confirmed theory.
Beyond the Standard Model, some theorists work to unite the electroweak and strong interactions within a Grand Unified Theory (GUT). Some attempts at GUTs hypothesize "shadow" particles, such that every known matter particle associates with an undiscovered force particle, and vice versa, altogether supersymmetry (SUSY). Other theorists seek to quantize the gravitational field by the modelling behaviour of its hypothetical force carrier, the graviton and achieve quantum gravity (QG). One approach to QG is loop quantum gravity (LQG). Still other theorists seek both QG and GUT within one framework, reducing all four fundamental interactions to a Theory of Everything (ToE). The most prevalent aim at a ToE is string theory, although to model matter particles, it added SUSY to force particles—and so, strictly speaking, became superstring theory. Multiple, seemingly disparate superstring theories were unified on a backbone, M-theory. Theories beyond the Standard Model remain highly speculative, lacking great experimental support.
Overview of the fundamental interactions.
In the conceptual model of fundamental interactions, matter consists of fermions, which carry properties called charges and spin ± (intrinsic angular momentum ±, where ħ is the reduced Planck constant). They attract or repel each other by exchanging bosons.
The interaction of any pair of fermions in perturbation theory can then be modelled thus:
The exchange of bosons always carries energy and momentum between the fermions, thereby changing their speed and direction. The exchange may also transport a charge between the fermions, changing the charges of the fermions in the process (e.g., turn them from one type of fermion to another). Since bosons carry one unit of angular momentum, the fermion's spin direction will flip from + to − (or vice versa) during such an exchange (in units of the reduced Planck's constant).
Because an interaction results in fermions attracting and repelling each other, an older term for "interaction" is force.
According to the present understanding, there are four fundamental interactions or forces: gravitation, electromagnetism, the weak interaction, and the strong interaction. Their magnitude and behaviour vary greatly, as described in the table below. Modern physics attempts to explain every observed physical phenomenon by these fundamental interactions. Moreover, reducing the number of different interaction types is seen as desirable. Two cases in point are the unification of:
Both magnitude ("relative strength") and "range", as given in the table, are meaningful only within a rather complex theoretical framework. It should also be noted that the table below lists properties of a conceptual scheme that is still the subject of ongoing research.
The modern (perturbative) quantum mechanical view of the fundamental forces other than gravity is that particles of matter (fermions) do not directly interact with each other, but rather carry a charge, and exchange virtual particles (gauge bosons), which are the interaction carriers or force mediators. For example, photons mediate the interaction of electric charges, and gluons mediate the interaction of color charges.
The interactions.
Gravity.
"Gravitation" is by far the weakest of the four interactions. The weakness of gravity can easily be demonstrated by suspending a pin using a simple magnet (such as a refrigerator magnet). The magnet is able to hold the pin against the gravitational pull of the entire Earth.
Yet gravitation is very important for macroscopic objects and over macroscopic distances for the following reasons. Gravitation:
Even though electromagnetism is far stronger than gravitation, electrostatic attraction is not relevant for large celestial bodies, such as planets, stars, and galaxies, simply because such bodies contain equal numbers of protons and electrons and so have a net electric charge of zero. Nothing "cancels" gravity, since it is only attractive, unlike electric forces which can be attractive or repulsive. On the other hand, all objects having mass are subject to the gravitational force, which only attracts. Therefore, only gravitation matters on the large-scale structure of the universe.
The long range of gravitation makes it responsible for such large-scale phenomena as the structure of galaxies and black holes and it retards the expansion of the universe. Gravitation also explains astronomical phenomena on more modest scales, such as planetary orbits, as well as everyday experience: objects fall; heavy objects act as if they were glued to the ground, and animals can only jump so high.
Gravitation was the first interaction to be described mathematically. In ancient times, Aristotle hypothesized that objects of different masses fall at different rates. During the Scientific Revolution, Galileo Galilei experimentally determined that this was not the case — neglecting the friction due to air resistance, and buoyancy forces if an atmosphere is present (e.g. the case of a dropped air-filled balloon vs a water-filled balloon) all objects accelerate toward the Earth at the same rate. Isaac Newton's law of Universal Gravitation (1687) was a good approximation of the behaviour of gravitation. Our present-day understanding of gravitation stems from Albert Einstein's General Theory of Relativity of 1915, a more accurate (especially for cosmological masses and distances) description of gravitation in terms of the geometry of spacetime.
Merging general relativity and quantum mechanics (or quantum field theory) into a more general theory of quantum gravity is an area of active research. It is hypothesized that gravitation is mediated by a massless spin-2 particle called the graviton.
Although general relativity has been experimentally confirmed (at least for weak fields ) on all but the smallest scales, there are rival theories of gravitation. Those taken seriously by itation neede the physics community all reduce to general relativity in some limit, and the focus of observational work is to establish limitations on what deviations from general relativity are possible.
Proposed extra dimensions could explain why the gravity force is so weak.
Electroweak interaction.
Electromagnetism and weak interaction appear to be very different at everyday low energies. They can be modeled using two different theories. However, above unification energy, on the order of 100 GeV, they would merge into a single electroweak force.
Electroweak theory is very important for modern cosmology, particularly on how the universe evolved. This is because shortly after the Big Bang, the temperature was approximately above 10 K. Electromagnetic force and weak force were merged into a combined electroweak force.
For contributions to the unification of the weak and electromagnetic interaction between elementary particles, Abdus Salam, Sheldon Glashow and Steven Weinberg were awarded the Nobel Prize in Physics in 1979.
Electromagnetism.
Electromagnetism is the force that acts between electrically charged particles. This phenomenon includes the electrostatic force acting between charged particles at rest, and the combined effect of electric and magnetic forces acting between charged particles moving relative to each other.
Electromagnetism is infinite-ranged like gravity, but vastly stronger, and therefore describes a number of macroscopic phenomena of everyday experience such as friction, rainbows, lightning, and all human-made devices using electric current, such as television, lasers, and computers. Electromagnetism fundamentally determines all macroscopic, and many atomic levels, properties of the chemical elements, including all chemical bonding.
In a four kilogram (~1 gallon) jug of water there are
formula_1
of total electron charge. Thus, if we place two such jugs a meter apart, the electrons in one of the jugs repel those in the other jug with a force of
formula_2
This is larger than the planet Earth would weigh if weighed on another Earth. The atomic nuclei in one jug also repel those in the other with the same force. However, these repulsive forces are canceled by the attraction of the electrons in jug A with the nuclei in jug B and the attraction of the nuclei in jug A with the electrons in jug B, resulting in no net force. Electromagnetic forces are tremendously stronger than gravity but cancel out so that for large bodies gravity dominates.
Electrical and magnetic phenomena have been observed since ancient times, but it was only in the 19th century that it was discovered that electricity and magnetism are two aspects of the same fundamental interaction. By 1864, Maxwell's equations had rigorously quantified this unified interaction. Maxwell's theory, restated using vector calculus, is the classical theory of electromagnetism, suitable for most technological purposes.
The constant speed of light in a vacuum (customarily described with the letter "c") can be derived from Maxwell's equations, which are consistent with the theory of special relativity. Einstein's 1905 theory of special relativity, however, which flows from the observation that the speed of light is constant no matter how fast the observer is moving, showed that the theoretical result implied by Maxwell's equations has profound implications far beyond electromagnetism on the very nature of time and space.
In another work that departed from classical electro-magnetism, Einstein also explained the photoelectric effect by hypothesizing that light was transmitted in quanta, which we now call photons. Starting around 1927, Paul Dirac combined quantum mechanics with the relativistic theory of electromagnetism. Further work in the 1940s, by Richard Feynman, Freeman Dyson, Julian Schwinger, and Sin-Itiro Tomonaga, completed this theory, which is now called quantum electrodynamics, the revised theory of electromagnetism. Quantum electrodynamics and quantum mechanics provide a theoretical basis for electromagnetic behavior such as quantum tunneling, in which a certain percentage of electrically charged particles move in ways that would be impossible under the classical electromagnetic theory, that is necessary for everyday electronic devices such as transistors to function.
Weak interaction.
The "weak interaction" or "weak nuclear force" is responsible for some nuclear phenomena such as beta decay. Electromagnetism and the weak force are now understood to be two aspects of a unified electroweak interaction — this discovery was the first step toward the unified theory known as the Standard Model. In the theory of the electroweak interaction, the carriers of the weak force are the massive gauge bosons called the W and Z bosons. The weak interaction is the only known interaction which does not conserve parity; it is left-right asymmetric. The weak interaction even violates CP symmetry but does conserve CPT.
Strong interaction.
The "strong interaction", or "strong nuclear force", is the most complicated interaction, mainly because of the way it varies with distance. At distances greater than 10 femtometers, the strong force is practically unobservable. Moreover, it holds only inside the atomic nucleus.
After the nucleus was discovered in 1908, it was clear that a new force was needed to overcome the electrostatic repulsion, a manifestation of electromagnetism, of the positively charged protons. Otherwise, the nucleus could not exist. Moreover, the force had to be strong enough to squeeze the protons into a volume that is 10 of that of the entire atom. From the short range of this force, Hideki Yukawa predicted that it was associated with a massive particle, whose mass is approximately 100 MeV.
The 1947 discovery of the pion ushered in the modern era of particle physics. Hundreds of hadrons were discovered from the 1940s to 1960s, and an extremely complicated theory of hadrons as strongly interacting particles was developed. Most notably:
While each of these approaches offered deep insights, no approach led directly to a fundamental theory.
Murray Gell-Mann along with George Zweig first proposed fractionally charged quarks in 1961. Throughout the 1960s, different authors considered theories similar to the modern fundamental theory of quantum chromodynamics (QCD) as simple models for the interactions of quarks. The first to hypothesize the gluons of QCD were Moo-Young Han and Yoichiro Nambu, who introduced the quark color charge and hypothesized that it might be associated with a force-carrying field. At that time, however, it was difficult to see how such a model could permanently confine quarks. Han and Nambu also assigned each quark color an integer electrical charge, so that the quarks were fractionally charged only on average, and they did not expect the quarks in their model to be permanently confined.
In 1971, Murray Gell-Mann and Harald Fritzsch proposed that the Han/Nambu color gauge field was the correct theory of the short-distance interactions of fractionally charged quarks. A little later, David Gross, Frank Wilczek, and David Politzer discovered that this theory had the property of asymptotic freedom, allowing them to make contact with experimental evidence. They concluded that QCD was the complete theory of the strong interactions, correct at all distance scales. The discovery of asymptotic freedom led most physicists to accept QCD since it became clear that even the long-distance properties of the strong interactions could be consistent with experiment if the quarks are permanently confined.
Assuming that quarks are confined, Mikhail Shifman, Arkady Vainshtein, and Valentine Zakharov were able to compute the properties of many low-lying hadrons directly from QCD, with only a few extra parameters to describe the vacuum. In 1980, Kenneth G. Wilson published computer calculations based on the first principles of QCD, establishing, to a level of confidence tantamount to certainty, that QCD will confine quarks. Since then, QCD has been the established theory of the strong interactions.
QCD is a theory of fractionally charged quarks interacting by means of 8 photon-like particles called gluons. The gluons interact with each other, not just with the quarks, and at long distances the lines of force collimate into strings. In this way, the mathematical theory of QCD not only explains how quarks interact over short distances but also the string-like behavior, discovered by Chew and Frautschi, which they manifest over longer distances.
Beyond the Standard Model.
Numerous theoretical efforts have been made to systematize the existing four fundamental interactions on the model of electroweak unification.
Grand Unified Theories (GUTs) are proposals to show that all of the fundamental interactions, other than gravity, arise from a single interaction with symmetries that break down at low energy levels. GUTs predict relationships among constants of nature that are unrelated in the SM. GUTs also predict gauge coupling unification for the relative strengths of the electromagnetic, weak, and strong forces, a prediction verified at the Large Electron–Positron Collider in 1991 for supersymmetric theories.
Theories of everything, which integrate GUTs with a quantum gravity theory face a greater barrier, because no quantum gravity theories, which include string theory, loop quantum gravity, and twistor theory, have secured wide acceptance. Some theories look for a graviton to complete the Standard Model list of force-carrying particles, while others, like loop quantum gravity, emphasize the possibility that time-space itself may have a quantum aspect to it.
Some theories beyond the Standard Model include a hypothetical fifth force, and the search for such a force is an ongoing line of experimental research in physics. In supersymmetric theories, there are particles that acquire their masses only through supersymmetry breaking effects and these particles, known as moduli can mediate new forces. Another reason to look for new forces is the recent discovery that the expansion of the universe is accelerating (also known as dark energy), giving rise to a need to explain a nonzero cosmological constant, and possibly to other modifications of general relativity. Fifth forces have also been suggested to explain phenomena such as CP violations, dark matter, and dark flow.

</doc>
<doc id="10891" url="https://en.wikipedia.org/wiki?curid=10891" title="Floppy disk">
Floppy disk

A floppy disk, also called a diskette or just disk, is a type of disk storage composed of a disk of thin and flexible magnetic storage medium, sealed in a rectangular plastic carrier lined with fabric that removes dust particles. Floppy disks are read and written by a floppy disk drive (FDD).
Floppy disks, initially as media and later in 5¼-inch (133 mm) and 3½-inch (90 mm) sizes, were a ubiquitous form of data storage and exchange from the mid-1970s well into the 2000s.
By 2010, computer motherboards are rarely manufactured with floppy drive support; 3½-inch floppy disks can be used with an external USB floppy disk drive, but USB drives for 5¼-inch, 8-inch, and non-standard diskettes are rare to non-existent. These formats are usually handled by older equipment.
While floppy disk drives still have some limited uses, especially with legacy industrial computer equipment, they have been superseded by data storage methods with much greater capacity, such as USB flash sticks, flash storage cards, portable external hard disk drives, optical discs, and computer networks.
History.
The first floppy disks, developed in the late 1960s, are in diameter; they became commercially available in 1971 as a component of IBM products and then were sold separately beginning in 1972 by Memorex and others. These disks and associated drives were produced and improved upon by IBM and other companies such as Memorex, Shugart Associates, and Burroughs Corporation. The term "floppy disk" appeared in print as early as 1970, and although in 1973 IBM announced its first media as "Type 1 Diskette" the industry continued to use the terms "floppy disk" or "floppy".
In 1976, Shugart Associates introduced the first 5¼-inch FDD. By 1978 there were more than 10 manufacturers producing such FDDs. There were competing floppy disk formats, with hard- and soft-sector versions and encoding schemes such as FM, MFM and GCR. The 5¼-inch format displaced the 8-inch one for most applications, and the hard-sectored disk format disappeared. In 1984, IBM introduced the 1.2 MB dual-sided floppy disk along with its AT model. IBM started using the 720 KB double-density 3½-inch microfloppy disk on its Convertible laptop computer in 1986 and the 1.44 MB high-density version with the PS/2 line in 1987. These disk drives could be added to older PC models. In 1988 IBM introduced a drive for 2.88 MB "DSED" diskettes in its top-of-the-line PS/2 models, but this was a commercial failure.
Throughout the early 1980s, limitations of the 5¼-inch format became clear. Originally designed to be more practical than the 8-inch format, it was itself too large; as the quality of recording media grew, data could be stored in a smaller area. A number of solutions were developed, with drives at 2, 2½, 3 and 3½ inches (and Sony's 90.0 mm × 94.0 mm disk) offered by various companies. They all shared a number of advantages over the old format, including a rigid case with a sliding metal cover over the head slot, which helped protect the delicate magnetic medium from dust and damage, and a sliding write protection tab, which was far more practical than the adhesive tabs used with earlier disks. The large market share of the 5¼-inch format made it difficult for these new formats to gain significant market share. A variant on the Sony design, introduced in 1982 by a large number of manufacturers, was then rapidly adopted; by 1988 the 3½-inch was outselling the 5¼-inch.
By the end of the 1980s, 5¼-inch disks had been superseded by 3½-inch disks. By the mid-1990s, 5¼-inch drives had virtually disappeared, as the 3½-inch disk became the predominant floppy disk. The advantages of the 3½-inch disk were its smaller size and its rigid case, which provided better protection from dirt and other environmental risks, while the 5¼-inch disk was available cheaper per piece throughout its history, usually with a price in the range of a third to half that of a 3½-inch disk.
Ubiquity.
Floppy disks became ubiquitous during the 1980s and 1990s in their use with personal computers to distribute software, transfer data, and create backups. Before hard disks became affordable to the general population, floppy disks were often used to store a computer's operating system (OS). Most home computers from that period have a primary OS and BASIC stored as ROM, with the option of loading a more advanced operating system from a floppy disk. By the early 1990s, the increasing software size meant large packages like Windows or Adobe Photoshop required a dozen disks or more. In 1996, there were an estimated five billion standard floppy disks in use. Then, distribution of larger packages was gradually replaced by CD-ROM and online distribution (for smaller programs). An attempt to continue the floppy disk was the SuperDisk in the late 1990s, with a capacity of 120 MB and backward-compatible with standard 3½-inch floppies; a format war briefly occurred between SuperDisk and other high-density floppy-disk products, although ultimately flash storage, recordable CDs/DVDs, and online storage would render the matter irrelevant. External USB-based floppy disk drives are still available; many modern systems provide firmware support for booting from such drives.
Decline.
Mechanically incompatible higher-density floppy disks were introduced, like the Iomega Zip disk. Adoption was limited by the competition between proprietary formats and the need to buy expensive drives for computers where the disks would be used. In some cases, failure in market penetration was exacerbated by release of higher-capacity versions of the drive and media not backward-compatible with the original drives, dividing the users between new and old adopters. A chicken-or-the-egg scenario ensued, with consumers wary of making costly investments into unproven and rapidly changing technologies, resulting in none of the technologies becoming an established standard.
Apple introduced the iMac in 1998 with a CD-ROM drive but no floppy drive; this made USB-connected floppy drives popular accessories, as the iMac came without any writable removable media device.
Recordable CDs with even greater capacity, compatible with existing infrastructure of CD-ROM drives, made the new floppy technologies obsolete. The floppy disk's remaining reusability advantage was then eliminated by re-writeable CDs. Networking, advancements in flash-based devices and widespread adoption of USB provided another alternative that in turn made both floppy disks and optical storage obsolete for some purposes. The rise of file sharing and multi-megapixel digital photography encouraged the use of files larger than most 3½-inch disks could hold. Floppy disks were commonly used as sneakernet carriers for file transfer, but the broad availability of LANs and fast Internet connections provided a simpler and faster method of transferring such files. Other removable storage devices have advantages in both capacity and performance when network connections are unavailable or when networks are inadequate. 
Use in the early 21st century.
Nonetheless, as of 2002 most manufacturers still provided floppy disk drives as standard equipment to meet user demand for file-transfer and an emergency boot device as well as the general secure feeling of having the familiar device. Subsequently, enabled by the widespread support for USB flash drives and BIOS boot, manufacturers and retailers progressively reduced the availability of floppy disk drives as standard equipment. In February 2003, Dell announced that floppy drives would no longer be pre-installed on Dell Dimension home computers, although they were still available as a selectable option and purchasable as an aftermarket OEM add-on. On 29 January 2007, PC World stated that only 2% of the computers they sold contained built-in floppy disk drives; once present stocks were exhausted, no more standard floppies would be sold. In 2009, Hewlett-Packard stopped supplying standard floppy drives on business desktops.
Floppy disks are used for emergency boots in aging systems lacking support for other bootable media and for BIOS updates, since most BIOS and firmware programs can still be executed from bootable floppy disks. If BIOS updates fail or become corrupt, floppy drives can sometimes be used to perform a recovery. The music and theatre industries still use equipment requiring standard floppy disks (e.g. synthesizers, samplers, drum machines, sequencers, and lighting consoles). Industrial automation equipment such as programmable machinery and industrial robots may not have a USB interface; data and programs are then loaded from disks, damageable in industrial environments. This may not be replaced due to cost or requirement for continuous availability; existing software emulation and virtualization do not solve this problem because no operating system is present or a customized operating system is used that has no drivers for USB devices. Hardware floppy disk emulators can be made to interface floppy-disk controllers to a USB port that can be used for flash drives.
Corporate computer environments may still make use of floppy disks for older machines that do not support the current company networks and in the case of laptops where Wi-Fi is not considered secure. The floppy disk provides for a controlled means of file transfer by permitting only a few files to be transmitted. This is as USB ports on enterprise computer terminals/workstations are often disabled in order to prevent employees from using a flash memory drive to take large amounts of data for unauthorized use. In addition, the loss of a floppy disk has less consequence than the loss of any modern piece of removable flash storage.
The abundance of old floppy drives has enabled forensics investigators to repurpose older computers as "front-ends" for forensic access to old floppy disks that have only recently been unearthed from some crime scenes.
Legacy.
For more than two decades, the floppy disk was the primary external writable storage device used. Most computing environments before the 1990s were non-networked, and floppy disks were the primary means of transferring data between computers, a method known informally as sneakernet. Unlike hard disks, floppy disks are handled and seen; even a novice user can identify a floppy disk. Because of these factors, a picture of a 3½-inch floppy disk has become an interface metaphor for saving data. The floppy disk symbol is still used by software on user-interface elements related to saving files, such as the release of Microsoft Office 2013, even though the physical floppy disks are largely obsolete.
Design.
Structure.
The 5¼-inch disk has a large circular hole in the center for the drive's spindle and a small oval aperture in both sides of the plastic to allow the drive's heads to read and write data; the magnetic medium can be spun by rotating it from the middle hole. A small notch on the right of the disk identifies that it is writable, detected by a mechanical switch or phototransistor above it; if it is not present, the disk is read-only. Punch devices were sold to convert read-only disks to writable ones and enable writing on the unused side of single sided disks; such modified disks became known as flippy disks. Tape may be used over the notch to protect writable disks from unwanted writing. This arrangement was the inverse of the system used on 8-inch floppy disks where the notch had to be covered before the disk could be written to.
Another LED/photo-transistor pair located near the center of the disk detects the "index hole" once per rotation in the magnetic disk; it is used to detect the angular start of each track and whether or not the disk is rotating at the correct speed. Early 8‑inch and 5¼‑inch disks had physical holes for each sector and were termed "hard sectored" disks. Later "soft sectored" disks had only one index hole, and sector position was determined by the disk controller or low level software from patterns marking the start of a sector. Generally, the same drives were used to read and write both types of disks, with only the disks and disk controllers differing. Some operating systems utilizing soft sectors, such as Apple DOS, did not use the index hole; the drives designed for such systems often lacked the corresponding sensor; this was mainly a hardware cost-saving measure.
Inside the disk are two layers of fabric, with the medium sandwiched in the middle. The fabric is designed to reduce friction between the medium and the outer casing, and catch particles of debris abraded off the disk to keep them from accumulating on the heads. The outer casing is usually a one-part sheet, double-folded with flaps glued or spot-welded together. The 8-inch disk had read-only logic that was the reverse of the 5¼-inch disk, with the slot on the side having to be taped over to "allow" writing.
The core of the 3½-inch disk is the same as the other two disks, but the front has only a label and a small aperture for reading and writing data, protected by the "slider" — a spring-loaded metal or plastic cover, pushed to the side on entry into the drive. Rather than having a hole in the center, it has a metal hub which mates to the spindle of the drive. Typical 3½-inch disk magnetic coating materials are:
Two holes at the bottom left and right indicate whether the disk is write-protected and whether it is high-density; these holes are spaced as far apart as the holes in punched A4 paper, allowing write-protected high-density floppies to be clipped into standard ring binders. A notch at top right ensures that the disk is in the correct orientation and an arrow at top left indicating direction of insertion. The drive usually has a button that when pressed ejects the disk with varying degrees of force, the discrepancy due to the ejection force provided by the spring of the slider cover. In IBM PC compatibles, Commodores, Apple II/IIIs, and other non-Apple-Macintosh machines with standard floppy disk drives, a disk may be inserted or ejected manually at any time. The drive has a disk-change switch that detects when a disk is ejected or inserted. Failure of this mechanical switch is a common source of disk corruption if a disk is changed and the drive (and hence the operating system) fails to notice.
One of the chief usability problems of the floppy disk is its vulnerability; even inside a closed plastic housing, the disk medium is highly sensitive to dust, condensation and temperature extremes. As with all magnetic storage, it is vulnerable to magnetic fields. Blank disks have been distributed with an extensive set of warnings, cautioning the user not to expose it to dangerous conditions. Disks must not be roughly treated or removed from the drive while the magnetic media is still spinning, since doing so is likely to cause damage to the disk, drive head, or stored data. On the other hand, the 3½‑inch floppy has been lauded for its mechanical usability by HCI expert Donald Norman:
Operation.
A spindle motor in the drive rotates the magnetic medium at a certain speed, while a stepper motor-operated mechanism moves the magnetic read/write head(s) radially along the surface of the disk. Both read and write operations require the media to be rotating and the head to contact the disk media, an action originally accomplished by a "disk load" solenoid.Later drives held the heads out of contact until a front-panel lever was rotated (5¼") or disk insertion was complete (3½"). To write data, current is sent through a coil in the head as the media rotates. The head's magnetic field aligns the magnetic particles directly below the head on the media. When the current is reversed the particles align in the opposite direction encoding the data digitally. To read data, the magnetic particles in the media induce a tiny voltage in the head coil as they pass under it. This small signal is amplified and sent to the floppy disk controller, which converts the streams of pulses from the media into data, checks it for errors, and sends it to the host computer system.
A blank unformatted diskette has a coating of magnetic oxide with no magnetic order to the particles. During formatting, the particles are aligned forming a pattern of magnetized tracks, each broken up into sectors, enabling the controller to properly read and write data. The tracks are concentric rings around the center, with spaces between tracks where no data is written; gaps with padding bytes are provided between the sectors and at the end of the track to allow for slight speed variations in the disk drive, and to permit better interoperability with disk drives connected to other similar systems. Each sector of data has a header that identifies the sector location on the disk. A cyclic redundancy check (CRC) is written into the sector headers and at the end of the user data so that the disk controller can detect potential errors. Some errors are soft and can be resolved by automatically re-trying the read operation; other errors are permanent and the disk controller will signal a failure to the operating system if multiple attempts to read the data still fail.
After a disk is inserted, a catch or lever at the front of the drive is manually lowered to prevent the disk from accidentally emerging, engage the spindle clamping hub, and in two-sided drives, engage the second read/write head with the media. In some 5¼-inch drives, insertion of the disk compresses and locks an ejection spring which partially ejects the disk upon opening the catch or lever. This enables a smaller concave area for the thumb and fingers to grasp the disk during removal. Newer 5¼-inch drives and all 3½-inch drives automatically engage the spindle and heads when a disk is inserted, doing the opposite with the press of the eject button. On Apple Macintosh computers with built-in floppy drives, the ejection button is replaced by software controlling an ejection motor which only does so when the operating system no longer needs to access the drive. The user could drag the image of the floppy drive to the trash can on the desktop to eject the disk. In the case of a power failure or drive malfunction, a loaded disk can be removed manually by inserting a straightened paper clip into a small hole at the drive's front panel, just as one would do with a CD-ROM drive in a similar situation.
Sizes.
Different sizes of floppy disks are mechanically incompatible, and disks can fit only one size of drive. Drive assemblies with both 3½-inch and 5¼-inch slots were available during the transition period between the sizes, but they contained two separate drive mechanisms. In addition, there are many subtle, usually software-driven incompatibilities between the two. 5¼-inch disks formatted for use with Apple II computers would be unreadable and treated as unformatted on a Commodore. As computer platforms began to form, attempts were made at interchangeability. For example, the "Superdrive" included from the Macintosh SE to the Power Macintosh G3 could read, write and format IBM PC format 3½-inch disks, but few IBM-compatible computers had drives that did the reverse. 8-inch, 5¼-inch and 3½-inch drives were manufactured in a variety of sizes, most to fit standardized drive bays. Alongside the common disk sizes were non-classical sizes for specialized systems.
8-inch floppy disk.
The first floppy disk was 8 inches in diameter, was protected by a flexible plastic jacket and was a read-only device used by IBM as a way of loading microcode. Read/Write floppy disks and their drives became available in 1972 but it was IBM's 1973 introduction of the 3740 data entry system that began the establishment of floppy disks, called by IBM the "Diskette 1," as an industry standard for information interchange. Early microcomputers used for engineering, business, or word processing often used one or more 8-inch disk drives for removable storage; the CP/M operating system was developed for microcomputers with 8-inch drives.
The family of 8-inch disks and drives increased over time and later versions could store up to 1.2 MB; many microcomputer applications did not need that much capacity on one disk, so a smaller size disk with lower-cost media and drives was feasible. The 5¼-inch drive succeeded the 8-inch size in many applications, and developed to about the same storage capacity as the original 8-inch size, using higher-density media and recording techniques.
5¼-inch floppy disk.
The head gap of an 80‑track high-density (1.2 MB in the MFM format) 5¼‑inch drive (a.k.a. Mini diskette, Mini disk, or Minifloppy) is smaller than that of a 40‑track double-density (360 KB) drive but can format, read and write 40‑track disks well provided the controller supports double stepping or has a switch to do such a process. A blank 40‑track disk formatted and written on an 80‑track drive can be taken to its native drive without problems, and a disk formatted on a 40‑track drive can be used on an 80‑track drive. Disks written on a 40‑track drive and then updated on an 80 track drive become unreadable on any 40‑track drives due to track width incompatibility.
Single sided disks were coated on both sides, despite the availability of more expensive double sided disks. The reason usually given for the higher cost was that double sided disks were certified error-free on both sides of the media. Architectural differences among computer platforms negated this claim, however, with RadioShack TRS-80 Model I computers using one side and the Apple II machines the other. Double-sided disks could be used in drives for single-sided disks, one side at a time, by turning them over (flippy disks); more expensive dual-head drives which could read both sides without turning over were later produced, and eventually became used universally.
3½-inch floppy disk.
In the early 1980s, a number of manufacturers introduced smaller floppy drives and media in various formats. A consortium of 21 companies eventually settled on a 3½-inch floppy disk (actually 90 mm wide) a.k.a. Micro diskette, Micro disk, or Micro floppy, similar to a Sony design, but improved to support both single-sided and double-sided media, with formatted capacities generally of 360 KB and 720 KB respectively. Single-sided drives shipped in 1983, and double sided in 1984. What became the most common format, the double-sided, high-density (HD) 1.44 MB disk drive, shipped in 1986.
The first Macintosh computers used single-sided 3½-inch floppy disks, but with 400 KB formatted capacity. These were followed in 1986 by double-sided 800 KB floppies. The higher capacity was achieved at the same recording density by varying the disk rotation speed with head position so that the linear speed of the disk was closer to constant. Later Macs could also read and write 1.44 MB HD disks in PC format with fixed rotation speed.
All 3½-inch disks have a rectangular hole in one corner which, if obstructed, write-enabled the disk. A sliding detented piece can be moved to block or reveal the part of the rectangular hole that is sensed by the drive. The HD 1.44 MB disks have a second, unobstructed hole in the opposite corner which identifies them as being of that capacity.
In IBM-compatible PCs, the three densities of 3½-inch floppy disks are backwards-compatible: higher density drives can read, write and format lower density media. It is physically possible to format a disk at the wrong density, although the resulting disk will not work properly. Fresh disks manufactured as high density can theoretically be formatted at double density only if no information has been written on the disk in high density, or the disk has been thoroughly demagnetized with a bulk eraser, as the magnetic strength of a high density record is stronger and overrides lower density, remaining on the disk and causing problems.
Writing at different densities than disks were intended for, sometimes by altering or drilling holes, was possible but deprecated. The holes on the right side of a 3½‑inch disk can be altered as to make some disk drives and operating systems treat the disk as one of higher or lower density, for bidirectional compatibility or economical reasons. Some computers, such as the PS/2 and Acorn Archimedes, ignored these holes altogether.
It is possible to make a 3½-inch floppy disk drive be recognized by a system as a 5¼‑inch 360 KB or 1200 KB drive, and to read and write disks with the same number of tracks and sectors as those disks; this had some application in data exchange with obsolete CP/M systems.
Other sizes.
Other smaller floppy size were proposed, especially for portable or pocket-sized devices that needed a smaller storage device. 3-inch disks similar in construction to 3½-inch were manufactured and used for a time, particularly by Amstrad computers and word processors. A 2-inch nominal size was introduced for compact pocket computers and was used with some electronic musical instrument controllers. Neither of these sizes became popular in IBM PC compatible computers.
Sizes, performance and capacity.
Floppy disk size is often referred to in inches, even in countries using metric and though the size is defined in metric. The ANSI specification of 3½-inch disks is entitled in part "90 mm (3.5 in)" though 90 mm is closer to 3.54 inches. Formatted capacities are generally set in terms of kilobytes and megabytes.
Data is generally written to floppy disks in sectors (angular blocks) and tracks (concentric rings at a constant radius). For example, the HD format of 3½-inch floppy disks uses 512 bytes per sector, 18 sectors per track, 80 tracks per side and two sides, for a total of 1,474,560 bytes per disk. Some disk controllers can vary these parameters at the user's request, increasing storage on the disk, although they may not be able to be read on machines with other controllers. For example, Microsoft applications were often distributed on 3½-inch 1.68 MB DMF disks formatted with 21 sectors instead of 18; they could still be recognized by a standard controller. On the IBM PC, MSX and most other microcomputer platforms, disks were written using a constant angular velocity (CAV) format, with the disk spinning at a constant speed and the sectors holding the same amount of information on each track regardless of radial location.
This was not the most efficient way to use the disk surface with available drive electronics; because the sectors have constant angular size, the 512 bytes in each sector are compressed more near the disk's center. A more space-efficient technique would be to increase the number of sectors per track toward the outer edge of the disk, from 18 to 30 for instance, thereby keeping nearly constant the amount of physical disk space used for storing each sector; an example is zone bit recording. Apple implemented this in early Macintosh computers by spinning the disk more slowly when the head was at the edge, while maintaining the data rate, allowing 400 KB of storage per side and an extra 160 KB on a double-sided disk. This higher capacity came with a disadvantage: the format used a unique drive mechanism and control circuitry, meaning that Mac disks could not be read on other computers. Apple eventually reverted to constant angular velocity on HD floppy disks with their later machines, still unique to Apple as they supported the older variable-speed formats.
Disk formatting is usually done by a utility program supplied by the computer OS manufacturer; generally, it sets up a file storage directory system on the disk, and initializes its sectors and tracks. Areas of the disk unusable for storage due to flaws can be locked (marked as "bad sectors") so that the operating system does not attempt to use them. This was time consuming so many environments had quick formatting which skipped the error checking process. When floppy disks were often used, disks pre-formatted for popular computers were sold. A formatted floppy disk does not include the sector and track headings of an unformatted disk; the difference in storage between them depends on the drive's application. Floppy disk drive and media manufacturers specify the unformatted capacity (for example, 2 MB for a standard 3½-inch HD floppy). It is implied that this should not be exceeded, since doing so will most likely result in performance problems. DMF was introduced permitting 1.68 MB to fit onto an otherwise standard 3½-inch disk; utilities then appeared allowing disks to be formatted as such.
Mixtures of decimal prefixes and binary sector sizes require care to properly calculate total capacity. Whereas semiconductor memory naturally favors powers of two (size doubles each time an address pin is added to the integrated circuit), the capacity of a disk drive is the product of sector size, sectors per track, tracks per side and sides (which in hard disk drives with multiple platters can be greater than 2). Although other sector sizes have been known in the past, formatted sector sizes are now almost always set to powers of two (256 bytes, 512 bytes, etc.), and, in some cases, disk capacity is calculated as multiples of the sector size rather than only in bytes, leading to a combination of decimal multiples of sectors and binary sector sizes. For example, 1.44 MB 3½-inch HD disks have the "M" prefix peculiar to their context, coming from their capacity of 2,880 512-byte sectors (1,440 KiB), inconsistent with either a decimal megabyte nor a binary mebibyte (MiB). Hence, these disks hold 1.47 MB or 1.41 MiB. Usable data capacity is a function of the disk format used, which in turn is determined by the FDD controller and its settings. Differences between such formats can result in capacities ranging from approximately 1300 to 1760 KiB (1.80 MB) on a "standard" 3½-inch high density floppy (and up to nearly 2 MB with utilities such as 2MGUI). The highest capacity techniques require much tighter matching of drive head geometry between drives, something not always possible and unreliable. For example, the LS-240 drive supports a 32 MB capacity on standard 3½-inch HD disks, but it is, however, a write-once technique, and requires its own drive.
The raw maximum transfer rate of 3½-inch HD floppy drives and interfaces, disregarding overheads, is as much as 1,000 kilobits/s, or approximately 83% that of single-speed CD‑ROM (71% of audio CD). This represents the speed of raw data bits moving under the read head; however, the effective speed is somewhat less due to space used for headers, gaps and other format fields.

</doc>
<doc id="10893" url="https://en.wikipedia.org/wiki?curid=10893" title="Fencing">
Fencing

Fencing, also called Olympic fencing emerged as a competitive sport at the end of the 19th century, with the Italian school having modified the "classical fencing", and the French school having later refined the Italian system.
Modern fencing uses three weapons, and so is divided respectively into three competitive scenes: foil, sabre (spelt "saber" in the United States) and épée. Most (but not all) competitive fencers choose to specialise in one of these only.
Competitive fencing is one of five activities which have been featured in every one of the modern Olympic Games, the other four being athletics, cycling, swimming, and gymnastics.
Competitive fencing.
Governing body.
Fencing is governed by Fédération Internationale d'Escrime or FIE. Today, its head office is in Lausanne, Switzerland. The FIE is composed of 145 national federations, each of which is recognised by its country's Olympic Committee as the sole representative of Olympic-style fencing in that country.
Rules.
The FIE maintains the current rules used for FIE sanctioned international events, including world cups, world championships and the Olympic Games. The FIE handles proposals to change the rules the first year after an Olympic year in the annual congress. The US Fencing Association has slightly different rules, but usually adhere to FIE standards.
History.
Fencing tracks his roots in the development of swordsmanship for duels and self defence into the current sport. The ancestor of modern fencing originated in Spain, where several books on fencing were written. "Treatise on Arms" was written by Diego de Valera between 1458 and 1471 and is one of the oldest surviving manuals on western fencing shortly before dueling came under official ban by the Catholic Monarchs. In conquest, the Spanish forces carried fencing around the world, particularly southern Italy, one of the major areas of strife between both nations. Fencing was mentioned in the play "The Merry Wives of Windsor" written sometime prior to 1602.
The mechanics of modern fencing originated in the 18th century in an Italian school of fencing of the Renaissance, and, under their influence, was improved by the French school of fencing. The Spanish school of fencing stagnated and was replaced by the Italian and French schools.
Development into a sport.
The shift towards fencing as a sport rather than as military training happened from the mid-18th century, and was led by Domenico Angelo, who established a fencing academy, Angelo's School of Arms, in Carlisle House, Soho, London in 1763. There, he taught the aristocracy the fashionable art of swordsmanship. His school was run by three generations of his family and dominated the art of European fencing for almost a century.
He established the essential rules of posture and footwork that still govern modern sport fencing, although his attacking and parrying methods were still much different from current practice. Although he intended to prepare his students for real combat, he was the first fencing master to emphasize the health and sporting benefits of fencing more than its use as a killing art, particularly in his influential book "L’École des armes" ("The School of Fencing"), published in 1763.
The first regularized fencing competition was held at the inaugural Grand Military Tournament and Assault at Arms in 1880, held at the Royal Agricultural Hall, in Islington in June. The Tournament featured a series of competitions between army officers and soldiers. Each bout was fought for five hits and the foils were pointed with black to aid the judges. The Amateur Gymnastic & Fencing Association drew up an official set of fencing regulations in 1896.
Fencing was part of the Olympics Games in the summer of 1896. Sabre events have been held at every Summer Olympics; foil events have been held at every Summer Olympics except 1908; Épée events have been held at every Summer Olympics except in the summer of 1896 because of unknown reasons.
Starting with épée in 1933, side judges were replaced by the Laurent-Pagan electrical scoring apparatus, with an audible tone and a red or green light indicating when a touch landed. Foil was automated in 1956, sabre in 1988. The scoring box reduced the bias in judging, and permitted more accurate scoring of faster actions, lighter touches, and more touches to the back and flank than before.
Equipment.
Weapons.
There are three weapons in modern fencing: foil, épée, and sabre. Each weapon has its own rules and strategies.
Foil.
The foil is a light thrusting weapon with a maximum weight of 500 grams. The foil may target the torso (including the back), neck, and groin, but not the arms or legs. The foil has a small circular hand guard that serves to protect the hand from direct stabs. As the hand is not a valid target in foil, this is primarily for safety. Touches are scored only with the tip; hits with the side of the blade do not count, and do not halt the action. Touches that land outside of the target area (called an "off-target touch") stop the action, but are not scored. Only a single touch can be scored by either fencer at one time. If both fencers land valid touches at the same time, the referee uses the rules of "right of way" to determine which fencer gets the point. If both fencers begin their attack at the same time, or the referee is unable to determine who was first, neither fencer scores a point. 
Épée.
The épée is a thrusting weapon like the foil, but heavier, with a maximum total weight of 770 grams. In épée, the entire body is valid target. The hand guard on the épée is a large circle that extends towards the pommel, effectively covering the hand, which is a valid target in épée. Like foil, all hits must be with the tip and not the sides of the blade. Hits with the side of the blade do not halt the action. As the entire body is legal target, there is no concept of an off-target touch, except if the fencer accidentally strikes the floor, setting off the electric tone. Unlike foil and sabre, épée does not use "right of way", and allows simultaneous hits by both fencers. However, if the score is tied in a match at the last point and a double touch is scored, the point is null and void.
Sabre.
The sabre is a light cutting and thrusting weapon that targets the entire body above the waist, except the weapon hand. Like the foil, the maximum legal weight of a sabre is 500 grams. The hand guard on the sabre extends from pommel to the base of where the blade connects to the hilt. This guard is generally turned outwards during sport to protect the sword arm from touches. Hits with the entire blade or point are valid. As in foil, touches that land outside of the target area are not scored. However, unlike foil, these "off-target" touches do not stop the action, and the fencing continues. In the case of both fencers landing a scoring touch, the referee determines which fencer receives the point for the action, again through the use of "right of way".
Protective clothing.
Most personal protective equipment for fencing is made of tough cotton or nylon. Kevlar was added to top level uniform pieces (jacket, breeches, underarm protector, lamé, and the bib of the mask) following the Smirnov incident at the 1982 World Championships in Rome. However, Kevlar breaks down into chlorine in UV light, complicating the cleaning process.
Other ballistic fabrics, such as Dyneema, have been developed that resist puncture, and which do not degrade the way that Kevlar does. FIE rules state that tournament wear must be made of fabric that resists a force of , and that the mask bib must resist twice that amount.
The complete fencing kit includes:
Traditionally, the fencer's uniform is white, and an instructor's uniform is black. This may be due to the occasional pre-electric practice of covering the point of the weapon in dye, soot, or colored chalk in order to make it easier for the referee to determine the placing of the touches. As this is no longer a factor in the electric era, the FIE rules have been relaxed to allow colored uniforms (save black). The guidelines also limit the permitted size and positioning of sponsorship logos.
Electric equipment.
A set of electric fencing equipment is required to participate in electric fencing. Electric equipment in fencing varies depending on the weapon with which it is used in accordance. The main component of a set of electric equipment is the body cord. The body cord serves as the connection between a fencer and a reel of wire that is part of a system for electrically detecting that the weapon has touched the opponent. There are two types: one for épée, and one for foil and sabre.
Épée body cords consist of two sets of three prongs each connected by a wire. One set plugs into the fencer's weapon, with the other connecting to the reel. Foil and saber body cords have only two prongs (or a twist-lock bayonet connector) on the weapon side, with the third wire connecting instead to the fencer's lamé. The need in foil and saber to distinguish between on and off-target touches requires a wired connection to the valid target area.
A body cord consists of three wires known as the A, B, and C lines. At the reel connector (and both connectors for Épée cords) The B pin is in the middle, the A pin is 1.5 cm to one side of B, and the C pin is 2 cm to the other side of B. This asymmetrical arrangement ensures that the cord cannot be plugged in the wrong way around.
In foil, the A line is connected to the lamé and the B line runs up a wire to the tip of the weapon. The B line is normally connected to the C line through the tip. When the tip is depressed, the circuit is broken and one of three things can happen:
In Épée, the A and B lines run up separate wires to the tip (there is no lamé). When the tip is depressed, it connects the A and B lines, resulting in a valid touch. However, if the tip is touching your opponents weapon (their C line) or the grounded strip, nothing happens when it is depressed, as the current is redirected to the C line. Grounded strips are particularly important in Épée, as without one, a touch to the floor registers as a valid touch (rather than off-target as in Foil).
In Sabre, similarly to Foil, the A line is connected to the lamé, but both the B and C lines are connected to the body of the weapon. Any contact between your B/C line (doesn't matter which, as they are always connected) and your opponent's A line (their lamé) results in a valid touch. There is no need for grounded strips in Sabre, as hitting something other than your opponent's lame does nothing.
In a professional fencing competition, a complete set of electric equipment is needed.
A complete set of foil electric equipment includes:
The electric equipment of sabre is very similar to that of foil. In addition, equipment used in sabre includes:
Épée fencers lack a lamé, conductive bib, and head cord due to their target area. Also, their body cords are constructed differently as described above. However, they possess all of the other components of a foil fencer's equipment.
Foil, épée, and sabre techniques.
Techniques or movements in fencing can be divided into two categories: offensive and quickly defensive. Some techniques can fall into both categories ("e.g." the beat). Certain techniques are used offensively, with the purpose of landing a hit on your opponent while holding the right of way (foil and sabre). Others are used defensively, to protect against a hit or obtain the right of way.
The attacks and defences may be performed in countless combinations of feet and hand actions. For example, fencer A attacks the arm of fencer B, drawing a high outside parry; fencer B then follows the parry with a high line riposte. Fencer A, expecting that, then makes his own parry by pivoting his blade under fencer B's weapon (from straight out to more or less straight down), putting fencer B's tip off target and fencer A now scoring against the low line by angulating the hand upwards.
Whenever a point is scored, the fencers will go back to their starting mark. The fight will start again after the following sentences have been said by the referee: "En garde" (On guard), "Êtes-vous prêts ?" (Are you ready ?, to which the fencers have to answer yes), "Allez" (Go).
Universities and schools.
Fencing has a long history with universities and schools for at least 500 years. At least one style of fencing, Mensur in Germany, is practiced only within universities, notably at Heidelberg. University students compete internationally at the World University Games. The United States holds two national level university tournaments including the NCAA championship and the USACFC National Championships tournaments in the USA and the BUCS fencing championships in the United Kingdom.
Equipment costs and the relatively small scale of the sport limits university fencing to a small number of schools. National fencing organisations have set up programmes to encourage more students to fence. Examples include the Regional Youth Circuit program in the USA and the Leon Paul Youth Development series in the UK.
In recent years, attempts have been made to introduce fencing to a wider and younger audience, by using foam and plastic swords, which require much less protective equipment. This makes it much less expensive to provide classes, and thus easier to take fencing to a wider range of schools than traditionally has been the case. There is even a competition series in Scotland – the Plastic-and-Foam Fencing FunLeague – specifically for Primary and early Secondary school-age children using this equipment.
The UK hosts two national competitions in which schools compete against each other directly: the Public Schools Fencing Championship, a competition only open to Independent Schools, and the Scottish Secondary Schools Championships, open to all secondary schools in Scotland. It contains both teams and individual events and is highly anticipated. Schools organise matches directly against one another and school age pupils can compete individually in the British Youth Championships.
Many universities in Ontario, Canada have fencing teams that participate in an annual inter-university competition called the OUA Finals.
Other variants.
Other variants include chair fencing, "one-hit épée" (one of the five events which constitute modern pentathlon) and the various types of non-Olympic competitive fencing. Chair fencing is similar to wheelchair fencing but for the able bodied. The opponents set up opposing chairs and fence while seated; all the usual rules of fencing are applied. An example of the latter is the American Fencing League (distinct from the United States Fencing Association): the format of competitions is different and the right of way rules are interpreted in a different way. In a number of countries, school and university matches deviate slightly from the FIE format.

</doc>
<doc id="10894" url="https://en.wikipedia.org/wiki?curid=10894" title="The Free Software Definition">
The Free Software Definition

The Free Software Definition written by Richard Stallman and published by Free Software Foundation (FSF), defines free software as being software that ensures that the end users have freedom in using, studying, sharing and modifying that software. The term "free" is used in the sense of "free speech," not of "free of charge." The earliest known publication of the definition was in the February 1986 edition of the now-discontinued GNU's Bulletin publication of FSF. The canonical source for the document is in the philosophy section of the GNU Project website. As of April 2008, it is published there in 39 languages. FSF publishes a list of licences which meet this definition.
The definition and the Four Freedoms.
The definition published by FSF in February 1986 had two points:
In 1996, when the gnu.org website was launched, "free software" was defined referring to "three levels of freedom" by adding an explicit mention of the freedom to study the software (which could be read in the two-point definition as being part of the freedom to change the program). Stallman later avoided the word "levels", saying that you need all of the freedoms, so it's misleading to think in terms of levels.
Finally, another freedom was added, to explicitly say that users should be able to run the program. The existing freedoms were already numbered one to three, but this freedom should come before the others, so it was added as "freedom zero".
The modern definition defines free software by whether or not the recipient has the following four freedoms:
Freedoms 1 and 3 require source code to be available because studying and modifying software without its source code is highly impractical.
Later definitions.
In July 1997, Bruce Perens published the Debian Free Software Guidelines. This was also used by Open Source Initiative (OSI) under the name "The Open Source Definition", the only change being that use of the term "free software" was replaced by OSI's alternative term for free software, "open-source software".
Comparison with the Open Source Definition.
Despite the philosophical differences between the free software movement and the open source movement, the official definitions of free software by the Free Software Foundation and of open source software by the Open Source Initiative basically refer to the same software licences, with a few minor exceptions. While stressing the philosophical differences, the Free Software Foundation comments:

</doc>
<doc id="10896" url="https://en.wikipedia.org/wiki?curid=10896" title="Felix Bloch">
Felix Bloch

Felix Bloch (23 October 1905 – 10 September 1983) was a Swiss physicist, working mainly in the U.S. He and Edward Mills Purcell were awarded the 1952 Nobel Prize for "their development of new ways and methods for nuclear magnetic precision measurements." In 1954–1955, he served for one year as the first Director-General of CERN.
Life and work.
Bloch was born in Zürich, Switzerland to Jewish parents Gustav and Agnes Bloch.
He was educated at the Cantonal Gymnasium in Zurich and at the Eidgenössische Technische Hochschule (ETHZ), also in Zürich. Initially studying engineering he soon changed to physics. During this time he attended lectures and seminars given by Peter Debye and Hermann Weyl at ETH Zürich and Erwin Schrödinger at the neighboring University of Zürich. A fellow student in these seminars was John von Neumann. Graduating in 1927 he continued his physics studies at the University of Leipzig with Werner Heisenberg, gaining his doctorate in 1928. His doctoral thesis established the quantum theory of solids, using Bloch waves to describe the electrons.
In 1940 he married Lore Misch.
He remained in European academia, studying with Wolfgang Pauli in Zürich, Niels Bohr in Copenhagen and Enrico Fermi in Rome before he went back to Leipzig assuming a position as privatdozent (lecturer). In 1933, immediately after Hitler came to power, he left Germany because he was Jewish. He emigrated to work at Stanford University in 1934. In the fall of 1938, Bloch began working with the University of California at Berkeley 37" cyclotron to determine the magnetic moment of the neutron. Bloch went on to become the first professor for theoretical physics at Stanford. In 1939, he became a naturalized citizen of the United States. During WW II he worked on nuclear power at Los Alamos National Laboratory, before resigning to join the radar project at Harvard University.
After the war he concentrated on investigations into nuclear induction and nuclear magnetic resonance, which are the underlying principles of MRI. In 1946 he proposed the Bloch equations which determine the time evolution of nuclear magnetization. When CERN was being set up in the early 1950s, its founders were searching for someone of the stature and international prestige to head the fledgling international laboratory, and in 1954 Professor Bloch became CERN's first Director-General, at the time when construction was getting under way on the present Meyrin site and plans for the first machines were being drawn up. After leaving CERN, he returned to Stanford University, where he in 1961 was made Max Stein Professor of Physics.
At Stanford, he was the advisor of Carson D. Jeffries, who became a professor of Physics at the University of California, Berkeley.
He died in Zurich.

</doc>
<doc id="10897" url="https://en.wikipedia.org/wiki?curid=10897" title="Fugue">
Fugue

In music, a fugue ( ) is a contrapuntal compositional technique in two or more voices, built on a subject (theme) that is introduced at the beginning in imitation (repetition at different pitches) and recurs frequently in the course of the composition.
The English term "fugue" originated in the 16th century and is derived from the French word "fugue" or the Italian "fuga". This in turn comes from Latin, also "fuga", which is itself related to both "fugere" ("to flee") and "fugare" ("to chase"). The adjectival form is "fugal". Variants include "fughetta" (literally, "a small fugue") and "fugato" (a passage in fugal style within another work that is not a fugue).
A fugue usually has three sections: an exposition, a development, and a final entry that contains the return of the subject in the fugue's tonic key. Some fugues have a recapitulation. In the Middle Ages, the term was widely used to denote any works in canonic style; by the Renaissance, it had come to denote specifically imitative works. Since the 17th century, the term "fugue" has described what is commonly regarded as the most fully developed procedure of imitative counterpoint.
Most fugues open with a short main theme, the subject, which then sounds successively in each voice (after the first voice is finished stating the subject, a second voice repeats the subject at a different pitch, and other voices repeat in the same way); when each voice has entered, the "exposition" is complete. This is often followed by a connecting passage, or "episode", developed from previously heard material; further "entries" of the subject then are heard in related keys. Episodes (if applicable) and entries are usually alternated until the "final entry" of the subject, by which point the music has returned to the opening key, or tonic, which is often followed by closing material, the coda. In this sense, a fugue is a style of composition, rather than a fixed structure.
The form evolved during the 18th century from several earlier types of contrapuntal compositions, such as imitative ricercars, capriccios, canzonas, and fantasias. The famous fugue composer Johann Sebastian Bach (1685–1750) shaped his own works after those of Johann Jakob Froberger (1616–1667), Johann Pachelbel (1653–1706), Girolamo Frescobaldi (1583–1643), Dieterich Buxtehude (c. 1637–1707) and others. With the decline of sophisticated styles at the end of the baroque period, the fugue's central role waned, eventually giving way as sonata form and the symphony orchestra rose to a dominant position. Nevertheless, composers continued to write and study fugues for various purposes; they appear in the works of Wolfgang Amadeus Mozart (1756–1791) and Ludwig van Beethoven (1770–1827), as well as modern composers such as Dmitri Shostakovich (1906–1975).
Musical outline.
A fugue begins with the "exposition" and is written according to certain predefined rules; in later portions the composer has more freedom, though a logical key structure is usually followed. Further entries of the subject will occur throughout the fugue, repeating the accompanying material at the same time. The various entries may or may not be separated by "episodes".
What follows is a chart displaying a fairly typical fugal outline, and an explanation of the processes involved in creating this structure.
The exposition.
A fugue begins with the exposition of its subject in one of the voices alone in the tonic key. After the statement of the subject, a second voice enters and states the subject with the subject transposed to another (often closely related) key, which is known as the "answer". To make the music run smoothly, it may also have to be altered slightly. When the answer is an exact copy of the subject to the dominant, it is classified as a "real answer"; if it has to be altered in any way it is a "tonal answer".
A tonal answer is usually called for when the subject begins with a prominent dominant note, or where there is a prominent dominant note very close to the beginning of the subject. To prevent an undermining of the music's sense of key, this note is transposed up a fourth to the tonic rather than up a fifth to the supertonic. Answers in the subdominant are also employed for the same reason.
While the answer is being stated, the voice in which the subject was previously heard continues with new material. If this new material is reused in later statements of the subject, it is called a "countersubject"; if this accompanying material is only heard once, it is simply referred to as "free counterpoint". The countersubject is written in invertible counterpoint at the octave or fifteenth. The distinction is made between the use of free counterpoint and regular countersubjects accompanying the fugue subject/answer, because in order for a countersubject to be heard accompanying the subject in more than one instance, it must be capable of sounding correctly above or below the subject, and must be conceived, therefore, in invertible or double counterpoint. In tonal music invertible contrapuntal lines must be written according to certain rules because several intervallic combinations, while acceptable in one particular orientation, are no longer permissible when inverted. For example, when the note "G" sounds in one voice above the note "C" in lower voice, the interval of a fifth is formed, which is considered consonant and entirely acceptable. When this interval is inverted ("C" in the upper voice above "G" in the lower), it forms a fourth, considered a dissonance in tonal contrapuntal practice, and requires special treatment, or preparation and resolution, if it is to be used. The countersubject, if sounding at the same time as the answer, is transposed to the pitch of the answer. Each voice then responds with its own subject or answer, and further countersubjects or free counterpoint may be heard.
When a tonal answer is used, it is customary for the exposition to alternate subjects (S) with answers (A), however, in some fugues this order is occasionally varied: e.g., see the SAAS arrangement of "Fugue No. 1 in C Major, BWV 846", from the "Well-Tempered Clavier, Book 1" by J. S. Bach. A brief codetta is often heard connecting the various statements of the subject and answer. This allows the music to run smoothly. The codetta, just as the other parts of the exposition, can be used throughout the rest of the fugue.
The first answer must occur as soon after the initial statement of the subject as possible; therefore the first codetta is often extremely short, or not needed. In the above example this is the case: the subject finishes on the quarter note (or crotchet) B-flat of the third beat of the second bar which harmonizes the opening G of the answer. The later codettas may be considerably longer, and often serve to (a) develop the material heard so far in the subject/answer and countersubject and possibly introduce ideas heard in the second countersubject or free counterpoint that follows (b) delay, and therefore heighten the impact of the reentry of the subject in another voice as well as modulating back to the tonic.
The exposition usually concludes when all voices have given a statement of the subject or answer. In some fugues, the exposition will end with a redundant entry, or an extra presentation of the theme. Furthermore, in some fugues the entry of one of the voices may be reserved until later, for example in the pedals of an organ fugue (see J. S. Bach's Fugue in C major for Organ, BWV 547).
The episode.
Further entries of the subject follow this initial exposition, either immediately (as for example in "Fugue No. 1 in C major, BWV 846" of the "Well-Tempered Clavier"), or separated by episodes. Episodic material is always modulatory and is usually based upon some element heard in the exposition. Each episode has primarily the function of transitioning for the next entry of the subject in a new key, and may also provide release from the strictness of form employed in the exposition, and middle-entries. André Gedalge states that the episode of the fugue is generally based on a series of imitations of the subject that have been fragmented.
The development.
Further entries of the subject, or middle entries, occur throughout the fugue. They must state the subject or answer at least once in its entirety, and may also be heard in combination with the countersubject(s) from the exposition, new countersubjects, free counterpoint, or any of these in combination. It is uncommon for the subject to enter alone in a single voice in the middle-entries as in the exposition; rather, it is usually heard with at least one of the countersubjects and/or other free contrapuntal accompaniments. Middle-entries tend to occur at pitches other than the initial. As shown in the typical structure above, these are often closely related keys such as the relative dominant and subdominant, although the key structure of fugues varies greatly. In the fugues of J. S. Bach, the first middle-entry occurs most often in the relative major or minor of the work's overall key, and is followed by an entry in the dominant of the relative major or minor when the fugue's subject requires a tonal answer. In the fugues of earlier composers (notably Buxtehude and Pachelbel), middle entries in keys other than the tonic and dominant tend to be the exception, and non-modulation the norm. One of the famous examples of such non-modulating fugue occurs in Buxtehude's Praeludium (Fugue and Chaconne) in C, BuxWV 137.
When there is no entrance of the subject and answer material, the composer can develop the subject by altering the subject . This is called an episode, often by "inversion", although the term is sometimes used synonymously with middle-entry and may also describe the exposition of completely new subjects, as in a double fugue for example (see below). In any of the entries within a fugue the subject may be altered, by inversion, retrograde (a less common form where the entire subject is heard back-to-front) and diminution (the reduction of the subject's rhythmic values by a certain factor), augmentation (the increase of the subject's rhythmic values by a certain factor) or any combination of them.
Example and analysis.
The excerpt below, bars 7–12 of J. S. Bach's Fugue no. 2 in C minor, BWV 847, from the "Well-Tempered Clavier", Book 1 illustrates the application of most of the characteristics described above. The fugue is for keyboard and in three voices, with regular countersubjects. This excerpt opens at last entry of the exposition: the subject is sounding in the bass, the first countersubject in the treble, while the middle-voice is stating a second version of the second countersubject, which concludes with the characteristic rhythm of the subject, and is always used together with the first version of the second countersubject. Following this an episode modulates from the tonic to the relative major by means of sequence, in the form of an accompanied canon at the fourth. Arrival in E-flat major is marked by a quasi perfect cadence across the barline, from the last quarter note beat of the first bar to the first beat of the second bar in the second system, and the first middle entry. Here Bach has altered countersubject 2 to accommodate the change of mode.
False entries.
At any point in the fugue there may be false entries of the subject, which include the start of the subject but are not completed. False entries are often abbreviated to the head of the subject, and anticipate the "true" entry of the subject, heightening the impact of the subject proper.
Counter-exposition.
The counter-exposition is a second exposition. However, there are only two entries, and the entries occur in reverse order. The counter-exposition in a fugue is separated from the exposition by an episode, and is in the same key as the original exposition.
Stretto.
Sometimes counter-expositions or the middle entries take place in "stretto," whereby one voice responds with the subject/answer before the first voice has completed its entry of the subject/answer, usually increasing the intensity of the music. Only one entry of the subject must be heard in its completion in a stretto. However, a stretto in which the subject/answer is heard in completion in all voices is known as "stretto maestrale" or "grand stretto". Strettos may also occur by inversion, augmentation and diminution. A fugue in which the opening exposition takes place in stretto form is known as a "close fugue" or "stretto fugue" (see for example, the "Gratias agimus tibi" and "" choruses from Bach's Mass in B Minor).
Final entries and coda.
The closing section of a fugue often includes one or two counter-expositions, and possibly a stretto, in the tonic; sometimes over a tonic or dominant pedal note. Any material that follows the final entry of the subject is considered to be the final coda and is normally cadential.
Types of fugue.
Simple fugue.
A simple fugue has only one subject, and does not utilize invertible counterpoint.
Double (triple, quadruple) fugue.
A double fugue has two subjects that are often developed simultaneously; similarly, it follows that a triple fugue has three subjects. There are two kinds of double fugue: (a) a fugue in which the second subject is presented simultaneously with the subject in the exposition (e.g. as in Kyrie Eleison of Mozart's Requiem in D minor), and (b) a fugue in which the second subject has its own exposition at some later point, and the two subjects are not combined until later (see for example, fugue no. 14 in f-sharp minor from Bach's "Well-Tempered Clavier" Book 2, or more famously, Bach's "St. Anne" Fugue in E-flat major, BWV 552, a triple fugue for organ.)
Counter-fugue.
A counter-fugue is a fugue in which the first answer is presented as the subject in inversion (upside down), and the inverted subject continues to feature prominently throughout the fugue. Examples include "Contrapunctus V" through "Contrapunctus VII", from Bach's "The Art of Fugue".
Permutation fugue.
Permutation fugue describes a type of composition (or technique of composition) in which elements of fugue and strict canon are combined. Each voice enters in succession with the subject, each entry alternating between tonic and dominant, and each voice, having stated the initial subject, continues by stating two or more themes (or countersubjects), which must be conceived in correct invertible counterpoint. (In other words, the subject and countersubjects must be capable of being played both above and below all the other themes without creating any unacceptable dissonances.) Each voice takes this pattern and states all the subjects/themes in the same order (and repeats the material when all the themes have been stated, sometimes after a rest). There is usually very little non-structural/thematic material. During the course of a permutation fugue, it is quite uncommon, actually, for every single possible voice-combination (or "permutation") of the themes to be heard. This limitation exists in consequence of sheer proportionality: the more voices in a fugue, the greater the amount of possible permutations. In consequence, composers exercise editorial judgment as to the most musical of permutations and processes leading thereto. One example of permutation fugue can be seen in the opening chorus of Bach's cantata, "Himmelskönig, sei willkommen", BWV182.
Permutation fugues differ from conventional fugue in that there are no connecting episodes, nor statement of the themes in related keys. So for example, the fugue of Bach's Passacaglia and Fugue in C minor, BWV 582 is not purely a permutation fugue, as it does have episodes between permutation expositions.
Invertible counterpoint is essential to permutation fugues but is not found in simple fugues.
Fughetta.
A fughetta is a short fugue that has the same characteristics as a fugue. Often the contrapuntal writing is not strict, and the setting less formal. See for example, variation 24 of Beethoven's "Diabelli Variations" Op. 120.
History.
Middle Ages and Renaissance Period.
The term "fuga" was used as far back as the Middle Ages, but was initially used to refer to any kind of imitative counterpoint, including canons, which are now thought of as distinct from fugues. Prior to the 16th century, fugue was originally a genre. It was not until the 16th century that fugal technique as it is understood today began to be seen in pieces, both instrumental and vocal. Fugal writing is found in works such as "fantasias", "ricercares" and "canzonas".
"Fugue" as a theoretical term first occurred in 1330 when Jacobus of Liege wrote about the fuga in his "Speculum musicae". The fugue arose from the technique of "imitation", where the same musical material was repeated starting on a different note. Gioseffo Zarlino, a composer, author, and theorist in the Renaissance, was one of the first to distinguish between the two types of imitative counterpoint: fugues and canons (which he called imitations). Originally this was to aid improvisation, but by the 1550s, it was considered a technique of composition. The Renaissance composer Giovanni Pierluigi da Palestrina (1525?–1594) wrote masses using modal counterpoint and imitation, and fugal writing became the basis for writing motets as well. Palestrina's imitative motets differed from fugues in that each phrase of the text had a different subject which was introduced and worked out separately, whereas a fugue continued working with the same subject or subjects throughout the entire length of the piece.
Baroque era.
It was in the Baroque period that the writing of fugues became central to composition, in part as a demonstration of compositional expertise. Fugues were incorporated into a variety of musical forms. Jan Pieterszoon Sweelinck, Girolamo Frescobaldi, Johann Jakob Froberger and Dieterich Buxtehude all wrote fugues, and George Frideric Handel included them in many of his oratorios. Keyboard suites from this time often conclude with a fugal gigue. Domenico Scarlatti has only a few fugues among his corpus of over 500 harpsichord sonatas. The French overture featured a quick fugal section after a slow introduction. The second movement of a sonata da chiesa, as written by Arcangelo Corelli and others, was usually fugal.
The Baroque period also saw a rise in the importance of music theory. Some fugues during the Baroque period were pieces designed to teach contrapuntal technique to students. The most influential text was published by Johann Joseph Fux (1660–1741), his "Gradus Ad Parnassum" ("Steps to Parnassus"), which appeared in 1725. This work laid out the terms of "species" of counterpoint, and offered a series of exercises to learn fugue writing. Fux's work was largely based on the practice of Palestrina's modal fugues. Mozart studied from this book, and it remained influential into the nineteenth century. Haydn, for example, taught counterpoint from his own summary of Fux, and thought of it as the basis for formal structure.
Bach's most famous fugues are those for the harpsichord in "The Well-Tempered Clavier", which many composers and theorists look at as the greatest model of fugue. "The Well-Tempered Clavier" comprises two volumes written in different times of Bach's life, each comprising 24 prelude and fugue pairs, one for each major and minor key. Bach is also known for his organ fugues, which are usually preceded by a prelude or toccata. The Art of Fugue, BWV 1080, is a collection of fugues (and four canons) on a single theme that is gradually transformed as the cycle progresses. Bach also wrote smaller single fugues, and put fugal sections or movements into many of his more general works.
J. S. Bach's influence extended forward through his son C.P.E. Bach and through the theorist Friedrich Wilhelm Marpurg (1718–1795) whose "Abhandlung von der Fuge" ("Treatise on the fugue", 1753) was largely based on J. S. Bach's work.
Classical era.
During the Classical era, the fugue was no longer a central or even fully natural mode of musical composition. Nevertheless, both Haydn and Mozart had periods of their careers in which they in some sense "rediscovered" fugal writing and used it frequently in their work.
Haydn was the leader of fugal composition and technique in the Classical era. Haydn's most famous fugues can be found in his Sun quartets (op. 20, 1772), of which three have fugal finales. This was a practice that Haydn repeated only once later in his quartet-writing career, with the finale of his quartet op. 50 no. 4 (1787). Some of the earliest examples of Haydn's use of counterpoint, however, are in three symphonies (No. 3, No. 13, and No. 40) that date from 1762–63. The earliest fugues, in both the symphonies and in the baryton trios, exhibit the influence of Joseph Fux's treatise on counterpoint, "Gradus ad Parnassum" (1725), which Haydn studied carefully. Haydn's second fugal period occurred after he heard, and was greatly inspired by, the oratorios of Handel during his visits to London (1791–1793, 1794–1795). Haydn then studied Handel's techniques and incorporated Handelian fugal writing into the choruses of his mature oratorios "The Creation" and "The Seasons," as well as several of his later symphonies, including No. 88, No. 95, and No. 101.
Wolfgang Amadeus Mozart studied counterpoint when young with Padre Martini in Bologna. However, the major impetus to fugal writing for Mozart was the influence of Baron Gottfried van Swieten in Vienna around 1782. Van Swieten, during diplomatic service in Berlin, had taken the opportunity to collect as many manuscripts by Bach and Handel as he could, and he invited Mozart to study his collection and also encouraged him to transcribe various works for other combinations of instruments. Mozart was evidently fascinated by these works, and wrote a set of transcriptions for string trio of fugues from Bach's "Well-Tempered Clavier", introducing them with preludes of his own. Mozart then set to writing fugues on his own, mimicking the Baroque style. These included the fugues for string quartet, K. 405 (1782) and a fugue in C Minor K. 426 for two pianos (1783). Later, Mozart incorporated fugal writing into his opera "Die Zauberflöte" and the finale of his "Symphony No. 41": . The parts of the Requiem he completed also contain several fugues (most notably the Kyrie, and the three fugues in the Domine Jesu; he also left behind a sketch for an Amen fugue which, some believe, would have come at the end of the Sequentia).
Ludwig van Beethoven was familiar with fugal writing from childhood, as an important part of his training was playing from "The Well-Tempered Clavier". During his early career in Vienna, Beethoven attracted notice for his performance of these fugues. There are fugal sections in Beethoven's early piano sonatas, and fugal writing is to be found in the second and fourth movements of the "Eroica Symphony" (1805). Beethoven incorporated fugues in his sonatas, and reshaped the episode's purpose and compositional technique for later generations of composers. Nevertheless, fugues did not take on a truly central role in Beethoven's work until his "late period." The finale of Beethoven's "Hammerklavier" Sonata contains a fugue, which was practically unperformed until the late 19th century, due to its tremendous technical difficulty and length. The last movement of his Cello Sonata, Op. 102 No. 2 is a fugue, and there are fugal passages in the last movements of his piano sonatas in A major Op.101 and in A flat major Op.110. According to Rosen (1971, p. 503) "With the finale of 110, Beethoven re-conceived the significance of the most traditional elements of fugue writing." Fugal passages are also found in the "Missa Solemnis" and all movements of the Ninth Symphony, except the third one. A massive, dissonant fugue forms the finale of his String Quartet, Op. 130 (1825); the latter was later published separately as Op. 133, the "Große Fuge" ("Great Fugue"). However, it is the fugue that opens Beethoven's String Quartet in C sharp minor, Op. 131 that several commentators regard as one of the composer's greatest achievements. Joseph Kerman (1966, p. 330) calls it "this most moving of all fugues". Sullivan (1927, p. 235) hears it as "the most superhuman piece of music that Beethoven has ever written." Philip Radcliffe (1965, p. 149) says "A bare description of its formal outline can give but little idea of the extraordinary profundity of this fugue."
Romantic era.
By the beginning of the Romantic era, fugue writing had become specifically attached to the norms and styles of the Baroque. Felix Mendelssohn wrote many fugues inspired by his study of the music of Johann Sebastian Bach. The Piano Sonata in B minor (1853) by Franz Liszt contains a powerful fugue, demanding incisive virtuosity from its player: Verdi included a whimsical example at the end of his opera "Falstaff." Bruckner and Mahler also included them in their respective symphonies. An example is the finale of Mahler's Symphony No.5.
20th century.
Twentieth-century composers brought fugue back to its position of prominence, realizing its uses in full instrumental works, its importance in development and introductory sections, and the developmental capabilities of fugal composition. The second movement of Ravel's piano suite "Le Tombeau de Couperin" (1917) is a fugue that Roy Howat (200, p. 88) describes as having "a subtle glint of jazz". Bartók's "Music for Strings, Percussion and Celesta" (1936) opens with a slow fugue that Pierre Boulez (1986, p. 346-7) regards as "certainly the finest and most characteristic example of Bartók's subtle style...probably the most "timeless" of all Bartók's works - a fugue that unfolds like a fan to a point of maximum intensity and then closes, returning to the mysterious atmosphere of the opening."
Igor Stravinsky also incorporated fugues into his works, including the "Symphony of Psalms" and the "Dumbarton Oaks" concerto. Stravinsky recognized the compositional techniques of Bach, and in the second movement of his "Symphony of Psalms" (1930), he lays out a fugue that is much like that of the Baroque era. It employs a double fugue with two distinct subjects, the first beginning in C and the second in E. Techniques such as stretto, sequencing, and the use of subject incipits are frequently heard in the movement.
Olivier Messiaen, writing about his "Vingt regards sur l'enfant-Jésus" (1944) wrote of the sixth piece of that collection, "Par Lui tout a été fait" ("By Him were all things made"): 
György Ligeti wrote a five-part double fugue for his "Requiem"'s second movement, the "Kyrie", in which each part (S,M,A,T,B) is subdivided in four-voice "bundless" that make a canon. The melodic material in this fugue is totally chromatic, with melismatic (running) parts overlaid onto skipping intervals, and use of polyrhythm (multiple simultaneous subdivisions of the measure), blurring everything both harmonically and rhythmically so as to create an aural aggregate, thus highlighting the theoretical/aesthetic question of the next section as to whether fugue is a form or a texture.
Benjamin Britten used a fugue in the final part of The Young Person's Guide to the Orchestra (1946). The Henry Purcell's theme is triumphantly cited at the end making it a choral fugue.
Canadian pianist and musical thinker Glenn Gould composed "So You Want to Write a Fugue?", a full-scale fugue set to a text that cleverly explicates its own musical form.
In popular music.
The middle part of the composition "The Endless Enigma" from the album Trilogy by ELP is a two-minute piano fugue.
In Jazz.
Two notable examples of jazz fugues are "Bach goes to Town", composed by the Welsh composer Alec Templeton and recorded by Benny Goodman in 1938, and "Concorde" composed by John Lewis and recorded by the Modern Jazz Quartet in 1955.
Musical form or texture?
A widespread view of the fugue is that it is not a musical form (in the sense that, say, sonata form is) but rather a technique of composition.
The Austrian musicologist Erwin Ratz argues that the formal organization of a fugue involves not only the arrangement of its theme and episodes, but also its harmonic structure. In particular, the exposition and coda tend to emphasize the tonic key, whereas the episodes usually explore more distant tonalities. Ratz stressed, however, that this is the core, underlying form ("Urform") of the fugue, from which individual fugues may deviate. Thus it is to be noted that while certain related keys are more commonly explored in fugal development, the overall structure of a fugue does not limit its harmonic structure. For example, a fugue may not even explore the dominant, one of the most closely related keys to the tonic. Bach's Fugue in B major from book one of the "Well Tempered Clavier" explores the relative minor, the supertonic and the subdominant. This is unlike later forms such as the sonata, which clearly prescribes which keys are explored (typically the tonic and dominant in an ABA form). Then, many modern fugues dispense with traditional tonal harmonic scaffolding altogether, and either use serial (pitch-oriented) rules, or (as the "Kyrie/Christe" in György Ligeti's "Requiem", Witold Lutosławski works), use panchromatic or even denser harmonic spectra.
Perceptions and aesthetics.
Fugue is the most complex of contrapuntal forms. In Ratz's words, "fugal technique significantly burdens the shaping of musical ideas, and it was given only to the greatest geniuses, such as Bach and Beethoven, to breathe life into such an unwieldy form and make it the bearer of the highest thoughts."
In presenting Bach's fugues as among the greatest of contrapuntal works, Peter Kivy points out that "counterpoint itself, since time out of mind, has been associated in the thinking of musicians with the profound and the serious" and argues that "there seems to be some rational justification for their doing so."
This is related to the idea that restrictions create freedom for the composer, by directing their efforts. He also points out that fugue writing has its roots in improvisation, and was, during the Renaissance, practiced as an improvisatory art. Writing in 1555, Nicola Vicentino, for example, suggests that:

</doc>
<doc id="10898" url="https://en.wikipedia.org/wiki?curid=10898" title="Fugue state">
Fugue state

Dissociative fugue, formerly fugue state or psychogenic fugue, is a . It is a rare psychiatric disorder characterized by reversible amnesia for personal identity, including the memories, personality, and other identifying characteristics of individuality. The state is usually short-lived (ranging from hours to days), but can last months or longer. Dissociative fugue usually involves unplanned travel or wandering, and is sometimes accompanied by the establishment of a new identity. It is no longer its own classification or diagnosis as it was in the DSM-IV, but now a facet of Dissociative Amnesia according to the DSM-5.
After recovery from fugue, previous memories usually return intact, but there is typically amnesia for the fugue episode. Additionally, an episode of fugue is not characterized as attributable to a psychiatric disorder if it can be related to the ingestion of psychotropic substances, to physical trauma, to a general medical condition, or to other psychiatric conditions such as dissociative identity disorder, delirium, or dementia. Fugues are usually precipitated by a stressful episode, and upon recovery there may be amnesia for the original stressor (dissociative amnesia).
Signs and symptoms.
Symptoms of a dissociative fugue include mild confusion, and once the fugue ends, possible depression, grief, shame and discomfort. People have also experienced a post-fugue anger.
Diagnosis.
A doctor may suspect dissociative fugue when people seem confused about their identity or are puzzled about their past or when confrontations challenge their new identity or absence of one. The doctor carefully reviews symptoms and does a physical examination to exclude physical disorders that may contribute to or cause memory loss. A psychological examination is also done.
Sometimes dissociative fugue cannot be diagnosed until people abruptly return to their pre-fugue identity and are distressed to find themselves in unfamiliar circumstances. The diagnosis is usually made retroactively when a doctor reviews the history and collects information that documents the circumstances before people left home, the travel itself, and the establishment of an alternative life.
Definition.
The cause of the fugue state is related to dissociative amnesia, ("DSM-IV Codes 300.12") which has several other subtypes: Selective Amnesia, Generalised Amnesia, Continuous Amnesia, Systematised Amnesia, in addition to the subtype "Dissociative Fugue".
Unlike retrograde amnesia (which is popularly referred to simply as "amnesia", the state where someone forgets events before brain damage), dissociative amnesia is not due to the direct physiological effects of a substance (e.g., a drug of abuse, a medication, "DSM-IV Codes 291.1 & 292.83") or a neurological or other general medical condition (e.g., Amnestic Disorder due to a head trauma, "DSM-IV Codes 294.0"). It is a complex neuropsychological process.
As the person experiencing a Dissociative Fugue may have recently suffered the reappearance of an event or person representing an earlier life trauma, the emergence of an armoring or defensive personality seems to be for some, a logical apprehension of the situation.
Therefore, the terminology "fugue state" may carry a slight linguistic distinction from "Dissociative Fugue", the former implying a greater degree of "motion". For the purposes of this article then, a "fugue state" would occur while one is "acting out" a "Dissociative Fugue".
The DSM-IV defines as:
The "Merck Manual" defines "Dissociative Fugue" as:
In support of this definition, the "Merck Manual" further defines dissociative amnesia as:
Prognosis.
The DSM-IV-TR states that the fugue may have a duration from hours to months, and recovery is usually rapid. However, some cases may be refractory. An individual usually has only one episode.
Society and culture.
Television.
In the TV series "Scandal", the character Quinn allegedly is in a dissociative fugue state in season two following the establishment of her new identity.
In the TV series "One Tree Hill", the character Clay suffers a fugue state in season nine.
In the TV series "Breaking Bad", the character Walter White fakes a fugue state to cover up his kidnapping at the hands of his drug distributor Tuco.
In the TV series "Under the Dome", the character Sam mentions that his nephew Junior's mother experienced fugue when Junior admits that he experienced blackouts.
In the TV series "Teen Wolf", the character Lydia suffers a fugue state in season two following being bitten by a werewolf.
In the TV series "Doctor Who", the character in the 2008 Christmas special, "The Next Doctor," Jackson Lake suffers a fugue state after witnessing the death of his wife by a Cyberman attack.
In the TV series "Bates Motel", the character Norman Bates suffers fugue state episodes in which he can react violently to a stressor including attempt to kill but has no memory of it when he recovers from it.
In the TV series "The Mentalist", the character Patrick Jane suffers a fugue state after nearly drowning.
In the third season of the TV series "", Lois Lane goes into a dissociative fugue as a result of suffering a blow to the head while escaping from Lex Luthor, who had kidnapped her. Initially, in her fugue state she takes on the personality of Wanda Detroit, a fictional lounge singer from her novel.
In the TV show Drop Dead Diva, a client named Daniel Porter experienced a nine-year fugue state after a single engine plane crash and now seeks shared custody of his son, Noah.
In the television series Rizzoli & Isles, a season five episode titled "...Goodbye", has Jane, Maura, and the squad dealing with the case of young woman who has a dissociative fugue episode where she believes she has killed someone. The character named "Jessica" by Maura does not remember anything about the murder but does remember that she likes the Red Sox and who her favorite player is. The character's memory is jogged by the realisation that she did not commit the murder of her significant other.
Books.
In "Perks of Being a Wallflower," Charlie, the main character, goes into a fugue state after taking LSD.
In John O'Farrell's The Man Who Forgot His Wife (16 March 2012) (2012, Doubleday, ISBN 978-0-385-60610-3 (11 October 2012) Black Swan ISBN 978-0-552-77163-4, Vaughan the main character is in a Fugue State
In Walker Percy's novels "The Last Gentleman" and "The Second Coming", the main character Will Barrett is referred to as suffering repeated fugue states, leading eventually to his diagnosis with the fictional disorder ″Hausmann's Syndrome.″
Short stories.
In the Norwegian folktale "Gidske," collected by Asbjørnsen and Moe, the eponymous heroine goes into what appears to be a fugue state after a humiliating experience of rejection by her master, for whom she has had romantic feelings.
In the short story "The Shadow Out of Time" by H. P. Lovecraft, the character Nathaniel Wingate Peaslee awakens after years of being the victim of an "identity swap" with a member of an ancient race of gods.
Film.
Dissociative fugue affects many characters in David Lynch films with the most explicit example being the protagonist of "Lost Highway".
In the film "Altered States" (1980) William Hurt's character Dr Edward Jessup emerges from a sensory deprivation tank/drug experience bloodied and aphasic and is misdiagnosed as having entered a fugue state post seizure in the tank. Jessup actually experienced a transient de-differentiation of his genetic structure and temporarily regressed into a mute "quasi-simian" creature complete with structural changes to his vocal chords.
In the year 2000 film "Nurse Betty", Renée Zellweger's character Betty witnesses the murder of her husband and experiences a fugue state.
In the film (and book) "Perks of Being a Wallflower," Charlie, the main character, undergoes a fugue state.
In the film and book "Bourne Identity", the protagonist Jason Bourne enters a fugue state after being shot.
In the 2015 film "88 (film)", Katharine Isabelle's character, Gwen, enters a fugue state following the death of her lover.
In the 1984 film "Paris, Texas, Travis Henderson, the main character, undergoes a fugue state.
Video games.
Cloud Strife from "Final Fantasy VII" entered a trauma-induced fugue state after witnessing the death of Zack Fair (as shown in the game ""). He took on Zack's identity and forgot his own memories and identity until later in the story.
In the video game "Assassin's Creed III", the character Desmond Miles experiences a fugue state upon first entering the Animus.
In the prologue of the game "Gothic 2" the main character experiences a fugue state after the destruction of the protecting shield of the penal colony.
Music.
In 2014, Vulfpeck, a funk instrumental group, released the album "Fugue State."
In 2012, PelleK, a power metal vocalist released the album "Bag of Tricks," which contains the song: "Fugue State"

</doc>
<doc id="10902" url="https://en.wikipedia.org/wiki?curid=10902" title="Force">
Force

In physics, a force is any interaction that, when unopposed, will change the motion of an object. In other words, a force can cause an object with mass to change its velocity (which includes to begin moving from a state of rest), i.e., to accelerate. Force can also be described by intuitive concepts such as a push or a pull. A force has both magnitude and direction, making it a vector quantity. It is measured in the SI unit of newtons and represented by the symbol F.
The original form of Newton's second law states that the net force acting upon an object is equal to the rate at which its momentum changes with time. If the mass of the object is constant, this law implies that the acceleration of an object is directly proportional to the net force acting on the object, is in the direction of the net force, and is inversely proportional to the mass of the object
Related concepts to force include: thrust, which increases the velocity of an object; drag, which decreases the velocity of an object; and torque, which produces changes in rotational speed of an object. In an extended body, each part usually applies forces on the adjacent parts; the distribution of such forces through the body is the so-called mechanical stress. Pressure is a simple type of stress. Stress usually causes deformation of solid materials, or flow in fluids.
Development of the concept.
Philosophers in antiquity used the concept of force in the study of stationary and moving objects and simple machines, but thinkers such as Aristotle and Archimedes retained fundamental errors in understanding force. In part this was due to an incomplete understanding of the sometimes non-obvious force of friction, and a consequently inadequate view of the nature of natural motion. A fundamental error was the belief that a force is required to maintain motion, even at a constant velocity. Most of the previous misunderstandings about motion and force were eventually corrected by Galileo Galilei and Sir Isaac Newton. With his mathematical insight, Sir Isaac Newton formulated laws of motion that were not improved-on for nearly three hundred years. By the early 20th century, Einstein developed a theory of relativity that correctly predicted the action of forces on objects with increasing momenta near the speed of light, and also provided insight into the forces produced by gravitation and inertia.
With modern insights into quantum mechanics and technology that can accelerate particles close to the speed of light, particle physics has devised a Standard Model to describe forces between particles smaller than atoms. The Standard Model predicts that exchanged particles called gauge bosons are the fundamental means by which forces are emitted and absorbed. Only four main interactions are known: in order of decreasing strength, they are: strong, electromagnetic, weak, and gravitational. High-energy particle physics observations made during the 1970s and 1980s confirmed that the weak and electromagnetic forces are expressions of a more fundamental electroweak interaction.
Pre-Newtonian concepts.
Since antiquity the concept of force has been recognized as integral to the functioning of each of the simple machines. The mechanical advantage given by a simple machine allowed for less force to be used in exchange for that force acting over a greater distance for the same amount of work. Analysis of the characteristics of forces ultimately culminated in the work of Archimedes who was especially famous for formulating a treatment of buoyant forces inherent in fluids.
Aristotle provided a philosophical discussion of the concept of a force as an integral part of Aristotelian cosmology. In Aristotle's view, the terrestrial sphere contained four elements that come to rest at different "natural places" therein. Aristotle believed that motionless objects on Earth, those composed mostly of the elements earth and water, to be in their natural place on the ground and that they will stay that way if left alone. He distinguished between the innate tendency of objects to find their "natural place" (e.g., for heavy bodies to fall), which led to "natural motion", and unnatural or forced motion, which required continued application of a force. This theory, based on the everyday experience of how objects move, such as the constant application of a force needed to keep a cart moving, had conceptual trouble accounting for the behavior of projectiles, such as the flight of arrows. The place where the archer moves the projectile was at the start of the flight, and while the projectile sailed through the air, no discernible efficient cause acts on it. Aristotle was aware of this problem and proposed that the air displaced through the projectile's path carries the projectile to its target. This explanation demands a continuum like air for change of place in general.
Aristotelian physics began facing criticism in Medieval science, first by John Philoponus in the 6th century.
The shortcomings of Aristotelian physics would not be fully corrected until the 17th century work of Galileo Galilei, who was influenced by the late Medieval idea that objects in forced motion carried an innate force of impetus. Galileo constructed an experiment in which stones and cannonballs were both rolled down an incline to disprove the Aristotelian theory of motion early in the 17th century. He showed that the bodies were accelerated by gravity to an extent that was independent of their mass and argued that objects retain their velocity unless acted on by a force, for example friction.
Newtonian mechanics.
Sir Isaac Newton sought to describe the motion of all objects using the concepts of inertia and force, and in doing so he found that they obey certain conservation laws. In 1687, Newton went on to publish his thesis "Philosophiæ Naturalis Principia Mathematica". In this work Newton set out three laws of motion that to this day are the way forces are described in physics.
First law.
Newton's First Law of Motion states that objects continue to move in a state of constant velocity unless acted upon by an external net force or "resultant force". This law is an extension of Galileo's insight that constant velocity was associated with a lack of net force (see a more detailed description of this below). Newton proposed that every object with mass has an innate inertia that functions as the fundamental equilibrium "natural state" in place of the Aristotelian idea of the "natural state of rest". That is, the first law contradicts the intuitive Aristotelian belief that a net force is required to keep an object moving with constant velocity. By making "rest" physically indistinguishable from "non-zero constant velocity", Newton's First Law directly connects inertia with the concept of relative velocities. Specifically, in systems where objects are moving with different velocities, it is impossible to determine which object is "in motion" and which object is "at rest". In other words, to phrase matters more technically, the laws of physics are the same in every inertial frame of reference, that is, in all frames related by a Galilean transformation.
For instance, while traveling in a moving vehicle at a constant velocity, the laws of physics do not change from being at rest. A person can throw a ball straight up in the air and catch it as it falls down without worrying about applying a force in the direction the vehicle is moving. This is true even though another person who is observing the moving vehicle pass by also observes the ball follow a curving parabolic path in the same direction as the motion of the vehicle. It is the inertia of the ball associated with its constant velocity in the direction of the vehicle's motion that ensures the ball continues to move forward even as it is thrown up and falls back down. From the perspective of the person in the car, the vehicle and everything inside of it is at rest: It is the outside world that is moving with a constant speed in the opposite direction. Since there is no experiment that can distinguish whether it is the vehicle that is at rest or the outside world that is at rest, the two situations are considered to be physically indistinguishable. Inertia therefore applies equally well to constant velocity motion as it does to rest.
The concept of inertia can be further generalized to explain the tendency of objects to continue in many different forms of constant motion, even those that are not strictly constant velocity. The rotational inertia of planet Earth is what fixes the constancy of the length of a day and the length of a year. Albert Einstein extended the principle of inertia further when he explained that reference frames subject to constant acceleration, such as those free-falling toward a gravitating object, were physically equivalent to inertial reference frames. This is why, for example, astronauts experience weightlessness when in free-fall orbit around the Earth, and why Newton's Laws of Motion are more easily discernible in such environments. If an astronaut places an object with mass in mid-air next to himself, it will remain stationary with respect to the astronaut due to its inertia. This is the same thing that would occur if the astronaut and the object were in intergalactic space with no net force of gravity acting on their shared reference frame. This principle of equivalence was one of the foundational underpinnings for the development of the general theory of relativity.
Second law.
A modern statement of Newton's Second Law is a vector equation:
where formula_2 is the momentum of the system, and formula_3 is the net (vector sum) force. In equilibrium, there is zero "net" force by definition, but (balanced) forces may be present nevertheless. In contrast, the second law states an "unbalanced" force acting on an object will result in the object's momentum changing over time.
By the definition of momentum,
where "m" is the mass and formula_5 is the velocity.
Newton's second law applies only to a system of constant mass, and hence "m" may be moved outside the derivative operator. The equation then becomes
By substituting the definition of acceleration, the algebraic version of Newton's Second Law is derived:
Newton never explicitly stated the formula in the reduced form above.
Newton's Second Law asserts the direct proportionality of acceleration to force and the inverse proportionality of acceleration to mass. Accelerations can be defined through kinematic measurements. However, while kinematics are well-described through reference frame analysis in advanced physics, there are still deep questions that remain as to what is the proper definition of mass. General relativity offers an equivalence between space-time and mass, but lacking a coherent theory of quantum gravity, it is unclear as to how or whether this connection is relevant on microscales. With some justification, Newton's second law can be taken as a quantitative definition of "mass" by writing the law as an equality; the relative units of force and mass then are fixed.
The use of Newton's Second Law as a "definition" of force has been disparaged in some of the more rigorous textbooks, because it is essentially a mathematical truism. Notable physicists, philosophers and mathematicians who have sought a more explicit definition of the concept of force include Ernst Mach, Clifford Truesdell and Walter Noll.
Newton's Second Law can be used to measure the strength of forces. For instance, knowledge of the masses of planets along with the accelerations of their orbits allows scientists to calculate the gravitational forces on planets.
Third law.
Newton's Third Law is a result of applying symmetry to situations where forces can be attributed to the presence of different objects. The third law means that all forces are "interactions" between different bodies, and thus that there is no such thing as a unidirectional force or a force that acts on only one body. Whenever a first body exerts a force F on a second body, the second body exerts a force −F on the first body. F and −F are equal in magnitude and opposite in direction. This law is sometimes referred to as the "action-reaction law", with F called the "action" and −F the "reaction". The action and the reaction are simultaneous:
If object 1 and object 2 are considered to be in the same system, then the net force on the system due to the interactions between objects 1 and 2 is zero since
This means that in a closed system of particles, there are no internal forces that are unbalanced. That is, the action-reaction force shared between any two objects in a closed system will not cause the center of mass of the system to accelerate. The constituent objects only accelerate with respect to each other, the system itself remains unaccelerated. Alternatively, if an external force acts on the system, then the center of mass will experience an acceleration proportional to the magnitude of the external force divided by the mass of the system.
Combining Newton's Second and Third Laws, it is possible to show that the linear momentum of a system is conserved. Using
and integrating with respect to time, the equation:
is obtained. For a system that includes objects 1 and 2,
which is the conservation of linear momentum. Using the similar arguments, it is possible to generalize this to a system of an arbitrary number of particles. This shows that exchanging momentum between constituent objects will not affect the net momentum of a system. In general, as long as all forces are due to the interaction of objects with mass, it is possible to define a system such that net momentum is never lost nor gained.
Special theory of relativity.
In the special theory of relativity, mass and energy are equivalent (as can be seen by calculating the work required to accelerate an object). When an object's velocity increases, so does its energy and hence its mass equivalent (inertia). It thus requires more force to accelerate it the same amount than it did at a lower velocity. Newton's Second Law
remains valid because it is a mathematical definition. But in order to be conserved, relativistic momentum must be redefined as:
where
The relativistic expression relating force and acceleration for a particle with constant non-zero rest mass formula_19 moving in the formula_20 direction is:
where the Lorentz factor
In the early history of relativity, the expressions formula_25 and formula_26 were called longitudinal and transverse mass. Relativistic force does not produce a constant acceleration, but an ever decreasing acceleration as the object approaches the speed of light. Note that formula_27 is undefined for an object with a non-zero rest mass at the speed of light, and the theory yields no prediction at that speed.
If formula_16 is very small compared to formula_17, then formula_30 is very close to 1 and 
is a close approximation. Even for use in relativity, however, one can restore the form of
through the use of four-vectors. This relation is correct in relativity when formula_33 is the four-force, formula_19 is the invariant mass, and formula_35 is the four-acceleration.
Descriptions.
Since forces are perceived as pushes or pulls, this can provide an intuitive understanding for describing forces. As with other physical concepts (e.g. temperature), the intuitive understanding of forces is quantified using precise operational definitions that are consistent with direct observations and compared to a standard measurement scale. Through experimentation, it is determined that laboratory measurements of forces are fully consistent with the conceptual definition of force offered by Newtonian mechanics.
Forces act in a particular direction and have sizes dependent upon how strong the push or pull is. Because of these characteristics, forces are classified as "vector quantities". This means that forces follow a different set of mathematical rules than physical quantities that do not have direction (denoted scalar quantities). For example, when determining what happens when two forces act on the same object, it is necessary to know both the magnitude and the direction of both forces to calculate the result. If both of these pieces of information are not known for each force, the situation is ambiguous. For example, if you know that two people are pulling on the same rope with known magnitudes of force but you do not know which direction either person is pulling, it is impossible to determine what the acceleration of the rope will be. The two people could be pulling against each other as in tug of war or the two people could be pulling in the same direction. In this simple one-dimensional example, without knowing the direction of the forces it is impossible to decide whether the net force is the result of adding the two force magnitudes or subtracting one from the other. Associating forces with vectors avoids such problems.
Historically, forces were first quantitatively investigated in conditions of static equilibrium where several forces canceled each other out. Such experiments demonstrate the crucial properties that forces are additive vector quantities: they have magnitude and direction. When two forces act on a point particle, the resulting force, the "resultant" (also called the "net force"), can be determined by following the parallelogram rule of vector addition: the addition of two vectors represented by sides of a parallelogram, gives an equivalent resultant vector that is equal in magnitude and direction to the transversal of the parallelogram. The magnitude of the resultant varies from the difference of the magnitudes of the two forces to their sum, depending on the angle between their lines of action. However, if the forces are acting on an extended body, their respective lines of application must also be specified in order to account for their effects on the motion of the body.
Free-body diagrams can be used as a convenient way to keep track of forces acting on a system. Ideally, these diagrams are drawn with the angles and relative magnitudes of the force vectors preserved so that graphical vector addition can be done to determine the net force.
As well as being added, forces can also be resolved into independent components at right angles to each other. A horizontal force pointing northeast can therefore be split into two forces, one pointing north, and one pointing east. Summing these component forces using vector addition yields the original force. Resolving force vectors into components of a set of basis vectors is often a more mathematically clean way to describe forces than using magnitudes and directions. This is because, for orthogonal components, the components of the vector sum are uniquely determined by the scalar addition of the components of the individual vectors. Orthogonal components are independent of each other because forces acting at ninety degrees to each other have no effect on the magnitude or direction of the other. Choosing a set of orthogonal basis vectors is often done by considering what set of basis vectors will make the mathematics most convenient. Choosing a basis vector that is in the same direction as one of the forces is desirable, since that force would then have only one non-zero component. Orthogonal force vectors can be three-dimensional with the third component being at right-angles to the other two.
Equilibrium.
Equilibrium occurs when the resultant force acting on a point particle is zero (that is, the vector sum of all forces is zero). When dealing with an extended body, it is also necessary that the net torque in it is 0.
There are two kinds of equilibrium: static equilibrium and dynamic equilibrium.
Static.
Static equilibrium was understood well before the invention of classical mechanics. Objects that are at rest have zero net force acting on them.
The simplest case of static equilibrium occurs when two forces are equal in magnitude but opposite in direction. For example, an object on a level surface is pulled (attracted) downward toward the center of the Earth by the force of gravity. At the same time, surface forces resist the downward force with equal upward force (called the normal force). The situation is one of zero net force and no acceleration.
Pushing against an object on a frictional surface can result in a situation where the object does not move because the applied force is opposed by static friction, generated between the object and the table surface. For a situation with no movement, the static friction force "exactly" balances the applied force resulting in no acceleration. The static friction increases or decreases in response to the applied force up to an upper limit determined by the characteristics of the contact between the surface and the object.
A static equilibrium between two forces is the most usual way of measuring forces, using simple devices such as weighing scales and spring balances. For example, an object suspended on a vertical spring scale experiences the force of gravity acting on the object balanced by a force applied by the "spring reaction force", which equals the object's weight. Using such tools, some quantitative force laws were discovered: that the force of gravity is proportional to volume for objects of constant density (widely exploited for millennia to define standard weights); Archimedes' principle for buoyancy; Archimedes' analysis of the lever; Boyle's law for gas pressure; and Hooke's law for springs. These were all formulated and experimentally verified before Isaac Newton expounded his Three Laws of Motion.
Dynamic.
Dynamic equilibrium was first described by Galileo who noticed that certain assumptions of Aristotelian physics were contradicted by observations and logic. Galileo realized that simple velocity addition demands that the concept of an "absolute rest frame" did not exist. Galileo concluded that motion in a constant velocity was completely equivalent to rest. This was contrary to Aristotle's notion of a "natural state" of rest that objects with mass naturally approached. Simple experiments showed that Galileo's understanding of the equivalence of constant velocity and rest were correct. For example, if a mariner dropped a cannonball from the crow's nest of a ship moving at a constant velocity, Aristotelian physics would have the cannonball fall straight down while the ship moved beneath it. Thus, in an Aristotelian universe, the falling cannonball would land behind the foot of the mast of a moving ship. However, when this experiment is actually conducted, the cannonball always falls at the foot of the mast, as if the cannonball knows to travel with the ship despite being separated from it. Since there is no forward horizontal force being applied on the cannonball as it falls, the only conclusion left is that the cannonball continues to move with the same velocity as the boat as it falls. Thus, no force is required to keep the cannonball moving at the constant forward velocity.
Moreover, any object traveling at a constant velocity must be subject to zero net force (resultant force). This is the definition of dynamic equilibrium: when all the forces on an object balance but it still moves at a constant velocity.
A simple case of dynamic equilibrium occurs in constant velocity motion across a surface with kinetic friction. In such a situation, a force is applied in the direction of motion while the kinetic friction force exactly opposes the applied force. This results in zero net force, but since the object started with a non-zero velocity, it continues to move with a non-zero velocity. Aristotle misinterpreted this motion as being caused by the applied force. However, when kinetic friction is taken into consideration it is clear that there is no net force causing constant velocity motion.
Forces in Quantum Mechanics.
The notion "force" keeps its meaning in quantum mechanics, though one is now dealing with operators instead of classical variables and though the physics is now described by the Schrödinger equation instead of Newtonian equations. This has the consequence that the results of a measurement are now sometimes "quantized", i.e. they appear in discrete portions. This is, of course, difficult to imagine in the context of "forces". However, the potentials V(x,y,z) or fields, from which the forces generally can be derived, are treated similar to classical position variables, i.e., formula_36.
This becomes different only in the framework of quantum field theory, where these fields are also quantized.
However, already in quantum mechanics there is one "caveat", namely the particles acting onto each other do not only possess the spatial variable, but also a discrete intrinsic angular momentum-like variable called the "spin", and there is the Pauli principle relating the space and the spin variables. Depending on the value of the spin, identical particles split into two different classes, fermions and bosons. If two identical fermions (e.g. electrons) have a "symmetric" spin function (e.g. parallel spins) the spatial variables must be "antisymmetric" (i.e. they exclude each other from their places much as if there was a repulsive force), and vice versa, i.e. for antiparallel "spins" the "position variables" must be symmetric (i.e. the apparent force must be attractive). Thus in the case of two fermions there is a strictly negative correlation between spatial and spin variables, whereas for two bosons (e.g. quanta of electromagnetic waves, photons) the correlation is strictly positive.
Thus the notion "force" loses already part of its meaning.
Feynman diagrams.
In modern particle physics, forces and the acceleration of particles are explained as a mathematical by-product of exchange of momentum-carrying gauge bosons. With the development of quantum field theory and general relativity, it was realized that force is a redundant concept arising from conservation of momentum (4-momentum in relativity and momentum of virtual particles in quantum electrodynamics). The conservation of momentum can be directly derived from the homogeneity or symmetry of space and so is usually considered more fundamental than the concept of a force. Thus the currently known fundamental forces are considered more accurately to be "fundamental interactions". When particle A emits (creates) or absorbs (annihilates) virtual particle B, a momentum conservation results in recoil of particle A making impression of repulsion or attraction between particles A A' exchanging by B. This description applies to all forces arising from fundamental interactions. While sophisticated mathematical descriptions are needed to predict, in full detail, the accurate result of such interactions, there is a conceptually simple way to describe such interactions through the use of Feynman diagrams. In a Feynman diagram, each matter particle is represented as a straight line (see world line) traveling through time, which normally increases up or to the right in the diagram. Matter and anti-matter particles are identical except for their direction of propagation through the Feynman diagram. World lines of particles intersect at interaction vertices, and the Feynman diagram represents any force arising from an interaction as occurring at the vertex with an associated instantaneous change in the direction of the particle world lines. Gauge bosons are emitted away from the vertex as wavy lines and, in the case of virtual particle exchange, are absorbed at an adjacent vertex.
The utility of Feynman diagrams is that other types of physical phenomena that are part of the general picture of fundamental interactions but are conceptually separate from forces can also be described using the same rules. For example, a Feynman diagram can describe in succinct detail how a neutron decays into an electron, proton, and neutrino, an interaction mediated by the same gauge boson that is responsible for the weak nuclear force.
Fundamental forces.
All of the forces in the universe are based on four fundamental interactions. The strong and weak forces are nuclear forces that act only at very short distances, and are responsible for the interactions between subatomic particles, including nucleons and compound nuclei. The electromagnetic force acts between electric charges, and the gravitational force acts between masses. All other forces in nature derive from these four fundamental interactions. For example, friction is a manifestation of the electromagnetic force acting between the atoms of two surfaces, and the Pauli exclusion principle, which does not permit atoms to pass through each other. Similarly, the forces in springs, modeled by Hooke's law, are the result of electromagnetic forces and the Exclusion Principle acting together to return an object to its equilibrium position. Centrifugal forces are acceleration forces that arise simply from the acceleration of rotating frames of reference.
The development of fundamental theories for forces proceeded along the lines of unification of disparate ideas. For example, Isaac Newton unified the force responsible for objects falling at the surface of the Earth with the force responsible for the orbits of celestial mechanics in his universal theory of gravitation. Michael Faraday and James Clerk Maxwell demonstrated that electric and magnetic forces were unified through one consistent theory of electromagnetism. In the 20th century, the development of quantum mechanics led to a modern understanding that the first three fundamental forces (all except gravity) are manifestations of matter (fermions) interacting by exchanging virtual particles called gauge bosons. This standard model of particle physics posits a similarity between the forces and led scientists to predict the unification of the weak and electromagnetic forces in electroweak theory subsequently confirmed by observation. The complete formulation of the standard model predicts an as yet unobserved Higgs mechanism, but observations such as neutrino oscillations indicate that the standard model is incomplete. A Grand Unified Theory allowing for the combination of the electroweak interaction with the strong force is held out as a possibility with candidate theories such as supersymmetry proposed to accommodate some of the outstanding unsolved problems in physics. Physicists are still attempting to develop self-consistent unification models that would combine all four fundamental interactions into a theory of everything. Einstein tried and failed at this endeavor, but currently the most popular approach to answering this question is string theory.
Gravitational.
What we now call gravity was not identified as a universal force until the work of Isaac Newton. Before Newton, the tendency for objects to fall towards the Earth was not understood to be related to the motions of celestial objects. Galileo was instrumental in describing the characteristics of falling objects by determining that the acceleration of every object in free-fall was constant and independent of the mass of the object. Today, this acceleration due to gravity towards the surface of the Earth is usually designated as formula_37 and has a magnitude of about 9.81 meters per second squared (this measurement is taken from sea level and may vary depending on location), and points toward the center of the Earth. This observation means that the force of gravity on an object at the Earth's surface is directly proportional to the object's mass. Thus an object that has a mass of formula_19 will experience a force:
In free-fall, this force is unopposed and therefore the net force on the object is its weight. For objects not in free-fall, the force of gravity is opposed by the reactions of their supports. For example, a person standing on the ground experiences zero net force, since his weight is balanced by a normal force exerted by the ground.
Newton's contribution to gravitational theory was to unify the motions of heavenly bodies, which Aristotle had assumed were in a natural state of constant motion, with falling motion observed on the Earth. He proposed a law of gravity that could account for the celestial motions that had been described earlier using Kepler's laws of planetary motion.
Newton came to realize that the effects of gravity might be observed in different ways at larger distances. In particular, Newton determined that the acceleration of the Moon around the Earth could be ascribed to the same force of gravity if the acceleration due to gravity decreased as an inverse square law. Further, Newton realized that the acceleration due to gravity is proportional to the mass of the attracting body. Combining these ideas gives a formula that relates the mass (formula_40) and the radius (formula_41) of the Earth to the gravitational acceleration:
where formula_43 is the distance between the two objects' centers of mass and formula_44 is the unit vector pointed in the direction away from the center of the first object toward the center of the second object.
This formula was powerful enough to stand as the basis for all subsequent descriptions of motion within the solar system until the 20th century. During that time, sophisticated methods of perturbation analysis were invented to calculate the deviations of orbits due to the influence of multiple bodies on a planet, moon, comet, or asteroid. The formalism was exact enough to allow mathematicians to predict the existence of the planet Neptune before it was observed.
It was only the orbit of the planet Mercury that Newton's Law of Gravitation seemed not to fully explain. Some astrophysicists predicted the existence of another planet (Vulcan) that would explain the discrepancies; however, despite some early indications, no such planet could be found. When Albert Einstein formulated his theory of general relativity (GR) he turned his attention to the problem of Mercury's orbit and found that his theory added a correction, which could account for the discrepancy. This was the first time that Newton's Theory of Gravity had been shown to be less correct than an alternative.
Since then, and so far, general relativity has been acknowledged as the theory that best explains gravity. In GR, gravitation is not viewed as a force, but rather, objects moving freely in gravitational fields travel under their own inertia in straight lines through curved space-time – defined as the shortest space-time path between two space-time events. From the perspective of the object, all motion occurs as if there were no gravitation whatsoever. It is only when observing the motion in a global sense that the curvature of space-time can be observed and the force is inferred from the object's curved path. Thus, the straight line path in space-time is seen as a curved line in space, and it is called the "ballistic trajectory" of the object. For example, a basketball thrown from the ground moves in a parabola, as it is in a uniform gravitational field. Its space-time trajectory (when the extra ct dimension is added) is almost a straight line, slightly curved (with the radius of curvature of the order of few light-years). The time derivative of the changing momentum of the object is what we label as "gravitational force".
Electromagnetic.
The electrostatic force was first described in 1784 by Coulomb as a force that existed intrinsically between two charges. The properties of the electrostatic force were that it varied as an inverse square law directed in the radial direction, was both attractive and repulsive (there was intrinsic polarity), was independent of the mass of the charged objects, and followed the superposition principle. Coulomb's law unifies all these observations into one succinct statement.
Subsequent mathematicians and physicists found the construct of the "electric field" to be useful for determining the electrostatic force on an electric charge at any point in space. The electric field was based on using a hypothetical "test charge" anywhere in space and then using Coulomb's Law to determine the electrostatic force. Thus the electric field anywhere in space is defined as
where formula_46 is the magnitude of the hypothetical test charge.
Meanwhile, the Lorentz force of magnetism was discovered to exist between two electric currents. It has the same mathematical character as Coulomb's Law with the proviso that like currents attract and unlike currents repel. Similar to the electric field, the magnetic field can be used to determine the magnetic force on an electric current at any point in space. In this case, the magnitude of the magnetic field was determined to be
where formula_48 is the magnitude of the hypothetical test current and formula_49 is the length of hypothetical wire through which the test current flows. The magnetic field exerts a force on all magnets including, for example, those used in compasses. The fact that the Earth's magnetic field is aligned closely with the orientation of the Earth's axis causes compass magnets to become oriented because of the magnetic force pulling on the needle.
Through combining the definition of electric current as the time rate of change of electric charge, a rule of vector multiplication called Lorentz's Law describes the force on a charge moving in a magnetic field. The connection between electricity and magnetism allows for the description of a unified "electromagnetic force" that acts on a charge. This force can be written as a sum of the electrostatic force (due to the electric field) and the magnetic force (due to the magnetic field). Fully stated, this is the law:
where formula_3 is the electromagnetic force, formula_46 is the magnitude of the charge of the particle, formula_53 is the electric field, formula_5 is the velocity of the particle that is crossed with the magnetic field (formula_55).
The origin of electric and magnetic fields would not be fully explained until 1864 when James Clerk Maxwell unified a number of earlier theories into a set of 20 scalar equations, which were later reformulated into 4 vector equations by Oliver Heaviside and Josiah Willard Gibbs. These "Maxwell Equations" fully described the sources of the fields as being stationary and moving charges, and the interactions of the fields themselves. This led Maxwell to discover that electric and magnetic fields could be "self-generating" through a wave that traveled at a speed that he calculated to be the speed of light. This insight united the nascent fields of electromagnetic theory with optics and led directly to a complete description of the electromagnetic spectrum.
However, attempting to reconcile electromagnetic theory with two observations, the photoelectric effect, and the nonexistence of the ultraviolet catastrophe, proved troublesome. Through the work of leading theoretical physicists, a new theory of electromagnetism was developed using quantum mechanics. This final modification to electromagnetic theory ultimately led to quantum electrodynamics (or QED), which fully describes all electromagnetic phenomena as being mediated by wave–particles known as photons. In QED, photons are the fundamental exchange particle, which described all interactions relating to electromagnetism including the electromagnetic force.
It is a common misconception to ascribe the stiffness and rigidity of solid matter to the repulsion of like charges under the influence of the electromagnetic force. However, these characteristics actually result from the Pauli exclusion principle. Since electrons are fermions, they cannot occupy the same quantum mechanical state as other electrons. When the electrons in a material are densely packed together, there are not enough lower energy quantum mechanical states for them all, so some of them must be in higher energy states. This means that it takes energy to pack them together. While this effect is manifested macroscopically as a structural force, it is technically only the result of the existence of a finite set of electron states.
Strong nuclear.
There are two "nuclear forces", which today are usually described as interactions that take place in quantum theories of particle physics. The strong nuclear force is the force responsible for the structural integrity of atomic nuclei while the weak nuclear force is responsible for the decay of certain nucleons into leptons and other types of hadrons.
The strong force is today understood to represent the interactions between quarks and gluons as detailed by the theory of quantum chromodynamics (QCD). The strong force is the fundamental force mediated by gluons, acting upon quarks, antiquarks, and the gluons themselves. The (aptly named) strong interaction is the "strongest" of the four fundamental forces.
The strong force only acts "directly" upon elementary particles. However, a residual of the force is observed between hadrons (the best known example being the force that acts between nucleons in atomic nuclei) as the nuclear force. Here the strong force acts indirectly, transmitted as gluons, which form part of the virtual pi and rho mesons, which classically transmit the nuclear force (see this topic for more). The failure of many searches for free quarks has shown that the elementary particles affected are not directly observable. This phenomenon is called color confinement.
Weak nuclear.
The weak force is due to the exchange of the heavy W and Z bosons. Its most familiar effect is beta decay (of neutrons in atomic nuclei) and the associated radioactivity. The word "weak" derives from the fact that the field strength is some 10 times less than that of the strong force. Still, it is stronger than gravity over short distances. A consistent electroweak theory has also been developed, which shows that electromagnetic forces and the weak force are indistinguishable at a temperatures in excess of approximately 10 kelvins. Such temperatures have been probed in modern particle accelerators and show the conditions of the universe in the early moments of the Big Bang.
Non-fundamental forces.
Some forces are consequences of the fundamental ones. In such situations, idealized models can be utilized to gain physical insight.
Normal force.
The normal force is due to repulsive forces of interaction between atoms at close contact. When their electron clouds overlap, Pauli repulsion (due to fermionic nature of electrons) follows resulting in the force that acts in a direction normal to the surface interface between two objects. The normal force, for example, is responsible for the structural integrity of tables and floors as well as being the force that responds whenever an external force pushes on a solid object. An example of the normal force in action is the impact force on an object crashing into an immobile surface.
Friction.
Friction is a surface force that opposes relative motion. The frictional force is directly related to the normal force that acts to keep two solid objects separated at the point of contact. There are two broad classifications of frictional forces: static friction and kinetic friction.
The static friction force (formula_56) will exactly oppose forces applied to an object parallel to a surface contact up to the limit specified by the coefficient of static friction (formula_57) multiplied by the normal force (formula_58). In other words, the magnitude of the static friction force satisfies the inequality:
The kinetic friction force (formula_60) is independent of both the forces applied and the movement of the object. Thus, the magnitude of the force equals:
where formula_62 is the coefficient of kinetic friction. For most surface interfaces, the coefficient of kinetic friction is less than the coefficient of static friction.
Tension.
Tension forces can be modeled using ideal strings that are massless, frictionless, unbreakable, and unstretchable. They can be combined with ideal pulleys, which allow ideal strings to switch physical direction. Ideal strings transmit tension forces instantaneously in action-reaction pairs so that if two objects are connected by an ideal string, any force directed along the string by the first object is accompanied by a force directed along the string in the opposite direction by the second object. By connecting the same string multiple times to the same object through the use of a set-up that uses movable pulleys, the tension force on a load can be multiplied. For every string that acts on a load, another factor of the tension force in the string acts on the load. However, even though such machines allow for an increase in force, there is a corresponding increase in the length of string that must be displaced in order to move the load. These tandem effects result ultimately in the conservation of mechanical energy since the work done on the load is the same no matter how complicated the machine.
Elastic force.
An elastic force acts to return a spring to its natural length. An ideal spring is taken to be massless, frictionless, unbreakable, and infinitely stretchable. Such springs exert forces that push when contracted, or pull when extended, in proportion to the displacement of the spring from its equilibrium position. This linear relationship was described by Robert Hooke in 1676, for whom Hooke's law is named. If formula_63 is the displacement, the force exerted by an ideal spring equals:
where formula_65 is the spring constant (or force constant), which is particular to the spring. The minus sign accounts for the tendency of the force to act in opposition to the applied load.
Continuum mechanics.
Newton's laws and Newtonian mechanics in general were first developed to describe how forces affect idealized point particles rather than three-dimensional objects. However, in real life, matter has extended structure and forces that act on one part of an object might affect other parts of an object. For situations where lattice holding together the atoms in an object is able to flow, contract, expand, or otherwise change shape, the theories of continuum mechanics describe the way forces affect the material. For example, in extended fluids, differences in pressure result in forces being directed along the pressure gradients as follows:
where formula_67 is the volume of the object in the fluid and formula_68 is the scalar function that describes the pressure at all locations in space. Pressure gradients and differentials result in the buoyant force for fluids suspended in gravitational fields, winds in atmospheric science, and the lift associated with aerodynamics and flight.
A specific instance of such a force that is associated with dynamic pressure is fluid resistance: a body force that resists the motion of an object through a fluid due to viscosity. For so-called "Stokes' drag" the force is approximately proportional to the velocity, but opposite in direction:
where:
More formally, forces in continuum mechanics are fully described by a stress–tensor with terms that are roughly defined as
where formula_73 is the relevant cross-sectional area for the volume for which the stress-tensor is being calculated. This formalism includes pressure terms associated with forces that act normal to the cross-sectional area (the matrix diagonals of the tensor) as well as shear terms associated with forces that act parallel to the cross-sectional area (the off-diagonal elements). The stress tensor accounts for forces that cause all strains (deformations) including also tensile stresses and compressions.
Fictitious forces.
There are forces that are frame dependent, meaning that they appear due to the adoption of non-Newtonian (that is, non-inertial) reference frames. Such forces include the centrifugal force and the Coriolis force. These forces are considered fictitious because they do not exist in frames of reference that are not accelerating. Because these forces are not genuine they are also referred to as "pseudo forces".
In general relativity, gravity becomes a fictitious force that arises in situations where spacetime deviates from a flat geometry. As an extension, Kaluza–Klein theory and string theory ascribe electromagnetism and the other fundamental forces respectively to the curvature of differently scaled dimensions, which would ultimately imply that all forces are fictitious.
Rotations and torque.
Forces that cause extended objects to rotate are associated with torques. Mathematically, the torque of a force formula_3 is defined relative to an arbitrary reference point as the cross-product:
where
Torque is the rotation equivalent of force in the same way that angle is the rotational equivalent for position, angular velocity for velocity, and angular momentum for momentum. As a consequence of Newton's First Law of Motion, there exists rotational inertia that ensures that all bodies maintain their angular momentum unless acted upon by an unbalanced torque. Likewise, Newton's Second Law of Motion can be used to derive an analogous equation for the instantaneous angular acceleration of the rigid body:
where
This provides a definition for the moment of inertia, which is the rotational equivalent for mass. In more advanced treatments of mechanics, where the rotation over a time interval is described, the moment of inertia must be substituted by the tensor that, when properly analyzed, fully determines the characteristics of rotations including precession and nutation.
Equivalently, the differential form of Newton's Second Law provides an alternative definition of torque:
Newton's Third Law of Motion requires that all objects exerting torques themselves experience equal and opposite torques, and therefore also directly implies the conservation of angular momentum for closed systems that experience rotations and revolutions through the action of internal torques.
Centripetal force.
For an object accelerating in circular motion, the unbalanced force acting on the object equals:
where formula_19 is the mass of the object, formula_16 is the velocity of the object and formula_43 is the distance to the center of the circular path and formula_44 is the unit vector pointing in the radial direction outwards from the center. This means that the unbalanced centripetal force felt by any object is always directed toward the center of the curving path. Such forces act perpendicular to the velocity vector associated with the motion of an object, and therefore do not change the speed of the object (magnitude of the velocity), but only the direction of the velocity vector. The unbalanced force that accelerates an object can be resolved into a component that is perpendicular to the path, and one that is tangential to the path. This yields both the tangential force, which accelerates the object by either slowing it down or speeding it up, and the radial (centripetal) force, which changes its direction.
Kinematic integrals.
Forces can be used to define a number of physical concepts by integrating with respect to kinematic variables. For example, integrating with respect to time gives the definition of impulse:
which by Newton's Second Law must be equivalent to the change in momentum (yielding the Impulse momentum theorem).
Similarly, integrating with respect to position gives a definition for the work done by a force:
which is equivalent to changes in kinetic energy (yielding the work energy theorem).
Power "P" is the rate of change d"W"/d"t" of the work "W", as the trajectory is extended by a position change formula_89 in a time interval d"t":
with formula_91 the velocity.
Potential energy.
Instead of a force, often the mathematically related concept of a potential energy field can be used for convenience. For instance, the gravitational force acting upon an object can be seen as the action of the gravitational field that is present at the object's location. Restating mathematically the definition of energy (via the definition of work), a potential scalar field formula_92 is defined as that field whose gradient is equal and opposite to the force produced at every point:
Forces can be classified as conservative or nonconservative. Conservative forces are equivalent to the gradient of a potential while nonconservative forces are not.
Conservative forces.
A conservative force that acts on a closed system has an associated mechanical work that allows energy to convert only between kinetic or potential forms. This means that for a closed system, the net mechanical energy is conserved whenever a conservative force acts on the system. The force, therefore, is related directly to the difference in potential energy between two different locations in space, and can be considered to be an artifact of the potential field in the same way that the direction and amount of a flow of water can be considered to be an artifact of the contour map of the elevation of an area.
Conservative forces include gravity, the electromagnetic force, and the spring force. Each of these forces has models that are dependent on a position often given as a radial vector formula_76 emanating from spherically symmetric potentials. Examples of this follow:
For gravity:
where formula_96 is the gravitational constant, and formula_97 is the mass of object "n".
For electrostatic forces:
where formula_99 is electric permittivity of free space, and formula_100 is the electric charge of object "n".
For spring forces:
where formula_65 is the spring constant.
Nonconservative forces.
For certain physical scenarios, it is impossible to model forces as being due to gradient of potentials. This is often due to macrophysical considerations that yield forces as arising from a macroscopic statistical average of microstates. For example, friction is caused by the gradients of numerous electrostatic potentials between the atoms, but manifests as a force model that is independent of any macroscale position vector. Nonconservative forces other than friction include other contact forces, tension, compression, and drag. However, for any sufficiently detailed description, all these forces are the results of conservative ones since each of these macroscopic forces are the net results of the gradients of microscopic potentials.
The connection between macroscopic nonconservative forces and microscopic conservative forces is described by detailed treatment with statistical mechanics. In macroscopic closed systems, nonconservative forces act to change the internal energies of the system, and are often associated with the transfer of heat. According to the Second law of thermodynamics, nonconservative forces necessarily result in energy transformations within closed systems from ordered to more random conditions as entropy increases.
Units of measurement.
The SI unit of force is the newton (symbol N), which is the force required to accelerate a one kilogram mass at a rate of one meter per second squared, or . The corresponding CGS unit is the dyne, the force required to accelerate a one gram mass by one centimeter per second squared, or . A newton is thus equal to 100,000 dynes.
The gravitational foot-pound-second English unit of force is the pound-force (lbf), defined as the force exerted by gravity on a pound-mass in the standard gravitational field of . The pound-force provides an alternative unit of mass: one slug is the mass that will accelerate by one foot per second squared when acted on by one pound-force.
An alternative unit of force in a different foot-pound-second system, the absolute fps system, is the poundal, defined as the force required to accelerate a one-pound mass at a rate of one foot per second squared. The units of slug and poundal are designed to avoid a constant of proportionality in Newton's Second Law.
The pound-force has a metric counterpart, less commonly used than the newton: the kilogram-force (kgf) (sometimes kilopond), is the force exerted by standard gravity on one kilogram of mass. The kilogram-force leads to an alternate, but rarely used unit of mass: the metric slug (sometimes mug or hyl) is that mass that accelerates at when subjected to a force of 1 kgf. The kilogram-force is not a part of the modern SI system, and is generally deprecated; however it still sees use for some purposes as expressing aircraft weight, jet thrust, bicycle spoke tension, torque wrench settings and engine output torque. Other arcane units of force include the sthène, which is equivalent to 1000 N, and the kip, which is equivalent to 1000 lbf.
See also Ton-force.
Force measurement.
See force gauge, spring scale, load cell

</doc>
<doc id="10905" url="https://en.wikipedia.org/wiki?curid=10905" title="Family law">
Family law

Family law (also called matrimonial law) is an area of the law that deals with family matters and domestic relations, including:
This list is not exhaustive and varies depending on jurisdiction. In many jurisdictions in the United States, the family courts see the most crowded dockets. Litigants representative of all social and economic classes are parties within the system.
For the conflict of laws elements dealing with transnational and interstate issues, see marriage (conflict), divorce (conflict) and nullity (conflict).

</doc>
<doc id="10909" url="https://en.wikipedia.org/wiki?curid=10909" title="Foonly">
Foonly

Foonly was a short-lived American computer company formed by Dave Poole, one of the principal Super Foonly designers as well as one of hackerdom's more colourful personalities. The company produced a series of DEC PDP-10 compatible computers, first the high-performance F-1, and later a series of smaller and less expensive designs. The first Foonly machine, the F-1, was the computational engine used to create some of the graphics in the 1982 film "Tron".
PDP-10 successor.
The PDP-10 successor was to have been built by the Super Foonly project at the Stanford Artificial Intelligence Laboratory (SAIL) along with a new operating system. The intention was to leapfrog from the old DEC timesharing system which SAIL was then running to a new generation, bypassing TENEX – at that time the ARPANET standard. The F-1 was the fastest PDP-10 architecture machine ever built, with a clock rate of 90-100 ns per cycle, but only one was ever made. ARPA funding for both the Super Foonly and the new operating system was cut in 1974. The design for Foonly contributed greatly to the design of the PDP-10 model KL10.
History.
The following few paragraphs are a personal account of the events, by Dave Dyer:
Foonly Inc. did not acquire any financial resources as a result of building the F-1. They turned to the market for low-end machines, producing a series of smaller, slower, and much less expensive DEC-10 clones that ran a TENEX variant called Foonex; this seriously limited their market. Also, the machines shipped as wire-wrapped engineering prototypes requiring individual attention from more than usually competent site personnel, and thus had significant reliability problems. Poole's legendary temper and unwillingness to suffer fools gladly did not help matters. By the time of DEC's Jupiter project cancellation in 1983, Foonly's proposal to build another F-1 was eclipsed by another DEC-10 clone, the Mars computer, and the company never quite recovered.
Added by Phil Petit, (one of the above-mentioned Foonly designers):
Added by Dan Martin - Principal Engineer for Tymshare Inc.
The main application for Tymshare's version of the F4 was a version of Doug Englebart's NLS system, developed when his team moved to Tymshare from SRI, called "Augment". The machine, called the 26KL, was marketed as the "Augment Engine" when running Augment.
October 6, 2015
External links.
Added by Paul Milleson - Principal Manager, Foonly Inc.

</doc>
<doc id="10911" url="https://en.wikipedia.org/wiki?curid=10911" title="Functional group">
Functional group

In organic chemistry, functional groups are specific groups (moieties) of atoms or bonds within molecules that are responsible for the characteristic chemical reactions of those molecules. The same functional group will undergo the same or similar chemical reaction(s) regardless of the size of the molecule it is a part of. However, its relative reactivity can be modified by other functional groups nearby. The atoms of functional groups are linked to each other and to the rest of the molecule by covalent bonds. When the group of covalently bound atoms bears a net charge, the group is referred to more properly as a polyatomic ion or a complex ion. Any subgroup of atoms of a compound also may be called a radical, and if a covalent bond is broken homolytically, the resulting fragment radicals are referred as free radicals.
Combining the names of functional groups with the names of the parent alkanes generates what is termed a systematic nomenclature for naming organic compounds. The first carbon atom after the carbon that attaches to the functional group is called the alpha carbon; the second, beta carbon, the third, gamma carbon, etc. If there is another functional group at a carbon, it may be named with the Greek letter, e.g., the gamma-amine in gamma-aminobutanoic acid is on the third carbon of the carbon chain attached to the carboxylic acid group.
Synthetic chemistry.
Organic reactions are facilitated and controlled by the functional groups of the reactants. In general, alkyls are unreactive and difficult to get to react selectively at the desired positions, with few exceptions. In contrast, unsaturated carbon functional groups, and carbon-oxygen and carbon-nitrogen functional groups have a more diverse array of reactions that are also selective. It may be necessary to create a functional group in the molecule to make it react. For example, to synthesize iso-octane (the 8-carbon ideal gasoline) from the unfunctionalized alkane isobutane (a 4-carbon gas), isobutane is first dehydrogenated into isobutene. This contains the alkene functional group and can now dimerize with another isobutene to give iso-octene, which is then catalytically hydrogenated to iso-octane using pressured hydrogen gas.
Functionalization.
Functionalization is the addition of functional groups onto the surface of a material by chemical synthesis methods. The functional group added can be subjected to ordinary synthesis methods to attach virtually any kind of organic compound onto the surface. Functionalization is employed for surface modification of industrial materials in order to achieve desired surface properties such as water repellent coatings for automobile windshields and non-biofouling, hydrophilic coatings for contact lenses. In addition, functional groups are used to covalently link functional molecules to the surface of chemical and biochemical devices such as microarrays and microelectromechanical systems. Catalysts can be attached to a material that has been functionalized. For example, silica is functionalized with an alkyl silicone, wherein the alkyl contains an amine functional group. A ligand such as an EDTA fragment is synthesized onto the amine, and a metal cation is complexed into the EDTA fragment. The EDTA is "not" adsorbed onto the surface, but connected by a permanent chemical bond. Functional groups are also used to covalently link molecules such as fluorescent dyes, nanoparticles, proteins, DNA, and other compounds of interest for a variety of applications such as sensing and basic chemical research.
Table of common functional groups.
The following is a list of common functional groups. In the formulas, the symbols R and R' usually denote an attached hydrogen, or a hydrocarbon side chain of any length, but may sometimes refer to any group of atoms.
Hydrocarbons.
Functional groups, called hydrocarbyl, that contain only carbon and hydrogen, but vary in the number and order of double bonds. Each one differs in type (and scope) of reactivity.
There are also a large number of branched or ring alkanes that have specific names, e.g., tert-butyl, bornyl, cyclohexyl, etc. Hydrocarbons may form charged structures: positively charged carbocations or negative carbanions. Carbocations are often named "-um". Examples are tropylium and triphenylmethyl cations and the cyclopentadienyl anion.
Groups containing halogens.
Haloalkanes are a class of molecule that is defined by a carbon–halogen bond. This bond can be relatively weak (in the case of an iodoalkane) or quite stable (as in the case of a fluoroalkane). In general, with the exception of fluorinated compounds, haloalkanes readily undergo nucleophilic substitution reactions or elimination reactions. The substitution on the carbon, the acidity of an adjacent proton, the solvent conditions, etc. all can influence the outcome of the reactivity.
Groups containing oxygen.
Compounds that contain C-O bonds each possess differing reactivity based upon the location and hybridization of the C-O bond, owing to the electron-withdrawing effect of sp-hybridized oxygen (carbonyl groups) and the donating effects of sp-hybridized oxygen (alcohol groups).
Groups containing nitrogen.
Compounds that contain nitrogen in this category may contain C-O bonds, such as in the case of amides.
Groups containing sulfur.
Compounds that contain sulfur exhibit unique chemistry due to their ability to form more bonds than oxygen, their lighter analogue on the periodic table. Substitutive nomenclature (marked as prefix in table) is preferred over functional class nomenclature (marked as suffix in table) for sulfides, disulfides, sulfoxides and sulfones.
Groups containing phosphorus.
Compounds that contain phosphorus exhibit unique chemistry due to their ability to form more bonds than nitrogen, their lighter analogues on the periodic table.
Groups containing boron.
Compounds containing boron exhibit unique chemistry due to their having partially filled octets and therefore acting as Lewis acids.
Names of radicals or moieties.
These names are used to refer to the moieties themselves or to radical species, and also to form the names of halides and substituents in larger molecules.
When the parent hydrocarbon is unsaturated, the suffix ("-yl", "-ylidene", or "-ylidyne") replaces "-ane" (e.g. "ethane" becomes "ethyl"); otherwise, the suffix replaces only the final "-e" (e.g. "ethyne" becomes "ethynyl").
Note that when used to refer to moieties, multiple single bonds differ from a single multiple bond. For example, a methylene bridge (methanediyl) has two single bonds, whereas a methylene group (methylidene) has one double bond. Suffixes can be combined, as in methylidyne (triple bond) vs. methylylidene (single bond and double bond) vs. methanetriyl (three single bonds).
There are some retained names, such as methylene for methanediyl, 1,x-phenylene for phenyl-1,x-diyl (where x is 2, 3, or 4), carbyne for methylidyne, and trityl for triphenylmethyl.

</doc>
<doc id="10913" url="https://en.wikipedia.org/wiki?curid=10913" title="Fractal">
Fractal

A fractal is a natural phenomenon or a mathematical set that exhibits a repeating pattern that displays at every scale. It is also known as expanding symmetry or evolving symmetry. If the replication is exactly the same at every scale, it is called a self-similar pattern. An example of this is the Menger Sponge. Fractals can also be nearly the same at different levels. This latter pattern is illustrated in the magnifications of the Mandelbrot set. Fractals also include the idea of a detailed pattern that repeats itself.
Fractals are different from other geometric figures because of the way in which they scale. Doubling the edge lengths of a polygon multiplies its area by four, which is two (the ratio of the new to the old side length) raised to the power of two (the dimension of the space the polygon resides in). Likewise, if the radius of a sphere is doubled, its volume scales by eight, which is two (the ratio of the new to the old radius) to the power of three (the dimension that the sphere resides in). But if a fractal's one-dimensional lengths are all doubled, the spatial content of the fractal scales by a power that is not necessarily an integer. This power is called the fractal dimension of the fractal, and it usually exceeds the fractal's topological dimension.
As mathematical equations, fractals are usually nowhere differentiable. An infinite fractal curve can be conceived of as winding through space differently from an ordinary line, still being a 1-dimensional line yet having a fractal dimension indicating it also resembles a surface.
The mathematical roots of the idea of fractals have been traced throughout the years as a formal path of published works, starting in the 17th century with notions of recursion, then moving through increasingly rigorous mathematical treatment of the concept to the study of continuous but not differentiable functions in the 19th century by the seminal work of Bernard Bolzano, Bernhard Riemann, and Karl Weierstrass, and on to the coining of the word "fractal" in the 20th century with a subsequent burgeoning of interest in fractals and computer-based modelling in the 20th century. The term "fractal" was first used by mathematician Benoît Mandelbrot in 1975. Mandelbrot based it on the Latin "frāctus" meaning "broken" or "fractured", and used it to extend the concept of theoretical fractional dimensions to geometric patterns in nature.
There is some disagreement amongst authorities about how the concept of a fractal should be formally defined. Mandelbrot himself summarized it as "beautiful, damn hard, increasingly useful. That's fractals." The general consensus is that theoretical fractals are infinitely self-similar, iterated, and detailed mathematical constructs having fractal dimensions, of which many examples have been formulated and studied in great depth. Fractals are not limited to geometric patterns, but can also describe processes in time. Fractal patterns with various degrees of self-similarity have been rendered or studied in images, structures and sounds and found in nature, technology, art, and law.
Introduction.
The word "fractal" often has different connotations for laypeople than for mathematicians, where the layperson is more likely to be familiar with fractal art than a mathematical conception. The mathematical concept is difficult to define formally even for mathematicians, but key features can be understood with little mathematical background.
The feature of "self-similarity", for instance, is easily understood by analogy to zooming in with a lens or other device that zooms in on digital images to uncover finer, previously invisible, new structure. If this is done on fractals, however, no new detail appears; nothing changes and the same pattern repeats over and over, or for some fractals, nearly the same pattern reappears over and over. Self-similarity itself is not necessarily counter-intuitive (e.g., people have pondered self-similarity informally such as in the infinite regress in parallel mirrors or the homunculus, the little man inside the head of the little man inside the head...). The difference for fractals is that the pattern reproduced must be detailed.
This idea of being detailed relates to another feature that can be understood without mathematical background: Having a fractional or fractal dimension greater than its topological dimension, for instance, refers to how a fractal scales compared to how geometric shapes are usually perceived. A regular line, for instance, is conventionally understood to be 1-dimensional; if such a curve is divided into pieces each 1/3 the length of the original, there are always 3 equal pieces. In contrast, consider the Koch snowflake. It is also 1-dimensional for the same reason as the ordinary line, but it has, in addition, a fractal dimension greater than 1 because of how its detail can be measured. The fractal curve divided into parts 1/3 the length of the original line becomes 4 pieces rearranged to repeat the original detail, and this unusual relationship is the basis of its fractal dimension.
This also leads to understanding a third feature, that fractals as mathematical equations are "nowhere differentiable". In a concrete sense, this means fractals cannot be measured in traditional ways. To elaborate, in trying to find the length of a wavy non-fractal curve, one could find straight segments of some measuring tool small enough to lay end to end over the waves, where the pieces could get small enough to be considered to conform to the curve in the normal manner of measuring with a tape measure. But in measuring a wavy fractal curve such as the Koch snowflake, one would never find a small enough straight segment to conform to the curve, because the wavy pattern would always re-appear, albeit at a smaller size, essentially pulling a little more of the tape measure into the total length measured each time one attempted to fit it tighter and tighter to the curve.
History.
The history of fractals traces a path from chiefly theoretical studies to modern applications in computer graphics, with several notable people contributing canonical fractal forms along the way. According to Pickover, the mathematics behind fractals began to take shape in the 17th century when the mathematician and philosopher Gottfried Leibniz pondered recursive self-similarity (although he made the mistake of thinking that only the straight line was self-similar in this sense). In his writings, Leibniz used the term "fractional exponents", but lamented that "Geometry" did not yet know of them. Indeed, according to various historical accounts, after that point few mathematicians tackled the issues and the work of those who did remained obscured largely because of resistance to such unfamiliar emerging concepts, which were sometimes referred to as mathematical "monsters". Thus, it was not until two centuries had passed that in 1872 Karl Weierstrass presented the first definition of a function with a graph that would today be considered fractal, having the non-intuitive property of being everywhere continuous but nowhere differentiable. Not long after that, in 1883, Georg Cantor, who attended lectures by Weierstrass, published examples of subsets of the real line known as Cantor sets, which had unusual properties and are now recognized as fractals. Also in the last part of that century, Felix Klein and Henri Poincaré introduced a category of fractal that has come to be called "self-inverse" fractals.
One of the next milestones came in 1904, when Helge von Koch, extending ideas of Poincaré and dissatisfied with Weierstrass's abstract and analytic definition, gave a more geometric definition including hand drawn images of a similar function, which is now called the Koch snowflake. Another milestone came a decade later in 1915, when Wacław Sierpiński constructed his famous triangle then, one year later, his carpet. By 1918, two French mathematicians, Pierre Fatou and Gaston Julia, though working independently, arrived essentially simultaneously at results describing what are now seen as fractal behaviour associated with mapping complex numbers and iterative functions and leading to further ideas about attractors and repellors (i.e., points that attract or repel other points), which have become very important in the study of fractals. Very shortly after that work was submitted, by March 1918, Felix Hausdorff expanded the definition of "dimension", significantly for the evolution of the definition of fractals, to allow for sets to have noninteger dimensions. The idea of self-similar curves was taken further by Paul Lévy, who, in his 1938 paper "Plane or Space Curves and Surfaces Consisting of Parts Similar to the Whole" described a new fractal curve, the Lévy C curve.
Different researchers have postulated that without the aid of modern computer graphics, early investigators were limited to what they could depict in manual drawings, so lacked the means to visualize the beauty and appreciate some of the implications of many of the patterns they had discovered (the Julia set, for instance, could only be visualized through a few iterations as very simple drawings]). That changed, however, in the 1960s, when Benoît Mandelbrot started writing about self-similarity in papers such as "How Long Is the Coast of Britain? Statistical Self-Similarity and Fractional Dimension", which built on earlier work by Lewis Fry Richardson. In 1975 Mandelbrot solidified hundreds of years of thought and mathematical development in coining the word "fractal" and illustrated his mathematical definition with striking computer-constructed visualizations. These images, such as of his canonical Mandelbrot set, captured the popular imagination; many of them were based on recursion, leading to the popular meaning of the term "fractal". Currently, fractal studies are essentially exclusively computer-based.
Characteristics.
One often cited description that Mandelbrot published to describe geometric fractals is "a rough or fragmented geometric shape that can be split into parts, each of which is (at least approximately) a reduced-size copy of the whole"; this is generally helpful but limited. Authors disagree on the exact definition of "fractal", but most usually elaborate on the basic ideas of self-similarity and an unusual relationship with the space a fractal is embedded in. One point agreed on is that fractal patterns are characterized by fractal dimensions, but whereas these numbers quantify complexity (i.e., changing detail with changing scale), they neither uniquely describe nor specify details of how to construct particular fractal patterns. In 1975 when Mandelbrot coined the word "fractal", he did so to denote an object whose Hausdorff–Besicovitch dimension is greater than its topological dimension. It has been noted that this dimensional requirement is not met by fractal space-filling curves such as the Hilbert curve.
According to Falconer, rather than being strictly defined, fractals should, in addition to being nowhere differentiable and able to have a fractal dimension, be generally characterized by a gestalt of the following features;
As a group, these criteria form guidelines for excluding certain cases, such as those that may be self-similar without having other typically fractal features. A straight line, for instance, is self-similar but not fractal because it lacks detail, is easily described in Euclidean language, has the same Hausdorff dimension as topological dimension, and is fully defined without a need for recursion.
Brownian motion.
A path generated by a one dimensional Wiener process is a fractal curve of dimension 1.5, and Brownian motion is a finite version of this.
Common techniques for generating fractals.
Images of fractals can be created by fractal generating programs.
Simulated fractals.
Fractal patterns have been modeled extensively, albeit within a range of scales rather than infinitely, owing to the practical limits of physical time and space. Models may simulate theoretical fractals or natural phenomena with fractal features. The outputs of the modelling process may be highly artistic renderings, outputs for investigation, or benchmarks for fractal analysis. Some specific applications of fractals to technology are listed elsewhere. Images and other outputs of modelling are normally referred to as being "fractals" even if they do not have strictly fractal characteristics, such as when it is possible to zoom into a region of the fractal image that does not exhibit any fractal properties. Also, these may include calculation or display artifacts which are not characteristics of true fractals.
Modeled fractals may be sounds, digital images, electrochemical patterns, circadian rhythms, etc.
Fractal patterns have been reconstructed in physical 3-dimensional space and virtually, often called "in silico" modeling. Models of fractals are generally created using fractal-generating software that implements techniques such as those outlined above. As one illustration, trees, ferns, cells of the nervous system, blood and lung vasculature, and other branching patterns in nature can be modeled on a computer by using recursive algorithms and L-systems techniques. The recursive nature of some patterns is obvious in certain examples—a branch from a tree or a frond from a fern is a miniature replica of the whole: not identical, but similar in nature. Similarly, random fractals have been used to describe/create many highly irregular real-world objects. A limitation of modeling fractals is that resemblance of a fractal model to a natural phenomenon does not prove that the phenomenon being modeled is formed by a process similar to the modeling algorithms.
Natural phenomena with fractal features.
Approximate fractals found in nature display self-similarity over extended, but finite, scale ranges. The connection between fractals and leaves, for instance, is currently being used to determine how much carbon is contained in trees. Phenomena known to have fractal features include: 
<br>
In creative works.
The paintings of American artist Jackson Pollock have a definite fractal dimension. While Pollock's paintings appear to be composed of chaotic dripping and splattering, computer analysis demonstrates a degree of self-similarity at different scales (levels of detail) in his work.
Decalcomania, a technique used by artists such as Max Ernst, can produce fractal-like patterns. It involves pressing paint between two surfaces and pulling them apart.
Cyberneticist Ron Eglash has suggested that fractal geometry and mathematics are prevalent in African art, games, divination, trade, and architecture. Circular houses appear in circles of circles, rectangular houses in rectangles of rectangles, and so on. Such scaling patterns can also be found in African textiles, sculpture, and even cornrow hairstyles.
In a 1996 interview with Michael Silverblatt, David Foster Wallace admitted that the structure of the first draft of "Infinite Jest" he gave to his editor Michael Pietsch was inspired by fractals, specifically the Sierpinski triangle (a.k.a. Sierpinski gasket) but that the edited novel is "more like a lopsided Sierpinsky Gasket".
See also.
Fractal-generating programs.
There are many fractal generating programs available, both free and commercial. Some of the fractal generating programs include:
Most of the above programs make two-dimensional fractals, with a few creating three-dimensional fractal objects, such as quaternions, mandelbulbs and mandelboxes.

</doc>
<doc id="10915" url="https://en.wikipedia.org/wiki?curid=10915" title="Fluid">
Fluid

In physics, a fluid is a substance that continually deforms (flows) under an applied shear stress. Fluids are a subset of the phases of matter and include liquids, gases, plasmas and, to some extent, plastic solids. Fluids can be defined as substances that have zero shear modulus or in simpler terms a fluid is a substance which cannot resist any shear force applied to it.
Although the term "fluid" includes both the liquid and gas phases, in common usage, "fluid" is often used as a synonym for "liquid", with no implication that gas could also be present. For example, "brake fluid" is hydraulic oil and will not perform its required incompressible function if there is gas in it. This colloquial usage of the term is also common in medicine and in nutrition ("take plenty of fluids").
Liquids form a free surface (that is, a surface not created by the container) while gases do not. The distinction between solids and fluid is not entirely obvious. The distinction is made by evaluating the viscosity of the substance. Silly Putty can be considered to behave like a solid or a fluid, depending on the time period over which it is observed. It is best described as a viscoelastic fluid. There are many examples of substances proving difficult to classify. A particularly interesting one is pitch, as demonstrated in the pitch drop experiment currently running at the University of Queensland.
Physics.
Fluids display properties such as:
These properties are typically a function of their inability to support a shear stress in static equilibrium.
Solids can be subjected to shear stresses, and to normal stresses—both compressive and tensile. In contrast, ideal fluids can only be subjected to normal, compressive stress which is called pressure. Real fluids display viscosity and so are capable of being subjected to low levels of shear stress.
Modelling.
In a solid, shear stress is a function of strain, but in a fluid, shear stress is a function of strain rate. A consequence of this behavior is Pascal's law which describes the role of pressure in characterizing a fluid's state. 
Depending on the relationship between shear stress, and the rate of strain and its derivatives, fluids can be characterized as one of the following:
The behavior of fluids can be described by the Navier–Stokes equations—a set of partial differential equations which are based on:
The study of fluids is fluid mechanics, which is subdivided into fluid dynamics and fluid statics depending on whether the fluid is in motion.

</doc>
<doc id="10916" url="https://en.wikipedia.org/wiki?curid=10916" title="FAQ">
FAQ

Frequently asked questions (FAQ) or Questions and Answers (Q&A), are listed questions and answers, all supposed to be commonly asked in some context, and pertaining to a particular topic. The format is commonly used on email mailing lists and other online forums, where certain common questions tend to recur.
"FAQ" is pronounced as either an initialism (F-A-Q) or an acronym. Since the acronym "FAQ" originated in textual media, its pronunciation varies; "F-A-Q",and "fack", are commonly heard. Depending on usage, the term may refer specifically to a single frequently asked question, or to an assembled list of many questions and their answers. Web page designers often label a single list of questions as a "FAQ", such as on Google.com, while using "FAQs" to denote multiple lists of questions such as on United States Treasury sites.
Origins.
While the name may be recent, the FAQ format itself is quite old. For instance, Matthew Hopkins wrote "The Discovery of Witches" in 1647 as a list of questions and answers, introduced as "Certaine Queries answered". Many old catechisms are in a question-and-answer (Q&A) format. Summa Theologica, written by Thomas Aquinas in the second half of the 13th century, is a series of common questions about Christianity to which he wrote a series of replies. Plato's dialogues are even older.
The "FAQ" is an Internet textual tradition originating from the technical limitations of early mailing lists from NASA in the early 1980s. The first FAQ developed over several pre-Web years starting from 1982 when storage was expensive. On ARPAnet's SPACE mailing list, the presumption was that new users would download archived past messages through ftp. In practice, this rarely happened and the users tended to post questions to the mailing list instead of searching its archives. Repeating the "right" answers becomes tedious, and went against developing netiquette. A series of different measures were set up by loosely affiliated groups of computer system administrators, from regularly posted messages to netlib-like query email daemons. The acronym "FAQ" was developed between 1982 and 1985 by Eugene Miya of NASA for the SPACE mailing list. The format was then picked up on other mailing lists and Usenet news groups. Posting frequency changed to monthly, and finally weekly and daily across a variety of mailing lists and newsgroups. The first person to post a weekly FAQ was Jef Poskanzer to the Usenet net.graphics/comp.graphics newsgroups. Eugene Miya experimented with the first daily FAQ.
Meanwhile, on Usenet, Mark Horton had started a series of "Periodic Posts" (PP) which attempted to answer trivial questions with appropriate answers. Periodic summary messages posted to Usenet newsgroups attempted to reduce the continual reposting of the same basic questions and associated wrong answers. On Usenet, posting questions which are covered in a group's FAQ came to be considered poor netiquette, as it showed that the poster has not done the expected background reading before asking others to provide answers. Some groups may have multiple FAQ on related topics, or even two or more competing FAQs explaining a topic from different points of view.
Another factor on early ARPANET mailing lists was people asking questions promising to 'summarize' received answers, then either neglecting to do this or else posting simple concatenations of received replies with little to no quality checking.
Modern developments.
Originally the term "FAQ" referred to the Frequently Asked Question itself, and the compilation of questions and answers was known as a "FAQ list" or some similar expression. The term became more frequently used to refer to the list, and a text consisting of questions and their answers is often called a FAQ regardless of whether the questions are actually "frequently" asked, if they are asked at all, or if there is even any way of asking questions.
In some cases informative documents not in the traditional FAQ style have also been described as FAQs, particularly the video game FAQ, which is often a detailed descriptions of gameplay, including tips, secrets, and beginning-to-end guidance. Rarely are videogame FAQs in a question-and-answer format, although they may contain a short section of questions and answers.
Over time, the accumulated FAQs across all USENET news groups sparked the creation of the "*.answers" moderated newsgroups such as comp.answers, misc.answers and sci.answers for crossposting and collecting FAQ across respective comp.*, misc.*, sci.* newsgroups.

</doc>
<doc id="10918" url="https://en.wikipedia.org/wiki?curid=10918" title="Fibonacci number">
Fibonacci number

In mathematics, the Fibonacci numbers or Fibonacci sequence are the numbers in the following integer sequence:
or (often, in modern usage):
By definition, the first two numbers in the Fibonacci sequence are either 1 and 1, or 0 and 1, depending on the chosen starting point of the sequence, and each subsequent number is the sum of the previous two.
In mathematical terms, the sequence "F" of Fibonacci numbers is defined by the recurrence relation
with seed values
or
The Fibonacci sequence is named after Italian mathematician Leonardo of Pisa, known as Fibonacci. His 1202 book "Liber Abaci" introduced the sequence to Western European mathematics, although the sequence had been described earlier as Virahanka numbers in Indian mathematics. By modern convention, the sequence begins either with "F" = 0 or with "F" = 1. The sequence described in "Liber Abaci" began with "F" = 1.
Fibonacci numbers are closely related to Lucas numbers formula_6 in that they form a complementary pair of Lucas sequences formula_7 and formula_8. They are intimately connected with the golden ratio; for example, the closest rational approximations to the ratio are 2/1, 3/2, 5/3, 8/5, ... .
Fibonacci numbers appear unexpectedly often in mathematics, so much so that there is an entire journal dedicated to their study, the Fibonacci Quarterly. Applications of Fibonacci numbers include computer algorithms such as the Fibonacci search technique and the Fibonacci heap data structure, and graphs called Fibonacci cubes used for interconnecting parallel and distributed systems. They also appear in biological settings, such as branching in trees, phyllotaxis (the arrangement of leaves on a stem), the fruit sprouts of a pineapple, the flowering of an artichoke, an uncurling fern and the arrangement of a pine cone's bracts.
Origins.
The Fibonacci sequence appears in Indian mathematics, in connection with Sanskrit prosody. In the Sanskrit tradition of prosody, there was interest in enumerating all patterns of long (L) syllables that are 2 units of duration, and short (S) syllables that are 1 unit of duration. Counting the different patterns of L and S of a given duration results in the Fibonacci numbers: the number of patterns that are "m" short syllables long is the Fibonacci number "F".
Susantha Goonatilake writes that the development of the Fibonacci sequence "is attributed in part to Pingala (200 BC), later being associated with Virahanka (c. 700 AD), Gopāla (c. 1135), and Hemachandra (c. 1150)". Parmanand Singh cites Pingala's cryptic formula "misrau cha" ("the two are mixed") and cites scholars who interpret it in context as saying that the cases for "m" beats ("F") is obtained by adding a to "F" cases and to the "F" cases. He dates Pingala before 450 BC.
However, the clearest exposition of the sequence arises in the work of Virahanka (c. 700 AD), whose own work is lost, but is available in a quotation by Gopala (c. 1135):
The sequence is also discussed by Gopala (before 1135 AD) and by the Jain scholar Hemachandra (c. 1150).
Outside of India, the Fibonacci sequence first appears in the book "Liber Abaci" (1202) by Fibonacci. Fibonacci considers the growth of an idealized (biologically unrealistic) rabbit population, assuming that: a newly born pair of rabbits, one male, one female, are put in a field; rabbits are able to mate at the age of one month so that at the end of its second month a female can produce another pair of rabbits; rabbits never die and a mating pair always produces one new pair (one male, one female) every month from the second month on. The puzzle that Fibonacci posed was: how many pairs will there be in one year?
At the end of the "n"th month, the number of pairs of rabbits is equal to the number of new pairs (which is the number of pairs in month "n" − 2) plus the number of pairs alive last month ("n" − 1). This is the "n"th Fibonacci number.
The name "Fibonacci sequence" was first used by the 19th-century number theorist Édouard Lucas.
List of Fibonacci numbers.
The first 21 Fibonacci numbers "F" for "n" = 0, 1, 2, …, 20 are:
The sequence can also be extended to negative index "n" using the re-arranged recurrence relation
Thus the bidirectional sequence is
Use in mathematics.
The Fibonacci numbers occur in the sums of "shallow" diagonals in Pascal's triangle (see Binomial coefficient).
These numbers also give the solution to certain enumerative problems. The most common such problem is that of counting the number of compositions of 1s and 2s that sum to a given total "n": there are "F" ways to do this.
For example, if "n" = 5, then "F" = "F" = 8 counts the eight compositions:
1+1+1+1+1 = 1+1+1+2 = 1+1+2+1 = 1+2+1+1 = 2+1+1+1 = 2+2+1 = 2+1+2 = 1+2+2,
all of which sum to "n" = 5 = 6−1.
The Fibonacci numbers can be found in different ways among the set of binary strings, or equivalently, among the subsets of a given set.
Relation to the golden ratio.
Closed-form expression.
Like every sequence defined by a linear recurrence with constant coefficients, the Fibonacci numbers have a closed-form solution. It has become known as Binet's formula, even though it was already known by Abraham de Moivre:
where
is the golden ratio , and
Since formula_15, this formula can also be written as
formula_16
To see this, note that φ and ψ are both solutions of the equations
so the powers of φ and ψ satisfy the Fibonacci recursion. In other words,
and
It follows that for any values "a" and "b", the sequence defined by
satisfies the same recurrence
If "a" and "b" are chosen so that "U" = 0 and "U" = 1 then the resulting sequence "U" must be the Fibonacci sequence. This is the same as requiring "a" and "b" satisfy the system of equations:
which has solution
producing the required formula.
Through algebraic manipulation, the equation can be rewritten to form a solution for any sequence formula_24:
formula_25
Computation by rounding.
Since
for all "n" ≥ 0, the number "F" is the closest integer to formula_27. Therefore, it can be found by rounding, that is by the use of the nearest integer function:
or in terms of the floor function:
Similarly, if we already know that the number "F" > 1 is a Fibonacci number, we can determine its index within the sequence by
Limit of consecutive quotients.
Johannes Kepler observed that the ratio of consecutive Fibonacci numbers converges. He wrote that "as 5 is to 8 so is 8 to 13, practically, and as 8 is to 13, so is 13 to 21 almost", and concluded that the limit approaches the golden ratio formula_31.
This convergence holds regardless of the starting values, excluding 0, 0. This can be derived from Binet's formula.
For example, the initial values 3 and 2 generate the sequence 3, 2, 5, 7, 12, 19, 31, 50, 81, 131, 212, 343, 555, …, etc. The ratio of consecutive terms in this sequence shows the same convergence towards the golden ratio.
Another consequence is that the limit of the ratio of two Fibonacci numbers offset by a particular finite deviation in index corresponds to the golden ratio raised by that deviation. Or, in other words:
Decomposition of powers of the golden ratio.
Since the golden ratio satisfies the equation
this expression can be used to decompose higher powers formula_35 as a linear function of lower powers, which in turn can be decomposed all the way down to a linear combination of formula_31 and 1. The resulting recurrence relationships yield Fibonacci numbers as the linear coefficients:
This equation can be proved by induction on "n".
This expression is also true for "n" < 1 if the Fibonacci sequence "F" is extended to negative integers using the Fibonacci rule formula_38
Matrix form.
A 2-dimensional system of linear difference equations that describes the Fibonacci sequence is
which yields formula_40. As the eigenvalues of the matrix A are formula_41 and formula_42, for the respective eigenvectors formula_43 and formula_44,
and the initial value formula_45, the th term is
from which the th element in the Fibonacci series
as an analytic function of is now read off directly:
Equivalently, the same computation is performed by diagonalization of A through use of its eigendecomposition:
where formula_49 and formula_50 .
The closed-form expression for the th element in the Fibonacci series is therefore given by
which again yields
The matrix A has a determinant of −1, and thus it is a 2×2 unimodular matrix.
This property can be understood in terms of the continued fraction representation for the golden ratio:
The Fibonacci numbers occur as the ratio of successive convergents of the continued fraction for , and the matrix formed from successive convergents of any continued fraction has a determinant of +1 or −1. The matrix representation gives the following closed expression for the Fibonacci numbers:
Taking the determinant of both sides of this equation yields Cassini's identity,
Moreover, since for any square matrix A, the following identities can be derived (they are obtained form two different coefficients of the matrix product, and one may easily deduce the second one from the first one by changing into ),
In particular, with "m" = "n",
These last two identities provide a way to compute Fibonacci numbers recursively in arithmetic operations and in time , where is the time for the multiplication of two numbers of "n" digits. This matches the time for computing the "n"th Fibonacci number from the closed-form matrix formula, but with fewer redundant steps if one avoids recomputing an already computed Fibonacci number (recursion with memoization).
Recognizing Fibonacci numbers.
The question may arise whether a positive integer "x" is a Fibonacci number. This is true if and only if one or both of formula_58 or formula_59 is a perfect square. This is because Binet's formula above can be rearranged to give
which allows one to find the position in the sequence of a given Fibonacci number.
This formula must return an integer for all "n", so the radical expression must be an integer (otherwise the logarithm does not even return a rational number).
Combinatorial identities.
Most identities involving Fibonacci numbers can be proved using combinatorial arguments using the fact that "F" can be interpreted as the number of sequences of 1s and 2s that sum to "n" − 1. This can be taken as the definition of "F", with the convention that "F" = 0, meaning no sum adds up to −1, and that "F" = 1, meaning the empty sum "adds up" to 0. Here, the order of the summand matters. For example, 1 + 2 and 2 + 1 are considered two different sums.
For example, the recurrence relation
or in words, the "n"th Fibonacci number is the sum of the previous two Fibonacci numbers, may be shown by dividing the "F" sums of 1s and 2s that add to "n" − 1 into two non-overlapping groups. One group contains those sums whose first term is 1 and the other those sums whose first term is 2. In the first group the remaining terms add to "n" − 2, so it has "F"("n" − 1) sums, and in the second group the remaining terms add to "n" − 3, so there are "F" sums. So there are a total of "F" + "F" sums altogether, showing this is equal to "F".
Similarly, it may be shown that the sum of the first Fibonacci numbers up to the "n"th is equal to the ("n" + 2)-nd Fibonacci number minus 1. In symbols:
This is done by dividing the sums adding to "n" + 1 in a different way, this time by the location of the first 2. Specifically, the first group consists of those sums that start with 2, the second group those that start 1 + 2, the third 1 + 1 + 2, and so on, until the last group, which consists of the single sum where only 1's are used. The number of sums in the first group is "F"("n"), "F"("n" − 1) in the second group, and so on, with 1 sum in the last group. So the total number of sums is "F"("n") + "F"("n" − 1) + ... + "F"(1) + 1 and therefore this quantity is equal to "F"("n" + 2).
A similar argument, grouping the sums by the position of the first 1 rather than the first 2, gives two more identities:
and
In words, the sum of the first Fibonacci numbers with odd index up to "F" is the (2"n")th Fibonacci number, and the sum of the first Fibonacci numbers with even index up to "F" is the (2"n" + 1)th Fibonacci number minus 1.
A different trick may be used to prove
or in words, the sum of the squares of the first Fibonacci numbers up to "F" is the product of the "n"th and ("n" + 1)th Fibonacci numbers. In this case note that Fibonacci rectangle of size "F" by "F"("n" + 1) can be decomposed into squares of size "F", "F", and so on to "F" = 1, from which the identity follows by comparing areas.
Other identities.
Numerous other identities can be derived using various methods. Some of the most noteworthy are:
Cassini and Catalan's Identities.
Cassini's identity states that
Catalan's identity is a generalization:
d'Ocagne's identity.
where "L" is the "n"'th Lucas number. The last is an identity for doubling "n"; other identities of this type are
by Cassini's identity.
These can be found experimentally using lattice reduction, and are useful in setting up the special number field sieve to factorize a Fibonacci number.
More generally,
Putting in this formula, one gets again the formulas of the end of above section Matrix form.
Power series.
The generating function of the Fibonacci sequence is the power series
This series is convergent for formula_76 and its sum has a simple closed-form:
This can be proved by using the Fibonacci recurrence to expand each coefficient in the infinite sum:
Solving the equation
for "s"("x") results in the above closed form.
If is the reciprocal of an integer "k" that is greater than 1, the closed form of the series becomes
In particular,
for all positive integers "m".
Some math puzzle-books present as curious the particular value that comes from "m"=1, which is formula_82 Similarly, "m"=2 gives formula_83
Reciprocal sums.
Infinite sums over reciprocal Fibonacci numbers can sometimes be evaluated in terms of theta functions. For example, we can write the sum of every odd-indexed reciprocal Fibonacci number as
and the sum of squared reciprocal Fibonacci numbers as
If we add 1 to each Fibonacci number in the first sum, there is also the closed form
and there is a "nested" sum of squared Fibonacci numbers giving the reciprocal of the golden ratio,
No closed formula for the reciprocal Fibonacci constant
is known, but the number has been proved irrational by Richard André-Jeannin.
The Millin series gives the identity
which follows from the closed form for its partial sums as "N" tends to infinity:
Primes and divisibility.
Divisibility properties.
Every 3rd number of the sequence is even and more generally, every "k"th number of the sequence is a multiple of "F". Thus the Fibonacci sequence is an example of a divisibility sequence. In fact, the Fibonacci sequence satisfies the stronger divisibility property
Any three consecutive Fibonacci numbers are pairwise coprime, which means that, for every "n",
Every prime number "p" divides a Fibonacci number that can be determined by the value of "p" modulo 5. If "p" is congruent to 1 or 4 (mod 5), then "p" divides "F", and if "p" is congruent to 2 or 3 (mod 5), then, "p" divides "F". The remaining case is that "p" = 5, and in this case "p" divides "F". These cases can be combined into a single formula, using the Legendre symbol:
Primality testing.
The above formula can be used as a primality test in the sense that if
When m is large—say a 500-bit number—then we can calculate F (mod n) efficiently using the matrix form. Thus
Here the matrix power A is calculated using Modular exponentiation, which can be adapted to matrices--modular exponentiation for matrices
Fibonacci primes.
A "Fibonacci prime" is a Fibonacci number that is prime. The first few are:
Fibonacci primes with thousands of digits have been found, but it is not known whether there are infinitely many.
"F" is divisible by "F", so, apart from "F" = 3, any Fibonacci prime must have a prime index. As there are arbitrarily long runs of composite numbers, there are therefore also arbitrarily long runs of composite Fibonacci numbers.
No Fibonacci number greater than "F" = 8 is one greater or one less than a prime number.
The only nontrivial
square Fibonacci number is 144. Attila Pethő proved in 2001 that there is only a finite number of perfect power Fibonacci numbers. In 2006, Y. Bugeaud, M. Mignotte, and S. Siksek proved that 8 and 144 are the only such non-trivial perfect powers.
Prime divisors of Fibonacci numbers.
With the exceptions of 1, 8 and 144 ("F" = "F", "F" and "F") every Fibonacci number has a prime factor that is not a factor of any smaller Fibonacci number (Carmichael's theorem). As a result, 8 and 144 ("F" and "F") are the only Fibonacci numbers that are the product of other Fibonacci numbers .
The divisibility of Fibonacci numbers by a prime "p" is related to the Legendre symbol formula_96 which is evaluated as follows:
If "p" is a prime number then
For example,
It is not known whether there exists a prime "p" such that
Such primes (if there are any) would be called Wall–Sun–Sun primes.
Also, if "p" ≠ 5 is an odd prime number then:
Example 1. "p" = 7, in this case "p" ≡ 3 (mod 4) and we have:
Example 2. "p" = 11, in this case "p" ≡ 3 (mod 4) and we have:
Example 3. "p" = 13, in this case "p" ≡ 1 (mod 4) and we have:
Example 4. "p" = 29, in this case "p" ≡ 1 (mod 4) and we have:
For odd "n", all odd prime divisors of "F" are congruent to 1 modulo 4, implying that all odd divisors of "F" (as the products of odd prime divisors) are congruent to 1 modulo 4.
For example,
All known factors of Fibonacci numbers "F"("i") for all "i" < 50000 are collected at the relevant repositories.
Periodicity modulo "n".
If the members of the Fibonacci sequence are taken mod "n", the resulting sequence is periodic with period at most "6n". The lengths of the periods for various "n" form the so-called Pisano periods . Determining a general formula for the Pisano periods is an open problem, which includes as a subproblem a special instance of the problem of finding the multiplicative order of a modular integer or of an element in a finite field. However, for any particular "n", the Pisano period may be found as an instance of cycle detection.
Right triangles.
Starting with 5, every second Fibonacci number is the length of the hypotenuse of a right triangle with integer sides, or in other words, the largest number in a Pythagorean triple. The length of the longer leg of this triangle is equal to the sum of the three sides of the preceding triangle in this series of triangles, and the shorter leg is equal to the difference between the preceding bypassed Fibonacci number and the shorter leg of the preceding triangle.
The first triangle in this series has sides of length 5, 4, and 3. Skipping 8, the next triangle has sides of length 13, 12 (5 + 4 + 3), and 5 (8 − 3). Skipping 21, the next triangle has sides of length 34, 30 (13 + 12 + 5), and 16 (21 − 5). This series continues indefinitely. The triangle sides "a", "b", "c" can be calculated directly:
These formulas satisfy formula_116 for all "n", but they only represent triangle sides when "n" > 2.
Any four consecutive Fibonacci numbers "F", "F", "F" and "F" can also be used to generate a Pythagorean triple in a different way:
Example 1: let the Fibonacci numbers be 1, 2, 3 and 5. Then:
Magnitude.
Since "F" is asymptotic to formula_119, the number of digits in "F" is asymptotic to formula_120. As a consequence, for every integer "d" > 1 there are either 4 or 5 Fibonacci numbers with "d" decimal digits.
More generally, in the base "b" representation, the number of digits in "F" is asymptotic to formula_121.
Applications.
The Fibonacci numbers are important in the computational run-time analysis of Euclid's algorithm to determine the greatest common divisor of two integers: the worst case input for this algorithm is a pair of consecutive Fibonacci numbers.
Brasch et al. 2012 show how a generalised Fibonacci sequence also can be connected to the field of economics. In particular, it is shown how a generalised Fibonacci sequence enters the control function of ﬁnite-horizon dynamic optimisation problems with one state and one control variable. The procedure is illustrated in an example often referred to as the Brock–Mirman economic growth model.
Yuri Matiyasevich was able to show that the Fibonacci numbers can be defined by a Diophantine equation, which led to his original solution of Hilbert's tenth problem.
The Fibonacci numbers are also an example of a complete sequence. This means that every positive integer can be written as a sum of Fibonacci numbers, where any one number is used once at most.
Moreover, every positive integer can be written in a unique way as the sum of "one or more" distinct Fibonacci numbers in such a way that the sum does not include any two consecutive Fibonacci numbers. This is known as Zeckendorf's theorem, and a sum of Fibonacci numbers that satisfies these conditions is called a Zeckendorf representation. The Zeckendorf representation of a number can be used to derive its Fibonacci coding.
Fibonacci numbers are used by some pseudorandom number generators.
They are also used in planning poker, which is a step in estimating in software development projects that use the Scrum (software development) methodology.
Fibonacci numbers are used in a polyphase version of the merge sort algorithm in which an unsorted list is divided into two lists whose lengths correspond to sequential Fibonacci numbers – by dividing the list so that the two parts have lengths in the approximate proportion φ. A tape-drive implementation of the polyphase merge sort was described in "The Art of Computer Programming".
Fibonacci numbers arise in the analysis of the Fibonacci heap data structure.
The Fibonacci cube is an undirected graph with a Fibonacci number of nodes that has been proposed as a network topology for parallel computing.
A one-dimensional optimization method, called the Fibonacci search technique, uses Fibonacci numbers.
The Fibonacci number series is used for optional lossy compression in the IFF 8SVX audio file format used on Amiga computers. The number series compands the original audio wave similar to logarithmic methods such as µ-law.
Since the conversion factor 1.609344 for miles to kilometers is close to the golden ratio (denoted φ), the decomposition of distance in miles into a sum of Fibonacci numbers becomes nearly the kilometer sum when the Fibonacci numbers are replaced by their successors. This method amounts to a radix 2 number register in golden ratio base φ being shifted. To convert from kilometers to miles, shift the register down the Fibonacci sequence instead.
In nature.
Fibonacci sequences appear in biological settings, in two consecutive Fibonacci numbers, such as branching in trees, arrangement of leaves on a stem, the fruitlets of a pineapple, the flowering of artichoke, an uncurling fern and the arrangement of a pine cone, and the family tree of honeybees. However, numerous poorly substantiated claims of Fibonacci numbers or golden sections in nature are found in popular sources, e.g., relating to the breeding of rabbits in Fibonacci's own unrealistic example, the seeds on a sunflower, the spirals of shells, and the curve of waves.
Przemysław Prusinkiewicz advanced the idea that real instances can in part be understood as the expression of certain algebraic constraints on free groups, specifically as certain Lindenmayer grammars.
A model for the pattern of florets in the head of a sunflower was proposed by H. Vogel in 1979. This has the form
where "n" is the index number of the floret and "c" is a constant scaling factor; the florets thus lie on Fermat's spiral. The divergence angle, approximately 137.51°, is the golden angle, dividing the circle in the golden ratio. Because this ratio is irrational, no floret has a neighbor at exactly the same angle from the center, so the florets pack efficiently. Because the rational approximations to the golden ratio are of the form "F"("j"):"F"("j" + 1), the nearest neighbors of floret number "n" are those at "n" ± "F"("j") for some index "j", which depends on "r", the distance from the center. It is often said that sunflowers and similar arrangements have 55 spirals in one direction and 89 in the other (or some other pair of adjacent Fibonacci numbers), but this is true only of one range of radii, typically the outermost and thus most conspicuous.
The bee ancestry code.
Fibonacci numbers also appear in the pedigrees of idealized honeybees, according to the following rules:
Thus, a male bee always has one parent, and a female bee has two.
If one traces the pedigree of any male bee (1 bee), he has 1 parent (1 bee), 2 grandparents, 3 great-grandparents, 5 great-great-grandparents, and so on. This sequence of numbers of parents is the Fibonacci sequence. The number of ancestors at each level, "F", is the number of female ancestors, which is "F", plus the number of male ancestors, which is "F". This is under the unrealistic assumption that the ancestors at each level are otherwise unrelated.
Generalizations.
The Fibonacci sequence has been generalized in many ways. These include:

</doc>
<doc id="10923" url="https://en.wikipedia.org/wiki?curid=10923" title="Fontainebleau">
Fontainebleau

Fontainebleau (; ) is a commune in the metropolitan area of Paris, France. It is located south-southeast of the centre of Paris. Fontainebleau is a sub-prefecture of the Seine-et-Marne department, and it is the seat of the "arrondissement" of Fontainebleau. The commune has the largest land area in the Île-de-France region; it is the only one to cover a larger area than Paris itself.
Fontainebleau, together with the neighbouring commune of Avon and three other smaller communes, form an urban area of 39,713 inhabitants (according to the 2001 census). This urban area is a satellite of Paris.
Fontainebleau is renowned for the large and scenic forest of Fontainebleau, a favourite weekend getaway for Parisians, as well as for the historical Château de Fontainebleau, which once belonged to the kings of France. It is also the home of INSEAD, one of the world's most elite business schools; of the "École supérieure d'ingénieurs en informatique et génie des télécommunications" (ESIGETEL), one of France's "grandes écoles"; and of a branch of the "École nationale supérieure des mines de Paris", the Paris School of Mines, also one of the elite "grandes écoles".
Inhabitants of Fontainebleau are called "Bellifontains".
History.
Fontainebleau has been recorded in different Latinised forms, such as, "Fons Bleaudi", "Fons Bliaudi", "Fons Blaadi" in the 12th and 13th centuries, with "Fontem blahaud" being recorded in 1137. It became "Fons Bellaqueus" in the 17th century, which gave rise to the name of the inhabitants as "Bellifontains". The name originates as a medieval composite of two words: "Fontaine–" meaning spring, or fountainhead, followed by a person’s Germanic name "Blizwald".
This hamlet was endowed with a royal hunting lodge and a chapel by Louis VII in the middle of the twelfth century. A century later, Louis IX, also called Saint Louis, who held Fontainebleau in high esteem and referred to it as "his wilderness", had a country house and a hospital constructed there.
Philip the Fair was born there in 1268 and died there in 1314. In all, thirty-four sovereigns, from Louis VI, the Fat, (1081–1137) to Napoleon III (1808–1873), spent time at Fontainebleau.
The connection between the town of Fontainebleau and the French monarchy was reinforced with the transformation of the royal country house into a true royal palace, the Palace of Fontainebleau. This was accomplished by the great builder-king, Francis I (1494–1547), who, in the largest of his many construction projects, reconstructed, expanded, and transformed the royal château at Fontainebleau into a residence that became his favourite, as well as the residence of his mistress, Anne, duchess of Étampes.
From the sixteenth to the eighteenth century, every monarch, from Francis I to Louis XV, made important renovations at the Palace of Fontainebleau, including demolitions, reconstructions, additions, and embellishments of various descriptions, all of which endowed it with a character that is a bit heterogeneous, but harmonious nonetheless.
On 18 October 1685, Louis XIV signed the "Edict of Fontainebleau" there. Also known as the "Revocation of the Edict of Nantes", this royal fiat reversed the permission granted to the Huguenots in 1598 to worship publicly in specified locations and hold certain other privileges. The result was that a large number of Protestants were forced to convert to the Catholic faith, killed, or forced into exile, mainly in the Low Countries, Prussia and in England.
The 1762 Treaty of Fontainebleau, a secret agreement between France and Spain concerning the Louisiana territory in North America, was concluded here. Also, preliminary negotiations, held before the 1763 Treaty of Paris was signed, ending the Seven Years' War, were at Fontainebleau.
During the French Revolution, Fontainebleau was temporarily renamed Fontaine-la-Montagne, meaning "Fountain by the Mountain". (The mountain referred to is the series of rocky formations located in the forest of Fontainebleau.)
On 29 October 1807, Manuel Godoy, chancellor to the Spanish king, Charles IV and Napoleon signed the Treaty of Fontainebleau, which authorized the passage of French troops through Spanish territories so that they might invade Portugal.
On 20 June 1812, Pope Pius VII arrived at the château of Fontainebleau, after a secret transfer from Savona, accompanied by his personal physician, Balthazard Claraz. In poor health, the Pope was the prisoner of Napoleon, and he remained in his genteel prison at Fontainebleau for nineteen months. From June 1812 until 23 January 1814, the Pope never left his apartments.
On 20 April 1814, Napoleon Bonaparte, shortly before his first abdication, bid farewell to the Old Guard, the renowned "grognards" (gripers) who had served with him since his very first campaigns, in the "White Horse Courtyard" (la cour du Cheval Blanc) at the Palace of Fontainebleau. (The courtyard has since been renamed the "Courtyard of Goodbyes".) According to contemporary sources, the occasion was very moving. The 1814 Treaty of Fontainebleau stripped Napoleon of his powers (but not his title as Emperor of the French) and sent him into exile on Elba.
Until the 19th century, Fontainebleau was a village and a suburb of Avon. Later, it developed as an independent residential city.
For the 1924 Summer Olympics, the town played host to the riding portion of the modern pentathlon event. This event took place near a golf course.
In July and August 1946, the town hosted the Franco-Vietnamese Conference, intended to find a solution to the long-contested struggle for Vietnam’s independence from France, but the conference ended in failure.
Fontainebleau also hosted the general staff of the Allied Forces in Central Europe (Allied Forces Center or AFCENT) and the land forces command (LANDCENT); the air forces command (AIRCENT) was located nearby at Camp Guynemer. These facilities were in place from the inception of NATO until France’s partial withdrawal from NATO in 1967 when the United States returned those bases to French control. NATO moved AFCENT to Brunssum in the Netherlands and AIRCENT to Ramstein in West Germany. (Note that the Supreme Headquarters Allied Powers Europe, also known as SHAPE, was located at Rocquencourt, west of Paris, quite a distance from Fontainebleau).
Tourism.
Fontainebleau is a popular tourist destination; each year, 300,000 people visit the palace and about 11 million people visit the forest.
Fontainebleau forest.
The forest of Fontainebleau surrounds the town and dozens of nearby villages. It is protected by France's "Office National des Forêts", and it is recognised as a French national park. It is managed in order that its wild plants and trees, such as the rare service tree of Fontainebleau, and its populations of birds, mammals, and butterflies, can be conserved. It is a former royal hunting park often visited by hikers and horse riders. The forest is also well regarded for bouldering and is particularly popular among climbers, as the biggest developed area of that kind in the world.
Royal Château de Fontainebleau.
The Royal Château de Fontainebleau is a large palace where the kings of France took their ease. It is also the site where the French royal court, from 1528 onwards, entertained the body of new ideas that became known as the Renaissance.
INSEAD.
The European (and historical) campus of the INSEAD business school is located at the edge of Fontainebleau, by the Lycee Francois Couperin. INSEAD students live in local accommodations around the Fontainebleau area, and especially in the surrounding towns.
Other Notables.
The graves of G. I. Gurdjieff and Katherine Mansfield can be found in the cemetery at Avon.
Transport.
Fontainebleau is served by two stations on the Transilien Paris–Lyon rail line: Fontainebleau–Avon and Thomery. Fontainebleau–Avon station, the station closest to the centre of Fontainebleau, is located near the dividing-line between the commune of Fontainebleau and the commune of Avon, on the Avon side of the border.
Twinning.
Fontainebleau is twinned with the following cities:

</doc>
<doc id="10929" url="https://en.wikipedia.org/wiki?curid=10929" title="Fighter aircraft">
Fighter aircraft

A fighter aircraft is a military aircraft designed primarily for air-to-air combat against other aircraft, as opposed to bombers and attack aircraft, whose main mission is to attack ground targets. The hallmarks of a fighter are its speed, maneuverability, and small size relative to other combat aircraft.
Many fighters have secondary ground-attack capabilities, and some are designed as dual-purpose fighter-bombers; often aircraft that do not fulfill the standard definition are called fighters. This may be for political or national security reasons, for advertising purposes, or other reasons.
A fighter's main purpose is to establish air superiority over a battlefield. Since World War I, achieving and maintaining air superiority has been considered essential for victory in conventional warfare. The success or failure of a belligerent's efforts to gain air supremacy hinges on several factors including the skill of its pilots, the tactical soundness of its doctrine for deploying its fighters, and the numbers and performance of those fighters. Because of the importance of air superiority, since the dawn of aerial combat armed forces have constantly competed to develop technologically superior fighters and to deploy these fighters in greater numbers, and fielding a viable fighter fleet consumes a substantial proportion of the defense budgets of modern armed forces.
Terminology.
The word "fighter" did not become the official English term for such aircraft until after World War I. In the British Royal Flying Corps and Royal Air Force these aircraft were referred to as "scouts" into the early 1920s. The U.S. Army called their fighters "pursuit" aircraft from 1916 until the late 1940s. In most languages a fighter aircraft is known as a "hunter", or "hunting aircraft" ("avion de chasse, jagdflugzeuge, avión de caza" etc.). Exceptions include Russian, where a fighter is an "истребитель" (pronounced "istrebitel"), meaning "exterminator", and Hebrew where it is "matose krav" (literally "battle plane").
As a part of military nomenclature, a letter is often assigned to various types of aircraft to indicate their use, along with a number to indicate the specific aircraft. The letters used to designate a fighter differ in various countries — in the English-speaking world, "F" is now used to indicate a fighter (e.g. F-35 or Spitfire F.22), though when the pursuit designation was used in the US, they were "P" types (e.g. P-40). In Russia "I" was used (I-16), while the French continue to use "C" (Nieuport 17 C.1).
Although the term "fighter" specifies aircraft designed to shoot down other aircraft, such designs are often also useful as multirole fighter-bombers, strike fighters, and sometimes lighter, fighter-sized tactical ground-attack aircraft. This has always been the case, for instance the Sopwith Camel and other "fighting scouts" of World War I performed a great deal of ground-attack work. In World War II, the USAAF and RAF often favored fighters over dedicated light bombers or dive bombers, and types such as the P-47 Thunderbolt and Hawker Hurricane that were no longer competitive aerial combat fighters were relegated to ground attack. Several aircraft, such as the F-111 and F-117, have received fighter designations but had no fighter capability due to political or other reasons. The F-111B variant was originally intended for a fighter role with the U.S. Navy, but it was cancelled. This blurring follows the use of fighters from their earliest days for "attack" or "strike" operations against ground targets by means of strafing or dropping small bombs and incendiaries. Versatile multirole fighter-bombers such as the F/A-18 Hornet are a less expensive option than having a range of specialized aircraft types.
Some of the most expensive fighters such as the US F-14 Tomcat, F-15 Eagle, F-22 Raptor and Russian Su-27 were employed as all-weather interceptors as well as air superiority fighter aircraft, while commonly developing air-to-ground roles late in their careers. An interceptor is generally an aircraft intended to target (or intercept) bombers and so often trades maneuverability for climb rate.
Development overview.
Fighters were developed in World War I to deny enemy aircraft and dirigibles the ability to gather information by reconnaissance. Early fighters were very small and lightly armed by later standards, and most were biplanes built with a wooden frame, covered with fabric, and limited to about 100 mph. As control of the airspace over armies became increasingly important all of the major powers developed fighters to support their military operations. Between the wars, wood was largely replaced by steel tubing, then aluminium tubing, and finally aluminium stressed skin structures began to predominate.
By World War II, most fighters were all-metal monoplanes armed with batteries of machine guns or cannons and some were capable of speeds approaching 400 mph. Most fighters up to this point had one engine, but a number of twin-engine fighters were built; however they were found to be outmatched against single-engine fighters and were relegated to other tasks, such as night fighters equipped with primitive radar sets.
By the end of the war, turbojet engines were replacing piston engines as the means of propulsion, further increasing aircraft speed. Since the weight of the engine was so much less than on piston engined fighters, having two engines was no longer a handicap and one or two were used, depending on requirements. This in turn required the development of ejection seats so the pilot could escape and G-suits to counter the much greater forces being applied to the pilot during maneuvers.
In the 1950s, radar was fitted to day fighters, since pilots could no longer see far enough ahead to prepare for any opposition. Since then, radar capabilities have grown enormously and are now the primary method of target acquisition. Wings were made thinner and swept back to reduce transonic drag, which required new manufacturing methods to obtain sufficient strength. Skins were no longer sheet metal riveted to a structure, but milled from large slabs of alloy. The sound barrier was broken, and after a few false starts due to required changes in controls, speeds quickly reached Mach 2—past which aircraft can't maneuver to avoid attack.
Air-to-air missiles largely replaced guns and rockets in the early 1960s since both were believed unusable at the speeds being attained, however the Vietnam War showed that guns still had a role to play, and most fighters built since then are fitted with cannon (typically between 20 and 30 mm in caliber) in addition to missiles. Most modern combat aircraft can carry at least a pair of air-to-air missiles.
In the 1970s, turbofans replaced turbojets, improving fuel economy enough that the last piston engined support aircraft could be replaced with jets, making multi-role combat aircraft possible. Honeycomb structures began to replace milled structures, and the first composite components began to appear on components subjected to little stress.
With the steady improvements in computers, defensive systems have become increasingly efficient. To counter this, stealth technologies have been pursued by the United States, Russia, India and China. The first step was to find ways to reduce the aircraft's reflectivity to radar waves by burying the engines, eliminating sharp corners and diverting any reflections away from the radar sets of opposing forces. Various materials were found to absorb the energy from radar waves, and were incorporated into special finishes that have since found widespread application. Composite structures have become widespread, including major structural components, and have helped to counterbalance the steady increases in aircraft weight—most modern fighters are larger and heavier than World War II medium bombers.
Piston engine fighters.
World War I.
The word "fighter" was first used to describe a two-seater aircraft with sufficient lift to carry a machine gun and its operator as well as the pilot. Some of the first such "fighters" belonged to the "gunbus" series of experimental gun carriers of the British Vickers company that culminated in the Vickers F.B.5 Gunbus of 1914. The main drawback of this type of aircraft was its lack of speed. Planners quickly realized that an aircraft intended to destroy its kind in the air had to be fast enough to catch its quarry.
Another type of military aircraft was to form the basis for an effective "fighter" in the modern sense of the word. It was based on the small fast aircraft developed before the war for such air races as the Gordon Bennett Cup and Schneider Trophy. The military scout airplane was not expected to carry serious armament, but rather to rely on its speed to reach the scout or reconnoiter location and return quickly to report—essentially an aerial horse. British scout aircraft, in this sense, included the Sopwith Tabloid and Bristol Scout. French equivalents included the Morane-Saulnier N.
Soon after the commencement of the war, pilots armed themselves with pistols, carbines, grenades, and an assortment of improvised weapons. Many of these proved ineffective as the pilot had to fly his airplane while attempting to aim a handheld weapon and make a difficult deflection shot. The first step in finding a real solution was to mount the weapon on the aircraft, but the propeller remained a problem since the best direction to shoot is straight ahead. Numerous solutions were tried. A second crew member behind the pilot could aim and fire a swivel-mounted machine gun at enemy airplanes; however, this limited the area of coverage chiefly to the rear hemisphere, and effective coordination of the pilot's maneuvering with the gunner's aiming was difficult. This option was chiefly employed as a defensive measure on two-seater reconnaissance aircraft from 1915 on. Both the SPAD S.A and the Royal Aircraft Factory B.E.9 added a second crewman ahead of the engine in a pod but this was both hazardous to the second crewman and limited performance. The Sopwith L.R.T.Tr. similarly added a pod on the top wing with no better luck.
An alternative was to build a "pusher" scout such as the Airco DH.2, with the propeller mounted behind the pilot. The main drawback was that the high drag of a pusher type's tail structure made it slower than a similar "tractor" aircraft.
A better solution for a single seat scout was to mount the machine gun (rifles and pistols having been dispensed with) to fire forwards but outside the propeller arc. Wing guns were tried but the unreliable weapons available required frequent clearing of jammed rounds and misfires and remained impractical until after the war. Mounting the machine gun over the top wing worked well and was used long after the ideal solution was found. The Nieuport 11 of 1916 and Royal Aircraft Factory S.E.5 of 1918 both used this system with considerable success; however, this placement made aiming difficult and the location made it difficult for a pilot to both maneuver and have access to the gun's breech. The British Foster mounting was specifically designed for this kind of application, fitted with the Lewis Machine gun, which due to its design was unsuitable for synchronizing.
The need to arm a tractor scout with a forward-firing gun whose bullets passed through the propeller arc was evident even before the outbreak of war and inventors in both France and Germany devised mechanisms that could time the firing of the individual rounds to avoid hitting the propeller blades. Franz Schneider, a Swiss engineer, had patented such a device in Germany in 1913, but his original work was not followed up. French aircraft designer Raymond Saulnier patented a practical device in April 1914, but trials were unsuccessful because of the propensity of the machine gun employed to hang fire due to unreliable ammunition.
In December 1914, French aviator Roland Garros asked Saulnier to install his synchronization gear on Garros' Morane-Saulnier Type L. Unfortunately the gas-operated Hotchkiss machine gun he was provided had an erratic rate of fire and it was impossible to synchronize it with a spinning propeller. As an interim measure, the propeller blades were armored and fitted with metal wedges to protect the pilot from ricochets. Garros' modified monoplane was first flown in March 1915 and he began combat operations soon thereafter. Garros scored three victories in three weeks before he himself was downed on 18 April and his airplane, along with its synchronization gear and propeller was captured by the Germans.
Meanwhile, the synchronization gear (called the "Stangensteuerung" in German, for "pushrod control system") devised by the engineers of Anthony Fokker's firm was the first system to see production contracts, and would make the Fokker "Eindecker" monoplane a feared name over the Western Front, despite its being an adaptation of an obsolete pre-war French Morane-Saulnier racing airplane, with a mediocre performance and poor flight characteristics. The first victory for the "Eindecker" came on 1 July 1915, when "Leutnant" Kurt Wintgens, flying with the "Feldflieger Abteilung 6" unit on the Western Front, forced down a Morane-Saulnier Type L two-seat "parasol" monoplane just east of Luneville. Wintgens' aircraft, one of the five Fokker M.5K/MG production prototype examples of the "Eindecker", was armed with a synchronized, air-cooled aviation version of the Parabellum MG14 machine gun.
The success of the "Eindecker" kicked off a competitive cycle of improvement among the combatants, both sides striving to build ever more capable single-seat fighters. The Albatros D.I and Sopwith Pup of 1916 set the classic pattern followed by fighters for about twenty years. Most were biplanes and only rarely monoplanes or triplanes. The strong box structure of the biplane provided a rigid wing that allowed the accurate lateral control essential for dogfighting. They had a single operator, who flew the aircraft and also controlled its armament. They were armed with one or two Maxim or Vickers machine guns, which were easier to synchronize than other types, firing through the propeller arc. Gun breeches were directly in front of the pilot, with obvious implications in case of accidents, but jams could be cleared in flight, while aiming was simplified.
The use of metal aircraft structures was pioneered before World War I by Breguet but would find its biggest proponent in Anthony Fokker, who used chrome-molybdenum steel tubing for the fuselage structure of all his fighter designs, while the innovative German engineer Hugo Junkers developed two all-metal, single-seat fighter monoplane designs with cantilever wings: the strictly experimental Junkers J 2 private-venture aircraft, made with steel, and some forty examples of the Junkers D.I, made with corrugated duralumin, all based on his experience in creating the pioneering Junkers J 1 all-metal airframe technology demonstration aircraft of late 1915. While Fokker would pursue steel tube fuselages with wooden wings until the late 1930s, and Junkers would focus on corrugated sheet metal, Dornier was the first to build a fighter (The Dornier-Zeppelin D.I) made with pre-stressed sheet aluminium and having cantelevered wings, a form that would replace all others in the 1930s.
As collective combat experience grew, the more successful pilots such as Oswald Boelcke, Max Immelmann, and Edward Mannock developed innovative tactical formations and maneuvers to enhance their air units' combat effectiveness.
Allied and—before 1918—German pilots of World War I were not equipped with parachutes, so in-flight fires or structural failure were often fatal. Parachutes were well-developed by 1918 having previously been used by balloonists, and were adopted by the German flying services during the course of that year (the famous Manfred von Richthofen "Red Baron" was wearing one when he was killed), but the allied command continued to oppose their use on various grounds.
In April 1917, during a brief period of German aerial supremacy a British pilot's average life expectancy was 93 flying hours, or about three weeks of active service. More than 50,000 airmen from both sides died during the war.
Inter-war period (1919–38).
Fighter development stagnated between the wars, especially in the United States and the United Kingdom, where budgets were small. In France, Italy and Russia, where large budgets continued to allow major development, both monoplanes and all metal structures were common. By the end of the 1920s, however, those countries overspent themselves and were overtaken in the 1930s by those powers that hadn't been spending heavily, namely the British, the Americans and the Germans.
Given limited defense budgets, air forces tended to be conservative in their aircraft purchases, and biplanes remained popular with pilots because of their agility, and remained in service long after they had ceased to be competitive. Designs such as the Gloster Gladiator, Fiat CR.42, and Polikarpov I-15 were common even in the late 1930s, and many were still in service as late as 1942. Up until the mid-1930s, the majority of fighters in the US, the UK, Italy and Russia remained fabric-covered biplanes.
Fighter armament eventually began to be mounted inside the wings, outside the arc of the propeller, though most designs retained two synchronized machine guns directly ahead of the pilot, where they were more accurate ( that being the strongest part of the structure, reducing the vibration to which the guns were subjected to ). Shooting with this traditional arrangement was also easier for the further reason that the guns shot directly ahead in the direction of the aircraft's flight, up to the limit of the guns range; unlike wing-mounted guns which to be effective required to be harmonised, that is, preset to shoot at an angle by ground crews so that their bullets would converge on a target area a set distance ahead of the fighter. Rifle-caliber .30 and .303 in (7.62 mm) caliber guns remained the norm, with larger weapons either being too heavy and cumbersome or deemed unnecessary against such lightly built aircraft. It was not considered unreasonable to use World War I-style armament to counter enemy fighters as there was insufficient air-to-air combat during most of the period to disprove this notion.
The rotary engine, popular during World War I, quickly disappeared, its development having reached the point where rotational forces prevented more fuel and air from being delivered to the cylinders, which limited horsepower. They were replaced chiefly by the stationary radial engine though major advances led to inline engines, which gained ground with several exceptional engines—including the V-12 Curtiss D-12. Aircraft engines increased in power several-fold over the period, going from a typical in the 1918 Fokker D.VII to in the 1938 Curtiss P-36. The debate between the sleek in-line engines versus the more reliable radial models continued, with naval air forces preferring the radial engines, and land-based forces often choosing in-line units. Radial designs did not require a separate (and vulnerable) cooling system, but had increased drag. In-line engines often had a better power-to-weight ratio, but there were radial engines that kept working even after having suffered significant battle damage.
Some air forces experimented with "heavy fighters" (called "destroyers" by the Germans). These were larger, usually twin-engined aircraft, sometimes adaptations of light or medium bomber types. Such designs typically had greater internal fuel capacity (thus longer range) and heavier armament than their single-engine counterparts. In combat, they proved vulnerable to more agile single-engine fighters.
The primary driver of fighter innovation, right up to the period of rapid re-armament in the late 1930s, were not military budgets, but civilian aircraft racing. Aircraft designed for these races introduced innovations like streamlining and more powerful engines that would find their way into the fighters of World War II. The most significant of these was the Schneider Cup races, where competition grew so fierce, only national governments could afford to enter.
At the very end of the inter-war period in Europe came the Spanish Civil War. This was just the opportunity the German "Luftwaffe", Italian "Regia Aeronautica", and the Soviet Union's Red Air Force needed to test their latest aircraft. Each party sent numerous aircraft types to support their sides in the conflict. In the dogfights over Spain, the latest Messerschmitt Bf 109 fighters did well, as did the Soviet Polikarpov I-16. The German design had considerably more room for development however and the lessons learned led to greatly improved models in World War II. The Russians, whose side lost, failed to keep up and despite newer models coming into service, I-16s were outfought by the improved Bf 109s in World War II, while remaining the most common Soviet front-line fighter into 1942. For their part, the Italians developed several monoplanes such as the Fiat G.50, but being short on funds, were forced to continue operating obsolete Fiat CR.42 biplanes.
From the early 1930s the Japanese had been at war against both the Chinese Nationalists and the Russians in China, and used the experience to improve both training and aircraft, replacing biplanes with modern cantilever monoplanes and creating a cadre of exceptional pilots for use in the Pacific War. In the United Kingdom, at the behest of Neville Chamberlain, (more famous for his 'peace in our time' speech) the entire British aviation industry was retooled, allowing it to change quickly from fabric covered metal framed biplanes to cantilever stressed skin monoplanes in time for the war with Germany.
The period of improving the same biplane design over and over was now coming to an end, and the Hawker Hurricane and Supermarine Spitfire finally started to supplant the Gloster Gladiator and Hawker Fury biplanes but many of the former remained in front-line service well past the start of World War II. While not a combatant themselves in Spain, they absorbed many of the lessons learned in time to use them.
The Spanish Civil War also provided an opportunity for updating fighter tactics. One of the innovations to result from the aerial warfare experience this conflict provided was the development of the "finger-four" formation by the German pilot Werner Mölders. Each fighter squadron (German: "Staffel") was divided into several flights ("Schwärme") of four aircraft. Each "Schwarm" was divided into two "Rotten", which was a pair of aircraft. Each "Rotte" was composed of a leader and a wingman. This flexible formation allowed the pilots to maintain greater situational awareness, and the two "Rotten" could split up at any time and attack on their own. The finger-four would become widely adopted as the fundamental tactical formation over the course of World War.
World War II.
World War II featured fighter combat on a larger scale than any other conflict to date. German Field Marshal Erwin Rommel noted the effect of airpower: "Anyone who has to fight, even with the most modern weapons, against an enemy in complete command of the air, fights like a savage against modern European troops, under the same handicaps and with the same chances of success." Throughout the war, fighters performed their conventional role in establishing air superiority through combat with other fighters and through bomber interception, and also often performed roles such as tactical air support and reconnaissance.
Fighter design varied widely among combatants. The Japanese and Italians favored lightly armed and armored but highly maneuverable designs such as the Japanese Nakajima Ki-27, Nakajima Ki-43 and Mitsubishi A6M Zero and Italy's Fiat G.50 and Macchi MC.200. In contrast, designers in Great Britain, Germany, the Soviet Union, and the United States believed that the increased speed of fighter aircraft would create "g"-forces unbearable to pilots who attempted maneuvering dogfights typical of the First World War, and their fighters were instead optimized for speed and firepower. In practice, while light, highly maneuverable aircraft did possess some advantages in fighter-versus-fighter combat, those could usually be overcome by sound tactical doctrine, and the design approach of the Italians and Japanese made their fighters ill-suited as interceptors or attack aircraft.
European theater.
During the invasion of Poland and the Battle of France, Luftwaffe fighters—primarily the Messerschmitt Bf 109—held air superiority, and the Luftwaffe played a major role in German victories in these campaigns. During the Battle of Britain, however, British Hurricanes and Spitfires proved roughly equal to Luftwaffe fighters. Additionally Britain's use of radar and the advantages of fighting above Britain's home territory allowed the RAF to deny Germany air superiority, saving Britain from possible German invasion and dealing the Axis a major defeat early in the Second World War.
On the Eastern Front, Soviet fighter forces were overwhelmed during the opening phases of Operation Barbarossa. This was a result of the tactical surprise at the outset of the campaign, the leadership vacuum within the Soviet military left by the Great Purge, and the general inferiority of Soviet designs at the time, such as the obsolescent I-15 biplane and the I-16. More modern Soviet designs, including the MiG-3, LaGG-3 and Yak-1, had not yet arrived in numbers and in any case were still inferior to the Messerschmitt Bf 109. As a result, during the early months of these campaigns, Axis air forces destroyed large numbers of Red Air Force aircraft on the ground and in one-sided dogfights.
In the later stages on the Eastern Front, Soviet training and leadership improved, as did their equipment. Late-war Soviet designs such as the Yakovlev Yak-3 and Lavochkin La-7 had performance comparable to the German Bf-109 and Focke-Wulf Fw 190. Also, significant numbers of British, and later U.S., fighter aircraft were supplied to aid the Soviet war effort as part of Lend-Lease, with the Bell P-39 Airacobra proving particularly effective in the lower-altitude combat typical of the Eastern Front. The Soviets were also helped indirectly by the American and British bombing campaigns, which forced the Luftwaffe to shift many of its fighters away from the Eastern Front in defense against these raids. The Soviets increasingly were able to challenge the Luftwaffe, and while the Luftwaffe maintained a qualitative edge over the Red Air Force for much of the war, the increasing numbers and efficacy of the Soviet Air Force were critical to the Red Army's efforts at turning back and eventually annihilating the Wehrmacht.
Meanwhile, air combat on the Western Front had a much different character. Much of this combat was centered around the strategic bombing campaigns of the RAF and the USAAF. Axis fighter aircraft focused on defending against Allied bombers while Allied fighters' main role was as bomber escorts. The RAF raided German cities at night, and both sides developed radar-equipped night fighters for these battles. The Americans, in contrast, flew daylight bombing raids into Germany. Unescorted Consolidated B-24 Liberators and Boeing B-17 Flying Fortress bombers, however, proved unable to fend off German interceptors (primarily Bf-109s and FW-190s). With the later arrival of long range fighters, particularly the North American P-51 Mustang, American fighters were able to escort daylight raids far into Germany and establish control of the skies over Western Europe.
By the time of Operation Overlord in June 1944, the Allies had gained near complete air superiority over the Western Front. This cleared the way both for intensified strategic bombing of German cities and industries, and for the tactical bombing of battlefield targets. With the Luftwaffe largely cleared from the skies, Allied fighters increasingly served as attack aircraft.
Allied fighters, by gaining air superiority over the European battlefield, played a crucial role in the eventual defeat of the Axis, which Reichmarshal Hermann Göring, commander of the German "Luftwaffe" summed up when he said: "When I saw Mustangs over Berlin, I knew the jig was up."
Pacific theater.
Major air combat during the war in the Pacific began with the entry of the Western Allies following Japan's attack against Pearl Harbor. The
Imperial Japanese Navy Air Service primarily operated the Mitsubishi A6M Zero, and the Imperial Japanese Army Air Service flew the Nakajima Ki-27 and the Nakajima Ki-43, initially enjoying great success, as these fighters generally had better range, maneuverability, speed and climb rates than their Allied counterparts. Additionally, Japanese pilots had received excellent training and many were combat veterans from Japan's campaigns in China. They quickly gained air superiority over the Allies, who at this stage of the war were often disorganized, under-trained and poorly equipped, and Japanese air power contributed significantly to their successes in the Philippines, Malaysia and Singapore, the Dutch East Indies and Burma.
By mid-1942, the Allies began to regroup and while some Allied aircraft such as the Brewster Buffalo and the P-39 were hopelessly outclassed by fighters like Japan's Zero, others such as the Army's P-40 and the Navy's Wildcat possessed attributes such as superior firepower, ruggedness and dive speed, and the Allies soon developed tactics (such as the Thach weave) to take advantage of these strengths. These changes soon paid dividends, as the Allied ability to deny Japan air superiority was critical to their victories at Coral Sea, Midway, Guadalcanal and New Guinea. In China, the Flying Tigers also used the same tactics with some success, although they were unable to stem the tide of Japanese advances there.
By 1943, the Allies began to gain the upper hand in the Pacific Campaign's air campaigns. Several factors contributed to this shift. First, second-generation Allied fighters such as the Hellcat and the P-38, and later the Corsair, the P-47 and the P-51, began arriving in numbers. These fighters outperformed Japanese fighters in all respects except maneuverability. Other problems with Japan's fighter aircraft also became apparent as the war progressed, such as their lack of armor and light armament, which made them inadequate as bomber interceptors or ground-attack planes—roles Allied fighters excelled at. Most importantly, Japan's training program failed to provide enough well-trained pilots to replace losses. In contrast, the Allies improved both the quantity and quality of pilots graduating from their training programs.
By mid-1944, Allied fighters had gained air superiority throughout the theater, which would not be contested again during the war. The extent of Allied quantitative and qualitative superiority by this point in the war was demonstrated during the Battle of the Philippine Sea, a lopsided Allied victory where Japanese fliers were downed in such numbers and with such ease that American fighter pilots likened it to a great turkey shoot.
Late in the war, Japan did begin to produce new fighters such as the Nakajima Ki-84 and the Kawanishi N1K to replace the venerable Zero, but these were produced only in small numbers, and in any case by that time Japan lacked trained pilots or sufficient fuel to mount a sustained challenge to Allied fighters. During the closing stages of the war, Japan's fighter arm could not seriously challenge raids over Japan by American B-29s, and was largely relegated to Kamikaze tactics.
Technological innovations.
Fighter technology advanced rapidly during the Second World War. Piston-engines, which powered the vast majority of World War II fighters, grew more powerful: at the beginning of the war fighters typically had engines producing between and , while by the end of the war many could produce over . For example, the Spitfire, one of the few fighters in continuous production throughout the war, was in 1939 powered by a Merlin II, while variants produced in 1945 were equipped with the Griffon 61. Nevertheless, these fighters could only achieve modest increases in top speed due to problems of compressibility created as aircraft and their propellers approached the sound barrier, and it was apparent that propeller-driven aircraft were approaching the limits of their performance. German jet and rocket powered fighters entered combat in 1944, although too late to impact the war's outcome. The same year the Allies' only operational jet fighter, the Gloster Meteor, also entered service.
World War II fighters also increasingly featured monocoque construction, which improved their aerodynamic efficiency while also adding structural strength. Laminar flow wings, which improved high speed performance, also came into use on fighters such as the P-51, while the Messerschmitt Me 262 and the Messerschmitt Me 163 featured swept wings that dramatically reduced drag at high subsonic speeds.
Armament also advanced during the war. The rifle-caliber machine guns that were common on prewar fighters could not easily down the more rugged warplanes of the era. Air forces began to replace or supplement them with cannons, which fired explosive shells that could blast a hole in an enemy aircraft—rather than relying on kinetic energy from a solid bullet striking a critical component of the aircraft, such as a fuel line or control cable, or the pilot. Cannons could bring down even heavy bombers with just a few hits, but their slower rate of fire made it difficult to hit fast moving fighters in a dogfight. Eventually, most fighters mounted cannons, sometimes in combination with machine guns.
The British epitomized this shift. Their standard early war fighters mounted eight calibre machine guns—whereas by mid-war they often featured a combination of machine guns and 20 mm cannons, and late in the war often only cannons. The Americans, in contrast, had problems designing a native cannon design, so instead placed multiple .50 caliber (12.7 mm) heavy machine guns on their fighters. Fighters were also increasingly fitted with bomb racks and air-to-surface ordnance such as bombs or rockets beneath their wings, and pressed into close air support roles as fighter-bombers. Although they carried less ordnance than light and medium bombers, and generally had a shorter range, they were cheaper to produce and maintain and their maneuverability made it easier for them to hit moving targets such as motorized vehicles. Moreover, if they encountered enemy fighters, their ordnance (which reduced lift and increased drag and therefore decreased performance) could be jettisoned and they could engage the enemy fighters, which eliminated the need for fighter escorts that bombers required. Heavily armed and sturdily constructed fighters such as Germany's Focke-Wulf Fw 190, Britain's Hawker Typhoon and Hawker Tempest, and America's P-40, Corsair, P-47 and P-38 all excelled as fighter-bombers, and since the Second World War ground attack has been an important secondary capability of many fighters.
World War II also saw the first use of airborne radar on fighters. The primary purpose of these radars was to help night fighters locate enemy bombers and fighters. Because of the bulkiness of these radar sets, they could not be carried on conventional single-engined fighters and instead were typically retrofitted to larger heavy fighters or light bombers such as Germany's Messerschmitt Bf 110 and Junkers Ju 88, Britain's Mosquito and Beaufighter, and America's A-20, which then served as night fighters. The Northrop P-61 Black Widow, a purpose-built night fighter, was the only fighter of the war that incorporated radar into its original design. Britain and America cooperated closely in the development of airborne radar, and Germany's radar technology generally lagged slightly behind Anglo-American efforts, while other combatants developed few radar-equipped fighters.
Post–World War II period.
Several prototype fighter programs begun early in 1945 continued on after the war and led to advanced piston-engine fighters that entered production and operational service in 1946. A typical example is the Lavochkin La-9 'Fritz', which was an evolution of the successful wartime Lavochkin La-7 'Fin'. Working through a series of prototypes, the La-120, La-126 and La-130, the Lavochkin design bureau sought to replace the La-7's wooden airframe with a metal one, as well as fit a laminar-flow wing to improve maneuver performance, and increased armament. The La-9 entered service in August 1946 and was produced until 1948; it also served as the basis for the development of a long-range escort fighter, the La-11 'Fang', of which nearly 1200 were produced 1947–1951. Over the course of the Korean War, however, it became obvious that the day of the piston-engined fighter was coming to a close and that the future would lie with the jet fighter.
This period also witnessed experimentation with jet-assisted piston engine aircraft. La-9 derivatives included examples fitted with two underwing auxiliary pulsejet engines (the La-9RD) and a similarly mounted pair of auxiliary ramjet engines (the La-138); however, neither of these entered service. One that did enter service – with the U.S. Navy in March 1945 – was the Ryan FR-1 Fireball; production was halted with the war's end on VJ-Day, with only 66 having been delivered, and the type was withdrawn from service in 1947. The USAAF had ordered its first 13 mixed turboprop-turbojet-powered pre-production prototypes of the Consolidated Vultee XP-81 fighter, but this program was also canceled by VJ Day, with 80% of the engineering work completed.
Rocket-powered fighters.
The first rocket-powered aircraft was the Lippisch Ente, which made a successful maiden flight in March 1928. The only pure rocket aircraft ever mass-produced was the Messerschmitt Me 163 in 1944, one of several German World War II projects aimed at developing rocket-powered aircraft. Later variants of the Me 262 (C-1a and C-2b) were also fitted with rocket powerplants, while earlier models were fitted with rocket boosters, but were not mass-produced with these modifications.
The USSR experimented with a rocket-powered interceptor in the years immediately following World War II, the Mikoyan-Gurevich I-270. Only two were built.
In the 1950s, the British developed mixed-power jet designs employing both rocket and jet engines to cover the performance gap that existed in turbojet designs. The rocket was the main engine for delivering the speed and height required for high-speed interception of high-level bombers and the turbojet gave increased fuel economy in other parts of flight, most notably to ensure the aircraft was able to make a powered landing rather than risking an unpredictable gliding return. The Saunders-Roe SR.53 was a successful design, and was planned for production when economics forced the British to curtail most aircraft programs in the late 1950s. Furthermore, rapid advancements in jet engine technology rendered mixed-power aircraft designs like Saunders-Roe's SR.53 (and its SR.177 maritime variant) obsolete. The American XF-91 Thunderceptor (the first U.S. fighter to exceed Mach 1 in level flight) met a similar fate for the same reason, and no hybrid rocket-and-jet-engine fighter design has ever been placed into service. The only operational implementation of mixed propulsion was Rocket-Assisted Take Off (RATO), a system rarely used in fighters.
Jet-powered fighters.
It has become common in the aviation community to classify jet fighters by "generations" for historical purposes. There are no official definitions of these generations; rather, they represent the notion that there are stages in the development of fighter design approaches, performance capabilities, and technological evolution. Also other authors have packed the fighters into different generations. For example, Richard P. Hallion of the Secretary of the Air Force's Action Group classified the F-16 as a sixth generation jet fighter.
The timeframes associated with each generation are inexact and are only indicative of the period during which their design philosophies and technology employment enjoyed a prevailing influence on fighter design and development. These timeframes also encompass the peak period of service entry for such aircraft.
First generation subsonic jet fighters (mid-1940s to mid-1950s).
The first generation of jet fighters comprised the initial, subsonic jet fighter designs introduced late in World War II and in the early post-war period. They differed little from their piston-engined counterparts in appearance, and many employed unswept wings. Guns remained the principal armament. The need to obtain a decisive advantage in maximum speed pushed the development of turbojet-powered aircraft forward. Top speeds for fighters rose steadily throughout World War II as more powerful piston engines were developed, and was approaching transonic flight speeds where the efficiency of propellers drops off, making further speed increases nearly impossible.
The first jets were developed during World War II and saw combat in the last two years of the war. Messerschmitt developed the first operational jet fighter, the Me 262. It was considerably faster than contemporary piston-driven aircraft, and in the hands of a competent pilot, was quite difficult for Allied pilots to defeat. The design was never deployed in numbers sufficient to stop the Allied air campaign, and a combination of fuel shortages, pilot losses, and technical difficulties with the engines kept the number of sorties low. Nevertheless, the Me 262 indicated the obsolescence of piston-driven aircraft. Spurred by reports of the German jets, Britain's Gloster Meteor entered production soon after and the two entered service around the same time in 1944. Meteors were commonly used to intercept the V-1 "buzz bomb", as they were faster than available piston-engined fighters at the low altitudes the flying bombs were flying. By the end of the war almost all work on piston-powered fighters had ended. A few designs combining piston and jet engines for propulsion – such as the Ryan FR Fireball – saw brief use, but by the end of the 1940s virtually all new fighters were jet-powered.
Despite their advantages, the early jet fighters were far from perfect. The operational lifespan of turbines were very short and engines were temperamental, while power could be adjusted only slowly and acceleration was poor (even if top speed was higher) compared to the final generation of piston fighters. Many squadrons of piston-engined fighters were retained until the early to mid-1950s, even in the air forces of the major powers (though the types retained were the best of the World War II designs). Innovations including ejection seats, air brakes and all-moving tailplanes became widespread in this period.
The Americans began using jet fighters operationally post-war, the wartime Bell P-59 having proven itself a failure. The Lockheed P-80 Shooting Star (soon re-designated F-80) was less elegant than the swept-wing Me 262, but had a cruise speed (]) as high as the maximum speed attainable by many piston-engined fighters. The British designed several new jets, including the distinctive twin boom de Havilland Vampire which was sold to the air forces of many nations.
The British transferred the technology of the Rolls-Royce Nene jet engine to the Soviets, who soon put it to use in their advanced Mikoyan-Gurevich MiG-15 fighter, which used fully swept wings that allowed flying closer to the speed of sound than straight-winged designs such as the F-80. Its top speed of proved quite a shock to the American F-80 pilots who encountered them over Korea, along with their armament of two 23 mm cannons and a single 37 mm cannon. Nevertheless, in the first jet-versus-jet dogfight, which occurred during the Korean War on 8 November 1950, an F-80 downed two North Korean MiG-15s.
The Americans responded by rushing their own swept-wing F-86 Sabre into battle against the MiGs, which had similar transsonic performance. The two aircraft had different strengths and weaknesses, but were similar enough that victory could go either way. While the Sabres were focused primarily on downing MiGs and scored favourably against those flown by the poorly trained North Koreans, the MiGs in turn decimated US bomber formations and forced the withdrawal of numerous American types from operational service.
The world's navies also transitioned to jets during this period, despite the need for catapult-launching of the new aircraft. Grumman's F9F Panther was adopted by the U.S. Navy as their primary jet fighter in the Korean War period, and it was one of the first jet fighters to employ an afterburner. The de Havilland Sea Vampire was the Royal Navy's first jet fighter. Radar was used on specialized night fighters such as the F3D Skyknight, which also downed MiGs over Korea, and later fitted to the F2H Banshee and swept wing F7U Cutlass and F3H Demon as all-weather / night fighters. Early versions of Infra-red (IR) air-to-air missiles (AAMs) such as the AIM-9 Sidewinder and radar guided missiles such as the AIM-7 Sparrow whose descendants are still in use, were first introduced on swept wing subsonic Demon and Cutlass naval fighters.
Second generation jet fighters (mid-1950s to early 1960s).
The development of second-generation fighters was shaped by technological breakthroughs, lessons learned from the aerial battles of the Korean War, and a focus on conducting operations in a nuclear warfare environment. Technological advances in aerodynamics, propulsion and aerospace building materials (primarily aluminium alloys) permitted designers to experiment with aeronautical innovations, such as swept wings, delta wings, and area-ruled fuselages. Widespread use of afterburning turbojet engines made these the first production aircraft to break the sound barrier, and the ability to sustain supersonic speeds in level flight became a common capability amongst fighters of this generation.
Fighter designs also took advantage of new electronics technologies that made effective radars small enough to carry aboard smaller aircraft. Onboard radars permitted detection of enemy aircraft beyond visual range, thereby improving the handoff of targets by longer-ranged ground-based warning and tracking radars. Similarly, advances in guided missile development allowed air-to-air missiles to begin supplementing the gun as the primary offensive weapon for the first time in fighter history. During this period, passive-homing infrared-guided (IR) missiles became commonplace, but early IR missile sensors had poor sensitivity and a very narrow field of view (typically no more than 30°), which limited their effective use to only close-range, tail-chase engagements. Radar-guided (RF) missiles were introduced as well, but early examples proved unreliable. These semi-active radar homing (SARH) missiles could track and intercept an enemy aircraft "painted" by the launching aircraft's onboard radar. Medium- and long-range RF air-to-air missiles promised to open up a new dimension of "beyond-visual-range" (BVR) combat, and much effort was placed in further development of this technology.
The prospect of a potential third world war featuring large mechanized armies and nuclear weapon strikes led to a degree of specialization along two design approaches: interceptors, such as the English Electric Lightning and Mikoyan-Gurevich MiG-21F; and fighter-bombers, such as the Republic F-105 Thunderchief and the Sukhoi Su-7B. Dogfighting, per se, was de-emphasized in both cases. The interceptor was an outgrowth of the vision that guided missiles would completely replace guns and combat would take place at beyond visual ranges. As a result, interceptors were designed with a large missile payload and a powerful radar, sacrificing agility in favor of high speed, altitude ceiling and rate of climb. With a primary air defense role, emphasis was placed on the ability to intercept strategic bombers flying at high altitudes. Specialized point-defense interceptors often had limited range and little, if any, ground-attack capabilities. Fighter-bombers could swing, between air superiority and ground-attack roles, and were often designed for a high-speed, low-altitude dash to deliver their ordnance. Television- and IR-guided air-to-surface missiles were introduced to augment traditional gravity bombs, and some were also equipped to deliver a nuclear bomb.
Third generation jet fighters (early 1960s to circa 1970).
The third generation witnessed continued maturation of second-generation innovations, but it is most marked by renewed emphases on maneuverability and traditional ground-attack capabilities. Over the course of the 1960s, increasing combat experience with guided missiles demonstrated that combat would devolve into close-in dogfights. Analog avionics began to appear, replacing older "steam-gauge" cockpit instrumentation. Enhancements to the aerodynamic performance of third-generation fighters included flight control surfaces such as canards, powered slats, and blown flaps. A number of technologies would be tried for Vertical/Short Takeoff and Landing, but thrust vectoring would be successful on the Harrier.
Growth in air combat capability focused on the introduction of improved air-to-air missiles, radar systems, and other avionics. While guns remained standard equipment (early models of F-4 being a notable exception), air-to-air missiles became the primary weapons for air superiority fighters, which employed more sophisticated radars and medium-range RF AAMs to achieve greater "stand-off" ranges, however, kill probabilities proved unexpectedly low for RF missiles due to poor reliability and improved electronic countermeasures (ECM) for spoofing radar seekers. Infrared-homing AAMs saw their fields of view expand to 45°, which strengthened their tactical usability. Nevertheless, the low dogfight loss-exchange ratios experienced by American fighters in the skies over Vietnam led the U.S. Navy to establish its famous "TOPGUN" fighter weapons school, which provided a graduate-level curriculum to train fleet fighter pilots in advanced Air Combat Maneuvering (ACM) and Dissimilar Air Combat Training (DACT) tactics and techniques.
This era also saw an expansion in ground-attack capabilities, principally in guided missiles, and witnessed the introduction of the first truly effective avionics for enhanced ground attack, including terrain-avoidance systems. Air-to-surface missiles (ASM) equipped with electro-optical (E-O) contrast seekers – such as the initial model of the widely used AGM-65 Maverick – became standard weapons, and laser-guided bombs (LGBs) became widespread in effort to improve precision-attack capabilities. Guidance for such precision-guided munitions (PGM) was provided by externally mounted targeting pods, which were introduced in the mid-1960s.
It also led to the development of new automatic-fire weapons, primarily chain-guns that use an electric motor to drive the mechanism of a cannon. This allowed a plane to carry a single multi-barrel weapon (such as the 20 mm Vulcan), and provided greater accuracy and rates of fire. Powerplant reliability increased and jet engines became "smokeless" to make it harder to sight aircraft at long distances.
Dedicated ground-attack aircraft (like the Grumman A-6 Intruder, SEPECAT Jaguar and LTV A-7 Corsair II) offered longer range, more sophisticated night attack systems or lower cost than supersonic fighters. With variable-geometry wings, the supersonic F-111 introduced the Pratt & Whitney TF30, the first turbofan equipped with afterburner. The ambitious project sought to create a versatile common fighter for many roles and services. It would serve well as an all-weather bomber, but lacked the performance to defeat other fighters. The McDonnell F-4 Phantom was designed around radar and missiles as an all-weather interceptor, but emerged as a versatile strike bomber nimble enough to prevail in air combat, adopted by the U.S. Navy, Air Force and Marine Corps. Despite numerous shortcomings that would be not be fully addressed until newer fighters, the Phantom claimed 280 aerial kills, more than any other U.S. fighter over Vietnam. With range and payload capabilities that rivaled that of World War II bombers such as B-24 Liberator, the Phantom would become a highly successful multirole aircraft.
Fourth generation jet fighters (circa 1970 to mid-1990s).
Fourth-generation fighters continued the trend towards multirole configurations, and were equipped with increasingly sophisticated avionics and weapon systems. Fighter designs were significantly influenced by the Energy-Maneuverability (E-M) theory developed by Colonel John Boyd and mathematician Thomas Christie, based upon Boyd's combat experience in the Korean War and as a fighter tactics instructor during the 1960s. E-M theory emphasized the value of aircraft specific energy maintenance as an advantage in fighter combat. Boyd perceived maneuverability as the primary means of getting "inside" an adversary's decision-making cycle, a process Boyd called the "OODA loop" (for "Observation-Orientation-Decision-Action"). This approach emphasized aircraft designs that were capable of performing "fast transients" – quick changes in speed, altitude, and direction – as opposed to relying chiefly on high speed alone.
E-M characteristics were first applied to the McDonnell Douglas F-15 Eagle, but Boyd and his supporters believed these performance parameters called for a small, lightweight aircraft with a larger, higher-lift wing. The small size would minimize drag and increase the thrust-to-weight ratio, while the larger wing would minimize wing loading; while the reduced wing loading tends to lower top speed and can cut range, it increases payload capacity and the range reduction can be compensated for by increased fuel in the larger wing. The efforts of Boyd's "Fighter Mafia" would result in the General Dynamics F-16 Fighting Falcon (now Lockheed Martin's).
The F-16's maneuverability was further enhanced by its slight aerodynamic instability. This technique, called "relaxed static stability" (RSS), was made possible by introduction of the "fly-by-wire" (FBW) flight control system (FLCS), which in turn was enabled by advances in computers and system integration techniques. Analog avionics, required to enable FBW operations, became a fundamental requirement and began to be replaced by digital flight control systems in the latter half of the 1980s. Likewise, Full Authority Digital Engine Controls (FADEC) to electronically manage powerplant performance was introduced with the Pratt & Whitney F100 turbofan. The F-16's sole reliance on electronics and wires to relay flight commands, instead of the usual cables and mechanical linkage controls, earned it the sobriquet of "the electric jet". Electronic FLCS and FADEC quickly became essential components of all subsequent fighter designs.
Other innovative technologies introduced in fourth-generation fighters include pulse-Doppler fire-control radars (providing a "look-down/shoot-down" capability), head-up displays (HUD), "hands on throttle-and-stick" (HOTAS) controls, and multi-function displays (MFD), all now essential equipment. Aircraft designers began to incorporate composite materials in the form of bonded aluminum honeycomb structural elements and graphite epoxy laminate skins to reduce weight. Infrared search-and-track (IRST) sensors became widespread for air-to-ground weapons delivery, and appeared for air-to-air combat as well. "All-aspect" IR AAM became standard air superiority weapons, which permitted engagement of enemy aircraft from any angle (although the field of view remained relatively limited). The first long-range active-radar-homing RF AAM entered service with the AIM-54 Phoenix, which solely equipped the Grumman F-14 Tomcat, one of the few variable-sweep-wing fighter designs to enter production. Even with the tremendous advancement of air-to-air missiles in this era, internal guns were standard equipment.
Another revolution came in the form of a stronger reliance on ease of maintenance, which led to standardisation of parts, reductions in the numbers of access panels and lubrication points, and overall parts reduction in more complicated equipment like the engines. Some early jet fighters required 50 man-hours of work by a ground crew for every hour the aircraft was in the air; later models substantially reduced this to allow faster turn-around times and more sorties in a day. Some modern military aircraft only require 10 man-hours of work per hour of flight time, and others are even more efficient.
Aerodynamic innovations included variable-camber wings and exploitation of the vortex lift effect to achieve higher angles of attack through the addition of leading-edge extension devices such as strakes.
Unlike interceptors of the previous eras, most fourth-generation air-superiority fighters were designed to be agile dogfighters (although the Mikoyan MiG-31 and Panavia Tornado ADV are notable exceptions). The continually rising cost of fighters, however, continued to emphasize the value of multirole fighters. The need for both types of fighters led to the "high/low mix" concept, which envisioned a high-capability and high-cost core of dedicated air-superiority fighters (like the F-15 and Su-27) supplemented by a larger contingent of lower-cost multi-role fighters (such as the F-16 and MiG-29).
Most fourth-generation fighters, such as the McDonnell Douglas F/A-18 Hornet and Dassault Mirage 2000, are true multirole warplanes, designed as such from the start. This was facilitated by multimode avionics that could switch seamlessly between air and ground modes. The earlier approaches of adding on strike capabilities or designing separate models specialized for different roles generally became "passé" (with the Panavia Tornado being an exception in this regard). Attack roles were generally assigned to dedicated ground-attack aircraft such as the Sukhoi Su-25 and the A-10 Thunderbolt II.
A typical US Air Force fighter wing of the period might contain a mix of one air superiority squadron (F-15C), one strike fighter squadron (F-15E), and two multirole fighter squadrons (F-16C).
Perhaps the most novel technology introduced for combat aircraft was "stealth", which involves the use of special "low-observable" (L-O) materials and design techniques to reduce the susceptibility of an aircraft to detection by the enemy's sensor systems, particularly radars. The first stealth aircraft introduced were the Lockheed F-117 Nighthawk attack aircraft (introduced in 1983) and the Northrop Grumman B-2 Spirit bomber (which first flew in 1989). Although no stealthy fighters per se appeared among the fourth generation, some radar-absorbent coatings and other L-O treatments developed for these programs are reported to have been subsequently applied to fourth-generation fighters.
4.5th generation jet fighters (1990s to 2005).
The end of the Cold War in 1991 led many governments to significantly decrease military spending as a "peace dividend". Air force inventories were cut. Research and development programs working on "fifth-generation" fighters took serious hits. Many programs were canceled during the first half of the 1990s, and those that survived were "stretched out". While the practice of slowing the pace of development reduces annual investment expenses, it comes at the penalty of increased overall program and unit costs over the long-term. In this instance, however, it also permitted designers to make use of the tremendous achievements being made in the fields of computers, avionics and other flight electronics, which had become possible largely due to the advances made in microchip and semiconductor technologies in the 1980s and 1990s. This opportunity enabled designers to develop fourth-generation designs – or redesigns – with significantly enhanced capabilities. These improved designs have become known as "Generation 4.5" fighters, recognizing their intermediate nature between the 4th and 5th generations, and their contribution in furthering development of individual fifth-generation technologies.
The primary characteristics of this sub-generation are the application of advanced digital avionics and aerospace materials, modest signature reduction (primarily RF "stealth"), and highly integrated systems and weapons. These fighters have been designed to operate in a "network-centric" battlefield environment and are principally multirole aircraft. Key weapons technologies introduced include beyond-visual-range (BVR) AAMs; Global Positioning System (GPS)-guided weapons, solid-state phased-array radars; helmet-mounted sights; and improved secure, jamming-resistant datalinks. Thrust vectoring to further improve transient maneuvering capabilities has also been adopted by many 4.5th generation fighters, and uprated powerplants have enabled some designs to achieve a degree of "supercruise" ability. Stealth characteristics are focused primarily on frontal-aspect radar cross section (RCS) signature-reduction techniques including radar-absorbent materials (RAM), L-O coatings and limited shaping techniques.
"Half-generation" designs are either based on existing airframes or are based on new airframes following similar design theory as previous iterations; however, these modifications have introduced the structural use of composite materials to reduce weight, greater fuel fractions to increase range, and signature reduction treatments to achieve lower RCS compared to their predecessors. Prime examples of such aircraft, which are based on new airframe designs making extensive use of carbon-fibre composites, include the Eurofighter Typhoon, Dassault Rafale, and Saab JAS 39 Gripen.
Apart from these fighter jets, most of the 4.5 generation aircraft are actually modified variants of existing airframes from the earlier fourth generation fighter jets. Such fighter jets are generally heavier and examples include the Boeing F/A-18E/F Super Hornet, which is an evolution of the 1970s F/A-18 Hornet design, the F-15E Strike Eagle, which is a ground-attack/multi-role variant of the F-15 Eagle, the Su-30MKI and Su-30MKK variants of the Sukhoi Su-30 and the MiG-29M, MiG-29K and MiG-35, upgraded versions of the Mikoyan MiG-29. The Su-30MKI and MiG-35 feature thrust vectoring engine nozzles to enhance maneuvering.
4.5 generation fighters first entered service in the early 1990s, and most of them are still being produced and evolved. It is quite possible that they may continue in production alongside fifth-generation fighters due to the expense of developing the advanced level of stealth technology needed to achieve aircraft designs featuring very low observables (VLO), which is one of the defining features of fifth-generation fighters. Of the 4.5th generation designs, the Strike Eagle, Super Hornet, Typhoon, Gripen, and Rafale have been used in combat.
The U.S. government has defined 4.5 generation fighter aircraft as those that "(1) have advanced capabilities, including— (A) AESA radar; (B) high capacity data-link; and (C) enhanced avionics; and (2) have the ability to deploy current and reasonably foreseeable advanced armaments."
Fifth generation jet fighters (2005 to the present).
The fifth generation was ushered in by the Lockheed Martin/Boeing F-22 Raptor in late 2005. Currently the cutting edge of fighter design, fifth-generation fighters are characterized by being designed from the start to operate in a network-centric combat environment, and to feature extremely low, all-aspect, multi-spectral signatures employing advanced materials and shaping techniques. They have multifunction AESA radars with high-bandwidth, low-probability of intercept (LPI) data transmission capabilities. The Infra-red search and track sensors incorporated for air-to-air combat as well as for air-to-ground weapons delivery in the 4.5th generation fighters are now fused in with other sensors for Situational Awareness IRST or SAIRST, which constantly tracks all targets of interest around the aircraft so the pilot need not guess when he glances. These sensors, along with advanced avionics, glass cockpits, helmet-mounted sights (not currently on F-22), and improved secure, jamming-resistant LPI datalinks are highly integrated to provide multi-platform, multi-sensor data fusion for vastly improved situational awareness while easing the pilot's workload. Avionics suites rely on extensive use of very high-speed integrated circuit (VHSIC) technology, common modules, and high-speed data buses. Overall, the integration of all these elements is claimed to provide fifth-generation fighters with a "first-look, first-shot, first-kill capability".
The AESA radar offers unique capabilities for fighters (and it is also quickly becoming essential for Generation 4.5 aircraft designs, as well as being retrofitted onto some fourth-generation aircraft). In addition to its high resistance to ECM and LPI features, it enables the fighter to function as a sort of "mini-AWACS," providing high-gain electronic support measures (ESM) and electronic warfare (EW) jamming functions.
Other technologies common to this latest generation of fighters includes integrated electronic warfare system (INEWS) technology, integrated communications, navigation, and identification (CNI) avionics technology, centralized "vehicle health monitoring" systems for ease of maintenance, fiber optics data transmission, stealth technology and even hovering capabilities. Maneuver performance remains important and is enhanced by thrust-vectoring, which also helps reduce takeoff and landing distances. Supercruise may or may not be featured; it permits flight at supersonic speeds without the use of the afterburner – a device that significantly increases IR signature when used in full military power.
A key attribute of fifth-generation fighters is a small radar cross-section. Great care has been taken in designing its layout and internal structure to minimize RCS over a broad bandwidth of detection and tracking radar frequencies; furthermore, to maintain its VLO signature during combat operations, primary weapons are carried in internal weapon bays that are only briefly opened to permit weapon launch. Furthermore, stealth technology has advanced to the point where it can be employed without a tradeoff with aerodynamics performance, in contrast to previous stealth efforts. Some attention has also been paid to reducing IR signatures, especially on the F-22. Detailed information on these signature-reduction techniques is classified, but in general includes special shaping approaches, thermoset and thermoplastic materials, extensive structural use of advanced composites, conformal sensors, heat-resistant coatings, low-observable wire meshes to cover intake and cooling vents, heat ablating tiles on the exhaust troughs (seen on the Northrop YF-23), and coating internal and external metal areas with radar-absorbent materials and paint (RAM/RAP).
Such aircraft are sophisticated and expensive. The U.S. Air Force originally planned to acquire 650 F-22s, but now only 187 will be built. As a result, its unit flyaway cost (FAC) is around US$150 million. To spread the development costs – and production base – more broadly, the Joint Strike Fighter (JSF) program enrolls eight other countries as cost- and risk-sharing partners. Altogether, the nine partner nations anticipate procuring over 3,000 Lockheed Martin F-35 Lightning II fighters at an anticipated average FAC of $80–85 million. The F-35, however, is designed to be a family of three aircraft, a conventional take-off and landing (CTOL) fighter, a short take-off and vertical landing (STOVL) fighter, and a Catapult Assisted Take Off But Arrested Recovery (CATOBAR) fighter, each of which has a different unit price and slightly varying specifications in terms of fuel capacity (and therefore range), size and payload.
Other countries have initiated fifth-generation fighter development projects, with Russia's Sukhoi PAK FA and Mikoyan LMFS. In October 2007, Russia and India signed an agreement for joint participation in a Fifth-Generation Fighter Aircraft Program (FGFA), which gives India responsibility for development of a two-seat model of the PAK-FA. India is also developing the Advanced Medium Combat Aircraft (AMCA). In December 2010, it was discovered that China is developing the 5th generation fighter Chengdu J-20. The J-20 took its maiden flight in January 2011 and is planned to be deployed in 2017–19 time frame. The Shenyang J-31 took its maiden flight on 31 October 2012. Japan is exploring its technical feasibility to produce fifth-generation fighters.
Sixth generation jet fighters.
A sixth generation jet fighter is a conceptual airplane expected to enter service in the United States Air Force and United States Navy in 2025–30 timeframe. The USAF seeks a new fighter for the 2030–50 period named the "Next Generation Tactical Aircraft"/"Next Gen TACAIR" The US Navy looks to replace its F/A-18E/F Super Hornets beginning in 2025 with the Next Generation Air Dominance air superiority fighter.

</doc>
<doc id="10930" url="https://en.wikipedia.org/wiki?curid=10930" title="February 25">
February 25


</doc>
<doc id="10931" url="https://en.wikipedia.org/wiki?curid=10931" title="Finite-state machine">
Finite-state machine

A finite-state machine (FSM) or finite-state automaton (plural: "automata"), or simply a state machine, is a mathematical model of computation used to design both computer programs and sequential logic circuits. It is conceived as an abstract machine that can be in one of a finite number of "states". The machine is in only one state at a time; the state it is in at any given time is called the "current state". It can change from one state to another when initiated by a triggering event or condition; this is called a "transition". A particular FSM is defined by a list of its states, and the triggering condition for each transition.
The behavior of state machines can be observed in many devices in modern society that perform a predetermined sequence of actions depending on a sequence of events with which they are presented. Simple examples are vending machines, which dispense products when the proper combination of coins is deposited, elevators, which drop riders off at upper floors before going down, traffic lights, which change sequence when cars are waiting, and combination locks, which require the input of combination numbers in the proper order.
Finite-state machines can model a large number of problems, among which are electronic design automation, communication protocol design, language parsing and other engineering applications. In biology and artificial intelligence research, state machines or hierarchies of state machines have been used to describe neurological systems. In linguistics, they are used to describe simple parts of the grammars of natural languages.
Considered as an abstract model of computation, the finite state machine is weak; it has less computational power than some other models of computation such as the Turing machine. That is, there are tasks that no FSM can do, but some Turing machines can. This is because the FSM memory is limited by the number of states.
FSMs are studied in the more general field of automata theory.
Example: coin-operated turnstile.
An example of a very simple mechanism that can be modeled by a state machine is a turnstile. A turnstile, used to control access to subways and amusement park rides, is a gate with three rotating arms at waist height, one across the entryway. Initially the arms are locked, blocking the entry, preventing patrons from passing through. Depositing a coin or token in a slot on the turnstile unlocks the arms, allowing a single customer to push through. After the customer passes through, the arms are locked again until another coin is inserted.
Considered as a state machine, the turnstile has two states: Locked and Unlocked. There are two inputs that affect its state: putting a coin in the slot (coin) and pushing the arm (push). In the locked state, pushing on the arm has no effect; no matter how many times the input push is given, it stays in the locked state. Putting a coin in – that is, giving the machine a coin input – shifts the state from Locked to Unlocked. In the unlocked state, putting additional coins in has no effect; that is, giving additional coin inputs does not change the state. However, a customer pushing through the arms, giving a push input, shifts the state back to Locked.
The turnstile state machine can be represented by a state transition table, showing for each state the new state and the output (action) resulting from each input
It can also be represented by a directed graph called a state diagram "(above)". Each of the states is represented by a node ("circle"). Edges ("arrows") show the transitions from one state to another. Each arrow is labeled with the input that triggers that transition. Inputs that don't cause a change of state (such as a coin input in the Unlocked state) are represented by a circular arrow returning to the original state. The arrow into the Locked node from the black dot indicates it is the initial state.
Concepts and terminology.
A "state" is a description of the status of a system that is waiting to execute a "transition". A transition is a set of actions to be executed when a condition is fulfilled or when an event is received.
For example, when using an audio system to listen to the radio (the system is in the "radio" state), receiving a "next" stimulus results in moving to the next station. When the system is in the "CD" state, the "next" stimulus results in moving to the next track. Identical stimuli trigger different actions depending on the current state.
In some finite-state machine representations, it is also possible to associate actions with a state:
Representations.
State/Event table.
Several state transition table types are used. The most common representation is shown below: the combination of current state (e.g. B) and input (e.g. Y) shows the next state (e.g. C). The complete action's information is not directly described in the table and can only be added using footnotes. A FSM definition including the full actions information is possible using state tables (see also virtual finite-state machine).
UML state machines.
The Unified Modeling Language has a notation for describing state machines. UML state machines overcome the limitations of traditional finite state machines while retaining their main benefits. UML state machines introduce the new concepts of hierarchically nested states and orthogonal regions, while extending the notion of actions. UML state machines have the characteristics of both Mealy machines and Moore machines. They support actions that depend on both the state of the system and the triggering event, as in Mealy machines, as well as entry and exit actions, which are associated with states rather than transitions, as in Moore machines.
SDL state machines.
The Specification and Description Language is a standard from ITU that includes graphical symbols to describe actions in the transition:
SDL embeds basic data types called Abstract Data Types, an action language, and an execution semantic in order to make the finite state machine executable.
Other state diagrams.
There are a large number of variants to represent an FSM such as the one in figure 3.
Usage.
In addition to their use in modeling reactive systems presented here, finite state automata are significant in many different areas, including electrical engineering, linguistics, computer science, philosophy, biology, mathematics, and logic. Finite state machines are a class of automata studied in automata theory and the theory of computation.
In computer science, finite state machines are widely used in modeling of application behavior, design of hardware digital systems, software engineering, compilers, network protocols, and the study of computation and languages.
Classification.
The state machines can be subdivided into Transducers, Acceptors, Classifiers and Sequencers.
Acceptors and recognizers.
Acceptors (also called recognizers and sequence detectors) produce binary output, indicating whether or not received input is accepted. Each state of an FSM is either "accepting" or "not accepting". Once all input has been received, if the current state is an accepting state, the input is accepted; otherwise it is rejected. As a rule, input is a series of symbols (characters); actions are not used. The example in figure 4 shows a finite state machine that accepts the string "nice". In this FSM, the only accepting state is state 7.
A machine could also be described as defining a language, that would contain every string accepted by the machine but none of the rejected ones; that language is "accepted" by the machine. By definition, the languages accepted by FSMs are the regular languages—; a language is regular if there is some FSM that accepts it.
The problem of determining the language accepted by a given FSA is an instance of the algebraic path problem—itself a generalization of the shortest path problem to graphs with edges weighted by the elements of an (arbitrary) semiring.
Start state.
The start state is usually shown drawn with an arrow "pointing at it from any where" (Sipser (2006) p. 34).
Accept (or final) states.
Accept states (also referred to as accepting or final states) are those at which the machine reports that the input string, as processed so far, is a member of the language it accepts. Accepting states are usually represented by double circles.
The start state can also be an accepting state, in which case the automaton accepts the empty string. If the start state is not an accepting state and there are no connecting edges to any of the accepting states, then the automaton is accepting nothing.
An example of an accepting state appears in Fig.5: a deterministic finite automaton (DFA) that detects whether the binary input string contains an even number of 0s.
"S" (which is also the start state) indicates the state at which an even number of 0s has been input. S is therefore an accepting state. This machine will finish in an accept state, if the binary string contains an even number of 0s (including any binary string containing no 0s). Examples of strings accepted by this DFA are ε (the empty string), 1, 11, 11…, 00, 010, 1010, 10110, etc…
Classifier is a generalization that, similar to acceptor, produces single output when terminates but has more than two terminal states.
Transducers.
Transducers generate output based on a given input and/or a state using actions. They are used for control applications and in the field of computational linguistics.
In control applications, two types are distinguished:
Generators.
The sequencers or generators are a subclass of aforementioned types that have a single-letter input alphabet. They produce only one sequence, which can be interpreted as output sequence of transducer or classifier outputs.
Determinism.
A further distinction is between deterministic (DFA) and non-deterministic (NFA, GNFA) automata. In deterministic automata, every state has exactly one transition for each possible input. In non-deterministic automata, an input can lead to one, more than one or no transition for a given state. This distinction is relevant in practice, but not in theory, as there exists an algorithm (the powerset construction) that can transform any NFA into a more complex DFA with identical functionality.
The FSM with only one state is called a combinatorial FSM and uses only input actions. This concept is useful in cases where a number of FSM are required to work together, and where it is convenient to consider a purely combinatorial part as a form of FSM to suit the design tools.
Alternative semantics.
There are other sets of semantics available to represent state machines. For example, there are tools for modeling and designing logic for embedded controllers. They combine hierarchical state machines, flow graphs, and truth tables into one language, resulting in a different formalism and set of semantics. Figure 8 illustrates this mix of state machines and flow graphs with a set of states to represent the state of a stopwatch and a flow graph to control the ticks of the watch. These charts, like Harel's original state machines, support hierarchically nested states, orthogonal regions, state actions, and transition actions.
FSM logic.
The next state and output of an FSM is a function of the input and of the current state. The FSM logic is shown in Figure 8.
Mathematical model.
In accordance with the general classification, the following formal definitions are found:
For both deterministic and non-deterministic FSMs, it is conventional to allow formula_6 to be a partial function, i.e. formula_13 does not have to be defined for every combination of formula_14 and formula_15. If an FSM formula_16 is in a state formula_17, the next symbol is formula_18 and formula_13 is not defined, then formula_16 can announce an error (i.e. reject the input). This is useful in definitions of general state machines, but less useful when transforming the machine. Some algorithms in their default form may require total functions.
A finite-state machine is a restricted Turing machine where the head can only perform "read" operations, and always moves from left to right.
If the output function is a function of a state and input alphabet (formula_31) that definition corresponds to the Mealy model, and can be modelled as a Mealy machine. If the output function depends only on a state (formula_32) that definition corresponds to the Moore model, and can be modelled as a Moore machine. A finite-state machine with no output function at all is known as a semiautomaton or transition system.
If we disregard the first output symbol of a Moore machine, formula_33, then it can be readily converted to an output-equivalent Mealy machine by setting the output function of every Mealy transition (i.e. labeling every edge) with the output symbol given of the destination Moore state. The converse transformation is less straightforward because a Mealy machine state may have different output labels on its incoming transitions (edges). Every such state needs to be split in multiple Moore machine states, one for every incident output symbol.
Optimization.
Optimizing an FSM means finding the machine with the minimum number of states that performs the same function. The fastest known algorithm doing this is the Hopcroft minimization algorithm. Other techniques include using an implication table, or the Moore reduction procedure. Additionally, acyclic FSAs can be minimized in linear time.
Implementation.
Hardware applications.
In a digital circuit, an FSM may be built using a programmable logic device, a programmable logic controller, logic gates and flip flops or relays. More specifically, a hardware implementation requires a register to store state variables, a block of combinational logic that determines the state transition, and a second block of combinational logic that determines the output of an FSM. One of the classic hardware implementations is the Richards controller.
A particular case of Moore FSM, when output is directly connected to the state flip-flops, that is when output function is simple identity, is known as Medvedev FSM. It is advised in chip design that no logic is placed between primary I/O and registers to minimize interchip delays, which are usually long and limit the FSM frequencies.
Through state encoding for low power state machines may be optimized to minimize power consumption.
Software applications.
The following concepts are commonly used to build software applications with finite state machines:
Finite automata and compilers.
Finite automata are often used in the frontend of programming language compilers. Such a frontend may comprise several finite state machines that implement a lexical analyzer and a parser.
Starting from a sequence of characters, the lexical analyzer builds a sequence of language tokens (such as reserved words, literals, and identifiers) from which the parser builds a syntax tree. The lexical analyzer and the parser handle the regular and context-free parts of the programming language's grammar.
Further reading.
Finite Markov chain processes.
Finite Markov-chain processes are also known as subshifts of finite type.

</doc>
<doc id="10933" url="https://en.wikipedia.org/wiki?curid=10933" title="Functional programming">
Functional programming

In computer science, functional programming is a programming paradigm—a style of building the structure and elements of computer programs—that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data. It is a declarative programming paradigm, which means programming is done with expressions. In functional code, the output value of a function depends only on the arguments that are input to the function, so calling a function "f" twice with the same value for an argument "x" will produce the same result "f(x)" each time. Eliminating side effects, i.e. changes in state that do not depend on the function inputs, can make it much easier to understand and predict the behavior of a program, which is one of the key motivations for the development of functional programming.
Functional programming has its roots in lambda calculus, a formal system developed in the 1930s to investigate computability, the Entscheidungsproblem, function definition, function application, and recursion. Many functional programming languages can be viewed as elaborations on the lambda calculus. Another well-known declarative programming paradigm, "logic programming", is based on relations.
In contrast, imperative programming changes state with commands in the source language, the most simple example being assignment. Imperative programming does have functions—not in the mathematical sense—but in the sense of subroutines. They can have side effects that may change the value of program state. Functions without return values therefore make sense. Because of this, they lack referential transparency, i.e. the same language expression can result in different values at different times depending on the state of the executing program.
Functional programming languages, especially purely functional ones such as Hope and Rex, have largely been emphasized in academia rather than in commercial software development. However, prominent programming languages which support functional programming such as Common Lisp, Scheme, Clojure, Wolfram Language (also known as Mathematica), Racket, Erlang, OCaml, Haskell, and F# have been used in industrial and commercial applications by a wide variety of organizations. Functional programming is also supported in some domain-specific programming languages like R (statistics), J, K and Q from Kx Systems (financial analysis), XQuery/XSLT (XML), and Opal. Widespread domain-specific declarative languages like SQL and Lex/Yacc use some elements of functional programming, especially in eschewing mutable values.
Programming in a functional style can also be accomplished in languages that are not specifically designed for functional programming. For example, the imperative Perl programming language has been the subject of a book describing how to apply functional programming concepts. This is also true of the PHP programming language. C# 3.0 and Java 8 added constructs to facilitate the functional style. The Julia language also offers functional programming abilities. An interesting case is that of Scala – it is frequently written in a functional style, but the presence of side effects and mutable state place it in a grey area between imperative and functional languages.
History.
Lambda calculus provides a theoretical framework for describing functions and their evaluation. Although it is a mathematical abstraction rather than a programming language, it forms the basis of almost all functional programming languages today. An equivalent theoretical formulation, combinatory logic, is commonly perceived as more abstract than lambda calculus and preceded it in invention. Combinatory logic and lambda calculus were both originally developed to achieve a clearer approach to the foundations of mathematics.
An early functional-flavored language was Lisp, developed by John McCarthy while at Massachusetts Institute of Technology (MIT) for the IBM 700/7000 series scientific computers in the late 1950s. Lisp introduced many features now found in functional languages, though Lisp is technically a multi-paradigm language. Scheme and Dylan were later attempts to simplify and improve Lisp.
Information Processing Language (IPL) is sometimes cited as the first computer-based functional programming language. It is an assembly-style language for manipulating lists of symbols. It does have a notion of "generator", which amounts to a function accepting a function as an argument, and, since it is an assembly-level language, code can be used as data, so IPL can be regarded as having higher-order functions. However, it relies heavily on mutating list structure and similar imperative features.
Kenneth E. Iverson developed APL in the early 1960s, described in his 1962 book "A Programming Language" (ISBN 9780471430148). APL was the primary influence on John Backus's FP. In the early 1990s, Iverson and Roger Hui created J. In the mid-1990s, Arthur Whitney, who had previously worked with Iverson, created K, which is used commercially in financial industries along with its descendant Q.
John Backus presented FP in his 1977 Turing Award lecture "Can Programming Be Liberated From the von Neumann Style? A Functional Style and its Algebra of Programs". He defines functional programs as being built up in a hierarchical way by means of "combining forms" that allow an "algebra of programs"; in modern language, this means that functional programs follow the principle of compositionality. Backus's paper popularized research into functional programming, though it emphasized function-level programming rather than the lambda-calculus style which has come to be associated with functional programming.
In the 1970s, ML was created by Robin Milner at the University of Edinburgh, and David Turner initially developed the language SASL at the University of St. Andrews and later the language Miranda at the University of Kent. Also in Edinburgh in the 1970s, Burstall and Darlington developed the functional language NPL. NPL was based on Kleene Recursion Equations and was first introduced in their work on program transformation. Burstall, MacQueen and Sannella then incorporated the polymorphic type checking from ML to produce the language Hope. ML eventually developed into several dialects, the most common of which are now OCaml and Standard ML. Meanwhile, the development of Scheme (a partly functional dialect of Lisp), as described in the influential Lambda Papers and the 1985 textbook "Structure and Interpretation of Computer Programs", brought awareness of the power of functional programming to the wider programming-languages community.
In the 1980s, Per Martin-Löf developed intuitionistic type theory (also called "constructive" type theory), which associated functional programs with constructive proofs of arbitrarily complex mathematical propositions expressed as dependent types. This led to powerful new approaches to interactive theorem proving and has influenced the development of many subsequent functional programming languages.
The Haskell language began with a consensus in 1987 to form an open standard for functional programming research; implementation releases have been ongoing since 1990.
Concepts.
A number of concepts and paradigms are specific to functional programming, and generally foreign to imperative programming (including object-oriented programming). However, programming languages are often hybrids of several programming paradigms, so programmers using "mostly imperative" languages may have utilized some of these concepts.
First-class and higher-order functions.
Higher-order functions are functions that can either take other functions as arguments or return them as results. In calculus, an example of a higher-order function is the differential operator formula_1, which returns the derivative of a function formula_2.
Higher-order functions are closely related to first-class functions in that higher-order functions and first-class functions both allow functions as arguments and results of other functions. The distinction between the two is subtle: "higher-order" describes a mathematical concept of functions that operate on other functions, while "first-class" is a computer science term that describes programming language entities that have no restriction on their use (thus first-class functions can appear anywhere in the program that other first-class entities like numbers can, including as arguments to other functions and as their return values).
Higher-order functions enable partial application or currying, a technique in which a function is applied to its arguments one at a time, with each application returning a new function that accepts the next argument. This allows one to succinctly express, for example, the successor function as the addition operator partially applied to the natural number one.
Pure functions.
Purely functional functions (or expressions) have no side effects (memory or I/O). This means that pure functions have several useful properties, many of which can be used to optimize the code:
While most compilers for imperative programming languages detect pure functions and perform common-subexpression elimination for pure function calls, they cannot always do this for pre-compiled libraries, which generally do not expose this information, thus preventing optimizations that involve those external functions. Some compilers, such as gcc, add extra keywords for a programmer to explicitly mark external functions as pure, to enable such optimizations. Fortran 95 also allows functions to be designated "pure".
Recursion.
Iteration (looping) in functional languages is usually accomplished via recursion. Recursive functions invoke themselves, allowing an operation to be performed over and over until the base case is reached. Though some recursion requires maintaining a stack, tail recursion can be recognized and optimized by a compiler into the same code used to implement iteration in imperative languages. The Scheme language standard requires implementations to recognize and optimize tail recursion. Tail recursion optimization can be implemented by transforming the program into continuation passing style during compiling, among other approaches.
Common patterns of recursion can be factored out using higher order functions, with catamorphisms and anamorphisms (or "folds" and "unfolds") being the most obvious examples. Such higher order functions play a role analogous to built-in control structures such as loops in imperative languages.
Most general purpose functional programming languages allow unrestricted recursion and are Turing complete, which makes the halting problem undecidable, can cause unsoundness of equational reasoning, and generally requires the introduction of inconsistency into the logic expressed by the language's type system. Some special purpose languages such as Coq allow only well-founded recursion and are strongly normalizing (nonterminating computations can be expressed only with infinite streams of values called codata). As a consequence, these languages fail to be Turing complete and expressing certain functions in them is impossible, but they can still express a wide class of interesting computations while avoiding the problems introduced by unrestricted recursion. Functional programming limited to well-founded recursion with a few other constraints is called total functional programming.
Strict versus non-strict evaluation.
Functional languages can be categorized by whether they use "strict (eager)" or "non-strict (lazy)" evaluation, concepts that refer to how function arguments are processed when an expression is being evaluated. The technical difference is in the denotational semantics of expressions containing failing or divergent computations. Under strict evaluation, the evaluation of any term containing a failing subterm will itself fail. For example, the expression:
will fail under strict evaluation because of the division by zero in the third element of the list. Under lazy evaluation, the length function will return the value 4 (i.e., the number of items in the list), since evaluating it will not attempt to evaluate the terms making up the list. In brief, strict evaluation always fully evaluates function arguments before invoking the function. Lazy evaluation does not evaluate function arguments unless their values are required to evaluate the function call itself.
The usual implementation strategy for lazy evaluation in functional languages is graph reduction. Lazy evaluation is used by default in several pure functional languages, including Miranda, Clean, and Haskell.
Type systems.
Especially since the development of Hindley–Milner type inference in the 1970s, functional programming languages have tended to use typed lambda calculus, as opposed to the untyped lambda calculus used in Lisp and its variants (such as Scheme). The use of algebraic datatypes and pattern matching makes manipulation of complex data structures convenient and expressive; the presence of strong compile-time type checking makes programs more reliable, while type inference frees the programmer from the need to manually declare types to the compiler.
Some research-oriented functional languages such as Coq, Agda, Cayenne, and Epigram are based on intuitionistic type theory, which allows types to depend on terms. Such types are called dependent types. These type systems do not have decidable type inference and are difficult to understand and program with. But dependent types can express arbitrary propositions in predicate logic. Through the Curry–Howard isomorphism, then, well-typed programs in these languages become a means of writing formal mathematical proofs from which a compiler can generate certified code. While these languages are mainly of interest in academic research (including in formalized mathematics), they have begun to be used in engineering as well. Compcert is a compiler for a subset of the C programming language that is written in Coq and formally verified.
A limited form of dependent types called generalized algebraic data types (GADT's) can be implemented in a way that provides some of the benefits of dependently typed programming while avoiding most of its inconvenience. GADT's are available in the Glasgow Haskell Compiler, in OCaml (since version 4.00) and in Scala (as "case classes"), and have been proposed as additions to other languages including Java and C#.
Referential Transparency.
Functional programs do not have assignment statements, that is, the value of a variable in a functional program never changes once defined. This eliminates any chances of side effects because any variable can be replaced with its actual value at any point of execution. So, functional programs are referentially transparent.
Consider C assignment statement codice_1, this changes the value assigned to the variable codice_2. Let us say that the initial value of codice_2 was codice_4, then two consecutive evaluations of the variable codice_2 will yield codice_4 and codice_7 respectively. Clearly, replacing codice_1 with either codice_4 or codice_7 gives a program with different meaning, and so the expression "is not" referentially transparent. In fact, assignment statements are never referentially transparent.
Now, consider another function such as codice_11 "is" transparent, as it will not implicitly change the input x and thus has no such side effects.
Functional programs exclusively use this type of function and are therefore referentially transparent.
Functional programming in non-functional languages.
It is possible to use a functional style of programming in languages that are not traditionally considered functional languages. For example, both D and Fortran 95 explicitly support pure functions.
JavaScript, Lua and Python had first class functions from their inception. Amrit Prem added support to Python for "lambda", "map", "reduce", and "filter" in 1994, as well as closures in Python 2.2, though Python 3 relegated "reduce" to the codice_12 standard library module. First-class functions have been introduced into other mainstream languages such as PHP 5.3, Visual Basic 9, C# 3.0, and C++11.
In Java, anonymous classes can sometimes be used to simulate closures; however, anonymous classes are not always proper replacements to closures because they have more limited capabilities. Java 8 supports lambda expressions as a replacement for some anonymous classes. However, the presence of checked exceptions in Java can make functional programming inconvenient, because it can be necessary to catch checked exceptions and then rethrow them—a problem that does not occur in other JVM languages that do not have checked exceptions, such as Scala.
In C#, anonymous classes are not necessary, because closures and lambdas are fully supported. Libraries and language extensions for immutable data structures are being developed to aid programming in the functional style in C#.
Many object-oriented design patterns are expressible in functional programming terms: for example, the strategy pattern simply dictates use of a higher-order function, and the visitor pattern roughly corresponds to a catamorphism, or fold.
Similarly, the idea of immutable data from functional programming is often included in imperative programming languages, for example the tuple in Python, which is an immutable array.
Comparison to imperative programming.
Functional programming is very different from imperative programming. The most significant differences stem from the fact that functional programming avoids side effects, which are used in imperative programming to implement state and I/O. Pure functional programming completely prevents side-effects and provides referential transparency.
Higher-order functions are rarely used in older imperative programming. A traditional imperative program might use a loop to traverse and modify a list. A functional program, on the other hand, would probably use a higher-order “map” function that takes a function and a list, generating and returning a new list by applying the function to each list item.
Simulating state.
There are tasks (for example, maintaining a bank account balance) that often seem most naturally implemented with state. Pure functional programming performs these tasks, and I/O tasks such as accepting user input and printing to the screen, in a different way.
The pure functional programming language Haskell implements them using monads, derived from category theory. Monads offer a way to abstract certain types of computational patterns, including (but not limited to) modeling of computations with mutable state (and other side effects such as I/O) in an imperative manner without losing purity. While existing monads may be easy to apply in a program, given appropriate templates and examples, many students find them difficult to understand conceptually, e.g., when asked to define new monads (which is sometimes needed for certain types of libraries).
Another way in which functional languages can simulate state is by passing around a data structure that represents the current state as a parameter to function calls. On each function call, a copy of this data structure is created with whatever differences are the result of the function. This is referred to as 'state-passing style'.
Impure functional languages usually include a more direct method of managing mutable state. Clojure, for example, uses managed references that can be updated by applying pure functions to the current state. This kind of approach enables mutability while still promoting the use of pure functions as the preferred way to express computations.
Alternative methods such as Hoare logic and uniqueness have been developed to track side effects in programs. Some modern research languages use effect systems to make the presence of side effects explicit.
Efficiency issues.
Functional programming languages are typically less efficient in their use of CPU and memory than imperative languages such as C and Pascal. This is related to the fact that some mutable data structures like arrays have a very straightforward implementation using present hardware (which is a highly evolved Turing machine). Flat arrays may be accessed very efficiently with deeply pipelined CPUs, prefetched efficiently through caches (with no complex pointer-chasing), or handled with SIMD instructions. It is also not easy to create their equally efficient general-purpose immutable counterparts. For purely functional languages, the worst-case slowdown is logarithmic in the number of memory cells used, because mutable memory can be represented by a purely functional data structure with logarithmic access time (such as a balanced tree). However, such slowdowns are not universal. For programs that perform intensive numerical computations, functional languages such as OCaml and Clean are only slightly slower than C. For programs that handle large matrices and multidimensional databases, array functional languages (such as J and K) were designed with speed optimizations.
Immutability of data can in many cases lead to execution efficiency by allowing the compiler to make assumptions that are unsafe in an imperative language, thus increasing opportunities for inline expansion.
Lazy evaluation may also speed up the program, even asymptotically, whereas it may slow it down at most by a constant factor (however, it may introduce memory leaks if used improperly). Launchbury 1993 discusses theoretical issues related to memory leaks from lazy evaluation, and O'Sullivan "et al." 2008 give some practical advice for analyzing and fixing them.
However, the most general implementations of lazy evaluation making extensive use of dereferenced code and data perform poorly on modern processors with deep pipelines and multi-level caches (where a cache miss may cost hundreds of cycles) .
Coding styles.
Imperative programs tend to emphasize the series of steps taken by a program in carrying out an action, while functional programs tend to emphasize the composition and arrangement of functions, often without specifying explicit "steps". A simple example illustrates this with two solutions to the same programming goal (calculating Fibonacci numbers).
Python.
The imperative example is in Python.
Version 1 – With Generators
Version 2 – Iterative
Version 3 – Recursive
Haskell.
A functional version (in Haskell) has a different feel to it:
Or, more concisely:
The imperative style describes the intermediate steps involved in calculating codice_13, and places those steps inside a loop statement. In contrast, the functional implementation shown here states the mathematical recurrence relation that defines the entire Fibonacci sequence, then selects an element from the sequence (see also recursion). This example relies on Haskell's lazy evaluation to create an "infinite" list of which only as much as needed (the first 10 elements in this case) will actually be computed. That computation happens when the runtime system carries out the action described by "main".
Perl 6.
As influenced by Haskell and others, Perl 6 has several functional and declarative approaches to problems. For example, you can declaratively build up a well-typed recursive version (the type constraints are optional) through signature pattern matching:
An alternative to this is to construct a lazy iterative sequence, which appears as an almost direct illustration of the sequence:
Erlang.
The same program in Erlang provides a simple example of how functional languages in general do not require their syntax to contain an "if" statement.
This program is contained within a module called "fibonacci" and declares that the start/1 function will be visible from outside the scope of this module.
The function start/1 accepts a single parameter (as denoted by the "/1" syntax) and then calls an internal function called do_fib/3.
In direct contrast to the imperative coding style, Erlang does not need an "if" statement because the Erlang runtime will examine the parameters being passed to a function, and call the first function having a signature that matches the current pattern of parameters. (Erlang syntax does provide an "if" statement, but it is considered syntactic sugar and, compared to its usage in imperative languages, plays only a minor role in application logic design).
In this case, it is unnecessary to test for a parameter value within the body of the function because such a test is implicitly performed by providing a set of function signatures that describe the different patterns of values that could be received by a function.
In the case above, the first version of do_fib/3 will only be called when the third parameter has the precise value of 1. In all other cases, the second version of do_fib/3 will be called.
This example demonstrates that functional programming languages often implement conditional logic "implicitly" by matching parameter patterns rather than "explicitly" by means of an "if" statement.
Elixir.
Elixir is a functional, concurrent, general-purpose programming language that runs on the Erlang virtual machine (BEAM).
The Fibonacci function can be written in Elixir as follows:
defmodule Fibonacci do
end
Lisp.
The Fibonacci function can be written in Common Lisp as follows:
The program can then be called as
D.
D has support for functional programming:
R.
R (programming language) is an environment for statistical computing and graphics. It is also a functional programming language.
The Fibonacci function can be written in R as a recursive function as follows:
Or it can be written as a singly recursive function:
Or it can be written as an iterative function:
The function can then be called as
Use in industry.
Functional programming has long been popular in academia, but with few industrial applications. However, recently several prominent functional programming languages have been used in commercial or industrial systems. For example, the Erlang programming language, which was developed by the Swedish company Ericsson in the late 1980s, was originally used to implement fault-tolerant telecommunications systems. It has since become popular for building a range of applications at companies such as T-Mobile, Nortel, Facebook, Électricité de France and WhatsApp. The Scheme dialect of Lisp was used as the basis for several applications on early Apple Macintosh computers, and has more recently been applied to problems such as training simulation software and telescope control. OCaml, which was introduced in the mid-1990s, has seen commercial use in areas such as financial analysis, driver verification, industrial robot programming, and static analysis of embedded software. Haskell, although initially intended as a research language, has also been applied by a range of companies, in areas such as aerospace systems, hardware design, and web programming.
Other functional programming languages that have seen use in industry include Scala, F#, (both being functional-OO hybrids with support for both purely functional and imperative programming) Wolfram Language, Lisp, Standard ML, and Clojure.
In education.
Functional programming is being used as a method to teach problem solving, algebra and geometric concepts.
It has also been used as a tool to teach classical mechanics in Structure and Interpretation of Classical Mechanics.

</doc>
