<doc id="2397" url="https://en.wikipedia.org/wiki?curid=2397" title="Anthony Hopkins">
Anthony Hopkins

Sir Philip Anthony Hopkins, CBE (born 31 December 1937), is a Welsh actor of film, stage, and television, and a composer and painter. After graduating from the Royal Welsh College of Music & Drama in 1957, he trained at the Royal Academy of Dramatic Art in London, and was then spotted by Laurence Olivier who invited him to join the Royal National Theatre. In 1968, he got his break in film in "The Lion in Winter", playing Richard I.
Considered to be one of the greatest living actors, Hopkins is well known for his portrayal of Hannibal Lecter in "The Silence of the Lambs", for which he won the Academy Award for Best Actor, its sequel "Hannibal", and the prequel "Red Dragon". Other notable films include "The Mask of Zorro", "The Bounty", "Meet Joe Black", "The Elephant Man", "Magic", "84 Charing Cross Road", "Bram Stoker's Dracula", "Legends of the Fall", "Thor", "The Remains of the Day", "Amistad", "Nixon", "The World's Fastest Indian", "Instinct", and "Fracture".
Along with his Academy Award, Hopkins has won three BAFTA Awards, two Emmys, and the Cecil B. DeMille Award. In 1993, he was knighted by Queen Elizabeth II for services to the arts. He received a star on the Hollywood Walk of Fame in 2003, and was made a Fellow of the British Academy of Film and Television Arts in 2008.
Early life.
Hopkins was born on New Year's Eve 1937, in Margam, a suburb of Port Talbot, Glamorgan. His parents are Annie Muriel ("née" Yeates) and Richard Arthur Hopkins, a baker. His school days were unproductive; he found that he would rather immerse himself in art, such as painting and drawing, or playing the piano, than attend to his studies. In 1949, to instill discipline, his parents insisted he attend Jones' West Monmouth Boys' School in Pontypool. He remained there for five terms and was then educated at Cowbridge Grammar School in the Vale of Glamorgan.
Hopkins was influenced and encouraged by Welsh compatriot Richard Burton, whom he met at the age of 15. Hopkins promptly enrolled at the Royal Welsh College of Music & Drama in Cardiff, from which he graduated in 1957. After two years in the British Army doing his national service, he moved to London, where he trained at the Royal Academy of Dramatic Art.
Career.
Hopkins made his first professional stage appearance in the Palace Theatre, Swansea, in 1960 with Swansea Little Theatre's production of "Have a Cigarette". In 1965, after several years in repertory, he was spotted by Laurence Olivier, who invited him to join the Royal National Theatre in London. Hopkins became Olivier's understudy, and filled in when Olivier was struck with appendicitis during a production of August Strindberg's "The Dance of Death". Olivier later noted in his memoir, "Confessions of an Actor", that "A new young actor in the company of exceptional promise named Anthony Hopkins was understudying me and walked away with the part of Edgar like a cat with a mouse between its teeth."
Despite his success at the National, Hopkins tired of repeating the same roles nightly and yearned to be in films. He made his small-screen debut in a 1967 BBC broadcast of "A Flea in Her Ear". His first starring role in a film came in 1964 in "Changes", a short directed by Drewe Henley, written and produced by James Scott and co-starring Jacqueline Pearce. In 1968, he got his break in "The Lion in Winter" playing Richard I. Although Hopkins continued in theatre (most notably at the National Theatre as Lambert Le Roux in "Pravda" by David Hare and Howard Brenton and as Antony in "Antony and Cleopatra" opposite Judi Dench as well as in the Broadway production of Peter Shaffer's "Equus") he gradually moved away from it to become more established as a television and film actor. He portrayed Charles Dickens in the BBC television film "The Great Inimitable Mr. Dickens" in 1970, and Pierre Bezukhov in the BBC's mini series "War and Peace" (1972). In 1972 he starred as WWI British Prime Minister David Lloyd George in "Young Winston", and in 1977 he played British Army officer John Frost in Richard Attenborough's WWII film "A Bridge Too Far".
In 1980, he starred in "The Elephant Man" as the English doctor Sir Frederick Treves, who attends to Joseph Merrick (portrayed by John Hurt), a severely deformed man in 19th century London. That year he also starred opposite Shirley MacLaine in "A Change of Seasons" and famously said "she was the most obnoxious actress I have ever worked with." In 1984, he starred opposite Mel Gibson in "The Bounty" as William Bligh, captain of the Royal Navy ship the HMS "Bounty", in a retelling of the mutiny on the "Bounty". In 1992, Hopkins portrayed Abraham Van Helsing in Francis Ford Coppola's "Bram Stoker's Dracula".
Set in 1950s post-war Britain, Hopkins starred opposite Emma Thompson in the critically acclaimed "The Remains of the Day" (1993). Hopkins was nominated for an Academy Award for Best Actor for his performance, and the film frequently ranks among the best British films of all time. Hopkins portrayed Oxford academic C. S. Lewis in the 1993 British biographical film "Shadowlands", and received the BAFTA Award for Best Actor. During the 1990s, Hopkins had the chance to work with Bart the Bear in two films: "Legends of the Fall" (1994) and "The Edge" (1997). According to trainer, Lynn Seus, "Tony Hopkins was absolutely brilliant with Bart...He acknowledged and respected him like a fellow actor. He would spend hours just looking at Bart and admiring him. He did so many of his own scenes with Bart."
Hopkins was Britain's highest paid performer in 1998, starring in "The Mask of Zorro" and "Meet Joe Black", and also agreed to reprise his role as Dr Hannibal Lecter for a fee of £15 million. In 2000, Hopkins narrated "Dr. Seuss' How the Grinch Stole Christmas". Hopkins received a star on the Hollywood Walk of Fame in 2003.
Hopkins stated that his role as Burt Munro, whom he portrayed in his 2005 film "The World's Fastest Indian", was his favourite. He also asserted that Munro was the easiest role that he had played because both men have a similar outlook on life. In 2006, Hopkins was the recipient of the Golden Globe Cecil B. DeMille Award for lifetime achievement. In 2008, he received the BAFTA Academy Fellowship Award, the highest award the British Film Academy can bestow.
On 24 February 2010, it was announced that Hopkins had been cast in "The Rite", which was released on 28 January 2011. He played a priest who is "an expert in exorcisms and whose methods are not necessarily traditional". Hopkins, who is quoted as saying "I don't know what I believe, myself personally", reportedly wrote a line--"Some days I don't know if I believe in God or Santa Claus or Tinkerbell"—into his character in order to identify with it. On the other hand, in other sources from the same time, he is quoted as saying that he did believe in God and had done so for decades. On 21 September 2011, Peter R. de Vries named Hopkins in the role of the Heineken owner Freddy Heineken in a future film about his kidnapping.
Hopkins portrayed Odin, the Allfather or "king" of Asgard, in the 2011 film adaptation of Marvel Comics' "Thor". Hopkins portrayed Alfred Hitchcock in Sacha Gervasi's biopic "Hitchcock", following his career while making "Psycho". The film was released on 23 November 2012. In 2013, he reprised his role as Odin in "". In 2014, he portrayed Methuselah in Darren Aronofsky's "Noah".
Hannibal Lecter.
Hopkins' most famous role is as the cannibalistic serial killer Hannibal Lecter in "The Silence of the Lambs", for which he won the Academy Award for Best Actor in 1991, with Jodie Foster as Clarice Starling, who also won for Best Actress. The film won Best Picture, Best Director and Academy Award for Best Adapted Screenplay. Hopkins reprised his role as Lecter twice; in Ridley Scott's "Hannibal" (2001), and "Red Dragon" (2002). His original portrayal of the character in "The Silence of the Lambs" has been labelled by the AFI as the number-one film villain. At the time he was offered the role, Hopkins was making a return to the London stage, performing in "M. Butterfly". He had come back to Britain after living for a number of years in Hollywood, having all but given up on a career there, saying, "Well that part of my life's over; it's a chapter closed. I suppose I'll just have to settle for being a respectable actor poncing around the West End and doing respectable BBC work for the rest of my life."
Hopkins played the iconic villain in adaptations of the first three of the Lecter novels by Thomas Harris. The author was reportedly very pleased with Hopkins' portrayal of his antagonist. However, Hopkins stated that "Red Dragon" would feature his final performance as the character, and that he would not reprise even a narrative role in the latest addition to the series, "Hannibal Rising".
Acting style.
Hopkins is renowned for his preparation for roles. He indicated in interviews that once he has committed to a project, he will go over his lines as many times as is needed (sometimes upwards of 200) until the lines sound natural to him, so that he can "do it without thinking". This leads to an almost casual style of delivery that belies the amount of groundwork done beforehand. While it can allow for some careful improvisation, it has also brought him into conflict with the occasional director who departs from the script, or demands what the actor views as an excessive number of takes. Hopkins has stated that after he is finished with a scene, he simply discards the lines, not remembering them later on. This is unlike others who usually remember their lines from a film, even years later.
Richard Attenborough, who directed Hopkins on five occasions, found himself going to great lengths during the filming of "Shadowlands" (1993) to accommodate the differing approaches of his two stars (Hopkins and Debra Winger), who shared many scenes. Whereas Hopkins, preferring the spontaneity of a fresh take, liked to keep rehearsals to a minimum, Winger rehearsed continuously. To allow for this, Attenborough stood in for Hopkins during Winger's rehearsals, only bringing him in for the last one before a take. The director praised Hopkins for "this extraordinary ability to make you believe when you hear him that it is the very first time he has ever said that line. It's an incredible gift."
Renowned for his ability to remember lines, Hopkins keeps his memory supple by learning things by heart such as poetry, and Shakespeare. In Steven Spielberg's "Amistad", Hopkins astounded the crew with his memorisation of a seven-page courtroom speech, delivering it in one go. An overawed Spielberg couldn't bring himself to call him Tony, and insisted on addressing him as Sir Anthony throughout the shoot.
Hopkins is a gifted mimic, adept at turning his native Welsh accent into whatever is required by a character. He duplicated the voice of his late mentor, Laurence Olivier, for additional scenes in "Spartacus" in its 1991 restoration. His interview on the 1998 relaunch edition of the British TV talk show "Parkinson" featured an impersonation of comedian Tommy Cooper. Hopkins has said acting "like a submarine" has helped him to deliver credible performances in his thriller movies. He said, "It's very difficult for an actor to avoid, you want to show a bit. But I think the less one shows the better."
Personal life.
Hopkins was made a Commander of the British Empire (CBE) in 1987, and was knighted at Buckingham Palace in 1993 for services to the arts. In 1988, Hopkins was made an Honorary D.Litt and in 1992 was awarded Honorary fellowship from the University of Wales, Lampeter. He was made a freeman of his hometown Port Talbot in 1996.
Hopkins resides in Malibu, California. He had moved to the US once before during the late 1970s to pursue his film career, but returned to London in the late 1980s. However, he decided to return to the US following his 1990s success. Retaining his British citizenship, he became a naturalised US citizen on 12 April 2000, with Hopkins stating: "I have dual citizenship, it just so happens I live in America".
Hopkins has been married three times. His first two wives were Petronella Barker from 1966 to 1972, and Jennifer Lynton from 1973 to 2002. He has a daughter from his first marriage, actress and singer Abigail Hopkins (born 20 August 1968). He married Stella Arroyave in 2003. On Christmas Eve 2012, he celebrated his 10th wedding anniversary by having a blessing at a private service at St David's Cathedral in Pembrokeshire.
Hopkins has offered his support to various charities and appeals, notably becoming President of the National Trust's Snowdonia Appeal, raising funds for the preservation of Snowdonia National Park in north Wales. In 1998 he donated £1 million towards the £3 million needed to aid the Trust's efforts in purchasing parts of Snowdon. Prior to the campaign, Hopkins authored "Anthony Hopkins' Snowdonia", which was published in 1995. Due to his contributions to Snowdonia, in addition to his film career, in 2004 Hopkins was named among the 100 Welsh Heroes in a Welsh poll.
Hopkins has been a patron of the YMCA centre in his hometown of Port Talbot, South Wales for more than 20 years, having first joined the YMCA in the 1950s. He supports other various philanthropic groups. He was a Guest of Honour at a Gala Fundraiser for Women in Recovery, Inc., a Venice, California-based non-profit organisation offering rehabilitation assistance to women in recovery from substance abuse. He is also a volunteer teacher at the Ruskin School of Acting in Santa Monica, California. Hopkins served as the Honorary Patron of The New Heritage Theatre Company in Boise, Idaho from 1997-2007, participating in fundraising and marketing efforts for the repertory theatre.
Hopkins is a recovering alcoholic; he stopped drinking on 25 December 1975. He said that a major help in his recovery was his belief in God. He has criticised atheism, saying "being an atheist must be like living in a closed cell with no windows". He quit smoking using the Allen Carr method. In 2008, he embarked on a weight loss programme, and by 2010, he had lost 80 pounds.
Hopkins contributed toward the refurbishment of a £2.3 million wing at his alma mater, the Royal Welsh College of Music & Drama in Cardiff, named the Anthony Hopkins Centre. It opened in 1999.
Hopkins is a prominent member of environmental protection group Greenpeace and as of early 2008 featured in a television advertisement campaign, voicing concerns about Japan's continuing annual whale hunt. He has also been a patron of RAPt (Rehabilitation for Addicted Prisoners Trust) since its early days and helped open their first intensive drug and alcohol rehabilitation unit at Downview (HM Prison) in 1992.
Hopkins is an admirer of the Welsh comedian Tommy Cooper. On 23 February 2008, as patron of the Tommy Cooper Society, he unveiled a commemorative statue in the entertainer's home town of Caerphilly. For the ceremony, he donned Cooper's trademark fez and performed a comic routine.
Other work.
In a 2012 interview, Hopkins stated, "I've been composing music all my life and if I'd been clever enough at school I would like to have gone to music college. As it was I had to settle for being an actor." In 1986, he released a single called "Distant Star", which peaked at No. 75 in the UK Singles Chart. In 2007, he announced he would retire temporarily from the screen to tour around the world. Hopkins has also written music for the concert hall, in collaboration with Stephen Barton as orchestrator. These compositions include "The Masque of Time", given its world premiere with the Dallas Symphony Orchestra in October 2008, and "Schizoid Salsa".
In 1990, Hopkins directed a film about his Welsh compatriot, poet Dylan Thomas, titled "Dylan Thomas: Return Journey", which was his directing debut for the screen. In the same year, as part of the restoration process for the Stanley Kubrick film "Spartacus", Hopkins was approached to re-record lines from a scene that was being added back to the film; this scene featured Laurence Olivier and Tony Curtis, with Hopkins recommended by Olivier's widow, Joan Plowright to perform her late husband's part thanks to his talent for mimicry.
In 1996, he directed "August", an adaptation of Chekhov's "Uncle Vanya" set in Wales. His first screenplay, an experimental drama called "Slipstream", which he also directed and scored, premiered at the Sundance Film Festival in 2007. In 1997, Hopkins narrated the BBC natural documentary series, "Killing for a Living", which showed predatory behaviour in nature. He narrated episode 1 through 3 before being replaced by John Shrapnel.
Hopkins is a fan of the BBC sitcom "Only Fools and Horses", and once remarked in an interview how he would love to appear in the series. Writer John Sullivan saw the interview, and with Hopkins in mind created the character Danny Driscoll, a local villain. However, filming of the new series coincided with the filming of "The Silence of the Lambs", making Hopkins unavailable. The role instead went to Roy Marsden.
On 31 October 2011, André Rieu released an album including a waltz which Hopkins had composed many years before, at the age of nineteen. Hopkins had never heard his composition, "And the Waltz Goes On", before it was premiered by Rieu's orchestra in Vienna; Rieu's album was given the same name as Hopkins' piece.
In January 2012, Hopkins released an album of classical music, entitled "Composer", performed by the City of Birmingham Symphony Orchestra, and released on CD via the UK radio station Classic FM. The album consists of nine of his original works and film scores, with one of the pieces titled "Margam" in tribute to his home town near Port Talbot in Wales.
In October 2015, Hopkins appeared as Sir in a BBC Two production of Ronald Harwood's "The Dresser", alongside Ian McKellen, Edward Fox and Emily Watson. "The Dresser" is set in a London theatre during the Blitz, where an aging actor-manager, Sir, prepares for his starring role in "King Lear" with the help of his devoted dresser, Norman.

</doc>
<doc id="2398" url="https://en.wikipedia.org/wiki?curid=2398" title="Ardal O'Hanlon">
Ardal O'Hanlon

Ardal O'Hanlon (; born 8 October 1965) is an Irish comedian and actor. He played Father Dougal McGuire in "Father Ted" and George Sunday in "My Hero".
Biography.
Early life.
O'Hanlon was born in 1965 in Carrickmacross, County Monaghan, Ireland, the son of Rory O'Hanlon, an Irish politician and doctor, and has five siblings. RTÉ's "Who do you think you are?" programme on Monday 6 October 2008 examined Ardal's family tree. He discovered that his paternal grandfather, Michael O'Hanlon, a UCD medicine student, had joined the Irish Republican Army during the Irish War of Independence and was a member of Michael Collins's squad which assassinated British secret service agents on the morning of Bloody Sunday. Details of his grandfather's activities survive in UCD Archives, as well as Blackrock College. It also transpired that on his mother's side he was a close relative of Peter Fenelon Collier, the founder of "Collier's Weekly" and "Collier's Encyclopedia".
O'Hanlon was schooled in Blackrock College in Dublin and graduated, in 1987, from the National Institute for Higher Education, Dublin (now Dublin City University) with a degree in Communications Studies.
Career.
Together with Kevin Gildea and Barry Murphy, Ardal O'Hanlon founded the International Comedy Cellar, upstairs in the International Bar on Dublin's South Wicklow Street. Dublin had no comedy scene at the time. As a stand up, O'Hanlon won the Hackney Empire New Act of the Year competition in 1994. For a time he was the presenter of "The Stand Up Show".
He was spotted by Graham Linehan, who was to cast him as Father Dougal McGuire in "Father Ted" (1995–98). In 1995 he received the Top TV Comedy Newcomer at the British Comedy Awards for this role. In 1995, he appeared (as Father Dougal) in a Channel 4 ident ("Hello, you're watching... television"), and during Comic Relief on BBC1. This was followed by the award-winning short comedy film "Flying Saucer Rock'n'Roll".
O'Hanlon moved into straight acting alongside Emma Fielding and Beth Goddard in the ITV comedy-drama "Big Bad World", which aired for two series in summer 1999 and winter 2001. He also played a minor role in "The Butcher Boy" as Joe's (Francie's best friend) father, and appeared in an episode of the original "Whose Line is it Anyway?".
In 2000, O'Hanlon starred in the comedy series "My Hero", in which he played a very naive superhero from the planet Ultron. His character juggled world-saving heroics with life in suburbia. He stayed in the role until early 2005 and was replaced by James Dreyfus for series 6 in 2006.
He also provided the voice of the lead character in the three Christmas television cartoon specials of "Robbie the Reindeer". He appeared in the 2005 BBC One sitcom "Blessed", written by Ben Elton; at the 2005 British Comedy Awards, it was publicly slated by Jonathan Ross, albeit in jest. Towards the end of 2005, he played an eccentric Scottish character, Coconut Tam, in the family-based film, "The Adventures of Greyfriars Bobby". Although more commonly on television, he has appeared on radio - on 18 July 2011, he appeared on "Quote... Unquote". Appropriately, one of his questions concerned a quotation from "Father Ted".
In 2006, O'Hanlon wrote and presented an RTE television series called "Leagues Apart", which saw him investigate the biggest and most passionate football rivalries in a number of European countries. Included were Roma vs Lazio in Italy, Barcelona vs Real Madrid in Spain, and Galatasaray vs Fenerbahce in Turkey. He followed this with another RTÉ show, "So You Want To Be Taoiseach?" in 2007. It was a political series where O'Hanlon gave tongue-in-cheek advice on how to go about becoming Taoiseach of Ireland. Both programmes went some way towards freeing O'Hanlon from his association with the character of Dougal in the minds of Irish audiences.
He appeared in the "Doctor Who" episode "Gridlock", broadcast on 14 April 2007, in which he played a cat-like creature named Thomas Kincade Brannigan. O'Hanlon appears in Series 3 of the TV show "Skins", playing Naomi Campbell (Lily Loveless)'s Politics teacher named Kieran, who attempted to kiss her. He then went on to form a relationship with Naomi's mother (Olivia Colman). O'Hanlon plays the lead role in Irish comedy television programme "Val Falvey, TD" on RTE One. He has recently performed in the Edinburgh Fringe.
In February 2011, O'Hanlon returned to the Gate Theatre, Dublin starring in the Irish premiere of Christopher Hampton's "God of Carnage", alongside Maura Tierney.
In 2011, he appeared in the comedy panel show "Argumental".
O'Hanlon has written a novel, "The Talk of the Town" (known in the United States as "Knick Knack Paddy Whack"), which was published in 1998. The novel is about a teenage boy, Patrick Scully, and his friends.
in February 2015 he officially launched the 2015 Sky Cat Laughs Comedy Festival which takes place in Kilkenny from 28 May- 1 June.
He plays the role of Peter the milkman in Sky 1 sitcom After Hours.
Stand-up.
Ardal has been doing stand up for many years appearing on many shows including Live at the Apollo, Michael McIntyre's Comedy Roadshow and Dave's One Night Stand. In 1994 he won the Hackney Empire New Act of the Year.
Personal life.
O'Hanlon is married to Melanie, whom he met as a teenager, and with whom he has three children: Emily, Rebecca and Red. He is a supporter of Leeds United.
His name comes from the norwegian city of Årdal.

</doc>
<doc id="2400" url="https://en.wikipedia.org/wiki?curid=2400" title="Advanced Micro Devices">
Advanced Micro Devices

Advanced Micro Devices, Inc. (AMD) is an American worldwide semiconductor company based in Sunnyvale, California, United States, that develops computer-processors and related technologies for business and consumer markets. While initially it manufactured its own processors, the company became fabless after GlobalFoundries was spun off in 2009. AMD's main products include microprocessors, motherboard chipsets, embedded processors and graphics processors for servers, workstations and personal computers, and embedded systems applications.
AMD is the second-largest supplier and only significant rival to Intel in the market for x86-based microprocessors. Since acquiring ATI in 2006, AMD and its competitor Nvidia have dominated the discrete graphics processor unit (GPU) market.
Company history.
First twelve years.
Advanced Micro Devices was formally incorporated on May 1, 1969, by Jerry Sanders, along with seven of his colleagues from Fairchild Semiconductor. Sanders, an electrical engineer who was the director of marketing at Fairchild, had like many Fairchild executives grown frustrated with the increasing lack of support, opportunity, and flexibility within that company, and decided to leave to start his own semiconductor company. The previous year Robert Noyce, who had invented the first practical integrated circuit or microchip in 1959 at Fairchild, had left Fairchild together with Gordon Moore and founded the semiconductor company Intel in July 1968.
In September 1969, AMD moved from its temporary location in Santa Clara to Sunnyvale, California. To immediately secure a customer base, AMD initially became a second source supplier of microchips designed by Fairchild and National Semiconductor. AMD first focused on producing logic chips. The company guaranteed quality control to United States Military Standard, an advantage in the early computer industry since unreliability in microchips was a distinct problem that customers – including computer manufacturers, the telecommunications industry, and instrument manufacturers – wanted to avoid.
In November 1969, the company manufactured its first product, the Am9300, a 4-bit MSI shift register, which began selling in 1970. Also in 1970, AMD produced its first proprietary product, the Am2501 logic counter, which was highly successful. Its best-selling product in 1971 was the Am2505, the fastest multiplier available.
In 1971, AMD entered the RAM chip market, beginning with the Am3101, a 64-bit bipolar RAM. That year AMD also greatly increased the sales volume of its linear integrated circuits, and by year end the company's total annual sales reached $4.6 million.
AMD went public in September 1972. The company was a second source for Intel MOS/LSI circuits by 1973, with products such as Am14/1506 and Am14/1507, dual 100-bit dynamic shift registers. By 1975, AMD was producing 212 products – of which 49 were proprietary, including the Am9102 (a static N-channel 1024-bit RAM) and three low-power Schottky MSI circuits: Am25LS07, Am25LS08, and Am25LS09.
Intel had created the first microprocessor, its 4-bit 4004, in 1971. By 1975, AMD entered the microprocessor market with the Am9080, a reverse-engineered clone of the Intel 8080, and the Am2900 bit-slice microprocessor family. When Intel began installing microcode in its microprocessors in 1976, it entered into a cross-licencing agreement with AMD, granting AMD a copyright license to the microcode in its microprocessors and peripherals, effective October 1976.
In 1977, AMD entered into a joint venture with Siemens, a German engineering conglomerate wishing to enhance its technology expertise and enter the U.S. market. Siemens purchased 20% of AMD's stock, giving AMD an infusion of cash to increase its product lines. That year the two companies also jointly established Advanced Micro Computers, located in Silicon Valley and in Germany, giving AMD an opportunity to enter the microcomputer development and manufacturing field, in particular based on AMD's second-source Zilog Z8000 microprocessors. When the two companies' vision for Advanced Micro Computers diverged, AMD bought out Siemens' stake in the U.S. division in 1979. AMD closed its Advanced Micro Computers subsidiary in late 1981, after switching focus to manufacturing second-source Intel x86 microprocessors.
Total sales in fiscal year 1978 topped $100 million, and in 1979, AMD debuted on the New York Stock Exchange. In 1979, production also began in AMD's new semiconductor fab in Austin; the company already had overseas assembly facilities in Penang and Manila, and it began construction on a semiconductor fab in San Antonio in 1981. In 1980, AMD began supplying semiconductor products for telecommunications, an industry undergoing rapid expansion and innovation.
Technology exchange agreement with Intel.
Intel had introduced the first x86 microprocessors in 1978. In 1981, IBM created its PC, and wanted Intel's x86 processors, but only under the condition that Intel also provide a second-source manufacturer for its patented x86 microprocessors. Intel and AMD entered into a 10-year technology exchange agreement, first signed in October 1981 and formally executed in February 1982. The terms of the agreement were that each company could acquire the right to become a second-source manufacturer for semiconductor products developed by the other; that is, each party could "earn" the right to manufacture and sell a product developed by the other, if agreed to, by exchanging the manufacturing rights to a product of equivalent technical complexity. The technical information and licenses needed to make and sell a part would be exchanged for a royalty to the developing company. The 1982 agreement also extended the 1976 AMD–Intel cross-licensing agreement through 1995. The agreement included the right to invoke arbitration of disagreements, and after five years the right of either party to end the agreement with one year's notice. The main result of the 1982 agreement was that AMD became a second-source manufacturer of Intel's x86 microprocessors and related chips, and Intel provided AMD with database tapes for its 8086, 80186, and 80286 chips.
Beginning in 1982, AMD began volume-producing second-source Intel-licensed 8086, 8088, 80186, and 80188 processors, and by 1984 its own Am286 clone of Intel's 80286 processor, for the rapidly growing market of IBM PCs and IBM clones. It also continued its successful concentration on proprietary bipolar chips. In 1983, it introduced INT.STD.1000, the highest manufacturing quality standard in the industry.
The company continued to spend greatly on research and development, and in addition to other breakthrough products, created the world's first 512K EPROM in 1984. That year AMD was listed in the book "The 100 Best Companies to Work for in America", and based on 1984 income it made the "Fortune" 500 list for the first time in 1985.
By mid-1985, however, the microchip market experienced a severe downturn, mainly due to longterm aggressive trade practices (dumping) from Japan, but also due to a crowded and non-innovative chip market in the U.S. AMD rode out the mid-1980s crisis by aggressively innovating and modernizing, devising the Liberty Chip program of designing and manufacturing one new chip or chip set per week for 52 weeks in fiscal year 1986, and by heavily lobbying the U.S. government until sanctions and restrictions were put into place to prevent predatory Japanese pricing. During this time period, AMD withdrew from the DRAM market, and at the same time made some headway into the CMOS market, which it had lagged in entering, having focused instead on bipolar chips.
AMD had some success in the mid-1980s with the AMD7910 and AMD7911 "World Chip" FSK modem, one of the first multi-standard devices that covered both Bell and CCITT tones at up to 1200 baud half duplex or 300/300 full duplex. Beginning in 1986, AMD embraced the perceived shift toward RISC with their own AMD Am29000 (29k) processor; the 29k survived as an embedded processor. The company also increased its EPROM memory market share in the late 1980s. Throughout the 1980s, AMD was a second-source supplier of Intel x86 processors. In 1991, it introduced its own 386-compatible Am386, an AMD-designed chip. Creating its own chips, AMD began to compete directly with Intel.
AMD had a large and successful flash memory business, even during the dotcom bust. In 2003, to divest some manufacturing and aid its overall cash flow, which was under duress from aggressive microprocessor competition from Intel, AMD spun-off its flash memory business and manufacturing into Spansion, a joint venture with Fujitsu, which had been co-manufacturing flash memory with AMD since 1993. AMD divested itself of Spansion in December 2005, in order to focus on the microprocessor market, and Spansion went public in an IPO.
AMD announced the acquisition of the graphics processor company ATI Technologies on July 24, 2006. AMD paid $4.3 billion in cash and 58 million shares of its stock, for a total of approximately $5.4 billion. The transaction completed on October 25, 2006. On August 30, 2010, AMD announced that it would retire the ATI brand name for its graphics chipsets in favor of the AMD brand name.
In October 2008, AMD announced plans to spin off manufacturing operations in the form of a multibillion-dollar joint venture with Advanced Technology Investment Co., an investment company formed by the government of Abu Dhabi. The new venture is called GlobalFoundries Inc. The partnership and spin-off gave AMD an infusion of cash and allowed AMD to focus solely on chip design. To assure the Abu Dhabi investors of the new venture's success, CEO Hector Ruiz stepped down as CEO of AMD in July 2008, while remaining Executive Chairman, in preparation to becoming Chairman of Global Foundries in March 2009. President and COO Dirk Meyer became AMD's CEO. Recessionary losses necessitated AMD cutting 1,100 jobs in 2009.
In August 2011, AMD announced that former Lenovo executive Rory Read would be joining the company as CEO, replacing Meyer. AMD announced in November 2011 plans to lay off more than 10% (1,400) of its employees from across all divisions worldwide. In October 2012, it announced plans to lay off an additional 15% of its workforce to reduce costs in the face of declining sales revenue.
AMD acquired the low-power server manufacturer SeaMicro in early 2012, with an eye to bringing out an ARM architecture server chip.
On October 8, 2014, AMD announced that Rory Read had stepped down after three years as president and chief executive officer. He was succeeded by Lisa Su, a key lieutenant who had been serving as chief operating officer since June.
On October 16, 2014, AMD announced a new restructuring plan along with its Q3 results. Effective July 1, 2014, AMD reorganized into two business groups: Computing and Graphics, which primarily includes desktop and notebook processors and chipsets, discrete GPUs, and professional graphics; and Enterprise, Embedded and Semi-Custom, which primarily includes server and embedded processors, dense servers, semi-custom SoC products, engineering services, and royalties. As part of this restructuring AMD announced that 7% of its global workforce would be laid off by the end of 2014.
History of CPUs and APUs.
IBM PC and the x86 architecture.
In February 1982, AMD signed a contract with Intel, becoming a licensed second-source manufacturer of 8086 and 8088 processors. IBM wanted to use the Intel 8088 in its IBM PC, but IBM's policy at the time was to require at least two sources for its chips. AMD later produced the Am286 under the same arrangement. In 1984 Intel, in order to shore up its advantage in the marketplace, internally decided to no longer cooperate with AMD in supplying product information, and delayed and eventually refused to convey the technical details of the Intel 80386 to AMD. In 1987, AMD invoked arbitration over the issue, and Intel reacted by cancelling the 1982 technological-exchange agreement altogether. After three years of testimony, AMD eventually won in arbitration in 1992, but Intel disputed this decision. Another long legal dispute followed, ending in 1994 when the Supreme Court of California sided with the arbitrator and AMD.
In 1990, Intel also countersued AMD, reneging on AMD's right to use derivatives of Intel's microcode for its cloned processors. In the face of uncertainty during the legal dispute, AMD was forced to develop clean room designed versions of Intel code for its x386 and x486 processors, the former long after Intel had released its own x386 in 1985. In March 1991, AMD released the Am386, its clone of the Intel 386 processor. By October of the same year it had sold one million units.
In 1993, AMD introduced the first of the Am486 family of processors, which proved popular with a large number of original equipment manufacturers, including Compaq, which signed an exclusive agreement using the Am486. Another Am486-based processor, the Am5x86, was released in November 1995 and continued AMD's success as a fast, cost-effective processor.
Finally, in an agreement effective 1996, AMD received the rights to the microcode in Intel's x386 and x486 processor families, but not the rights to the microcode in the following generations of processors.
K5, K6, Athlon, Duron, and Sempron.
AMD's first in-house x86 processor was the K5, which was launched in 1996. The "K" was a reference to Kryptonite. (In comic books, the only substance which could harm Superman was Kryptonite. This is a reference to Intel's hegemony over the market, i.e., an anthropomorphization of them as Superman.) The numeral "5" refers to the fifth generation of x86 processors; rival Intel had previously introduced its line of fifth-generation x86 processors as Pentium because the U.S. Trademark and Patent Office had ruled that mere numbers could not be trademarked.
In 1996, AMD purchased NexGen, specifically for the rights to their Nx series of x86-compatible processors. AMD gave the NexGen design team their own building, left them alone, and gave them time and money to rework the Nx686. The result was the K6 processor, introduced in 1997. Although the K6 was based on Socket 7, variants such as K6-3/450 were faster than Intel's Pentium II (sixth-generation processor).
The K7 was AMD's seventh-generation x86 processor, making its debut on June 23, 1999, under the brand name Athlon. Unlike previous AMD processors, it could not be used on the same motherboards as Intel's, due to licensing issues surrounding Intel's Slot 1 connector, and instead used a Slot A connector, referenced to the Alpha processor bus. The Duron was a lower-cost and limited version of the Athlon (64KB instead of 256KB L2 cache) in a 462-pin socketed PGA (socket A) or soldered directly onto the motherboard. Sempron was released as a lower-cost Athlon XP, replacing Duron in the socket A PGA era. It has since been migrated upward to all new sockets, up to AM3.
On October 9, 2001, the Athlon XP was released. On February 10, 2003, the Athlon XP with 512KB L2 Cache was released.
Athlon 64, Opteron and Phenom.
The K8 was a major revision of the K7 architecture, with the most notable features being the addition of a 64-bit extension to the x86 instruction set (called x86-64, AMD64, or x64), the incorporation of an on-chip memory controller, and the implementation of an extremely high performance point-to-point interconnect called HyperTransport, as part of the Direct Connect Architecture. The technology was initially launched as the Opteron server-oriented processor on April 22, 2003. Shortly thereafter it was incorporated into a product for desktop PCs, branded Athlon 64.
On April 21, 2005, AMD released the first dual core Opteron, an x86-based server CPU. A month later, AMD released the Athlon 64 X2, the first desktop-based dual core processor family. In May 2007, AMD abandoned the string "64" in its dual-core desktop product branding, becoming Athlon X2, downplaying the significance of 64-bit computing in its processors. Further updates involved improvements to the microarchitecture, and a shift of target market from mainstream desktop systems to value dual-core desktop systems. In 2008, AMD started to release dual-core Sempron processors exclusively in China, branded as the Sempron 2000 series, with lower HyperTransport speed and smaller L2 cache. Thus AMD completed its dual-core product portfolio for each market segment.
After K8 came K10. In September 2007, AMD released the first K10 processors – nine quad-core Third Generation Opteron processors – followed in November by the Phenom processor for desktop. K10 processors came in dual-core, triple-core, and quad-core versions, with all cores on a single die. AMD released a new platform, codenamed "Spider", which utilized the new Phenom processor, as well as an R770 GPU and a 790 GX/FX chipset from the AMD 700 chipset series. However, AMD built the Spider at 65nm, which was uncompetitive with Intel's smaller and more power-efficient 45nm.
In January 2009, AMD released a new processor line dubbed Phenom II, a refresh of the original Phenom built using the 45 nm process. AMD's new platform, codenamed “Dragon”, utilised the new Phenom II processor, and an ATI R770 GPU from the R700 GPU family, as well as a 790 GX/FX chipset from the AMD 700 chipset series. The Phenom II came in dual-core, triple-core and quad-core variants, all using the same die, with cores disabled for the triple-core and dual-core versions. The Phenom II resolved issues that the original Phenom had, including a low clock speed, a small L3 cache and a Cool'n'Quiet bug that decreased performance. The Phenom II cost less but was not performance-competitive with Intel's mid-to-high-range Core 2 Quads. The Phenom II also enhanced the Phenom's memory controller, allowing it to use DDR3 in a new native socket AM3, while maintaining backwards compatibility with AM2+, the socket used for the Phenom, and allowing the use of the DDR2 memory that was used with the platform.
In April 2010, AMD released a new Phenom II hexa-core (6-core) processor codenamed "Thuban". This was a totally new die based on the hexa-core “Istanbul” Opteron processor. It included AMD's “turbo core” technology, which allows the processor to automatically switch from 6 cores to 3 faster cores when more pure speed is needed. AMD's enthusiast platform, codenamed "Leo", utilized the new Phenom II, a new chipset from the AMD 800 chipset series and an ATI “Cypress” GPU from the Evergreen GPU series.
The Magny Cours and Lisbon server parts were released in 2010. The Magny Cours part came in 8 to 12 cores and the Lisbon part in 4 and 6 core parts. Magny Cours is focused on performance while the Lisbon part is focused on high performance per watt. Magny Cours is an MCM (Multi-Chip Module) with two hexa-core “Istanbul” Opteron parts. This will use a new G34 socket for dual and quad socket processors and thus will be marketed as Opteron 61xx series processors. Lisbon uses C32 socket certified for dual socket use or single socket use only and thus will be marketed as Opteron 41xx processors. Both will be built on a 45 nm SOI process.
Fusion becomes the AMD APU, and new microarchitectures.
Following AMD's 2006 acquisition of Canadian graphics company ATI Technologies, an initiative codenamed "Fusion" was announced to integrate a CPU and GPU together on some of AMD's microprocessors, including a built in PCI Express link to accommodate separate PCI Express peripherals, eliminating the northbridge chip from the motherboard. The initiative intended to move some of the processing originally done on the CPU (e.g. floating-point unit operations) to the GPU, which is better optimized for some calculations. The Fusion was later renamed to the AMD APU (Accelarated Processing Unit).
Llano was AMD's first APU built for laptops. Llano was the second APU released, targeted at the mainstream market. Incorporating a CPU and GPU on the same die, as well as northbridge functions, and using "Socket FM1" with DDR3 memory. The CPU part of the processor was based on the Phenom II "Deneb" processor. AMD suffered an unexpected decrease in revenue based on production problems for the Llano.
Bulldozer is AMD's microarchitecture codename for server and desktop AMD FX processors first released on October 12, 2011. This family 15h microarchitecture is the successor to the family 10h (K10) microarchitecture design. Bulldozer is designed from scratch, not a development of earlier processors. The core is specifically aimed at 10-125 W TDP computing products. AMD claims dramatic performance-per-watt efficiency improvements in high-performance computing (HPC) applications with Bulldozer cores. While hopes were very high that Bulldozer would bring AMD to be performance competitive with arch rival Intel once more, most benchmarks were disappointing. In some cases the new Bulldozer products were slower than the K10 model they were built to replace.
Hondo is AMD's latest processor series used in Tablet computers.
Piledriver is the name of AMD's microarchitecture used in some AMD FX processors released in 2012. This AMD FX series processor lineup is called Vishera, and targets the desktop performance market.
Jaguar is a x86-64 microarchitecture codename for a processor core that is used in various APUs from AMD aimed at the low-power/low-cost market. It is also used as the microarchitecture for the custom APUs in the PS4 and Xbox One (which contain CPU, GPU and memory).
Jaguar's predecessor, Bobcat, was revealed during a speech from AMD executive vice-president Henri Richard in Computex 2007 and was put into production Q1 2011. One of the major supporters was executive vice-president Mario A. Rivas who felt it was difficult to compete in the x86 market with a single core optimized for the 10-100 W range and actively promoted the development of the simpler core with a target range of 1-10 watts. In addition, it was believed that the core could migrate into the hand-held space if the power consumption can be reduced to less than 1 W.
ARM architecture-based chip.
AMD intends to release 64-bit ARM System on Chips (SoC) that will begin sampling in early 2014 and shipping in the second half of 2015. They will be for use in servers as a low-power alternative to current x86 chips. Their implementation using the ARM architecture is codenamed "Seattle", based on the Cortex A57 core design (ARMv8-A), and will contain 8 and 16 cores each. They will include the proprietary SeaMicro "Freedom Fabric", as well as support for 128 GB RAM, and 10 gigabit Ethernet. This is to be followed by the custom ARM core K12 core, expected in 2016-2017.
Zen based CPUs and APUs.
Zen is a new architecture for x86-64 based CPUs and APUs, built from the ground up by a team led by Jim Keller, beginning with his arrival in 2012, and taping out before his departure in September 2015. Zen will be built on the 14 nm node and have a renewed focus on single-core performance and HSA compatibility. Zen will be the first chip encompassing CPUs and APUs from AMD built for a single socket. It will also support DDR4. It is expected to be released mid-late 2016, following the availability of the AMD A10-7890K FM2+ desktop CPU.
Other products and technologies.
Graphics products (discrete and AMD APU technology).
AMD's portfolio of dedicated graphics processors includes product families and associated technologies aimed at the consumer, professional and high-performance computing markets, such as:
 technologies found in AMD products include:
AMD Catalyst is a collection of proprietary device driver software available for Microsoft Windows and Linux.
Since 2007, AMD has participated in the development of free and open-source graphics device drivers. The programming specifications for a number of chipsets and features were published in several rounds. Employees hired by AMD for this purpose contribute code to the Direct Rendering Manager in the Linux kernel.
AMD motherboard chipsets.
Before the launch of Athlon 64 processors in 2003, AMD designed chipsets for their processors spanning the K6 and K7 processor generations. The chipsets include the AMD-640, AMD-751 and the AMD-761 chipsets. The situation changed in 2003 with the release of Athlon 64 processors, and AMD chose not to further design its own chipsets for its desktop processors while opening the desktop platform to allow other firms to design chipsets. This was the “Open Platform Management Architecture” with ATI, VIA and SiS developing their own chipset for Athlon 64 processors and later Athlon 64 X2 and Athlon 64 FX processors, including the Quad FX platform chipset from Nvidia.
The initiative went further with the release of Opteron server processors as AMD stopped the design of server chipsets in 2004 after releasing the AMD-8111 chipset, and again opened the server platform for firms to develop chipsets for Opteron processors. As of today, Nvidia and Broadcom are the sole designing firms of server chipsets for Opteron processors.
As the company completed the acquisition of ATI Technologies in 2006, the firm gained the ATI design team for chipsets which previously designed the Radeon Xpress 200 and the Radeon Xpress 3200 chipsets. AMD then renamed the chipsets for AMD processors under AMD branding (for instance, the CrossFire Xpress 3200 chipset was renamed as AMD 580X CrossFire chipset). In February 2007, AMD announced the first AMD-branded chipset since 2004 with the release of the AMD 690G chipset (previously under the development codename "RS690"), targeted at mainstream IGP computing. It was the industry's first to implement a HDMI 1.2 port on motherboards, shipping for more than a million units. While ATI had aimed at releasing an Intel IGP chipset, the plan was scrapped and the inventories of Radeon Xpress 1250 (codenamed "RS600", sold under ATI brand) was sold to two OEMs, Abit and ASRock. Although AMD stated the firm would still produce Intel chipsets, Intel had not granted the license of FSB to ATI.
On November 15, 2007, AMD announced a new chipset series portfolio, the AMD 7-Series chipsets, covering from enthusiast multi-graphics segment to value IGP segment, to replace the AMD 480/570/580 chipsets and AMD 690 series chipsets, marking AMD's first enthusiast multi-graphics chipset. Discrete graphics chipsets were launched on November 15, 2007 as part of the codenamed "Spider" desktop platform, and IGP chipsets were launched at a later time in spring 2008 as part of the codenamed "Cartwheel" platform.
AMD returned to the server chipsets market with the AMD 800S series server chipsets. It includes support for up to six SATA 6.0 Gbit/s ports, the C6 power state, which is featured in Fusion processors and AHCI 1.2 with SATA FIS–based switching support. This is a chipset family supporting Phenom processors and Quad FX enthusiast platform (890FX), IGP(890GX).
AMD Live!
, AMD LIVE! was a platform marketing initiative focusing the consumer electronics segment, with an Active TV initiative for streaming Internet videos from web video services such as YouTube, into AMD Live! PC as well as connected digital TVs, together with a scheme for an ecosystem of certified peripherals for the ease of customers to identify peripherals for AMD LIVE! systems for digital home experience, called "AMD LIVE! Ready".
AMD Quad FX platform.
The AMD Quad FX platform, being an extreme enthusiast platform, allows two processors to connect through HyperTransport, which is a similar setup to dual-processor (2P) servers, excluding the use of buffered memory/registered memory DIMM modules, and a server motherboard, the current setup includes two Athlon 64 FX-70 series processors and a special motherboard. AMD pushed the platform for the surging demands for what AMD calls "megatasking", the ability to do more tasks on a single system. The platform refreshes with the introduction of Phenom FX processors and the next-generation RD790 chipset, codenamed "FASN8".
Server platform.
AMD's first multi-processor server platform, codenamed "Fiorano", consists of AMD SR5690 + SP5100 server chipsets, supporting 45 nm, codenamed "Shanghai" Socket F+ processors and registered DDR2 memory. It was followed by the "Maranello" platform supporting 45 nm, codenamed "Istanbul", Socket G34 processors with DDR3 memory. On single-processor platform, the codenamed "Catalunya" platform consists of codenamed "Suzuka" 45 nm quad-core processor with AMD SR5580 + SP5100 chipset and DDR3 support.
AMD's x86 virtualization extension to the 64-bit x86 architecture is named "AMD Virtualization", also known by the abbreviation "AMD-V", and is sometimes referred to by the code name "Pacifica". AMD processors using Socket AM2, Socket S1, and Socket F include AMD Virtualization support. AMD Virtualization is also supported by release two (8200, 2200 and 1200 series) of the Opteron processors. The third generation (8300 and 2300 series) of Opteron processors will see an update in virtualization technology, specifically the Rapid Virtualization Indexing (also known by the development name "Nested Page Tables"), alongside the tagged TLB and Device Exclusion Vector (DEV).
AMD also promotes the "AMD I/O Virtualization Technology" (also known as IOMMU) for I/O virtualization. The AMD IOMMU specification has been updated to version 1.2. The specification describes the use of a HyperTransport architecture.
AMD's server initiatives include the following:
Desktop platforms.
Starting in 2007, AMD, following Intel, began using codenames for its desktop platforms such as "Spider" or "Dragon". The platforms, unlike Intel's approach, will refresh every year, putting focus on platform specialization. The platform includes components such as AMD processors, chipsets, ATI graphics and other features, but continued to the open platform approach, and welcome components from other vendors such as VIA, SiS, and Nvidia, as well as wireless product vendors.
Updates to the platform includes the implementation of IOMMU I/O Virtualization with 45 nm generation of processors, and the AMD 800 chipset series in 2009.
Embedded systems.
In February 2002, AMD acquired Alchemy Semiconductor for its Alchemy line of MIPS processors for the hand-held and portable media player markets. On June 13, 2006, AMD officially announced that the line was to be transferred to Raza Microelectronics, Inc., a designer of MIPS processors for embedded applications.
In August 2003, AMD also purchased the Geode business which was originally the Cyrix MediaGX from National Semiconductor to augment its existing line of embedded x86 processor products. During the second quarter of 2004, it launched new low-power Geode NX processors based on the K7 Thoroughbred architecture with speeds of fanless processors and , and processor with fan, of TDP 25 W. This technology is used in a variety of embedded systems (Casino slot machines and customer kiosks for instance), several UMPC designs in Asia markets, as well as the OLPC XO-1 computer, an inexpensive laptop computer intended to be distributed to children in developing countries around the world. The Geode LX processor was announced in 2005 and is said will continue to be available through 2015.
For the past couple of years AMD has been introducing 64-bit processors into its embedded product line starting with the AMD Opteron processor. Leveraging the high throughput enabled through HyperTransport and the Direct Connect Architecture these server class processors have been targeted at high end telecom and storage applications. In 2007, AMD added the AMD Athlon, AMD Turion and Mobile AMD Sempron processors to its embedded product line. Leveraging the same 64-bit instruction set and Direct Connect Architecture as the AMD Opteron but at lower power levels, these processors were well suited to a variety of traditional embedded applications. Throughout 2007 and into 2008, AMD has continued to add both single-core Mobile AMD Sempron and AMD Athlon processors and dual-core AMD Athlon X2 and AMD Turion processors to its embedded product line and now offers embedded 64-bit solutions starting with 8W TDP Mobile AMD Sempron and AMD Athlon processors for fan-less designs up to multi-processor systems leveraging multi-core AMD Opteron processors all supporting longer than standard availability.
The ATI acquisition included the Imageon and Xilleon product lines. In late 2008, the entire handheld division was sold off to Qualcomm, who have since produced the Adreno series. The Xilleon division was sold to Broadcom.
In April 2007, AMD announced the release of the M690T integrated graphics chipset for embedded designs. This enabled AMD to offer complete processor and chipset solutions targeted at embedded applications requiring high performance 3D and video such as emerging digital signage, kiosk and Point of Sale applications. The M690T was followed by the M690E specifically for embedded applications which removed the TV output, which required Macrovision licensing for OEMs, and enabled native support for dual TMDS outputs, enabling dual independent DVI interfaces.
In 2008, AMD announced the Radeon E2400, the first discrete GPU in their embedded product line offering the same long term availability as their other embedded products. That was followed in 2009 with the higher performance Radeon E4690 discrete GPU.
In 2009, AMD announced their first BGA packaged e64 architecture processors, known as the ASB1 family.
In 2010, AMD announced a second generation BGA platform referred to as ASB2. They also announced several new AM3 based processors with support for DDR3 memory.
In January 2011, AMD announced the AMD Embedded G-Series Accelerated Processing Unit. The first Fusion family APU for embedded applications. This announcement was followed by announcements for the high performance AMD Radeon E6760 and the value-conscious Radeon E6460 discrete GPUs. These solutions all added support for DirectX 11, OpenGL 4.1 and OpenCL 1.1.
In May 2012, AMD Announced the AMD Embedded R-Series Accelerated Processing Unit. This family of products incorporates the Bulldozer CPU architecture, and Discrete-class AMD Radeon™ HD 7000G Series graphics.
AMD Embedded solutions offer 5+ year product life.
Production and fabrication.
Ever since the spin-off of AMD's fabrication plants in early 2009, GlobalFoundries has been responsible for producing AMD's processors.
GlobalFoundries' main microprocessor manufacturing facilities are located in Dresden, Germany. Additionally, highly integrated microprocessors are manufactured in Taiwan made by third-party manufacturers under strict license from AMD. Between 2003 and 2005, they constructed a second manufacturing plant ( 90 nm process SOI) in the same complex in order to increase the number of chips they could produce, thus becoming more competitive with Intel. The new plant was named "Fab 36", in recognition of AMD's 36 years of operation, and reached full production in mid-2007. Fab 36 was renamed to "Fab 1" during the spin-off of AMD's manufacturing business during the creation of GlobalFoundries. In July 2007, AMD announced that they completed the conversion of Fab 1 Module 1 from to 65 nm. They then shifted their focus to the 45 nm conversion.
Corporate affairs.
Partnerships.
AMD utilizes strategic industry partnerships to further its business interests as well as to rival Intel's dominance and resources.
A partnership between AMD and Alpha Processor Inc. developed HyperTransport, a point-to-point interconnect standard which was turned over to an industry standards body for finalization. It is now used in modern motherboards that are compatible with AMD processors.
AMD also formed a strategic partnership with IBM, under which AMD gained silicon on insulator (SOI) manufacturing technology, and detailed advice on 90 nm implementation. AMD announced that the partnership would extend to 2011 for 32 nm and 22 nm fabrication-related technologies.
To facilitate processor distribution and sales, AMD is loosely partnered with end-user companies, such as HP, Compaq, ASUS, Acer, and Dell.
In 1993, AMD established a 50-50 partnership with Fujitsu called FASL, and merged into a new company called FASL LLC in 2003. The joint venture went public under the name Spansion and ticker symbol SPSN in December 2005, with AMD shares drop to 37%. AMD no longer directly participates in the Flash memory devices market now as AMD entered into a non-competition agreement, as of December 21, 2005, with Fujitsu and Spansion, pursuant to which it agreed not to directly or indirectly engage in a business that manufactures or supplies standalone semiconductor devices (including single chip, multiple chip or system devices) containing only Flash memory.
On May 18, 2006, Dell announced that it would roll out new servers based on AMD's Opteron chips by year's end, thus ending an exclusive relationship with Intel. In September 2006, Dell began offering AMD Athlon X2 chips in their desktop line-up.
In June 2011, HP announced new business and consumer notebooks equipped with the latest versions of AMD APUsaccelerated processing units. AMD will power HP's Intel-based business notebooks as well.
In the spring of 2013, AMD announced that it would be powering all three major next-generation consoles. The Xbox One and Sony PlayStation 4 are both powered by a custom-built AMD APU, and the Nintendo Wii U is powered by an AMD GPU. According to AMD, having their processors in all three of these consoles will greatly assist developers with cross-platform development to competing consoles and PCs as well as increased support for their products across the board.
Litigation with Intel.
AMD has a long history of litigation with former partner and x86 creator Intel.
Guinness World Record Achievement.
On August 31, 2011, in Austin, Texas, AMD achieved a Guinness World Record for the "Highest frequency of a computer processor": 8.429 GHz. The company ran an 8-core FX-8150 processor with only one active module (two cores), and cooled with liquid helium. The previous record was 8.308 GHz, with an Intel Celeron 352 (one core).
On November 1, 2011, geek.com reported that Andre Yang, an overclocker from Taiwan, used an FX-8150 to set another record: 8.461 GHz.
Corporate social responsibility.
In its 2012 report on progress relating to conflict minerals, the Enough Project rated AMD the fifth most progressive of 24 consumer electronics companies.

</doc>
<doc id="2402" url="https://en.wikipedia.org/wiki?curid=2402" title="Albrecht Dürer">
Albrecht Dürer

Albrecht Dürer (; ; 21 May 1471 – 6 April 1528) was a painter, printmaker and theorist of the German Renaissance. Born in Nuremberg, Dürer established his reputation and influence across Europe when he was still in his twenties, due to his high-quality woodcut prints.
He was in communication with the major Italian artists of his time, including Raphael, Giovanni Bellini and Leonardo da Vinci, and from 1512 he was patronized by emperor Maximilian I.
His vast body of work includes engravings, his preferred technique in his later prints, altarpieces, portraits and self-portraits, watercolours and books. 
The woodcuts, such as the "Apocalypse" series (1498), retain a more Gothic flavour than the rest of his work. 
His well-known engravings include the "Knight, Death, and the Devil" (1513), "Saint Jerome in his Study" (1514) and "Melencolia I" (1514), which has been the subject of extensive analysis and interpretation. His watercolours also mark him as one of the first European landscape artists, while his ambitious woodcuts revolutionized the potential of that medium.
Dürer's introduction of classical motifs into Northern art, through his knowledge of Italian artists and German humanists, has secured his reputation as one of the most important figures of the Northern Renaissance. This is reinforced by his theoretical treatises, which involve principles of mathematics, perspective and ideal proportions.
Early life (1471–90).
Dürer was born on 21 May 1471, third child and second son of his parents, who had between fourteen and eighteen children. His father, Albrecht Dürer the Elder, was a successful goldsmith, originally Ajtósi, who in 1455 had moved to Nuremberg from Ajtós, near Gyula in Hungary. The German name "Dürer" is a translation from the Hungarian, "Ajtósi". Initially, it was "Türer," meaning doormaker, which is "ajtós" in Hungarian (from "ajtó", meaning door). A door is featured in the coat-of-arms the family acquired. Albrecht Dürer the Younger later changed "Türer", his father's diction of the family's surname, to "Dürer", to adapt to the local Nuremberg dialect. Albrecht Dürer the Elder married Barbara Holper, the daughter of his master, when he himself became a master in 1467.
Dürer's godfather was Anton Koberger, who left goldsmithing to become a printer and publisher in the year of Dürer's birth and quickly became the most successful publisher in Germany, eventually owning twenty-four printing-presses and having many offices in Germany and abroad. Koberger's most famous publication was the "Nuremberg Chronicle", published in 1493 in German and Latin editions. It contained an unprecedented 1,809 woodcut illustrations (albeit with many repeated uses of the same block) by the Wolgemut workshop. Dürer may well have worked on some of these, as the work on the project began while he was with Wolgemut.
Because Dürer left autobiographical writings and became very famous by his mid-twenties, his life is well documented by several sources. After a few years of school, Dürer started to learn the basics of goldsmithing and drawing from his father. Though his father wanted him to continue his training as a goldsmith, he showed such a precocious talent in drawing that he started as an apprentice to Michael Wolgemut at the age of fifteen in 1486. A self-portrait, a drawing in silverpoint, is dated 1484 (Albertina, Vienna) "when I was a child," as his later inscription says. Wolgemut was the leading artist in Nuremberg at the time, with a large workshop producing a variety of works of art, in particular woodcuts for books. Nuremberg was then an important and prosperous city, a centre for publishing and many luxury trades. It had strong links with Italy, especially Venice, a relatively short distance across the Alps.
"Wanderjahre" and marriage (1490–94).
After completing his term of apprenticeship, Dürer followed the common German custom of taking "Wanderjahre"—in effect gap years —in which the apprentice learned skills from artists in other areas; Dürer was to spend about four years away. He left in 1490, possibly to work under Martin Schongauer, the leading engraver of Northern Europe, but who died shortly before Dürer's arrival at Colmar in 1492. It is unclear where Dürer travelled in the intervening period, though it is likely that he went to Frankfurt and the Netherlands. In Colmar, Dürer was welcomed by Schongauer's brothers, the goldsmiths Caspar and Paul and the painter Ludwig. In 1493 Dürer went to Strasbourg, where he would have experienced the sculpture of Nikolaus Gerhaert. Dürer's first painted self-portrait (now in the Louvre) was painted at this time, probably to be sent back to his fiancée in Nuremberg.
In early 1492 Dürer travelled to Basel to stay with another brother of Martin Schongauer, the goldsmith Georg. Very soon after his return to Nuremberg, on 7 July 1494, at the age of 23, Dürer was married to Agnes Frey following an arrangement made during his absence. Agnes was the daughter of a prominent brass worker (and amateur harpist) in the city. However, no children resulted from the marriage.
First journey to Italy (1494–95).
Within three months of his marriage, Dürer left for Italy, alone, perhaps stimulated by an outbreak of plague in Nuremberg. He made watercolour sketches as he traveled over the Alps. Some have survived and others may be deduced from accurate landscapes of real places in his later work, for example his engraving "Nemesis".
In Italy, he went to Venice to study its more advanced artistic world. Through Wolgemut's tutelage, Dürer had learned how to make prints in drypoint and design woodcuts in the German style, based on the works of Martin Schongauer and the Housebook Master. He also would have had access to some Italian works in Germany, but the two visits he made to Italy had an enormous influence on him. He wrote that Giovanni Bellini was the oldest and still the best of the artists in Venice. His drawings and engravings show the influence of others, notably Antonio Pollaiuolo with his interest in the proportions of the body, Andrea Mantegna, Lorenzo di Credi and others. Dürer probably also visited Padua and Mantua on this trip.
Return to Nuremberg (1495–1505).
On his return to Nuremberg in 1495, Dürer opened his own workshop (being married was a requirement for this). Over the next five years his style increasingly integrated Italian influences into underlying Northern forms. Dürer's father died in 1502, and his mother died in 1513. His best works in the first years of the workshop were his woodcut prints, mostly religious, but including secular scenes such as "The Men's Bath House" (ca. 1496). These were larger and more finely cut than the great majority of German woodcuts hitherto, and far more complex and balanced in composition.
It is now thought unlikely that Dürer cut any of the woodblocks himself; this task would have been performed by a specialist craftsman. However, his training in Wolgemut's studio, which made many carved and painted altarpieces and both designed and cut woodblocks for woodcut, evidently gave him great understanding of what the technique could be made to produce, and how to work with block cutters. Dürer either drew his design directly onto the woodblock itself, or glued a paper drawing to the block. Either way, his drawings were destroyed during the cutting of the block.
His famous series of sixteen great designs for the "Apocalypse" is dated 1498, as is his engraving of" St. Michael Fighting the Dragon". He made the first seven scenes of the "Great Passion" in the same year, and a little later, a series of eleven on the Holy Family and saints. The "Seven Sorrows Polyptych", commissioned by Frederick III of Saxony in 1496, was executed by Dürer and his assistants c. 1500. Around 1503–1505 he produced the first seventeen of a set illustrating the "Life of the Virgin", which he did not finish for some years. Neither these, nor the "Great Passion," were published as sets until several years later, but prints were sold individually in considerable numbers.
During the same period Dürer trained himself in the difficult art of using the burin to make engravings. It is possible he had begun learning this skill during his early training with his father, as it was also an essential skill of the goldsmith. In 1496 he executed the "Prodigal Son", which the Italian Renaissance art historian Giorgio Vasari singled out for praise some decades later, noting its Germanic quality. He was soon producing some spectacular and original images, notably "Nemesis" (1502), "The Sea Monster" (1498), and "Saint Eustace" (c. 1501), with a highly detailed landscape background and animals. His landscapes of this period, such as "Pond in the Woods" and "Willow Mill", are quite different from his earlier watercolours. There is a much greater emphasis on capturing atmosphere, rather than depicting topography. 
He made a number of Madonnas, single religious figures, and small scenes with comic peasant figures. Prints are highly portable and these works made Dürer famous throughout the main artistic centres of Europe within a very few years.
The Venetian artist Jacopo de' Barbari, whom Dürer had met in Venice, visited Nuremberg in 1500, and Dürer said that he learned much about the new developments in perspective, anatomy, and proportion from him. De' Barbari was unwilling to explain everything he knew, so Dürer began his own studies, which would become a lifelong preoccupation. A series of extant drawings show Dürer's experiments in human proportion, leading to the famous engraving of "Adam and Eve" (1504), which shows his subtlety while using the burin in the texturing of flesh surfaces. This is the only existing engraving signed with his full name.
Dürer created large numbers of preparatory drawings, especially for his paintings and engravings, and many survive, most famously the "Betende Hände" ("Praying Hands") from circa 1508, a study for an apostle in the Heller altarpiece. He also continued to make images in watercolour and bodycolour (usually combined), including a number of still lifes of meadow sections or animals, including his "Young Hare" (1502) and the "Great Piece of Turf" (1503).
Second journey to Italy (1505–1507).
In Italy, he returned to painting, at first producing a series of works executed in tempera on linen. These include portraits and altarpieces, notably, the Paumgartner altarpiece and the "Adoration of the Magi". In early 1506, he returned to Venice and stayed there until the spring of 1507. By this time Dürer's engravings had attained great popularity and were being copied. In Venice he was given a valuable commission from the emigrant German community for the church of San Bartolomeo. This was the altar-piece known as the "Adoration of the Virgin" or the "Feast of Rose Garlands". It includes portraits of members of Venice's German community, but shows a strong Italian influence. It was subsequently acquired by the Emperor Rudolf II and taken to Prague. Other paintings Dürer produced in Venice include "The Virgin and Child with the Goldfinch", "Christ among the Doctors" (supposedly produced in a mere five days), and a number of smaller works.
Nuremberg and the masterworks (1507–1520).
Despite the regard in which he was held by the Venetians, Dürer returned to Nuremberg by mid-1507, remaining in Germany until 1520. His reputation had spread throughout Europe and he was on friendly terms and in communication with most of the major artists including Raphael, Giovanni Bellini and — mainly through Lorenzo di Credi — Leonardo da Vinci.
Between 1507 and 1511 Dürer worked on some of his most celebrated paintings: "Adam and Eve" (1507), "The Martyrdom of the Ten Thousand" (1508, for Frederick of Saxony), "Virgin with the Iris" (1508), the altarpiece "Assumption of the Virgin" (1509, for Jacob Heller of Frankfurt), and "Adoration of the Trinity" (1511, for Matthaeus Landauer). During this period he also completed two woodcut series, the Great Passion and the Life of the Virgin, both published in 1511 together with a second edition of the Apocalypse series. The post-Venetian woodcuts show Dürer's development of chiaroscuro modelling effects, creating a mid-tone throughout the print to which the highlights and shadows can be contrasted.
Other works from this period include the thirty-seven woodcut subjects of the Little Passion, published first in 1511, and a set of fifteen small engravings on the same theme in 1512. Indeed, complaining that painting did not make enough money to justify the time spent when compared to his prints, he produced no paintings from 1513 to 1516. However, in 1513 and 1514 Dürer created his three most famous engravings: "Knight, Death, and the Devil" (1513, probably based on Erasmus's treatise "Enchiridion militis Christiani"), "St. Jerome in his Study", and the much-debated "Melencolia I" (both 1514).
In 1515, he created his "woodcut of a Rhinoceros" which had arrived in Lisbon from a written description and sketch by another artist, without ever seeing the animal himself. An image of the Indian rhinoceros, the image has such force that it remains one of his best-known and was still used in some German school science text-books as late as last century. In the years leading to 1520 he produced a wide range of works, including the woodblocks for the first western printed star charts in 1515 and portraits in tempera on linen in 1516.
Patronage of Maximilian I.
From 1512, Maximilian I became Dürer's major patron. His commissions included "The Triumphal Arch", a vast work printed from 192 separate blocks, the symbolism of which is partly informed by Pirckheimer's translation of Horapollo's "Hieroglyphica". The design program and explanations were devised by Johannes Stabius, the architectural design by the master builder and court-painter Jörg Kölderer and the woodcutting itself by Hieronymous Andreae, with Dürer as designer-in-chief. "The Arch" was followed by "The Triumphal Procession", the program of which was worked out in 1512 by and includes woodcuts by Albrecht Altdorfer and Hans Springinklee, as well as Dürer.
Dürer worked with pen on the marginal images for an edition of the Emperor's printed Prayer-Book; these were quite unknown until facsimiles were published in 1808 as part of the first book published in lithography. Dürer's work on the book was halted for an unknown reason, and the decoration was continued by artists including Lucas Cranach the Elder and Hans Baldung. Dürer also made several portraits of the Emperor, including one shortly before Maximilian's death in 1519.
Journey to the Netherlands (1520–21).
Maximilian's death came at a time when Dürer was concerned he was losing "my sight and freedom of hand" (perhaps caused by arthritis) and increasingly affected by the writings of Martin Luther. In July 1520 Dürer made his fourth and last major journey, to renew the Imperial pension Maximilian had given him and to secure the patronage of the new emperor, Charles V, who was to be crowned at Aachen. Dürer journeyed with his wife and her maid via the Rhine to Cologne and then to Antwerp, where he was well received and produced numerous drawings in silverpoint, chalk and charcoal. In addition to going to the coronation, he made excursions to Cologne (where he admired the painting of Stefan Lochner), Nijmegen, 's-Hertogenbosch, Bruges (where he saw Michelangelo's Madonna of Bruges), Ghent (where he admired van Eyck's altarpiece), and Zeeland.
Dürer took a large stock of prints with him and wrote in his diary to whom he gave, exchanged or sold them, and for how much. This provides rare information of the monetary value placed on prints at this time. Unlike paintings, their sale was very rarely documented. While providing valuable documentary evidence, Dürer's Netherlandish diary also reveals that the trip was not a profitable one. For example, Dürer offered his last portrait of Maximilian to his daughter, Margaret of Austria, but eventually traded the picture for some white cloth after Margaret disliked the portrait and declined to accept it. During this trip he also met Bernard van Orley, Jan Provoost, Gerard Horenbout, Jean Mone, Joachim Patinir and Tommaso Vincidor, though he did not, it seems, meet Quentin Matsys.
At the request of Christian II of Denmark, Dürer went to Brussels to paint the King's portrait. There he saw "the things which have been sent to the king from the golden land"—the Aztec treasure that Hernán Cortés had sent home to Holy Roman Emperor Charles V following the fall of Mexico. Dürer wrote that this treasure "was much more beautiful to me than miracles. These things are so precious that they have been valued at 100,000 florins". Dürer also appears to have been collecting for his own cabinet of curiosities, and he sent back to Nuremberg various animal horns, a piece of coral, some large fish fins, and a wooden weapon from the East Indies.
Having secured his pension, Dürer finally returned home in July 1521, having caught an undetermined illness—perhaps malaria —which afflicted him for the rest of his life, and greatly reduced his rate of work.
Final years in Nuremberg (1521–28).
On his return to Nuremberg, Dürer worked on a number of grand projects with religious themes, including a crucifixion scene and a Sacra Conversazione, though neither was completed. This may have been due in part to his declining health, but perhaps also because of the time he gave to the preparation of his theoretical works on geometry and perspective, the proportions of men and horses, and fortification.
However, one consequence of this shift in emphasis was that during the last years of his life, Dürer produced comparatively little as an artist. In painting, there was only a portrait of , a , , and two panels showing St. John with St. Peter in and St. Paul with St. Mark in the . This last great work, the Four Apostles, was given by Dürer to the City of Nuremberg—although he was given 100 guilders in return.
As for engravings, Dürer's work was restricted to portraits and illustrations for his treatise. The portraits include Cardinal-Elector Albert of Mainz; Frederick the Wise, elector of Saxony; the humanist scholar Willibald Pirckheimer; Philipp Melanchthon, and Erasmus of Rotterdam. For those of the Cardinal, Melanchthon, and Dürer's final major work, a drawn portrait of the Nuremberg patrician Ulrich Starck, Dürer depicted the sitters in profile, perhaps reflecting a more mathematical approach.
Despite complaining of his lack of a formal classical education, Dürer was greatly interested in intellectual matters and learned much from his boyhood friend Willibald Pirckheimer, whom he no doubt consulted on the content of many of his images. He also derived great satisfaction from his friendships and correspondence with Erasmus and other scholars. Dürer succeeded in producing two books during his lifetime. "The Four Books on Measurement" were published at Nuremberg in 1525 and was the first book for adults on mathematics in German, as well as being cited later by Galileo and Kepler. The other, a work on city fortifications, was published in 1527. "The Four Books on Human Proportion" were published posthumously, shortly after his death in 1528.
Dürer died in Nuremberg at the age of 56, leaving an estate valued at 6,874 florins—a considerable sum. His large house (purchased in 1509 from the heirs of the astronomer Bernhard Walther), where his workshop was located and where his widow lived until her death in 1539, remains a prominent Nuremberg landmark. It is now a museum. He is buried in the "Johannisfriedhof" cemetery.
Dürer and the Reformation.
Dürer's writings suggest that he may have been sympathetic to Martin Luther's ideas, though it is unclear if he ever left the Catholic Church. Dürer wrote of his desire to draw Luther in his diary in 1520: "And God help me that I may go to Dr. Martin Luther; thus I intend to make a portrait of him with great care and engrave him on a copper plate to create a lasting memorial of the Christian man who helped me overcome so many difficulties." In a letter to Nicholas Kratzer in 1524, Dürer wrote "because of our Christian faith we have to stand in scorn and danger, for we are reviled and called heretics." Most tellingly, Pirckheimer wrote in a letter to Johann Tscherte in 1530: "I confess that in the beginning I believed in Luther, like our Albert of blessed memory...but as anyone can see, the situation has become worse." Dürer may even have contributed to the Nuremberg City Council's mandating Lutheran sermons and services in March 1525. Notably, Dürer had contacts with various reformers, such as Zwingli, Andreas Karlstadt, Melanchthon, Erasmus and Cornelius Grapheus from whom Dürer received Luther's "Babylonian Captivity" in 1520.
Dürer's later works have also been claimed to show Protestant sympathies. For example, his woodcut of "The Last Supper" of 1523 has often been understood to have an evangelical theme, focussing as it does on Christ espousing the Gospel, as well the inclusion of the Eucharistic cup, an expression of Protestant utraquism, although this interpretation has been questioned. The delaying of the engraving of St Philip, completed in 1523 but not distributed until 1526, may have been due to Dürer's uneasiness with images of Saints; even if Dürer was not an iconoclast, in his last years he evaluated and questioned the role of art in religion.
Legacy and influence.
Dürer exerted a huge influence on the artists of succeeding generations, especially in printmaking, the medium through which his contemporaries mostly experienced his art, as his paintings were predominantly in private collections located in only a few cities. His success in spreading his reputation across Europe through prints was undoubtedly an inspiration for major artists such as Raphael, Titian, and Parmigianino, all of whom collaborated with printmakers in order to promote and distribute their work.
His work in engraving seems to have had an intimidating effect upon his German successors, the "Little Masters" who attempted few large engravings but continued Dürer's themes in small, rather cramped compositions. Lucas van Leyden was the only Northern European engraver to successfully continue to produce large engravings in the first third of the 16th century. The generation of Italian engravers who trained in the shadow of Dürer all either directly copied parts of his landscape backgrounds (Giulio Campagnola and Christofano Robetta), or whole prints (Marcantonio Raimondi and Agostino Veneziano). However, Dürer's influence became less dominant after 1515, when Marcantonio perfected his new engraving style, which in turn travelled over the Alps to dominate Northern engraving also.
In painting, Dürer had relatively little influence in Italy, where probably only his altarpiece in Venice was seen, and his German successors were less effective in blending German and Italian styles. His intense and self-dramatizing self-portraits have continued to have a strong influence up to the present, especially on painters in the 19th and 20th century who desired a more dramatic portrait style. Dürer has never fallen from critical favour, and there have been significant revivals of interest in his works in Germany in the "Dürer Renaissance" of about 1570 to 1630, in the early nineteenth century, and in German nationalism from 1870 to 1945.
Dürer's study of human proportions and the use of transformations to a coordinate grid to demonstrate facial variation inspired similar work by D'Arcy Thompson in his book "On Growth and Form".
The Lutheran Church remembers Dürer as a great Christian annually on April 6, along with Lucas Cranach the Elder and Hans Burgkmair. The liturgical calendar of the Episcopal Church (USA) remembers him, Cranach and Matthias Grünewald on August 5.
Theoretical works.
In all his theoretical works, in order to communicate his theories in the German language rather than in Latin, Dürer used graphic expressions based on a vernacular, craftsmen's language. For example, 'Schneckenlinie' ('snail-line') was his term for a spiral form. Thus, Dürer contributed to the expansion in German prose which Martin Luther had begun with his translation of the Bible.
"Four Books on Measurement".
Dürer's work on geometry is called the "Four Books on Measurement" ("Underweysung der Messung mit dem Zirckel und Richtscheyt" or "Instructions for Measuring with Compass and Ruler"). The first book focuses on linear geometry. Dürer's geometric constructions include helices, conchoids and epicycloids. He also draws on Apollonius, and Johannes Werner's 'Libellus super viginti duobus elementis conicis' of 1522.
The second book moves onto two dimensional geometry, i.e. the construction of regular polygons. Here Dürer favours the methods of Ptolemy over Euclid.
The third book applies these principles of geometry to architecture, engineering and typography.
In architecture Dürer cites Vitruvius but elaborates his own classical designs and columns. In typography, Dürer depicts the geometric construction of the Latin alphabet, relying on Italian precedent. However, his construction of the Gothic alphabet is based upon an entirely different modular system. The fourth book completes the progression of the first and second by moving to three-dimensional forms and the construction of polyhedra. Here Dürer discusses the five Platonic solids, as well as seven Archimedean semi-regular solids, as well as several of his own invention.
In all these, Dürer shows the objects as nets. Finally, Dürer discusses the Delian Problem and moves on to the 'construzione legittima', a method of depicting a cube in two dimensions through linear perspective. It was in Bologna that Dürer was taught (possibly by Luca Pacioli or Bramante) the principles of linear perspective, and evidently became familiar with the 'costruzione legittima' in a written description of these principles found only, at this time, in the unpublished treatise of Piero della Francesca. He was also familiar with the 'abbreviated construction' as described by Alberti and the geometrical construction of shadows, a technique of Leonardo da Vinci. Although Dürer made no innovations in these areas, he is notable as the first Northern European to treat matters of visual representation in a scientific way, and with understanding of Euclidean principles. In addition to these geometrical constructions, Dürer discusses in this last book of "Underweysung der Messung" an assortment of mechanisms for drawing in perspective from models and provides woodcut illustrations of these methods that are often reproduced in discussions of perspective.
"Four Books on Human Proportion".
Dürer's work on human proportions is called the "Four Books on Human Proportion" ("Vier Bücher von Menschlicher Proportion") of 1528. The first book was mainly composed by 1512/13 and completed by 1523, showing five differently constructed types of both male and female figures, all parts of the body expressed in fractions of the total height. Dürer based these constructions on both Vitruvius and empirical observations of, "two to three hundred living persons," in his own words. The second book includes eight further types, broken down not into fractions but an Albertian system, which Dürer probably learned from Francesco di Giorgio's 'De harmonica mundi totius' of 1525. In the third book, Dürer gives principles by which the proportions of the figures can be modified, including the mathematical simulation of convex and concave mirrors; here Dürer also deals with human physiognomy. The fourth book is devoted to the theory of movement.
Appended to the last book, however, is a self-contained essay on aesthetics, which Dürer worked on between 1512 and 1528, and it is here that we learn of his theories concerning 'ideal beauty'. Dürer rejected Alberti's concept of an objective beauty, proposing a relativist notion of beauty based on variety. Nonetheless, Dürer still believed that truth was hidden within nature, and that there were rules which ordered beauty, even though he found it difficult to define the criteria for such a code. In 1512/13 his three criteria were function ('Nutz'), naïve approval ('Wohlgefallen') and the happy medium ('Mittelmass'). However, unlike Alberti and Leonardo, Dürer was most troubled by understanding not just the abstract notions of beauty but also as to how an artist can create beautiful images. Between 1512 and the final draft in 1528, Dürer's belief developed from an understanding of human creativity as spontaneous or inspired to a concept of 'selective inward synthesis'. In other words, that an artist builds on a wealth of visual experiences in order to imagine beautiful things. Dürer's belief in the abilities of a single artist over inspiration prompted him to assert that "one man may sketch something with his pen on half a sheet of paper in one day, or may cut it into a tiny piece of wood with his little iron, and it turns out to be better and more artistic than another's work at which its author labours with the utmost diligence for a whole year."
List of works.
For lists of Albrecht Dürer's works, see:

</doc>
<doc id="2403" url="https://en.wikipedia.org/wiki?curid=2403" title="Australian rules football">
Australian rules football

Australian rules football, officially known as Australian football, also called football, footy, or Aussie rules (and in some regions marketed as AFL after the Australian Football League, the most popular and only fully professional Australian football league in the country), is a sport played between two teams of eighteen players on the field of either an Australian football ground, a modified cricket field, or a similarly sized sports venue.
The main way to score points is by kicking the ball between the two tall goal posts. The team with the higher total score at the end of the match wins unless a draw is declared.
During general play, players may position themselves anywhere on the field and use any part of their bodies to move the ball. The primary methods are kicking, handballing and running with the ball. There are rules on how the ball can be handled: for example, players running with the ball must intermittently bounce or touch it on the ground. Throwing the ball is not allowed and players must not get caught holding the ball. A distinctive feature of the game is the mark, where players anywhere on the field who catch a ball from a kick (with specific conditions) are awarded possession. Possession of the ball is in dispute at all times except when a "free kick" or mark is paid.
Australian football is a contact sport in which players can tackle using their hands or use their whole body to obstruct opponents. Dangerous physical contact (such as pushing an opponent in the back), interference when marking and deliberately slowing the play are discouraged with free kicks, distance penalties or suspension for a certain number of matches, depending on the seriousness of the infringement. Frequent physical contests, spectacular marking, fast movement of both players and the ball and high scoring are the game's main attributes.
The game's origins can be traced to football matches played in Melbourne in 1858. Australian football became codified in May 1859 when the first laws were published by the Melbourne Football Club.
Australian football has the highest spectator attendance of all sports in Australia. The sport is also played at amateur level in many countries and in several variations.
The most prestigious competition is the Australian Football League (AFL), culminating in the annual AFL Grand Final, currently the highest attended club championship event in the world. The rules of Australian football are governed by the AFL Commission with the advice of the AFL's Laws of the Game Committee.
History.
Origins.
There is documented evidence of "foot-ball" being played sporadically in the Australian colonies in the first half of the 19th century. While the exact rules of these games are unknown, they were most likely forms of folk football, and share no causal link with Australian rules football.
In 1858, public schools in Melbourne, Victoria, are first recorded organising football games modelled on precedents at English schools. The earliest recorded match, held on 15 June, was between Scotch College and Melbourne Grammar School on the St Kilda foreshore.
On 10 July 1858, the Melbourne-based "Bell's Life in Victoria and Sporting Chronicle" published a letter by Tom Wills, captain of the Victoria cricket team, calling for the formation of a "foot-ball club" with a "code of laws" to keep cricketers fit during winter. Born in Australia, Wills learnt a nascent form of rugby football whilst a pupil at Rugby School in the English Midlands, and returned to his homeland a star athlete and cricketer. His letter is regarded by many historians as giving impetus for the development of a new code of football today known as Australian football.
On 31 July, Wills' friend Jerry Bryant organised a scratch match at the Richmond Paddock adjoining the Melbourne Cricket Ground. Trees were used for goal posts and there were no boundaries and the match lasted from 1 p.m. until dark. There were no rules and fights frequently broke out. Melbourne being a relatively young city, the majority of the early players were migrants and the media of the time noted that participants of each nationality played the game their own distinctive way: some were "guided by their particular set of rules, others by no rules at all".
Another significant milestone in the sport's development was a match played under experimental rules between Melbourne Grammar School and Scotch College, held at the Richmond Paddock. This 40-a-side contest, umpired by Wills and Scotch College teacher John Macadam, began on 7 August and continued over two subsequent Saturdays, ending in a draw with each side kicking one goal. It is commemorated with a statue outside the Melbourne Cricket Ground, and the two schools have competed annually ever since in the Cordner-Eggleston Cup, the world's oldest continuous football competition.
The theory that Australian football was derived from Gaelic football emerged in the early 20th century, despite the fact that Australian football was codified almost 30 years before the Irish game. There is no archival evidence in favour of a Gaelic origin, and the style of play shared between the two modern codes was evident in Australia long before the Irish game evolved in a similar direction. Since the 1980s, the theory that Australian football comes from the Aboriginal game of Marn Grook has also gained attention. It is claimed that Tom Wills, growing up amongst Aborigines in Victoria, may have seen or played Marn Grook, and used elements from the game when formulating the laws of Australian football. This is purely speculative, and according to Wills biographer Greg de Moore, all research points to him "having been almost solely influenced by his experience at Rugby School".
First rules.
A loosely organised Melbourne side, captained by Tom Wills, played matches in the winter of 1858. The following year, on 14 May, the Melbourne Football Club officially came into being, making it the world's oldest professional football club. Three days after its formation, Wills and three other members—journalists W. J. Hammersley and J. B. Thompson, and teacher Thomas H. Smith—met at the Parade Hotel in East Melbourne, owned by Jerry Bryant, and drafted ten simple rules: "The Rules of the Melbourne Football Club". These are the earliest known laws of Australian rules football. The document was signed by Wills, Hammersley, Thompson, Smith, J. Sewell, Alex Bruce and T. Butterworth. Importantly, the rules were widely publicised and distributed. Having been codified in 1859, Australian football is older than any other major football code, including soccer (codified in 1863) and rugby union (codified in 1871).
Early competition in Victoria.
In 1859 several new football clubs formed including the Castlemaine Football Club, Geelong Football Club and the Melbourne University Football Club. While many one-off matches are recorded to have taken place between several early teams from Melbourne's suburbs and country Victoria (such as the Ballarat and Geelong competitions), in the early days many had not yet formed clubs for regular competition.
The first trophy for Australian Rules Football, the 1861 Challenge Cup, was won in 1862 under Melbourne's rules by University over Melbourne. The competition continued into the 1860s with the addition of other teams from Melbourne's suburbs. Two further competitions, the South Yarra Challenge Cup (which had evolved from the Caledonian Games) and "Second Twenties" were held in the 1860s and 1870s.
Some rival rules eventually gave way to an acceptance of the Melbourne rules. In 1860, the Melbourne Football Club redrafted its rules following the input of several other clubs. The requirement to bounce the ball while running was introduced in a significant redraft of the Melbourne rules in 1866 by H. C. A. Harrison and his rules committee to satisfy the Geelong Football Club's own set of very different rules. Behind posts, introduced at this time, are also believed to have come directly from the Geelong rules. The new rules became known as the Victorian rules, which became more widely adopted. In 1869, a 100-minute time limit was introduced to the game for the first time. Previous to this, winners were decided in a number of ways, but most commonly the first side to kick two goals.
The relationship with cricket primarily came out of co-existence and many of football's founders were cricketers. As a result, the sport shares some terminology (i.e. "umpires" and "boundary"). However cricket authorities did not initially allow football to be played on their grounds and in the early years football was played primarily in parks. The first football match played at the Melbourne Cricket Ground (MCG) was not until 1876. Cricket authorities soon saw the opportunity to capitalise on the rapid growth of Australian football, however, and soon most grounds in Victoria were expanded to accommodate the dual purpose, a situation that continues to this day.
Football matches between 1859 and 1899 were played in a 20-per-side format.
Spread to other colonies.
Gradually the game, known at first as "Melbourne rules" which became "Victorian rules" and then "Australian rules" or "Australasian rules," gained roots in other Australasian colonies—beginning with South Australia (1860), Tasmania (1864), Queensland (1866), and New Zealand (1871). In 1877, the sport's first governing bodies, the South Australian Football Association and the Victorian Football Association were formed on 30 April and 17 May respectively. The game began to be played in New South Wales in 1877, in Western Australia in 1881 (during the 1880s, young men sent to school in Adelaide brought the game home when they had finished their education—more came from the eastern states later in the Western Australian gold rush) and the Australian Capital Territory in 1911. By 1916, the game was first played in the Northern Territory, establishing a permanent presence in all Australian states and mainland territories.
Despite being the dominant code in Victoria, South Australia and Western Australia late in the century the code was beginning to decline in New South Wales and Queensland largely due to competition with other more popular football codes, as well as interstate rivalries and the lack of strong local governing bodies. In the case of Sydney, denial of access to grounds and the loss of professional players to other football codes directly inhibited to the game's growth.
The first intercolonial match had been played between Victoria and South Australia on 2 August 1879.
Emergence of the VFL.
In 1896, delegates from the stronger and wealthier VFA clubs—Carlton, Collingwood, Essendon, Fitzroy, Geelong, Melbourne, St Kilda and South Melbourne—met to form a breakaway competition and in 1897, the Victorian Football League (VFL), was born as an eight-team competition. Popularity of the VFL grew rapidly and by 1925 with 12 teams, had become the most prominent league in the game and would dominate so many aspects of the sport from that point on.
Effects of the two world wars.
Both World War I and World War II had a devastating effect on Australian football and on Australian sport in general. While scratch matches were played by Australian "diggers" in remote locations around the world, the game lost many of its great players to wartime service. Some competitions never fully recovered. Between 1914 and 1915 talks were held for a proposed amalgamation with rugby league, the predominant code of football in New South Wales and Queensland, resulting in a trial run. World War I saw the game in New Zealand go into recess for three quarters of a century. In Queensland, the state league went into recess for the duration of the war. VFL club University left the league and went into recess due to severe casualties. The WAFL lost two clubs and the SANFL was suspended for one year in 1916 due to heavy club losses. The ANZAC Day clash is one example of how the war continues to be remembered in the football community.
Interstate football and the Australian National Football Council.
The Australian National Football Council's primary role was to govern the game at national level to facilitate interstate representative and club competition.
The ANFC ran the Championship of Australia, the first national club competition, which commenced in 1888 and saw clubs from different states compete on an even playing field. During this time, the Port Adelaide won a record four national club championships. Although clubs from other states were at times invited, the final was almost always between the premiers from the two strongest state competitions of the time—South Australia and Victoria—and the majority of matches were played in Adelaide at the request of the SAFA/SAFL. The last match was played in 1976, with North Adelaide being the last non-Victorian winner in 1972. Between 1976 and 1987, the ANFC, and later the Australian Football Championships (AFC) ran a night series, which invited clubs and representative sides from around the country to participate in a knock-out tournament parallel to the premiership seasons, which Victorian sides still dominated.
With the lack of international competition, representative matches between state teams were regarded with great importance. Originating from the early intercolonial matches, these tests continued well after Federation of Australia in 1901 and the Australian Football Council co-ordinated regular interstate carnivals. In 1908, a Jubilee Australasian Football Carnival was held to celebrate 50 years of Australian Football. The carnival included teams representing Victoria, South Australia, Queensland, New South Wales, Tasmania, Western Australia and New Zealand. Due in part to Victoria recruiting some of the best players to play in the VFL, Victoria dominated interstate matches for three quarters of a century. Representative football was kept alive longer than the national club competition with the introduction of State of origin rules in 1977. The new rules mean that rather than representing the state of their adopted club, players would return to play for the state they were first recruited in. This instantly broke Victoria's stranglehold over state titles and Western Australia and South Australia began to win more of their games against Victoria. Both New South Wales and Tasmania scored surprise victories at home against Victoria in 1990.
Towards a national competition.
In 1978, the term Barassi Line was used to describe the dichotomy that existed in Australia's football culture, where Australian Football was the most popular code of football in all states except New South Wales and Queensland where rugby reigned supreme. This description prompted the first suggestions of regular interstate club competition and of overcoming the dichotomy and establishing a national league.
By 1980, the way the game was played had changed dramatically due to innovative coaching tactics, with the phasing out of many of the game's kicking styles and the increasing use of handball; while presentation was influenced by television.
In 1982, in a move that heralded big changes within the sport, one of the original VFL clubs and now struggling, South Melbourne, relocated to Sydney and became known as the Sydney Swans. In the late 1980s, due to the poor financial standing of many of the Victorian clubs, the VFL pursued a more national competition. Two more non-Victorian clubs, the West Coast Eagles and the Brisbane Bears, began playing in 1987. In their early years, the Sydney and Brisbane clubs struggled both on and off-field because the substantial TV revenues they generated by playing on a Sunday went to the VFL. To protect these revenues the VFL granted significant draft concessions and financial aid to keep them competitive. Each club was required to pay a licence fee which allowed the Victorian-based clubs to survive.
The league changed its name to the Australian Football League (AFL) following the 1989 season. In 1991, it gained its first South Australian team, the Adelaide Crows, which paid five million dollars to enter the AFL. During the next five years, two more non-Victorian teams, the Fremantle Dockers and Port Adelaide Power, joined the league. In 2011 and 2012, respectively, two new non-Victorian teams were added to the competition, the Gold Coast Suns and Greater Western Sydney Giants. The AFL, currently with 18 member clubs, is the sport's elite competition and the most powerful body and continues to seek further opportunities to expand into new markets.
Following the emergence of the Australian Football League, the SANFL, WAFL and other state leagues rapidly declined to a secondary status. Apart from these there are many semi-professional and amateur leagues around Australia, where they play a very important role in the community, and particularly so in rural areas. The VFA, still in existence a century after the original schism, merged with the former VFL reserves competition in 1998. The new entity adopted the VFL name and remained a primarily state-based competition.
State of origin games also declined in importance, especially after an increasing number of withdrawals by AFL players. The AFL turned its focus for representation to an annual International Rules Series against Ireland in 1998 before abolishing State of Origin in 1999. The second-tier state and territorial leagues still contest interstate representative matches.
Although a Tasmanian AFL Bid has been ongoing, the AFL's recent focus has been gaining market share in lucrative broadcasting rights in two populous Australian states, helped by introducing clubs on the Gold Coast and in Greater Western Sydney. The AFL regularly schedules pre-season exhibition matches in all Australian states and territories as part of the Regional Challenge.
The AFL has signalled further expansion by scheduling a competition match in New Zealand 2013 and up to three competition matches in 2014 played in Wellington.
Laws of the game.
Field.
Both the ball and the field of play are oval in shape. No more than 18 players of each team are permitted to be on the field at any time.
Up to three interchange (reserve) players may be swapped for those on the field at any time during the game. In Australian rules terminology, these players wait for substitution "on the bench"—an area with a row of seats on the sideline. Players must interchange through a designated interchange "gate" with strict penalties for too many players from one team on the field. In addition, some leagues like the AFL have each team designate one player as a substitute who can be used to make a single permanent exchange of players during a game.
There is no offside rule nor are there set positions in the rules; unlike many other forms of football, players from both teams may disperse across the whole field before the start of play. However, a typical on-field structure consists of six forwards, six defenders or "backmen" and six midfielders, usually two wingmen, one centre and three followers, including a ruckman, ruck-rover and rover. Only four players from each team are allowed within the centre square () at every centre bounce, which occurs at the commencement of each quarter, and to restart the game after a goal is scored. There are also other rules pertaining to allowed player positions during set plays (that is, after a mark or free kick) and during kick-ins following the scoring of a behind.
Match duration.
A game consists of four quarters and a timekeeper officiates their duration. At professional level quarters consist of 20 minutes of play, with the clock being stopped for instances such as scores, the ball going out of play or at the umpire's discretion. The umpire signals "time-off" to stop the clock for various reasons, such as the player in possession being tackled to the ground and leading to stagnant play as neither side can recover the ball. Time resumes when the umpire signals "time-on" or when the ball is brought into play. Such stoppages generally lead to quarters being extended by between five and ten minutes. The official game clock is only known on the field by the timekeepers. Official game time is not displayed to the players or the public; the only knowledge they have of time is when sirens sound to mark the beginning and end of each quarter. Official time may be approximated by broadcasters to display to television audiences. Teams change ends at the end of each quarter; umpires change ends at half time.
General play.
Games are officiated by umpires. Before the game, the winner of a coin toss determines which directions the teams will play to begin. Australian football begins after the first siren, when the umpire bounces the ball on the ground (or throws it into the air if the condition of the ground is poor), and the two ruckmen (typically the tallest players from each team) battle for the ball in the air on its way back down. This is known as the "ball-up". Certain disputes during play may also be settled with a "ball-up" from the point of contention. If the ball ever goes out of bounds (beyond the oval boundary line around the edge of the field), a boundary umpire will stand with his back to the infield and return the ball into play with a "throw-in", a high backwards toss back into the field of play.
The ball can be propelled in any direction by way of a foot, clenched fist (called a handball or "handpass") or open-hand tap but it cannot be thrown under any circumstances. Once a player takes possession of the ball he must dispose of it by either kicking or handballing it. Any other method of disposal is illegal and will result in a free kick to the opposing team. This is usually called "incorrect disposal", "dropping the ball" or "throwing". If the ball is not in the possession of one player it can be moved on with any part of the body.
A player may run with the ball, but it must be bounced or touched on the ground at least once every 15 metres. Opposition players may bump or tackle the player to obtain the ball and, when tackled, the player must dispose of the ball cleanly or risk being penalised for holding the ball. The ball carrier may only be tackled between the shoulders and knees. If the opposition player forcefully contacts a player in the back while performing a tackle, the opposition player will be penalised for a push in the back. If the opposition tackles the player with possession below the knees (a "low tackle" or a "trip") or above the shoulders (a "high tackle"), the team with possession of the football gets a free kick.
If a player takes possession of the ball that has travelled more than from another player's kick, by way of a catch, it is claimed as a "mark" (meaning that the game stops while he prepares to kick from the point at which he marked). Alternatively, he may choose to "play on" forfeiting the set shot in the hope of pressing an advantage for his team (rather than allowing the opposition to reposition while he prepares for the free kick). Once a player has chosen to play on, normal play resumes and the player who took the mark is again able to be tackled.
There are different styles of kicking depending on how the ball is held in the hand. The most common style of kicking seen in today's game, principally because of its superior accuracy, is the drop punt, where the ball is dropped from the hands down, almost to the ground, to be kicked so that the ball rotates in a reverse end over end motion as it travels through the air. Other commonly used kicks are the torpedo punt (also known as the spiral, barrel, or screw punt), where the ball is held flatter at an angle across the body, which makes the ball spin around its long axis in the air, resulting in extra distance (similar to the traditional motion of an American football punt), and the checkside punt or "banana", kicked across the ball with the outside of the foot used to curve the ball (towards the right if kicked off the right foot) towards targets that are on an angle. There is also the "snap", which is almost the same as a checkside punt except that it is kicked off the inside of the foot and curves in the opposite direction. It is also possible to kick the ball so that it bounces along the ground. This is known as a "grubber". Grubbers can bounce in a straight line, or curve to the left or right.
Apart from free kicks, marks or when the ball is in the possession of an umpire for a "ball up" or "throw in", the ball is always in dispute and any player from either side can take possession of the ball.
Scoring.
A "goal", worth 6 points, is scored when the football is propelled through the goal posts at any height (including above the height of the posts) by way of a kick from the attacking team. It may fly through "on the full" (without touching the ground) or bounce through, but must not have been touched, on the way, by any player from either team. A goal cannot be scored from the foot of an opposition (defending) player.
A "behind", worth 1 point, is scored when the ball passes between a goal post and a behind post at any height, or if the ball hits a goal post, or if any player sends the ball between the goal posts by touching it with any part of the body other than a foot. A behind is also awarded to the attacking team if the ball touches any part of an opposition player, including his foot, before passing between the goal posts. When an opposition player deliberately scores a behind for the attacking team (generally as a last resort, because of the risk of their scoring a goal) this is termed a rushed behind. Before the start of the 2009 season, there was no additional penalty imposed for rushing a behind, compared to any other behind. However, for the start of the 2009 season a new rule was announced awarding a free kick against any player who deliberately rushes a behind.
The goal umpire signals a goal with two hands pointed forward at elbow height, or a behind with one hand. The umpire then confirms the signal with the other goal umpire by waving flags above their heads.
The team that has scored the most points at the end of play wins the game. If the scores are level on points at the end of play, then the game is a draw; extra time applies only during finals matches in some competitions.
As an example of a score report, consider a match between and . Collingwood's score of 16 goals and 12 behinds equates to 108 points. St Kilda's score of 7 goals and 10 behinds equates to a 52-point tally. Collingwood wins the match by a margin of 56 points. Such a result would be written as:
And said:
Additionally, it can be said that:
The home team is typically listed first and the visiting side is listed second. The scoreline is written with respect to the home side.
For example, won in successive weeks, once as the home side and once as the visiting side. These would be written out thus:
Structure and competitions.
The "football season" proper is from March to August (early autumn to late winter in Australia) with finals being held in September and October. In the tropics, the game is sometimes played in the wet season (October to March). Pre-season competitions in southern Australia usually begin in late February.
The AFL is recognised by the Australian Sports Commission as being the National Sporting Organisation for Australian Football. There are also seven state/territory-based organisations in Australia, most of which are now either owned by or affiliated to the AFL.
Most of these hold annual semi-professional club competitions while the others oversee more than one league. Local semi-professional or amateur organisations and competitions are often affiliated to their state organisations.
The AFL is the "de facto" world governing body for Australian football. There are also a number of affiliated organisations governing amateur clubs and competitions around the world.
For almost all Australian football club competitions the aim is to win the "Premiership". The premiership is always decided by a "finals series". The teams that occupy the highest positions on the "ladder" after the "home-and-away" season play off in a "semi-knockout" finals series, culminating in a single Grand Final match to determine the premiers. Typically between four and eight teams contest the finals series. The team which finishes first on the ladder after the home-and-away season is referred to as a "minor premier", but this usually holds little stand-alone significance, other than receiving a better draw in the finals.
Many suburban and amateur leagues have a sufficient number of teams to be played across several tiered divisions, with promotion of the lower division premiers and relegation of the upper division's last placed team at the end of each year. At present, none of the top level national or state level leagues in Australia are large enough to warrant this structure.
Women's Australian football.
The level of interest shown by women in Australian football in considered unique among the world's football codes. It was the case in the 19th-century, as it is in modern times, that women made up approximately half of crowds at Australian football matches—a far greater proportion than association football and the two rugby codes. This has been attributed in part to the egalitarian character of Australian football's origins in public parks where women could mingle freely and support the game in various ways.
As of 2015, over 280,000 females participate in the game across Australia. The AFL Women's National Championships is the premier competition for women's Australian football. On the back of the inaugural AFL Women's Draft in 2013 and a series of exhibition matches at the MCG, the AFL is set to establish a semi-professional, nationally televised women's league competition by 2020. Gillon McLachlan, CEO of the AFL, stated that he would like the push the league's founding date to 2017.
Australian football internationally.
Australian football is played at an amateur level in various countries around the world. Over 50 countries are home to clubs or leagues who play regularly. Twenty countries have participated in the Euro Cup and 22 countries have participated in the International Cup with both competitions prohibiting Australian players. Over 20 countries have either affiliation or working agreements with the AFL. There have been several players in the VFL/AFL who were born outside Australia and since 1982, an increasing number of players have been recruited from outside Australia through initiatives such as the Irish experiment and more recently, international scholarship programs.
In the late 19th and early 20th centuries, the game spread with the Australian diaspora to areas such as New Zealand and South Africa; however this growth went into rapid decline following World War I. After World War II, the sport experienced a small amount of growth in the Pacific region, particularly in Nauru, where Australian football is the national sport, as well as Papua New Guinea and New Zealand.
Most of the current amateur clubs and leagues in existence have developed since the 1980s, when leagues began to be established in North America, Europe and Asia. The sport developed a cult following in the United States when matches were broadcast on ESPN in the late 1980s. As the size of the Australian diaspora has increased, so has the number of clubs outside Australia. This expansion has been further aided by multiculturalism and assisted by exhibition matches as well as exposure generated through players who have converted to and from other football codes. In Papua New Guinea, New Zealand, South Africa, Canada, and the United States there are many thousands of players.
Prince Charles is the Patron of AFL Europe. In 2013, participation across AFL Europe's 21 member nations was more than 5,000 players, the majority of which are European nationals rather than Australian expats. The sport also has a growing presence in India.
The AFL became the de facto governing body when it pushed for the closure of the International Australian Football Council in 2002. The Australian Football International Cup is currently the highest level of senior international competition.
International rules football.
Since 1967 there have been many matches between Australian football teams (mainly from Australia) and Gaelic football teams (mainly from Ireland), under various sets of hybrid, compromise rules known as International rules football. In 1984, the first official representative matches of International Rules were played, and these were played annually each October between the AFL and the Gaelic Athletic Association, also known as the GAA, between 1998 and 2006. These were part of the official International Rules Series, which attracted large crowds and media interest in Ireland and Australia. In 2007 the international rules series was abandoned because of the aggression and the severity of the Australian team in the previous year, but in 2008, under new rules to protect the player with the ball, it resumed in Australia.
Cultural impact and popularity.
Australian football is a sport rich in tradition and Australian cultural references, especially surrounding the rituals of gameday for players, officials and supporters.
Australian football has been an inspiration for writers and poets including Manning Clarke, Bruce Dawe and Philip Hodgins. Paintings by Arthur Streeton ("The National Game", 1889) and Sidney Nolan ("Footballer", 1946) helped to establish Australian football as a serious subject for artists. Many Aboriginal artists have explored the game, often fusing it with the mythology of their region. Statues of Australian football identities can be found throughout the country. In cartooning, WEG's VFL/AFL premiership posters—inaugurated in 1954—have achieved iconic status among Australian football fans. Dance sequences based on Australian football feature heavily in Robert Helpmann's 1964 ballet "The Display", his first and most famous work for the Australian Ballet. The game has also inspired well-known plays such as "And the Big Men Fly" (1963) by Alan Hopgood and David Williamson's "The Club" (1977), which was adapted into a 1980 film by director Bruce Beresford. Mike Brady's 1979 hit "Up There Cazaly" is considered an Australian football anthem, and references to the sport can be found in works by popular musicians, from singer-songwriter Paul Kelly to the alternative rock band TISM. Many Australian football video games have been released, most notably the AFL series.
Australian football has attracted more overall interest among Australians (as measured by the Sweeney Sports report) than any other football code, and, when compared with all sports throughout the nation, has consistently ranked first in the winter reports, and most recently third behind cricket and swimming in summer.
In 2006, 615,549 registered participants played Australian football in Australia. Participation increased 7.84% between 2005 and 2006. The Australian Sports Commission statistics show a 64% increase in the total number of participants over the 10-year period between 2001 and 2010. In 2008 there were 35,000 people in 32 countries playing in structured competitions of Australian football outside of Australia.
Many related games have emerged from Australian football, mainly with variations of contact to encourage greater participation. These include kick-to-kick (and its variants end-to-end footy and marks up), Auskick, rec footy, 9-a-side footy, masters Australian football, handball and longest-kick competitions. Players outside of Australia sometimes engage in related games adapted to available fields, like metro footy (played on gridiron fields) and Samoa rules (played on rugby fields).
Australian Football Hall of Fame.
For the centenary of the VFL/AFL in 1996, the Australian Football Hall of Fame was established. In that year 136 identities were inducted, including 100 players, 10 coaches, 10 umpires, 10 administrators and six media representatives.
The elite "Legend" status was bestowed on 12 members of the Hall of Fame in 1996: Ron Barassi, Haydn Bunton Senior, Roy Cazaly, John Coleman, Jack Dyer, Polly Farmer, Leigh Matthews, John Nicholls, Bob Pratt, Dick Reynolds, Bob Skilton and Ted Whitten (see above list for further details).
The following thirteen members have been promoted to the status of "Legend" since 1996: Ian Stewart (1997), Gordon Coventry (1998), Peter Hudson (1999), Kevin Bartlett (2000), Barrie Robran (2001), Bill Hutchison (2003), Jock McHale (2005), Darrel Baldock (2006), Norm Smith (2007), Alex Jesaulenko (2008), Kevin Murray (2010), Barry Cable (2012), and Tony Lockett (2015).

</doc>
<doc id="2405" url="https://en.wikipedia.org/wiki?curid=2405" title="Aon (company)">
Aon (company)

Aon plc is a British multinational corporation headquartered in London, United Kingdom, that provides risk management, insurance and reinsurance brokerage, investment banking, human resource solutions and outsourcing services. Aon has approximately 500 offices worldwide, serving 120 countries with 65,000 employees.
In 2011, Aon was ranked as the largest insurance broker in the world based on revenue. Aon was the principal partner and global shirt sponsor of the Premier League team Manchester United F.C. from 2010 until 2014.
Aon was created in 1982, when the Ryan Insurance Group merged with the Combined Insurance Company of America. In 1987, that company was renamed Aon, a Gaelic word meaning .
In January 2012, Aon announced that its headquarters would be moved to London.
Corporate overview.
Aon is a global professional services firm that advises clients on the topics of risk and people. The company is a provider of risk management, insurance and reinsurance brokerage, human resource solutions and outsourcing services.
Aon is divided into three business units that each specialize in a particular line of business. The firm's risk management business, Aon Risk Solutions provides retail property/casualty, liability, and other insurance products for groups and businesses, as well as risk management services. Its reinsurance business, Aon Benfield, specializes in reinsurance brokerage and investment banking & capital advisory (through Aon Securities Inc.). The firm's human resource solutions business, Aon Hewitt, provides consulting and outsourcing services to clients.
History.
W. Clement Stone's mother bought a small Detroit insurance agency, and in 1918 brought her son into the business. Mr. Stone sold low-cost, low-benefit accident insurance, underwriting and issuing policies on-site. The next year he founded his own agency, the Combined Registry Co.
As the Great Depression began, Stone reduced his workforce and improved training. Forced by his son's respiratory illness to winter in the South, Stone moved to Arkansas and Texas. In 1939 he bought American Casualty Insurance Co. of Dallas, Texas. It was consolidated with other purchases as the Combined Insurance Co. of America in 1947. The company continued through the 1950s and 1960s, continuing to sell health and accident policies. In the 1970s, Combined expanded overseas despite being hit hard by the recession.
In 1982, after 10 years of stagnation under Clement Stone Jr., the elder Stone, then 79, resumed control until the completion of a merger with Ryan Insurance Co. allowed him to transfer control to Patrick Ryan. Ryan, the son of a Ford dealer in Wisconsin, had started his company as an auto credit insurer in 1964. In 1976, the company bought the insurance brokerage units of the Esmark conglomerate. Ryan focused on insurance brokering and added more upscale insurance products. He also trimmed staff and took other cost-cutting measures, and in 1987 he changed Combined's name to Aon. In 1992, he bought Dutch insurance broker Hudig-Langeveldt. In 1995, the company sold its remaining direct life insurance holdings to General Electric to focus on consulting. The following year, it began offering hostile takeover insurance policies to small and mid-sized companies.
Aon built a global presence through purchases. In 1997, it bought The Minet Group, as well as insurance brokerage Alexander & Alexander Services, Inc. in a deal that made Aon (temporarily) the largest insurance broker worldwide. The firm made no US buys in 1998, but doubled its employee base with purchases including Spain's largest retail insurance broker, Gil y Carvajal, and the formation of Aon Korea, the first non-Korean firm of its kind to be licensed there.
Responding to industry demands, Aon announced its new fee disclosure policy in 1999, and the company reorganised to focus on buying personal line insurance firms and to integrate its acquisitions. That year it bought Nikols Sedgwick Group, an Italian insurance firm, and formed RiskAttack (with Zurich US), a risk analysis and financial management concern aimed at technology companies. The cost of integrating its numerous purchases, however, hammered profits in 1999.
Despite its troubles, in 2000 Aon bought Reliance Group's accident and health insurance business, as well as Actuarial Sciences Associates, a compensation and employee benefits consulting company. Later in that year, however, the company decided to cut 6% of its workforce as part of a restructuring effort. In 2003, the company saw revenues increase primarily because of rate hikes in the insurance industry. Also that year, Endurance Specialty, a Bermuda-based underwriting operation that Aon helped to establish in November 2001 along with other investors, went public. The next year Aon sold most of its holdings in Endurance.
In late 2007, Aon announced the divestiture of its underwriting business. With this move, the firm sold off its two major underwriting subsidiaries: Combined Insurance Company of America (acquired by ACE Limited for $2.4 billion) and Sterling Life Insurance Company (purchased by Munich Re Group for $352 million). The low margin and capital-intensive nature of the underwriting industry was the primary reason for the firm's decision to divest. Upon completion of the move, Aon turned its attention to expanding its broking and consulting capabilities.
This growth strategy manifested in November 2008 when Aon announced it had acquired reinsurance intermediary and capital advisor Benfield Group Limited for $1.75 billion. The acquisition amplified the firm's broking capabilities, positioning Aon one of the largest players in the reinsurance brokerage industry.
In 2010, Aon made its most significant acquisition to date with the purchase of Lincolnshire, Illinois-based Hewitt Associates for $4.9 billion. Aside from drastically boosting Aon's human resources consulting capacity and entering the firm into the business process outsourcing industry, the move added 23,000 colleagues and more than $3 billion in revenue.
September 11 attacks.
Aon's New York offices were on the 92nd and 98th–105th floors of the South Tower of the World Trade Center at the time of the 11 September 2001 terrorist attack. When the North Tower was struck at 8:46 a.m., many executives began evacuating their employees from the upper floors of the South Tower. The evacuation of Aon's offices, ordered by Eric Eisenberg, was carried out quickly as 924 of the estimated 1,100 Aon employees present at the time managed to evacuate the building before United Airlines Flight 175 struck it twenty stories below them at 9:03 a.m.
However, many were influenced to stay by security guards and security announcements, or did not exit the building in time. As a result, 175 employees of Aon were killed in the attacks, including Eisenberg and Kevin Cosgrove, a vice-president of the company, who made a call to 911 when the tower collapsed at 9:59 a.m.
Spitzer investigation.
In 2004–2005, Aon, along with other brokers including Marsh & McLennan and Willis, fell under regulatory investigation under New York Attorney General Eliot Spitzer and other state attorneys general. At issue was the practice of insurance companies' payments to brokers (known as contingent commissions). The payments were thought to bring a conflict of interest, swaying broker decisions on behalf of carriers, rather than customers. In the spring of 2005, without acknowledging any wrongdoing, Aon agreed to a $190 million settlement, payable over 30 months.
UK regulatory breach.
In January 2009, Aon was fined £5.25 million in the UK by the Financial Services Authority, who stated that the fine related to the company's inadequate bribery and corruption controls, claiming that between 14 January 2005 and 30 September 2007 Aon had failed to properly assess the risks involved in its dealings with overseas firms and individuals. The Authority did not find that any money had actually made its way to illegal organisations. Aon qualified for a 30% discount on the fine as a result of its co-operation with the investigation. Aon said its conduct was not deliberate, adding it had since "significantly strengthened and enhanced its controls around the usage of third parties".
US Foreign Corrupt Practices Act violations.
In December 2011, Aon Corporation paid a $16.26 million penalty to the US Securities and Exchange Commission (SEC) and the US Department of Justice (DOJ) for violations of the US Foreign Corrupt Practices Act (FCPA).
According to the SEC, Aon's subsidiaries made improper payments of over $3.6 million to government officials and third-party facilitators in Costa Rica, Egypt, Vietnam, Indonesia, the United Arab Emirates, Myanmar and Bangladesh, between 1983 and 2007, to obtain and retain insurance contracts.
Major acquisitions.
On 16 June 2014, Aon announced that it agreed to buy National Flood Services, Inc., a leading processor of flood insurance, from Stoneriver Group, L.P.
On 22 October 2012, Aon announced that it agreed to buy OmniPoint, Inc, a Workday consulting firm. Financial terms were not disclosed.
On 19 July 2011, Aon announced that it bought Westfield Financial Corp., the owner of insurance-industry consulting firm Ward Financial Group, from Ohio Farmers Insurance Co. Financial terms were not disclosed.
On 7 April 2011, Aon announced that it had acquired Johannesburg, South Africa-based Glenrand MIB. Financial terms were not disclosed.
On 12 July 2010, Aon announced that it had agreed to buy Lincolnshire, Illinois-based Hewitt Associates for $4.9 billion in cash and stock.
On 5 Mar 2010, Hewitt Associates announced that it acquired Senior Educators Ltd. The acquisition offers companies a new way to address retiree medical insurance commitments.
On 22 August 2008, Aon announced that it had acquired London-based Benfield Group. The acquiring price was US$1.75 billion or £935 million, with US$170 million of debt.
Operations.
Manchester United.
On 3 June 2009, it was reported that Aon had signed a four-year shirt sponsorship deal with English football giant Manchester United. On 1 June 2010, Aon replaced American insurance company AIG as the principal sponsor of the club. The Aon logo was prominently displayed on the front of the club's shirts until the 2014/2015 season when Chevrolet replaced them. The deal was said to be worth £80 million over four years, replacing United's deal with AIG as the most lucrative shirt deal in history at the time.
In April 2013, Aon signed a new eight-year deal with Manchester United to rename their training ground as the Aon Training Complex and sponsor the club's training kits, reportedly worth £180 million to the club.

</doc>
<doc id="2406" url="https://en.wikipedia.org/wiki?curid=2406" title="Alban Berg">
Alban Berg

Alban Maria Johannes Berg (; ; February 9, 1885 – December 24, 1935) was an Austrian composer of the Second Viennese School. His compositional style combined Romantic lyricism with twelve-tone technique.
Biography.
Early life.
Berg was born in Vienna, the third of four children of Johanna and Conrad Berg. His family lived comfortably until the death of his father in 1900.
Education.
He was more interested in literature than music as a child and did not begin to compose until he was fifteen, when he started to teach himself music. In late February or early March 1902 he fathered a child with Marie Scheuchl, a servant girl in the Berg family household. His daughter, Albine, was born on December 4, 1902.
Berg had little formal music education before he became a student of Arnold Schoenberg in October 1904. With Schoenberg he studied counterpoint, music theory, and harmony. By 1906, he was studying music full-time; by 1907, he began composition lessons. His student compositions included five drafts for piano sonatas. He also wrote songs, including his "Seven Early Songs" ("Sieben Frühe Lieder"), three of which were Berg's first publicly performed work in a concert that featured the music of Schoenberg's pupils in Vienna that year. The early sonata sketches eventually culminated in Berg's Piano Sonata, Op. 1 (1907–1908); it is one of the most formidable "first" works ever written. Berg studied with Schoenberg for six years until 1911. Berg admired him as a composer and mentor, and they remained close lifelong friends.
Among Schoenberg's teaching was the idea that the unity of a musical composition depends upon all its aspects being derived from a single basic idea; this idea was later known as "developing variation". Berg passed this on to his students, one of whom, Theodor W. Adorno, stated: "The main principle he conveyed was that of variation: everything was supposed to develop out of something else and yet be intrinsically different". The Piano Sonata is an example—the whole composition is derived from the work's opening quartal gesture and its opening phrase.
Innovation.
Berg was a part of Vienna's cultural elite during the heady "fin de siècle" period. His circle included the musicians Alexander von Zemlinsky and Franz Schreker, the painter Gustav Klimt, the writer and satirist Karl Kraus, the architect Adolf Loos, and the poet Peter Altenberg. In 1906, Berg met the singer Helene Nahowski, daughter of a wealthy family (said by some to be in fact the illegitimate daughter of Emperor Franz Joseph I of Austria from his liaison with Anna Nahowski); despite the outward hostility of her family, the two were married on May 3, 1911.
In 1913, two of Berg's "Five Songs on Picture Postcard Texts by Peter Altenberg" (1912) were premièred in Vienna, conducted by Schoenberg in the infamous "Skandalkonzert". Settings of aphoristic poetic utterances, the songs are accompanied by a very large orchestra. The performance caused a riot, and had to be halted. This was a crippling blow to Berg's self-confidence: he effectively withdrew the work, which is surely one of the most innovative and assured first orchestral compositions in the literature, and it was not performed in full until 1952. The full score remained unpublished until 1966.
From 1915 to 1918, Berg served in the Austro-Hungarian Army and during a period of leave in 1917 he accelerated work on his first opera, "Wozzeck". After the end of World War I, he settled again in Vienna, where he taught private pupils. He also helped Schoenberg run his Society for Private Musical Performances, which sought to create the ideal environment for the exploration and appreciation of unfamiliar new music by means of open rehearsals, repeat performances, and the exclusion of professional critics.
Berg had a particular interest in the number 23, using it to structure several works. Various suggestions have been made as to the reason for this interest: that he took it from the Biorhythms theory of Wilhelm Fliess, in which a 23-day cycle is considered significant, or because he first suffered an asthma attack on 23rd of the month.
Success of "Wozzeck" and inception of "Lulu" 1925-29.
Three excerpts from "Wozzeck" were performed in 1924, and this brought Berg his first public success. The opera, which Berg completed in 1922, was first performed on December 14, 1925, when Erich Kleiber conducted the first performance in Berlin. Today "Wozzeck" is seen as one of the century's most important works. Berg made a start on his second opera, the three act "Lulu", in 1928 but interrupted the work in 1929 for the concert aria "Der Wein" which he completed that summer. "Der Wein" presaged "Lulu" in a number of ways, including vocal style, orchestration, design and text.
Other well-known Berg compositions include the "Lyric Suite" (1926), which was later shown to employ elaborate cyphers to document a secret love affair; the post-Mahlerian "Three Pieces for Orchestra" (completed in 1915 but not performed until after "Wozzeck"); and the "Chamber Concerto" ("Kammerkonzert", 1923–25) for violin, piano, and 13 wind instruments: this latter is written so conscientiously that Pierre Boulez has called it "Berg's strictest composition" and it, too, is permeated by cyphers and posthumously disclosed hidden programs.
Final years 1930-35.
Life for the musical world was becoming increasingly difficult in the 1930s both in Vienna and Germany due to the rising tide of antisemitism and the Nazi cultural ideology that denounced modernity. Even to have an association with someone who was Jewish could lead to denunciation, and Berg's "crime" was to have studied with the Jewish composer Arnold Schoenberg. Berg found that opportunities for his work to be performed in Germany were becoming rare, and eventually his music was proscribed and placed on the list of degenerate music. In 1932 Berg and his wife acquired an isolated lodge, the "Waldhaus" on the southern shore of the "Wörthersee", near Schiefling am See in Carinthia, where he was able to work in seclusion, mainly on Lulu and the Violin Concerto. At the end of 1934 Berg became involved in the political intrigues around finding a replacement for Clemens Krauss as director of the Vienna State Opera. As more of the performances of his work in Germany were cancelled by the Nazis, who had come to power in early 1933, he needed to ensure the new director would be an advocate for modernist music. Originally the premiere of Lulu had been planned for the Berlin State Opera, where Erich Kleiber continued to champion his music and had conducted the premiere of "Wozzeck" in 1925, but now this was looking increasingly uncertain, and Lulu was rejected by the Berlin authorities in the spring of 1934. Kleiber's production of the Lulu symphonic suite on 30 November 1934 in Berlin was also the occasion of his resignation in protest at the extent of conflation of culture with politics. Even in Vienna, the opportunities for the Vienna School of musicians was dwindling.
Berg had interrupted the orchestration of "Lulu" because of an unexpected (and financially much-needed) commission from the Russian-American violinist Louis Krasner for a Violin Concerto (1935). This profoundly elegiac work, composed at unaccustomed speed and posthumously premièred, has become Berg's best-known and beloved composition. Like much of his mature work, it employs an idiosyncratic adaptation of Schoenberg's "dodecaphonic" or twelve-tone technique, that enables the composer to produce passages openly evoking tonality, including quotations from historical tonal music, such as a Bach chorale and a Carinthian folk song. The Violin Concerto was dedicated "to the memory of an Angel", Manon Gropius, the deceased daughter of architect Walter Gropius and Alma Mahler.
Berg died in Vienna, on Christmas Eve 1935, from blood poisoning apparently caused by an insect-sting-induced carbuncle on his back that occurred in November. He was 50 years old.
Aftermath.
Berg completed the orchestration of only the first two acts of "Lulu" before he died. The first two acts were successfully premièred in Zürich in 1937, but for personal reasons Helene Berg subsequently imposed a ban on any attempt to "complete" the final act, which Berg had in fact completed in particell (short score) format. An orchestration was therefore commissioned in secret from Friedrich Cerha and premièred in Paris (under Pierre Boulez) only in 1979, soon after Helene Berg's own death. The complete opera has rapidly entered the repertoire as one of the landmarks of contemporary music and, like "Wozzeck", remains a consistent audience draw.
Legacy.
Berg is remembered as one of the most important composers of the 20th century and to date is the most widely performed opera composer among the Second Viennese School. He is considered to have brought more "human values" to the twelve-tone system, his works seen as more "emotional" than Schoenberg's. Critically he is seen to have preserved the Viennese tradition in his music. His popularity has been more easily secured than many other Modernists since he plausibly combined both Romantic and Expressionist idioms. Though Berg's Romanticism at one time seemed a drawback for some more modernist composers, the Berg scholar Douglas Jarman writes in the New Grove: "As the 20th century closed, the 'backward-looking' Berg suddenly came as eorg Perle remarked, to look like its most forward-looking composer."

</doc>
<doc id="2408" url="https://en.wikipedia.org/wiki?curid=2408" title="Analytical chemistry">
Analytical chemistry

Analytical chemistry is the study of the separation, identification, and quantification of the chemical components of natural and artificial materials. Qualitative analysis gives an indication of the identity of the chemical species in the sample, and quantitative analysis determines the amount of certain components in the substance. The separation of components is often performed prior to analysis.
Analytical methods can be separated into classical and instrumental. Classical methods (also known as wet chemistry methods) use separations such as precipitation, extraction, and distillation and qualitative analysis by color, odor, or melting point. Classical quantitative analysis is achieved by measurement of weight or volume. Instrumental methods use an apparatus to measure physical quantities of the analyte such as light absorption, fluorescence, or conductivity. The separation of materials is accomplished using chromatography, electrophoresis or field flow fractionation methods.
Analytical chemistry is also focused on improvements in experimental design, chemometrics, and the creation of new measurement tools to provide better chemical information. Analytical chemistry has applications in forensics, bioanalysis, clinical analysis, environmental analysis, and materials analysis.
History.
Analytical chemistry has been important since the early days of chemistry, providing methods for determining which elements and chemicals are present in the object in question. During this period significant contributions to analytical chemistry include the development of systematic elemental analysis by Justus von Liebig and systematized organic analysis based on the specific reactions of functional groups.
The first instrumental analysis was flame emissive spectrometry developed by Robert Bunsen and Gustav Kirchhoff who discovered rubidium (Rb) and caesium (Cs) in 1860.
Most of the major developments in analytical chemistry take place after 1900. During this period instrumental analysis becomes progressively dominant in the field. In particular many of the basic spectroscopic and spectrometric techniques were discovered in the early 20th century and refined in the late 20th century.
The separation sciences follow a similar time line of development and also become increasingly transformed into high performance instruments. In the 1970s many of these techniques began to be used together to achieve a complete characterization of samples.
Starting in approximately the 1970s into the present day analytical chemistry has progressively become more inclusive of biological questions (bioanalytical chemistry), whereas it had previously been largely focused on inorganic or small organic molecules. Lasers have been increasingly used in chemistry as probes and even to start and influence a wide variety of reactions. The late 20th century also saw an expansion of the application of analytical chemistry from somewhat academic chemical questions to forensic, environmental, industrial and medical questions, such as in histology.
Modern analytical chemistry is dominated by instrumental analysis. Many analytical chemists focus on a single type of instrument. Academics tend to either focus on new applications and discoveries or on new methods of analysis. The discovery of a chemical present in blood that increases the risk of cancer would be a discovery that an analytical chemist might be involved in. An effort to develop a new method might involve the use of a tunable laser to increase the specificity and sensitivity of a spectrometric method. Many methods, once developed, are kept purposely static so that data can be compared over long periods of time. This is particularly true in industrial quality assurance (QA), forensic and environmental applications. Analytical chemistry plays an increasingly important role in the pharmaceutical industry where, aside from QA, it is used in discovery of new drug candidates and in clinical applications where understanding the interactions between the drug and the patient are critical.
Classical methods.
Although modern analytical chemistry is dominated by sophisticated instrumentation, the roots of analytical chemistry and some of the principles used in modern instruments are from traditional techniques many of which are still used today. These techniques also tend to form the backbone of most undergraduate analytical chemistry educational labs.
Qualitative analysis.
A qualitative analysis determines the presence or absence of a particular compound, but not the mass or concentration. By definition, qualitative analyses do not measure quantity.
Chemical tests.
There are numerous qualitative chemical tests, for example, the acid test for gold and the Kastle-Meyer test for the presence of blood.
Flame test.
Inorganic qualitative analysis generally refers to a systematic scheme to confirm the presence of certain, usually aqueous, ions or elements by performing a series of reactions that eliminate ranges of possibilities and then confirms suspected ions with a confirming test. Sometimes small carbon containing ions are included in such schemes. With modern instrumentation these tests are rarely used but can be useful for educational purposes and in field work or other situations where access to state-of-the-art instruments are not available or expedient.
Quantitative analysis.
Gravimetric analysis.
Gravimetric analysis involves determining the amount of material present by weighing the sample before and/or after some transformation. A common example used in undergraduate education is the determination of the amount of water in a hydrate by heating the sample to remove the water such that the difference in weight is due to the loss of water.
Volumetric analysis.
Titration involves the addition of a reactant to a solution being analyzed until some equivalence point is reached. Often the amount of material in the solution being analyzed may be determined. Most familiar to those who have taken chemistry during secondary education is the acid-base titration involving a color changing indicator. There are many other types of titrations, for example potentiometric titrations.
These titrations may use different types of indicators to reach some equivalence point.
Instrumental methods.
Spectroscopy.
Spectroscopy measures the interaction of the molecules with electromagnetic radiation. Spectroscopy consists of many different applications such as atomic absorption spectroscopy, atomic emission spectroscopy, ultraviolet-visible spectroscopy, x-ray fluorescence spectroscopy, infrared spectroscopy, Raman spectroscopy, dual polarisation interferometry, nuclear magnetic resonance spectroscopy, photoemission spectroscopy, Mössbauer spectroscopy and so on.
Mass spectrometry.
Mass spectrometry measures mass-to-charge ratio of molecules using electric and magnetic fields. There are several ionization methods: electron impact, chemical ionization, electrospray, fast atom bombardment, matrix assisted laser desorption ionization, and others. Also, mass spectrometry is categorized by approaches of mass analyzers: magnetic-sector, quadrupole mass analyzer, quadrupole ion trap, time-of-flight, Fourier transform ion cyclotron resonance, and so on.
Electrochemical analysis.
Electroanalytical methods measure the potential (volts) and/or current (amps) in an electrochemical cell containing the analyte. These methods can be categorized according to which aspects of the cell are controlled and which are measured. The three main categories are potentiometry (the difference in electrode potentials is measured), coulometry (the cell's current is measured over time), and voltammetry (the cell's current is measured while actively altering the cell's potential).
Thermal analysis.
Calorimetry and thermogravimetric analysis measure the interaction of a material and heat.
Separation.
Separation processes are used to decrease the complexity of material mixtures. Chromatography, electrophoresis and Field Flow Fractionation are representative of this field.
Hybrid techniques.
Combinations of the above techniques produce a "hybrid" or "hyphenated" technique. Several examples are in popular use today and new hybrid techniques are under development. For example, gas chromatography-mass spectrometry, gas chromatography-infrared spectroscopy, liquid chromatography-mass spectrometry, liquid chromatography-NMR spectroscopy. liquid chromagraphy-infrared spectroscopy and capillary electrophoresis-mass spectrometry.
Hyphenated separation techniques refers to a combination of two (or more) techniques to detect and separate chemicals from solutions. Most often the other technique is some form of chromatography. Hyphenated techniques are widely used in chemistry and biochemistry. A slash is sometimes used instead of hyphen, especially if the name of one of the methods contains a hyphen itself.
Microscopy.
The visualization of single molecules, single cells, biological tissues and nanomaterials is an important and attractive approach in analytical science. Also, hybridization with other traditional analytical tools is revolutionizing analytical science. Microscopy can be categorized into three different fields: optical microscopy, electron microscopy, and scanning probe microscopy. Recently, this field is rapidly progressing because of the rapid development of the computer and camera industries.
Lab-on-a-chip.
Devices that integrate (multiple) laboratory functions on a single chip of only millimeters to a few square centimeters in size and that are capable of handling extremely small fluid volumes down to less than picoliters.
Errors.
Error can be defined as numerical difference between observed value and true value.
In error the true value and observed value in chemical analysis can be related with each other by the equation
where 
Error of a measurement is an inverse measure of accurate measurement i.e. smaller the error greater the accuracy of the measurement. Errors are expressed relatively as:
Standards.
Standard curve.
A general method for analysis of concentration involves the creation of a calibration curve. This allows for determination of the amount of a chemical in a material by comparing the results of unknown sample to those of a series of known standards. If the concentration of element or compound in a sample is too high for the detection range of the technique, it can simply be diluted in a pure solvent. If the amount in the sample is below an instrument's range of measurement, the method of addition can be used. In this method a known quantity of the element or compound under study is added, and the difference between the concentration added, and the concentration observed is the amount actually in the sample.
Internal standards.
Sometimes an internal standard is added at a known concentration directly to an analytical sample to aid in quantitation. The amount of analyte present is then determined relative to the internal standard as a calibrant. An ideal internal standard is isotopically-enriched analyte which gives rise to the method of isotope dilution.
Standard addition.
The method of standard addition is used in instrumental analysis to determine concentration of a substance (analyte) in an unknown sample by comparison to a set of samples of known concentration, similar to using a calibration curve. Standard addition can be applied to most analytical techniques and is used instead of a calibration curve to solve the matrix effect problem.
Signals and noise.
One of the most important components of analytical chemistry is maximizing the desired signal while minimizing the associated noise. The analytical figure of merit is known as the signal-to-noise ratio (S/N or SNR).
Noise can arise from environmental factors as well as from fundamental physical processes.
Thermal noise.
Thermal noise results from the motion of charge carriers (usually electrons) in an electrical circuit generated by their thermal motion. Thermal noise is white noise meaning that the power spectral density is constant throughout the frequency spectrum.
The root mean square value of the thermal noise in a resistor is given by
where "k" is Boltzmann's constant, "T" is the temperature, "R" is the resistance, and formula_5 is the bandwidth of the frequency formula_6.
Shot noise.
Shot noise is a type of electronic noise that occurs when the finite number of particles (such as electrons in an electronic circuit or photons in an optical device) is small enough to give rise to statistical fluctuations in a signal.
Shot noise is a Poisson process and the charge carriers that make up the current follow a Poisson distribution. The root mean square current fluctuation is given by
where "e" is the elementary charge and "I" is the average current. Shot noise is white noise.
Flicker noise.
Flicker noise is electronic noise with a 1/"ƒ" frequency spectrum; as "f" increases, the noise decreases. Flicker noise arises from a variety of sources, such as impurities in a conductive channel, generation and recombination noise in a transistor due to base current, and so on. This noise can be avoided by modulation of the signal at a higher frequency, for example through the use of a lock-in amplifier.
Environmental noise.
Environmental noise arises from the surroundings of the analytical instrument. Sources of electromagnetic noise are power lines, radio and television stations, wireless devices, Compact fluorescent lamps and electric motors. Many of these noise sources are narrow bandwidth and therefore can be avoided. Temperature and vibration isolation may be required for some instruments.
Noise reduction.
Noise reduction can be accomplished either in computer hardware or software. Examples of hardware noise reduction are the use of shielded cable, analog filtering, and signal modulation. Examples of software noise reduction are digital filtering, ensemble average, boxcar average, and correlation methods.
Applications.
Analytical chemistry research is largely driven by performance (sensitivity, selectivity, robustness, linear range, accuracy, precision, and speed), and cost (purchase, operation, training, time, and space). Among the main branches of contemporary analytical atomic spectrometry, the most widespread and universal are optical and mass spectrometry. In the direct elemental analysis of solid samples, the new leaders are laser-induced breakdown and laser ablation mass spectrometry, and the related techniques with transfer of the laser ablation products into inductively coupled plasma. Advances in design of diode lasers and optical parametric oscillators promote developments in fluorescence and ionization spectrometry and also in absorption techniques where uses of optical cavities for increased effective absorption pathlength are expected to expand. The use of plasma- and laser-based methods is increasing. An interest towards absolute (standardless) analysis has revived, particularly in emission spectrometry.
great effort is put in shrinking the analysis techniques to chip size. Although there are few examples of such systems competitive with traditional analysis techniques, potential advantages include size/portability, speed, and cost. (micro Total Analysis System (µTAS) or Lab-on-a-chip). Microscale chemistry reduces the amounts of chemicals used.
Many developments improve the analysis of biological systems. Examples of rapidly expanding fields in this area are:
Analytical chemistry has played critical roles in the understanding of basic science to a variety of practical applications, such as biomedical applications, environmental monitoring, quality control of industrial manufacturing, forensic science and so on.
The recent developments of computer automation and information technologies have extended analytical chemistry into a number of new biological fields. For example, automated DNA sequencing machines were the basis to complete human genome projects leading to the birth of genomics. Protein identification and peptide sequencing by mass spectrometry opened a new field of proteomics.
Analytical chemistry has been an indispensable area in the development of nanotechnology. Surface characterization instruments, electron microscopes and scanning probe microscopes enables scientists to visualize atomic structures with chemical characterizations.

</doc>
<doc id="2411" url="https://en.wikipedia.org/wiki?curid=2411" title="A cappella">
A cappella

A cappella (Italian for "in the manner of the chapel") music is specifically group or solo singing without instrumental accompaniment, or a piece intended to be performed in this way. It contrasts with cantata, which is accompanied singing. The term "a cappella" was originally intended to differentiate between Renaissance polyphony and Baroque concertato style. In the 19th century a renewed interest in Renaissance polyphony coupled with an ignorance of the fact that vocal parts were often doubled by instrumentalists led to the term coming to mean unaccompanied vocal music. The term is also used, albeit rarely, as a synonym for alla breve.
Religious origins.
A cappella music was originally used in religious music, especially church music as well as anasheed and zemirot. Gregorian chant is an example of a cappella singing, as is the majority of secular vocal music from the Renaissance. The madrigal, up until its development in the early Baroque into an instrumentally-accompanied form, is also usually in a cappella form. Jewish and Christian music were originally a cappella, and this practice has continued in both of these religions as well as in Islam.
Christian.
The polyphony of Christian a cappella music began to develop in Europe around the late 15th century, with compositions by Josquin des Prez. The early a cappella polyphonies may have had an accompanying instrument, although this instrument would merely double the singers' parts and was not independent. By the 16th century, a cappella polyphony had further developed, but gradually, the cantata began to take the place of a cappella forms. 16th century a cappella polyphony, nonetheless, continued to influence church composers throughout this period and to the present day. Recent evidence has shown that some of the early pieces by Palestrina, such as what was written for the Sistine Chapel was intended to be accompanied by an organ "doubling" some or all of the voices. Such is seen in the life of Palestrina becoming a major influence on Bach, most notably in the aforementioned "Mass in B Minor". Other composers that utilized the a cappella style, if only for the occasional piece, were Claudio Monteverdi and his masterpiece, "Lagrime d'amante al sepolcro dell'amata" (A lover's tears at his beloved's grave), which was composed in 1610, and Andrea Gabrieli when upon his death it was discovered many choral pieces, one of which was in the unaccompanied style. Learning from the preceding two composeres, Heinrich Schütz utilized the a cappella style in numerous pieces, chief among these were the pieces in the oratorio style, which were traditionally performed during the Easter week and dealt with the religious subject matter of that week, such as Christ's suffering and the Passion. Five of Schutz's "Historien" were Easter pieces, and of these the latter three, which dealt with the passion from three different viewpoints, those of Matthew, Luke and John, were all done a cappella style. This was a near requirement for this type of piece, and the parts of the crowd were sung while the solo parts which were the quoted parts from either Christ or the authors were performed in a plainchant.
Byzantine Rite.
In the Byzantine Rite of the Eastern Orthodox Church and the Eastern Catholic Churches, the music performed in the liturgies is exclusively sung without instrumental accompaniment. Bishop Kallistos Ware says, "The service is sung, even though there may be no choir... In the Orthodox Church today, as in the early Church, singing is unaccompanied and instrumental music is not found." This "a cappella" behavior arises from strict interpretation of Psalms 150, which states, "Let every thing that hath breath praise the Lord. Praise ye the Lord." In keeping with this philosophy, early Russian "musika" which started appearing in the late 17th century, in what was known as "khorovïye kontsertï" (choral concertos) made a cappella adaptations of Venetian-styled pieces, such as the treatise, "Grammatika musikiyskaya" (1675), by Nikolai Diletsky. Divine Liturgies and Western Rite masses composed by famous composers such as Peter Tchaikovsky, Sergei Rachmaninoff, Alexander Arkhangelsky, and Mykola Leontovych are fine examples of this.
Opposition to instruments in worship.
Present-day Christian religious bodies known for conducting their worship services without musical accompaniment include some Presbyterian churches devoted to the regulative principle of worship, Old Regular Baptists, Primitive Baptists, Plymouth Brethren, Churches of Christ, the Old German Baptist Brethren, Doukhobors the Byzantine Rite and the Amish, Old Order Mennonites and Conservative Mennonites. Certain high church services and other musical events in liturgical churches (such as the Roman Catholic Mass and the Lutheran Divine Service) may be a cappella, a practice remaining from apostolic times. Many Mennonites also conduct some or all of their services without instruments. Sacred Harp, a type of folk music, is an a cappella style of religious singing with shape notes, usually sung at singing conventions.
Opponents of musical instruments in the Christian worship believe that such opposition is supported by the Christian scriptures and Church history. The scriptures typically referenced are Matthew 26:30; Acts 16:25; Romans 15:9; 1 Corinthians 14:15; Ephesians 5:19; Colossians 3:16; Hebrews 2:12, 13:15; James 5:13, which show examples and exhortations for Christians to sing.
There is no reference to instrumental music in early church worship in the New Testament, or in the worship of churches for the first six centuries. Several reasons have been posited throughout church history for the absence of instrumental music in church worship.
Christians who believe in a cappella music today believe that in the Israelite worship assembly during Temple worship only the Priests of Levi sang, played, and offered animal sacrifices, whereas in the church era, all Christians are commanded to sing praises to God. They believe that if God wanted instrumental music in New Testament worship, He would have commanded not just singing, but singing and playing like he did in the Hebrew scriptures.
The first recorded example of a musical instrument in Roman Catholic worship was a pipe organ introduced by Pope Vitalian into a cathedral in Rome around 670.
Instruments have divided Christendom since their introduction into worship. They were considered a Catholic innovation, not widely practiced until the 18th century, and were opposed vigorously in worship by a number of Protestant Reformers, including Martin Luther (1483–1546), Ulrich Zwingli, John Calvin (1509–1564) and John Wesley (1703–1791). Alexander Campbell referred to the use of an instrument in worship as "a cow bell in a concert". In Sir Walter Scott's "The Heart of Midlothian", the heroine, Jeanie Deans, a Scottish Presbyterian, writes to her father about the church situation she has found in England (bold added):
Acceptance of instruments in worship.
Those who do not adhere to the regulative principle of interpreting Christian scripture, believe that limiting praise to the unaccompanied chant of the early church is not commanded in scripture, and that churches in any age are free to offer their songs with or without musical instruments.
Those who subscribe to this interpretation believe that since the Christian scriptures never counter instrumental language with any negative judgment on instruments, opposition to instruments instead comes from an interpretation of history. There is no written opposition to musical instruments in any setting in the first century and a half of Christian churches (33 AD to 180AD). The use of instruments for Christian worship during this period is also undocumented. Toward the end of the 2nd century, Christians began condemning the instruments themselves. Those who oppose instruments today believe these Church Fathers had a better understanding of God's desire for the church, but there are significant differences between the teachings of these Church Fathers and Christian opposition to instruments today.
Since "a cappella" singing brought a new polyphony (more than one note at a time) with instrumental accompaniment, it is not surprising that Protestant reformers who opposed the instruments (such as Calvin and Zwingli) also opposed the polyphony. While Zwingli was burning organs in Switzerland – Luther called him a fanatic – the Church of England was burning books of polyphony.
Some Holiness Churches such as the Free Methodist Church opposed the use of musical instruments in church worship until the mid-20th century. The Free Methodist Church allowed for local church decision on the use of either an organ or piano in the 1943 Conference before lifting the ban entirely in 1955.
Jewish.
While worship in the Temple in Jerusalem included musical instruments (), traditional Jewish religious services in the Synagogue, both before and after the last destruction of the Temple, did not include musical instruments given the practice of scriptural cantillation. The use of musical instruments is traditionally forbidden on the Sabbath out of concern that players would be tempted to repair (or tune) their instruments, which is forbidden on those days. (This prohibition has been relaxed in many Reform and some Conservative congregations.) Similarly, when Jewish families and larger groups sing traditional Sabbath songs known as zemirot outside the context of formal religious services, they usually do so a cappella, and Bar and Bat Mitzvah celebrations on the Sabbath sometimes feature entertainment by a cappella ensembles. During the Three Weeks musical instruments are prohibited. Many Jews consider a portion of the 49-day period of the counting of the omer between Passover and Shavuot to be a time of semi-mourning and instrumental music is not allowed during that time. This has led to a tradition of a cappella singing sometimes known as "sefirah" music.
The popularization of the Jewish chant may be found in the writings of the Jewish philosopher Philo, born 20 BCE. Weaving together Jewish and Greek thought, Philo promoted praise without instruments, and taught that "silent singing" (without even vocal chords) was better still. This view parted with the Jewish scriptures, where Israel offered praise with instruments by God's own command (). The shofar is the only temple instrument still being used today in the synagogue, and it is only used from Rosh Chodesh Elul through the end of Yom Kippur. The shofar is used by itself, without any vocal accompaniment, and is limited to a very strictly defined set of sounds and specific places in the synagogue service.
In the United States.
Peter Christian Lutkin, dean of the Northwestern University School of Music, helped popularize a cappella music in the United States by founding the Northwestern A Cappella Choir in 1906. The A Cappella Choir was "the first permanent organization of its kind in America."
A strong and prominent a cappella tradition was begun in the midwest part of the United States in 1911 by F. Melius Christiansen, a music faculty member at St. Olaf College in Northfield, Minnesota. The St. Olaf College Choir was established as an outgrowth of the local St. John's Lutheran Church, where Christiansen was organist and the choir was composed, at least partially, of students from the nearby St. Olaf campus. The success of the ensemble was emulated by other regional conductors, and a rich tradition of a cappella choral music was born in the region at colleges like Concordia College (Moorhead, Minnesota), Augustana College (Rock Island, Illinois), Wartburg College (Waverly, Iowa), Luther College (Decorah, Iowa), Gustavus Adolphus College (St. Peter, Minnesota), Augustana College (Sioux Falls, South Dakota), and Augsburg College (Minneapolis, Minnesota). The choirs typically range from 40 to 80 singers and are recognized for their efforts to perfect blend, intonation, phrasing and pitch in a large choral setting.
Major movements in modern a cappella over the past century include Barbershop and doo wop. The Barbershop Harmony Society, Sweet Adelines International, and Harmony Inc. host educational events including Harmony University, Directors University, and the International Educational Symposium, and international contests and conventions, recognizing international champion choruses and quartets.
These days, many a cappella groups can be found in high schools and colleges. There are amateur Barbershop Harmony Society and professional groups that sing a cappella exclusively. Although a cappella is technically defined as singing without instrumental accompaniment, some groups use their voices to emulate instruments; others are more traditional and focus on harmonizing. A cappella styles range from gospel music to contemporary to barbershop quartets and choruses.
A cappella music was popularized between the late 2000s and the mid 2010s with media hits such as the 2009–2014 TV show "The Sing-Off", the musical "Perfect Harmony", and the musical comedy film series "Pitch Perfect".
Recording artists.
In July 1943, as a result of the American Federation of Musicians boycott of US recording studios, the a cappella vocal group "The Song Spinners" had a best-seller with "Comin' In On A Wing And A Prayer". In the 1950s several recording groups, notably The Hi-Los and the Four Freshmen, introduced complex jazz harmonies to a cappella performances. The King's Singers are credited with promoting interest in small-group a cappella performances in the 1960s. In 1983 an a cappella group known as The Flying Pickets had a Christmas 'number one' in the UK with a cover of Yazoo's (known in the US as Yaz) "Only You". A cappella music attained renewed prominence from the late 1980s onward, spurred by the success of Top 40 recordings by artists such as The Manhattan Transfer, Bobby McFerrin, Huey Lewis and the News, All-4-One, The Nylons, Backstreet Boys and Boyz II Men.
Contemporary a cappella includes many vocal groups and bands who add vocal percussion or beatboxing to create a pop/rock/gospel sound, in some cases very similar to bands with instruments. Examples of such professional groups include Straight No Chaser, Pentatonix, The House Jacks, Rockapella, Mosaic, and M-pact. There also remains a strong a cappella presence within Christian music, as some denominations purposefully do not use instruments during worship. Examples of such groups are Take 6, Glad and Acappella. Arrangements of popular music for small a cappella ensembles typically include one voice singing the lead melody, one singing a rhythmic bass line, and the remaining voices contributing chordal or polyphonic accompaniment.
A cappella can also describe the isolated vocal track(s) from a multitrack recording that originally included instrumentation. These vocal tracks may be remixed or put onto vinyl records for DJs, or released to the public so that fans can remix them. One such example is the a cappella release of Jay-Z's "Black Album", which Danger Mouse mixed with The Beatles' "White Album" to create "The Grey Album".
A cappella's growth is not limited to live performance, with hundreds of recorded a cappella albums produced over the past decade. As of December 2006, the Recorded A Cappella Review Board (RARB) had reviewed over 660 a cappella albums since 1994, and its popular discussion forum had over 900 users and 19,000 articles.
Recording artist Brandy Norwood included a song on her 2008 album "Human" titled "A Capella (Something's Missing)". Brandy uses her voice for background music in this song, showing her capabilities of using her voice as an instrument. No other instruments are used, except for an electric guitar.
In 2010, American dance recording artist Kelis released a song called "Acapella" as the first single from her album "Flesh Tone". The song is not actually performed a cappella, but rather explains that before her son was born, her life was without music.
In 2013, an artist by the name Smooth McGroove rose to prominence with his style of a cappella music. He is best known for his a cappella covers of video game music tracks on YouTube.
Musical theater.
A cappella has been used as the sole orchestration for original works of musical theater that have had commercial runs Off-Broadway (theaters in New York City with 99 to 500 seats) only four times. The first was Avenue X which opened on 28 January 1994 and ran for 77 performances. It was produced by Playwrights Horizons with book by John Jiler, music and lyrics by Ray Leslee. The musical style of the show's score was primarily Doo-Wop as the plot revolved around Doo-Wop group singers of the 1960s.
In 2001, The Kinsey Sicks, produced and starred in the critically acclaimed off-Broadway hit, "DRAGAPELLA! Starring the Kinsey Sicks" at New York's legendary Studio 54. That production received a nomination for a Lucille Lortel award as Best Musical and a Drama Desk nomination for Best Lyrics. It was directed by Glenn Casale with original music and lyrics by Ben Schatz.
The a cappella musical Perfect Harmony, a comedy about two high school a cappella groups vying to win the National championship, made its Off Broadway debut at Theatre Row’s Acorn Theatre on 42nd Street in New York City in October, 2010 after a successful out-of-town run at the Stoneham Theatre, in Stoneham, Massachusetts. Perfect Harmony features the hit music of The Jackson 5, Pat Benatar, Billy Idol, Marvin Gaye, Scandal, Tiffany, The Romantics, The Pretenders, The Temptations, The Contours, The Commodores, Tommy James & the Shondells and The Partridge Family, and has been compared to a cross between Altar Boyz and The 25th Annual Putnam County Spelling Bee.
The fourth a cappella musical to appear Off-Broadway, In Transit, premiered 5 October 2010 and was produced by Primary Stages with book, music, and lyrics by Kristen Anderson-Lopez, James-Allen Ford, Russ Kaplan, and Sara Wordsworth. Set primarily in the New York City subway system its score features an eclectic mix of musical genres (including jazz, hip hop, Latin, rock, and country). In Transit incorporates vocal beat boxing into its contemporary a cappella arrangements through the use of a subway beat boxer character. Beat boxer and actor Chesney Snow performed this role for the 2010 Primary Stages production. According to the show's website, it is scheduled to reopen for an open-ended commercial run in the Fall of 2011. In 2011 the production received four Lucille Lortel Award nominations including Outstanding Musical, Outer Critics Circle and Drama League nominations, as well as five Drama Desk nominations including Outstanding Musical and won for Outstanding Ensemble Performance.
, no show with a cappella orchestrations has ever run on Broadway.
Barbershop style.
Barbershop music is one of several uniquely American art forms. The earliest reports of this style of a cappella music involved African Americans. The earliest documented quartets all began in barbershops. In 1938, the first formal men's barbershop organization was formed, known as the Society for the Preservation and Encouragement of Barber Shop Quartet Singing in America (S.P.E.B.S.Q.S.A), and in 2004 rebranded itself and officially changed its public name to the Barbershop Harmony Society (BHS). Today the BHS has over 22,000 members in approximately 800 chapters across the United States, and the barbershop style has spread around the world with organizations in many other countries. The Barbershop Harmony Society provides a highly organized competition structure for a cappella quartets and choruses singing in the barbershop style.
In 1945, the first formal women's barbershop organization, Sweet Adelines, was formed. In 1953 Sweet Adelines became an international organization, although it didn't change its name to Sweet Adelines International until 1991. The membership of nearly 25,000 women, all singing in English, includes choruses in most of the fifty United States as well as in Australia, Canada, England, Finland, Germany, Ireland, Japan, New Zealand, Scotland, Sweden, Wales and the Netherlands. Headquartered in Tulsa, Oklahoma, the organization encompasses more than 1,200 registered quartets and 600 choruses.
In 1959, a second women's barbershop organization started as a break off from Sweet Adelines due to ideological differences. Based on democratic principles which continue to this day, Harmony, Inc. is smaller than its counterpart, but has an atmosphere of friendship and competition. With about 2,500 members in the United States and Canada, Harmony, Inc. uses the same rules in contest that the Barbershop Harmony Society uses. Harmony, Inc. is registered in Providence, Rhode Island.
In other countries.
Sri Lanka.
Composer Dinesh Subasinghe became the first Sri Lankan to write a cappella pieces for SATB choirs. He wrote "The Princes of the Lost Tribe" and "Ancient Queen of Somawathee" for Menaka De Shabandu and Bridget Halpe's choirs, respectively, based on historical incidents in ancient Sri Lanka.
Sweden.
The European a cappella tradition is especially strong in the countries around the Baltic and perhaps most so in Sweden as described by Richard Sparks in his doctoral thesis "The Swedish Choral Miracle" in 2000.
Swedish a cappella choirs have over the last 25 years won around 25% of the annual prestigious European Grand Prix for Choral Singing (EGP) that despite its name is open to choirs from all over the world (see list of laureates in the Wikipedia article on the EGP competition).
The reasons for the strong Swedish dominance are as explained by Richard Sparks manifold; suffice to say here that there is a long-standing tradition, an unsusually large proportion of the populations (5% is often cited) regularly sing in choirs, the Swedish choral director Eric Ericson had an enormous impact on a cappella choral development not only in Sweden but around the world, and finally there are a large number of very popular primary and secondary schools ("music schools") with high admission standards based on auditions that combine a rigid academic regimen with high level choral singing on every school day, a system that started with Adolf Fredrik's Music School in Stockholm in 1939 but has spread over the country.
United Kingdom.
A cappella has gained attention in the UK in recent years, with many groups forming at British universities by students seeking an alternative singing pursuit to traditional choral and chapel singing. This movement has been bolstered by organisations such as The Voice Festival UK.
Collegiate.
It is not clear exactly where collegiate a cappella began. The Rensselyrics of Rensselaer Polytechnic Institute (formerly known as the RPI Glee Club), established in 1873 is perhaps the oldest known collegiate a cappella group. However the longest continuously-singing group is probably The Whiffenpoofs of Yale University, which was formed in 1909 and once included Cole Porter as a member. Collegiate a cappella groups grew throughout the 20th century. Some notable historical groups formed along the way include Princeton University's Tigertones (1946), Colgate University's The Colgate 13 (1942), Dartmouth College's Aires (1946), Cornell University's Cayuga's Waiters (1949) and The Hangovers (1968), the University of Maine Maine Steiners (1958), the Columbia University Kingsmen (1949), the Jabberwocks of Brown University (1949), and the University of Rochester YellowJackets (1956). All-women a cappella groups followed shortly, frequently as a parody of the men's groups: the Smiffenpoofs of Smith College (1936), The Shwiffs of Connecticut College (The She-Whiffenpoofs, 1944), and The Chattertocks of Brown University (1951). A cappella groups exploded in popularity beginning in the 1990s, fueled in part by a change in style popularized by the Tufts University Beelzebubs and the Boston University Dear Abbeys. The new style used voices to emulate modern rock instruments, including vocal percussion/"beatboxing". Some larger universities now have multiple groups. Groups often join one another in on-campus concerts, such as the Georgetown Chimes' Cherry Tree Massacre, a 3-weekend a cappella festival held each February since 1975, where over a hundred collegiate groups have appeared, as well as International Quartet Champions The Boston Common and the contemporary commercial a cappella group Rockapella. Co-ed groups have produced many up-and-coming and major artists, including John Legend, an alumnus of the Counterparts at the University of Pennsylvania, and Sara Bareilles, an alumna of Awaken A Cappella at University of California, Los Angeles. Mira Sorvino is an alumna of the Harvard-Radcliffe Veritones of Harvard College where she had the solo on Only You by Yaz.
A cappella is gaining popularity among South Asians with the emergence of primarily Hindi-English College groups. The first South Asian a cappella group was Penn Masala, founded in 1996 at the University of Pennsylvania. Co-ed South Asian a cappella groups are also gaining in popularity. The first co-ed south Asian a cappella was Anokha, from the University of Maryland, formed in 2001. Also, Dil se, another co-ed a cappella from UC Berkeley, hosts the "Anahat" competition at the University of California, Berkeley annually. Maize Mirchi, the co-ed a cappella group from the University of Michigan hosts "Sa Re Ga Ma Pella", an annual South Asian a cappella invitational with various groups from the Midwest.
Jewish-interest groups such as Tufts University's Shir Appeal, University of Chicago's Rhythm and Jews, Binghamton University's Kaskeset, Ohio State University's Meshuganotes, Rutgers University's Kol Halayla, New York University's Ani V'Ata and Yale University's Magevet are also gaining popularity across the U.S.
Increased interest in modern a cappella (particularly collegiate a cappella) can be seen in the growth of awards such as the Contemporary A Cappella Recording Awards (overseen by the Contemporary A Cappella Society) and competitions such as the International Championship of Collegiate A Cappella for college groups and the Harmony Sweepstakes for all groups. In December 2009, a new television competition series called "The Sing-Off" aired on NBC. The show featured eight a cappella groups from the United States and Puerto Rico vying for the prize of $100,000 and a recording contract with Epic Records/Sony Music. The show was judged by Ben Folds, Shawn Stockman, and Nicole Scherzinger and was won by an all-male group from Puerto Rico called Nota. The show returned for a second and third season, won by Committed and Pentatonix, respectively.
Each year, hundreds of Collegiate a cappella groups submit their strongest songs in a competition to be on The Best of College A Cappella (BOCA), an album compilation of tracks from the best college a cappella groups around the world. The album is produced by Varsity Vocals – which also produces the International Championship of Collegiate A Cappella – and Deke Sharon. A group chosen to be on the BOCA album earns much credibility among the a cappella community.
Collegiate a cappella groups may also submit their tracks to Voices Only, a two-disc series released at the beginning of each school year. A Voices Only album has been released every year since 2005.
In addition, all women's a cappella groups can send their strongest song tracks to the Women’s A Cappella Association (WACA) for its annual best of women's a cappella album. WACA offers another medium for women's voices to receive recognition and has released an album every year since 2014, featuring women's groups from across the United States.
Emulating instruments.
In addition to singing words, some a cappella singers also emulate instrumentation by reproducing instrumental sounds with their vocal cords and mouth. One of the earliest 20th century practitioners of this method were The Mills Brothers whose early recordings of the 1930s clearly stated on the label that all instrumentation was done vocally. More recently, "Twilight Zone" by 2 Unlimited was sung a cappella to the instrumentation on the comedy television series "Tompkins Square". Another famous example of emulating instrumentation instead of singing the words is the theme song for "The New Addams Family" series on Fox Family Channel (now ABC Family). Groups such as Vocal Sampling and Undivided emulate Latin rhythms a cappella. In the 1960s, the Swingle Singers used their voices to emulate musical instruments to Baroque and Classical music. Vocal artist Bobby McFerrin is famous for his instrumental emulation. A cappella group Naturally Seven recreates entire songs using vocal tones for every instrument.
The Swingle Singers used nonsense words to sound like instruments, but have been known to produce non-verbal versions of musical instruments. Like the other groups, examples of their music can be found on YouTube. Beatboxing, more accurately known as vocal percussion, is a technique used in a cappella music popularized by the hip-hop community, where rap is often performed a cappella also. The advent of vocal percussion added new dimensions to the a cappella genre and has become very prevalent in modern arrangements. Petra Haden used a four-track recorder to produce an a cappella version of "The Who Sell Out" including the instruments and fake advertisements on her album "" in 2005. Haden has also released a cappella versions of Journey's "Don't Stop Believin'", The Beach Boys' "God Only Knows" and Michael Jackson's "Thriller". In 2009, Toyota commissioned Haden to perform three songs for television commercials for the third-generation Toyota Prius, including an a cappella version of The Bellamy Brothers' 1970s song "Let Your Love Flow".
Christian rock group Relient K recorded the song "Plead the Fifth" a cappella on its album "Five Score and Seven Years Ago". The group recorded lead singer Matt Thiessen making drum noises and played them with an electronic drum machine to record the song.
The German metal band van Canto uses vocal noises to imitate guitars on covers of well-known rock and metal songs (such as "Master of Puppets" by Metallica) as well as original compositions. Although they are generally classified as a cappella metal, the band also includes a drummer, and uses amplifiers on some songs to distort the voice to sound more like an electric guitar.

</doc>
<doc id="2414" url="https://en.wikipedia.org/wiki?curid=2414" title="Arrangement">
Arrangement

In music, an arrangement is a musical reconceptualization of a previously composed work. It may differ from the original work by means of reharmonization, melodic paraphrasing, orchestration, or development of the formal structure. Arranging differs from orchestration in that the latter process is limited to the assignment of notes to instruments for performance by an orchestra, concert band, or other musical ensemble. Arranging "involves adding compositional techniques, such as new thematic material for introductions, transitions, or modulations, and endings... Arranging is the art of giving an existing melody musical variety".
Classical music.
Arrangements and transcriptions of classical and serious music go back to the early history of this genre. In particular, music written for the piano has frequently undergone this treatment. The suite of ten piano pieces "Pictures at an Exhibition", by Modest Mussorgsky, has been arranged over twenty times, notably by Maurice Ravel.
Due to his lack of expertise in orchestration, the American composer George Gershwin had his "Rhapsody in Blue" orchestrated and arranged by Ferde Grofé.
Popular music.
Popular music recordings often include parts for brass, string, and other instruments which were added by arrangers and not composed by the original songwriters. Popular music arrangements may also be considered to include new releases of existing songs with a new musical treatment. These changes can include alterations to tempo, meter, key, instrumentation, and other musical elements.
Well-known examples include Joe Cocker's version of the Beatles' "With a Little Help from My Friends" and Ike And Tina Turner's version of Creedence Clearwater Revival's "Proud Mary". The American group Vanilla Fudge and British group Yes based their early careers on radical re-arrangements of contemporary hits. Bonnie Pointer performed disco and Motown-themed versions of "Heaven Must Have Sent You." Remixes, such as in dance music, can also be considered arrangements.
Though arrangers may contribute substantially to finished musical products, for copyright and royalty purposes, they usually hold no legal claim to their work.
Jazz.
Arrangements for small jazz combos are usually informal, minimal, and uncredited. Larger ensembles have generally had greater requirements for notated arrangements, though the early Count Basie big band is known for its many "head" arrangements, so called because they were worked out by the players themselves, memorized (in the player's "head"), and never written down. Most arrangements for big bands, however, were written down and credited to a specific arranger, as with arrangements by Sammy Nestico and Neal Hefti for Count Basie's later big bands.
Don Redman made innovations in jazz arranging as a part of Fletcher Henderson's orchestra in the 1920s. Redman's arrangements introduced a more intricate melodic presentation and "soli" performances for various sections of the big band. Benny Carter became Henderson's primary arranger in the early 1930s, becoming known for his arranging abilities in addition to his previous recognition as a performer. Beginning in 1938, Billy Strayhorn became an arranger of great renown for the Duke Ellington orchestra. Jelly Roll Morton is sometimes considered the earliest jazz arranger. While he toured around the years 1912 to 1915, he wrote down parts to enable "pick-up" bands to perform his compositions.
Big band arrangements are informally called "charts". In the swing era they were usually either arrangements of popular songs or they were entirely new compositions. Duke Ellington's and Billy Strayhorn's arrangements for the Duke Ellington big band were usually new compositions, and some of Eddie Sauter's arrangements for the Benny Goodman band and Artie Shaw's arrangements for his own band were new compositions as well. It became more common to arrange sketchy jazz combo compositions for big band after the bop era.
After 1950, the big bands declined in number. However, several bands continued and arrangers provided renowned arrangements. Gil Evans wrote a number of large-ensemble arrangements in the late 1950s and early 1960s intended for recording sessions only. Other arrangers of note include Vic Schoen, Pete Rugolo, Oliver Nelson, Johnny Richards, Billy May, Thad Jones, Maria Schneider, Bob Brookmeyer, Lou Marini, Nelson Riddle, Ralph Burns, Billy Byers, Gordon Jenkins, Ray Conniff, Henry Mancini, Ray Reach, and Claus Ogerman.
In the 21st century, the Big Band arrangement has made a modest comeback. Gordon Goodwin, Roy Hargrove, and Christian McBride have all rolled out New Big Bands with both original compositions and new arrangements of standard tunes.
Arranging for instrumental groups.
Strings.
The string section is a body of instruments composed of various stringed instruments. By the 19th century orchestral music in Europe had standardized the string section into the following homogeneous instrumental groups: first violins, second violins, violas, cellos, and double basses. The string section in a multi-sectioned orchestra is referred sometimes to as the "string choir."
The harp is also a stringed instrument, but is not a member of or homogeneous with the violin family and is not considered part of the string choir. Samuel Adler classifies the harp as a plucked string instrument in the same category as the guitar (acoustic or electric), mandolin, banjo, or zither. Like the harp these instruments do not belong to the violin family and are not homogeneous with the string choir. In modern arranging these instruments are considered part of the rhythm section. The electric string bass and upright string bass—depending on the circumstance—can be treated by the arranger as either string section or rhythm section instruments.
A group of instruments in which each member plays a unique part—rather than playing in unison with other like instruments—is referred to as a chamber ensemble. A chamber ensemble made up entirely of strings of the violin family is referred to by its size. A string trio consists of three players, a string quartet four, a string quintet five, and so on.
In most circumstances the string section is treated by the arranger as one homogeneous unit and its members are required to play preconceived material rather than improvise.
A string section can be utilized on its own (this is referred to as a string orchestra) or in conjunction with any of the other instrumental sections. More than one string orchestra can be utilized.
A standard string section (vln., vln 2., vla., vcl, cb.) with each section playing unison allows the arranger to create a five-part texture. Often an arranger will divide each violin section in half or thirds to achieve a denser texture. It is possible to carry this division to its logical extreme in which each member of the string section plays his or her own unique part.
Size of the string section.
Artistic, budgetary and logistical concerns will determine the size and instrumentation of a string section. The Broadway musical West Side Story, in 1957, was booked into the Winter Garden theater; composer Leonard Bernstein disliked the playing of "house" viola players he would have to use there, and so he chose to leave them out of the show's instrumentation; a benefit was the creation of more space in the pit for an expanded percussion section.
George Martin, producer and arranger for The Beatles, warns arrangers about the intonation issues when only two like instruments play in unison. "After a string quartet," Martin explains, "I do not think there is a satisfactory sound for strings until one has at least three players on each line...as a rule two stringed instruments together create a slight "beat" which does not give a smooth sound."
While any combination and number of string instruments is possible in a section, a traditional string section sound is achieved with a violin-heavy balance of instruments.

</doc>
<doc id="2416" url="https://en.wikipedia.org/wiki?curid=2416" title="Athanasian Creed">
Athanasian Creed

The Athanasian Creed, or Quicunque Vult (also "Quicumque Vult"), is a Christian statement of belief focused on Trinitarian doctrine and Christology. The Latin name of the creed, "Quicunque vult", is taken from the opening words, "Whosoever wishes". The creed has been used by Christian churches since the sixth century. It is the first creed in which the equality of the three persons of the Trinity is explicitly stated. It differs from the Nicene-Constantinopolitan and Apostles' Creeds in the inclusion of anathemas, or condemnations of those who disagree with the creed (like the original Nicene Creed).
Widely accepted among Western Christians, including the Roman Catholic Church and some Anglican churches, Lutheran churches (it is considered part of the Lutheran confessions in the Book of Concord), and ancient, liturgical churches generally, the Athanasian Creed has been used in public worship less and less frequently, but part of it can be found as an "Authorized Affirmation of Faith" in the recent (2000) Common Worship liturgy of the Church of England ain Volume page 14. The creed has never gained much acceptance in liturgy among Eastern Christians. It was designed to distinguish Nicene Christianity from the heresy of Arianism. Liturgically, this Creed was recited at the Sunday Office of Prime in the Western Church; it is not in common use in the Eastern Church. Today, the Athanasian Creed is rarely used even in the Western Church. When used, one common practice is to use it once a year on Trinity Sunday.
Origin.
A medieval account credited Athanasius of Alexandria, the famous defender of Nicene theology, as the author of the Creed. According to this account, Athanasius composed it during his exile in Rome and presented it to Pope Julius I as a witness to his orthodoxy. This traditional attribution of the Creed to Athanasius was first called into question in 1642 by Dutch Protestant theologian G.J. Voss, and it has since been widely accepted by modern scholars that the creed was not authored by Athanasius, that it was not originally called a creed at all, nor was Athanasius' name originally attached to it. Athanasius' name seems to have become attached to the creed as a sign of its strong declaration of Trinitarian faith. The reasoning for rejecting Athanasius as the author usually relies on a combination of the following:
The use of the creed in a sermon by Caesarius of Arles, as well as a theological resemblance to works by Vincent of Lérins, point to Southern Gaul as its origin. The most likely time frame is in the late fifth or early sixth century AD – at least 100 years after Athanasius. The theology of the creed is firmly rooted in the Augustinian tradition, using exact terminology of Augustine's "On the Trinity" (published 415 AD). In the late 19th century, there was a great deal of speculation about who might have authored the creed, with suggestions including Ambrose of Milan, Venantius Fortunatus, and Hilary of Poitiers, among others. The 1940 discovery of a lost work by Vincent of Lérins, which bears a striking similarity to much of the language of the Athanasian Creed, have led many to conclude that the creed originated either with Vincent or with his students. For example, in the authoritative modern monograph about the creed, J.N.D. Kelly asserts that Vincent of Lérin was not its author, but that it may have come from the same milieu, namely the area of Lérins in southern Gaul. The oldest surviving manuscripts of the Athanasian Creed date from the late 8th century.
Content.
The Athanasian Creed is usually divided into two sections: lines 1–28 addressing the doctrine of the Trinity, and lines 29–44 addressing the doctrine of Christology. Enumerating the three persons of the Trinity (i.e., Father, the Son, and the Holy Spirit), the first section of the creed ascribes the divine attributes to each individually. Thus, each person of the Trinity is described as uncreated ("increatus"), limitless ("Immensus"), eternal ("æternus"), and omnipotent ("omnipotens"). While ascribing the divine attributes and divinity to each person of the Trinity, thus avoiding subordinationism, the first half of the Athanasian Creed also stresses the unity of the three persons in the one Godhead, thus avoiding a theology of tritheism. Furthermore, although one God, the Father, Son, and Holy Spirit are distinct from each other. For the Father is neither made nor begotten; the Son is not made but is begotten from the Father; the Holy Spirit is neither made nor begotten but proceeds from the Father and the Son (filioque).
The text of the Athanasian Creed is as follows:
The Christology of the second section is more detailed than that of the Nicene Creed, and reflects the teaching of the First Council of Ephesus (431) and the definition of the Council of Chalcedon (451). The 'Athanasian' Creed uses the Nicene term "homoousios"' ('one substance', 'one in Being') not only with respect to the relation of the Son to the Father according to his divine nature, but that the Son is "homoousios" with his mother Mary, according to his human nature.
The Creed's wording thus excludes not only Sabellianism and Arianism, but the Christological heresies of Nestorianism and Eutychianism. A need for a clear confession against Arianism arose in western Europe when the Ostrogoths and Visigoths, who had Arian beliefs, invaded at the beginning of the 5th century.
The final section of this Creed also moved beyond the Nicene (and Apostles') Creeds in making negative statements about the people's fate: "They that have done good shall go into life everlasting: and they that have done evil into everlasting fire." This caused considerable debate in England in the mid-nineteenth century, centred on the teaching of Frederick Denison Maurice.
Uses.
Composed of 44 rhythmic lines, the Athanasian Creed appears to have been intended as a liturgical document – that is, the original purpose of the creed was to be spoken or sung as a part of worship. The creed itself uses the language of public worship, speaking of the worship of God rather than the language of belief ("Now this is the catholic faith: We worship one God"). In the Catholic Church in medieval times, this creed was recited following the Sunday sermon or at the Sunday Office of Prime. The creed was often set to music and used in the place of a Psalm.
Early Protestants inherited the late medieval devotion to the Athanasian Creed, and it was considered to be authoritative in many Protestant churches. The statements of Protestant belief (confessional documents) of various Reformers commend the Athanasian Creed to their followers, including the Augsburg Confession, the Formula of Concord, the Second Helvetic Confession, the Belgic Confession, the Bohemian Confession and the Thirty-nine Articles. A metric version titled "Quicumque vult", with a musical setting, was published in "The Whole Booke of Psalmes" printed by John Day in 1562. Among modern Lutheran and Reformed churches adherence to the Athanasian Creed is prescribed by the earlier confessional documents, but the creed does not receive much attention outside of occasional use – especially on Trinity Sunday.
In Reformed circles, it is included (for example) in the Christian Reformed Churches of Australia's Book of Forms (publ. 1991). However, it is rarely recited in public worship.
In the successive Books of Common Prayer of the reformed Church of England, from 1549 to 1662, its recitation was provided for on 19 occasions each year, a practice which continued until the nineteenth century, when vigorous controversy regarding its statement about 'eternal damnation' saw its use gradually decline. It remains one of the three Creeds approved in the Thirty-Nine Articles, and is printed in several current Anglican prayer books (e.g. A Prayer Book for Australia (1995)). As with Roman Catholic practice, its use is now generally only on Trinity Sunday or its octave. The Episcopal Church based in the United States has never provided for its use in worship, but added it to its Book of Common Prayer for the first time in 1979, where it is included in small print in a reference section entitled "Historical Documents of the Church."
In Roman Catholic churches, it was traditionally said at Prime on Sundays after Epiphany and Pentecost, except when a Double feast or day within an octave occurred, and on Trinity Sunday. In the 1960 reforms, it was reduced to once a year on Trinity Sunday. It has been effectively dropped from the Catholic liturgy since the Second Vatican Council. It is however maintained in the "Forma Extraordinaria", per the decree Summorum Pontificum, and also in the rite of exorcism, both in the "Forma Ordinaria" and the "Forma Extraordinaria" of the Roman Rite.
In Lutheranism, the Athanasian Creed is—along with the Apostles' and Nicene Creeds—one of the three ecumenical creeds placed at the beginning of the 1580 Book of Concord, the historic collection of authoritative doctrinal statements (confessions) of the Lutheran Church. It is still used in the liturgy on Trinity Sunday.
A common visualisation of the first half of the Creed is the Shield of the Trinity.

</doc>
<doc id="2417" url="https://en.wikipedia.org/wiki?curid=2417" title="Alicante">
Alicante

Alicante (, ), or (), both the Valencian and Spanish being official names, is a city and port in Spain on the Costa Blanca, the capital of the province of Alicante and of the comarca of Alacantí, in the south of the Valencian Community. It is also a historic Mediterranean port. The population of the city of Alicante proper was 332,067, estimated , ranking as the second-largest Valencian city. Including nearby municipalities, the Alicante conurbation had 452,462 residents. The population of the metropolitan area (including Elche and satellite towns) was 757,085 estimates, ranking as the eighth-largest metropolitan area of Spain.
Naming.
The name of the city echoes the Arabic name "Laqant" (لَقَنْت) or "Al-Laqant" (ألَلَقَنْت), which in turn reflects the Latin "Lucentum".
History.
The area around Alicante has been inhabited for over 7000 years. The first tribes of hunter gatherers moved down gradually from Central Europe between 5000 and 3000 BC. Some of the earliest settlements were made on the slopes of Mount Benacantil. By 1000 BC Greek and Phoenician traders had begun to visit the eastern coast of Spain, establishing small trading ports and introducing the native Iberian tribes to the alphabet, iron and the pottery wheel. By the 3rd century BC, the rival armies of Carthage and Rome began to invade and fight for control of the Iberian Peninsula. The Carthaginian general Hamilcar Barca established the fortified settlement of "Akra Leuka" (Greek: , meaning "White Mountain" or "White Point"), where Alicante stands today.
Although the Carthaginians conquered much of the land around Alicante, the Romans would eventually rule Hispania Tarraconensis for over 700 years. By the 5th century AD, Rome was in decline and the Roman predecessor town of Alicante, known as "Lucentum" (Latin), was more or less under the control of the Visigothic warlord Theudimer. However neither the Romans nor the Goths put up much resistance to the Arab conquest of "Medina Laqant" in the 8th century. The Moors ruled southern and eastern Spain until the 13th century "Reconquista" (Reconquest). Alicante was finally taken in 1246 by the Castilian king Alfonso X, but it passed soon and definitively to the Kingdom of Valencia in 1298 with King James II of Aragon. It gained the status of Royal Village ("Vila Reial") with representation in the medieval Valencian Parliament ("Corts Valencianes").
After several decades of being the battlefield where the Kingdom of Castile and the Crown of Aragon clashed, Alicante became a major Mediterranean trading station exporting rice, wine, olive oil, oranges and wool. But between 1609 and 1614 King Felipe III expelled thousands of Moriscos who had remained in Valencia after the Reconquista, due to their cooperation with Barbary pirates who continually attacked coastal cities and caused much harm to trade. This act cost the region dearly; with so many skilled artisans and agricultural labourers gone, the feudal nobility found itself sliding into bankruptcy. Things got worse in the early 18th century; after the War of Spanish Succession, Alicante went into a long, slow decline, surviving through the 18th and 19th centuries by making shoes and growing agricultural produce such as oranges and almonds, and thanks to its fisheries. The end of the 19th century witnessed a sharp recovery of the local economy with increasing international trade and the growth of the city harbour leading to increased exports of several products (particularly during World War I when Spain was a neutral country).
During the early 20th century, Alicante was a minor capital that enjoyed the benefit of Spain's neutrality during World War I, and that provided new opportunities for the local industry and agriculture. The Rif War in the 1920s saw numerous "alicantinos" drafted to fight in the long and bloody campaigns in the former Spanish protectorate (Northern Morocco) against the Rif rebels. The political unrest of the late 1920s led to the victory of Republican candidates in local council elections throughout the country, and the abdication of King Alfonso XIII. The proclamation of the Second Spanish Republic was much celebrated in the city on 14 April 1931. The Spanish Civil War broke out on 17 July 1936. Alicante was the last city loyal to the Republican government to be occupied by dictator Franco's troops on 1 April 1939, and its harbour saw the last Republican government officials fleeing the country. Vicious air bombings were targeted on Alicante during the three years of civil conflict, most notably the bombing by the Italian "Aviazione Legionaria" of the Mercado de Abastos on 25 May 1938 in which more than 300 civilians perished.
The late 1950s and early 1960s saw the onset of a lasting transformation of the city by the tourist industry. Large buildings and complexes rose in nearby Albufereta (e.g. El Barco) and Playa de San Juan, with the benign climate being the biggest draw to attract prospective buyers and tourists who kept the hotels reasonably busy. New construction benefited the whole economy, as the development of the tourism sector also spawned new businesses such as restaurants, bars and other tourist-oriented enterprises. Also, the old airfield at Rabassa was closed and air traffic moved to the new El Altet Airport, which made a more convenient and modern facility for charter flights bringing tourists from northern European countries.
When dictator Franco died in 1975, his successor Juan Carlos I played his part as the living symbol of the transition of Spain to a democratic constitutional monarchy. The governments of regional communities were given constitutional status as "nationalities", and their governments were given more autonomy, including that of the Valencian region, the "Generalitat Valenciana".
The Port of Alicante has been reinventing itself since the industrial decline the city suffered in the 1980s (with most mercantile traffic lost to Valencia's harbour). In recent years, the Port Authority has established it as one of the most important ports in Spain for cruises, with 72 calls to port made by cruise ships in 2007 bringing some 80,000 passengers and 30,000 crew to the city each year. The moves to develop the port for more tourism have been welcomed by the city and its residents, but the latest plans to develop an industrial estate in the port have caused great controversy.
Economy.
Until the global recession which started in 2008, Alicante was one of the fastest-growing cities in Spain. The boom depended partly on tourism directed to the beaches of the Costa Blanca and particularly on the second residence-construction boom which started in the 1960s and revived again by the late 1990s. Services and public administration also play a major role in the city's economy. The construction boom has raised many environmental concerns and both the local autonomous government and city council are under scrutiny by the European Union. The construction surge was the subject of hot debates among politicians and citizens alike. The latest of many public battles concerns the plans of the Port Authority of Alicante to construct an industrial estate on reclaimed land in front of the city's coastal strip, in breach of local, national and European regulations. (See Port of Alicante for details).
The city serves as the headquarters of the European Union's Office for Harmonization in the Internal Market and a sizeable population of European public workers live there.
The campus of the University of Alicante lies in San Vicente del Raspeig, bordering the city of Alicante to the north. More than 27,000 students attend the University.
Since 2005 Ciudad de la Luz, one of the largest film studios in Europe, has had its base in Alicante. The studio has shot Spanish and international movies such as "Asterix at the Olympic Games" by Frédéric Forestier and Thomas Langmann, "Manolete" by Menno Meyjes.
Population.
The official population of Alicante in 2014 was 332,067 inhabitants and 757,085 in the metropolitan area "Alicante-Elche". About 15% of the population is foreign, most of them immigrants from Argentina, Ecuador, United Kingdom and Colombia who have arrived in the previous 20 years. There are also immigrants from other countries such as Germany, Romania, Russia, Algeria, Ukraine, Morocco and Italy, many of whom coming outside the EU are under illegal alien status and therefore are not accounted for in official population figures. The real percentage of foreign residents is higher, since the Alicante metropolitan area is home to many Northern European retirees who are officially still residents of their own countries. In the same pattern, a sizable number of permanent residents are Spanish nationals who officially still live in Madrid, the Basque provinces, or other areas of the country.
Government.
Gabriel Echávarri is the Mayor of the city. He was elected for the post on June 13, 2015, following the municipal elections on May 24, 2015. He was supported by the votes from his own group (6), plus those from leftist parties Guanyar Alacant (6) and Compromís (3), as well as from centre-right party Ciudadanos (6). The People's Party ("Partido Popular", PP), with only 8 elected seats, lost the majority.
In the previous municipal elections of May 2011, Sonia Castedo of People's Party won the elections with an absolute majority, but resigned in December 2014 due to her involvement in several corruption scandals, at present being under investigation. Her fellow party member Miguel Valor went on to become mayor up until Echávarri's election.
At the foot of the main staircase of the City Hall Building ("Ayuntamiento") is the zero point ("cota cero"), used as the point of reference for measuring the height above or below sea level of any point in Spain, due to the marginal tidal variations of the Mediterranean sea in Alicante.
Climate.
Alicante enjoys mild winter temperatures, hot summers and little rain, concentrated in equinoctial periods. Scientists tend to classify the climate of Alicante region according to Köppen climate classification as hot semi-arid ("BSh"). On average the temperature ranges between and in January, and between and in August, with an average annual temperature of . Daily variations in temperature are generally small because of the stabilising influence of the sea, although occasional periods of westerly wind can produce temperature changes of or more. Seasonal variations in temperature are also relatively small, meaning that winters are mild and summers are hot.
The average rainfall is per year. The cold drop means that September and October are the wettest months. Rainfall can be torrential, reaching over in a 24-hour period, leading to severe flooding. Because of this irregularity, only 35 rainy days are observed on average per year, and the annual number of sunshine hours is 2,953.
The record maximum temperature of was observed on 4 July 1994. The record minimum temperature of was recorded on 26 December 1970. The worst flooding in modern history occurred on 30 September 1997 when of rain fell within six hours. Temperatures under are very rare. Snow is unknown since 1926
Transport.
Alicante Airport outranks its Valencian counterpart, being among the busiest airports in Spain after Madrid, Barcelona, Palma de Mallorca and Málaga. It is connected with Madrid and Barcelona by frequent Iberia and Vueling flights, and with many Western European cities through carriers such as Ryanair, Easyjet, Air Berlin, Monarch Airlines, and Jet2.com. There are also regular flights to Algeria and Russia.
Alicante railway station is used by Cercanías linking Alicante with suburbs and Murcia. Long-range RENFE trains run frequently to Madrid, Barcelona, and Valencia.
Alicante Tram connects the city with outlying settlements along Costa Blanca. As of 2011, electric tram-trains run up to Benidorm, and diesel trains go further to Dénia.
The city has regular ferry services to the Balearic Islands and Algeria. The city is strongly fortified, with a spacious harbour.
Main sights.
Amongst the most notable features of the city are the Castle of Santa Bárbara, which sits high above the city, and the port of Alicante. The latter was the subject of bitter controversy in 2006–2007 as residents battled, successfully, to keep it from being changed into an industrial estate.
The Santa Bárbara castle is situated on Mount Benacantil, overlooking the city. The tower ("La Torreta") at the top, is the oldest part of the castle, while part of the lowest zone and the walls were constructed later in the 18th century.
The promenade "Explanada de España", lined by palm trees, is paved with 6.5 million marble floor tiles creating a wavy form and is one of the most lovely promenades in Spain. The Promenade extends from the Port of Alicante to the Gran Vía and ends at the famous statue of Mark Hersch. For the people of Alicante, the promenade is the meeting place for the traditional Spanish "paseo", or stroll along the waterfront in the evenings, and a venue for outdoor musical concerts. At the end of the promenade is a monument by the artist Bañuls of the 19th century.
"Barrio de la Santa Cruz" is a colourful quarter of the old city, situated on the south-west of Santa Bárbara castle. Its small houses climb up the hill leading to the walls and the castle, through narrow streets decorated with flags and tubs of flowers.
"L'Ereta Park" is situated on the foothills of Mount Benacantil, on the way to the castle. It runs from the Santa Bárbara castle down to the old part of Alicante and consists of several levels, routes, decks and rest stops which offer a panoramic view overlooking the city.
"El Palmeral Park" is one of the favorite parks of Alicante's citizens. It includes walking trails, children's playgrounds, ponds and brooks, picnic tables and an auditorium for concerts.
Just a few kilometers from Alicante on the Mediterranean Sea lies Tabarca island. What was once a haven for Barbary pirates is now a beautiful tourist attraction.
Other sights include:
There are a dozen museums in Alicante. On exhibition at the Archaeological Museum of Alicante (MARQ) are local artifacts dating from 100,000 years ago till the early 20th century. The collection is divided into different rooms representing three divisions of archaeological methodology: ground, urban and underwater archaeology, with dioramas, audiovisual and interactive zones. The archaeological museum won the European Museum of the Year Award in 2004. Gravina Museum of Fine Arts presents a number of paintings and sculptures from the 16th century to the 19th century. Asegurada Museum of Contemporary Art houses a major collection of twentieth-century art, composed mainly of works donated by .
Festivals.
The most important festival, the "Bonfires of Saint John" ("Fogueres de Sant Joan"), takes place during the summer solstice. This is followed a week later by seven nights of firework and pyrotechnic contests between companies on the urban beach "Playa del Postiguet". Another well-known festival is "Moros i Cristians" in Altozano or "San Blas" district. Overall, the city boasts a year-round nightlife for the enjoyment of tourists, residents, and a large student population of the University of Alicante. The nightlife social scene tends to shift to nearby Playa de San Juan (St. John's Beach) during the summer months.
Every summer in Alicante, a two-month-long programme of music, theatre and dance is staged in the Paseo del Puerto.
Sport.
The two established Alicante football teams are Hércules CF, which competes in the Spanish Segunda División B, and Alicante CF, which plays in Tercera División and was dissolved in 2014 due to economic problems. Hércules CF is more well known as it was in the first league in Spain during 96/97 and had many popular players as David Trezeguet, Royston Drenthe and Aedo Valvez. It is also known because, thanks to this team winning over FC Barcelona, Real Madrid won the league in 1997. Nowadays it hosts their home games at Estadio José Rico Pérez.
Basketball club Lucentum Alicante participates in the Spanish basketball league. It plays at the Centro de Tecnificación de Alicante.
Alicante serves as headquarters and the starting point of Volvo Ocean Race, a yacht race around the world. The latest race sailed in October 2014.
International relations.
Twin towns – sister cities.
Alicante is twinned with:
In 2009 a bid was made to twin Newcastle, United Kingdom with Alicante.

</doc>
<doc id="2418" url="https://en.wikipedia.org/wiki?curid=2418" title="August 4">
August 4


</doc>
<doc id="2421" url="https://en.wikipedia.org/wiki?curid=2421" title="Albrecht Achilles">
Albrecht Achilles

Albrecht Achilles may refer to:

</doc>
<doc id="2422" url="https://en.wikipedia.org/wiki?curid=2422" title="Ann Widdecombe">
Ann Widdecombe

Ann Noreen Widdecombe, (born 4 October 1947) is a former British Conservative Party politician and has been a novelist since 2000. She is a Privy Councillor and was the Member of Parliament for Maidstone from 1987 to 1997 and for Maidstone and The Weald from 1997 to 2010. She was a social conservative and a member of the Conservative Christian Fellowship. She retired from politics at the 2010 general election. Since 2002 she has also made numerous television and radio appearances, including as a television presenter. She is a convert from Anglicanism to Roman Catholicism.
As an MP, Widdecombe was known for opposing the legality of abortion, her opposition to various issues of LGBT equality such as an equal age of consent and the repeal of Section 28, her support for the re-introduction of the death penalty, the retention of blasphemy laws and her opposition to fox hunting.
Early life.
Born in Bath, Somerset, Widdecombe is the daughter of Rita Noreen (née Plummer; 1911-2007) and Ministry of Defence civil servant James Murray Widdecombe. Widdecombe's maternal grandfather, James Henry Plummer, was born to an Irish Catholic family of English descent in Crosshaven, County Cork in 1874. She attended the Royal Naval School in Singapore, and La Sainte Union Convent School in Bath. She then read Latin at the University of Birmingham and later attended Lady Margaret Hall, Oxford, to read Philosophy, Politics and Economics (PPE). She worked for Unilever (1973–75) and then as an administrator at the University of London (1975–87) before entering Parliament.
Councillor.
From 1976 to 1978, Widdecombe was a councillor on Runnymede District Council in Surrey. She contested the seat of Burnley in Lancashire in the 1979 general election and then, against David Owen, the Plymouth Devonport seat in the 1983 general election.
Member of Parliament.
She was first elected to the House of Commons in the 1987 general election as member for the constituency of Maidstone (which became Maidstone and The Weald in 1997).
Political views.
As an MP, Widdecombe expressed conservative views, including opposition to abortion; it was understood during her time in frontline politics that she would not become Health Secretary as long as this involved responsibility for abortions. Although a committed Christian, she has characterised the issue as one of life and death on which her view had been the same when she was agnostic. Along with John Gummer MP, she converted from the Church of England to the Catholic Church following the decision of the Church of England on the Ordination of women as priests. In her speech at the 2000 Conservative conference, she called for a zero tolerance policy of prosecution, with the punishment of £100 GBP fines for users of cannabis. This was well received by rank-and-file Conservative delegates.
Widdecombe consistently opposed LGBT equality. On the issue of an equal age of consent, she said in 2000: "I do not believe that issues of equality should override the imperatives of protecting the young." In 2003, Widdecombe proposed an amendment opposing repeal of Section 28 of the Local Government Act, which banned the promotion of homosexuality by local governments. Out of the 17 parliamentary votes considered by the Public Whip website to concern equal rights for homosexuals, Widdecombe took the opposing position in 15 cases, not being present at the other two votes. Widdecombe has also expressed her opposition to same-sex marriage, introduced by David Cameron's government in 2014, claiming that "the state must have a preferred model" and "a union that is generally open to procreation".
She is a committed animal lover and one of the few Conservative MPs to have consistently voted for the ban on fox hunting. Widdecombe was among more than 20 high-profile people who signed a letter to Members of Parliament in 2015 to oppose David Cameron's plan to amend the Hunting Act 2004.
She has expressed a variety of views on scientific issues such as climate change but has been opposed to legislation reducing emissions. Her views on the subject appear to have hardened over time. In 2007, she wrote that she did not want to belittle the issue but was sceptical of the claims that specific actions would prevent catastrophe, then in 2008 that her doubts had been "crystalised" by Nigel Lawson's book "An Appeal to Reason", before stating in 2009 that "There is no climate change, hasn’t anybody looked out of their window recently?" She was one of the five MPs who voted against the Climate Change Act 2008. In 2011 she expressed the view that "climate change money should go to armed services". The previous year, she voted to support a parliamentary motion supporting homeopathy, criticizing the Science and Technology Committee's Report on the subject.
Over the years, Widdecombe has expressed her support for a reintroduction of the death penalty, which was abolished in the UK in 1965. She notably spoke of her support for its reintroduction for the worst cases of murder in the aftermath of the murder of two 10-year-old girls from Soham, Cambridgeshire, in August 2002, in the Soham murders. She supported the argument that the death penalty would have deterrent value, as within five years of its abolition the national murder rate had more than doubled.
In government.
Widdecombe joined John Major's government as Parliamentary Under-Secretary of State for Social Security in 1990. In 1993, she was moved to the Department of Employment, and she was promoted to Minister of State the following year. In 1995, she joined the Home Office as Minister of State for Prisons and visited every prison in Britain.
Shadow Cabinet.
After the fall of the Conservative government to Labour in 1997, she served as Shadow Health Secretary between 1998 and 1999 and later as Shadow Home Secretary between 1999 and 2001 under William Hague.
Leadership contest and backbenches.
During the 2001 Conservative leadership election, she could not find sufficient support amongst Conservative MPs for her leadership candidacy. She first supported Michael Ancram, who was eliminated in the first round, and then Kenneth Clarke, who lost in the final round. She afterwards declined to serve in Iain Duncan Smith's Shadow Cabinet (although she indicated on the television programme "When Louis Met...", prior to the leadership contest, that she wished to retire to the backbenches anyway).
In the 2005 leadership election, she initially supported Kenneth Clarke again. Once he was eliminated, she turned support towards Liam Fox. Following Fox's subsequent elimination, she took time to reflect before finally declaring for David Davis. She expressed reservations over the eventual winner David Cameron, feeling that he did not, like the other candidates, have a proven track record, and she was later a leading figure in parliamentary opposition to his A-List policy, which she has said is "an insult to women". At the October 2006 Conservative Conference, she was Chief Dragon in a political version of the television programme "Dragons' Den", in which A-list candidates were invited to put forward a policy proposal, which was then torn apart by her team of Rachel Elnaugh, Oliver Letwin and Michael Brown.
In an interview with "Metro" in September 2006 she stated that if Parliament were of a normal length, it was likely she would retire at the next general election. She confirmed her intention to stand down to "The Observer"'s Pendennis diary in September 2007, and again in October 2007 after Prime Minister Gordon Brown quashed speculation of an autumn 2007 general election.
Widdecombe was one of the 98 MPs who voted to keep their expense details secret. When the expenses claims were leaked, however, Widdecombe was described by "The Daily Telegraph" as one of the "saints" amongst all MPs.
In May 2009, following the resignation of Michael Martin as Speaker of the House of Commons, it was reported that Widdecombe was gathering support for election as interim Speaker until the next general election. On 11 June 2009, she confirmed her bid to be the Speaker. She made it through to the second ballot but came last and was eliminated.
Widdecombe retired from politics at the 2010 general election. It was rumoured that she would be a Conservative candidate for Police and Crime Commissioner in 2012, but she refused. She has since spoken about her opposition to the Coalition Government and her surprise at not being given a peerage by David Cameron.
Recognition.
Widdecombe was appointed an Honorary Fellow of Canterbury Christ Church University at a ceremony held at Canterbury Cathedral on 30 January 2009.
Personal life and family.
Until her retirement at the 2010 general election, Widdecombe divided her time between her two homes – one in London and one in the village of Sutton Valence, Kent, in her constituency. She sold both of these properties, however, upon deciding to retire at the next general election. She shared her home in London with her widowed mother, Rita Widdecombe, until Rita's death, on 25 April 2007, aged 95. In March 2008, she purchased a house in Haytor, on Dartmoor in Devon, to where she has now retired. Her brother, Malcolm (1937–2010), who was an Anglican Canon in Bristol, retired in May 2009 and died of metastatic oesophageal cancer on 12 October 2010. Her nephew, Rev Roger Widdecombe, is an Anglican priest.
She has never married nor had any children. In November 2007 on BBC Radio 4 she described how a journalist once produced a profile on her with the assumption that she had had at least "one sexual relationship", to which Widdecombe replied: "Be careful, that's the way you get sued". When interviewer Jenni Murray asked if she had ever had a sexual relationship, Widdecombe laughed "it's nobody else's business".
Widdecombe has a fondness for cats and has a section of her website devoted to all the pet cats with which she has shared her life. In a recent interview, Widdecombe talked about her appreciation of music despite describing herself as "pretty well tone-deaf".
Religious views.
Widdecombe is a practising Catholic. She converted in 1993 after leaving the Church of England. Her reasons for leaving the latter were many, as she explained to reporters from the "New Statesman":
In 2010, Widdecombe turned down an offer to be Britain's next ambassador to the Holy See, being prevented from accepting by suffering a detached retina. She was made a Dame of the Order of St. Gregory the Great by Pope Benedict XVI for services to politics and public life on 31 January 2013.
Controversies.
In 1990, following the assassination of the Conservative politician Ian Gow by the Provisional Irish Republican Army (IRA), the Eastbourne by-election for his seat in the House of Commons was won by the Liberal Democrat David Bellotti. Upon the announcement, Widdecombe told the voters that the IRA would be "toasting their success".
In 1996, Widdecombe, as prisons minister, defended the Government's policy to shackle pregnant prisoners with handcuffs and chains when in hospital receiving ante-natal care. Widdecombe told the Commons the restrictions were needed to prevent prisoners from escaping. "Some MPs may like to think that a pregnant woman would not or could not escape. Unfortunately this is not true. The fact is that hospitals are not secure places in which to keep prisoners, and since 1990, 20 women have escaped from hospitals". Jack Straw, Labour's Home Affairs spokesman at the time, said it was "degrading and unnecessary" for a woman to be shackled at any stage .
In 1997, during the Conservative leadership election of William Hague, Widdecombe spoke out against Michael Howard, under whom she had served when he was Home Secretary. She remarked that "there is something of the night about him". The remark was considered to be extremely damaging to Howard, who was frequently satirised as a vampire thereafter. He came last in the poll. Howard went on to become party leader in 2003, however, and Widdecombe then stated, "I explained fully what my objections were in 1997 and I do not retract anything I said then. But this is 2005 and we have to look to the future and not the past."
In 2001, when Michael Portillo was running for leader of the Conservative Party, Widdecombe described him and his allies as "backbiters". She went on to say that, should he be appointed leader, she would never give him her allegiance.
Media work and appearances.
In 2002, she took part in the ITV programme "Celebrity Fit Club". Also in 2002 she took part in a Louis Theroux television documentary, depicting her life, both in and out of politics. In March 2004 she briefly became "The Guardian" newspaper's agony aunt, introduced with an Emma Brockes interview. In 2005 BBC Two showed six episodes of "The Widdecombe Project", an agony aunt television programme. In 2005, she appeared in a new series of "Celebrity Fit Club", but this time as a panel member dispensing wisdom and advice to the celebrities taking part. Also in 2005, she presented the show "Ann Widdecombe to the Rescue" in which she acted as an agony aunt, dispensing no-nonsense advice to disputing families, couples, and others across the UK. In 2005, she also appeared in a discussion programme on Five to discuss who had been England's greatest monarch since the Norman Conquest; her choice of monarch was Charles II.
She was the guest host of news quiz "Have I Got News for You" twice, in 2006 and 2007. Following her second appearance, Widdecombe vowed she would never appear on the show again because of comments made by panellist Jimmy Carr. She wrote, "His idea of wit is a barrage of filth and the sort of humour most men grow out of in their teens... here's no amount of money for which I would go through those two recording hours again. At one stage I nearly walked out." She did, however, stand by her appraisal of regular panellists Ian Hislop and Paul Merton, whom she has called "the fastest wits in showbusiness".
In 2007, she awarded the "University Challenge" trophy to the winners. In the same year, she was cast as herself in "The Sound of Drums", the 12th episode of the third series of the science-fiction drama "Doctor Who" supporting Mr Saxon, the alias of the Master.
Since 2007, Widdecombe has fronted a television series called "Ann Widdecombe Versus", on ITV1, in which she speaks to various people about things related to her as an MP, with an emphasis on confronting those responsible for problems she wished to tackle. On 15 August 2007 she talked about prostitution, the next week about benefits and the week after that about truancy. A fourth episode was screened on 18 September 2008 in which she travelled around London and Birmingham talking to girl gangs.
In 2009, Widdecombe appeared with Archbishop John Onaiyekan in an "Intelligence Squared" debate in which they defended the motion that the Catholic Church was a force for good. Arguing against the motion were Stephen Fry and Christopher Hitchens.
In October 2010, she appeared on BBC One's "Strictly Come Dancing", partnered by Anton du Beke, winning the support of some viewers despite low marks from the judges. After nine weeks of routines strongly flavoured by comedy the couple had received enough support in the public vote to stay in the contest. Widdecombe was eliminated from the competition on Sunday 5 December after the public vote had been combined with the judges' score; she was with Scott Maslen of "EastEnders" in the bottom two.
Widdecombe is currently filming a new quiz show with herself as questionmaster, for the Sky Atlantic channel, called "Cleverdicks". The show is being shown in 30 one-hour episodes in an initial series, starting 2012. It features four contestants, usually high quality members of the UK national quiz circuit and ends with a money round for the winner of each show.
On 23 April 2012 Widdecombe presented an hour-long documentary for BBC Radio 5 Live, "Drunk Again: Ann Widdecombe Investigates", looking at how the British attitude to getting drunk has changed over the last few years.
It was revealed in October 2012, that the year's Children in Need's appeal night will feature a "Strictly Come Dancing" special with former show favourites Russell Grant and Ann Widdecombe.
On 4 November 2012, Ann presented guest hosted one episode of BBC's "Songs of Praise" programme about singleness.
In October 2014 she appeared in the BBC series "Celebrity Antiques Road Trip", partnered with expert Mark Stacey, where the pair beat the rival team of Craig Revel Horwood and Catherine Southon.
Ann Widdecombe was a part of BBC TV Series 24 Hours in the Past, along with Colin Jackson, Alistair McGowan, Miquita Oliver, Tyger Drew-Honey and Zoe Lucker. The 4 part series was aired from 28 April till 19 May 2015 on BBC 1
Stage acting career.
Following her retirement, Widdecombe made her stage debut, on 9 December 2011, at The Orchard Theatre, Dartford in the Christmas pantomime "Snow White and the Seven Dwarfs", alongside "Strictly Come Dancing" judge Craig Revel Horwood. In April 2012, she had a ten-minute non-singing cameo part in Gaetano Donizetti's comic opera "La Fille Du Regiment", playing the Duchesse de Crackentorp. Ann reprised her pantomime performance, again with Revel Horwood, at The Swan Theatre, High Wycombe in December 2012.
Other interests.
Her non-political accomplishments include being a popular novelist. Widdecombe also currently writes a weekly column for the "Daily Express".
In October 2006, she pledged to boycott British Airways for suspending a worker who refused to hide her cross. The matter was resolved when the company reversed the suspension. In November 2006, she moved into the house of an Islington Labour Councillor to experience life on a council estate, her response to her experience being "Five years ago I made a speech in the House of Commons about the forgotten decents. I have spent the last week on estates in the Islington area finding out that they are still forgotten."
In January 2011 Widdecombe was joint President of the North of England Education Conference in Blackpool. She shared the responsibility with a young person from the town. She has also become a patron of The Grace Charity for M.E.
Widdecombe revealed, in an April 2012 interview with Matt Chorley of The Independent, that she was writing her autobiography, which she described as ".. rude about all and sundry, but an amount of truth is always necessary."
Widdecombe is a Patron of the charity Safe Haven for Donkeys in the Holy Land (SHADH) and in 2014 visited the SHADH Donkey Sanctuary in Palestine.

</doc>
<doc id="2425" url="https://en.wikipedia.org/wiki?curid=2425" title="Aurangzeb">
Aurangzeb

Abul Muzaffar Muhi-ud-Din Muhammad Aurangzeb (14 October 1618 – 20 February 1707), commonly known as Aurangzeb Alamgir and by his imperial title Alamgir ("world conqueror" or "universe conqueror") and simply referred to as Aurangzeb was the sixth Mughal Emperor and ruled over most of the Indian subcontinent during some parts of his reign. His reign lasted for 49 years from 1658 until his death in 1707.
Aurangzeb was a notable expansionist and during his reign, the Mughal Empire temporarily reached its greatest extent. During his lifetime, victories in the south expanded the Mughal Empire to more than 3.2 million square kilometres and he ruled over a population estimated as being in the range of 100–150 million subjects, with an annual yearly tribute of £38,624,680 in 1690 (the highest in the world at that time).
Aurangzeb's policies partly abandoned the legacy of pluralism, which remains a very controversial aspect of his reign and led to the downfall of the Mughal Empire. Rebellions and wars led to the exhaustion of the imperial Mughal treasury and army. He was a strong-handed authoritarian ruler, and following his death the expansionary period of the Mughal Empire came to an end, and centralized control of the empire declined rapidly.
Early life.
Aurangzeb was born on 4 November 1618, in Dahod, Gujarat. He was the third son and sixth child of Shah Jahan and Mumtaz Mahal. His father was a governor of Gujarat at that time. In June 1626, after an unsuccessful rebellion by his father, Aurangzeb and his brother Dara Shikoh were kept as hostages under their grandparents' (Nur Jahan and Jahangir) Lahore court. On 26 February 1628, Shah Jahan was officially declared the Mughal Emperor, and Aurangzeb returned to live with his parents at Agra Fort, where Aurangzeb received his formal education in Arabic and Persian. His daily allowance was fixed at Rs. 500 which he spent on religious education and the study of history. He also accused his brothers of alcoholism and womanising.
On 28 May 1633 Aurangzeb escaped death when a powerful war elephant stampeded through the Mughal Imperial encampment. He rode against the elephant and struck its trunk with a lance, and successfully defended himself from being crushed. Aurangzeb's valour was appreciated by his father who conferred him the title of "Bahadur" (Brave) and had him weighed in gold and presented gifts worth Rs. 200,000. This event was celebrated in Persian and Urdu verses and Aurangzeb said:
Early military campaigns and administration.
Bundela War.
On 15 December 1634, Aurangzeb was given his first command, comprising 10,000 horse and 4000 troopers. He was allowed to use the red tent, which was an imperial prerogative.
Subsequently, Aurangzeb was nominally in charge of the force sent to Bundelkhand with the intent of subduing the rebellious ruler of Orchha, Jhujhar Singh, who had attacked another territory in defiance of Shah Jahan's policy and was refusing to atone for his actions. By arrangement, Aurangzeb stayed in the rear, away from the fighting, and took the advice of his generals as the Mughal Army gathered and commenced the Siege of Orchha in 1635. The campaign was successful and Singh was removed from power.
Viceroy of the Deccan.
Aurangzeb was appointed Viceroy of the Deccan in 1636. After Shah Jahan's vassals had been devastated by the alarming expansion of Ahmednagar during the reign of the Nizam Shahi boy-prince Murtaza Shah III, the emperor dispatched Aurangzeb, who in 1636 brought the Nizam Shahi dynasty to an end. In 1637, Aurangzeb married the Safavid princess, Dilras Banu Begum, also known as Rabia-ud-Daurani. She was his first wife and chief consort. He also had an infatuation with a slave girl, Hira Bai, whose death at a young age greatly affected him. In his old age, he was under the charms of his concubine, Udaipuri Bai. The latter had formerly been a companion to Dara Shikoh. In the same year, 1637, Aurangzeb was placed in charge of annexing the small Rajput kingdom of Baglana, which he did with ease.
In 1644, Aurangzeb's sister, Jahanara, was burned when the chemicals in her perfume were ignited by a nearby lamp while in Agra. This event precipitated a family crisis with political consequences. Aurangzeb suffered his father's displeasure by not returning to Agra immediately but rather three weeks later. Shah Jahan had been nursing Jahanara back to health in that time and thousands of vassals had arrived in Agra to pay their respects. Shah Jahan was outraged to see Aurangzeb enter the interior palace compound in military attire and immediately dismissed him from his position of Viceroy of the Deccan; Aurangzeb was also no longer allowed to use red tents or to associate himself with the official military standard of the Mughal emperor.
In 1645, he was barred from the court for seven months and mentioned his grief to fellow Mughal commanders. Thereafter, Shah Jahan appointed him governor of Gujarat where he served well and was rewarded for bringing stability.
In 1647, Shah Jahan moved Aurangzeb from Gujarat to be governor of Balkh, replacing a younger son, Murad Baksh, who had proved ineffective there. The area was under attack from Uzbek and Turkoman tribes. Whilst the Mughal artillery and muskets were a formidable force, so too were the skirmishing skills of their opponents. The two sides were in stalemate and Aurangzeb discovered that his army could not live off the land, which was devastated by war. With the onset of winter, he and his father had to make a largely unsatisfactory deal with the Uzbeks, giving away territory in exchange for nominal recognition of Mughal sovereignty. The Mughal force suffered still further with attacks by Uzbeks and other tribesmen as it retreated through snow to Kabul. By the end of this two-year campaign, into which Aurangzeb had been plunged at a late stage, a vast sum of money had been expended for little gain.
Further inauspicious military involvements followed, as Aurangzeb was appointed governor of Multan and Sindh. His efforts in 1649 and 1652 to dislodge the Safavids at Kandahar, which they had recently retaken after a decade of Mughal control, both ended in failure as winter approached. The logistical problems of supplying an army at the extremity of the empire, combined with the poor quality of armaments and the intransigence of the opposition have been cited by John Richards as the reasons for failure, and a third attempt in 1653, led by Dara Shikoh, met with the same outcome.
Dara Shikoh's appointment followed the removal of Aurangzeb, who once again became Viceroy in the Deccan. He regretted this and harboured feelings that Shikoh had manipulated the situation to serve his own ends. Aurangbad's two "jagirs" (land grants) were moved there as a consequence of his return and, because the Deccan was a relatively impoverished area, this caused him to lose out financially. So poor was the area that grants were required from Malwa and Gujarat in order to maintain the administration and the situation caused ill-feeling between father and son. Shah Jahan insisted that things could be improved if Aurangzeb made efforts to develop cultivation, but the efforts that were made proved too slow in producing results to satisfy the emperor.
Aurangzeb proposed to resolve the situation by attacking the dynastic occupants of Golconda (the Qutb Shahis) and Bijapur (the Adil Shahis). As an adjunct to resolving the financial difficulties, the proposal would also extend Mughal influence by accruing more lands. Again, he was to feel that Dara had exerted influence on his father: believing that he was on the verge of victory in both instances, Aurangzeb was frustrated that Shah Jahan chose then to settle for negotiations with the opposing forces rather than pushing for complete victory.
War of Succession.
The four sons of Shah Jahan all held posts as governors during their father's reign. The emperor favoured the eldest, Dara Shikoh, and this had caused resentment among the younger three, who sought at various times to strengthen alliances between themselves and against Dara. There was no Muslim tradition of primogeniture and historian Satish Chandra says that "In the ultimate resort, connections among the powerful military leaders, and military strength and capacity er the real arbiters." Jacques Weber, emeritus professor of modern history at the University of Nantes, explains that "... the loyalties of these officials seem to have been motivated more by their own interests, the closeness of the family relation and above all the charisma of the pretenders than by ideological divides." The contest for power was primarily between Dara Shikoh and Aurangzeb because, although all four sons had demonstrated competence in their official roles, it was around these two that the supporting cast of officials and other influential people mostly circulated. There were ideological differences — Dara was an intellectual and a religious liberal in the mould of Akbar, while Aurangzeb was much more conservative — but, as historians Barbara D. Metcalf and Thomas R. Metcalf say, "To focus on divergent philosophies neglects the fact that Dara was a poor general and leader. It also ignores the fact that factional lines in the succession dispute were not, by and large, shaped by ideology ..." Muslims and Hindus did not divide along religious lines in their support for one pretender or the other nor, according to Chandra, is there much evidence to support the belief that Jahanara and other members of the royal family were split in their support. Jahanara, certainly, interceded at various times on behalf of all of the princes and was well-regarded by Aurangzeb even though she shared the religious outlook of Dara.
In 1656, a general under Qutb Shahi dynasty named Musa Khan lead an army of 12,000 Musketeers to attack Aurangzeb, and later on the same campaign Aurangzeb in turn rode against an army consisting 8,000 horsemen and 20,000 Karnataka Musketeers
Having made clear that he wanted Dara to succeed him, Shah Jahan became ill with stranguary in 1657 and was closeted under the care of his favourite son in the newly built city of Shahjahanabad (Old Delhi). Rumours of the death of Shah Jahan abounded and the younger sons were concerned that Dara might be hiding it for Machiavellian reasons. Thus, they took action: Shah Shuja prepared to contest the throne from Bengal, where he had been governor since 1637, while Murad did the same in his governorship of Gujarat and Aurangzeb did so in the Deccan. It is not known whether these preparations were made in the mistaken belief that the rumours of death were true or whether the challengers were just taking advantage of the situation.
After regaining some of his health, Shah Jahan moved to Agra and Dara urged him to send forces to challenge Shah Shuja and Murad, who had declared themselves rulers in their respective territories. While Shah Shuja was defeated at Banares in February 1658, the army sent to deal with Murad discovered to their surprise that he and Aurangzeb had combined their forces, the two brothers having agreed to partition the empire once they had gained control of it. The two armies clashed at Dharmat in April 1658, with Aurangzeb being the victor. Shuja was being chased through Bihar and the victory of Aurangzeb proved this to be a poor decision by Dara Shikoh, who now had a defeated force on one front and a successful force unnecessarily pre-occupied on another. Realising that his recalled Bihar forces would not arrive at Agra in time to resist the emboldened Aurangzeb's advance, Dara scrambled to form alliances in order but found that Aurangzeb had already courted key potential candidates. When Dara's disparate, hastily concocted army clashed with Aurangzeb's well-disciplined, battle-hardened force at the Battle of Samugarh in late May, neither Dara's men nor his generalship were any match for Aurangzeb. Dara had also become over-confident in his own abilities and, by ignoring advice not to lead in battle while his father was alive, he cemented the idea that he had usurped the throne. "After the defeat of Dara, Shah Jahan was imprisoned in the fort of Agra where he spent eight long years under the care of his favourite daughter Jahanara."
Aurangzeb then broke his arrangement with Murad Baksh, which probably had been his intention all along. Instead of looking to partition the empire between himself and Murad, he had his brother arrested and imprisoned at Gwalior Fort. Murad was executed on 4 December 1661, ostensibly for the murder of the "diwan" of Gujarat some time earlier. The allegation was encouraged by Aurangzeb, who caused the "diwan's" son to seek retribution for the death under the principles of Sharia law. Meanwhile, Dara gathered his forces, and moved to the Punjab. The army sent against Shuja was trapped in the east, its generals Jai Singh and Dilir Khan submitted to Aurangzeb, but Dara's son, Suleiman Shikoh, escaped. Aurangzeb offered Shah Shuja the governorship of Bengal. This move had the effect of isolating Dara Shikoh and causing more troops to defect to Aurangzeb. Shah Shuja, who had declared himself emperor in Bengal began to annex more territory and this prompted Aurangzeb to march from Punjab with a new and large army that fought during the Battle of Khajwa, where Shah Shuja and his chain-mail armored war elephants were routed by the forces loyal to Aurangzeb. Shah Shuja then fled to Arakan (in present-day Burma), where he was executed by the local rulers.
With Shuja and Murad disposed of, and with his father immured in Agra, Aurangzeb pursued Dara Shikoh, chasing him across the north-western bounds of the empire. Aurangzeb claimed that Dara was no longer a Muslim and accused him of poisoning the Mughal Grand Vizier Saadullah Khan. Both of these statements however lacked any evidence. After a series of battles, defeats and retreats, Dara was betrayed by one of his generals, who arrested and bound him. In 1658, Aurangzeb arranged his formal coronation in Delhi.
"On 10 August 1659, Dara was executed on grounds of apostasy." Having secured his position, Aurangzeb confined his frail father at the Agra Fort but did not mistreat him. Shah Jahan was cared for by Jahanara and died in 1666.
Reign.
Establishment of Islamic law.
Historian Katherine Brown has noted that "The very name of Aurangzeb seems to act in the popular imagination as a signifier of politico-religious bigotry and repression, regardless of historical accuracy." The subject is controversial and, despite no proof, has resonated in modern times with popularly accepted claims that he intended to destroy the Bamiyan Buddhas. As a political and religious conservative, Aurangzeb chose not to follow the liberal religious viewpoints of his predecessors after his ascension. Shah Jahan had already moved away from the liberalism of Akbar, although in a token manner rather than with the intent of suppressing Hinduism, and Aurangzeb took the change still further. Though the approach to faith of Akbar, Jahangir and Shah Jahan was more syncretic than Babur, the founder of the empire, Aurangzeb's position is not so obvious. His emphasis on sharia competed, or was directly in conflict, with his insistence that "zawabit" or secular decrees could supersede sharia. Despite claims of sweeping edicts and policies, contradictory accounts exist. He sought to codify Hanafi law by the work of several hundred jurists, called Fatawa-e-Alamgiri. It is possible the War of Succession and continued incursions combined with Shah Jahan's spending made cultural expenditure impossible.
As emperor, Aurangzeb banned alcoholism, gambling, castration, servitude, eunuchs, music, nautch and narcotics in the Mughal Empire. He learnt that at Sindh, Multan, Thatta and particularly at Varanasi, the Hindu Brahmins attracted large numbers of indigenous local Muslims to their discourses. He ordered the Subahdars of these provinces to demolish the schools and the temples of non-Muslims. Aurangzeb also ordered Subahdars to punish Muslims who dressed like non-Muslims. The executions of the antinomian Sufi mystic Sarmad Kashani and the ninth Sikh Guru Tegh Bahadur bear testimony to Aurangzeb's religious intolerance; the former was beheaded on multiple accounts of heresy, the latter, according to Sikhs, because he objected to Aurangzeb's forced conversions. According to other sources, there is no official account that Aurangzeb forcefully converted people. He imposed Jizya on non-Muslims. Further, Aurangzeb levied discriminatory taxes on Hindu merchants at the rate of 5% as against 2.5% on Muslim merchants. He ordered to dismiss all Hindu "quanungos" and "patwaris" from revenue administration.
Another instance of Aurangzeb's notoriety was his policy of temple destruction, for which figures vary wildly from 80 to 60,000. Indian historian Harbans Mukhia wrote that "In the end, as recently recorded in Richard Eaton's careful tabulation, some 80 temples were demolished between 1192 and 1760 (15 in Aurangzeb's reign) and he compares this figure with the claim of 60,000 demolitions, advanced rather nonchalantly by 'Hindu nationalist' propagandists,' although even in that camp professional historians are slightly more moderate." Among the Hindu temples he demolished were the three most sacred: the Kashi Vishwanath temple, Kesava Deo temple and Somnath temple. He built large mosques in their place. In 1679, he ordered destruction of several prominent temples that had become associated with his enemies: these included the temples of Khandela, Udaipur, Chittor and Jodhpur. Historian Richard Eaton believes the overall understanding of temples to be flawed. As early as the sixth century, temples became vital political landmarks as well as religious ones. He writes that not only was temple desecration widely practised and accepted, it was a necessary part of political struggle. 
Francois Bernier, who traveled and chronicled Mughal India during the War of Succession, notes the distaste of both Shah Jahan and Aurangzeb for Christians. This led to the demolition of Christian settlements near the European factories and enslavement of Christian converts by Shah Jahan. Furthermore, Aurangzeb stopped all the aid to Christian missionaries (Frankish Padres) that had been initiated by Akbar and Jahangir.
Ram Puniyani states that Aurangzeb was not always fanatically anti-Hindu, and kept changing his policies depending on the needs of the situation. He banned the construction of new temples, but permitted the repair and maintenance of existing temples. He also made generous donations of "jagir"s to several temples to win the sympathies of his Hindu subjects. There are several "firman"s (orders) in his name, supporting temples and gurudwaras, including Mahakaleshwar temple of Ujjain, Balaji temple of Chitrakoot, Umananda Temple of Guwahati and the Shatrunjaya Jain temples. During his time, the number of Hindu Mansabdars increased from 22% to 31% in the Mughal administration as he needed them to continue his fight in the Deccan.
Execution of opponents.
The first prominent execution during the long reign of Aurangzeb started with that of his brother Prince Dara Shikoh, who was accused of being influenced by Hinduism although some sources argue it was done for political reasons. Aurangzeb had his allied brother Prince Murad Baksh held for murder, judged and then executed. Aurangzeb is accused of poisoning his imprisoned nephew Sulaiman Shikoh.
Aurangzeb then executed Sarmad Kashani a controversial Sufi mystic of Jewish origins.
Later Aurangzeb executed Sambhaji the leader of the Maratha Confederacy. During his trial he was found guilty of murder and violence, atrocities against the Muslims of Burhanpur and Bahadurpur in Berar by Marathas under his command.
The Sikh leader Guru Tegh Bahadur was arrested on orders by Aurangzeb, found guilty of blasphemy by a Qadi's court and executed.
Expansion of the Mughal Empire.
Throughout his reign, Aurangzeb engaged in almost constant warfare. He built up a massive army and began a program of military expansion along all the boundaries of his empire. He pushed north-west into the Punjab and also drove south, conquering two further Muslim kingdoms - the Adil Shahis of Bijapur and Qutbshahis of Golconda — to add to the defeat of the Ahmednagar Sultanate that had been accomplished in 1636 while he had been viceroy of the Deccan. These new territories were administered by the Mughal Nawabs loyal to Aurangzeb.
Soon after seizing the throne, Aurangzeb began advancements against the unruly Sultan of Bijapur and during 1657, the Mughals are known to have utilized rockets during the Siege of Bidar, against Sidi Marjan. Aurangzeb's forces discharged rockets and grenades while scaling the walls, and Sidi Marjan himself was mortally wounded after a rocket struck his large gunpowder depot. After twenty-seven days of hard fighting, Bidar was captured by the Mughals.
In 1663, during his visit to Ladakh, Aurangzeb established direct control over that part of the empire and loyal subjects such as Deldan Namgyal agreed to pledge tribute and loyalty. Deldan Namgyal is also known to have constructed a Grand Mosque in Leh, which he dedicated to Mughal rule.
In 1664, Shaista Khan (the son of Asaf Khan IV), was appointed the Subedar of Bengal. He immediately eliminated Portuguese and Arakanese pirates from the region, and in 1666 led an army of 70,000 men to recapture the port of Chittagong from the Arakanese king Sanda Thudhamma. Chittagong remained a key port throughout Mughal rule.
In 1685, Aurangzeb dispatched his son, Muhammad Azam Shah, with a force of nearly 50,000 men to capture Bijapur Fort and defeat Sikandar Adil Shah (the ruler of Bijapur) who refused to be a vassal. The Mughals could not make any advancements upon Bijapur Fort mainly because of the superior usage of cannon batteries on both sides. Outraged by the stalemate Aurangzeb himself arrived on 4 September 1686 and commanded the Siege of Bijapur; after eight days of fighting, the Mughals were victorious.
Only one remaining ruler, Abul Hasan Qutb Shah (the Qutbshahi ruler of Golconda), refused to surrender. He and his servicemen fortified themselves at Golconda and fiercely protected the Kollur Mine, which was then probably the world's most productive diamond mine, and an important economic asset. In 1687, Aurangzeb led his grand Mughal army against the Deccan Qutbshahi fortress during the Siege of Golconda. The Qutbshahis had constructed massive fortifications throughout successive generations on a granite hill over 400 ft high with an enormous eight-mile long wall enclosing the city. The main gates of Golconda had the ability to repulse any war elephant attack. Although the Qutbshahis maintained the impregnability of their walls, at night Aurangzeb and his infantry erected complex scaffolding that allowed them to scale the high walls. During the eight-month siege the Mughals faced many hardships including the death of their experienced commander Kilich Khan Bahadur. Eventually, Aurangzeb and his forces managed to penetrate the walls by capturing a gate, and their entry into the fort led Abul Hasan Qutb Shah to surrender peacefully.
Military equipment.
Mughal cannon making skills advanced during the 17th century. One of the most impressive Mughal cannons is known as the Zafarbaksh, which is a very rare "composite cannon", that required skills in both wrought-iron forge welding and bronze-casting technologies and the in-depth knowledge of the qualities of both metals.
Aurangzeb military entourage consisted of 16 cannons including the "Azdaha Paikar" (which, was capable of firing a 33.5 kg ordnance) and "Fateh Rahber" (20 feet long with Persian and Arabic inscriptions).
The "Ibrahim Rauza" was also a famed cannon, which was well known for its multi-barrels. François Bernier, the personal physician to Aurangzeb, observed versatile Mughal gun-carriages each drawn by two horses.
Despite these innovations, most soldiers used bows and arrows, the quality of sword manufacture was so poor that they preferred to use ones imported from England, and the operation of the cannons was entrusted not to Mughals but to European gunners. Other weapons used during the period included rockets, cauldrons of boiling oil, muskets and manjaniqs (stone-throwing catapults).
Infantry who were later called Sepoy and who specialized in siege and artillery emerged during the reign of Aurangzeb
War elephants.
In the year 1703, the Mughal commander at Coromandel, Daud Khan Panni spent 10,500 coins to purchase 30 to 50 war elephants from Ceylon.
Art and Culture.
Aurangzeb was known to be of a more austere nature than his predecessors. Being religious he encouraged Islamic calligraphy. His reign also saw the building of the Lahore badshahi Mosque, and Bibi ka Maqbara in Aurangabad for his wife Rabia-ud-Daurani.
Calligraphy.
The Mughal Emperor Aurangzeb is known to have patronized works of Islamic Calligraphy during his reign particularly Syed Ali Tabrizi.
Architecture.
Unlike his father, Aurangzeb was not much interested in architecture. The structure of Bibi Ka Maqbara in Aurangabad,which now is a historical monument was constructed by the sons of Aurangzeb in remembrance of their mother. The inspiration came from Taj mahal as is quite visible from its architecture. Aurangzeb ordered the construction of the Badshahi Mosque in Lahore. Aurangzeb constructed a small marble mosque known as the Moti Masjid (Pearl Mosque) in the Red Fort complex in Delhi. He also constructed a mosque on Benares. The mosque he constructed in Srinagar is still the largest in Kashmir.
Textile.
The Textile industry in the Mughal Empire emerged very firmly during the reign of the Mughal Emperor Aurangzeb and was particularly well noted by Francois Bernier, a French physician of the Mughal Emperor. Francois Bernier writes how "Karkanahs", or workshops for the artisans, particularly in textiles flourished by "employing hundreds of embroiderers, who were superintended by a master". He further writes how "Artisans manufacture of silk, fine brocade, and other fine muslins, of which are made turbans, robes of gold flowers, and tunics worn by females, so delicately fine as to wear out in one night, and cost even more if they were well embroidered with fine needlework".
He also explains the different techniques employed to produce such complicated textiles such as "Himru" (whose name is Persian for "brocade"), "Paithani" (whose pattern is identical on both sides), "Mushru" (satin weave) and how "Kalamkari", in which fabrics are painted or block-printed, was a technique that originally came from Persia. Francois Bernier provided some of the first, impressive descriptions of the designs and the soft, delicate texture of Pashmina Shawls also known as "Kani", which were very valued for their warmth and comfort among the Mughals, and how these textiles and shawls eventually began to find their way to France and England.
Foreign relations.
As soon as he became emperor, Aurangzeb sent some of the finest ornate gifts such as carpets, lamps, tiles and others to the Islamic shrines at Mecca and Medina. He also ordered the construction of very large ships in Surat that would transport these gifts and even pilgrims to the Hijaz. These annual expeditions organized by Aurangzeb were led by Mir Aziz Badakhshi who died in Mecca of natural causes but managed to deliver more than 45,000 silver coins and several thousand Kaftans of honor.
Relations with the Uzbek.
Subhan Quli, Balkh's Uzbek ruler was the first to recognize him in 1658 and requested for a general alliance, he worked alongside the new Mughal Emperor since 1647 when Aurangzeb was the Subedar of Balkh.
Relations with the Safavid dynasty.
Aurangzeb received the embassy of Abbas II of Persia in 1660 and returned them with gifts. However relations between the Mughal Empire and the Safavid dynasty were tense because the Persians attacked the Mughal army positioned near Kandahar. Aurangzeb prepared his armies in the Indus River Basin for a counteroffensive, but Abbas II's death in 1666 caused Aurangzeb to end all hostilities. Aurangzeb's rebellious son, Sultan Muhammad Akbar, sought refuge with Suleiman I of Persia, who had rescued him from the Imam of Musqat and later refused to assist him in any military adventures against Aurangzeb.
Relations with the French.
In the year 1667 the French East India Company ambassadors Le Gouz and Bebert presented Louis XIV of France's letter which urged the protection of French merchants from various rebels in the Deccan. In response to the letter Aurangzeb issued a Firman allowing the French to open a factory in Surat.
Relations with the Sultanate of Maldives.
In the 1660s, the Sultan of the Maldives, Ibrahim Iskandar I, requested help from Aurangzeb's representative, the Faujdar of Balasore. The sultan was concerned about the impact of Dutch and English trading ships but the powers of Aurangzeb did not extend to the seas, the Maldives were not under his governance and nothing came of the request.
Relations with the Ottoman Empire.
In 1688 the desperate Ottoman Sultan Suleiman II urgently requested for assistance against the rapidly advancing Austrians, during the Ottoman–Habsburg War. However, Aurangzeb and his forces were heavily engaged in the Deccan Wars against the Marathas to commit any formal assistance to their Ottoman allies.
Relations with the English.
In 1686, the English East India Company, which had unsuccessfully tried to obtain a firman, an imperial directive that would grant England regular trading privileges throughout the Mughal empire, initiated the so-called Child's War. This hostility against the empire ended in disaster for the English, particularly when Aurangzeb dispatched a strong fleet from Janjira commanded by the Sidi Yaqub and manned by Mappila loyal to Ali Raja Ali II and Abyssinian sailors firmly blockaded Bombay in 1689. In 1690 the company sent envoys to Aurangzeb's camp to plead for a pardon. The company's envoys had to prostrate themselves before the emperor, pay a large indemnity, and promise better behavior in the future.
In September 1695, English pirate Henry Every perpetrated one of the most profitable pirate raids in history with his capture of a Grand Mughal convoy near Surat. The Indian ships had been returning home from their annual pilgrimage to Mecca when the pirates struck, capturing the "Ganj-i-Sawai", reportedly the greatest ship in the Muslim fleet, and its escorts in the process. When news of the piracy reached the mainland, a livid Aurangzeb nearly ordered an armed attack against the English-governed city of Bombay, though he finally agreed to compromise after the East India Company promised to pay financial reparations, estimated at £600,000 by the Mughal authorities. Meanwhile, Aurangzeb shut down four of the East India Company's factories, imprisoned the workers and captains (who were nearly lynched by a rioting mob), and threatened to put an end to all English trading in India until Every was captured. The Privy Council and East India Company offered a massive bounty for Every's apprehension, leading to the first worldwide manhunt in recorded history. However, Every successfully eluded capture.
In 1702, Aurangzeb sent Daud Khan Panni, the Mughal Empire's Subhedar of the Carnatic region, to besiege and blockade Fort St. George for more than three months. The governor of the fort Thomas Pitt was instructed by the English East India Company to sue for peace.
Administrative reforms.
Revenue.
Aurangzeb's exchequer raised a record £100 million in annual revenue through various sources like taxes, customs and land revenue, "et al." from 24 provinces.
Coins.
Aurangzeb felt that verses from the "Quran" should not be stamped on coins, as done in former times, because they were constantly touched by the hands and feet of people. His coins had the name of the mint city and the year of issue on one face, and, the following couplet on other:
Rebellions.
By 1700, the Marathas attacked the Mughal provinces from the Deccan and secessionist agendas from the Rajputs, Hindu Jats and Sikhs rebelled against the Mughal Empire's administrative and economic systems.
Jat rebellion.
In 1669, Hindu Jats began to organize a rebellion that is believed to have been caused by Aurangzeb's imposition of Jizya (a form of organized religious taxation). The Jats were led by "Gokula", a rebel landholder from Tilpat. By the year 1670 20,000 Jat rebels were quelled and the Mughal Army took control of Tilpat, Gokula's personal fortune amounted to 93,000 gold coins and hundreds of thousands of silver coins.
Gokula was caught and executed. But the Jats continued to terrorize the Mughals and attacked Akbar's mausoleum the gold, silver and fine carpets within the tomb . There are claims that Jats caused two large silver doors at the entrance of the Taj Mahal to be stolen and melted down. However, Jats later established their independent state of Bharatpur.
Mughal–Maratha Wars.
In 1657, while Aurangzeb attacked Golconda and Bijapur in the Deccan, the Hindu Maratha warrior aristocrat, Shivaji, used guerrilla tactics to take control of three Adil Shahi forts formerly under his father's command. With these victories, Shivaji assumed de facto leadership of many independent Maratha clans. The Marathas harried the flanks of the warring Adil Shahis and Mughals, gaining weapons, forts, and territory. Shivaji's small and ill-equipped army survived an all out Adil Shahi attack, and Shivaji personally killed the Adil Shahi general, Afzal Khan. With this event, the Marathas transformed into a powerful military force, capturing more and more Adil Shahi and Mughal territories. Shivaji went on to neutralise Mughal power in the region.
In 1659, Aurangzeb sent his trusted general and maternal uncle Shaista Khan, the Wali in Golconda to recover forts lost to the Maratha rebels. Shaista Khan drove into Maratha territory and took up residence in Pune. But in a daring raid on the governor's palace in Pune during a midnight wedding celebration, the Marathas killed Shaista Khan's son and maimed Shaista Khan by cutting off the fingers of his hand. Shaista Khan, however, survived and was re-appointed the administrator of Bengal going on to become a key commander in the war against the Ahoms.
Shivaji captured forts belonging to both Mughals and Bijapur. At last Aurangzeb ordered the armament of the Daulatabad Fort with two bombards (the Daulatabad Fort was later utilized as a Mughal bastion during the Deccan Wars). Aurangzeb also sent his general Raja Jai Singh of Amber, a Hindu Rajput, to attack the Marathas. Jai Singh won the fort of Purandar after fierce battle in which the Maratha commander Murarbaji fell. Foreseeing defeat, Shivaji agreed for a truce and a meeting with Aurangjeb at Delhi. Jai Singh also promised Shivaji his safety, placing him under the care of his own son, the future Raja Ram Singh I. However, circumstances at the Mughal court were beyond the control of the Raja, and when Shivaji and his son Sambhaji went to Agra to meet Aurangzeb, they were placed under house arrest, from which they managed to effect a daring escape.
Shivaji returned to the Deccan, and crowned himself "Chhatrapati" or the ruler of the Maratha Confederacy in 1674. While Aurangzeb continued to send troops against him, Shivaji expanded Maratha control throughout the Deccan until his death in 1680. Shivaji was succeeded by his son, Sambhaji. Militarily and politically, Mughal efforts to control the Deccan continued to fail.
Aurangzeb's reign over the empire reached its climax the emperor, he no longer honored the rights of Christians, Jews, Muslims, Dadupanthis, Stargazers, Malakis, Atheists, Brahmins, Jains, in fact all the communities of the Empire. His imposition of Jizya upon communities that were not adherents of Islam, led to the rise of the opportunistic Shivaji and his Maratha Confederacy, whose leadership clearly indicated to Aurangzeb in a letter "Protesting against Imposition of Jaziya (2nd April 1679)".
On the other hand, Aurangzeb's third son Akbar left the Mughal court along with a few Muslim Mansabdar supporters and joined Muslim rebels in the Deccan. Aurangzeb in response moved his court to Aurangabad and took over command of the Deccan campaign. The rebels were defeated and Akbar fled south to the shelter of Sambhaji, Shivaji's successor. More battles ensued, and Akbar fled to Persia and never returned.
In 1689, Aurangzeb's forces captured Sambhaji. His successor Rajaram and his Maratha forces fought individual battles against the forces of the Mughal Empire, and territory changed hands repeatedly during years of interminable warfare. As there was no central authority among the Marathas, Aurangzeb was forced to contest every inch of territory, at great cost in lives and money. Even as Aurangzeb drove west, deep into Maratha territory – notably conquering Satara — the Marathas expanded their attacks further into Mughal lands – Malwa, Hyderabad and Jinji in Tamil Nadu. Aurangzeb waged continuous war in the Deccan for more than two decades with no resolution. He thus lost about a fifth of his army fighting rebellions led by the Marathas in Deccan India. He traveled a long distance to the Deccan to conquer the Marathas and eventually died at the age of 90, still fighting the Marathas.
Aurangzeb's shift from conventional warfare to anti-insurgency in the Deccan region shifted the paradigm of Mughal military thought. There were conflicts between Marathas and Mughals in Pune, Jinji, Malwa and Vadodara. The Mughal Empire's port city of Surat was sacked twice by the Marathas during the reign of Aurangzeb and the valuable port was in ruins.
Ahom campaign.
While Aurangzeb and his brother Shah Shuja had been fighting against each other, the Hindu rulers of Kuch Behar and Assam took advantage of the disturbed conditions in the Mughal Empire, had invaded imperial dominions. For three years they were not attacked, but in 1660 Mir Jumla II, the viceroy of Bengal, was ordered to recover the lost territories.
The Mughals set out in November 1661, and within weeks occupied the capital of Kuch Behar after a few fierce skirmishes. The Kuch Behar was annexed, and the Mughal Army reorganized and began to retake their territories in Assam. Mir Jumla II's forces captured Pandu, Guwahati, and Kajali practically unopposed. In February 1662, Mir Jumla II initiated the Siege of Simalugarh and after the Mughal cannon breached the fortifications, the Ahoms abandoned the fort and escaped. Mir Jumla II then proceeded towards Garhgaon the capital of the Ahom kingdom, which was reached on 17 March 1662, although the ruler Raja Sutamla fled and the victorious Mughals captured 100 elephants, about 300,000 coins of silver, 8000 shields, 1000 ships, and 173 massive stores of rice.
Later that year in December 1663, the aged Mir Jumla II died on his way back to Dacca of natural causes, but skirmishes continued between the Mughals and Ahoms after the rise of Chakradhwaj Singha, who refused to pay further indemnity to the Mughals and during the wars that continued the Mughals suffered great hardships. Munnawar Khan emerged as a leading figure and is known to have supplied food to vulnerable Mughal forces in the region near Mathurapur. Although the Mughals under the command of Syed Firoz Khan the Faujdar at Guwahati were overrun by two Ahom armies in the year 1667, but they continued to hold and maintain presence along their the eastern territories even after the Battle of Saraighat in the year 1671.
The Battle of Saraighat was fought in 1671 between the Mughal empire (led by the Kachwaha king, Raja Ramsingh I), and the Ahom Kingdom (led by Lachit Borphukan) on the Brahmaputra river at Saraighat, now in Guwahati. Although much weaker, the Ahom Army defeated the Mughal Army by brilliant uses of the terrain, clever diplomatic negotiations to buy time, guerrilla tactics, psychological warfare, military intelligence and by exploiting the sole weakness of the Mughal forces—its navy.
The Battle of Saraighat was the last battle in the last major attempt by the Mughals to extend their empire into Assam. Though the Mughals managed to regain Guwahati briefly after a later Borphukan deserted it, the Ahoms wrested control in the Battle of Itakhuli in 1682 and maintained it till the end of their rule.
Satnami rebellion.
In May 1672, the Satnami sect obeying the commandments of an "old toothless woman" (according to Mughal accounts) organized a massive revolt in the agricultural heartlands of the Mughal Empire. The Satnamis were known to have shaved off their heads and even eyebrows and had temples in many regions of Northern India. They began a large-scale rebellion 75 miles southwest of Delhi.
The Satnamis believed they were invulnerable to Mughal bullets and believed they could multiply in any region they entered. The Satnamis initiated their march upon Delhi and overran small-scale Mughal infantry units.
Aurangzeb responded by organizing a Mughal army of 10,000 troops and artillery, and dispatched detachments of his own personal Mughal imperial guards to carry out several tasks. In order to boost Mughal morale, Aurangzeb wrote Islamic prayers, made amulets, and drew designs that would become emblems in the Mughal Army. This rebellion would have a serious aftermath effect on the Punjab.
Sikh Rebels.
Early in Aurangzeb's reign, various insurgent groups of Sikhs engaged Mughal troops in increasingly bloody battles. The ninth Sikh Guru, Guru Tegh Bahadur, like his predecessors was opposed to conversion of the local population as he considered it wrong. According to Sikh sources, approached by Kashmiri Pandits to help them retain their faith and avoid forced religious conversions, Guru Tegh Bahadur took on Aurangzeb. Other sources however state that Aurangzeb did not forcefully convert people. The emperor perceived the rising popularity of the Guru as a threat to his sovereignty and in 1670 had him executed, which infuriated the Sikhs. In response, Guru Tegh Bahadur's son and successor, Guru Gobind Singh, further militarized his followers, starting with the establishment of Khalsa in 1699, eight years before Aurangzeb's death. In 1705, Guru Gobind Singh sent a letter entitled "Zafarnamah" to Aurangzeb. This drew attention to Auranzeb's cruelty and how he had betrayed Islam. The letter caused him much distress and remorse. Guru Gobind Singh's formation of Khalsa in 1699 led to the establishment of the Sikh Confederacy and later Sikh Empire.
Pashtun rebellion.
The Pashtun revolt in 1672 under the leadership of the warrior poet Khushal Khan Khattak of Kabul, was triggered when soldiers under the orders of the Mughal Governor Amir Khan allegedly molested women of the Pashtun tribes in modern-day Kunar Province of Afghanistan. The Safi tribes retaliated against the soldiers. This attack provoked a reprisal, which triggered a general revolt of most of tribes. Attempting to reassert his authority, Amir Khan led a large Mughal Army to the Khyber Pass, where the army was surrounded by tribesmen and routed, with only four men, including the Governor, managing to escape.
After that the revolt spread, with the Mughals suffering a near total collapse of their authority in the Pashtun belt. The closure of the important Attock-Kabul trade route along the Grand Trunk road was particularly disastrous. By 1674, the situation had deteriorated to a point where Aurangzeb camped at Attock to personally take charge. Switching to diplomacy and bribery along with force of arms, the Mughals eventually split the rebels and partially suppressed the revolt, although they never managed to wield effective authority outside the main trade route.
Death and legacy.
By 1689, almost all of Southern India was a part of the Mughal Empire and after the conquest of Golconda, Aurangzeb may have been the richest and most powerful man alive. Mughal victories in the south expanded the Mughal Empire to 3.2 million square kilometres, with a population estimated as being between 100 million and 150 million. But this supremacy was short-lived. Jos Gommans, Professor of Colonial and Global History at the University of Leiden, says that "... the highpoint of imperial centralisation under emperor Aurangzeb coincided with the start of the imperial downfall."
Aurangzeb's vast imperial campaigns against rebellion-affected areas of the Mughal Empire caused his opponents to exaggerate the "importance" of their rebellions. The results of his campaigns were made worse by the incompetence of his regional Nawabs.
Muslim views regarding Aurangzeb vary. Most Muslim historians believe that Aurangzeb was the last powerful ruler of an empire inevitably on the verge of decline. The major rebellions organized by the Sikhs and the Marathas had deep roots in the remote regions of the Mughal Empire.
Unlike his predecessors, Aurangzeb considered the royal treasury to be held in trust for the citizens of his empire. He made caps and copied the Quran to earn money for his use. Aurangzeb constructed a small marble mosque known as the Moti Masjid (Pearl Mosque) in the Red Fort complex in Delhi. However, his constant warfare, especially with the Marathas, drove his empire to the brink of bankruptcy just as much as the wasteful personal spending and opulence of his predecessors. Aurangzeb knew he would not return to the throne after his final campaign against the Marathas in 1706, in which he was joined by newly emerging commanders in the Mughal army such as Syed Hassan Ali Khan Barha, Saadat Ali Khan and Asaf Jah I, and Daud Khan.
The Indologist Stanley Wolpert, emeritus professor at UCLA, says that:
Even when ill and dying, Aurangzeb made sure that the populace knew he was still alive, for if they had thought otherwise then the turmoil of another war of succession was likely. He died in Ahmednagar on 20 February 1707 at the age of 88, having outlived many of his children. His modest open-air grave in Khuldabad expresses his deep devotion to his Islamic beliefs. It is sited in the courtyard of the shrine of the Sufi saint Shaikh Burhan-u'd-din Gharib, who was a disciple of Nizamuddin Auliya of Delhi.
Brown writes that after his death, "a string of weak emperors, wars of succession, and coups by noblemen heralded the irrevocable weakening of Mughal power". She notes that the populist but "fairly old-fashioned" explanation for the decline is that there was a reaction to Aurangzeb's oppression. Aurangzeb's son, Bahadur Shah I, succeeded him and the empire, both because of Aurangzeb's over-extension and because of Bahadur Shah's weak military and leadership qualities, entered a period of terminal decline. Immediately after Bahadur Shah occupied the throne, the Maratha Empire – which Aurangzeb had held at bay, inflicting high human and monetary costs even on his own empire – consolidated and launched effective invasions of Mughal territory, seizing power from the weak emperor. Within decades of Aurangzeb's death, the Mughal Emperor had little power beyond the walls of Delhi.
Full title.
His full imperial title was: Al-Sultan al-Azam wal Khaqan al-Mukarram Hazrat Abul Muzaffar Muhy-ud-Din Muhammad Aurangzeb Bahadur Alamgir I, Badshah Ghazi, Shahanshah-e-Sultanat-ul-Hindiya Wal Mughaliya.
References.
Notes
Citations

</doc>
<doc id="2427" url="https://en.wikipedia.org/wiki?curid=2427" title="Alexandrine">
Alexandrine

An alexandrine () is a line of poetic meter comprising 12 syllables. Alexandrines are common in the German literature of the Baroque period and in French poetry of the early modern and modern periods. Drama in English often used alexandrines before Marlowe and Shakespeare, by whom it was supplanted by iambic pentameter (5-foot verse). In non-Anglo-Saxon or French contexts, the term dodecasyllable is often used.
Origin.
There is some doubt as to the origin of the name; but most probably it is derived from a collection of Alexandrine romances, collected in the 12th century, of which Alexander the Great was the hero, and in which he was represented, somewhat like the British Arthur, as the pride and crown of chivalry. Before the publication of this work most of the trouvère romances appeared in octosyllabic verse. There is also a theory that the form was invented by the 12th-century poet Alexander of Paris. The new work, which was henceforth to set the fashion to French literature, was written in lines of twelve syllables, but with a freedom of pause which was afterwards greatly curtailed. The new fashion, however, was not adopted all at once. The metre fell into disuse until the reign of Francis I, when it was revived by Jean-Antoine de Baïf, one of the seven poets known as La Pléiade.
Syllabic verse.
In syllabic verse, such as that used in French literature, an alexandrine is a line of twelve syllables. Most commonly, the line is divided into two equal parts by a caesura between the sixth and seventh syllables. Alternatively, the line is divided into three four-syllable sections by two caesuras.
The dramatic works of Pierre Corneille and Jean Racine are typically composed of rhyming alexandrine couplets. The caesura after the sixth syllable is here marked ||. Note that in these examples, as in the vast majority of pre-20th-century French poetry, the pronunciation of the "e muet" follows rigid, indeed formal, rules: normally it is pronounced if followed by a consonant sound. Thus "partîm-euh cinq cents", "esclav-euh des Mores" and, still in the 20th-century verse of the Eluard extract, "perl-euh z-en placard".
<poem>Nous partîmes cinq cents ; || mais par un prompt renfort
Nous nous vîmes trois mille || en arrivant au port
Baudelaire's "Les Bijoux" (The Jewels) is a typical example of the use of the alexandrine in 19th-century French poetry:
<poem>La très-chère était nue, || et, connaissant mon cœur,
Elle n'avait gardé || que ses bijoux sonores,
Dont le riche attirail || lui donnait l'air vainqueur
Qu'ont dans leurs jours heureux || les esclaves des Mores.</poem>
Even a 20th-century Surrealist, such as Paul Éluard, used alexandrines on occasion, such as in these lines from "L'Égalité des sexes" (in "Capitale de la douleur") (note the variation between caesuras after the sixth syllable, and after fourth and eighth):
<poem>Ni connu la beauté || des yeux, beauté des pierres,
Celle des gouttes d'eau, || des perles en placard,
Des pierres nues || et sans squelette, || ô ma statue</poem>
Accentual-syllabic verse.
In accentual-syllabic verse, it is a line of Iambic hexameter—a line of six feet or measures ("iambs"), each of which has two syllables with an unstressed syllable followed by a stressed syllable. It is also usual for there to be a caesura between the sixth and seventh syllables (as the examples from Pope below illustrate). Robert Bridges noted that in the lyrical sections of "Samson Agonistes", Milton significantly varied the placement of the caesura.
In Edmund Spenser's "Faerie Queene" eight lines of pentameter are followed by an alexandrine, the eponymous Spenserian stanza. The six-foot line slowed the regular rhythm of the five-foot lines. After Spenser, alexandrine couplets were used by Michael Drayton in his "Poly-Olbion".
Alexander Pope famously characterized the alexandrine's potential to slow or speed the flow of a poem in two rhyming couplets consisting of an iambic pentameter followed by an alexandrine:
<poem>A needless alexandrine ends the song
That like a wounded snake, drags its slow length along.</poem>
A few lines later Pope continues:
<poem>Not so, when swift Camilla scours the Plain,
Flies o'er th'unbending corn and skims along the Main.</poem>
As in the Spenserian stanza above, alexandrines are sometimes mixed with pentameter verse. Shakespeare used them rarely in his blank verse. In the Restoration and eighteenth century, poetry written in couplets is sometimes varied by the introduction of a triplet in which the third line is an alexandrine, as in this sample from Dryden, which introduces a 5-5-6 triplet after two pentameter couplets:
<poem>But satire needs not those, and wit will shine
Through the harsh cadence of a rugged line
A noble error, and but seldom made,
When poets are by too much force betrayed.
Thy generous fruits, though gathered ere their prime,
Still showed a quickness; and maturing time
But mellows what we write to the dull sweets of rhyme.</poem>
Alexandrines also formed the first line of the couplet form Poulter's Measure (the second line being a fourteener) as exemplified in Henry Howard, Earl of Surrey's poem, "Complaint of the Absence of her lover, being upon the sea" (1547).
Modern references.
In the comic book "Asterix and Cleopatra", the author Goscinny inserted a pun about alexandrines: when the Druid Panoramix ("Getafix" in the English translation) meets his Alexandrian (Egyptian) friend the latter exclaims "Je suis, mon cher ami, || très heureux de te voir" at which Panoramix observes "C'est un Alexandrin" ("That's an alexandrine!"/"He's an Alexandrian!"). The pun can also be heard in the theatrical adaptations. The English translation renders this as "My dear old Getafix || How good to see you here", with the reply "Aha, an Alexandrine".

</doc>
<doc id="2428" url="https://en.wikipedia.org/wiki?curid=2428" title="Analog computer">
Analog computer

An analog computer is a form of computer that uses the continuously changeable aspects of physical phenomena such as electrical, mechanical, or hydraulic quantities to model the problem being solved. In contrast, digital computers represent varying quantities symbolically, as their numerical values change. As an analog computer does not use discrete values, but rather continuous values, processes cannot be reliably repeated with exact equivalence, as they can with Turing machines. Analog computers do not suffer from the quantization noise inherent in digital computers, but are limited instead by analog noise.
Analog computers were widely used in scientific and industrial applications where digital computers of the time lacked sufficient performance. Analog computers can have a very wide range of complexity. Slide rules and nomographs are the simplest, while naval gunfire control computers and large hybrid digital/analog computers were among the most complicated. Systems for process control and protective relays used analog computation to perform control and protective functions.
The advent of digital computing and its success made analog computers largely obsolete in 1950s and 1960s, though they remain in use in some specific applications, like the flight computer in aircraft, and for teaching control systems in universities.
Setup.
Setting up an analog computer required scale factors to be chosen, along with initial conditions—that is, starting values. Another essential was creating the required network of interconnections between computing elements. Sometimes it was necessary to re-think the structure of the problem so that the computer would function satisfactorily. No variables could be allowed to exceed the computer's limits, and differentiation was to be avoided, typically by rearranging the "network" of interconnects, using integrators in a different sense.
Running an electronic analog computer, assuming a satisfactory setup, started with the computer held with some variables fixed at their initial values. Moving a switch released the holds and permitted the problem to run. In some instances, the computer could, after a certain running time interval, repeatedly return to the initial-conditions state to reset the problem, and run it again.
Timeline of analog computers.
Precursors.
This is a list of examples of early computation devices which are considered to be precursors of the modern computers. Some of them may even have been dubbed as 'computers' by the press, although they may fail to fit the modern definitions.
The Antikythera mechanism is believed to be the earliest known mechanical analog "computer", according to Derek J. de Solla Price. It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to "circa" 100 BC. Devices of a level of complexity comparable to that of the Antikythera mechanism would not reappear until a thousand years later.
Many mechanical aids to calculation and measurement were constructed for astronomical and navigation use. 
The planisphere was a star chart invented by Abū Rayḥān al-Bīrūnī in the early 11th century. The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BC and is often attributed to Hipparchus. A combination of the planisphere and dioptra, the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy. An astrolabe incorporating a mechanical calendar computer and gear-wheels was invented by Abi Bakr of Isfahan, Persia in 1235. Abū Rayhān al-Bīrūnī invented the first mechanical geared lunisolar calendar astrolabe, an early fixed-wired knowledge processing machine with a gear train and gear-wheels, "circa" 1000 AD.
The sector, a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, was developed in the late 16th century and found application in gunnery, surveying and navigation.
The planimeter was a manual instrument to calculate the area of a closed figure by tracing over it with a mechanical linkage.
The slide rule was invented around 1620–1630, shortly after the publication of the concept of the logarithm. It is a hand-operated analog computer for doing multiplication and division. As slide rule development progressed, added scales provided reciprocals, squares and square roots, cubes and cube roots, as well as transcendental functions such as logarithms and exponentials, circular and hyperbolic trigonometry and other functions. Aviation is one of the few fields where slide rules are still in widespread use, particularly for solving time–distance problems in light aircraft.
The tide-predicting machine invented by Sir William Thomson in 1872 was of great utility to navigation in shallow waters. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location.
The differential analyser, a mechanical analog computer designed to solve differential equations by integration, used wheel-and-disc mechanisms to perform the integration. In 1876 Lord Kelvin had already discussed the possible construction of such calculators, but he had been stymied by the limited output torque of the ball-and-disk integrators. In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output. The torque amplifier was the advance that allowed these machines to work. Starting in the 1920s, Vannevar Bush and others developed mechanical differential analyzers.
Modern era.
The Dumaresq was a mechanical calculating device invented around 1902 by Lieutenant John Dumaresq of the Royal Navy. It was an analog computer which related vital variables of the fire control problem to the movement of one's own ship and that of a target ship. It was often used with other devices, such as a Vickers range clock to generate range and deflection data so the gun sights of the ship could be continuously set. A number of versions of the Dumaresq were produced of increasing complexity as development proceeded.
By 1912 Arthur Pollen had developed an electrically driven mechanical analog computer for fire-control systems, based on the differential analyser. It was used by the Imperial Russian Navy in World War I.
Starting in 1929, AC network analyzers were constructed to solve calculation problems related to electrical power systems that were too large to solve with numerical methods at the time. These were essentially scale models of the electrical properties of the full-size system. Since network analyzers could handle problems too large for analytic methods or hand computation, they were also used to solve problems in nuclear physics and in the design of structures. More than 50 large network analyzers were built by the end of the 1950s.
World War II era gun directors, gun data computers, and bomb sights used mechanical analog computers. Mechanical analog computers were very important in gun fire control in World War II, The Korean War and well past the Vietnam War; they were made in significant numbers.
The FERMIAC was an analog computer invented by physicist Enrico Fermi in 1947 to aid in his studies of neutron transport. Project Cyclone was an analog computer developed by Reeves in 1950 for the analysis and design of dynamic systems. Project Typhoon was an analog computer developed by RCA in 1952. It consisted of over 4000 electron tubes and used 100 dials and 6000 plug-in connectors to program. The MONIAC Computer was a hydraulic model of a national economy first unveiled in 1949.
Computer Engineering Associates was spun out of Caltech in 1950 to provide commercial services using the "Direct Analogy Electric Analog Computer" ("the largest and most impressive general-purpose analyzer facility for the solution of field problems") developed there by Gilbert D. McCann, Charles H. Wilts, and Bart Locanthi.
Educational analog computers illustrated the principles of analog calculation. The Heathkit EC-1, a $199 educational analog computer, was made by the Heath Company, USA c. 1960. It was programmed using patch cords that connected nine operational amplifiers and other components. General Electric also marketed an "educational" analog computer kit of a simple design in the early 1960s consisting of a two transistor tone generator and three potentiometers wired such that the frequency of the oscillator was nulled when the potentiometer dials were positioned by hand to satisfy an equation. The relative resistance of the potentiometer was then equivalent to the formula of the equation being solved. Multiplication or division could be performed depending on which dials were considered inputs and which was the output. Accuracy and resolution was limited and a simple slide rule was more accurate; however, the unit did demonstrate the basic principle.
In industrial process control, thousands of analog loop controllers were used to automatically regulate temperature, flow, pressure, or other process conditions. The technology of these controllers ranged from purely mechanical integrators, through vacuum-tube and solid-state devices, to emulation of analog controllers by microprocessors.
Electronic analog computers.
The similarity between linear mechanical components, such as springs and dashpots (viscous-fluid dampers), and electrical components, such as capacitors, inductors, and resistors is striking in terms of mathematics. They can be modeled using equations of the same form.
However, the difference between these systems is what makes analog computing useful. If one considers a simple mass–spring system, constructing the physical system would require making or modifying the springs and masses. This would be followed by attaching them to each other and an appropriate anchor, collecting test equipment with the appropriate input range, and finally, taking measurements. In more complicated cases, such as suspensions for racing cars, experimental construction, modification, and testing is both complicated and expensive.
The electrical equivalent can be constructed with a few operational amplifiers (op amps) and some passive linear components; all measurements can be taken directly with an oscilloscope. In the circuit, the (simulated) 'stiffness of the spring', for instance, can be changed by adjusting the parameters of a capacitor. The electrical system is an analogy to the physical system, hence the name, but it is less expensive to construct, generally safer, and typically much easier to modify.
As well, an electronic circuit can typically operate at higher frequencies than the system being simulated. This allows the simulation to run faster than real time (which could, in some instances, be hours, weeks, or longer). Experienced users of electronic analog computers said that they offered a comparatively intimate control and understanding of the problem, relative to digital simulations.
The drawback of the mechanical-electrical analogy is that electronics are limited by the range over which the variables may vary. This is called dynamic range. They are also limited by noise levels. Floating-point digital calculations have a comparatively huge dynamic range.
These electric circuits can also easily perform a wide variety of simulations. For example, voltage can simulate water pressure and electric current can simulate rate of flow in terms of cubic metres per second. An integrator can provide the total accumulated volume of liquid, using an input current proportional to the (possibly varying) flow rate.
Analog computers are especially well-suited to representing situations described by differential equations. Occasionally, they were used when a differential equation proved very difficult to solve by traditional means.
The accuracy of an analog computer is limited by its computing elements as well as quality of the internal power and electrical interconnections. The precision of the analog computer readout was limited chiefly by the precision of the readout equipment used, generally three or four significant figures. The precision of a digital computer is limited by the word size; arbitrary-precision arithmetic, while relatively slow, provides any practical degree of precision that might be needed.
Many small computers dedicated to specific computations are still part of industrial regulation equipment, but from the 1950s to the 1970s, general-purpose analog computers were the only systems fast enough for real time simulation of dynamic systems, especially in the aircraft, military and aerospace field.
In the 1960s, the major manufacturer was Electronic Associates of Princeton, New Jersey, with its 231R Analog Computer (vacuum tubes, 20 integrators) and subsequently its 8800 Analog Computer (solid state operational amplifiers, 64 integrators). Its challenger was Applied Dynamics of Ann Arbor, Michigan.
Although the basic technology for analog computers is usually operational amplifiers (also called "continuous current amplifiers" because they have no low frequency limitation), in the 1960s an attempt was made in the French ANALAC computer to use an alternative technology: medium frequency carrier and non dissipative reversible circuits.
In the 1970s every big company and administration concerned with problems in dynamics had a big analog computing center, for example: 
Analog–digital hybrids.
Analog computing devices are fast, digital computing devices are more versatile and accurate, so the idea is to combine the two processes for the best efficiency. An example of such hybrid elementary device is the hybrid multiplier where one input is an analog signal, the other input is a digital signal and the output is analog. It acts as an analog potentiometer upgradable digitally. This kind of hybrid technique is mainly used for fast dedicated real time computation when computing time is very critical as signal processing for radars and generally for controllers in embedded systems.
In the early 1970s analog computer manufacturers tried to tie together their analog computer with a digital computer to get the advantages of the two techniques. In such systems, the digital computer controlled the analog computer, providing initial set-up, initiating multiple analog runs, and automatically feeding and collecting data. The digital computer may also participate to the calculation itself using analog-to-digital and digital-to-analog converters.
The largest manufacturer of hybrid computers was Electronics Associates. Their hybrid computer model 8900 was made of a digital computer and one or more analog consoles. These systems were mainly dedicated to large projects such as the Apollo program and Space Shuttle at NASA, or Ariane in Europe, especially during the integration step where at the beginning everything is simulated, and progressively real components replace their simulated part. 
Only one company was known as offering general commercial computing services on its hybrid computers, CISI of France, in the 1970s.
The best reference in this field is the 100 000 simulations runs for each certification of the automatic landing systems of Airbus and Concorde aircraft. 
After 1980, purely digital computers progressed more and more rapidly and were fast enough to compete with analog computers.
One key to the speed of analog computers was their fully parallel computation, but this was also a limitation. The more equations required for a problem, the more analog components were needed, even when the problem wasn't time critical. "Programming" a problem meant interconnecting the analog operators; even with a removable wiring panel this was not very versatile. Today there are no more big hybrid computers, but only hybrid components.
Implementations.
Mechanical analog computers.
While a wide variety of mechanisms have been developed throughout history, some stand out because of their theoretical importance, or because they were manufactured in significant quantities.
Most practical mechanical analog computers of any significant complexity used rotating shafts to carry variables from one mechanism to another. Cables and pulleys were used in a Fourier synthesizer, a tide-predicting machine, which summed the individual harmonic components. Another category, not nearly as well known, used rotating shafts only for input and output, with precision racks and pinions. The racks were connected to linkages that performed the computation. At least one US Naval sonar fire control computer of the later 1950s, made by Librascope, was of this type, as was the principal computer in the Mk. 56 Gun Fire Control System.
Online, there is a remarkably clear illustrated reference (OP 1140) that describes the fire control computer mechanisms. 
For adding and subtracting, precision miter-gear differentials were in common use in some computers; the Ford Instrument Mark I Fire Control Computer contained about 160 of them.
Integration with respect to another variable was done by a rotating disc driven by one variable. Output came from a pickoff device (such as a wheel) positioned at a radius on the disc proportional to the second variable. (A carrier with a pair of steel balls supported by small rollers worked especially well. A roller, its axis parallel to the disc's surface, provided the output. It was held against the pair of balls by a spring.)
Arbitrary functions of one variable were provided by cams, with gearing to convert follower movement to shaft rotation.
Functions of two variables were provided by three-dimensional cams. In one good design, one of the variables rotated the cam. A hemispherical follower moved its carrier on a pivot axis parallel to that of the cam's rotating axis. Pivoting motion was the output. The second variable moved the follower along the axis of the cam. One practical application was ballistics in gunnery.
Coordinate conversion from polar to rectangular was done by a mechanical resolver (called a "component solver" in US Navy fire control computers). Two discs on a common axis positioned a sliding block with pin (stubby shaft) on it. One disc was a face cam, and a follower on the block in the face cam's groove set the radius. The other disc, closer to the pin, contained a straight slot in which the block moved. The input angle rotated the latter disc (the face cam disc, for an unchanging radius, rotated with the other (angle) disc; a differential and a few gears did this correction).
Referring to the mechanism's frame, the location of the pin corresponded to the tip of the vector represented by the angle and magnitude inputs. Mounted on that pin was a square block.
Rectilinear-coordinate outputs (both sine and cosine, typically) came from two slotted plates, each slot fitting on the block just mentioned. The plates moved in straight lines, the movement of one plate at right angles to that of the other. The slots were at right angles to the direction of movement. Each plate, by itself, was like a Scotch yoke, known to steam engine enthusiasts.
During World War II, a similar mechanism converted rectilinear to polar coordinates, but it was not particularly successful and was eliminated in a significant redesign (USN, Mk. 1 to Mk. 1A).
Multiplication was done by mechanisms based on the geometry of similar right triangles. Using the trigonometric terms for a right triangle, specifically opposite, adjacent, and hypotenuse, the adjacent side was fixed by construction. One variable changed the magnitude of the opposite side. In many cases, this variable changed sign; the hypotenuse could coincide with the adjacent side (a zero input), or move beyond the adjacent side, representing a sign change.
Typically, a pinion-operated rack moving parallel to the (trig.-defined) opposite side would position a slide with a slot coincident with the hypotenuse. A pivot on the rack let the slide's angle change freely. At the other end of the slide (the angle, in trig, terms), a block on a pin fixed to the frame defined the vertex between the hypotenuse and the adjacent side.
At any distance along the adjacent side, a line perpendicular to it intersects the hypotenuse at a particular point. The distance between that point and the adjacent side is some fraction that is the product of "1" the distance from the vertex, and "2" the magnitude of the opposite side.
The second input variable in this type of multiplier positions a slotted plate perpendicular to the adjacent side. That slot contains a block, and that block's position in its slot is determined by another block right next to it. The latter slides along the hypotenuse, so the two blocks are positioned at a distance from the (trig.) adjacent side by an amount proportional to the product.
To provide the product as an output, a third element, another slotted plate, also moves parallel to the (trig.) opposite side of the theoretical triangle. As usual, the slot is perpendicular to the direction of movement. A block in its slot, pivoted to the hypotenuse block positions it.
A special type of integrator, used at a point where only moderate accuracy was needed, was based on a steel ball, instead of a disc. It had two inputs, one to rotate the ball, and the other to define the angle of the ball's rotating axis. That axis was always in a plane that contained the axes of two movement-pickoff rollers, quite similar to the mechanism of a rolling-ball computer mouse (in this mechanism, the pickoff rollers were roughly the same diameter as the ball). The pickoff roller axes were at right angles.
A pair of rollers "above" and "below" the pickoff plane were mounted in rotating holders that were geared together. That gearing was driven by the angle input, and established the rotating axis of the ball. The other input rotated the "bottom" roller to make the ball rotate.
Essentially, the whole mechanism, called a component integrator, was a variable-speed drive with one motion input and two outputs, as well as an angle input. The angle input varied the ratio (and direction) of coupling between the "motion" input and the outputs according to the sine and cosine of the input angle.
Although they did not accomplish any computation, electromechanical position servos were essential in mechanical analog computers of the "rotating-shaft" type for providing operating torque to the inputs of subsequent computing mechanisms, as well as driving output data-transmission devices such as large torque-transmitter synchros in naval computers.
Other non-computational mechanisms included internal odometer-style counters with interpolating drum dials for indicating internal variables, and mechanical multi-turn limit stops.
Considering that accurately controlled rotational speed in analog fire-control computers was a basic element of their accuracy, there was a motor with its average speed controlled by a balance wheel, hairspring, jeweled-bearing differential, a twin-lobe cam, and spring-loaded contacts (ship's AC power frequency was not necessarily accurate, nor dependable enough, when these computers were designed).
Electronic analog computers.
Electronic analog computers typically have front panels with numerous jacks (single-contact sockets) that permit patch cords (flexible wires with plugs at both ends) to create the interconnections which define the problem setup. In addition, there are precision high-resolution potentiometers (variable resistors) for setting up (and, when needed, varying) scale factors. In addition, there is likely to be a zero-center analog pointer-type meter for modest-accuracy voltage measurement. Stable, accurate voltage sources provide known magnitudes.
Typical electronic analog computers contain anywhere from a few to a hundred or more operational amplifiers ("op amps"), named because they perform mathematical operations. Op amps are a particular type of feedback amplifier with very high gain and stable input (low and stable offset). They are always used with precision feedback components that, in operation, all but cancel out the currents arriving from input components. The majority of op amps in a representative setup are summing amplifiers, which add and subtract analog voltages, providing the result at their output jacks. As well, op amps with capacitor feedback are usually included in a setup; they integrate the sum of their inputs with respect to time.
Integrating with respect to another variable is the nearly exclusive province of mechanical analog integrators; it is almost never done in electronic analog computers. However, given that a problem solution does not change with time, time can serve as one of the variables.
Other computing elements include analog multipliers, nonlinear function generators, and analog comparators.
Electrical elements such as inductors and capacitors used in electrical analog computers had to be carefully manufactured to reduce non-ideal effects. For example, in the construction of AC power network analyzers, one motive for using higher frequencies for the calculator (instead of the actual power frequency) was that higher-quality inductors could be more easily made. Many general-purpose analog computers avoided the use of inductors entirely, re-casting the problem in a form that could be solved using only resistive and capacitive elements, since high-quality capacitors are relatively easy to make.
The use of electrical properties in analog computers means that calculations are normally performed in real time (or faster), at a speed determined mostly by the frequency response of the operational amplifiers and other computing elements. In the history of electronic analog computers, there were some special high-speed types.
Nonlinear functions and calculations can be constructed to a limited precision (three or four digits) by designing function generators — special circuits of various combinations of resistors and diodes to provide the nonlinearity. Typically, as the input voltage increases, progressively more diodes conduct.
When compensated for temperature, the forward voltage drop of a transistor's base-emitter junction can provide a usably accurate logarithmic or exponential function. Op amps scale the output voltage so that it is usable with the rest of the computer.
Any physical process which models some computation can be interpreted as an analog computer. Some examples, invented for the purpose of illustrating the concept of analog computation, include using a bundle of spaghetti as a model of sorting numbers; a board, a set of nails, and a rubber band as a model of finding the convex hull of a set of points; and strings tied together as a model of finding the shortest path in a network. These are all described in Dewdney (1984).
Components.
Analog computers often have a complicated framework, but they have, at their core, a set of key components which perform the calculations, which the operator manipulates through the computer's framework.
Key hydraulic components might include pipes, valves and containers.
Key mechanical components might include rotating shafts for carrying data within the computer, miter gear differentials, disc/ball/roller integrators, cams (2-D and 3-D), mechanical resolvers and multipliers, and torque servos.
Key electrical/electronic components might include:
The core mathematical operations used in an electric analog computer are:
In some analog computer designs, multiplication is much preferred to division. Division is carried out with a multiplier in the feedback path of an Operational Amplifier.
Differentiation with respect to time is not frequently used, and in practice is avoided by redefining the problem when possible. It corresponds in the frequency domain to a high-pass filter, which means that high-frequency noise is amplified; differentiation also risks instability.
Limitations.
In general, analog computers are limited by non-ideal effects. An analog signal is composed of four basic components: DC and AC magnitudes, frequency, and phase. The real limits of range on these characteristics limit analog computers. Some of these limits include the operational amplifier offset, finite gain, and frequency response, noise floor, non-linearities, temperature coefficient, and parasitic effects within semiconductor devices. For commercially available electronic components, ranges of these aspects of input and output signals are always figures of merit.
Decline.
In 1950s to 1970s, digital computers based on first vacuum tubes, transistors, integrated circuits and then micro-processors became more economical and precise. This led digital computers to largely replace analog computers. Even so, some research in analog computation is still being done. A few universities still use analog computers to teach control system theory. The American company Comdyna manufactures small analog computers. At Indiana University Bloomington, Jonathan Mills has developed the Extended Analog Computer based on sampling voltages in a foam sheet. At the Harvard Robotics Laboratory, analog computation is a research topic. Lyric Semiconducto's error correction circuits use analog probabilistic signals. Slide rules are still popular among aircraft personnel. 
Resurgence in VLSI technology.
With the development of very-large-scale integration (VLSI) technology, Yannis Tsividis' group at Columbia University has been revisiting analog/hybrid computers design in standard CMOS process. Two VLSI chips have been developed, an 80th-order analog computer (250 nm) by Glenn Cowan in 2005 and an 4th-order hybrid computer (65 nm) developed by Ning Guo in 2015, both targeting at energy-efficient ODE/PDEs applications. Glenn's chip contains 16 macros, in which there are 25 analog computing blocks, namely integrators, multipliers, fanouts, few nonlinear blocks. Ning's chip contains one macro block, in which there are 26 computing blocks including integrators, multipliers, fanouts, ADCs, SRAMs and DACs. Arbitrary nonlinear function generation is made possible by the ADC+SRAM+DAC chain, where the SRAM block stores the nonlinear function data. The experiments from the related publications revealed that VLSI analog/hybrid computers demonstrated about 1~2 orders magnitude of advantage in both solution time and energy while achieving accuracy within 5%, which points to the promise of using analog/hybrid computing techniques in the area of energy-efficient approximate computing.
Practical examples.
These are examples of analog computers that have been constructed or practically used:
Analog (audio) synthesizers can also be viewed as a form of analog computer, and their technology was originally based in part on electronic analog computer technology. The ARP 2600's Ring Modulator was actually a moderate-accuracy analog multiplier.
The Simulation Council (or Simulations Council) was an association of analog computer users in USA. It is now known as The Society for Modeling and Simulation International. The Simulation Council newsletters from 1952 to 1963 are available online and show the concerns and technologies at the time, and the common use of analog computers for missilry.
Real computers.
Computer theorists often refer to idealized analog computers as real computers (because they operate on the set of real numbers). Digital computers, by contrast, must first quantize the signal into a finite number of values, and so can only work with the rational number set (or, with an approximation of irrational numbers).
These idealized analog computers may "in theory" solve problems that are intractable on digital computers; however as mentioned, in reality, analog computers are far from attaining this ideal, largely because of noise minimization problems. "In theory", ambient noise is limited by quantum noise (caused by the quantum movements of ions). Ambient noise may be severely reduced – but never to zero – by using cryogenically cooled parametric amplifiers. Moreover, given unlimited time and memory, the (ideal) digital computer may also solve real number problems.

</doc>
<doc id="2429" url="https://en.wikipedia.org/wiki?curid=2429" title="Audio">
Audio

Audio may refer to:

</doc>
<doc id="2431" url="https://en.wikipedia.org/wiki?curid=2431" title="Minute and second of arc">
Minute and second of arc

A minute of arc (MOA), arcminute (arcmin) or minute arc is a unit of angular measurement equal to one-sixtieth () of one degree. As one degree is of a circle, one minute of arc is of a circle (or, in radians, ). It is used in fields that involve very small angles, such as astronomy, optometry, ophthalmology, optics, navigation, land surveying and marksmanship.
The number of square arcminutes in a complete sphere is formula_1 approximately 148,510,660 square arcminutes.
A second of arc (arcsecond, arcsec) is of an arcminute, of a degree, of a circle, and (about ) of a radian. This is approximately the angle subtended by a U.S. dime coin (18mm) at a distance of . An arcsecond is also the angle subtended by
To express even smaller angles, standard SI prefixes can be employed; the milliarcsecond (mas), for instance, is commonly used in astronomy.
Symbols and abbreviations.
The standard symbol for marking the arcminute is the prime (′) (U+2032), though a single quote (') (U+0027) is commonly used where only ASCII characters are permitted. One arcminute is thus written 1′. It is also abbreviated as arcmin or amin or, less commonly, the prime with a circumflex over it (formula_2).
The standard symbol for the arcsecond is the double prime (″) (U+2033), though a double quote (") (U+0022) is commonly used where only ASCII characters are permitted. One arcsecond is thus written 1″. It is also abbreviated as arcsec or asec.
In celestial navigation, seconds of arc are rarely used in calculations, the preference usually being for degrees, minutes and decimals of a minute, written for example as 42° 25.32′ or 42° 25.322′. This notation has been carried over into marine GPS receivers, which normally display latitude and longitude in the latter format by default.
Uses.
Firearms.
The arcminute is commonly found in the firearms industry and literature, particularly concerning the accuracy of rifles, though the industry refers to it as minute of angle (MOA). It is especially popular with shooters familiar with the Imperial measurement system because 1 MOA subtends 1.047 inches at 100 yards, a traditional distance on target ranges. This calculation applies to distances beyond 100 yards, for example, 500 yards = 5.235 inches, and 1000 yards = 10.47 inches. Since most modern rifle scopes are adjustable in half (), quarter (), or eighth () MOA increments, also known as "clicks", this makes zeroing and adjustments much easier. For example, if the point of impact is 3 inches high and 1.5 inches left of the point of aim at 100 yards, the scope needs to be adjusted 3 MOA down, and 1.5 MOA right. Such adjustments are trivial when the scope's adjustment dials have a MOA scale printed on them, and even figuring the right number of clicks is relatively easy on scopes that "click" in fractions of MOA. 
One thing to be aware of is that some scopes, including some higher-end models, are calibrated such that an adjustment of 1 MOA corresponds to exactly 1 inch, rather than 1.047". This is commonly known as the Shooter's MOA (SMOA) or Inches Per Hundred Yards (IPHY). While the difference between one true MOA and one SMOA is less than half of an inch even at 1000 yards, this error compounds significantly on longer range shots that may require adjustment upwards of 20-30 MOA to compensate for the bullet drop. If a shot requires an adjustment of 20 MOA or more, the difference between true MOA and SMOA will add up to 1 inch or more. In competitive target shooting, this might mean the difference between a hit and a miss.
The physical group size equivalent to "m" minutes of arc can be calculated as follows: group size = tan() × distance. In the example previously given, for 1 minute of arc, and substituting 3,600 inches for 100 yards, 3,600 tan() ≈ 1.047 inches. In metric units 1 MOA at 100 meters ≈ 2.908 centimeters.
Sometimes, a precision firearm's accuracy will be measured in MOA. This simply means that under ideal conditions i.e. no wind, match-grade ammo, clean barrel, and a vise or a benchrest used to eliminate shooter error, the gun is capable of producing a group of shots whose center points (center-to-center) fit into a circle, the average diameter of circles in several groups can be subtended by that amount of arc. For example, a "1 MOA rifle" should be capable, under ideal conditions, of shooting an average 1-inch groups at 100 yards. Most higher-end rifles are warrantied by their manufacturer to shoot under a given MOA threshold (typically 1 MOA or better) with specific ammunition and no error on the shooter's part. For example, Remington's M24 Sniper Weapon System is required to shoot 0.8 MOA or better, or be rejected.
Rifle manufacturers and gun magazines often refer to this capability as "sub-MOA", meaning it shoots under 1 MOA. This means that a single group of 3 to 5 shots at 100 yards, or the average of several groups, will measure less than 1 MOA between the two furthest shots in the group, i.e. all shots fall within 1 MOA. If larger samples are taken (i.e., more shots per group) then group size typically increases, however this will ultimately average out. If a rifle was truly a 1 MOA rifle, it would be just as likely that two consecutive shots land exactly on top of each other as that they land 1 MOA apart. For 5 shot groups, based on 95% confidence a rifle that normally shoots 1 MOA can be expected to shoot groups between 0.58 MOA and 1.47 MOA, although the majority of these groups will be under 1 MOA. What this means in practice is if a rifle that shoots 1-inch groups on average at 100 yards shoots a group measuring 0.7 inches followed by a group that is 1.3 inches this is not statistically abnormal.
The Metric System counterpart of the MOA is the MilRad, being equal to one 1000th of the target range, laid out on a circle that has the observer as centre and the target range as radius. The number of MilRads on a full such circle therefore always is equal to 2 × × 1000, regardless the target range. Therefore, 1 MOA = 0.2908 MilRad. This means that an object which spans 1 MilRad on the reticle is at a range that is in meters equal to the object's size in millimeters (e.g. an object of 100 mm @ 1 Milrad is 100 meters away). So there is no conversion factor required, contrary to the MOA system. The markings on a reticle that mark MilRads are called MilDots. Such reticle is called a "MilDot Reticle".
Cartography.
Minutes of arc (and its subunit, seconds of arc or SOA—equal to a sixtieth of a MOA) are also used in cartography and navigation. At sea level one minute of arc along the equator or a meridian (indeed, any great circle) equals approximately one Nautical mile (). A second of arc, one sixtieth of this amount, is about 30 meters or roughly 100 feet. The exact distance varies along meridian arcs because the figure of the Earth is slightly oblate.
Positions are traditionally given using degrees, minutes, and seconds of arcs for latitude, the arc north or south of the equator, and for longitude, the arc east or west of the Prime Meridian. Any position on or above the Earth's reference ellipsoid can be precisely given with this method. However, because of the somewhat clumsy base-60 nature of minutes and seconds, positions are frequently expressed in fractional degrees only, expressed in decimal form to an equal amount of precision. Degrees given to three decimal places ( of a degree) have about the precision of degrees-minutes-seconds ( of a degree) and specify locations within about 120 meters or 400 feet.
Property cadastral surveying.
Related to cartography, property boundary surveying using the metes and bounds system relies on fractions of a degree to describe property lines' angles in reference to cardinal directions. A boundary "mete" is described with a beginning reference point, the cardinal direction North or South followed by an angle less than 90 degrees and a second cardinal direction, and a linear distance. The boundary runs the specified linear distance from the beginning point, the direction of the distance being determined by rotating the first cardinal direction the specified angle toward the second cardinal direction. For example, "North 65° 39′ 18″ West 85.69 feet" would describe a line running from the starting point 85.69 feet in a direction 65° 39′ 18″ (or 65.655°) away from north toward the west.
Astronomy.
The arcminute and arcsecond are also used in astronomy. Degrees (and therefore arcminutes) are used to measure declination, or angular distance north or south of the celestial equator. The arcsecond is also often used to describe parallax, due to very small parallax angles for stellar parallax, and tiny angular diameters (e.g. Venus varies between 10″ and 60″). The parallax, proper motion and angular diameter of a star may also be written in milliarcseconds (mas), or thousandths of an arcsecond. The parsec gets its name from "parallax second", for those arcseconds.
The ESA astrometric space probe Gaia is hoped to measure star positions to 20 microarcseconds (µas) when it begins producing catalog positions sometime after 2016. There are about 1.3 trillion µas in a circle. As seen from Earth, one µas is about the size of a period at the end of a sentence in the Apollo mission manuals left on the moon. Currently the best catalog positions of stars actually measured are in terms of milliarcseconds, by the U.S. Naval Observatory. A milliarcsecond is about the size of a dime atop the Eiffel Tower as seen from New York City.
Apart from the Sun, the star with the largest angular diameter from Earth is R Doradus, a red supergiant with a diameter of 0.05 arcsecond. Because of the effects of atmospheric seeing, ground-based telescopes will smear the image of a star to an angular diameter of about 0.5 arcsecond; in poor seeing conditions this increases to 1.5 arcseconds or even more. The dwarf planet Pluto has proven difficult to resolve because its angular diameter is about 0.1 arcsecond. This is roughly equivalent to a (40 mm) ping-pong ball viewed at a distance of 50 miles (80 km).
Space telescopes are not affected by the Earth's atmosphere but are diffraction limited. For example, the Hubble space telescope can reach an angular size of stars down to about 0.1″. Techniques exist for improving seeing on the ground. Adaptive optics, for example, can produce images around 0.05 arcsecond on a 10 m class telescope.
Human vision.
In humans, 20/20 vision is the ability to resolve a spatial pattern separated by a visual angle of one minute of arc.
A 20/20 letter subtends 5 minutes of arc total.
Materials.
The deviation from parallelism between two surfaces, for instance in optical engineering, is usually measured in arcminutes or arcseconds.
External links.
MOA / mils By Robert Simeone

</doc>
<doc id="2433" url="https://en.wikipedia.org/wiki?curid=2433" title="Alberto Giacometti">
Alberto Giacometti

Alberto Giacometti (; 10 October 1901 – 11 January 1966) was a Swiss sculptor, painter, draughtsman and printmaker.
He was born in the canton Graubünden's southerly alpine valley Val Bregaglia, as the eldest of four children to Giovanni Giacometti, a well-known post-Impressionist painter. Coming from an artistic background, he was interested in art from an early age.
Early life.
Giacometti was born in Borgonovo, now part of the Swiss municipality of Bregaglia, near the Italian border. He was a descendant of Protestant refugees escaping the inquisition. Alberto attended the Geneva School of Fine Arts. His brothers Diego (1902–85) and Bruno (1907–2012) would go on to become artists as well. Additionally, Zaccaria Giacometti, later professor of constitutional law and chancellor of the University of Zurich grew up together with them, having been orphaned at the age of 12 in 1905.
In 1922 he moved to Paris to study under the sculptor Antoine Bourdelle, an associate of Rodin. It was there that Giacometti experimented with cubism and surrealism and came to be regarded as one of the leading surrealist sculptors. Among his associates were Miró, Max Ernst, Picasso, Bror Hjorth and Balthus.
Between 1936 and 1940, Giacometti concentrated his sculpting on the human head, focusing on the sitter's gaze. He preferred models he was close to, his sister and the artist Isabel Rawsthorne (then known as Isabel Delmer). This was followed by a phase in which his statues of Isabel became stretched out; her limbs elongated. Obsessed with creating his sculptures exactly as he envisioned through his unique view of reality, he often carved until they were as thin as nails and reduced to the size of a pack of cigarettes, much to his consternation. A friend of his once said that if Giacometti decided to sculpt you, "he would make your head look like the blade of a knife". After his marriage to Annette Arm in 1946 his tiny sculptures became larger, but the larger they grew, the thinner they became. Giacometti said that the final result represented the sensation he felt when he looked at a woman.
His paintings underwent a parallel procedure. The figures appear isolated and severely attenuated, as the result of continuous reworking. Subjects were frequently revisited: one of his favorite models was his younger brother Diego Giacometti. A third brother, Bruno Giacometti, was a noted architect.
Later years.
In 1958 Giacometti was asked to create a monumental sculpture for the Chase Manhattan Bank building in New York, which was beginning construction. Although he had for many years "harbored an ambition to create work for a public square", he "had never set foot in New York, and knew nothing about life in a rapidly evolving metropolis. Nor had he ever laid eyes on an actual skyscraper", according to his biographer James Lord. Giacometti's work on the project resulted in the four figures of standing women—his largest sculptures—entitled "Grande femme debout" I through IV (1960). The commission was never completed, however, because Giacometti was unsatisfied by the relationship between the sculpture and the site, and abandoned the project.
In 1962, Giacometti was awarded the grand prize for sculpture at the Venice Biennale, and the award brought with it worldwide fame. Even when he had achieved popularity and his work was in demand, he still reworked models, often destroying them or setting them aside to be returned to years later. The prints produced by Giacometti are often overlooked but the catalogue raisonné, "Giacometti – The Complete Graphics and 15 Drawings by Herbert Lust" (Tudor 1970), comments on their impact and gives details of the number of copies of each print. Some of his most important images were in editions of only 30 and many were described as rare in 1970.
In his later years Giacometti's works were shown in a number of large exhibitions throughout Europe. Riding a wave of international popularity, and despite his declining health, he travelled to the United States in 1965 for an exhibition of his works at the Museum of Modern Art in New York. As his last work he prepared the text for the book "Paris sans fin", a sequence of 150 lithographs containing memories of all the places where he had lived.
Death.
Giacometti died in 1966 of heart disease (pericarditis) and chronic bronchitis at the Kantonsspital in Chur, Switzerland. His body was returned to his birthplace in Borgonovo, where he was interred close to his parents. In May 2007 the executor of his widow's estate, former French foreign minister Roland Dumas, was convicted of illegally selling Giacometti's works to a top auctioneer, Jacques Tajan, who was also convicted. Both were ordered to pay €850,000 to the Alberto and Annette Giacometti Foundation.
Artistic analysis.
Regarding Giacometti's sculptural technique and according to the Metropolitan Museum of Art: "The rough, eroded, heavily worked surfaces of Three Men Walking (II), 1949, typify his technique. Reduced, as they are, to their very core, these figures evoke lone trees in winter that have lost their foliage. Within this style, Giacometti would rarely deviate from the three themes that preoccupied him—the walking man; the standing, nude woman; and the bust—or all three, combined in various groupings".""
In a letter to Pierre Matisse, Giacometti wrote: "Figures were never a compact mass but like a transparent construction". In the letter, Giacometti writes about how he looked back at the realist, classical busts of his youth with nostalgia, and tells the story of the existential crisis which precipitated the style he became known for.
" rediscovere the wish to make compositions with figures. For this I had to make (quickly I thought; in passing), one or two studies from nature, just enough to understand the construction of a head, of a whole figure, and in 1935 I took a model. This study should take, I thought, two weeks and then I could realize my compositions...I worked with the model all day from 1935 to 1940...Nothing was as I imagined. A head, became for me an object completely unknown and without dimensions."
Since Giacometti achieved exquisite realism with facility when he was executing busts in his early adolescence, Giacometti's difficulty in re-approaching the figure as an adult is generally understood as a sign of existential struggle for meaning, rather than as a technical deficit.
Giacometti was a key player in the Surrealist art movement, but his work resists easy categorization. Some describe it as formalist, others argue it is expressionist or otherwise having to do with what Deleuze calls "blocs of sensation" (as in Deleuze's analysis of Francis Bacon). Even after his excommunication from the Surrealist group, while the intention of his sculpting was usually imitation, the end products were an expression of his emotional response to the subject. He attempted to create renditions of his models the way he saw them, and the way he thought they ought to be seen. He once said that he was sculpting not the human figure but "the shadow that is cast".
Scholar William Barrett in "Irrational Man: A Study in Existential Philosophy" (1962), argues that the attenuated forms of Giacometti's figures reflect the view of 20th century modernism and existentialism that modern life is increasingly empty and devoid of meaning. "All the sculptures of today, like those of the past, will end one day in pieces...So it is important to fashion ones work carefully in its smallest recess and charge every particle of matter with life."
A new exhibition in Paris, since September 2011, shows how Giacometti strongly drew his inspiration for his work from Etruscan art.
Legacy.
Exhibitions.
Giacometti's work has been the subject of numerous solo exhibitions including most recently Pera Museum, Istanbul (2015) Pushkin Museum, Moscow (2008); “The Studio of Alberto Giacometti: Collection of the Fondation Alberto et Annette Giacometti”, Centre Pompidou, Paris (2007–2008); Kunsthal Rotterdam (2008); Fondation Beyeler, Basel (2009), Buenos Aires (2012); and Kunsthalle Hamburg (2013).
The National Portrait Gallery, London's first solo exhibition of Giacometti's work, "Pure Presence" opened to five star reviews on 13 October 2015 (to January 10, 2016, in honour of the fiftieth anniversary of the artist's death).
Public collections.
Giacometti's work is displayed in numerous public collections, including:
Art Foundations.
The Fondation Alberto et Annette Giacometti, having received a bequest from Alberto Giacometti's widow Annette, holds a collection of circa 5,000 works, frequently displayed around the world through exhibitions and long-term loans. A public interest institution, the Foundation was created in 2003 and aims at promoting, disseminating, preserving and protecting Alberto Giacometti's work.
The Alberto Giacometti-Stiftung established in Zürich in 1965, holds a smaller collection of works acquired from the collection of the Pittsburgh industrialist G. David Thompson.
Notable sales.
In November 2000 a Giacometti bronze, "Grande Femme Debout I", sold for $14.3 million. "Grande Femme Debout II" was bought by the Gagosian Gallery for $27.4 million at Christie's auction in New York City on May 6, 2008.
"L'Homme qui marche I", a life-sized bronze sculpture of a man, became one of the most expensive works of art and the most expensive sculpture ever sold at auction on February 2, 2010, when it sold for £65 million (US$104.3 million) at Sotheby's, London. "Grande tête mince", a large bronze bust, sold for $53.3 million just three months later.
"L'Homme au doigt" ("Pointing Man") sold for $126 million (£81314455.32), or $141.3 million with fees, in Christie's May 11, 2015 Looking Forward to the Past sale in New York, a record for a sculpture at auction. The work had been in the same private collection for 45 years.
Other legacy.
Giacometti created the monument on the grave of Gerda Taro at Père Lachaise Cemetery.
In 2001 he was included in the exhibition held at the National Portrait Gallery, London.
Giacometti and his sculpture "L'Homme qui marche I" appear on the current 100 Swiss Franc banknote.
According to a lecture by Michael Peppiatt at Cambridge University on July 8, 2010, Giacometti, who had a friendship with author/playwright Samuel Beckett, created a tree for the set of a 1961 Paris production of "Waiting For Godot".

</doc>
<doc id="2439" url="https://en.wikipedia.org/wiki?curid=2439" title="Anthem">
Anthem

An anthem is a musical composition of celebration, usually used as a symbol for a distinct group, particularly the national anthems of countries. Originally, and in music theory and religious contexts, it also refers more particularly to short sacred choral work and still more particularly to a specific form of Anglican church music
Etymology.
"Anthem" is derived from the Greek ("antíphōna") via Old English . Both words originally referred to antiphons, a call-and-response style of singing. The adjectival form is "anthemic".
History.
Anthems were originally a form of liturgical music. In the Church of England, the rubric appoints them to follow the third collect at morning and evening prayer. Several anthems are included in the British coronation service. The words are selected from Holy Scripture or in some cases from the Liturgy and the music is generally more elaborate and varied than that of psalm or hymn tunes. Being written for a trained choir rather than the congregation, the Anglican anthem is analogous to the motet of the Roman Catholic and Lutheran Churches but represents an essentially English musical form. Anthems may be described as "verse", "full", or "full with verse", depending on whether they are intended for soloists, the full choir, or both.
The anthem developed as a replacement for the Catholic "votive antiphon" commonly sung as an appendix to the main office to the Blessed Virgin Mary or other saints. During the Elizabethan period, notable anthems were composed by Thomas Tallis, William Byrd, Tye, and Farrant but they were not mentioned in the Book of Common Prayer until 1662 when the famous rubric "In quires and places where they sing here followeth the Anthem" first appears. Early anthems tended to be simple and homophonic in texture, so that the words could be clearly heard. During the 17th century, notable anthems were composed by Orlando Gibbons, Henry Purcell, and John Blow, with the verse anthem becoming the dominant musical form of the Restoration. In the 18th century, famed anthems were composed by Croft, Boyce, James Kent, James Nares, Benjamin Cooke, and Samuel Arnold. In the 19th, Samuel Sebastian Wesley wrote anthems influenced by contemporary oratorio which stretch to several movements and last twenty minutes or longer. Later in the century, Charles Villiers Stanford used symphonic techniques to produce a more concise and unified structure. Many anthems have been composed since this time, generally by organists rather than professional composers and often in a conservative style. Major composers have usually composed anthems in response to commissions and for special occasions. Examples include Edward Elgar's 1912 "Great is the Lord" and 1914 "Give unto the Lord" (both with orchestral accompaniment), Benjamin Britten's 1943 "Rejoice in the Lamb" (a modern example of a multi-movement anthem, today heard mainly as a concert piece), and, on a much smaller scale, Ralph Vaughan Williams's 1952 "O Taste and See" written for the coronation of Queen Elizabeth II. With the relaxation of the rule, in England at least, that anthems should be only in English, the repertoire has been greatly enhanced by the addition of many works from the Latin repertoire.
The word "anthem" is now commonly used to describe any celebratory song or composition for a distinct group, as in national anthems. Many pop songs are used as sports anthems, notably including Queen's "We Are the Champions" and "We Will Rock You", and some sporting events have their own anthems, most notably including UEFA Champions League. Further, some songs are artistically styled as anthems, whether or not they are used as such, including Marilyn Manson's "Irresponsible Hate Anthem", Silverchair's "Anthem for the Year 2000", and Toto's "Child's Anthem".

</doc>
<doc id="2440" url="https://en.wikipedia.org/wiki?curid=2440" title="Albrecht Altdorfer">
Albrecht Altdorfer

Albrecht Altdorfer (c. 1480 – February 12, 1538) was a German painter, engraver and architect of the Renaissance working in Regensburg. Along with Lucas Cranach the Elder and Wolf Huber he is regarded to be the main representative of the so-called Danube School setting biblical and historical subjects against landscape backgrounds of expressive colours. As an artist also making small intricate engravings he is seen to belong to the Nuremberg Little Masters.
Biography.
Altdorfer was born in Regensburg or Altdorf around 1480.
He acquired an interest in art from his father, Ulrich Altdorfer, who was a painter and miniaturist. At the start of his career, he won public attention by creating small, intimate modestly scaled works in unconventional media and with eccentric subject matter. He settled in the free imperial city of Regensburg, a town located on the Danube River in 1505, eventually becoming the town architect and a town councillor. His first signed works date to c. 1506, including engravings and drawings such the "Stygmata of St. Francis" and "St. Jerome". His models were niellos and copper engravings from the workshops of Jacopo de Barbari and Albrecht Dürer.
Around 1511 or earlier, he travelled down the river and south into the Alps, where the scenery moved him so deeply that he became the first landscape painter in the modern sense, making him the leader of the Danube School, a circle that pioneered landscape as an independent genre, in southern Germany. From 1513 he was at the service of Maximilian I in Innsbruck, where he received several commissions from the imperial court. During the turmoil of the Protestant Reformation, he dedicated mostly to architecture; paintings of the period, showing his increasing attention to architecture, include the "Nativity of the Virgin".
In 1529 he executed "The Battle of Alexander at Issus" for Duke William IV of Bavaria. In the 1520s he returned to Regensburg as a wealthy man, and became a member of the city's council. He was also responsible for the fortifications of Regensburg.
In that period his works are influenced by artists such as Giorgione and Lucas Cranach, as shown by his "Crucifixion". In 1535 he was in Vienna. He died at Regensburg in 1538.
The remains of Altdorfer's surviving work comprises 55 panels, 120 drawings, 125 woodcuts, 78 engravings, 36 etchings, 24 paintings on parchment, and fragments from a mural for the bathhouse of the Kaiserhof in Regensburg. This production extends at least over the period 1504–1537. He signed and dated each one of his works.
Painting.
Altdorfer was the pioneer painter of pure landscape, making them the subject of the painting, as well as compositions dominated by their landscape. He believed that the human figure shouldn't disrupt nature, but rather participate in it or imitate its natural processes. Taking and developing the landscape style of Lucas Cranach the Elder, he shows the hilly landscape of the Danube valley with thick forests of drooping and crumbling firs and larches hung with moss, and often dramatic colouring from a rising or setting sun. His "Landscape with Footbridge" (National Gallery, London) of 1518–1520 is claimed to be the first pure landscape in oil. In this painting, Altdorfer places a large tree that is cut off by the margins at the center of the landscape, making it the central axis and focus within the piece. He uses anthropomorphism to give the tree human qualities such as the drapery of its limbs. He also made many fine finished drawings, mostly landscapes, in pen and watercolour such as the "Landscape with the Woodcutter" in 1522. The drawing opens at ground level on a clearing surrounding an enormous tree that is placed in the center, dominating the picture. It poses and gesticulates as if it was human, splaying its branches out in every corner. Halfway up the tree trunk, hangs a gabled shrine. At the time, a shrine like this might shelter an image of the Crucifixion or the Virgin Mary, but since it is turned away from the viewer, we are not sure what it truly is. At the bottom of the tree, a tiny figure of a seated man, crossed legged, holds a knife and axe, declaring his status in society/occupation.
Also, he often painted scenes of historical and biblical subjects, set in atmospheric landscapes. His best religious scenes are intense, with their glistening lights and glowing colours sometimes verging on the expressionistic. They often depict moments of intimacy between Christ and his mother, or various saints. His sacral masterpiece and one of the most famous religious works of art of the later Middle Ages is "The Legend of St. Sebastian" and "The Passion of Christ" of the so-called "Sebastian Altar" in "St. Florian's Priory" ("Stift Sankt Florian") near Linz, Upper Austria. When closed the altarpiece displayed the four panels of the legend of St. Sebastian’s Martyrdom, while the opened wings displayed the Stations of the Cross. Today the altarpiece is dismantled and the predellas depicting the two final scenes, "Entombment" and "Resurrection" were sold to Kunsthistorisches Museum in Vienna in 1923 and 1930. Both these paintings share a similar formal structure that consists of an open landscape that is seen beyond and through the opening of a dark grotto. The date of completion on the resurrection panel is 1518.
Altdorfer often distorts perspective to subtle effect. His donor figures are often painted completely out of scale with the main scene, as in paintings of the previous centuries. He also painted some portraits; overall his painted oeuvre was not large. In his later works, Altdorfer moved more towards mannerism and began to depict the human form to the conformity of the Italian model, as well as dominate the picture with frank colors.
Paintings in Munich.
His rather atypical "Battle of Issus" (or of "Alexander") of 1529 was commissioned by William IV, Duke of Bavaria as part of a series of eight historical battle scenes destined to hang in the Residenz in Munich. Albrecht Altdorfer's depiction of the moment in 333 BCE when Alexander the Great routed Darius III for supremacy in Asia Minor is vast in ambition, sweeping in scope, vivid in imagery, rich in symbols, and obviously heroic—the Iliad of painting, as literary critic Friedrich Schlegel suggested In the painting, a swarming cast of thousands of soldiers surround the central action: Alexander on his white steed, leading two rows of charging cavalrymen, dashes after a fleeing Darius, who looks anxiously over his shoulder from a chariot. The opposing armies are distinguished by the colors of their uniforms: Darius' army in red and Alexander's in blue. The upper half of "The Battle of Alexander" expands with unreal rapidity into an arcing panorama comprehending vast coiling tracts of globe and sky. The victory also lies on the planar surface; The sun outshone the moon just as the Imperial and allied army successfully repel the Turks.
By making the mass number of soldiers blend within the landscape/painting, it shows that he believed that the usage and depiction of landscape was just as significant as a historical event, such as a war. He renounced the office of "Mayor of Regensburg" to accept the commission. Few of his other paintings resemble this apocalyptic scene of two huge armies dominated by an extravagant landscape seen from a very high viewpoint, which looks south over the whole Mediterranean from modern Turkey to include the island of Cyprus and the mouths of the Nile and the Red Sea (behind the isthmus to the left) on the other side. However his style here is a development of that of a number of miniatures of battle-scenes he had done much earlier for Maximilian I in his illuminated manuscript "Triumphal Procession" in 1512-14. It is thought to be the earliest painting to show the curvature of the Earth from a great height.
The "Battle" is now in the Alte Pinakothek, which has the best collection of Altdorfer's paintings, including also his small "St. George and the Dragon" (1510), in oil on parchment, where the two figures are tiny and almost submerged in the lush, dense forest that towers over them. Altdorfer seems to exaggerate the measurements of the forest in comparison to the figures: the leaves appear to be larger than the horse, showing the significance of nature and landscape. He also emphasizes line within the work, by displaying the upward growth of the forest with the vertical and diagonal lines of the trunks. There is a small opening of the forest on the lower right hand corner that provides a rest for your eyes. It serves to create depth within the painting and is the only place you can see the characters. The human form is completely absorbed by the thickness of the forest. Fantastic light effects provide a sense of mystery and dissolve the outline of objects. Without the contrast of light, the figures would blend in with its surrounding environment. Altdorfer's figures are invariably the complement of his romantic landscapes; for them he borrowed Albrecht Dürer's inventive iconography, but the panoramic setting is personal and has nothing to do with the fantasy landscapes of the Netherlands A "" (1526) set outside an Italianate skyscraper of a palace shows his interest in architecture. Another small oil on parchment, "Danube Landscape with Castle Wörth" (c. 1520) is one of the earliest accurate topographical paintings of a particular building in its setting, of a type that was to become a cliché in later centuries.
Printmaking.
Altdorfer was a significant printmaker, with numerous engravings and about ninety-three woodcuts. These included some for the "Triumphs of Maximilian", where he followed the overall style presumably set by Hans Burgkmair, although he was able to escape somewhat from this in his depictions of the more disorderly baggage-train, still coming through a mountain landscape. However most of his best prints are etchings, many of landscapes; in these he was able most easily to use his drawing style. He was one of the most successful early etchers, and was unusual for his generation of German printmakers in doing no book illustrations. He often combined etching and engraving techniques in a single plate, and produced about 122 intaglio prints altogether. Many of Altdorfer's prints are quite small in size, and he is considered to be of the main members of the group of artists known as the Little Masters. Arthur Mayger Hind considers his graphical work to be somewhat lacking in technical skill but with an "intimate personal touch", and notes his characteristic feeling for landscape.
Public life.
As the superintendent of the municipal buildings Altdorfer had overseen the construction of several commercial structures, such as a slaughterhouse and a building for wine storage, possibly even designing them. He was considered to be an outstanding politician of his day. In 1517 he was a member of the "Ausseren Rates", the council on external affairs, and in this capacity was involved in the expulsion of the Jews, the destruction of the synagogue and in its place the construction of a church and shrine to the Schöne Maria that occurred in 1519. Altdorfer made etchings of the interior of the synagogue and designed a woodcut of the cult image of the Schöne Maria. In 1529–1530 he was also charged with reinforcing certain city fortifications in response to the Turkish threat.
Albrecht's brother, Erhard Altdorfer, was also a painter and printmaker in woodcut and engraving, and a pupil of Lucas Cranach the Elder.

</doc>
<doc id="2441" url="https://en.wikipedia.org/wiki?curid=2441" title="House of Ascania">
House of Ascania

The House of Ascania () is a dynasty of German rulers. It is also known as the House of Anhalt, after Anhalt, its longest-held possession.
The Ascanians are named after Ascania (or Ascaria) Castle, "Schloss Askanien", which is located near and named after Aschersleben. The castle was seat of the County of Ascania, a title that was later subsumed into the titles of the princes of Anhalt.
History.
The earliest known member of the house, Esiko, Count of Ballenstedt, first appears in a document of 1036, and is assumed to have been a grandson (through his mother) of Odo I, Margrave of the Saxon Ostmark. From Odo, the Ascanians inherited large properties in the Saxon Eastern March.
Esiko's grandson was Otto, Count of Ballenstedt, who died in 1123. By Otto's marriage to Eilika, daughter of Magnus, Duke of Saxony, the Ascanians became heirs to half of the property of the House of Billung, former dukes of Saxony. 
Otto's son, Albert the Bear, became, with the help of his mother's inheritance, the first Ascanian duke of Saxony in 1139. But he lost control of Saxony soon to the rival House of Guelph.
However, Albert inherited the Margraviate of Brandenburg from its last Wendish ruler, Pribislav, in 1157, and became the first Ascanian margrave. Albert, and his descendants of the House of Ascania, then made considerable progress in Christianizing and Germanizing the lands. As a borderland between German and Slavic cultures, the country was known as a march. 
In 1237 and 1244 two towns, Cölln and Berlin were founded during the rule of Otto and Johann, grandsons of Margrave Albert the Bear, (later they were united into one city, Berlin). The emblem of the House of Ascania, red eagle and bear, became the heraldic emblems of Berlin.
In 1320 the Brandenburg Ascanian line came to an end.
After the Emperor had deposed the Guelph rulers of Saxony in 1180, Ascanians returned to rule the Duchy of Saxony, which had been reduced to its eastern half by the Emperor. However, even in eastern Saxony, the Ascanians could establish control only in limited areas, mostly near the River Elbe. 
In the 13th century, the Principality of Anhalt was split off from the Duchy, and later, the remaining state was split into Saxe-Lauenburg and Saxe-Wittenberg. The Ascanian dynasties in the two Saxon states became extinct in 1689 and in 1422, respectively, but Ascanians continued to rule in the smaller state of Anhalt and its various subdivisions until monarchy was abolished in 1918. 
Catherine the Great, Empress of Russia from 1762–1796, was a member of the House of Ascania, herself the daughter of Christian August, Prince of Anhalt-Zerbst.

</doc>
<doc id="2443" url="https://en.wikipedia.org/wiki?curid=2443" title="Acceleration">
Acceleration

Acceleration, in physics, is the rate of change of velocity of an object. An object's acceleration is the net result of any and all forces acting on the object, as described by Newton's Second Law. The SI unit for acceleration is metre per second squared (m s). Accelerations are vector quantities (they have magnitude and direction) and add according to the parallelogram law. As a vector, the calculated net force is equal to the product of the object's mass (a scalar quantity) and its acceleration.
For example, when a car starts from a standstill (zero relative velocity) and travels in a straight line at increasing speeds, it is accelerating in the direction of travel. If the car turns, there is an acceleration toward the new direction. In this example, we can call the forward acceleration of the car a "linear acceleration", which passengers in the car might experience as a force pushing them back into their seats. When changing direction, we might call this "non-linear acceleration", which passengers might experience as a sideways force. If the speed of the car decreases, this is an acceleration in the opposite direction from the direction of the vehicle, sometimes called deceleration. Passengers may experience deceleration as a force lifting them forwards. Mathematically, there is no separate formula for deceleration: both are changes in velocity. Each of these accelerations (linear, non-linear, deceleration) might be felt by passengers until their velocity (speed and direction) matches that of the car.
Definition and properties.
Average acceleration.
An object's average acceleration over a period of time is its change in velocity formula_1 divided by the duration of the period formula_2. Mathematically,
Instantaneous acceleration.
Instantaneous acceleration, meanwhile, is the limit of the average acceleration over an infinitesimal interval of time. In the terms of calculus, instantaneous acceleration is the derivative of the velocity vector with respect to time:
It can be seen that the integral of the acceleration function is the velocity function ; that is, the area under the curve of an acceleration vs. time ( vs. ) graph corresponds to velocity.
As acceleration is defined as the derivative of velocity, , with respect to time and velocity is defined as the derivative of position, , with respect to time, acceleration can be thought of as the second derivative of with respect to :
Units.
Acceleration has the dimensions of velocity (L/T) divided by time, i.e. L.T. The SI unit of acceleration is the metre per second squared (m s); or "metre per second per second", as the velocity in metres per second changes by the acceleration value, every second.
Other forms.
An object moving in a circular motion—such as a satellite orbiting the Earth—is accelerating due to the change of direction of motion, although its speed may be constant. In this case it is said to be undergoing "centripetal" (directed towards the center) acceleration.
Proper acceleration, the acceleration of a body relative to a free-fall condition, is measured by an instrument called an accelerometer.
In classical mechanics, for a body with constant mass, the (vector) acceleration of the body's center of mass is proportional to the net force vector (i.e. sum of all forces) acting on it (Newton's second law):
where F is the net force acting on the body, "m" is the mass of the body, and a is the center-of-mass acceleration. As speeds approach the speed of light, relativistic effects become increasingly large.
Tangential and centripetal acceleration.
The velocity of a particle moving on a curved path as a function of time can be written as:
with "v"("t") equal to the speed of travel along the path, and
a unit vector tangent to the path pointing in the direction of motion at the chosen moment in time. Taking into account both the changing speed "v(t)" and the changing direction of u, the acceleration of a particle moving on a curved path can be written using the chain rule of differentiation for the product of two functions of time as:
where u is the unit (inward) normal vector to the particle's trajectory (also called "the principal normal"), and r is its instantaneous radius of curvature based upon the osculating circle at time "t". These components are called the tangential acceleration and the normal or radial acceleration (or centripetal acceleration in circular motion, see also circular motion and centripetal force).
Geometrical analysis of three-dimensional space curves, which explains tangent, (principal) normal and binormal, is described by the Frenet–Serret formulas.
Special cases.
Uniform acceleration.
"Uniform" or "constant" acceleration is a type of motion in which the velocity of an object changes by an equal amount in every equal time period.
A frequently cited example of uniform acceleration is that of an object in free fall in a uniform gravitational field. The acceleration of a falling body in the absence of resistances to motion is dependent only on the gravitational field strength "g" (also called "acceleration due to gravity"). By Newton's Second Law the force, "F", acting on a body is given by:
Because of the simple analytic properties of the case of constant acceleration, there are simple formulas relating the displacement, initial and time-dependent velocities, and acceleration to the time elapsed:
where
In particular, the motion can be resolved into two orthogonal parts, one of constant velocity and the other according to the above equations. As Galileo showed, the net result is parabolic motion, which describes, e. g., the trajectory of a projectile in a vacuum near the surface of Earth.
Circular motion.
Uniform circular motion, that is constant speed along a circular path, is an example of a body experiencing acceleration resulting in velocity of a constant magnitude but change of direction. In this case, because the direction of the object's motion is constantly changing, being tangential to the circle, the object's linear velocity vector also changes, but its speed does not. This acceleration is a radial acceleration since it is always directed toward the centre of the circle and takes the magnitude:
where formula_24 is the object's linear speed along the circular path. Equivalently, the radial acceleration vector (formula_25) may be calculated from the object's angular velocity formula_26:
where formula_28 is a vector directed from the centre of the circle and equal in magnitude to the radius. The negative shows that the acceleration vector is directed towards the centre of the circle (opposite to the radius).
The acceleration and the net force acting on a body in uniform circular motion are directed "toward" the centre of the circle; that is, it is centripetal. Whereas the so-called 'centrifugal force' appearing to act outward on the body is really a pseudo force experienced in the frame of reference of the body in circular motion, due to the body's linear momentum at a tangent to the circle.
With nonuniform circular motion, i.e., the speed along the curved path changes, a transverse acceleration is produced equal to the rate of change of the angular speed around the circle times the radius of the circle. That is,
The transverse (or tangential) acceleration is directed at right angles to the radius vector and takes the sign of the angular acceleration (formula_30).
Relation to relativity.
Special relativity.
The special theory of relativity describes the behavior of objects traveling relative to other objects at speeds approaching that of light in a vacuum. Newtonian mechanics is exactly revealed to be an approximation to reality, valid to great accuracy at lower speeds. As the relevant speeds increase toward the speed of light, acceleration no longer follows classical equations.
As speeds approach that of light, the acceleration produced by a given force decreases, becoming infinitesimally small as light speed is approached; an object with mass can approach this speed asymptotically, but never reach it.
General relativity.
Unless the state of motion of an object is known, it is totally impossible to distinguish whether an observed force is due to gravity or to acceleration—gravity and inertial acceleration have identical effects. Albert Einstein called this the principle of equivalence, and said that only observers who feel no force at all—including the force of gravity—are justified in concluding that they are not accelerating.

</doc>
<doc id="2444" url="https://en.wikipedia.org/wiki?curid=2444" title="Conservation-restoration of cultural heritage">
Conservation-restoration of cultural heritage

The conservation-restoration of cultural heritage focuses on protection and care of tangible cultural heritage, including artworks, architecture, archaeology, and museum collections. Conservation activities include preventive conservation, examination, documentation, research, treatment, and education. This field is closely allied with conservation science, curators and registrars.
Definition.
Conservation of cultural heritage involves protection and restoration using "any methods that prove effective in keeping that property in as close to its original condition as possible for as long as possible." Conservation of cultural heritage is often associated with art collections and museums and involves collection care and management through tracking, examination, documentation, exhibition, storage, preventative conservation, and restoration.
The scope has widened from art conservation, involving protection and care of artwork and architecture, to conservation of cultural heritage, also including protection and care of a broad set of other cultural and historical works. Conservation of cultural heritage would more accurately be described as a type of ethical stewardship.
Conservation of cultural heritage applies simple ethical guidelines:
Often there are compromises between preserving appearance, maintaining original design and material properties, and ability to reverse changes. Reversibility is now emphasized so as to reduce problems with future treatment, investigation, and use.
In order for conservators to decide upon an appropriate conservation strategy and apply their professional expertise accordingly, they must take into account views of the stakeholder, the values and meaning of the work, and the physical needs of the material.
Cesare Brandi in his "Theory of Restoration", describes restoration as "the methodological moment in which the work of art is appreciated in its material form and in its historical and aesthetic duality, with a view to transmitting it to the future".
History.
Key dates.
Some consider the tradition of conservation of cultural heritage in Europe to have begun in 1565 with the restoration of the Sistine Chapel frescoes, but more ancient examples include the work of Cassiodorus.
Brief history.
The care of cultural heritage has a long history, one that was primarily aimed at fixing and mending objects for their continued use and aesthetic enjoyment. Until the early 20th century, artists were normally the ones called upon to repair damaged artworks. During the 19th century, however, the fields of science and art became increasingly intertwined as scientists such as Michael Faraday began to study the damaging effects of the environment to works of art. Louis Pasteur carried out scientific analysis on paint as well. However, perhaps the first organized attempt to apply a theoretical framework to the conservation of cultural heritage came with the founding in the United Kingdom of the Society for the Protection of Ancient Buildings in 1877. The society was founded by William Morris and Philip Webb, both of whom were deeply influenced by the writings of John Ruskin. During the same period, a French movement with similar aims was being developed under the direction of Eugène Viollet-le-Duc, an architect and theorist, famous for his restorations of medieval buildings.
Conservation of cultural heritage as a distinct field of study initially developed in Germany, where in 1888 Friedrich Rathgen became the first chemist to be employed by a Museum, the Koniglichen Museen, Berlin (Royal Museums of Berlin). He not only developed a scientific approach to the care of objects in the collections, but disseminated this approach by publishing a Handbook of Conservation in 1898. The early development of conservation of cultural heritage in any area of the world is usually linked to the creation of positions for chemists within museums. In the United Kingdom, pioneering research into painting materials and conservation, ceramics, and stone conservation was conducted by Arthur Pillans Laurie, academic chemist and Principal of Heriot-Watt University from 1900. Laurie's interests were fostered by William Holman Hunt. In 1924 the chemist Harold Plenderleith began to work at the British Museum with Dr. Alexander Scott in the newly created Department of Scientific and Industrial Research, thus giving birth to the conservation profession in the UK. This department was created by the museum to address the deteriorating condition of objects in the collection, damages which were a result of their being stored in the London Underground tunnels during the First World War. The creation of this department moved the focus for the development of conservation theory and practice from Germany to Britain, and made the latter a prime force in this fledgling field. In 1956 Plenderleith wrote a significant handbook called The Conservation of Antiquities and Works of Art, which supplanted Rathgen's earlier tome and set new standards for the development of art and conservation science.
In the United States, the development of conservation of cultural heritage can be traced to the Fogg Art Museum, and Edward Waldo Forbes, its director from 1909 to 1944. He encouraged technical investigation, and was Chairman of the Advisory Committee for the first technical journal, Technical Studies in the Field of the Fine Arts, published by the Fogg from 1932 to 1942. Importantly he also brought onto the museum staff chemists. Rutherford John Gettens was the first of usch in the US to be permanently employed by an art museum. He worked with George L. Stout, the founder and first editor of Technical Studies. Gettens and Stout co-authored Painting Materials: A Short Encyclopaedia in 1942, reprinted in 1966. This compendium is still cited regularly. Only a few dates and descriptions in Gettens' and Stout's book are now outdated.
George T. Oliver, of Oliver Brothers Art Restoration and Art Conservation-Boston
(Est. 1850 in New York City) invented the vacuum hot table for relining paintings in 1920’s; he filed a patent for the table in 1937. Taylor's prototype table, which he designed and constructed, is still in operation. Oliver Brothers is believed to be the first and the oldest continuously operating art restoration company in the United States.
The focus of conservation development then accelerated in Britain and America, and it was in Britain that the first International Conservation Organisations developed. The International Institute for Conservation of Historic and Artistic Works (IIC) was incorporated under British law in 1950 as "a permanent organization to co-ordinate and improve the knowledge, methods, and working standards needed to protect and preserve precious materials of all kinds." The rapid growth of conservation professional organizations, publications, journals, newsletters, both internationally and in localities, has spearheaded the development of the conservation profession, both practically and theoretically. Art historians and theorists such as Cesare Brandi have also played a significant role in developing conservation science theory. In recent years ethical concerns have been at the forefront of developments in conservation. Most significantly has been the idea of Preventive conservation. This concept is based in part on the pioneering work by Garry Thomson CBE, and his book the Museum Environment, first published in 1978. Thomson was associated with the National Gallery (London), it was here that he established a set of guidelines or environmental controls for the best conditions in which objects could be stored and displayed within the Museum Environment. Although his exact guidelines are no longer rigidly followed they did inspire this field of conservation.
Ethics.
The conservator's work is guided by ethical standards. These take the form of applied ethics. Ethical standards have been established across the world, and national and international ethical guidelines have been written. One such example is:
Conservation OnLine's Ethical issues in conservation provides a number of articles on ethical issues in conservation; example of codes of ethics and guidelines for professional conduct in conservation and allied fields; and charters and treaties pertaining to ethical issues involving the preservation of cultural property.
As well as standards of practice conservators deal with wider ethical concerns, such as the debates as to whether all art is worth preserving.
Caring for cultural heritage.
Collections care.
Many cultural works are sensitive to environmental conditions such as temperature, humidity and exposure to light and ultraviolet light. They must be protected in a controlled environment where such variables are maintained within a range of damage-limiting levels. Shielding from sunlight of artifacts such as watercolour paintings for example is usually necessary to prevent fading of pigments.
Collections care is an important element of museum policy. It is an essential responsibility of members of the museum profession to create and maintain a protective environment for the collections in their care, whether in store, on display, or in transit. A museum should carefully monitor the condition of collections to determine when an artifact requires conservation work and the services of a qualified conservator.
Interventive conservation.
Interventive Conservation refers to any act by a conservator that involves a direct interaction between the conservator and the cultural material. These interventive treatments could involve cleaning, stabilizing, repair, or even replacement of parts of the original object. It is essential that the conservator should fully justify any such work. Complete documentation of the work carried out before, during, and after the treatment rules out chances of later doubts.
The principal goal of a cultural conservator is to nullify or at least reduce the rate of deterioration of an object. This can be achieved through either non-interventive or interventive methodologies. Interventive methodologies include all those actions taken by the conservator to directly intervene with the material fabric of the object. Such actions include surface cleaning such as varnish removal, or consolidation such as securing flaking paint. Such interventive actions are carried out for a variety of reasons including aesthetic choices, stabilization needs for structural integrity, or for cultural requirements for intangible continuity.
One of the guiding principles of conservation of cultural heritage has traditionally been the idea of reversibility, that all interventions with the object should be fully reversible and that the object should be able to be returned to the state in which it was prior to the conservator's intervention. Although this concept remains a guiding principle of the profession, it has been widely critiqued within the conservation profession and is now considered by many to be "a fuzzy concept." Another important principle of conservation is that all alterations should be well documented and should be clearly distinguishable from the original object.
An example of a highly publicized interventive conservation effort would be the conservation work conducted on the Sistine Chapel.
The conservation laboratory.
Conservators routinely use chemical and scientific analysis for the examination and treatment of cultural works. The modern conservation laboratory uses equipment such as microscopes, spectrometers, and various x-ray regime instruments to better understand objects and their components. The data thus collected helps in deciding the conservation treatments to be provided to the object.
Country by country look.
United States.
Heritage Preservation, in partnership with the Institute of Museum and Library Services, a U.S. federal agency, produced The Heritage Health Index. The results of this work was the report A Public Trust at Risk: The Heritage Health Index Report on the State of America's Collections, which was published in December 2005 and concluded that immediate action is needed to prevent the loss of 190 million artifacts that are in need of conservation treatment. The report made four recommendations:
United Kingdom.
In October 2006, the Department for Culture, Media and Sport, a governmental department, authored a document: "Understanding the Future: Priorities for England's Museums". This document was based on several years of consultation aimed to lay out the government's priorities for museums in the 21st century.
The document listed the following as priorities for the next decade:
The conservation profession response to this report was on the whole less than favourable, the Institute of Conservation (ICON) published their response under the title "A Failure of Vision". It had the following to say:
Concluding: 
Further to this the ICON website summary report lists the following specific recommendations:
In November 2008, the UK based think tank Demos published an influential pamphlet entitled 'It's a material world: caring for the public realm', in which they argue for integrating the public directly into efforts to conserve material culture, particularly that which is in the public, their argument, as stated on page 16, demonstrates their belief that society can benefit from conservation as a paradigm as well as a profession:
Training.
Training in conservation of cultural heritage for many years took the form of an apprenticeship, whereby an apprentice slowly developed the necessary skills to undertake their job. For some specializations within conservation this is still the case. However, it is more common in the field of conservation today that the training required to become a practicing conservator comes from a recognized university course in conservation of cultural heritage.
The University can rarely provide all the necessary training in first hand experience that an apprenticeship can, and therefore in addition to graduate level training the profession also tends towards encouraging conservation students to spend time as an intern.
Conservation of cultural heritage is an interdisciplinary field as conservators have backgrounds in the fine arts, sciences (including chemistry, biology, and materials science), and closely related disciplines, such as art history, archaeology, studio art, and anthropology. They also have design, fabrication, artistic, and other special skills necessary for the practical application of that knowledge.
Within the various schools that teach conservation of cultural heritage, the approach differs according to the educational and vocational system within the country, and the focus of the school itself. This is acknowledged by the American Institute for Conservation who advise "Specific admission requirements differ and potential candidates are encouraged to contact the programs directly for details on prerequisites, application procedures, and program curriculum".
Associations and professional organizations.
Societies devoted to the care of cultural heritage have been in existence around the world for many years. One early example is the founding in 1877 of the Society for the Protection of Ancient Buildings in Britain to protect the built heritage, this society continues to be active today.
The built heritage was also at the forefront of the growth of member based organizations in the United States for example, founded in 1889, the Richmond, Virginia-based Preservation Virginia (formerly known as the Association for the Preservation of Virginia Antiquities) was the United States' first statewide historic preservation group. In 2003, it changed its name to reflect its wider focus in statewide preservation issues.
Today, professional conservators join and take part in the activities of numerous conservation associations and professional organizations with the wider field, and within their area of specialization such as Isfahan (Esfahan) city in Iran.
These organizations exist to "support the conservation professionals who preserve our cultural heritage".
This involves upholding professional standards, promoting research and publications, providing educational opportunities, and fostering the exchange of knowledge among cultural conservators, allied professionals, and the public.
External links.
External lists.
External lists of international cultural heritage documents:

</doc>
<doc id="2447" url="https://en.wikipedia.org/wiki?curid=2447" title="Anton Chekhov">
Anton Chekhov

Anton Pavlovich Chekhov (; , ; 29 January 1860 – 15 July 1904) was a Russian playwright and short story writer who is considered to be among the greatest writers of short fiction in history. His career as a playwright produced four classics and his best short stories are held in high esteem by writers and critics. Along with Henrik Ibsen and August Strindberg, Chekhov is often referred to as one of the three seminal figures in the birth of early modernism in the theater.Chekhov practiced as a medical doctor throughout most of his literary career: "Medicine is my lawful wife", he once said, "and literature is my mistress." 
Chekhov renounced the theatre after the disastrous reception of "The Seagull" in 1896, but the play was revived to acclaim in 1898 by Constantin Stanislavski's Moscow Art Theatre, which subsequently also produced Chekhov's "Uncle Vanya" and premiered his last two plays, "Three Sisters" and "The Cherry Orchard". These four works present a challenge to the acting ensemble as well as to audiences, because in place of conventional action Chekhov offers a "theatre of mood" and a "submerged life in the text".
Chekhov had at first written stories only for financial gain, but as his artistic ambition grew, he made formal innovations which have influenced the evolution of the modern short story. He made no apologies for the difficulties this posed to readers, insisting that the role of an artist was to ask questions, not to answer them.
Biography.
Childhood.
Anton Chekhov was born on the feast day of St. Anthony the Great (17 January Old Style) 29 January 1860, the third of six surviving children, in Taganrog, a port on the Sea of Azov in southern Russia. His father, Pavel Yegorovich Chekhov, the son of a former serf, was from a village Vilkhovatka near Kobeliaky (Poltava Region in modern-day Ukraine) and ran a grocery store. A director of the parish choir, devout Orthodox Christian, and physically abusive father, Pavel Chekhov has been seen by some historians as the model for his son's many portraits of hypocrisy. Chekhov's mother, Yevgeniya (Morozova), was an excellent storyteller who entertained the children with tales of her travels with her cloth-merchant father all over Russia. "Our talents we got from our father," Chekhov remembered, "but our soul from our mother."
In adulthood, Chekhov criticised his brother Alexander's treatment of his wife and children by reminding him of Pavel's tyranny: "Let me ask you to recall that it was despotism and lying that ruined your mother's youth. Despotism and lying so mutilated our childhood that it's sickening and frightening to think about it. Remember the horror and disgust we felt in those times when Father threw a tantrum at dinner over too much salt in the soup and called Mother a fool."
Chekhov attended the Greek School in Taganrog and the Taganrog "Gymnasium" (since renamed the Chekhov Gymnasium), where he was kept down for a year at fifteen for failing an examination in Ancient Greek. He sang at the Greek Orthodox monastery in Taganrog and in his father's choirs. In a letter of 1892, he used the word "suffering" to describe his childhood and recalled:
He later became an atheist.
In 1876, Chekhov's father was declared bankrupt after overextending his finances building a new house. To avoid debtor's prison he fled to Moscow, where his two eldest sons, Alexander and Nikolay, were attending university. The family lived in poverty in Moscow, Chekhov's mother physically and emotionally broken by the experience. Chekhov was left behind to sell the family's possessions and finish his education.
Chekhov remained in Taganrog for three more years, boarding with a man called Selivanov who, like Lopakhin in "The Cherry Orchard", had bailed out the family for the price of their house. Chekhov had to pay for his own education, which he managed by private tutoring, catching and selling goldfinches, and selling short sketches to the newspapers, among other jobs. He sent every ruble he could spare to his family in Moscow, along with humorous letters to cheer them up. During this time, he read widely and analytically, including the works of Cervantes, Turgenev, Goncharov, and Schopenhauer, and wrote a full-length comic drama, "Fatherless", which his brother Alexander dismissed as "an inexcusable though innocent fabrication." Chekhov also enjoyed a series of love affairs, one with the wife of a teacher.
In 1879, Chekhov completed his schooling and joined his family in Moscow, having gained admission to the medical school at I.M. Sechenov First Moscow State Medical University.
Early writings.
Chekhov now assumed responsibility for the whole family. To support them and to pay his tuition fees, he wrote daily short, humorous sketches and vignettes of contemporary Russian life, many under pseudonyms such as "Antosha Chekhonte" (Антоша Чехонте) and "Man without a Spleen" (Человек без селезенки). His prodigious output gradually earned him a reputation as a satirical chronicler of Russian street life, and by 1882 he was writing for "Oskolki" ("Fragments"), owned by Nikolai Leykin, one of the leading publishers of the time. Chekhov's tone at this stage was harsher than that familiar from his mature fiction.
In 1884, Chekhov qualified as a physician, which he considered his principal profession though he made little money from it and treated the poor free of charge.
In 1884 and 1885, Chekhov found himself coughing blood, and in 1886 the attacks worsened, but he would not admit his tuberculosis to his family or his friends. He confessed to Leykin, "I am afraid to submit myself to be sounded by my colleagues." He continued writing for weekly periodicals, earning enough money to move the family into progressively better accommodations.
Early in 1886 he was invited to write for one of the most popular papers in St. Petersburg, "Novoye Vremya" ("New Times"), owned and edited by the millionaire magnate Alexey Suvorin, who paid a rate per line double Leykin's and allowed Chekhov three times the space. Suvorin was to become a lifelong friend, perhaps Chekhov's closest.
Before long, Chekhov was attracting literary as well as popular attention. The sixty-four-year-old Dmitry Grigorovich, a celebrated Russian writer of the day, wrote to Chekhov after reading his short story "The Huntsman" that "You have "real" talent, a talent that places you in the front rank among writers in the new generation." He went on to advise Chekhov to slow down, write less, and concentrate on literary quality.
Chekhov replied that the letter had struck him "like a thunderbolt" and confessed, "I have written my stories the way reporters write up their notes about fires — mechanically, half-consciously, caring nothing about either the reader or myself." The admission may have done Chekhov a disservice, since early manuscripts reveal that he often wrote with extreme care, continually revising. Grigorovich's advice nevertheless inspired a more serious, artistic ambition in the twenty-six-year-old. In 1888, with a little string-pulling by Grigorovich, the short story collection "At Dusk" ("V Sumerkakh") won Chekhov the coveted Pushkin Prize "for the best literary production distinguished by high artistic worth."
Turning points.
In 1887, exhausted from overwork and ill health, Chekhov took a trip to Ukraine, which reawakened him to the beauty of the steppe. On his return, he began the novella-length short story "," which he called "something rather odd and much too original," and which was eventually published in "Severny Vestnik" ("The Northern Herald"). In a narrative that drifts with the thought processes of the characters, Chekhov evokes a chaise journey across the steppe through the eyes of a young boy sent to live away from home, and his companions, a priest and a merchant. "The Steppe" has been called a "dictionary of Chekhov's poetics", and it represented a significant advance for Chekhov, exhibiting much of the quality of his mature fiction and winning him publication in a literary journal rather than a newspaper.
In autumn 1887, a theatre manager named Korsh commissioned Chekhov to write a play, the result being "Ivanov", written in a fortnight and produced that November. Though Chekhov found the experience "sickening" and painted a comic portrait of the chaotic production in a letter to his brother Alexander, the play was a hit and was praised, to Chekhov's bemusement, as a work of originality. Although Chekhov did not fully realize it at the time, Chekhov's plays, such as "The Seagull" (written in 1895), "Uncle Vanya" (written in 1897), "The Three Sisters" (written in 1900), and "The Cherry Orchard" (written in 1903) served as a revolutionary backbone to what is common sense to the medium of acting to this day: an effort to recreate and express the "realism" of how people truly act and speak with each other and translating it to the stage in order to manifest the human condition as accurately as possible in hopes to make the audience reflect upon their own definition of what it means to be human, warts and all. This philosophy of approaching the art of acting has stood not only steadfast, but as the cornerstone of acting for much of the 20th century to this day. Mikhail Chekhov considered "Ivanov" a key moment in his brother's intellectual development and literary career. From this period comes an observation of Chekhov's that has become known as "Chekhov's gun", a dramatic principle that requires that every element in a narrative be necessary and irreplaceable, and that everything else be removed.
The death of Chekhov's brother Nikolay from tuberculosis in 1889 influenced "A Dreary Story", finished that September, about a man who confronts the end of a life that he realises has been without purpose. Mikhail Chekhov, who recorded his brother's depression and restlessness after Nikolay's death, was researching prisons at the time as part of his law studies, and Anton Chekhov, in a search for purpose in his own life, himself soon became obsessed with the issue of prison reform.
Sakhalin.
In 1890, Chekhov undertook an arduous journey by train, horse-drawn carriage, and river steamer to the Russian Far East and the "katorga", or penal colony, on Sakhalin Island, north of Japan, where he spent three months interviewing thousands of convicts and settlers for a census. The letters Chekhov wrote during the two-and-a-half-month journey to Sakhalin are considered to be among his best. His remarks to his sister about Tomsk were to become notorious.
The inhabitants of Tomsk later retaliated by erecting a mocking statue of Chekhov.
Chekhov witnessed much on Sakhalin that shocked and angered him, including floggings, embezzlement of supplies, and forced prostitution of women. He wrote, "There were times I felt that I saw before me the extreme limits of man's degradation." He was particularly moved by the plight of the children living in the penal colony with their parents. For example:
Chekhov later concluded that charity was not the answer, but that the government had a duty to finance humane treatment of the convicts. His findings were published in 1893 and 1894 as "Ostrov Sakhalin" ("The Island of Sakhalin"), a work of social science, not literature, that is worthy and informative rather than brilliant. Chekhov found literary expression for the "Hell of Sakhalin" in his long short story "," the last section of which is set on Sakhalin, where the murderer Yakov loads coal in the night while longing for home. Chekhov's writing on Sakhalin is the subject of brief comment and analysis in Haruki Murakami's novel 1Q84. It is also the subject of a poem by the Nobel Prize winner Seamus Heaney, "Chekhov on Sakhalin" (collected in the volume "Station Island").
Melikhovo.
In 1892, Chekhov bought the small country estate of Melikhovo, about forty miles south of Moscow, where he lived with his family until 1899 . "It's nice to be a lord," he joked to his friend Ivan Leontyev (who wrote humorous pieces under the pseudonym Shcheglov), but he took his responsibilities as a landlord seriously and soon made himself useful to the local peasants. As well as organising relief for victims of the famine and cholera outbreaks of 1892, he went on to build three schools, a fire station, and a clinic, and to donate his medical services to peasants for miles around, despite frequent recurrences of his tuberculosis.
Mikhail Chekhov, a member of the household at Melikhovo, described the extent of his brother's medical commitments:
Chekhov's expenditure on drugs was considerable, but the greatest cost was making journeys of several hours to visit the sick, which reduced his time for writing. However, Chekhov's work as a doctor enriched his writing by bringing him into intimate contact with all sections of Russian society: for example, he witnessed at first hand the peasants' unhealthy and cramped living conditions, which he recalled in his short story "Peasants". Chekhov visited the upper classes as well, recording in his notebook: "Aristocrats? The same ugly bodies and physical uncleanliness, the same toothless old age and disgusting death, as with market-women."
In 1894, Chekhov began writing his play "The Seagull" in a lodge he had built in the orchard at Melikhovo. In the two years since he had moved to the estate, he had refurbished the house, taken up agriculture and horticulture, tended the orchard and the pond, and planted many trees, which, according to Mikhail, he "looked after ... as though they were his children. Like Colonel Vershinin in his "Three Sisters", as he looked at them he dreamed of what they would be like in three or four hundred years."
The first night of "The Seagull", at the Alexandrinsky Theatre in St. Petersburg on 17 October 1896, was a fiasco, as the play was booed by the audience, stinging Chekhov into renouncing the theatre. But the play so impressed the theatre director Vladimir Nemirovich-Danchenko that he convinced his colleague Constantin Stanislavski to direct a new production for the innovative Moscow Art Theatre in 1898. Stanislavski's attention to psychological realism and ensemble playing coaxed the buried subtleties from the text, and restored Chekhov's interest in playwriting. The Art Theatre commissioned more plays from Chekhov and the following year staged "Uncle Vanya", which Chekhov had completed in 1896.
Yalta.
In March 1897, Chekhov suffered a major hemorrhage of the lungs while on a visit to Moscow. With great difficulty he was persuaded to enter a clinic, where the doctors diagnosed tuberculosis on the upper part of his lungs and ordered a change in his manner of life.
After his father's death in 1898, Chekhov bought a plot of land on the outskirts of Yalta and built a villa, into which he moved with his mother and sister the following year. Though he planted trees and flowers, kept dogs and tame cranes, and received guests such as Leo Tolstoy and Maxim Gorky, Chekhov was always relieved to leave his "hot Siberia" for Moscow or travels abroad. He vowed to move to Taganrog as soon as a water supply was installed there. In Yalta he completed two more plays for the Art Theatre, composing with greater difficulty than in the days when he "wrote serenely, the way I eat pancakes now". He took a year each over "Three Sisters" and "The Cherry Orchard".
On 25 May 1901, Chekhov married Olga Knipper quietly, owing to his horror of weddings. She was a former protegée and sometime lover of Nemirovich-Danchenko whom he had first met at rehearsals for "The Seagull". Up to that point, Chekhov, known as "Russia's most elusive literary bachelor," had preferred passing liaisons and visits to brothels over commitment. He had once written to Suvorin:
The letter proved prophetic of Chekhov's marital arrangements with Olga: he lived largely at Yalta, she in Moscow, pursuing her acting career. In 1902, Olga suffered a miscarriage; and Donald Rayfield has offered evidence, based on the couple's letters, that conception may have occurred when Chekhov and Olga were apart, although Russian scholars have rejected that claim. The literary legacy of this long-distance marriage is a correspondence that preserves gems of theatre history, including shared complaints about Stanislavski's directing methods and Chekhov's advice to Olga about performing in his plays.
In Yalta, Chekhov wrote one of his most famous stories, "The Lady with the Dog" (also called "Lady with Lapdog"), which depicts what at first seems a casual liaison between a married man and a married woman in Yalta. Neither expects anything lasting from the encounter, but they find themselves drawn back to each other, risking the security of their family lives.
Death.
By May 1904, Chekhov was terminally ill with tuberculosis. Mikhail Chekhov recalled that "everyone who saw him secretly thought the end was not far off, but the nearer was to the end, the less he seemed to realise it." On 3 June, he set off with Olga for the German spa town of Badenweiler in the Black Forest, from where he wrote outwardly jovial letters to his sister Masha, describing the food and surroundings, and assuring her and his mother that he was getting better. In his last letter, he complained about the way German women dressed.
Chekhov's death has become one of "the great set pieces of literary history," retold, embroidered, and fictionalised many times since, notably in the short story "Errand" by Raymond Carver. In 1908, Olga wrote this account of her husband's last moments:
Chekhov's body was transported to Moscow in a refrigerated railway car meant for oysters, a detail that offended Gorky. Some of the thousands of mourners followed the funeral procession of a General Keller by mistake, to the accompaniment of a military band. Chekhov was buried next to his father at the Novodevichy Cemetery.
Legacy.
A few months before he died, Chekhov told the writer Ivan Bunin that he thought people might go on reading his writings for seven years. "Why seven?" asked Bunin. "Well, seven and a half," Chekhov replied. "That's not bad. I've got six years to live."
Chekhov's posthumous reputation greatly exceeded his expectations. The ovations for the play "The Cherry Orchard" in the year of his death served to demonstrate the Russian public's acclaim for the writer, which placed him second in literary celebrity only to Tolstoy, who outlived him by six years. Tolstoy was an early admirer of Chekhov's short stories and had a series that he deemed "first quality" and "second quality" bound into a book. In the first category were: "Children", "The Chorus Girl", "A Play", "Home", "Misery", "The Runaway", "In Court", "Vanka", "Ladies", "The Malefactors", "The Boys", "Darkness", "Sleepy", "The Helpmate", and "The Darling"; in the second: "A Transgression", "Sorrow", "The Witch", "Verochka", "In a Strange Land", "The Cook's Wedding", "A Tedious Business", "An Upheaval", "Oh! The Public!", "The Mask", "A Woman's Luck", "Nerves", "The Wedding", "A Defenseless Creature", and "Peasant Wives."
In Chekhov's lifetime, British and Irish critics generally did not find his work pleasing; E. J. Dillon thought "the effect on the reader of Chekhov's tales was repulsion at the gallery of human waste represented by his fickle, spineless, drifting people" and R. E. C. Long said "Chekhov's characters were repugnant, and that Chekhov reveled in stripping the last rags of dignity from the human soul". After his death, Chekhov was reappraised. Constance Garnett's translations won him an English-language readership and the admiration of writers such as James Joyce, Virginia Woolf, and Katherine Mansfield, whose story "The Child Who Was Tired" is similar to Chekhov's "Sleepy". The Russian critic D. S. Mirsky, who lived in England, explained Chekhov's popularity in that country by his "unusually complete rejection of what we may call the heroic values." In Russia itself, Chekhov's drama fell out of fashion after the revolution, but it was later incorporated into the Soviet canon. The character of Lopakhin, for example, was reinvented as a hero of the new order, rising from a modest background to eventually possess the gentry's estates.
One of the first non-Russians to praise Chekhov's plays was George Bernard Shaw, who subtitled his "Heartbreak House" "A Fantasia in the Russian Manner on English Themes," and pointed out similarities between the predicament of the British landed class and that of their Russian counterparts as depicted by Chekhov: "the same nice people, the same utter futility."
In the United States, Chekhov's reputation began its rise slightly later, partly through the influence of Stanislavski's system of acting, with its notion of subtext: "Chekhov often expressed his thought not in speeches," wrote Stanislavski, "but in pauses or between the lines or in replies consisting of a single word ... the characters often feel and think things not expressed in the lines they speak." The Group Theatre, in particular, developed the subtextual approach to drama, influencing generations of American playwrights, screenwriters, and actors, including Clifford Odets, Elia Kazan and, in particular, Lee Strasberg. In turn, Strasberg's Actors Studio and the "Method" acting approach influenced many actors, including Marlon Brando and Robert De Niro, though by then the Chekhov tradition may have been distorted by a preoccupation with realism. In 1981, the playwright Tennessee Williams adapted "The Seagull" as "The Notebook of Trigorin". One of Anton's nephews, Michael Chekhov would also contribute heavily to modern theatre, particularly through his unique acting methods which differed from Stanislavski's.
Despite Chekhov's acclaim during his lifetime as a playwright, some writers believe that his short stories represent the greater achievement. Raymond Carver, who wrote the short story "Errand" about Chekhov's death, believed that Chekhov was the greatest of all short story writers:
Ernest Hemingway, another writer influenced by Chekhov, was more grudging: "Chekhov wrote about six good stories. But he was an amateur writer." And Vladimir Nabokov criticized Chekhov's "medley of dreadful prosaisms, ready-made epithets, repetitions." But he also declared "The Lady with the Dog" "one of the greatest stories ever written" in its depiction of a problematic relationship, and described Chekhov as writing "the way one person relates to another the most important things in his life, slowly and yet without a break, in a slightly subdued voice."
For the writer William Boyd, Chekhov's historical accomplishment was to abandon what William Gerhardie called the "event plot" for something more "blurred, interrupted, mauled or otherwise tampered with by life."
Virginia Woolf mused on the unique quality of a Chekhov story in "The Common Reader" (1925):
While a Professor of Comparative Literature at Princeton University, Michael Goldman presented his view on defining the elusive quality of Chekhov's comedies stating: "Having learned that Chekhov is comic ... Chekhov is comic in a very special, paradoxical way. His plays depend, as comedy does, on the vitality of the actors to make pleasurable what would otherwise be painfully awkward -- inappropriate speeches, missed connections, "faux pas", stumbles, childishness -- but as part of a deeper pathos; the stumbles are not pratfalls but an energized, graceful dissolution of purpose."
Alan Twigg, the chief editor and publisher of the Canadian book review magazine "BC Bookworld wrote,
Chekhov has also influenced the work of Japanese playwrights including Shimizu Kunio, Yōji Sakate, and Ai Nagai. Critics have noted similarities in how Chekhov and Shimizu use a mixture of light humor as well as an intense depictions of longing. Sakate adapted several of Chekhov's plays and transformed them in the general style of "nō". Nagai also adapted Chekhov's plays, including "Three Sisters", and transformed his dramatic style into Nagai's style of satirical realism while emphasizing the social issues depicted on the play.
Chekhov's works have been adapted for the screen, including Sidney Lumet's "The Sea Gull" and Louis Malle's "Vanya on 42nd Street". His work has also served as inspiration or been referenced in numerous films. In Andrei Tarkovsky's 1975 film "The Mirror", characters discuss his short story Ward No. 6. Plays by Chekhov are also referenced in Francois Truffault's 1980 drama film "The Last Metro", which is set in a theater. A portion of a stage production of "Three Sisters" appears in the 2014 drama film "Still Alice".
External links.
Biographical
Works

</doc>
<doc id="2448" url="https://en.wikipedia.org/wiki?curid=2448" title="Action Against Hunger">
Action Against Hunger

Action Against Hunger is a global humanitarian organization committed to ending world hunger. The organization helps malnourished children while providing communities with access to safe water and sustainable solutions to hunger.
History.
Action Against Hunger was established in 1979 by a group of French doctors, scientists, and writers. Nobel Prize-winning physicist Alfred Kastler served as the organization's first chairman.
The group initially provided assistance to Afghanistan refugees in Pakistan, famine-stricken Ugandan communities, and Cambodian refugees in Thailand. It expanded to address additional humanitarian concerns in Africa, the Middle East, Southeast Asia, the Balkans and elsewhere during the 1980s and 1990s. Action Against Hunger's Scientific Committee pioneered the therapeutic milk formula (F100), now used by all major humanitarian aid organizations to treat acute malnutrition. As a result, the global mortality rate of severely malnourished children under the age of five has been reduced from 25% to 5%. A few years later, therapeutic milk was repackaged as ready-to-use therapeutic foods (RUTFs), a peanut-based paste packaged like a power bar. These bars allow for the treatment of malnutrition at home, and do not require any preparation or refrigeration.
Action Against Hunger – USA was established in 1997. The international network currently has headquarters in five countries – France, Spain, the United States, Canada, and the UK. 
Restaurants Against Hunger.
Action Against Hunger's Restaurants Against Hunger Campaign partners with leaders from the food and beverage industry to bring attention to global hunger. Each year, the campaign raises funds and support for Action Against Hunger's programs.

</doc>
<doc id="2452" url="https://en.wikipedia.org/wiki?curid=2452" title="AW">
AW

A&W, AW, Aw, aW or aw may refer to:

</doc>
<doc id="2457" url="https://en.wikipedia.org/wiki?curid=2457" title="Apoptosis">
Apoptosis

Apoptosis (from Ancient Greek ἀπόπτωσις "falling off") is a process of programmed cell death that occurs in multicellular organisms. Biochemical events lead to characteristic cell changes (morphology) and death. These changes include blebbing, cell shrinkage, nuclear fragmentation, chromatin condensation, chromosomal DNA fragmentation, and global mRNA decay. Between 50 and 70 billion cells die each day due to apoptosis in the average human adult. For an average child between the ages of 8 and 14, approximately 20 billion to 30 billion cells die a day.
In contrast to necrosis, which is a form of traumatic cell death that results from acute cellular injury, apoptosis is a highly regulated and controlled process that confers advantages during an organism's lifecycle. For example, the separation of fingers and toes in a developing human embryo occurs because cells between the digits undergo apoptosis. Unlike necrosis, apoptosis produces cell fragments called apoptotic bodies that phagocytic cells are able to engulf and quickly remove before the contents of the cell can spill out onto surrounding cells and cause damage.
Because apoptosis cannot stop once it has begun, it is a highly regulated process. Apoptosis can be initiated through one of two pathways. In the "intrinsic pathway" the cell kills itself because it senses cell stress, while in the "extrinsic pathway" the cell kills itself because of signals from other cells. Both pathways induce cell death by activating caspases, which are proteases, or enzymes that degrade proteins. The two pathways both activate initiator caspases, which then activate executioner caspases, which then kill the cell by degrading proteins indiscriminately.
Research on apoptosis has increased substantially since the early 1990s. In addition to its importance as a biological phenomenon, defective apoptotic processes have been implicated in a wide variety of diseases. Excessive apoptosis causes atrophy, whereas an insufficient amount results in uncontrolled cell proliferation, such as cancer.
Some factors like Fas receptors and caspases promote apoptosis, while some members of the Bcl-2 family of proteins inhibit apoptosis.
Discovery and etymology.
German scientist Karl Vogt was first to describe the principle of apoptosis in 1842. In 1885, anatomist Walther Flemming delivered a more precise description of the process of programmed cell death. However, it was not until 1965 that the topic was resurrected. While studying tissues using electron microscopy, John Foxton Ross Kerr at University of Queensland was able to distinguish apoptosis from traumatic cell death. Following the publication of a paper describing the phenomenon, Kerr was invited to join Alastair R Currie, as well as Andrew Wyllie, who was Currie's graduate student, at University of Aberdeen. In 1972, the trio published a seminal article in the British Journal of Cancer. Kerr had initially used the term programmed cell necrosis, but in the article, the process of natural cell death was called "apoptosis". Kerr, Wyllie and Currie credited James Cormack, a professor of Greek language at University of Aberdeen, with suggesting the term apoptosis. Kerr received the Paul Ehrlich and Ludwig Darmstaedter Prize on March 14, 2000, for his description of apoptosis. He shared the prize with Boston biologist H. Robert Horvitz.
For many years, the terms "apoptosis" and "programmed cell death" were not highly cited. What transformed cell death from obscurity to a major field of research were two things: the identification of components of the cell death control and effector mechanisms, and the linkage of abnormalities in cell death to human disease, in particular cancer.
The 2002 Nobel Prize in Medicine was awarded to Sydney Brenner, Horvitz and John E. Sulston for their work identifying genes that control apoptosis. The genes were identified by studies in the nematode "C. elegans" and these same genes function in humans for apoptosis.
In Greek, apoptosis translates to the "falling off" of leaves from a tree. Cormack, professor of Greek language, reintroduced the term for medical use as it had a medical meaning for the Greeks over two thousand years before. Hippocrates used the term to mean "the falling off of the bones". Galen extended its meaning to "the dropping of the scabs". Cormack was no doubt aware of this usage when he suggested the name. Debate continues over the correct pronunciation, with opinion divided between a pronunciation with the second "p" silent ( ) and the second "p" pronounced (), as in the original Greek. In English, the "p" of the Greek "-pt-" consonant cluster is typically silent at the beginning of a word (e.g. pterodactyl, Ptolemy), but articulated when used in combining forms preceded by a vowel, as in helicopter or the orders of insects: diptera, lepidoptera, etc.
In the original Kerr, Wyllie & Currie paper, there is a footnote regarding the pronunciation:
"We are most grateful to Professor James Cormack of the Department of Greek, University of Aberdeen, for suggesting this term. The word "apoptosis" () is used in Greek to describe the "dropping off" or "falling off" of petals from flowers, or leaves from trees. To show the derivation clearly, we propose that the stress should be on the penultimate syllable, the second half of the word being pronounced like "ptosis" (with the "p" silent), which comes from the same root "to fall", and is already used to describe the drooping of the upper eyelid."
Activation mechanisms.
The initiation of apoptosis is tightly regulated by activation mechanisms, because once apoptosis has begun, it inevitably leads to the death of the cell. The two best-understood activation mechanisms are of are the intrinsic pathway (also called the mitochondrial pathway) and the extrinsic pathway. The "intrinsic pathway" is activated by intracellular signals generated when cells are stressed and depends on the release of proteins from the intermembrane space of mitochondria. The "extrinsic pathway" is activated by extracellular ligands binding to cell-surface death receptors, which leads to the formation of the death-inducing signaling complex (DISC).
A cell initiates intracellular apoptotic signaling in response to a stress, which may bring about cell suicide. The binding of nuclear receptors by glucocorticoids, heat, radiation, nutrient deprivation, viral infection, hypoxia and increased intracellular calcium concentration,
for example, by damage to the membrane, can all trigger the release of intracellular apoptotic signals by a damaged cell. A number of cellular components, such as poly ADP ribose polymerase, may also help regulate apoptosis.
Before the actual process of cell death is precipitated by enzymes, apoptotic signals must cause regulatory proteins to initiate the apoptosis pathway. This step allows those signals to cause cell death, or the process to be stopped, should the cell no longer need to die. Several proteins are involved, but two main methods of regulation have been identified: the targeting of mitochondria functionality, or directly transducing the signal via adaptor proteins to the apoptotic mechanisms. An extrinsic pathway for initiation identified in several toxin studies is an increase in calcium concentration within a cell caused by drug activity, which also can cause apoptosis via a calcium binding protease calpain.
Intrinsic pathway.
The mitochondria are essential to multicellular life. Without them, a cell ceases to respire aerobically and quickly dies. This fact forms the basis for some apoptotic pathways. Apoptotic proteins that target mitochondria affect them in different ways. They may cause mitochondrial swelling through the formation of membrane pores, or they may increase the permeability of the mitochondrial membrane and cause apoptotic effectors to leak out. they are very closely related to intrinsic pathway, and tumors arise more frequently through intrinsic pathway than the extrinsic pathway because of sensitivity. There is also a growing body of evidence indicating that nitric oxide is able to induce apoptosis by helping to dissipate the membrane potential of mitochondria and therefore make it more permeable. Nitric oxide has been implicated in initiating and inhibiting apoptosis through its possible action as a signal molecule of subsequent pathways that activate apoptosis.
Mitochondrial proteins known as SMACs (second mitochondria-derived activator of caspases) are released into the cell's cytosol following the increase in permeability of the mitochondia membranes. SMAC binds to "proteins that inhibit apoptosis" (IAPs) thereby deactivating them, and preventing the IAPs from arresting the process and therefore allowing apoptosis to proceed. IAP also normally suppresses the activity of a group of cysteine proteases called caspases, which carry out the degradation of the cell, therefore the actual degradation enzymes can be seen to be indirectly regulated by mitochondrial permeability.
Cytochrome c is also released from mitochondria due to formation of a channel, the mitochondrial apoptosis-induced channel (MAC), in the outer mitochondrial membrane, and serves a regulatory function as it precedes morphological change associated with apoptosis. Once cytochrome c is released it binds with Apoptotic protease activating factor – 1 ("Apaf-1") and ATP, which then bind to "pro-caspase-9" to create a protein complex known as an apoptosome. The apoptosome cleaves the pro-caspase to its active form of caspase-9, which in turn activates the effector "caspase-3".
MAC (not to be confused with the Membrane Attack Complex formed by complement activation, also commonly denoted as MAC), also called "Mitochondrial Outer Membrane Permeabilization Pore" is regulated by various proteins, such as those encoded by the mammalian Bcl-2 family of anti-apoptopic genes, the homologs of the "ced-9" gene found in "C. elegans". "Bcl-2" proteins are able to promote or inhibit apoptosis by direct action on MAC/MOMPP. Bax and/or Bak form the pore, while Bcl-2, Bcl-xL or Mcl-1 inhibit its formation.
Extrinsic pathway.
Two theories of the direct initiation of apoptotic mechanisms in mammals have been suggested: the "TNF-induced" (tumour necrosis factor) model and the "Fas-Fas ligand-mediated" model, both involving receptors of the "TNF receptor" (TNFR) family coupled to extrinsic signals.
TNF path
TNF-alpha is a cytokine produced mainly by activated macrophages, and is the major extrinsic mediator of apoptosis. Most cells in the human body have two receptors for TNF-alpha: TNFR1 and TNFR2. The binding of TNF-alpha to TNFR1 has been shown to initiate the pathway that leads to caspase activation via the intermediate membrane proteins TNF receptor-associated death domain (TRADD) and Fas-associated death domain protein (FADD). cIAP1/2 can inhibit TNF-α signaling by binding to TRAF2. FLIP inhibits the activation of caspase-8. Binding of this receptor can also indirectly lead to the activation of transcription factors involved in cell survival and inflammatory responses. However, signalling through TNFR1 might also induce apoptosis in a caspase-independent manner. The link between TNF-alpha and apoptosis shows why an abnormal production of TNF-alpha plays a fundamental role in several human diseases, especially in autoimmune diseases.
Fas path
The fas receptor First apoptosis signal (fas) – (also known as "Apo-1" or "CD95") binds the Fas ligand (FasL), a transmembrane protein part of the TNF family. The interaction between Fas and FasL results in the formation of the "death-inducing signaling complex" (DISC), which contains the FADD, caspase-8 and caspase-10. In some types of cells (type I), processed caspase-8 directly activates other members of the caspase family, and triggers the execution of apoptosis of the cell. In other types of cells (type II), the "Fas"-DISC starts a feedback loop that spirals into increasing release of proapoptotic factors from mitochondria and the amplified activation of caspase-8.
Common components
Following "TNF-R1" and "Fas" activation in mammalian cells a balance between proapoptotic (BAX, BID, BAK, or BAD) and anti-apoptotic ("Bcl-Xl" and "Bcl-2") members of the "Bcl-2" family is established. This balance is the proportion of proapoptotic homodimers that form in the outer-membrane of the mitochondrion. The proapoptotic homodimers are required to make the mitochondrial membrane permeable for the release of caspase activators such as cytochrome c and SMAC. Control of proapoptotic proteins under normal cell conditions of nonapoptotic cells is incompletely understood, but in general, Bax or Bak are activated by the activation of BH3-only proteins, part of the Bcl-2 family.
Caspases
Caspases play the central role in the transduction of ER apoptotic signals. Caspases are proteins that are highly conserved, cysteine-dependent aspartate-specific proteases. There are two types of caspases: initiator caspases, caspase 2,8,9,10,11,12, and effector caspases, caspase 3,6,7. The activation of initiator caspases requires binding to specific oligomeric activator protein. Effector caspases are then activated by these active initiator caspases through proteolytic cleavage. The active effector caspases then proteolytically degrade a host of intracellular proteins to carry out the cell death program.
Caspase-independent apoptotic pathway
There also exists a caspase-independent apoptotic pathway that is mediated by AIF (apoptosis-inducing factor).
Apoptosis Model in Amphibians.
 Amphibian frog "Xenopus laevis" serves as an ideal model system for the study of the mechanisms of apoptosis. In fact, iodine and thyroxine also stimulate the spectacular apoptosis of the cells of the larval gills, tail and fins in amphibians metamorphosis, and stimulate the evolution of their nervous system transforming the aquatic, vegetarian tadpole into the terrestrial, carnivorous frog.
Proteolytic caspase cascade: Killing the cell.
Many pathways and signals lead to apoptosis, but there is only one mechanism that actually causes the death of a cell. After a cell receives stimulus, it undergoes organized degradation of cellular organelles by activated proteolytic caspases. In addition to the destruction of cellular organelles, mRNA is rapidly and globally degraded by a mechanism that is not yet fully characterized. mRNA decay is triggered very early in apoptosis.
A cell undergoing apoptosis shows a characteristic morphology:
Apoptosis progresses quickly and its products are quickly removed, making it difficult to detect or visualize. During karyorrhexis, endonuclease activation leaves short DNA fragments, regularly spaced in size. These give a characteristic "laddered" appearance on agar gel after electrophoresis. Tests for DNA laddering differentiate apoptosis from ischemic or toxic cell death.
Removal of dead cells.
The removal of dead cells by neighboring phagocytic cells has been termed efferocytosis.
Dying cells that undergo the final stages of apoptosis display phagocytotic molecules, such as phosphatidylserine, on their cell surface. Phosphatidylserine is normally found on the inner leaflet surface of the plasma membrane, but is redistributed during apoptosis to the extracellular surface by a protein known as scramblase. These molecules mark the cell for phagocytosis by cells possessing the appropriate receptors, such as macrophages. Upon recognition, the phagocyte reorganizes its cytoskeleton for engulfment of the cell. The removal of dying cells by phagocytes occurs in an orderly manner without eliciting an inflammatory response.
Pathway knock-outs.
Many knock-outs have been made in the apoptosis pathways to test the function of each of the proteins. Several caspases, in addition to APAF-1 and FADD, have been mutated to determine the new phenotype. In order to create a tumor necrosis factor (TNF) knockout, an exon containing the nucleotides 3704-5364 was removed from the gene. This exon encodes a portion of the mature TNF domain, as well as the leader sequence, which is a highly conserved region necessary for proper intracellular processing. TNF-/- mice develop normally and have no gross structural or morphological abnormalities. However, upon immunization with SRBC (sheep red blood cells), these mice demonstrated a deficiency in the maturation of an antibody response; they were able to generate normal levels of IgM, but could not develop specific IgG levels. Apaf-1 is the protein that turns on caspase 9 by cleavage to begin the caspase cascade that leads to apoptosis. Since a -/- mutation in the APAF-1 gene is embryonic lethal, a gene trap strategy was used in order to generate an APAF-1 -/- mouse. This assay is used to disrupt gene function by creating an intragenic gene fusion. When an APAF-1 gene trap is introduced into cells, many morphological changes occur, such as spina bifida, the persistence of interdigital webs, and open brain. In addition, after embryonic day 12.5, the brain of the embryos showed several structural changes. APAF-1 cells are protected from apoptosis stimuli such as irradiation. A BAX-1 knock-out mouse exhibits normal forebrain formation and a decreased programmed cell death in some neuronal populations and in the spinal cord, leading to an increase in motor neurons.
The caspase proteins are integral parts of the apoptosis pathway, so it follows that knock-outs made have varying damaging results. A caspase 9 knock-out leads to a severe brain malformation. A caspase 8 knock-out leads to cardiac failure and thus embryonic lethality. However, with the use of cre-lox technology, a caspase 8 knock-out has been created that exhibits an increase in peripheral T cells, an impaired T cell response, and a defect in neural tube closure. These mice were found to be resistant to apoptosis mediated by CD95, TNFR, etc. but not resistant to apoptosis caused by UV irradiation, chemotherapeutic drugs, and other stimuli. Finally, a caspase 3 knock-out was characterized by ectopic cell masses in the brain and abnormal apoptotic features such as membrane blebbing or nuclear fragmentation. A remarkable feature of these KO mice is that they have a very restricted phenotype: Casp3, 9, APAF-1 KO mice have deformations of neural tissue and FADD and Casp 8 KO showed defective heart development, however in both types of KO other organs developed normally and some cell types were still sensitive to apoptotic stimuli suggesting that unknown proapoptotic pathways exist.
Methods for distinguishing apoptotic from necrotic (necroptotic) cells.
In order to perform analysis of apoptotic versus necrotic (necroptotic) cells, one can do analysis of morphology by time-lapse microscopy, flow fluorocytometry, and transmission electron microscopy. There are also various biochemical techniques for analysis of cell surface markers (phosphatidylserine exposure versus cell permeability by flow fluorocytometry), cellular markers such as DNA fragmentation (flow fluorocytometry), caspase activation, Bid cleavage, and cytochrome c release (Western blotting). It is important to know how primary and secondary necrotic cells can be distinguished by analysis of supernatant for caspases, HMGB1, and release of cytokeratin 18. However, no distinct surface or biochemical markers of necrotic cell death have been identified yet, and only negative markers are available. These include absence of apoptotic parameters (caspase activation, cytochrome c release, and oligonucleosomal DNA fragmentation) and differential kinetics of cell death markers (phosphatidylserine exposure and cell membrane permeabilization). A selection of techniques that can be used to distinguish apoptosis from necroptotic cells could be found in these references.
Implication in disease.
Defective pathways.
The many different types of apoptotic pathways contain a multitude of different biochemical components, many of them not yet understood. As a pathway is more or less sequential in nature, it is a victim of causality; removing or modifying one component leads to an effect in another. In a living organism, this can have disastrous effects, often in the form of disease or disorder. A discussion of every disease caused by modification of the various apoptotic pathways would be impractical, but the concept overlying each one is the same: The normal functioning of the pathway has been disrupted in such a way as to impair the ability of the cell to undergo normal apoptosis. This results in a cell that lives past its "use-by-date" and is able to replicate and pass on any faulty machinery to its progeny, increasing the likelihood of the cell's becoming cancerous or diseased.
A recently described example of this concept in action can be seen in the development of a lung cancer called NCI-H460. The "X-linked inhibitor of apoptosis protein" (XIAP) is overexpressed in cells of the H460 cell line. XIAPs bind to the processed form of caspase-9, and suppress the activity of apoptotic activator cytochrome c, therefore overexpression leads to a decrease in the amount of proapoptotic agonists. As a consequence, the balance of anti-apoptotic and proapoptotic effectors is upset in favour of the former, and the damaged cells continue to replicate despite being directed to die.
Dysregulation of p53.
The tumor-suppressor protein p53 accumulates when DNA is damaged due to a chain of biochemical factors. Part of this pathway includes alpha-interferon and beta-interferon, which induce transcription of the "p53" gene, resulting in the increase of p53 protein level and enhancement of cancer cell-apoptosis. p53 prevents the cell from replicating by stopping the cell cycle at G1, or interphase, to give the cell time to repair, however it will induce apoptosis if damage is extensive and repair efforts fail. Any disruption to the regulation of the "p53" or interferon genes will result in impaired apoptosis and the possible formation of tumors.
Inhibition.
Inhibition of apoptosis can result in a number of cancers, autoimmune diseases, inflammatory diseases, and viral infections. It was originally believed that the associated accumulation of cells was due to an increase in cellular proliferation, but it is now known that it is also due to a decrease in cell death. The most common of these diseases is cancer, the disease of excessive cellular proliferation, which is often characterized by an overexpression of IAP family members. As a result, the malignant cells experience an abnormal response to apoptosis induction: Cycle-regulating genes (such as p53, ras or c-myc) are mutated or inactivated in diseased cells, and further genes (such as bcl-2) also modify their expression in tumors.
HeLa cell.
Apoptosis in HeLa cells is inhibited by proteins produced by the cell; these inhibitory proteins target retinoblastoma tumor-suppressing proteins. These tumor-suppressing proteins regulate the cell cycle, but are rendered inactive when bound to an inhibitory protein. HPV E6 and E7 are inhibitory proteins expressed by the human papillomavirus, HPV being responsible for the formation of the cervical tumor from which HeLa cells are derived. HPV E6 causes p53, which regulates the cell cycle, to become inactive. HPV E7 binds to retinoblastoma tumor suppressing proteins and limits its ability to control cell division. These two inhibitory proteins are partially responsible for HeLa cells' immortality by inhibiting apoptosis to occur. CDV (Canine Distemper Virus) is able to induce apoptosis despite the presence of these inhibitory proteins. This is an important oncolytic property of CDV: this virus is capable of killing canine lymphoma cells. Oncoproteins E6 and E7 still leave p53 inactive, but they are not able to avoid the activation of caspases induced from the stress of viral infection. These oncolytic properties provided a promising link between CDV and lymphoma apoptosis, which can lead to development of alternative treatment methods for both canine lymphoma and human non-Hodgkin lymphoma. Defects in the cell cycle are thought to be responsible for the resistance to chemotherapy or radiation by certain tumor cells, so a virus that can induce apoptosis despite defects in the cell cycle is useful for cancer treatment.
Treatments.
The main method of treatment for death signaling-related diseases involves either increasing or decreasing the susceptibility of apoptosis in diseased cells, depending on whether the disease is caused by either the inhibition of or excess apoptosis. For instance, treatments aim to restore apoptosis to treat diseases with deficient cell death, and to increase the apoptotic threshold to treat diseases involved with excessive cell death. To stimulate apoptosis, one can increase the number of death receptor ligands (such as TNF or TRAIL), antagonize the anti-apoptotic Bcl-2 pathway, or introduce Smac mimetics to inhibit the inhibitor (IAPs). The addition of agents such as Herceptin, Iressa, or Gleevec works to stop cells from cycling and causes apoptosis activation by blocking growth and survival signaling further upstream. Finally, adding p53-MDM2 complexes displaces p53 and activates the p53 pathway, leading to cell cycle arrest and apoptosis. Many different methods can be used either to stimulate or to inhibit apoptosis in various places along the death signaling pathway.
Apoptosis is a multi-step, multi-pathway cell-death programme that is inherent in every cell of the body. In cancer, the apoptosis cell-division ratio is altered. Cancer treatment by chemotherapy and irradiation kills target cells primarily by inducing apoptosis.
Hyperactive apoptosis.
On the other hand, loss of control of cell death (resulting in excess apoptosis) can lead to neurodegenerative diseases, hematologic diseases, and tissue damage. The progression of HIV is directly linked to excess, unregulated apoptosis. In a healthy individual, the number of CD4+ lymphocytes is in balance with the cells generated by the bone marrow; however, in HIV-positive patients, this balance is lost due to an inability of the bone marrow to regenerate CD4+ cells. In the case of HIV, CD4+ lymphocytes die at an accelerated rate through uncontrolled apoptosis, when stimulated.
Treatments.
Treatments aiming to inhibit works to block specific caspases. Finally, the Akt protein kinase promotes cell survival through two pathways. Akt phosphorylates and inhibits Bas (a Bcl-2 family member), causing Bas to interact with the 14-3-3 scaffold, resulting in Bcl dissociation and thus cell survival. Akt also activates IKKα, which leads to NF-κB activation and cell survival. Active NF-κB induces the expression of anti-apoptotic genes such as Bcl-2, resulting in inhibition of apoptosis. NF-κB has been found to play both an antiapoptotic role and a proapoptotic role depending on the stimuli utilized and the cell type.
HIV progression.
The progression of the human immunodeficiency virus infection into AIDS is due primarily to the depletion of CD4+ T-helper lymphocytes in a manner that is too rapid for the body's bone marrow to replenish the cells, leading to a compromised immune system. One of the mechanisms by which T-helper cells are depleted is apoptosis, which results from a series of biochemical pathways:
Cells may also die as direct consequences of viral infections. HIV-1 expression induces tubular cell G2/M arrest and apoptosis. The progression from HIV to AIDS is not immediate or even necessarily rapid; HIV's cytotoxic activity toward CD4+ lymphocytes is classified as AIDS once a given patient's CD4+ cell count falls below 200.
Viral infection.
Viral induction of apoptosis occurs when one or several cells of a living organism are infected with a virus, leading to cell death. Cell death in organisms is necessary for the normal development of cells and the cell cycle maturation. It is also important in maintaining the regular functions and activities of cells.
Viruses can trigger apoptosis of infected cells via a range of mechanisms including:
Canine distemper virus (CDV) is known to cause apoptosis in central nervous system and lymphoid tissue of infected dogs in vivo and in vitro.
Apoptosis caused by CDV is typically induced via the extrinsic pathway, which activates caspases that disrupt cellular function and eventually leads to the cells death. In normal cells, CDV activates caspase-8 first, which works as the initiator protein followed by the executioner protein caspase-3. However, apoptosis induced by CDV in HeLa cells does not involve the initiator protein caspase-8. HeLa cell apoptosis caused by CDV follows a different mechanism than that in vero cell lines. This change in the caspase cascade suggests CDV induces apoptosis via the intrinsic pathway, excluding the need for the initiator caspase-8. The executioner protein is instead activated by the internal stimuli caused by viral infection not a caspase cascade.
The Oropouche virus (OROV) is found in the family "Bunyaviridae". The study of apoptosis brought on by "Bunyaviridae" was initiated in 1996, when it was observed that apoptosis was induced by the La Crosse virus into the kidney cells of baby hamsters and into the brains of baby mice.
OROV is a disease that is transmitted between humans by the biting midge ("Culicoides paraensis"). It is referred to as a zoonotic arbovirus and causes febrile illness, characterized by the onset of a sudden fever known as Oropouche fever.
The Oropouche virus also causes disruption in cultured cells – cells that are cultivated in distinct and specific conditions. An example of this can be seen in HeLa cells, whereby the cells begin to degenerate shortly after they are infected.
With the use of gel electrophoresis, it can be observed that OROV causes DNA fragmentation in HeLa cells. It can be interpreted by counting, measuring, and analyzing the cells of the Sub/G1 cell population. When HeLA cells are infected with OROV, the cytochrome C is released from the membrane of the mitochondria, into the cytosol of the cells. This type of interaction shows that apoptosis is activated via an intrinsic pathway.
In order for apoptosis to occur within OROV, viral uncoating, viral internalization, along with the replication of cells is necessary. Apoptosis in some viruses is activated by extracellular stimuli. However, studies have demonstrated that the OROV infection causes apoptosis to be activated through intracellular stimuli and involves the mitochondria.
Many viruses encode proteins that can inhibit apoptosis. Several viruses encode viral homologs of Bcl-2. These homologs can inhibit proapoptotic proteins such as BAX and BAK, which are essential for the activation of apoptosis. Examples of viral Bcl-2 proteins include the Epstein-Barr virus BHRF1 protein and the adenovirus E1B 19K protein. Some viruses express caspase inhibitors that inhibit caspase activity and an example is the CrmA protein of cowpox viruses. Whilst a number of viruses can block the effects of TNF and Fas. For example, the M-T2 protein of myxoma viruses can bind TNF preventing it from binding the TNF receptor and inducing a response. Furthermore, many viruses express p53 inhibitors that can bind p53 and inhibit its transcriptional transactivation activity. As a consequence, p53 cannot induce apoptosis, since it cannot induce the expression of proapoptotic proteins. The adenovirus E1B-55K protein and the hepatitis B virus HBx protein are examples of viral proteins that can perform such a function.
Viruses can remain intact from apoptosis in particular in the latter stages of infection. They can be exported in the "apoptotic bodies" that pinch off from the surface of the dying cell, and the fact that they are engulfed by phagocytes prevents the initiation of a host response. This favours the spread of the virus.
Plants.
Programmed cell death in plants has a number of molecular similarities to that of animal apoptosis, but it also has differences, notable ones being the presence of a cell wall and the lack of an immune system that removes the pieces of the dead cell. Instead of an immune response, the dying cell synthesizes substances to break itself down and places them in a vacuole that ruptures as the cell dies. Whether this whole process resembles animal apoptosis closely enough to warrant using the name "apoptosis" (as opposed to the more general "programmed cell death") is unclear.
Caspase-independent apoptosis.
The characterization of the caspases allowed the development of caspase inhibitors, which can be used to determine whether a cellular process involves active caspases. Using these inhibitors it was discovered that cells can die while displaying a morphology similar to apoptosis without caspase activation. Later studies linked this phenomenon to the release of AIF (apoptosis-inducing factor) from the mitochondria and its translocation into the nucleus mediated by its NLS (nuclear localization signal). Inside the mitochondria, AIF is anchored to the inner membrane. In order to be released, the protein is cleaved by a calcium-dependent calpain protease.
Apoptosis protein subcellular location prediction.
In 2003, a method was developed for predicting subcellular location of apoptosis proteins.
Subsequent to this, various modes of Chou's pseudo amino acid composition were developed for improving the quality of predicting subcellular localization of apoptosis proteins based on their sequence information alone.

</doc>
<doc id="2459" url="https://en.wikipedia.org/wiki?curid=2459" title="Appomattox">
Appomattox

Appomattox may refer to:

</doc>
<doc id="2460" url="https://en.wikipedia.org/wiki?curid=2460" title="Anal sex">
Anal sex

Anal sex or anal intercourse is generally the insertion and thrusting of the erect penis into a person's anus, or anus and rectum, for sexual pleasure. Other forms of anal sex include fingering, the use of sex toys for anal penetration, oral sex performed on the anus (anilingus), and pegging. Though the term "anal sex" most commonly means penile-anal penetration, sources sometimes use the term "anal intercourse" to refer exclusively to penile-anal penetration, and "anal sex" to refer to any form of anal sexual activity, especially between pairings as opposed to anal masturbation.
While anal sex is commonly associated with male homosexuality, research shows that not all gay males engage in anal sex and that it is not uncommon in heterosexual relationships. Types of anal sex can also be a part of lesbian sexual practices. People may experience pleasure from anal sex by stimulation of the anal nerve endings, and orgasm may be achieved through anal penetration – by indirect stimulation of the prostate in men, indirect stimulation of the clitoris or an area of the vagina associated with the G-spot in women, and other sensory nerves (especially the pudendal nerve). However, people may also find anal sex painful, sometimes extremely so, which may be primarily due to psychological factors in some cases.
As with most forms of sexual activity, anal sex participants risk contracting sexually transmitted infections (STIs/STDs). Anal sex is considered a high-risk sexual practice because of the vulnerability of the anus and rectum. The anal and rectal tissues are delicate and do not provide natural lubrication, so they can easily tear and permit disease transmission, especially if lubricant is not used. Anal sex without protection of a condom is considered the riskiest form of sexual activity, and therefore health authorities such as the World Health Organization (WHO) recommend safe sex practices for anal sex.
Strong views are often expressed about anal sex. It is controversial in various cultures, especially with regard to religious prohibitions. This is commonly due to prohibitions against anal sex among males or teachings about the procreative purpose of sexual activity. It may be regarded as taboo or unnatural, and is a criminal offense in some countries, punishable by corporal or capital punishment; by contrast, people also regard anal sex as a natural and valid form of sexual activity that may be as fulfilling as other desired sexual expressions. They may regard it as an enhancing element of their sex lives or as their primary form of sexual activity.
Anatomy and stimulation.
The abundance of nerve endings in the anal region and rectum can make anal sex pleasurable for men or women. The internal and external sphincter muscles control the opening and closing of the anus; these muscles, which are sensitive membranes made up of many nerve endings, facilitate pleasure or pain during anal sex. The "Human Sexuality: An Encyclopedia" states that "the inner third of the anal canal is less sensitive to touch than the outer two-thirds, but is more sensitive to pressure" and that "the rectum is a curved tube about eight or nine inches long and has the capacity, like the anus, to expand".
Research indicates that anal sex occurs significantly less frequently than other sexual behaviors, but its association with dominance and submission, as well as taboo, makes it an appealing stimulus to people of all sexual orientations. In addition to sexual penetration by the penis, people may use sex toys such as butt plugs or anal beads, engage in fingering, anilingus, pegging, anal masturbation or fisting for anal sexual activity, and different sex positions may also be included. Fisting is the least practiced of the activities, partly because it is uncommon that people can relax enough to accommodate an object as big as a fist being inserted into the anus.
In a male receptive partner, being anally penetrated can produce a pleasurable sensation due to the inserted penis rubbing or brushing against the prostate (also known as the "male G-spot") through the anal wall. This can result in pleasurable sensations and can lead to an orgasm in some cases. Prostate stimulation can produce a "deeper" orgasm, sometimes described by men as more widespread and intense, longer-lasting, and allowing for greater feelings of ecstasy than orgasm elicited by penile stimulation only. The prostate is located next to the rectum and is the larger, more developed male homologue (variation) to the female Skene's glands. However, though the experiences are different, male orgasms by penile stimulation are also centered in the prostate gland. It is also common, and likely most typical, that men will not reach orgasm as receptive partners solely from anal sex.
General statistics indicate that 70–80% of women require direct clitoral stimulation to achieve orgasm. The clitoris is composed of more than the externally visible glans (head). With its glans or body as a whole estimated to have around 8,000 sensory nerve endings, the clitoris surrounds the vagina and urethra, and may have a similar connection with the anus. The vagina is flanked on each side by the clitoral crura, the internal "legs" of the clitoris, which are highly sensitive and become engorged with blood when sexually aroused. In addition to nerve endings present within the anus and rectum, women may find anal stimulation pleasurable due to indirect stimulation of these "legs". Indirect stimulation of the clitoris through anal penetration may also be caused by the shared sensory nerves, especially the pudendal nerve, which gives off the inferior anal nerves and divides into two terminal branches: the perineal nerve and the dorsal nerve of the clitoris.
The Gräfenberg spot, or G-spot, is a debated area of female anatomy, particularly among doctors and researchers, but it is typically described as being located behind the female pubic bone surrounding the urethra and accessible through the anterior wall of the vagina; it and other areas of the vagina are considered to have tissue and nerves that are related to the clitoris. Besides the shared anatomy of the aforementioned sensory nerves, orgasm by indirect stimulation of the clitoris or G-spot area through anal penetration is possible because of the close proximity between the vaginal cavity and the rectal cavity. Achieving orgasm solely by anal stimulation is rare among women. Direct stimulation of the clitoris, a G-spot area, or both, during anal sex can help some women enjoy the activity and reach orgasm from it.
Stimulation from anal sex can additionally be affected by popular perception or portrayals of the activity, such as erotica or pornography. In pornography, anal sex is commonly portrayed as a desirable, painless routine that does not require personal lubricant; this can result in couples performing anal sex without care, and men and women believing that it is unusual for women, as receptive partners, to find discomfort or pain instead of pleasure from the activity. By contrast, each person's sphincter muscles react to penetration differently, the anal sphincters have tissues that are more prone to tearing, and the anus and rectum, unlike the vagina, do not provide natural lubrication for sexual penetration. Researchers say adequate application of a personal lubricant, relaxation, and communication between sexual partners are crucial to avoid pain or damage to the anus or rectum. Additionally, ensuring that the anal area is clean and the bowel is empty, for both aesthetics and practicality, may be desired by participants.
Male to female.
Behaviors and views.
The anal sphincters are usually tighter than the pelvic muscles of the vagina, which can enhance the sexual pleasure for the inserting male during male-to-female anal intercourse because of the pressure applied to the penis. Men may also enjoy the penetrative role during anal sex because of its association with dominance, because it is made more alluring by a female partner or society in general insisting that it is forbidden, or because it presents an additional option for penetration.
While some women find being a receptive partner during anal intercourse painful or uncomfortable, or only engage in the act to please a male sexual partner, other women find the activity pleasurable or prefer it to vaginal intercourse. The vaginal walls contain significantly fewer nerve endings than the clitoris and anus, and therefore intense sexual pleasure, including orgasm, from vaginal sexual stimulation is less likely to occur than from direct clitoral stimulation in the majority of women. However, anal sexual stimulation is not necessarily more likely to result in orgasm than vaginal sexual stimulation; the types of nerves and how they interact with each other are factors, as the belief that there is complete separation between the vagina and clitoris is a misconception aided by misunderstandings of what and how big the clitoris actually is.
In a 2010 clinical review article of heterosexual anal sex, the term "anal intercourse" is used to refer specifically to penile-anal penetration, and "anal sex" is used to refer to any form of anal sexual activity. The review suggests that anal sex is exotic among the sexual practices of some heterosexuals and that "for a certain number of heterosexuals, anal intercourse is pleasurable, exciting, and perhaps considered more intimate than vaginal sex".
Anal intercourse is sometimes used as a substitute for vaginal intercourse during menstruation. The likelihood of pregnancy occurring during anal sex is greatly reduced, as anal sex alone cannot lead to pregnancy unless sperm is somehow transported to the vaginal opening. Because of this, some couples practice anal intercourse as a form of contraception, often in the absence of a condom.
Male-to-female anal sex is commonly viewed as a way of preserving female virginity because it is non-procreative and does not tear the hymen; a person, especially a teenage girl or woman, who engages in anal sex or other sexual activity with no history of having engaged in vaginal intercourse is often regarded among heterosexuals and researchers as not having yet experienced virginity loss. This is sometimes termed "technical virginity." Heterosexuals may view anal sex as "fooling around" or as foreplay, a view that "dates to the late 1600s, with explicit 'rules' appearing around the turn of the twentieth century, as in marriage manuals defining petting as 'literally every caress known to married couples but does not include complete sexual intercourse'".
Prevalence.
In 1992, a study conducted by the U.S. Centers for Disease Control and Prevention (CDC) found that 26% of men 18 to 59 and 20% of women 18 to 59 had engaged in heterosexual anal sex; a similar 2005 survey (also conducted by the U.S. Centers for Disease Control and Prevention) found a rising incidence of anal sex relations in the American heterosexual population. The survey showed that 40% of men and 35% of women between 25 and 44 had engaged in heterosexual anal sex. In terms of overall numbers of survey respondents, seven times as many women as gay men said that they engaged in anal intercourse, with this figure reflecting the larger heterosexual population size.
In a 2007 report regarding the prevalence and correlates of heterosexual anal and oral sex among adolescents and adults in the United States, a National Survey of Family Growth (NSFG) found that 34% men and 30% women reported ever participating in heterosexual anal sex. The percentage of participants reporting heterosexual anal sex was significantly higher among 20- to 24-year-olds and peaked among 30- to 34-year-olds. A 2008 survey focused on a younger demographic of teenagers and young adults, aged 15–21. It found that 16% of 1350 surveyed had had this type of sex in the previous 3 months, with condoms being used 29% of the time. However, given the subject matter, the survey hypothesized the prevalence was probably underestimated.
In Kimberly R. McBride's 2010 clinical review on heterosexual anal intercourse and other forms of anal sexual activity, it is suggested that changing norms may affect the frequency of heterosexual anal sex. McBride and her colleagues investigated the prevalence of non-intercourse anal sex behaviors among a sample of men (n=1,299) and women (n=1,919) compared to anal intercourse experience and found that 51% of men and 43% of women had participated in at least one act of oral–anal sex, manual–anal sex, or anal sex toy use. The report states the majority of men (n=631) and women (n=856) who reported heterosexual anal intercourse in the past 12 months were in exclusive, monogamous relationships: 69% and 73%, respectively. The review added that "most research on anal intercourse addresses men who have sex with men (MSM), with relatively little attention given to anal intercourse and other anal sexual behaviors between heterosexual partners" and "esearch is quite rare that specifically differentiates the anus as a sexual organ or addresses anal sexual function or dysfunction as legitimate topics. As a result, we do not know the extent to which anal intercourse differs qualitatively from coitus."
According to a 2010 study from the National Survey of Sexual Health and Behavior (NSSHB) that was authored by Debby Herbenick and other researchers, although anal intercourse is reported by fewer women than other partnered sex behaviors, partnered women in the age groups between 18–49 are significantly more likely to report having anal sex in the past 90 days. As of 2011, this survey provides the most up to date data about anal sex at the population level.
Figures for prevalence can vary among different demographics, regions and nationalities. A 1999 South Korean survey of 586 women documented that 3.5% of the respondents reported having had anal sex. By contrast, a 2001 French survey of five hundred female respondents concluded that a total of 29% had engaged in this practice, with one third of these confirming to have enjoyed the experience.
Figures for the prevalence of sexual behavior can also fluctuate over time. Edward O. Laumann's 1992 survey, reported in "The Social Organization of Sexuality: Sexual Practices in the United States", found that about 20% of heterosexuals had engaged in male-to-female anal sex. Sex researcher Alfred Kinsey, working in the 1940s, had found that number to be closer to 40% at the time. A researcher from the University of British Columbia in 2005 put the number of heterosexuals who have engaged in this practice at between 30% and 50%. According to Columbia University's health website Go Ask Alice!: "Studies indicate that about 25 percent of heterosexual couples have had anal sex at least once, and 10 percent regularly have anal penetration." The increase of anal sexual activity among heterosexuals has also been linked to the increase in anal pornography, especially if a person views it more regularly than a person who does not.
Male to male.
Behaviors and views.
Historically, anal sex has been commonly associated with male homosexuality. However, many gay men and men who have sex with men in general (those who identify as gay, bisexual, heterosexual or have not identified their sexual identity) do not engage in anal sex. Among men who have anal sex with other men, the insertive partner may be referred to as the "top" and the one being penetrated may be referred to as the "bottom". Those who enjoy either role may be referred to as "versatile".
Gay men who prefer anal sex may view it as their version of intercourse and a natural expression of intimacy that is capable of providing pleasure. The notion that it might resonate with gay men with the same emotional significance that vaginal sex resonates with heterosexuals has also been considered. Some men who have sex with men, however, believe that being a receptive partner during anal sex questions their masculinity.
Men who have sex with men may also prefer to engage in frot or other forms of mutual masturbation because they find it more pleasurable or more affectionate, to preserve technical virginity, or as safe sex alternatives to anal sex, while other frot advocates denounce anal sex as degrading to the receptive partner and unnecessarily risky.
Prevalence.
Reports with regard to the prevalence of anal sex among gay men in the west have varied over time. Magnus Hirschfeld, in his 1914 work "The Homosexuality of Men and Women", reported the rate of anal sex among gay men surveyed to be 8%, the least favored of all the practices documented. By the 1950s in the United Kingdom, it was thought that about 15% of gay males had anal sex.
Similar to the Hirschfeld study, scholars state that oral sex and mutual masturbation are more common than anal stimulation among gay men in long-term relationships. They say that anal intercourse is generally more popular among gay male couples than among heterosexual couples, but that "it ranks behind oral sex and mutual masturbation" among both sexual orientations in prevalence. Wellings et al. reported that "the equation of 'homosexual' with 'anal' sex among men is common among lay and health professionals alike" and that "yet an Internet survey of 18,000 MSM across Europe (EMIS, 2011) showed that oral sex was most commonly practised, followed by mutual masturbation, with anal intercourse in third place". A 2011 survey by "The Journal of Sexual Medicine" found similar results for U.S. gay and bisexual men.
Various older studies on male-to-male anal sex differ significantly. The 1994 Laumann study suggests that 80% of gay men practice anal sex and 20% never engage in it at all. A survey in "The Advocate" in 1994 indicated that 46% of gay men preferred to penetrate their partners, while 43% preferred to be the receptive partner. A survey conducted from 1994 to 1997 in San Francisco by the Stop AIDS Project indicated that over the course of the study, among men who have sex with men instead of solely gay men, the proportion engaging in anal sex increased from 57.6% to 61.2%. The National Institutes of Health (NIH), with their report published in the "BMJ" in 1999, stated that two thirds of gay men have anal sex. Other sources suggest that roughly three-fourths of gay men have had anal sex at one time or another in their lives, with an equal percentage participating as tops and bottoms.
Female to male.
Women may sexually stimulate a man's anus by fingering the exterior or interior areas of the anus; they may also stimulate the perineum (which, for males, is between the base of the scrotum and the anus), massage the prostate or engage in anilingus. Sex toys, such as a dildo, may also be used. The practice of a woman penetrating a man's anus with a strap-on dildo for sexual activity is called pegging.
Commonly, heterosexual men reject the idea of being receptive partners during anal sex because they believe it is a feminine act, can make them vulnerable, or contradicts their sexual orientation (for example, that it is indicative that they are gay). National Institutes of Health (NIH) information published in the "BMJ" in 1999, however, states:There are little published data on how many heterosexual men would like their anus to be sexually stimulated in a heterosexual relationship. Anecdotally, it is a substantial number. What data we do have almost all relate to penetrative sexual acts, and the superficial contact of the anal ring with fingers or the tongue is even less well documented but may be assumed to be a common sexual activity for men of all sexual orientations.
Reece et al. reported in 2010 that receptive anal intercourse is infrequent among men overall, stating that "an estimated 7% of men 14 to 94 years old reported being a receptive partner during anal intercourse".
Female to female.
With regard to lesbian sexual practices, anal sex includes fingering, use of a dildo or other sex toys, or anilingus. Some lesbians do not like anal sex, and anilingus is less often practiced among female same-sex couples.
There is less research on anal sexual activity among women who have sex with women compared to couples of other sexual orientations. In 1987, a non-scientific study (Munson) was conducted of more than 100 members of a lesbian social organization in Colorado. When asked what techniques they used in their last ten sexual encounters, lesbians in their 30s were twice as likely as other age groups to engage in anal stimulation (with a finger or dildo). While author Tom Boellstorff, when particularly examining anal sex among gay and lesbian individuals in Indonesia, stated that he had not heard of oral-anal contact or anal penetration as recognized forms of lesbian sexuality but assume they take place, author Felice Newman, in "," cites anal sex as a part of lesbian sexual practices.
Health risks.
General risks.
Anal sex can expose its participants to two principal dangers: infections due to the high number of infectious microorganisms not found elsewhere on the body, and physical damage to the anus and rectum due to their fragility. Unprotected penile-anal penetration, colloquially known as "barebacking", carries a higher risk of passing on sexually transmitted infections (STIs/STDs) because the anal sphincter is a delicate, easily torn tissue that can provide an entry for pathogens. The high concentration of white blood cells around the rectum, together with the risk of tearing and the colon's function to absorb fluid, are what place those who engage in anal sex at high risk of STIs. Use of condoms, ample lubrication to reduce the risk of tearing, and safer sex practices in general, reduce the risk of STI transmission. However, a condom can break or otherwise come off during anal sex, and this is more likely to happen with anal sex than with other sex acts because of the tightness of the anal sphincters during friction. As with other sexual practices, people without sound knowledge about the sexual risks involved are susceptible to STIs; for example, because of their definitions of sexual activity and virginity loss, teenagers may consider vaginal intercourse riskier than anal intercourse and believe that a STI can only result from vaginal intercourse.
Unprotected receptive anal sex (with an HIV positive partner) is the sex act most likely to result in HIV transmission. Other infections that can be transmitted by unprotected anal sex are human papillomavirus (HPV) (which can increase risk of anal cancer); typhoid fever; amoebiasis; chlamydia; cryptosporidiosis; E. coli infections; giardiasis; gonorrhea; hepatitis A; hepatitis B; hepatitis C; herpes simplex; Kaposi's sarcoma-associated herpesvirus (HHV-8); lymphogranuloma venereum; Mycoplasma hominis; Mycoplasma genitalium; pubic lice; salmonellosis; shigella; syphilis; tuberculosis; and Ureaplasma urealyticum.
There are a variety of factors that make male-to-female anal intercourse riskier for a female than for a male. For example, besides the risk of HIV transmission being higher for anal intercourse than for vaginal intercourse, the risk of injury to the woman during anal intercourse is significantly higher than the risk of injury to her during vaginal intercourse because of the durability of the vaginal tissues compared to the anal tissues. Additionally, if a man moves from anal intercourse immediately to vaginal intercourse without a condom or without changing it, infections (including urinary tract infections) can arise in the vagina due to bacteria present within the anus; these infections can also result from switching between vaginal sex and anal sex by the use of fingers or sex toys.
Though anal sex alone does not lead to pregnancy, pregnancy can still occur with anal sex or other forms of sexual activity if the penis is near the vagina (such as during intercrural sex or other genital-genital rubbing) and its sperm is deposited near the vagina's entrance and travels along the vagina's lubricating fluids; the risk of pregnancy can also occur without the penis being near the vagina because sperm may be transported to the vaginal opening by the vagina coming in contact with fingers or other non-genital body parts that have come in contact with semen.
Pain during receptive anal sex among gay men (or men who have sex with men) is formally known as "anodyspareunia." One study found that about 12% of gay men find it too painful to pursue receptive anal sex, and concluded that the perception of anal sex as painful is as likely to be psychologically or emotionally based as it is to be physically based. Another study that examined pain during insertive and receptive anal sex in gay men found that 3% of tops (insertive partners) and 16% of bottoms (receptive partners) reported significant pain. Factors predictive of pain during anal sex include inadequate lubrication, feeling tense or anxious, lack of stimulation, as well as lack of social ease with being gay and being closeted. Research has found that psychological factors can in fact be the primary contributors to the experience of pain during anal intercourse and that adequate communication between sexual partners can prevent it, countering the notion that pain is always inevitable during anal sex.
Physical damage and cancer.
Anal sex can exacerbate hemorrhoids and therefore result in bleeding; in other cases, the formation of a hemorrhoid is attributed to anal sex. If bleeding occurs as a result of anal sex, it may also be because of a tear in the anal or rectal tissues (an anal fissure) or perforation (a hole) in the colon, the latter of which being a serious medical issue that should be remedied by immediate medical attention. Because of the rectum's lack of elasticity, the anal mucous membrane being thin, and small blood vessels being present directly beneath the mucous membrane, tiny tears and bleeding in the rectum usually result from penetrative anal sex, though the bleeding is usually minor and therefore usually not visible. By contrast to other anal sexual behaviors, anal fisting poses a more serious danger of damage due to the deliberate stretching of the anal and rectal tissues; anal fisting injuries include anal sphincter lacerations and rectal and sigmoid colon (rectosigmoid) perforation, which might result in death.
Repetitive penetrative anal sex may result in the anal sphincters becoming weakened, which may cause rectal prolapse or affect the ability to hold in feces (a condition known as fecal incontinence). Rectal prolapse is relatively uncommon, however, especially in men, and its causes are not well understood. Kegel exercises have been used to strengthen the anal sphincters and overall pelvic floor, and may help prevent or remedy fecal incontinence. A 1993 study indicated that fourteen out of a sample of forty men receiving anal intercourse experienced episodes of frequent fecal incontinence. However, a 1997 study found no difference in levels of fecal incontinence between gay men who engaged in anal sex and heterosexual men who did not, and criticized the earlier study for its inclusion of flatulence in its definition of fecal incontinence.
Most cases of anal cancer are related to infection with the human papilloma virus (HPV). Anal sex alone does not cause anal cancer; the risk of anal cancer through anal sex is attributed to HPV infection, which is often contracted through unprotected anal sex. Anal cancer is relatively rare, and significantly less common than cancer of the colon or rectum (colorectal cancer); the American Cancer Society states that it affects approximately 7,060 people (4,430 in women and 2,630 in men) and results in approximately 880 deaths (550 in women and 330 in men) in the United States, and that, though anal cancer has been on the rise for many years, it is mainly diagnosed in adults, "with an average age being in the early 60s" and it "affects women somewhat more often than men." Though anal cancer is serious, treatment for it is "often very effective" and most anal cancer patients can be cured of the disease; the American Cancer Society adds that "receptive anal intercourse also increases the risk of anal cancer in both men and women, particularly in those younger than the age of 30. Because of this, men who have sex with men have a high risk of this cancer."
Other cultural views.
General.
Different cultures have had different views on anal sex throughout human history, with some cultures more positive about the activity than others. Historically, anal sex has been restricted or condemned, especially with regard to religious beliefs; it has also commonly been used as a form of domination, usually with the active partner (the one who is penetrating) representing masculinity and the passive partner (the one who is being penetrated) representing femininity. A number of cultures have especially recorded the practice of anal sex between males, and anal sex between males has been especially stigmatized or punished. In some societies, if discovered to have engaged in the practice, the individuals involved were put to death, such as by decapitation, burning, or mutilation.
Although anal sex has been more accepted in modern times, and is often considered a natural, pleasurable form of sexual expression, with some people, men in particular, only interested in anal sex for sexual satisfaction, engaging in the act is still punished in some societies. For example, regarding LGBT rights in Iran, Iran's Penal Code states in Article 109 that "both men involved in same-sex penetrative (anal) or non-penetrative sex will be punished" and "Article 110 states that those convicted of engaging in anal sex will be executed and that the manner of execution is at the discretion of the judge".
Ancient and non-Western cultures.
The term "Greek love" has long been used to refer to anal intercourse, and in modern times, "doing it the Greek way" is sometimes used as slang for anal sex. Ancient Greeks accepted romantic or sexual relationships between males as a balanced sex life (having males and women as lovers), and they considered this "normal (as long as one partner was an adult and the other was aged between twelve and fifteen)".
Male-male anal sex was not a universally accepted practice in Ancient Greece; it was the target of jokes in some Athenian comedies. Aristophanes, for instance, mockingly alludes to the practice, claiming, "Most citizens are "europroktoi" (wide-arsed) now." The terms "kinaidos", "europroktoi", and "katapygon" were used by Greek residents to categorize men who chronically practiced passive anal intercourse. While pedagogic pederasty was an essential element in the education of male youths, these relationships, at least in Athens and Sparta, were expected to steer clear of penetrative sex of any kind. There are few works of pottery or other art that display anal sex between men and adolescent boys, let alone between adult men. Greek artwork of sexual interaction between men and boys usually depicted fondling or intercrural sex, which was not condemned for violating or feminizing boys, while male-male anal intercourse was usually depicted between males of the same age-group. Intercrural sex was not considered penetrative and two males engaging in it was considered a "clean" act. Some sources explicitly state that anal sex between men and boys was criticized as shameful and seen as a form of hubris. Nonetheless, evidence suggests that the younger partner in pederastic relationships (i.e., the "eromenos") did engage in receptive anal intercourse so long as no one accused him of being 'feminine'.
In later Roman-era Greek poetry, anal sex became a common literary convention, represented as taking place with "eligible" youths: those who had attained the proper age but had not yet become adults. Seducing those not of proper age (for example, non-adolescent children) into the practice was considered very shameful for the adult, and having such relations with a male who was no longer adolescent was considered more shameful for the young male than for the one mounting him; Greek courtesans, or hetaerae, are said to have frequently practiced male-female anal intercourse as a means of preventing pregnancy.
A male citizen taking the passive (or receptive) role in anal intercourse was condemned in Rome as an act of "impudicitia" (immodesty or unchastity); free men, however, frequently took the active role with a young male slave, known as a "catamite" or "puer delicatus". The latter was allowed because anal intercourse was considered equivalent to vaginal intercourse in this way; men were said to "take it like a woman" (muliebria pati, "to undergo womanly things") when they were anally penetrated, but when a man performed anal sex on a woman, she was thought of as playing the boy's role. Likewise, women were believed to only be capable of anal sex or other sex acts with women if they possessed an exceptionally large clitoris or a dildo. The passive partner in any of these cases was always considered a woman or a boy because being the one who penetrates was characterized as the only appropriate way for an adult male citizen to engage in sexual activity, and he was therefore considered unmanly if he was the one who was penetrated; slaves could be considered "non-citizen". Although Roman men often availed themselves of their own slaves or others for anal intercourse, Roman comedies and plays presented Greek settings and characters for explicit acts of anal intercourse, and this may be indicative that the Romans thought of anal sex as something specifically "Greek".
In Japan, records (including detailed shunga) show that some males engaged in penetrative anal intercourse with males, and evidence suggestive of widespread male-female anal intercourse in a pre-modern culture can be found in the erotic vases, or stirrup-spout pots, made by the Moche people of Peru; in a survey, of a collection of these pots, it was found that 31 percent of them depicted male-female anal intercourse significantly more than any other sex act. Moche pottery of this type belonged to the world of the dead, which was believed to be a reversal of life. Therefore, the reverse of common practices was often portrayed. The Larco Museum houses an erotic gallery in which this pottery is showcased.
19th century anthropologist Richard Francis Burton theorized that there is a geographical Sotadic zone wherein penetrative intercourse between males is particularly prevalent and accepted; moreover he was one of the first writers to advance the premise that such an orientation is biologically determined.
Western cultures.
In many western countries, anal sex has generally been taboo since the Middle Ages, when heretical movements were sometimes attacked by accusations that their members practiced anal sex among themselves. At that time, celibate members of the Christian clergy were accused of engaging in "sins against nature", including anal sex.
The term "buggery" originated in medieval Europe as an insult used to describe the rumored same-sex sexual practices of the heretics from a sect originating in Bulgaria, where its followers were called "bogomils"; when they spread out of the country, they were called "buggres" (from the ethnonym "Bulgars"). Another term for the practice, more archaic, is "pedicate" from the Latin "pedicare", with the same meaning.
The Renaissance poet Pietro Aretino advocated anal sex in his "Sonetti Lussuriosi" (Lust Sonnets). While men who engaged in homosexual relationships were generally suspected of engaging in anal sex, many such individuals did not. Among these, in recent times, have been André Gide, who found it repulsive; and Noël Coward, who had a horror of disease, and asserted when young that "I'd never do anything – well the disgusting thing they do – because I know I could get something wrong with me".
Religion.
Judaism.
The Mishneh Torah, a text considered authoritative by Orthodox Jewish sects, states "since a man’s wife is permitted to him, he may act with her in any manner whatsoever. He may have intercourse with her whenever he so desires and kiss any organ of her body he wishes, and he may have intercourse with her naturally or unnaturally raditionally, "unnaturally" refers to anal and oral se, provided that he does not expend semen to no purpose. Nevertheless, it is an attribute of piety that a man should not act in this matter with levity and that he should sanctify himself at the time of intercourse."
Christianity.
Christian texts may sometimes euphemistically refer to anal sex as the "peccatum contra naturam" (the sin against nature, after Thomas Aquinas) or "Sodomitica luxuria" (sodomitical lusts, in one of Charlemagne's ordinances), or "peccatum illud horribile, inter christianos non-nominandum" (that horrible sin that among Christians is not to be named).
Islam.
"Liwat", or the sin of Lot's people, which has come to be interpreted as referring generally to same-sex sexual activity, is commonly officially prohibited by Islamic sects; there are parts of the Quran which talk about smiting on Sodom and Gomorrah, and this is thought to be a reference to unnatural sex, and so there are hadith and Islamic laws which prohibit it. While, concerning Islamic belief, it is objectionable to use the words "al-Liwat" and "luti" to refer to homosexuality because it is blasphemy toward the prophet of Allah, and therefore the terms "sodomy" and "homosexuality" are preferred, same-sex male practitioners of anal sex are called "luti" or "lutiyin" in plural and are seen as criminals in the same way that a thief is a criminal, meaning that they are giving in to a universal temptation.
Buddhism.
The most common formulation of Buddhist ethics is the Five Precepts. These precepts take the form of voluntary, personal undertakings, not divine mandate or instruction. The third of the Precepts is "To refrain from committing sexual misconduct". However, "sexual misconduct" (Sanskrit: "Kāmesu micchācāra", literally "sense gratifications arising from the 5 senses") is subject to interpretation relative to the social norms of the followers. Buddhism, in its fundamental form, does not define what is right and what is wrong in absolute terms for lay followers. Therefore, the interpretation of what kinds of sexual activity are acceptable for a layman is not a religious matter as far as Buddhism is concerned.
Hinduism.
Although Hindu society does not formally acknowledge sexuality between men, it formally acknowledges and gives space to sexuality between men and third genders as a variation of male-female sex (meaning a part of heterosexuality, rather than homosexuality, if analyzed in western terms). Hijras, Alis, Kotis, etc. (the various forms of third gender that exist in India today) are all characterized by the gender role of having receptive anal and oral sex with men. However, sexuality between males (as distinct from third genders) has thrived, mostly unspoken and informally, without being seen as different in the way it is seen in the west; young men involved in "such relationships do not consider themselves to be 'homosexual' but conceive their behavior in terms of sexual desire, opportunity and pleasure".
References.
Notes
Further reading

</doc>
<doc id="2466" url="https://en.wikipedia.org/wiki?curid=2466" title="Aarau">
Aarau

Aarau (, locally ɑːræ) is the capital of the northern Swiss canton of Aargau. The city is also the capital of the district of Aarau. It is German-speaking and predominantly Protestant. Aarau is situated on the Swiss plateau, in the valley of the Aare, on the river's right bank, and at the southern foot of the Jura mountains, and is west of Zürich, and northeast of Bern. The municipality borders directly on the Canton of Solothurn to the west. It is the second-largest city in Aargau after Wettingen. At the beginning of 2010 Rohr became a suburb of Aarau.
The official language of Aarau is (the Swiss variety of Standard) German, but the main spoken language is the local variant of the Alemannic Swiss German dialect.
Geography and geology.
The old city of Aarau is situated on a rocky outcrop at a narrowing of the Aare river valley, at the southern foot of the Jura mountains. Newer districts of the city lie to the south and east of the outcrop, as well as higher up the mountain, and in the valley on both sides of the Aare.
The neighboring municipalities are Küttigen to the north, Rohr and Buchs to the east, Suhr to the south-east, Unterentfelden to the south, and Eppenberg-Wöschnau and Erlinsbach to the west.
Aarau and the nearby neighboring municipalities have grown together and now form an interconnected agglomeration. The only exception is Unterentfelden whose settlements are divided from Aarau by the extensive forests of Gönhard and Zelgli.
Approximately nine-tenths of the city is south of the Aar, and one tenth is to the north. It has an area, , of . Of this area, 6.3% is used for agricultural purposes, while 34% is forested. Of the rest of the land, 55.2% is settled (buildings or roads) and the remainder (4.5%) is non-productive (rivers or lakes). The lowest elevation, , is found at the banks of the Aar, and the highest elevation, at , is the Hungerberg on the border with Küttigen.
History.
Prehistory.
A few artifacts from the Neolithic period were found in Aarau. Near the location of the present train station, the ruins of a settlement from the Bronze Age (about 1000 BC) have been excavated. The Roman road between Salodurum (Solothurn) and Vindonissa passed through the area, along the route now covered by the Bahnhofstrasse. In 1976 divers in the Aare found part of a seven-meter wide wooden bridge from the late Roman times.
Middle Ages.
Aarau was founded around AD 1240 by the counts of Kyburg. Aarau is first mentioned in 1248 as "Arowe". Around 1250 it was mentioned as "Arowa". However the first mention of a city sized settlement was in 1256. The town was ruled from the "Rore" tower, which has been incorporated into the modern city hall.
In 1273 the counts of Kyburg died out. Agnes of Kyburg, who had no male relations, sold the family's lands to King Rudolf I von Habsburg. He granted Aarau its city rights in 1283. In the 14th century the city was expanded in two stages, and a second defensive wall was constructed. A deep ditch separated the city from its "suburb;" its location is today marked by a wide street named "Graben" (meaning Ditch).
In 1415 Bern invaded lower Aargau with the help of Solothurn. Aarau capitulated after a short resistance, and was forced to swear allegiance to the new rulers. In the 16th century, the rights of the lower classes were abolished.
In March 1528 the citizens of Aarau allowed the introduction of Protestantism at the urging of the Bernese. A growth in population during the 16th Century led to taller buildings and denser construction methods. Early forms of industry developed at this time; however, unlike in other cities, no guilds were formed in Aarau.
On 11 August 1712, the Peace of Aarau was signed into effect. This granted each canton the right to choose their own religion thereby ending Catholicism's control. Starting in the early 18th century, the textile industry was established in Aarau. German immigration contributed to the city's favorable conditions, in that they introduced the cotton and silk factories. These highly educated immigrants were also responsible for educational reform and the enlightened, revolutionary spirit that developed in Aarau.
1798: Capital of the Helvetic Republic.
On 27 December 1797, the last Tagsatzung of the Old Swiss Confederacy was held in Aarau. Two weeks later a French envoy continued to foment the revolutionary opinions of the city. The contrast between a high level of education and a low level of political rights was particularly great in Aarau, and the city refused to send troops to defend the Bernese border. By Mid-March 1798 Aarau was occupied by French troops.
On 22 March 1798 Aarau was declared the capital of the Helvetic Republic. It is therefore the first capital of a unified Switzerland. Parliament met in the city hall. On 20 September, the capital was moved to Lucerne.
Aarau as canton capital.
In 1803, Napoleon ordered the fusion of the cantons of Aargau, Baden and Fricktal. Aarau was declared the capital of the new, enlarged canton of Aargau. In 1820 the city wall was torn down, with the exception of the individual towers and gates, and the defensive ditches were filled in.
The wooden bridge, dating from the Middle Ages, across the Aare was destroyed by floods three times in thirty years, and was replaced with a steel suspension bridge in 1851. This was replaced by a concrete bridge in 1952. The city was linked up to the Swiss Central Railway in 1856.
The textile industry in Aarau broke down in about 1850 because of the protectionist tariff policies of neighboring states. Other industries had developed by that time to replace it, including the production of mathematical instruments, shoes and cement. Beginning in 1900, numerous electrical enterprises developed. By the 1960s, more citizens worked in service industries or for the canton-level government than in manufacturing. During the 1980s many of the industries left Aarau completely.
In 1802 the Canton School was established; it was the first non-parochial high school in Switzerland. It developed a good reputation, and was home to Nobel Prize winners Albert Einstein, Paul Karrer, and Werner Arber, as well as several Swiss politicians and authors.
The purchase of a manuscript collection in 1803 laid the foundation for what would become the Cantonal Library, which contains a Bible annotated by Huldrych Zwingli, along with the manuscripts and incunabula. More newspapers developed in the city, maintaining the revolutionary atmosphere of Aarau. Beginning in 1820, Aarau has been a refuge for political refugees.
The urban educational and cultural opportunities of Aarau were extended through numerous new institutions. A Theatre and Concert Hall was constructed in 1883, which was renovated and expanded in 1995–96. The Aargau Nature Museum opened in 1922. A former cloth warehouse was converted into a small theatre in 1974, and the alternative culture center KIFF (Culture in the fodder factory) was established in a former animal fodder factory.
Origin of the name.
The earliest use of the place name was in 1248 (in the form Arowe), and probably referred to the settlement in the area before the founding of the city. It comes, along with the name of the River Aare (which was called Arula, Arola, and Araris in early times), from the German Au, meaning floodplain.
Old town.
The historic old town forms an irregular square, consisting of four parts (called "Stöcke"). To the south lies the Laurenzenvorstadt, that is, the part of the town formerly outside the city wall. One characteristic of the city is its painted gables, for which Aarau is sometimes called the "City of beautiful Gables". The old town, Laurenzenvorstadt, government building, cantonal library, state archive and art museum are all listed as heritage sites of national significance.
The buildings in the old city originate, on the whole, from building projects during the 16th century, when nearly all the Middle Age period buildings were replaced or expanded. The architectural development of the city ended in the 18th century, when the city began to expand beyond its (still existing) wall. Most of the buildings in the "suburb" date from this time.
The "Schlössli" (small Castle), Rore Tower and the upper gate tower have remained nearly unchanged since the 13th century. The "Schlössli" is the oldest building in the city. It was already founded at the time of the establishment of the city shortly after 1200; the exact date is not known. City hall was built around Rore Tower in 1515.
The upper gate tower stands beside the southern gate in the city wall, along the road to Lucerne and Bern. The jail has been housed in it since the Middle Ages. A Carillon was installed in the tower in the middle of the 20th century, the bells for which were provided by the centuries-old bell manufacturers of Aarau.
The town church was built between 1471 and 1478. During the Reformation, in 1528, its twelve altars and accompanying pictures were destroyed. The "Justice fountain" (Gerechtskeitbrunnen) was built in 1634, and is made of French limestone; it includes a statue of Lady Justice made of sandstone, hence the name. It was originally in the street in front of city hall, but was moved to its present location in front of the town church in 1905 due to increased traffic.
Economy.
, Aarau had an unemployment rate of 2.35%. , there were 48 people employed in the primary economic sector and about 9 businesses involved in this sector. 4,181 people are employed in the secondary sector and there are 164 businesses in this sector. 20,186 people are employed in the tertiary sector, with 1,461 businesses in this sector. This is a total of over 24,000 jobs, since Aarau's population is about 16,000 it draws workers from many surrounding communities. there were 8,050 total workers who lived in the municipality. Of these, 4,308 or about 53.5% of the residents worked outside Aarau while 17,419 people commuted into the municipality for work. There were a total of 21,161 jobs (of at least 6 hours per week) in the municipality.
The largest employer in Aarau is the cantonal government, the offices of which are distributed across the entire city at numerous locations. One of the two head offices of the "Aargauer Zeitung", Switzerland's fifth largest newspaper, is located in Aarau, as are the Tele M1 television channel studios, and several radio stations.
Kern & Co., founded in 1819, was an internationally known geodetic instrument manufacturer based in Aarau. However, it was taken over by Wild Leitz in 1988, and was closed in 1991.
More than half of the workers in Aarau live in the city's suburbs, or farther away in the surrounding area. This leads to a busy rush hour, and regular traffic jams. Statistically, Aarau has the most jobs per capita of any Swiss city.
The small scale of Aarau causes it to continually expand the borders of its growth. The urban center lies in the middle of the "Golden Triangle" between Zürich, Bern, and Basel, and Aarau is having increasing difficulty in maintaining the independence of its economic base from the neighboring large cities. The idea of merging Aarau with its neighboring suburbs has been recently discussed in the hope of arresting the slowly progressing losses.
Manufacture include bells, mathematical instruments, electrical goods, cotton textiles, cutlery, chemicals, shoes, and other products. Aarau is famous for the quality of their instruments, cutlery and their bells.
Markets and fairs.
Every Saturday morning there is a vegetable market in the "Graben" at the edge of the Old City. It is supplied with regional products. In the last week of September the MAG (Market of Aarauer Tradesmen) takes place there, with regional companies selling their products. The "Rüeblimärt" is held in the same place on the first Wednesday in November, which is a Carrot fair. The Aarau fair is held at the ice skating rink during the Spring.
Transport.
Aarau railway station is a terminus of the S-Bahn Zürich on the line S3.
The town is also served with public transport provided by Busbetrieb Aarau AG.
Population.
The population of Aarau grew continuously from 1800 until about 1960, when the city reached a peak population of 17,045, more than five times its population in 1800. However, since 1960 the population has fallen by 8%. There are three reasons for this population loss: firstly, since the completion of Telli (a large apartment complex), the city has not had any more considerable land developments. Secondly, the number of people per household has fallen; thus, the existing dwellings do not hold as many people. Thirdly, population growth was absorbed by neighboring municipalities in the regional urban area, and numerous citizens of Aarau moved into the countryside. This trend might have stopped since the turn of the 21st century. Existing industrial developments are being used for new purposes instead of standing empty.
Aarau has a population (as of ) of . , 19.8% of the population was made up of foreign nationals. Over the last 10 years the population has grown at a rate of 1%. Most of the population () speaks German (84.5%), with Italian being second most common ( 3.3%) and Serbo-Croatian being third ( 2.9%).
The age distribution, , in Aarau is; 1,296 children or 8.1% of the population are between 0 and 9 years old and 1,334 teenagers or 8.4% are between 10 and 19. Of the adult population, 2,520 people or 15.8% of the population are between 20 and 29 years old. 2,518 people or 15.8% are between 30 and 39, 2,320 people or 14.6% are between 40 and 49, and 1,987 people or 12.5% are between 50 and 59. The senior population distribution is 1,588 people or 10.0% of the population are between 60 and 69 years old, 1,219 people or 7.7% are between 70 and 79, there are 942 people or 5.9% who are between 80 and 89,and there are 180 people or 1.1% who are 90 and older.
, there were 1,365 homes with 1 or 2 persons in the household, 3,845 homes with 3 or 4 persons in the household, and 2,119 homes with 5 or more persons in the household. The average number of people per household was 1.99 individuals. there were 1,594 single family homes (or 18.4% of the total) out of a total of 8,661 homes and apartments.
In Aarau about 74.2% of the population (between age 25–64) have completed either non-mandatory upper secondary education or additional higher education (either university or a "Fachhochschule"). Of the school age population (), there are 861 students attending primary school, there are 280 students attending secondary school, there are 455 students attending tertiary or university level schooling, there are 35 students who are seeking a job after school in the municipality.
Sport.
The football club FC Aarau play in the Swiss Super League. Their stadium is the Stadion Brügglifeld. In the 1992/93 season they won the Swiss National League A managed by Austrian Rolf Fringer.
Sites.
Heritage sites of national significance.
Aarau is home to a number of sites that are listed as Swiss heritage sites of national significance. The list includes three churches; the Christian Catholic parish house, the Catholic parish house, and the Reformed "City Church". There are five government buildings on the list; the Cantonal Library, which contains many pieces important to the nation's history, and Art Gallery, the old Cantonal School, the Legislature, the Cantonal Administration building, and the archives. Three gardens or parks are on the list; "Garten Schmidlin", "Naturama Aargau" and the "Schlossgarten". The remaining four buildings on the list are; the former Rickenbach Factory, the Crematorium, the "Haus zum Erker" at Rathausgasse 10 and the "Restaurant Zunftstube" at Pelzgasse.
Tourist Sites.
The Bally Shoe company has a unique shoe museum in the city. There is also the Trade Museum which contain stained glass windows from Muri Convent and paintings.
Religion.
From the , 4,473 or 28.9% are Roman Catholic, while 6,738 or 43.6% belonged to the Swiss Reformed Church. Of the rest of the population, there are 51 individuals (or about 0.33% of the population) who belong to the Christian Catholic i.e. Old Catholic faith.
Government.
Legislative.
In place of a town meeting, a town assembly ("Einwohnerrat") of 50 members is elected by the citizens, and follows the policy of proportional representation. It is responsible for approving tax levels, preparing the annual account, and the business report. In addition, it can issue regulations. The term of office is four years. In the last two elections the parties had the following representation:
At the district level, some elements of the government remain a direct democracy. There are optional and obligatory referendums, and the population retains the right to establish an initiative.
Executive.
The executive authority is the town council ("Stadtrat"). The term of office is four years, and its members are elected by a plurality voting system. It leads and represents the municipality. It carries out the resolutions of the assembly, and those requested by the canton and national level governments.
The seven members (and their party) for the period 2006–2009 are:
National elections.
In the 2007 federal election the most popular party was the SP which received 27.9% of the vote. The next three most popular parties were the SVP (22.1%), the FDP (17.5%) and the Green Party (11.8%).
Coat of arms.
The blazon of the municipal coat of arms is "Argent an Eagle displayed Sable beaked langued and membered Gules and a Chief of the last."
International relations.
Twin towns – Sister cities.
Aarau is twinned with:

</doc>
<doc id="2467" url="https://en.wikipedia.org/wiki?curid=2467" title="Aargau">
Aargau

The Canton of Aargau (German "Kanton" ; rarely anglicized Argovia; see also other names) is one of the more northerly cantons of Switzerland. It is situated by the lower course of the Aare, which is why the canton is called Aar-gau (meaning "Aare province"). It is one of the most densely populated regions of Switzerland.
History.
Early history.
The area of Aargau and the surrounding areas were controlled by the Helvetians, a member of the Celts, as far back as 200 BC, eventually being occupied by the Romans and then by the 6th century, the Franks. The Romans built a major settlement called Vindonissa, near the present location of Brugg.
Medieval Aargau.
In early medieval times, the Aargau was a disputed border region between the duchies of Alamannia and Burgundy. A line of the von Wetterau (Conradines) intermittently held the countship of Aargau from 750 until about 1030, when they lost it (having in the meantime taken the name von Tegerfelden). From the extinction in 1254 of the Hohenstaufen dynasty until 1415, the area was ruled by the Habsburgs, and many castles from that time still stand (examples include Habsburg, Lenzburg, Tegerfelden, Bobikon, Stin and Wildegg). The Habsburgs founded a number of monasteries (with some structures enduring, e.g., in Wettingen and Muri), the closing of which by the government in 1841 was a contributing factor to the outbreak of the Swiss civil war – the "Sonderbund War" – in 1847.
Under the Swiss Confederation.
When Frederick IV of Habsburg sided with Antipope John XXIII at the Council of Constance, Emperor Sigismund placed him under the Imperial ban. In July 1414, the Pope visited Bern and received assurances from them, that they would move against the Habsburgs. A few months later the Swiss Confederation denounced the Treaty of 1412. Shortly thereafter in 1415, Bern and the rest of the Swiss Confederation used the ban as a pretext to invade the Aargau. The Confederation was able to quickly conquer the towns of Aarau, Lenzburg, Brugg and Zofingen along with most of the Habsburg castles. Bern kept the southwest portion (Zofingen, Aarburg, Aarau, Lenzburg, and Brugg), northward to the confluence of the Aare and Reuss. The important city of Baden was taken by a united Swiss army and governed by all 8 members of the Confederation. Some districts, named the "Freie Ämter" ("free bailiwicks") – Mellingen, Muri, Villmergen, and Bremgarten, with the countship of Baden – were governed as "subject lands" by all or some of the Confederates. Shortly after the conquest of the Aargau by the Swiss, Frederick humbled himself to the Pope. The Pope reconciled with him and ordered all of the taken lands to be returned. The Swiss refused and years later after no serious attempts at re-acquisition, the Duke officially relinquished rights to the Swiss.
Unteraargau or Berner Aargau.
Bern's portion of the Aargau came to be known as the Unteraargau, though can also be called the Berner or Bernese Aargau. In 1514 Bern expanded north into the Jura and so came into possession of several strategically important mountain passes into the Austrian Fricktal. This land was added to the Unteraargau and was directly ruled from Bern. It was divided into seven rural bailiwicks and four administrative cities, Aarau, Zofingen, Lenzburg and Brugg. While the Habsburgs were driven out, many of their minor nobles were allowed to keep their lands and offices, though over time they lost power to the Bernese government. The bailiwick administration was based on a very small staff of officials, mostly made up of Bernese citizens, but with a few locals.
When Bern converted during the Protestant Reformation in 1528, the Unteraargau also converted. At the beginning of the 16th century a number of anabaptists migrated into the upper Wynen and Rueder valleys from Zürich. Despite pressure from the Bernese authorities in the 16th and 17th centuries anabaptism never entirely disappeared from the Unteraargau.
Bern used the Aargau bailiwicks mostly as a source of grain for the rest of the city-state. The administrative cities remained economically only of regional importance. However, in the 17th and 18th centuries Bern encouraged industrial development in Unteraargau and by the late 18th century it was the most industrialized region in the city-state. The high industrialization led to high population growth in the 18tf century, for example between 1764 and 1798, the population grew by 35%, far more than in other parts of the canton. In 1870 the proportion of farmers in Aarau, Lenzburg, Kulm, and Zofingen districts was 34–40%, while in the other districts it was 46–57%.
Freie Ämter.
The rest of the Freie Ämter were collectively administered as subject territories by the rest of the Confederation. Muri "Amt" was assigned to Zürich, Lucerne, Schwyz, Unterwalden, Zug and Glarus, while the "Ämter" of Meienberg, Richensee and Villmergen were first given to Lucerne alone. The final boundary was set in 1425 by an arbitration tribunal and Lucerne had to give the three "Ämter" to be collectively ruled. The four "Ämter" were then consolidated under a single Confederation bailiff into what was known in the 15th century as the "Waggental" Bailiwick (). In the 16th century, it came to be known as the "Vogtei der Freien Ämter". While the "Freien Ämter" often had independent lower courts, they were forced to accept the Confederation's sovereignty. Finally, in 1532, the canton of Uri became part of the collective administration of the Freien Ämter.
At the time of Reformation, the majority of the Ämter converted to the new faith. In 1529, a wave of iconoclasm swept through the area and wiped away much of the old religion. After the defeat of Zürich in the second Battle of Kappel in 1531, the victorious five Catholic cantons marched their troops into the Freie Ämter and reconverted them to Catholicism.
In the First War of Villmergen, in 1656, and the Toggenburg War (or Second War of Villmergen), in 1712, the Freie Ämter became the staging ground for the warring Reformed and Catholic armies. While the peace after the 1656 war did not change the status quo, the fourth Peace of Aarau in 1712 brought about a reorganization of power relations. The victory gave Zürich the opportunity to force the Catholic cantons out of the government in the county of Baden and the adjacent area of the Freie Ämter. The Freie Ämter were then divided in two by a line drawn from the gallows in Fahrwangen to the Oberlunkhofen church steeple. The northern part, the so-called Unteren Freie Ämter (lower Freie Ämter), which included the districts of Boswil (in part) and Hermetschwil and the Niederamt, were ruled by Zürich, Bern and Glarus. The southern part, the Oberen Freie Ämter (upper Freie Ämter), were ruled by the previous seven cantons but Bern was added to make an eighth.
During the Helvetic Republic (1798–1803), the county of Baden, the Freie Ämter and the area known as the Kelleramt were combined into the Canton of Baden.
County of Baden.
The County of Baden was a shared condominium of the entire Old Swiss Confederacy. After the Confederacy conquest in 1415, they retained much of the Habsburg legal structure, which caused a number of problems. The local nobility had the right to hold the low court in only about one fifth of the territory. There were over 30 different nobles who had the right to hold courts scattered around the surrounding lands. All these overlapping jurisdictions caused numerous conflicts, but gradually the Confederation was able to acquire these rights in the County. The cities of Baden, Bremgarten and Mellingen became the administrative centers and held the high courts. Together with the courts, the three administrative centers had considerable local autonomy, but were ruled by a governor who was appointed by the "Acht Orte" every two years. After the Protestant victory at the Second Battle of Villmergen, the administration of the County changed slightly. Instead of the "Acht Orte" appointing a bailiff together, Zürich and Bern each appointed the governor for 7 out of 16 years while Glarus appointed him for the remaining 2 years.
The chaotic legal structure and fragmented land ownership combined with a tradition of dividing the land among all the heirs in an inheritance prevented any large scale reforms. The governor tried in the 18th century to reform and standardize laws and ownership across the County, but with limited success. With an ever changing administration, the County lacked a coherent long-term economic policy or support for reforms. By the end of the 18th century there were no factories or mills and only a few small cottage industries along the border with Zürich. Road construction first became a priority after 1750, when Zürich and Bern began appointing a governor for seven years.
During the Protestant Reformation, some of the municipalities converted to the new faith. However, starting in 1531, some of the old parishes were converted back to the old faith. The governors were appointed from both Catholic and Protestant cantons and since they changed every two years, neither faith gained a majority in the County.
The County was the only federal condominium in the 17th century where Jews were tolerated. In 1774, they were restricted to just two towns, Endingen and Lengnau. While the rural upper class tried several times to finally expel the Jews, the financial interests of the authorities prevented this. The Jews were directly subordinate to the governor starting in 1696 when they were forced to buy a protecting and shielding letter every 16 years from the governor.
After the French invasion, on 19 March 1798, the governments of Zürich and Bern agreed to the creation of the short lived Canton of Baden in the Helvetic Republic. With the Act of Mediation in 1803, the Canton of Baden was dissolved. Portions of the lands of the former County of Baden now became the District of Baden in the newly created Canton of Aargau. After World War II, this formerly agrarian region saw striking growth and became the district with the largest and densest population in the Canton (110,000 in 1990, 715 persons per km).
Forming the canton of Aargau.
The contemporary canton of Aargau was formed in 1803, a canton of the Swiss Confederation as a result of the Act of Mediation. It was a combination of three short-lived cantons of the Helvetic Republic: Aargau (1798–1803), Baden (1798–1803) and Fricktal (1802–1803). Its creation is therefore rooted in the Napoleonic era. In the year 2003, the canton of Aargau celebrated its 200th anniversary.
French forces occupied the Aargau from 10 March to 18 April 1798; thereafter the Bernese portion became the canton of Aargau and the remainder formed the canton of Baden. Aborted plans to merge the two halves came in 1801 and 1802, and they were eventually united under the name Aargau, which was then admitted as a full member of the reconstituted Confederation following the Act of Mediation. Some parts of the canton of Baden at this point were transferred to other cantons: the "Amt" of Hitzkirch to Lucerne, whilst Hüttikon, Oetwil an der Limmat, Dietikon and Schlieren went to Zürich. In return, Lucerne's "Amt" of Merenschwand was transferred to Aargau (district of Muri).
The Fricktal, ceded in 1802 by Austria via Napoleonic France to the Helvetic Republic, was briefly a separate canton of the Helvetic Republic (the canton of Fricktal) under a "Statthalter" ('Lieutenant'), but on 19 March 1803 (following the Act of Mediation) was incorporated into the canton of Aargau.
The former cantons of Baden and Fricktal can still be identified with the contemporary districts – the canton of Baden is covered by the districts of Zurzach, Baden, Bremgarten, and Muri (albeit with the gains and losses of 1803 detailed above); the canton of Fricktal by the districts of Rheinfelden and Laufenburg (except for Hottwil which was transferred to that district in 2010).
Chief magistracy.
The chief magistracy of Aargau changed its style repeatedly:
The Jewish history in Aargau.
In the 17th century, Jews were banished from Switzerland. However, a few families were permitted to live in two villages, Endingen and Lengnau, in Aargau which became the Jewish ghetto in Switzerland. During this period, Jews and Christians were not allowed to live under the same roof, neither were Jews allowed to own land or houses. They were taxed at a much higher rate than others and, in 1712, the Lengnau community was "pillaged." In 1760, they were further restricted regarding marriages and multiplying. This remained the case until the 19th century. In 1799, all special tolls were abolished, and, in 1802, the poll tax was removed. On 5 May 1809, they were declared citizens and given broad rights regarding trade and farming. They were still restricted to Endingen and Lengnau until 7 May 1846, when their right to move and reside freely within the canton of Aargau was granted. On 24 September 1856, the Swiss Federal Council granted them full political rights within Aargau, as well as broad business rights; however the majority Christian population did not abide by these new liberal laws fully. The time of 1860 saw the canton government voting to grant suffrage in all local rights and to give their communities autonomy. Before the law was enacted, it was repealed due to vocal opposition led by the Ultramonte Party. Finally, the federal authorities in July 1863, granted all Jews full rights of citizens. However, they did not receive all of the rights in Endingen and Lengn until a resolution of the Grand Council, on 15 May 1877, granted citizens' rights to the members of the Jewish communities of those places, giving them charters under the names of New Endingen and New Lengnau. The Swiss Jewish Kulturverein was instrumental in this fight from its founding in 1862 until it was dissolved 20 years later. During this period of diminished rights, they were not even allowed to bury their dead in Swiss soil and had to bury their dead on an island called "Judenäule" (Jews' Isle) on the Rhine near Waldshut. Beginning in 1603, the deceased Jews of the Surbtal communities were buried on the river island which was leased by the Jewish community. As the island was repeatedly flooded and devastated, in 1750 the Surbtal Jews asked the "Tagsatzung" to establish the Endingen cemetery in the vicinity of their communities.
Geography.
The capital of the canton is Aarau, which is located on its western border, on the Aare. The canton borders Germany to the north, the Rhine forming the border. To the west lie the Swiss cantons of Basel-Landschaft, Solothurn and Bern; the canton of Lucerne lies south, and Zürich and Zug to the east. Its total area is . It contains both large rivers, the Aare and the Reuss.
The canton of Aargau is one of the least mountainous Swiss cantons, forming part of a great table-land, to the north of the Alps and the east of the Jura, above which rise low hills. The surface of the country is beautifully diversified, undulating tracts and well-wooded hills alternating with fertile valleys watered mainly by the Aare and its tributaries. The valleys alternate with pleasant hills, most of which are full of woods. Slightly over one-third of the canton is wooded (), while nearly half is used from farming (). or about 2.4% of the canton is considered unproductive, mostly lakes (notably Lake Hallwil) and streams. With a population density of 450/km (1,200/sq mi), the canton has a relatively high amount of land used for human development, with or about 15% of the canton developed for housing or transportation.
It contains the famous hot sulphur springs of Baden and Schinznach-Bad, while at Rheinfelden there are very extensive saline springs. Just below Brugg the Reuss and the Limmat join the Aar, while around Brugg are the ruined castle of Habsburg, the old convent of Königsfelden (with fine painted medieval glass) and the remains of the Roman settlement of "Vindonissa" (Windisch).
Fahr Abbey forms a small exclave of the canton, otherwise surrounded by the canton of Zürich, and since 2008 is part of the Aargau municipality of Würenlos.
Political subdivisions.
Districts.
Aargau is divided into 11 districts:
The most recent change in district boundaries occurred in 2010 when Hottwil transferred from Brugg to Laufenburg, following its merger with other municipalities, all of which were in Laufenburg.
Municipalities.
There are (as of 2014) 213 municipalities in the canton of Aargau. As with most Swiss cantons there has been a trend since the early 2000s for municipalities to merge, though mergers in Aargau have so far been less radical than in other cantons.
Coat of arms.
The blazon of the coat of arms is "Per pale, dexter: sable, a fess wavy argent, charged with two cotises wavy azure; sinister: sky blue, three mullets of five argent."
The flag and arms of Aargau date to 1803 and are an original design by Samuel Ringier-Seelmatter; the current official design, specifying the stars as five-pointed, dates to 1930.
Demographics.
Aargau has a population () of . , 21.5% of the population are resident foreign nationals. Over the last 10 years (2000–2010) the population has changed at a rate of 11%. Migration accounted for 8.7%, while births and deaths accounted for 2.8%. Most of the population () speaks German (477,093 or 87.1%) as their first language, Italian is the second most common (17,847 or 3.3%) and Serbo-Croatian is the third (10,645 or 1.9%). There are 4,151 people who speak French and 618 people who speak Romansh.
Of the population in the canton, 146,421 or about 26.7% were born in Aargau and lived there in 2000. There were 140,768 or 25.7% who were born in the same canton, while 136,865 or 25.0% were born somewhere else in Switzerland, and 107,396 or 19.6% were born outside of Switzerland.
, children and teenagers (0–19 years old) make up 24.3% of the population, while adults (20–64 years old) make up 62.3% and seniors (over 64 years old) make up 13.4%.
, there were 227,656 people who were single and never married in the canton. There were 264,939 married individuals, 27,603 widows or widowers and 27,295 individuals who are divorced.
, there were 224,128 private households in the canton, and an average of 2.4 persons per household. There were 69,062 households that consist of only one person and 16,254 households with five or more people. , the construction rate of new housing units was 6.5 new units per 1000 residents. The vacancy rate for the canton, , was 1.54%.
The majority of the population is centered on one of three areas: the Aare Valley, the side branches of the Aare Valley, or along the Rhine.
Historic population.
The historical population is given in the following chart:
Politics.
In the 2011 federal election, the most popular party was the SVP which received 34.7% of the vote. The next three most popular parties were the SP/PS (18.0%), the FDP (11.5%) and the CVP (10.6%).
The SVP received about the same percentage of the vote as they did in the 2007 Federal election (36.2% in 2007 vs 34.7% in 2011). The SPS retained about the same popularity (17.9% in 2007), the FDP retained about the same popularity (13.6% in 2007) and the CVP retained about the same popularity (13.5% in 2007).
Religion.
From the , 219,800 or 40.1% were Roman Catholic, while 189,606 or 34.6% belonged to the Swiss Reformed Church. Of the rest of the population, there were 11,523 members of an Orthodox church (or about 2.10% of the population), there were 3,418 individuals (or about 0.62% of the population) who belonged to the Christian Catholic Church, and there were 29,580 individuals (or about 5.40% of the population) who belonged to another Christian church. There were 342 individuals (or about 0.06% of the population) who were Jewish, and 30,072 (or about 5.49% of the population) who were Islamic. There were 1,463 individuals who were Buddhist, 2,089 individuals who were Hindu and 495 individuals who belonged to another church. 57,573 (or about 10.52% of the population) belonged to no church, are agnostic or atheist, and 15,875 individuals (or about 2.90% of the population) did not answer the question.
Education.
In Aargau about 212,069 or (38.7%) of the population have completed non-mandatory upper secondary education, and 70,896 or (12.9%) have completed additional higher education (either university or a "Fachhochschule"). Of the 70,896 who completed tertiary schooling, 63.6% were Swiss men, 20.9% were Swiss women, 10.4% were non-Swiss men and 5.2% were non-Swiss women.
Economy.
, Aargau had an unemployment rate of 3.6%. , there were 11,436 people employed in the primary economic sector and about 3,927 businesses involved in this sector. 95,844 people were employed in the secondary sector and there were 6,055 businesses in this sector. 177,782 people were employed in the tertiary sector, with 21,530 businesses in this sector.
Of the working population, 19.5% used public transportation to get to work, and 55.3% used a private car. Public transportation – bus and train – is provided by Busbetrieb Aarau AG.
The farmland of the canton of Aargau is some of the most fertile in Switzerland. Dairy farming, cereal and fruit farming are among the canton's main economic activities. The canton is also industrially developed, particularly in the fields of electrical engineering, precision instruments, iron, steel, cement and textiles.
Three of Switzerland's five nuclear power plants are in the canton of Aargau (Beznau I + II and Leibstadt). Additionally, the many rivers supply enough water for numerous hydroelectric power plants throughout the canton. The canton of Aargau is often called "the energy canton".
A significant number of people commute into the financial center of the city of Zürich, which is just across the cantonal border. As such the per capita cantonal income (in 2005) is 49,209 CHF.
Tourism is significant, particularly for the hot springs at Baden and Schinznach-Bad, the ancient castles, the landscape, and the many old museums in the canton. Hillwalking is another tourist attraction but is of only limited significance.

</doc>
<doc id="2470" url="https://en.wikipedia.org/wiki?curid=2470" title="Aba">
Aba

Aba may refer to:

</doc>
<doc id="2471" url="https://en.wikipedia.org/wiki?curid=2471" title="Ababda people">
Ababda people

The Ababda or Ababde – the Gebadei of Pliny, and possibly the Troglodytes of other classical writers – are nomads living in the area between the Nile and the Red Sea, in the vicinity of Aswan in Egypt and north Sudan. They are a subgroup of the Beja people who are bilingual in Beja and Arabic.
Overview.
The Ababda extend from the Nile at Aswan to the Red Sea, and reach northward to the Qena-Quseir road, thus occupying the southern border of Egypt east of the Nile. They call themselves "sons of the Jinns." With some of the clans of the Bisharin and possibly the Hadendoa, they represent the Blemmyes of classic geographers, and their location today is almost identical with that assigned them in Roman times.
They were constantly at war with the Romans, who eventually conquered them. In the Middle Ages, they were known as Beja, and convoyed pilgrims from the Nile valley to Aidhab, the port of embarkation for Jeddah. From time immemorial, they have acted as guides to caravans through the Nubian desert and up the Nile valley as far as Sennar. They intermarried with the Nubians, and settled in small colonies at Shendi and elsewhere up to Muhammad Ali's conquest of the region in the early 19th century. They are still great trade carriers, and visit very distant districts.

</doc>
<doc id="2472" url="https://en.wikipedia.org/wiki?curid=2472" title="American Quarter Horse">
American Quarter Horse

The American Quarter Horse is an American breed of horse that excels at sprinting short distances. Its name came from its ability to outdistance other horse breeds in races of a quarter mile or less; some have been clocked at speeds up to 55 mph (88.5 km/h). The American Quarter Horse is the most popular breed in the United States today, and the American Quarter Horse Association is the largest breed registry in the world, with almost 3 million American Quarter Horses currently registered.
The American Quarter Horse is well known both as a race horse and for its performance in rodeos, horse shows and as a working ranch horse. The compact body of the American Quarter Horse is well-suited to the intricate and speedy maneuvers required in reining, cutting, working cow horse, barrel racing, calf roping, and other western riding events, especially those involving live cattle. The American Quarter Horse is also shown in English disciplines, driving, and many other equestrian activities.
Breed history.
Colonial era.
In the 17th century, colonists on the eastern seaboard of what today is the United States began to cross imported English Thoroughbred horses with assorted "native" horses such as the Chickasaw horse, which was a breed developed by Native American people from horses descended from Spain in the 1500s, developed from Iberian, Arabian and Barb stock brought to what is now the Southeastern United States by the Conquistadors.
One of the most famous of these early imports was Janus, a Thoroughbred who was the grandson of the Godolphin Arabian. He was foaled in 1746, and imported to colonial Virginia in 1756. The influence of Thoroughbreds like Janus contributed genes crucial to the development of the colonial "Quarter Horse". The breed is sometimes referred to as the "Famous American Quarter Running Horse". The resulting horse was small, hardy, and quick, and was used as a work horse during the week and a race horse on the weekends.
As flat racing became popular with the colonists, the Quarter Horse gained even more popularity as a sprinter over courses that, by necessity, were shorter than the classic racecourses of England, and were often no more than a straight stretch of road or flat piece of open land. When matched against a Thoroughbred, local sprinters often won. As the Thoroughbred breed became established in America, many colonial Quarter Horses were included in the original American stud books, starting a long association between the Thoroughbred breed and what would later become officially known as the "Quarter Horse," named after the race distance at which it excelled. with some individuals being clocked at up to 55 mph.
Westward expansion.
In the 19th century, pioneers heading West needed a hardy, willing horse. On the Great Plains, settlers encountered horses that descended from the Spanish stock Hernán Cortés and other Conquistadors had introduced into the viceroyalty of New Spain, which today includes the Southwestern United States and Mexico. These horses of the west included herds of feral animals known as Mustangs, as well as horses domesticated by Native Americans, including the Comanche, Shoshoni and Nez Perce tribes. As the colonial Quarter Horse was crossed with these western horses, the pioneers found that the new crossbred had innate "cow sense," a natural instinct for working with cattle, making it popular with cattlemen on ranches.
Development as a distinct breed.
Early foundation sires of Quarter horse type included Steel Dust, foaled 1843; Shiloh (or Old Shiloh), foaled 1844; Old Cold Deck (1862); Lock's Rondo, one of many "Rondo" horses, foaled in 1880; Old Billy—again, one of many "Billy" horses—foaled circa 1880; Traveler, a stallion of unknown breeding, known to have been in Texas by 1889; and Peter McCue, foaled 1895, registered as a Thoroughbred but of disputed pedigree.
The main duty of the ranch horse in the American West was working cattle. Even after the invention of the automobile, horses were still irreplaceable for handling livestock on the range. Thus, major Texas cattle ranches, such as the King Ranch, the 6666 (Four Sixes) Ranch, and the Waggoner Ranch played a significant role in the development of the modern Quarter Horse. The skills needed by cowboys and their horses became the foundation of the rodeo, a contest which began with informal competition between cowboys and expanded to become a major competitive event throughout the west. To this day, the Quarter Horse dominates the sport both in speed events and in competition that emphasizes the handling of live cattle.
However, sprint races were also popular weekend entertainment and racing became a source of economic gain for breeders as well. As a result, more Thoroughbred blood was added back into the developing American Quarter Horse breed. The American Quarter Horse also benefitted from the addition of Arabian, Morgan and even Standardbred bloodlines.
In 1940, the American Quarter Horse Association (AQHA) was formed by a group of horsemen and ranchers from the southwestern United States dedicated to preserving the pedigrees of their ranch horses. The horse honored with the first registration number, P-1, was Wimpy, a descendant of the King Ranch foundation sire Old Sorrel. Other sires alive at the founding of the AQHA were given the earliest registration numbers Joe Reed P-3, Chief P-5, Oklahoma Star P-6, Cowboy P-12, and Waggoner's Rainy Day P-13. The Thoroughbred race horse Three Bars, alive in the early years of the AQHA, is recognized by the American Quarter Horse Hall of Fame as one of the significant foundation sires for the Quarter Horse breed. Other significant Thoroughbred sires seen in early AQHA pedigrees include Rocket Bar, Top Deck and Depth Charge.
"Appendix" and "Foundation" horses.
Since the American Quarter Horse formally established itself as a breed, the AQHA stud book has remained open to additional Thoroughbred blood via a performance standard. An "Appendix" American Quarter Horse is a first generation cross between a registered Thoroughbred and an American Quarter Horse or a cross between a "numbered" American Quarter Horse and an "appendix" American Quarter Horse. The resulting offspring is registered in the "appendix" of the American Quarter Horse Association's studbook, hence the nickname. Horses listed in the appendix may be entered in competition, but offspring are not initially eligible for full AQHA registration. If the Appendix horse meets certain conformational criteria and is shown or raced successfully in sanctioned AQHA events, the horse can earn its way from the appendix into the permanent studbook, making its offspring eligible for AQHA registration
Since Quarter Horse/Thoroughbred crosses continue to enter the official registry of the American Quarter Horse breed, this creates a continual gene flow from the Thoroughbred breed into the American Quarter Horse breed, which has altered many of the characteristics that typified the breed in the early years of its formation. Some breeders, who argue that the continued infusion of Thoroughbred bloodlines is beginning to compromise the integrity of the breed standard, favor the earlier style of horse and have created several separate organizations to promote and register "Foundation" Quarter Horses.
Quarter Horses today.
The American Quarter Horse is best known today as a show horse, race horse, reining and cutting horse, rodeo competitor, ranch horse, and all-around family horse. Quarter horses compete well in rodeo events such as barrel racing, calf roping and team roping; and gymkhana or O-Mok-See. Other stock horse events such as cutting and reining are open to all breeds but also dominated by American Quarter Horse. Large purses allow top competitors to earn over a million dollars in some of these events.
The breed is not only well-suited for western riding and cattle work. Many race tracks offer Quarter Horses a wide assortment of pari-mutuel horse racing with purses in the millions. Quarter Horses have also been trained to compete in dressage and can be good jumpers. They are also used for recreational trail riding and in mounted police units.
The American Quarter Horse has also been exported worldwide. European nations such as Germany and Italy have imported large numbers of Quarter Horses. Next to the American Quarter Horse Association (which also encompasses Quarter Horses from Canada), the second largest registry of Quarter Horses is in Brazil, followed by Australia. With the internationalization of the discipline of reining and its acceptance as one of the official seven events of the World Equestrian Games, there is a growing international interest in Quarter Horses. Countries like Japan, Switzerland and Israel that did not have traditional stock horse industries have begun to compete with American Quarter Horses in their own nations and internationally. The American Quarter Horse is the most popular breed in the United States today, and the American Quarter Horse Association is the largest breed registry in the world, with over 5 million American Quarter Horses registered worldwide.
Breed characteristics.
The modern Quarter Horse has a small, short, refined head with a straight profile, and a strong, well-muscled body, featuring a broad chest and powerful, rounded hindquarters. They usually stand between high, although some Halter-type and English hunter-type horses may grow as tall as .
There are two main body types: the stock type and the hunter or racing type. The stock horse type is shorter, more compact, stocky and well muscled, yet agile. The racing and hunter type Quarter Horses are somewhat taller and smoother muscled than the stock type, more closely resembling the Thoroughbred.
Quarter Horses come in nearly all colors. The most common color is sorrel, a brownish red, part of the color group called chestnut by most other breed registries. Other recognized colors include bay, black, brown, buckskin, palomino, gray, dun, red dun, grullo (also occasionally referred to as blue dun), red roan, blue roan, bay roan, perlino, cremello, and white. In the past, spotted color patterns were excluded, but now with the advent of DNA testing to verify parentage, the registry accepts all colors as long as both parents are registered.
Stock type.
Reining and cutting horses are smaller in stature, with quick, agile movements and very powerful hindquarters. Western pleasure show horses are often slightly taller, with slower movements, smoother gaits, and a somewhat more level topline – though still featuring the powerful hindquarters characteristic of the Quarter Horse. 
Halter type.
Horses shown in-hand in Halter competition are larger yet, with a very heavily muscled appearance, while retaining small heads with wide jowls and refined muzzles. There is controversy amongst owners, breeder and veterinarians regarding the health effects of the extreme muscle mass that is currently fashionable in the specialized halter horse, which typically is and weighs in at over when fitted for halter competition. Not only are there concerns about the weight to frame ratio on the horse's skeletal system, but the massive build is also linked to HYPP. (See "Genetic diseases" below))
Racing and hunter type.
Quarter Horse race horses are bred to sprint short distances ranging from 220 to 870 yards. Thus, they have long legs and are leaner than their stock type counterparts, but are still characterized by muscular hindquarters and powerful legs. Quarter horses race primarily against other Quarter horses, and their sprinting ability has earned them the nickname, "the world's fastest athlete." The show hunter type is slimmer, even more closely resembling a Thoroughbred, usually reflecting a higher percentage of appendix breeding. They are shown in hunter/jumper classes at both breed shows and in open USEF-rated horse show competition.
Genetic diseases.
There are several genetic diseases of concern to Quarter Horse breeders:

</doc>
<doc id="2473" url="https://en.wikipedia.org/wiki?curid=2473" title="Abacá">
Abacá

Abacá ( ; ), binomial name Musa textilis, is a species of banana native to the Philippines, grown as a commercial crop in the Philippines, Ecuador, and Costa Rica. The plant, also known as Manila hemp, has great economic importance, being harvested for its fiber, also called Manila hemp, extracted from the leaf-stems. The plant grows to , and averages about . The fiber was originally used for making twines and ropes; now most is pulped and used in a variety of specialized paper products including tea bags, filter paper and banknotes. It is classified as a hard fiber, along with coir, henequin and sisal.
Description.
The abacá plant is stoloniferous, meaning that the plant produces runners or shoots along the ground that then root at each segment. Cutting and transplanting rooted runners is the primary technique for creating new plants, since seed growth is substantially slower. Abacá has a "false trunk" or pseudostem about in diameter. The leaf stalks (petioles) are expanded at the base to form sheaths that are tightly wrapped together to form the pseudostem. There are from 12 to 25 leaves, dark green on the top and pale green on the underside, sometimes with large brown patches. They are oblong in shape with a deltoid base. They grow in succession. The petioles grow to at least in length. When the plant is mature, the flower stalk grows up inside the pseudostem. The male flower has 5 petals, each about long. The leaf sheaths contain the valuable fiber. After harvesting, the coarse fibers range in length from long. They are composed primarily of cellulose, lignin, and pectin.
The fruit, which is inedible and is rarely seen as harvesting occurs before the plant fruits, grows to about in length and in diameter. It has black turbinate seeds that are in diameter.
Systematics.
The abacá plant belongs to the banana family, Musaceae; it resembles the closely related wild seeded bananas, "Musa acuminata" and "Musa balbisiana". Its scientific name is "Musa textilis". Within the genus "Musa", it is placed in section "Callimusa" (now including the former section "Australimusa"), members of which have a diploid chromosome number of 2n = 20.
History.
Before synthetic textiles came into use, "M. textilis" was a major source of high quality fiber: soft, silky and fine. Europeans first came into contact with it when Magellan made land in the Philippines in 1521, as the natives were cultivating it and utilizing it in bulk for textiles already. By 1897, the Philippines were exporting almost 100,000 tons of abacá, and it was one of the three biggest cash crops, along with tobacco and sugar. In fact, from 1850 through the end of the 19th century, sugar or abacá alternated with each other as the biggest export crop of the Philippines. This 19th century trade was predominantly with the United States and the making of ropes was done mainly in New England, although in time the rope-making was moved back to the Philippines. Excluding the Philippines, abacá was first cultivated on a large scale in Sumatra in 1925 under the Dutch, who had observed its cultivation in the Philippines for cordage since the nineteenth century, followed up by plantings in Central America in 1929 sponsored by the U.S. Department of Agriculture. It also was transplanted into India and Guam. Commercial planting began in 1930 in British North Borneo; with the commencement of World War II, the supply from the Philippines was eliminated by the Japanese. After the war, the U.S. Department of Agriculture started production in Panama, Costa Rica, Honduras, and Guatemala. Today, abacá is produced commercially in only three countries: Philippines, Ecuador, and Costa Rica. The Philippines produces between 85% and 95% of the world's abacá, and the production employs 1.5 million people. Production has declined because of virus diseases.
Ancestors of the modern abaca might have originated from the Eastern Philippines where there are lot of rains (no pronounced dry season), in fact wild type of abaca can still be found in the interior forests of Catanduanes Island which is often not cultivated. Today, Catanduanes has many other modern kinds of abaca which are more competitive. For many years, breeders from various research institutions have made the cultivated varieties of Catanduanes Island even more competitive in local and international markets. This results in the optimum production of the island which had a consistent highest production throughout the archipelago.
Uses.
Due to its strength, it is a sought after product and is the strongest of the natural fibers. It is used by the paper industry for such specialty uses such as teabags, banknotes and decorative papers. It can be used to make handcrafts such as bags, carpets, clothing and furniture. Abacá rope is very durable, flexible and resistant to salt water damage, allowing its use in hawsers, ship's lines and fishing nets. A rope can require to break. Abacá fiber was once used primarily for rope, but this application is now of minor significance. Lupis is the finest quality of abacá. Sinamay is woven chiefly from abacá.
Textiles.
The inner fibers are used in the making of hats, including the "Manila hats," hammocks, matting, cordage, ropes, coarse twines, and types of canvas. It is called Manila hemp in the market although it is unlike true hemp, and is lso known as Cebu hemp and Davao hemp. Abacá cloth is found in museum collections around the world, like the Boston Museum of Fine Arts and the Textile Museum of Canada.
Cultivation.
The plant is normally grown in well-drained loamy soil, using rhizomes planted at the start of the rainy season. In addition, new plants can be started by seeds. Growers harvest abacá fields every three to eight months after an initial growth period of 12–25 months. Harvesting is done by removing the leaf-stems after flowering but before fruit appears. The plant loses productivity between 15 and 40 years. The slopes of volcanoes provide a preferred growing environment. Harvesting generally includes several operations involving the leaf sheaths:
When the processing is complete, the bundles of fiber are pale and lustrous with a length of .
In Costa Rica, more modern harvest and drying techniques are being developed to accommodate the very high yields obtained there.
More than 80% of modern abaca production comes from the Philippines. Bicol is the top producing region while Catanduanes Island remains the top producing province in the Philippines. The Philippine Rural Development Program (PRDP) and the Department of Agriculture reported that in 2009-2013, Bicol Region had 39% share of Philippine abaca producution while overwhelming 92% comes from Catanduanes Island. Eastern Visayas, the second largest producer had 24% and the Davao Region, the third largest producer had 11% of the total production.
Pathogens.
Abacá is vulnerable to a number of pathogens, notably abaca bunchy top virus and abaca bract mosaic virus.

</doc>
<doc id="2474" url="https://en.wikipedia.org/wiki?curid=2474" title="Abaddon">
Abaddon

The Hebrew term Abaddon (, ""), and its Greek equivalent Apollyon (, "Apollyon"), appears in the Bible as both a place of destruction and as the name of an angel. In the Hebrew Bible, "abaddon" is used with reference to a bottomless pit, often appearing alongside the place שאול ("sheol"), meaning the realm of the dead. In the New Testament Book of Revelation, an angel called Abaddon is described as the king of an army of locusts; his name is first transcribed in Greek (Revelation 9:11—"whose name in Hebrew is Abaddon" (Ἀβαδδὼν)), and then translated ("which in Greek means the Destroyer" (Ἀπολλύων, "Apollyon")). The Latin Vulgate and the Douay Rheims Bible have additional notes (not present in the Greek text), "in Latin Exterminans", "exterminans" being the Latin word for "destroyer".
Etymology.
According to the Brown Driver Briggs lexicon, the Hebrew "abaddon" (Hebrew: אבדון; avadon) is an intensive form of the Semitic root and verb stem "abad" (אָבַד) "perish" (transitive "destroy"), which occurs 184 times in the Hebrew Bible. The Septuagint, an early Greek translation of the Hebrew Bible, renders "abaddon" as "ἀπώλεια", while the Greek "Apollyon" comes from "apollumi" (ἀπόλλυμι), "to destroy". The Greek term "Apollyon" (Ἀπολλύων, "the destroyer"), is the active participle of "apollumi" (ἀπόλλυμι, "to destroy"), and is not used as a name in classical Greek texts.
Judaism.
Hebrew Bible.
The term "abaddon" appears six times in the Masoretic text of the Hebrew Bible; "abaddon" means destruction or "place of destruction", or the realm of the dead, and is accompanied by Sheol. 
Second Temple era texts.
The text of the Thanksgiving Hymns—which was found in the Dead Sea Scrolls—tells of "the Sheol of Abaddon" and of the "torrents of Belial ha burst into Abaddon". The "Biblical Antiquities" (misattributed to Philo) mentions Abaddon as a place (destruction) rather than an individual. Abaddon is also one of the compartments of Gehenna. By extension, it can mean an underworld abode of lost souls, or Gehenna.
Rabbinical literature.
In some legends, Abaddon is identified as a realm where the damned lie in fire and snow, one of the places in Gehenna that Moses visited.
Christianity.
New Testament.
The Christian scriptures contain the first known depiction of "Abaddon" as an individual entity instead of a place.
In Revelation 9:11, Abaddon is described as "Destroyer", the angel of the abyss, and as the king of a plague of locusts resembling horses with crowned human faces, women's hair, lions' teeth, wings, iron breast-plates, and a tail with a scorpion's stinger that torments for five months anyone who does not have the seal of God on their foreheads.
The symbolism of Revelation 9:11 leaves the identity of Abaddon open to interpretation. Protestant commentator Matthew Henry (1708) believed Abaddon to be the Antichrist, whereas the Jamieson-Fausset-Brown Commentary (1871) and Henry H. Halley (1922) identified the angel as Satan. Latter-Day Saints believe that the use of "Abaddon" in Revelation 9 refers to the devil.
In contrast, the Methodist publication "The Interpreter's Bible" states: "Abaddon, however, is an angel not of Satan but of God, performing his work of destruction at God's bidding," citing the context at Revelation chapter 20, verses 1 through 3. Jehovah's Witnesses as well cite Revelation 20:1-3 where the angel having "the key of the abyss" is actually shown to be a representative of God, one from heaven, and, rather than being "satanic," is the one that binds Satan and hurls him into the abyss; concluding that "Abaddon" is another name for Jesus Christ after his resurrection. 
Gnostic Texts.
In the 3rd century Acts of Thomas, Abaddon is the name of a demon, or the devil himself.
Abaddon is given particularly important roles in two sources, a homily entitled "The Enthronement of Abbaton" by pseudo-Timothy of Alexandria, and the Apocalypse of Bartholomew. In the homily by Timothy, Abbaton was first named "Muriel", and had been given the task by God of collecting the earth which would be used in the creation of Adam. Upon completion of this task, the angel was appointed as a guardian. Everyone, including the angels, demons, and corporeal entities feared him. Abbaton was promised that any who venerated him in life could be saved. Abaddon is also said to have a prominent role in the Last Judgement, as the one who will take the souls to the Valley of Josaphat. He is described in the Apocalypse of Bartholomew as being present in the Tomb of Jesus at the moment of his resurrection.

</doc>
<doc id="2475" url="https://en.wikipedia.org/wiki?curid=2475" title="Abadeh">
Abadeh

Abadeh (, also Romanized as Ābādeh) is a city in and the capital of Abadeh County, in Fars Province, Iran. Abadeh is situated at an elevation of in a fertile plain on the high road between Isfahan and Shiraz, from the former and from the latter. At the 2006 census, its population was 52,042, in 14,184 families. As of 2009, the population was estimated to be 59042.
It is the largest city in the Northern Fars Region (South Central-Iran), which is famed for its carved wood-work, made of the wood of pear and box trees. Sesame oil, castor oil, grain, and various fruits are also produced there. The area is famous for its Abadeh rugs.
An interesting fact is that Abadeh is closer, road-distance-wise to 4 provincial capitals of Isfahan (193 km), Yasuj (197 km), Yazd (217 km), and Shahrekord (237 km) compared to the distance to the provincial capital of its corresponding province, Shiraz (260 km).
Landmarks and crafts.
Abadeh historical monuments include Emirate Kolah Farangi, Tymcheh Sarafyan and Khaje tomb, located in the Khoja mountains.
Abadeh crafts can be embroidered in cotton. The town also produces Abadeh rugs.
Transportation.
Expressway 65 passes through Abadeh. This situation helps Abadeh to improve its capabilities compared to the neighboring city, Eqlid. Road 78 makes connections from Abadeh to Abarkuh, Yazd Eqlid and Yasuj. It has a junction with Abadeh Shiraz Expressway 24 km south of the city. A road starts from Abadeh Ring Road to Soqad and Semirom, Road 55.
The railroad from Isfahan to Shiraz passes Abadeh and there are train services at Abadeh Railway Station to Shiraz, Esfahan, Tehran and Mashad. Abadeh Airport (OISA) was planned to be built in mid 1990s.
Sport.
Abadeh's main sport is Football, like the rest of the country. The main stadium is Takhti Stadium located in Mo'allem Square. The main team in Abadeh is Behineh Rahbar Abadeh F.C. which is currently playing in Iran Football's 3rd Division after finishing first in Fars Provincial League (FPL) last year. It played in Hazfi Cup 2010-11 reaching the fourth round.
Air Defense Base.
Iran announced in 2012 the construction of the largest air defense site in the southern Iranian city of Abadeh.
Climate.
Abadeh features a continental semi-arid climate (Köppen "BSk") with extreme heat and dryness over summer, and cold (extreme at times) and wet winter, with huge variations between daytime and nighttime throughout the year.

</doc>
<doc id="2476" url="https://en.wikipedia.org/wiki?curid=2476" title="Abae">
Abae

Abae (, "Abai") is an ancient town in the northeastern corner of Phocis, in Greece. It was famous in antiquity for its oracle of Apollo Abaeus, one of those consulted by Croesus, king of Lydia, and Mardonius, among others.
History.
It was rich in treasures, but was destroyed by the Persians in the invasion of Xerxes in 480 BCE, and a second time by the Boeotians and remained in a ruined state.It was rebuilt by Hadrian.
The oracle was, however, still consulted, e.g. by the Thebans before Leuctra in 371 BCE. The temple, along with the village of the same name, may have escaped destruction during the Third Sacred War (355–346 BCE), due to the respect given to the inhabitants; however it was in a very dilapidated state when seen by Pausanias in the 2nd century CE, though some restoration, as well as the building of a new temple, was undertaken by Emperor Hadrian.
The sanctity of the shrine ensured certain privileges to the people of Abae, and these were confirmed by the Romans. The Persians did not reflect this opinion and would destroy all the temples that they overcame, Abae included. The Greek pledged to not rebuild them as a memorial of the ravages of the Persians.
Among the most exciting recent archaeological discoveries in Greece is the recognition that the sanctuary site near the modern village of Kalapodi is not only the site of the oracle of Apollon at Abai but that it was in constant use for cult practices from early Mycenaean times to the Roman period. It is thus the first site where the archaeology confirms the continuity of Mycenaean and Classical Greek religion, which has been inferred from the presence of the names of Classical Greek divinities on Linear B texts from Pylos and Knossos.
The fortified site described below, originally identified as Abae by Colonel William Leake in the 19th century, is much more likely to be that of the Sanctuary of Artemis at Hyampolis.
The polygonal walls of the acropolis may still be seen in a fair state of preservation on a circular hill standing about above the little plain of Exarcho; one gateway remains, and there are also traces of town walls below. The temple site was on a low spur of the hill, below the town. An early terrace wall supports a precinct in which are a stoa and some remains of temples; these were excavated by the British School at Athens in 1894, but little was found.

</doc>
<doc id="2477" url="https://en.wikipedia.org/wiki?curid=2477" title="Abakan">
Abakan

Abakan (; Khakas: , "Ağban") is the capital city of the Republic of Khakassia, Russia, located in the central part of Minusinsk Depression, at the confluence of the Yenisei and Abakan Rivers. As of the 2010 Census, it had a population of 165,214—a slight increase over 165,197 recorded during the 2002 Census and a further increase from 154,092 recorded during the 1989 Census.
History.
Abakansky "ostrog" (), also known as Abakansk (), was built at the mouth of the Abakan River in 1675. In the 1780s, the "selo" of Ust-Abakanskoye () was established in this area. It was granted town status and given its current name on April 30, 1931.
Chinese exiles.
In 1940, Russian construction workers found ancient ruins during the construction of a highway between Abakan and Askiz. When the site was excavated by Soviet archaeologists in 1941–1945, they realized that they had discovered a building absolutely unique for the area: a large (1500 square meters) Chinese-style, likely Han Dynasty era (206 BCE–220 CE) palace. The identity of the high-ranking personage who lived luxuriously in Chinese style, far outside of the borders of the Han Empire, has remained a matter for discussion ever since. Russian archaeologist L.A. Yevtyukhova surmised, based on circumstantial evidence, that the palace may have been the residence of Li Ling, a Chinese general who had been defeated by the Xiongnu in 99 BCE, and defected to them as a result. While this opinion has remained popular, other views have been expressed as well. More recently, for example, it was claimed by A.A. Kovalyov as the residence of Lu Fang (), a Han throne pretender from the Guangwu era.
Lithuanian exiles.
In the late 18th and during the 19th century, Lithuanian participants in the 1794, 1830–1831, and 1863 rebellions against the Russian rule were exiled to Abakan. A group of camps was established where prisoners were forced to work in the coal mines. After Stalin's death, Lithuanian exiles from the nearby settlements moved in.
Administrative and municipal status.
Abakan is the capital of the republic. Within the framework of administrative divisions, it is incorporated as the City of Abakan—an administrative unit with the status equal to that of the districts As a municipal division, the City of Abakan is incorporated as Abakan Urban Okrug.
Economy.
The city has a river port, industry enterprises, Katanov State University of Khakasia, and three theatres. Furthermore, it has a commercial center that produces footwear, foodstuffs, and metal products.
Transportation.
Abakan (together with Tayshet) was a terminal of the major Abakan-Taishet Railway. Now it is an important railway junction.
The city is served by the Abakan International Airport.
Military.
The 100th Air Assault Brigade of the Russian Airborne Troops was based in the city until ca. 1996.
Sites.
Abakan's sites of interest include Holy Transfiguration Cathedral, "Good Angel of Peace" sculpture, park of topiary art, and many others.
Sports.
Bandy is the one of the most popular sports in the city. Sayany-Khakassia was playing in the top-tier Super League in the 2012–13 season but was relegated for the 2013–14 season. Russian Government Cup was played here in 1988 and in 2012.
Climate.
Abakan has a continental climate with semi-arid influences (Köppen climate classification "BSk"). Temperature differences between seasons are extreme, which is typical for Siberia. Precipitation is concentrated in the summer and is less common because of rain shadow from nearby mountains.

</doc>
<doc id="2482" url="https://en.wikipedia.org/wiki?curid=2482" title="Arc de Triomphe">
Arc de Triomphe

The Arc de Triomphe de l'Étoile (, "Triumphal Arch of the Star") is one of the most famous monuments in Paris. It stands in the centre of the Place Charles de Gaulle (originally named "Place de l'Étoile"), at the western end of the Champs-Élysées. It should not be confused with a smaller arch, the Arc de Triomphe du Carrousel, which stands west of the Louvre. The Arc de Triomphe honours those who fought and died for France in the French Revolutionary and the Napoleonic Wars, with the names of all French victories and generals inscribed on its inner and outer surfaces. Beneath its vault lies the Tomb of the Unknown Soldier from World War I.
The Arc de Triomphe is the linchpin of the "Axe historique" (historic axis) – a sequence of monuments and grand thoroughfares on a route which runs from the courtyard of the Louvre to the Grande Arche de la Défense. The monument was designed by Jean Chalgrin in 1806 and its iconographic program pits heroically nude French youths against bearded Germanic warriors in chain mail. It set the tone for public monuments with triumphant patriotic messages.
The monument stands in height, wide and deep. The large vault is high and wide. The small vault is high and wide. Its design was inspired by the Roman Arch of Titus. The Arc de Triomphe is built on such a large scale that, three weeks after the Paris victory parade in 1919 (marking the end of hostilities in World War I), Charles Godefroy flew his Nieuport biplane through it, with the event captured on newsreel.
It was the tallest triumphal arch in existence until the completion of the Monumento a la Revolución in Mexico City in 1938, which is high. The Arch of Triumph in Pyongyang, completed in 1982, is modelled on the Arc de Triomphe and is slightly taller at .
History.
The Arc is located on the right bank of the Seine at the centre of a dodecagonal configuration of twelve radiating avenues. It was commissioned in 1806 after the victory at Austerlitz by Emperor Napoleon at the peak of his fortunes. Laying the foundations alone took two years and, in 1810, when Napoleon entered Paris from the west with his bride Archduchess Marie-Louise of Austria, he had a wooden mock-up of the completed arch constructed. The architect, Jean Chalgrin, died in 1811 and the work was taken over by Jean-Nicolas Huyot. During the Bourbon Restoration, construction was halted and it would not be completed until the reign of King Louis-Philippe, between 1833 and 1836, by the architects Goust, then Huyot, under the direction of Héricart de Thury. On 15 December 1840, brought back to France from Saint Helena, Napoleon's remains passed under it on their way to the Emperor's final resting place at the Invalides. Prior to burial in the Panthéon, the body of Victor Hugo was displayed under the Arc during the night of 22 May 1885.
The sword carried by the "Republic" in the "Marseillaise" relief broke off on the day, it is said, that the Battle of Verdun began in 1916. The relief was immediately hidden by tarpaulins to conceal the accident and avoid any undesired ominous interpretations.
On 7 August 1919, Charles Godefroy successfully flew his biplane under the Arc. Jean Navarre was the pilot who was tasked to make the flight, but he died on 10 July 1919 when he crashed near Villacoublay while training for the flight.
Following its construction, the "Arc de Triomphe" became the rallying point of French troops parading after successful military campaigns and for the annual Bastille Day Military Parade. Famous victory marches around or under the Arc have included the Germans in 1871, the French in 1919, the Germans in 1940, and the French and Allies in 1944 and 1945. A United States postage stamp of 1945 shows the "Arc de Triomphe" in the background as victorious American troops march down the Champs-Élysées and U.S. airplanes fly overhead on 29 August 1944. After the interment of the Unknown Soldier, however, all military parades (including the aforementioned post-1919) have avoided marching through the actual arch. The route taken is up to the arch and then around its side, out of respect for the tomb and its symbolism. Both Hitler in 1940 and de Gaulle in 1944 observed this custom.
By the early 1960s, the monument had grown very blackened from coal soot and automobile exhaust, and during 1965–1966 it was cleaned through bleaching.
In the prolongation of the Avenue des Champs-Élysées, a new arch, the Grande Arche de la Défense, was built in 1982, completing the line of monuments that forms Paris's "Axe historique". After the "Arc de Triomphe du Carrousel" and the "Arc de Triomphe de l'Étoile", the "Grande Arche" is the third arch built on the same perspective.
In 1995, the Armed Islamic Group of Algeria placed a bomb near the Arc de Triomphe which wounded 17 people as part of a campaign of bombings.
Design.
The astylar design is by Jean Chalgrin (1739–1811), in the Neoclassical version of ancient Roman architecture (see, for example, the triumphal Arch of Titus). Major academic sculptors of France are represented in the sculpture of the "Arc de Triomphe": Jean-Pierre Cortot; François Rude; Antoine Étex; James Pradier and Philippe Joseph Henri Lemaire. The main sculptures are not integral friezes but are treated as independent trophies applied to the vast ashlar masonry masses, not unlike the gilt-bronze appliqués on Empire furniture. The four sculptural groups at the base of the Arc are "The Triumph of 1810" (Cortot), "Resistance" and "Peace" (both by Antoine Étex) and the most renowned of them all, "Departure of the Volunteers of 1792" commonly called "La Marseillaise" (François Rude). The face of the allegorical representation of France calling forth her people on this last was used as the belt buckle for the honorary rank of Marshal of France. Since the fall of Napoleon (1815), the sculpture representing "Peace" is interpreted as commemorating the "Peace of 1815".
In the attic above the richly sculptured frieze of soldiers are 30 shields engraved with the names of major French victories in the French Revolution and Napoleonic wars. The inside walls of the monument list the names of 660 people, among which are 558 French generals of the First French Empire; The names of those generals killed in battle are underlined. Also inscribed, on the shorter sides of the four supporting columns, are the names of the major French victories in the Napoleonic Wars. The battles that took place in the period between the departure of Napoleon from Elba to his final defeat at Waterloo are not included.
For four years from 1882 to 1886, a monumental sculpture by Alexandre Falguière topped the arch. Titled "Le triomphe de la Révolution" (the Triumph of the Revolution), it depicted a chariot drawn by horses preparing "to crush Anarchy and Despotism". It remained there only four years before falling in ruins.
Inside the monument, a permanent exhibition conceived by the artist Maurice Benayoun and the architect Christophe Girault opened in February 2007. The steel and new media installation interrogates the symbolism of the national monument, questioning the balance of its symbolic message during the last two centuries, oscillating between war and peace.
Tomb of the Unknown Soldier.
Beneath the Arc is the Tomb of the Unknown Soldier from World War I. Interred on Armistice Day 1920, it has the first eternal flame lit in Western and Eastern Europe since the Vestal Virgins' fire was extinguished in the fourth century. It burns in memory of the dead who were never identified (now in both world wars).
A ceremony is held at the Tomb of the Unknown Soldier every 11 November on the anniversary of the armistice signed by the Entente Powers and Germany in 1918. It was originally decided on 12 November 1919 to bury the unknown soldier's remains in the Panthéon, but a public letter-writing campaign led to the decision to bury him beneath the Arc de Triomphe. The coffin was put in the chapel on the first floor of the Arc on 10 November 1920, and put in its final resting place on 28 January 1921. The slab on top bears the inscription ("Here lies a French soldier who died for the fatherland 1914–1918").
In 1961, American President John F. Kennedy and First Lady Jacqueline Kennedy paid their respects at the Tomb of the Unknown Soldier, accompanied by French President Charles de Gaulle. After the 1963 assassination of President Kennedy, Mrs Kennedy remembered the eternal flame at the Arc de Triomphe and requested that an eternal flame be placed next to her husband's grave at Arlington National Cemetery in Virginia. President Charles de Gaulle went to Washington to attend the state funeral, and witnessed Jacqueline Kennedy lighting the eternal flame that had been inspired by her visit to France.
Access.
The "Arc de Triomphe" is accessible by the RER and Métro, with exit at the Charles de Gaulle—Étoile station. Because of heavy traffic on the roundabout of which the Arc is the centre, it is recommended that pedestrians use one of two underpasses located at the "Champs Élysées" and the "Avenue de la Grande Armée". A lift will take visitors almost to the top – to the attic, where there is a small museum which contains large models of the Arc and tells its story from the time of its construction. Another 46 steps remain to climb in order to reach the top, the "terrasse", from where one can enjoy a panoramic view of Paris.

</doc>
<doc id="2483" url="https://en.wikipedia.org/wiki?curid=2483" title="April 21">
April 21


</doc>
<doc id="2484" url="https://en.wikipedia.org/wiki?curid=2484" title="ATM">
ATM

ATM or atm may refer to:

</doc>
<doc id="2487" url="https://en.wikipedia.org/wiki?curid=2487" title="Amazonite">
Amazonite

Amazonite (sometimes called "Amazon stone") is a green variety of microcline feldspar.
The name is taken from that of the Amazon River, from which certain green stones were formerly obtained, but it is doubtful whether green feldspar occurs in the Amazon area.
Amazonite is a mineral of limited occurrence. Formerly it was obtained almost exclusively from the area of Miass in the Ilmensky Mountains, 50 miles southwest of Chelyabinsk, Russia, where it occurs in granitic rocks. More recently, high-quality crystals have been obtained from Pike's Peak, Colorado, where it is found associated with smoky quartz, orthoclase, and albite in a coarse granite or pegmatite. Crystals of amazonite can also be found in Crystal Park, El Paso County, Colorado. Other locations in the United States which yield amazonite include the Morefield Mine in Amelia, Virginia. It is also found in pegmatite in Madagascar and in Brazil.
Because of its bright green color when polished, amazonite is sometimes cut and used as a cheap gemstone, although it is easily fractured, and loses its gloss due to its softness.
For many years, the source of amazonite's color was a mystery. Naturally, many people assumed the color was due to copper because copper compounds often have blue and green colors. More recent studies suggest that the blue-green color results from small quantities of lead and water in the feldspar.

</doc>
<doc id="2490" url="https://en.wikipedia.org/wiki?curid=2490" title="Ambrosius Bosschaert">
Ambrosius Bosschaert

Ambrosius Bosschaert the Elder (18 January 1573 – 1621) was a still life painter of the Dutch Golden Age.
Biography.
He was born in Antwerp, where he started his career, but he spent most of it in Middelburg (1587–1613), where he moved with his family because of the threat of religious persecution. He specialized in painting still lifes with flowers, which he signed with the monogram AB (the B in the A). At the age of twenty-one, he joined the city’s Guild of Saint Luke and later became dean. Not long after, Bosschaert had married and established himself as a leading figure in the fashionable floral painting genre. 
He had three sons who all became flower painters; Ambrosius II, Johannes and Abraham. His brother-in-law Balthasar van der Ast also lived and worked in his workshop and moved with him on his travels. Bosschaert later worked in Amsterdam (1614), Bergen op Zoom (1615–1616), Utrecht (1616–1619), and Breda (1619). In 1619 when he moved to Utrecht, his brother-in-law van der Ast entered the Utrecht Guild of St. Luke, where the renowned painter Abraham Bloemaert had just become dean. The painter Roelandt Savery (1576–1639) entered the St. Luke’s guild in Utrecht at about the same time. Savery had considerable influence on the Bosschaert dynasty. When Bosschaert died in The Hague while on commission there for a flower piece, Balthasar van der Ast took over running his workshop and pupils.
Style.
His bouquets were painted symmetrically and with scientific accuracy in small dimensions and normally on copper. They sometimes included symbolic and religious meanings. At the time of his death, Bosschaert was working on an important commission in the Hague. That piece is now in the collection in Stockholm. Bosschaert became one of the first artists to specialize in still life painting, and he started a tradition of painting detailed flower bouquets, which typically included tulips and roses. Thanks to the booming seventeenth-century Dutch art market, he became highly successful, as the inscription on one of his paintings attests.
Legacy.
His sons and his pupil and brother-in-law, Balthasar van der Ast, were among those to uphold the Bosschaert dynasty which continued until the mid-17th century.
It may not be a coincidence that this trend coincided with a national obsession with exotic flowers which made flower portraits highly sought after.
Although he was highly in demand, he did not create many pieces because he was also employed as an art dealer.

</doc>
<doc id="2493" url="https://en.wikipedia.org/wiki?curid=2493" title="Anthroposophy">
Anthroposophy

Anthroposophy, a philosophy founded by Rudolf Steiner, postulates the existence of an objective, intellectually comprehensible spiritual world that is accessible by direct experience through inner development. More specifically, it aims to develop faculties of perceptive imagination, inspiration and intuition through the cultivation of a form of thinking independent of sensory experience, and to present the results thus derived in a manner subject to rational verification. Anthroposophy aims to attain in its study of spiritual experience the precision and clarity attained by the natural sciences in their investigations of the physical world. The philosophy has double roots in German idealism and German mysticism and was initially expressed in language drawn from Theosophy.
Anthroposophical ideas have been applied practically in many areas including Steiner/Waldorf education, special education (most prominently through the Camphill Movement), biodynamic agriculture, medicine, ethical banking, organizational development, and the arts. The Anthroposophical Society has its international center at the Goetheanum in Dornach, Switzerland.
Modern analysts, including Michael Shermer, have termed anthroposophy's application in areas such as medicine, biology and biodynamic agriculture pseudoscience.
History.
The early work of the founder of anthroposophy, Rudolf Steiner, culminated in his "Philosophy of Freedom" (also translated as "The Philosophy of Spiritual Activity" and "Intuitive Thinking as a Spiritual Path"). Here, Steiner developed a concept of free will based on inner experiences, especially those that occur in the creative activity of independent thought.
By the beginning of the twentieth century, Steiner's interests turned to explicitly spiritual areas of research. His work began to interest others interested in spiritual ideas; among these was the Theosophical Society. From 1900 on, thanks to the positive reception given to his ideas, Steiner focused increasingly on his work with the Theosophical Society becoming the secretary of its section in Germany in 1902. During the years of his leadership, membership increased dramatically, from a few individuals to sixty-nine Lodges.
By 1907, a split between Steiner and the mainstream Theosophical Society had begun to become apparent. While the Society was oriented toward an Eastern and especially Indian approach, Steiner was trying to develop a path that embraced Christianity and natural science. The split became irrevocable when Annie Besant, then president of the Theosophical Society, began to present the child Jiddu Krishnamurti as the reincarnated Christ. Steiner strongly objected and considered any comparison between Krishnamurti and Christ to be nonsense; many years later, Krishnamurti also repudiated the assertion. Steiner's continuing differences with Besant led him to separate from the Theosophical Society Adyar; he was followed by the great majority of the membership of the Theosophical Society's German Section, as well as members of other national sections.
By this time, Steiner had reached considerable stature as a spiritual teacher. He spoke about what he considered to be his direct experience of the Akashic Records (sometimes called the "Akasha Chronicle"), thought to be a spiritual chronicle of the history, pre-history, and future of the world and mankind. In a number of works, Steiner described a path of inner development he felt would let anyone attain comparable spiritual experiences. Sound vision could be developed, in part, by practicing rigorous forms of ethical and cognitive self-discipline, concentration, and meditation; in particular, a person's moral development must precede the development of spiritual faculties.
In 1912, the Anthroposophical Society was founded. After World War I, the Anthroposophical movement took on new directions. Projects such as schools, centers for those with special needs, organic farms and medical clinics were established, all inspired by anthroposophy.
In 1923, faced with differences between older members focusing on inner development and younger members eager to become active in the social transformations of the time, Steiner refounded the Society in an inclusive manner and established a School for Spiritual Science. As a spiritual basis for the refounded movement, Steiner wrote a "" which remains a central meditative expression of anthroposophical ideas.
Steiner died just over a year later, in 1925. The Second World War temporarily hindered the anthroposophical movement in most of Continental Europe, as the Anthroposophical Society and most of its daughter movements (e.g. Steiner/Waldorf education) were banned by the National Socialists (Nazis); virtually no anthroposophists ever joined the National Socialist Party.
By 2007, national branches of the Anthroposophical Society had been established in fifty countries, and about 10,000 institutions around the world were working on the basis of anthroposophy. In the same year, the Anthroposophical Society was called the "most important esoteric society in European history."
Etymology.
"Anthroposophy" is an amalgam of the Greek terms ("anthropos" = "human") and ("sophia" = "wisdom"). It is listed by Nathan Bailey (1742) as meaning "the knowledge of the nature of man" (OED). Authors whose usage of the term predates Steiner's include occultist Agrippa von Nettesheim, alchemist Thomas Vaughan ("Anthroposophia Theomagica"), and philosopher Robert Zimmermann.
Steiner began using the term in the early 1900s as an alternative to the term "theosophy" (divine wisdom), a term central to the Theosophical Society, with which Steiner was associated at the time, and to a long tradition of European esotericists. Steiner probably first encountered the word "anthroposophy" in the work of Zimmermann, some of whose lectures in the University of Vienna he had attended while a student.
Central ideas.
Spiritual knowledge and freedom.
Anthroposophical proponents aim to extend the clarity of the scientific method to phenomena of human soul-life and to spiritual experiences. This requires developing new faculties of objective spiritual perception, which Steiner maintained was possible for humanity today. The steps of this process of inner development he identified as consciously achieved "imagination", "inspiration" and "intuition". Steiner believed results of this form of spiritual research should be expressed in a way that can be understood and evaluated on the same basis as the results of natural science: "The anthroposophical schooling of thinking leads to the development of a non-sensory, or so-called supersensory consciousness, whereby the spiritual researcher brings the experiences of this realm into ideas, concepts, and expressive language in a form which people can understand who do not yet have the capacity to achieve the supersensory experiences necessary for individual research."
Steiner hoped to form a spiritual movement that would free the individual from any external authority: "The most important problem of all human thinking is this: to comprehend the human being as a personality grounded in him or herself." For Steiner, the human capacity for rational thought would allow individuals to comprehend spiritual research on their own and bypass the danger of dependency on an authority.
Steiner contrasted the anthroposophical approach with both conventional mysticism, which he considered lacking the clarity necessary for exact knowledge, and natural science, which he considered arbitrarily limited to investigating the outer world.
Nature of the human being.
In "Theosophy", Steiner suggested that human beings unite a physical body of a nature common to (and that ultimately returns to) the inorganic world; a life body (also called the etheric body), in common with all living creatures (including plants); a bearer of sentience or consciousness (also called the astral body), in common with all animals; and the ego, which anchors the faculty of self-awareness unique to human beings.
Anthroposophy describes a broad evolution of human consciousness. Early stages of human evolution possess an intuitive perception of reality, including a clairvoyant perception of spiritual realities. Humanity has progressively evolved an increasing reliance on intellectual faculties and a corresponding loss of intuitive or clairvoyant experiences, which have become atavistic. The increasing intellectualization of consciousness, initially a progressive direction of evolution, has led to an excessive reliance on abstraction and a loss of contact with both natural and spiritual realities. However, to go further requires new capacities that combine the clarity of intellectual thought with the imagination, and beyond this with consciously achieved inspiration and intuitive insights.
Anthroposophy speaks of the reincarnation of the human spirit: that the human being passes between stages of existence, incarnating into an earthly body, living on earth, leaving the body behind and entering into the spiritual worlds before returning to be born again into a new life on earth. After the death of the physical body, the human spirit recapitulates the past life, perceiving its events as they were experienced by the objects of its actions. A complex transformation takes place between the review of the past life and the preparation for the next life. The individual's karmic condition eventually leads to a choice of parents, physical body, disposition, and capacities that provide the challenges and opportunities that further development requires, which includes karmically chosen tasks for the future life.
Steiner described some conditions that determine the interdependence of a person's lives, or karma.
Evolution.
The anthroposophical view of evolution considers all animals to have evolved from an early, unspecialized form. As the least specialized animal, human beings have maintained the closest connection to the archetypal form; contrary to the Darwinian conception of human evolution, all other animals "devolve" from this archetype. The spiritual archetype originally created by spiritual beings was devoid of physical substance; only later did this descend into material existence on Earth. In this view, human evolution has accompanied the Earth's evolution throughout the existence of the Earth.
Anthroposophy took over from Theosophy a complex system of cycles of world development and human evolution. The evolution of the world is said to have occurred in cycles. The first phase of the world consisted only of heat. In the second phase, a more active condition, light, and a more condensed, gaseous state separate out from the heat. In the third phase, a fluid state arose, as well as a sounding, forming energy. In the fourth (current) phase, solid physical matter first exists. This process is said to have been accompanied by an evolution of consciousness which led up to present human culture.
Ethics.
The anthroposophical view is that good is found in the balance between two polar, generally evil influences on world and human evolution. Two spiritual adversaries endeavour to tempt and corrupt humanity: these are often described through their mythological embodiments, Lucifer and his counterpart Ahriman, which have both positive and negative aspects. Lucifer is the light spirit, which "plays on human pride and offers the delusion of divinity", but also motivates creativity and spirituality; Ahriman is the dark spirit, which tempts human beings to "...deny hei link with divinity and to live entirely on the material plane", but also stimulates intellectuality and technology. Both figures exert a negative effect on humanity when their influence becomes misplaced or one-sided, yet their influences are necessary for human freedom to unfold.
Each human being has the task to find a balance between these opposing influences, and each is helped in this task by the mediation of the "Representative of Humanity", also known as the Christ being, a spiritual entity who stands between and harmonizes the two extremes.
Applications.
Applications of anthroposophy include:
Steiner/Waldorf education.
This is a pedagogical movement with over 1000 Steiner or Waldorf schools (the latter name stems from the first such school, founded in Stuttgart in 1919) located in some 60 countries; the great majority of these are independent (private) schools. Sixteen of the schools have been affiliated with the United Nations' UNESCO Associated Schools Project Network, which sponsors education projects that foster improved quality of education throughout the world, in particular in terms of its ethical, cultural, and international dimensions. Waldorf schools receive full or partial governmental funding in some European nations, Australia and in parts of the United States (as Waldorf method public or charter schools).
The schools are located in a wide variety of communities and cultures: from the impoverished "favelas" of São Paulo to the wealthy suburbs of New York City; in India, Egypt, Australia, the Netherlands, Mexico and South Africa. Though most of the early Waldorf schools were teacher-founded, the schools today are usually initiated and later supported by an active parent community. Waldorf education is one of the most visible practical applications of an anthroposophical view and understanding of the human being and has been characterized as "the leader of the international movement for a New Education,"
Biodynamic agriculture.
Biodynamic agriculture, the first intentional form of organic farming, began in the 1920s when Rudolf Steiner gave a series of lectures since published as "Agriculture". Steiner is considered one of the founders of the modern organic farming movement.
Anthroposophical medicine.
Steiner gave several series of lectures to physicians and medical students. Out of those grew a complementary medical movement intending to "extend the knowledge gained through the methods of the natural sciences of the present age with insights from spiritual science." This movement now includes hundreds of M.D.s, chiefly in Europe and North America, and has its own clinics, hospitals, and medical schools.
One of the most studied applications has been the use of mistletoe extracts in cancer therapy. The extracts are generally no longer used to reduce or inhibit tumor growth, for which verifiable results have been found in vitro and in animal studies but not in humans, but instead to improve the patients' quality of life and to reduce tumor-induced symptoms and the side-effects of chemotherapy and radiotherapy. According to the National Cancer Institute, "Mistletoe extract has been shown to kill cancer cells in the laboratory and to affect the immune system. However, there is limited evidence that mistletoe's effects on the immune system help the body fight cancer... At present, the use of mistletoe cannot be recommended outside the context of well-designed clinical trials."
Several pharmaceutical companies have grown out of anthroposophical medicine, including Weleda, Wala, and Dr. Hauschka.
Special needs education and services.
In 1922, Ita Wegman founded an anthroposophical center for special needs education, the Sonnenhof, in Switzerland. In 1940, Karl König founded the Camphill Movement in Scotland. The latter in particular has spread widely, and there are now over a hundred Camphill communities and other anthroposophical homes for children and adults in need of special care in about 22 countries around the world. Both Karl König, Thomas Weihs and others have written extensively on these ideas underlying Special education.
Architecture.
Steiner himself designed around thirteen buildings, many of them significant works in a unique, organic—expressionist architectural style. Foremost among these are his designs for the two Goetheanum buildings in Dornach, Switzerland. Thousands of further buildings have been built by later generations of anthroposophic architects.
Architects who have been strongly influenced by the anthroposophic style include Imre Makovecz in Hungary, Hans Scharoun and Joachim Eble in Germany, Erik Asmussen in Sweden, Kenji Imai in Japan, Thomas Rau, Anton Alberts and Max van Huut in the Netherlands, Christopher Day and Camphill Architects in the UK, Thompson and Rose in America, Denis Bowman in Canada, and Walter Burley Griffin and Gregory Burgess in Australia.
One of the most famous contemporary buildings by an anthroposophical architect is ING House, an ING Bank building in Amsterdam, which has received several awards for its ecological design and approach to a self-sustaining ecology as an autonomous building and example of sustainable architecture.
Eurythmy.
In the arts, Steiner's new art of eurythmy gained early renown. Eurythmy seeks to renew the spiritual foundations of dance, revealing speech and music in visible movement. There are now active stage groups and training centers, mostly of modest proportions, in approximately 16 countries.
Social finance.
Around the world today are a number of banks, companies, charities, and schools for developing co-operative forms of business using Steiner's ideas about economic associations, aiming at harmonious and socially responsible roles in the world economy. The first anthroposophic bank was the "Gemeinschaftsbank für Leihen und Schenken" in Bochum, Germany, founded in 1974.
Socially responsible banks founded out of anthroposophy in the English-speaking world include Triodos Bank, founded in 1980 and active in the UK, Netherlands, Germany, Belgium, and Spain.
Cultura Sparebank dates from 1982 when a group of Norwegian anthroposophists start to grow the idea of having ethical banking but only in the late 90s the bank starts to operate as a savings bank in Norway.
La Nef in France and RSF Social Finance in San Francisco are other examples.
Organizational development, counselling and biography work.
Bernard Lievegoed, a psychiatrist, founded a new method of individual and institutional development oriented towards humanizing organizations and linked with Steiner's ideas of the threefold social order. This work is represented by the NPI Institute for Organizational Development in the Netherlands and sister organizations in many other countries. Various forms of biographic and counselling work have been developed on the basis of anthroposophy.
Speech and drama.
There are also anthroposophical movements to renew speech and drama, the most important of which are based in the work of Marie Steiner-von Sivers ("speech formation", also known as "Creative Speech") and the "Chekhov Method" originated by Michael Chekhov (nephew of Anton Chekhov).
Social goals.
For a period after World War I, Steiner was extremely active and well known in Germany, in part because he lectured widely proposing social reforms. Steiner was a sharp critic of nationalism, which he saw as outdated, and a proponent of achieving social solidarity through individual freedom. A petition proposing a radical change in the German constitution and expressing his basic social ideas (signed by Herman Hesse, among others) was widely circulated. His main book on social reform is "Toward Social Renewal".
Anthroposophy continues to aim at reforming society through maintaining and strengthening the independence of the spheres of cultural life, human rights and the economy. It emphasizes a particular ideal in each of these three realms of society:
Esoteric path.
Paths of spiritual development.
According to Steiner, a real spiritual world exists, out of which the material one gradually condensed and evolved. Steiner held that the spiritual world can be researched in the right circumstances through direct experience, by persons practicing rigorous forms of ethical and cognitive self-discipline. Steiner described many exercises he said were suited to strengthening such self-discipline; the most complete exposition of these is found in his book "How To Know Higher Worlds". The aim of these exercises is to develop higher levels of consciousness through meditation and observation. Details about the spiritual world, Steiner suggested, could on such a basis be discovered and reported, though no more infallibly than the results of natural science.
Steiner regarded his research reports as being important aids to others seeking to enter into spiritual experience. He suggested that a combination of spiritual exercises (for example, concentrating on an object such as a seed), moral development (control of thought, feelings and will combined with openness, tolerance and flexibility) and familiarity with other spiritual researchers' results would best further an individual's spiritual development. He consistently emphasised that any inner, spiritual practice should be undertaken in such a way as not to interfere with one's responsibilities in outer life. Steiner distinguished between what he considered were true and false paths of spiritual investigation.
In anthroposophy, artistic expression is also treated as a potentially valuable bridge between spiritual and material reality.
Prerequisites to and stages of inner development.
Steiner's stated prerequisites to beginning on a spiritual path include a willingness to take up serious cognitive studies, a respect for factual evidence, and a responsible attitude. Central to progress on the path itself is a harmonious cultivation of the following qualities:
Steiner sees meditation as a concentration and enhancement of the power of thought. By focusing consciously on an idea, feeling or intention the meditant seeks to arrive at pure thinking, a state exemplified by but not confined to pure mathematics. In Steiner's view, conventional sensory-material knowledge is achieved through relating perception and concepts. The anthroposophic path of esoteric training articulates three further stages of supersensory knowledge, which do not necessarily follow strictly sequentially in any single individual's spiritual progress.
Spiritual exercises.
Steiner described numerous exercises he believed would bring spiritual development; other anthroposophists have added many others. A central principle is that "for every step in spiritual perception, three steps are to be taken in moral development." According to Steiner, moral development reveals the extent to which one has achieved control over one's inner life and can exercise it in harmony with the spiritual life of other people; it shows the real progress in spiritual development, the fruits of which are given in spiritual perception. It also guarantees the capacity to distinguish between false perceptions or illusions (which are possible in perceptions of both the outer world and the inner world) and true perceptions: i.e., the capacity to distinguish in any perception between the influence of subjective elements (i.e., viewpoint) and objective reality.
Place in Western philosophy.
Steiner built upon Goethe's conception of an imaginative power capable of synthesizing the sense-perceptible form of a thing (an image of its outer appearance) and the concept we have of that thing (an image of its inner structure or nature). Steiner added to this the conception that a further step in the development of thinking is possible when the thinker observes his or her own thought processes. "The organ of observation and the observed thought process are then identical, so that the condition thus arrived at is simultaneously one of perception through thinking and one of thought through perception."
Thus, in Steiner's view, we can overcome the subject-object divide through inner activity, even though all human experience begins by being conditioned by it. In this connection, Steiner examines the step from thinking determined by outer impressions to what he calls sense-free thinking. He characterizes thoughts he considers without sensory content, such as mathematical or logical thoughts, as free deeds. Steiner believed he had thus located the origin of free will in our thinking, and in particular in sense-free thinking.
Some of the epistemic basis for Steiner's later anthroposophical work is contained in the seminal work, Philosophy of Freedom. In his early works, Steiner sought to overcome what he perceived as the dualism of Cartesian idealism and Kantian subjectivism by developing Goethe's conception of the human being as a natural-supernatural entity, that is: natural in that humanity is a product of nature, supernatural in that through our conceptual powers we extend nature's realm, allowing it to achieve a reflective capacity in us as philosophy, art and science. Steiner was one of the first European philosophers to overcome the subject-object split in Western thought. Though not well known among philosophers, his philosophical work was taken up by Owen Barfield (and through him influenced the Inklings, an Oxford group of Christian writers that included J. R. R. Tolkien and C. S. Lewis).
Christian and Jewish mystical thought have also influenced the development of anthroposophy.
Union of science and spirit.
Steiner believed in the possibility of applying the clarity of scientific thinking to spiritual experience, which he saw as deriving from an objectively existing spiritual world. Steiner identified mathematics, which attains certainty through thinking itself, thus through inner experience rather than empirical observation, as the basis of his epistemology of spiritual experience.
Relationship to religion.
Christ as the center of earthly evolution.
Steiner's writing, though appreciative of all religions and cultural developments, emphasizes Western tradition as having evolved to meet contemporary needs. He describes Christ and his mission on earth of bringing individuated consciousness as having a particularly important place in human evolution, whereby:
Thus, anthroposophy considers there to be a being who unifies all religions, and who is not represented by any particular religious faith. This being is, according to Steiner, not only the Redeemer of the Fall from Paradise, but also the unique pivot and meaning of earth's evolutionary processes and of human history. To describe this being, Steiner periodically used terms such as the "Representative of Humanity" or the "good spirit" rather than any denominational term.
Divergence from conventional Christian thought.
Steiner's views of Christianity diverge from conventional Christian thought in key places, and include gnostic elements:
Judaism.
Rudolf Steiner wrote and lectured on Judaism and Jewish issues for much of his life. In the 1880s and 1890s, he took part in debates on anti-semitism and on assimilation. He was a fierce opponent of anti-semitism and supported the unconditional acceptance and integration of the Jews in Europe. He also supported Émile Zola's position in the Dreyfus affair. In his later life, Steiner was accused by the Nazis of being a Jew, and Adolf Hitler called anthroposophy "Jewish methods". The anthroposophical institutions in Germany were banned during Nazi rule and several anthroposophists sent to concentration camps.
Steiner emphasized Judaism's central importance to the constitution of the modern era in the West but suggested that to appreciate the spirituality of the future it would need to overcome its tendency toward abstraction. Important early anthroposophists who were Jewish included two central members on the executive boards of the precursors to the modern Anthroposophical Society, and Karl König, the founder of the Camphill movement. Martin Buber and Hugo Bergmann, who viewed Steiner's social ideas as a solution to the Arab–Jewish conflict, were also influenced by anthroposophy.
There are several anthroposophical organisations in Israel, including the anthroposophical kibbutz Harduf, founded by Jesaiah Ben-Aharon. A number of these organizations are striving to foster positive relationships between the Arab and Jewish populations: The Harduf Waldorf school includes both Jewish and Arab faculty and students, and has extensive contact with the surrounding Arab communities. In Hilf near Haifa, there is a joint Arab-Jewish Waldorf kindergarten, the first joint Arab-Jewish kindergarten in Israel.
Christian Community.
Towards the end of Steiner's life, a group of theology students (primarily Lutheran, with some Roman Catholic members) approached Steiner for help in reviving Christianity, in particular "to bridge the widening gulf between modern science and the world of spirit." They approached a notable Lutheran pastor, Friedrich Rittelmeyer, who was already working with Steiner's ideas, to join their efforts. Out of their co-operative endeavor, the "Movement for Religious Renewal", now generally known as The Christian Community, was born. Steiner emphasized that he considered this movement, and his role in creating it, to be independent of his anthroposophical work, as he wished anthroposophy to be independent of any particular religion or religious denomination.
Reception.
Supporters.
Anthroposophy's supporters include Pulitzer Prize-winning and Nobel Laureate Saul Bellow, Nobel prize winner Selma Lagerlöf, Andrei Bely, Joseph Beuys, Owen Barfield, architect Walter Burley Griffin, Wassily Kandinsky, Andrei Tarkovsky, Bruno Walter, and Right Livelihood Award winners Sir George Trevelyan and Ibrahim Abouleish. Albert Schweitzer was a friend of Steiner's and was supportive of his ideals for cultural renewal.
Scientific basis.
Though Rudolf Steiner studied natural science at the Vienna Technical University at the undergraduate level, his doctorate was in epistemology and very little of his work is directly concerned with the empirical sciences. In his mature work, when he did refer to science it was often to present phenomenological or Goethean science as an alternative to what he considered the materialistic science of his contemporaries.
His primary interest was in applying the methodology of science to realms of inner experience and the spiritual worlds (Steiner's appreciation that the essence of science is its method of inquiry is unusual among esotericists), and Steiner called anthroposophy "Geisteswissenschaft" (lit.: Science of the mind, or cultural or spiritual science), a term generally used in German to refer to the humanities and social sciences; in fact, the term "science" is used more broadly in Europe as a general term that refers to any exact knowledge.
Whether this is a sufficient basis for anthroposophy to be considered a spiritual science has been a matter of controversy. As Freda Easton explained in her study of Waldorf schools, "Whether one accepts anthroposophy as a science depends upon whether one accepts Steiner's interpretation of a science that extends the consciousness and capacity of human beings to experience their inner spiritual world." Sven Ove Hansson has disputed anthroposophy's claim to a scientific basis, stating that its ideas are not empirically derived and neither reproducible nor testable.
Carlo Willmann points out that as, on its own terms, anthroposophical methodology offers no possibility of being falsified except through its own procedures of spiritual investigation, no intersubjective validation is possible by conventional scientific methods; it thus cannot stand up to positivistic science's criticism. Peter Schneider calls such objections untenable on the grounds that if a non-sensory, non-physical realm exists, then according to Steiner the experiences of pure thinking possible within the normal realm of consciousness would already be experiences of that, and it would be impossible to exclude the possibility of empirically grounded experiences of other supersensory content.
Olav Hammer suggests that anthroposophy carries scientism "to lengths unparalleled in any other Esoteric position" due to its dependence upon claims of clairvoyant experience, its subsuming natural science under "spiritual science", and its development of what Hammer calls "fringe" sciences such as anthroposophical medicine and biodynamic agriculture justified partly on the basis of the ethical and ecological values they promote, rather than purely on a scientific basis.
Though Steiner saw that spiritual vision itself is difficult for others to achieve, he recommended open-mindedly exploring and rationally testing the results of such research; he also urged others to follow a spiritual training that would allow them directly to apply the methods he used eventually to achieve comparable results. Some results of Steiner's research have been investigated and supported by scientists working to further and extend scientific observation in directions suggested by an anthroposophical approach.
Anthony Storr stated about Rudolf Steiner's Anthroposophy: "His belief system is so eccentric, so unsupported by evidence, so manifestly bizarre, that rational skeptics are bound to consider it delusional... But, whereas Einstein's way of perceiving the world by thought became confirmed by experiment and mathematical proof, Steiner's remained intensely subjective and insusceptible of objective confirmation."
Religious nature.
As an explicitly spiritual movement, anthroposophy has sometimes been called a religious philosophy. In 2005, a California federal court ruled that a group alleging that anthroposophy is a religion for Establishment Clause purposes did not provide any legally admissible evidence in support of this view; the case is under appeal. In 2000, a French court ruled that a government minister's description of anthroposophy as a cult was defamatory.
Statements on race.
Anthroposophical ideas have been criticized from both sides in the race debate:
The Anthroposophical Society in America has stated:
We explicitly reject any racial theory that may be construed to be part of Rudolf Steiner's writings. The Anthroposophical Society in America is an open, public society and it rejects any purported spiritual or scientific theory on the basis of which the alleged superiority of one race is justified at the expense of another race.

</doc>
<doc id="2494" url="https://en.wikipedia.org/wiki?curid=2494" title="Aurochs">
Aurochs

The aurochs ( or ; pl. aurochs, or rarely aurochsen, aurochses), also urus, ure ("Bos primigenius"), is an extinct type of large wild cattle that inhabited Europe, Asia and North Africa. It is the ancestor of domestic cattle. The species survived in Europe until the last recorded aurochs died in the Jaktorów Forest, Poland in 1627.
During the Neolithic Revolution, which occurred during the early Holocene, there were at least two aurochs domestication events: one related to the Indian subspecies, leading to zebu cattle; the other one related to the Eurasian subspecies, leading to taurine cattle. Other species of wild bovines were also domesticated, namely the wild water buffalo, gaur, and banteng. In modern cattle, numerous breeds share characteristics of the aurochs, such as a dark colour in the bulls with a light eel stripe along the back (the cows being lighter), or a typical aurochs-like horn shape.
Taxonomy.
The aurochs was variously classified as "Bos primigenius", "Bos taurus", or, in old sources, "Bos urus". However, in 2003, the International Commission on Zoological Nomenclature "conserved the usage of 17 specific names based on wild species, which are pre-dated by or contemporary with those based on domestic forms", confirming "Bos primigenius" for the aurochs. Taxonomists who consider domesticated cattle a subspecies of the wild aurochs should use "B. primigenius taurus"; those who consider domesticated cattle to be a separate species may use the name "B. taurus", which the Commission has kept available for that purpose.
Etymology.
The words "aurochs", "urus", and "wisent" have all been used synonymously in English. However, the extinct aurochs/urus is a completely separate species from the still-extant wisent, also known as European bison. The two were often confused, and some 16th-century illustrations of aurochs and wisents have hybrid features.
The word "urus" (; plural "uri") is a Latin word, but was borrowed into Latin from Germanic (cf. Old English/Old High German "ūr", Old Norse "úr"). In German, OHG "ūr" was compounded with "ohso" "ox", giving "ūrohso", which became early modern "Aurochs". The modern form is "Auerochse".
The word "aurochs" was borrowed from early modern German, replacing archaic "urochs", also from an earlier form of German. The word is invariable in number in English, though sometimes back-formed singular "auroch" and innovated plural "aurochses" occur. The use in English of the plural form "" is nonstandard, but mentioned in "The Cambridge Encyclopedia of the English Language". It is directly parallel to the German plural "Ochsen" (singular "Ochse") and recreates by analogy the same distinction as English "ox" (singular) and "oxen" (plural).
Evolution.
During the Pliocene, the colder climate caused an extension of open grassland, which led to the evolution of large grazers, such as wild bovines. "Bos acutifrons" is an extinct species of cattle that has been suggested as an ancestor for the aurochs.
The oldest aurochs remains have been dated to about 2 million years ago, in India. The Indian subspecies was the first to appear. During the Pleistocene, the species migrated west into the Middle East (western Asia) as well as to the east. They reached Europe about 270,000 years ago. The South Asian domestic cattle, or zebu, descended from Indian aurochs at the edge of the Thar Desert; the zebu is resistant to drought. Domestic yak, gayal and Javan cattle do not descend from aurochs.
The first complete mitochondrial genome (16,338 base pairs) DNA sequence analysis of "Bos primigenius" from an archaeologically verified and exceptionally well preserved aurochs bone sample was published in 2010.
Three wild subspecies of aurochs are recognized. Only the Eurasian subspecies survived until recent times.
Description.
The appearance of the aurochs has been reconstructed from skeletal material, historical descriptions and contemporaneous depictions, such as cave paintings, engravings or Sigismund von Herberstein’s illustration. The work by Charles Hamilton Smith is a copy of a painting owned by a merchant in Augsburg, which may date to the 16th century. Scholars have proposed that Smith's illustration was based on a cattle/aurochs hybrid, or an aurochs-like breed. The aurochs was depicted in prehistoric cave paintings and described in Julius Caesar's "The Gallic War, Book 6, Ch. 28".
Size.
The aurochs was one of the largest herbivores in postglacial Europe, comparable to the wisent, the European bison. The size of an aurochs appears to have varied by region: in Europe, northern populations were bigger on average than those from the south. For example, during the Holocene, aurochs from Denmark and Germany had an average height at the shoulders of in bulls and in cows, while aurochs populations in Hungary had bulls reaching . The body mass of aurochs appears to have shown some variability. Some individuals were comparable in weight to the wisent and the banteng, reaching around , whereas those from the late-middle Pleistocene are estimated to have weighed up to , as much as the largest gaur (the largest extant bovid). The sexual dimorphism between bull and cow was strongly expressed, with the cows being significantly shorter than bulls on average.
Horns.
Because of the massive horns, the frontal bones of aurochs were elongated and broad. The horns of the aurochs were characteristic in size, curvature and orientation. They were curved in three directions: upwards and outwards at the base, then swinging forwards and inwards, then inwards and upwards. Aurochs horns could reach in length and between in diameter. The horns of bulls were larger, with the curvature more strongly expressed than in cows. The horns grew from the skull at a 60° angle to the muzzle, facing forwards.
Body shape.
The proportions and body shape of the aurochs were strikingly different from many modern cattle breeds. For example, the legs were considerably longer and more slender, resulting in a shoulder height that nearly equalled the trunk length. The skull, carrying the large horns, was substantially larger and more elongated than in most cattle breeds. As in other wild bovines, the body shape of the aurochs was athletic and, especially in bulls, showed a strongly expressed neck and shoulder musculature. Therefore, the forehand was larger than the rear, similar to the wisent but unlike many domestic cattle. Even in carrying cows, the udder was small and hardly visible from the side; this feature is equal to that of other wild bovines. 
Coat colour.
The coat colour of the aurochs can be reconstructed by using historical and contemporary depictions. In his letter to Conrad Gesner (1602), Anton Schneeberger describes the aurochs, a description that agrees with cave paintings in Lascaux and Chauvet. Calves were born a chestnut colour. Young bulls changed their coat colour at a few months' old to a very deep brown or black, with a white eel stripe running down the spine. Cows retained the reddish-brown colour. Both sexes had a light-coloured muzzle. Some North African engravings show aurochs with a light-coloured "saddle" on the back, but otherwise there is no evidence of variation in coat colour throughout its range. A passage from Mucante (1596), describing the “wild ox” as gray, but is ambiguous and may refer to the wisent. Egyptian grave paintings show cattle with a reddish-brown coat colour in both sexes, with a light saddle, but the horn shape of these suggest that they may depict domestic cattle. Remains of aurochs hair were not known until the early 1980s.
Colour of forelocks.
Some primitive cattle breeds display similar coat colours to the aurochs, including the black colour in bulls with a light eel stripe, a pale mouth, and similar sexual dimorphism in colour. A feature often attributed to the aurochs is blond forehead hairs. Historical descriptions tell that the aurochs had long and curly forehead hair, but none mentions a certain colour for it. Cis van Vuure (2005) says that, although the colour is present in a variety of primitive cattle breeds, it is probably a discolouration that appeared after domestication. The gene responsible for this feature has not yet been identified. Zebu breeds show lightly coloured inner sides of the legs and belly, caused by the so-called zebu-tipping gene. It has not been tested if this gene is present in remains of the wild form of the zebu, the Indian aurochs.
Behaviour and ecology.
Like many bovids, aurochs formed herds for at least one part of the year. These probably did not number much more than thirty. If aurochs had similar social behaviour as their descendents, social status was gained through displays and fights, in which cows engaged as well as bulls. Indeed, it was reported that aurochs bulls often had severe fights. As in other wild cattle, ungulates that form unisexual herds, there was considerable sexual dimorphism. Ungulates that form herds containing animals of both sexes, such as horses, have more weakly developed sexual dimorphism.
During the mating season, which probably took place during the late summer or early autumn, the bulls had severe fights, and evidence from the forest of Jaktorów shows these could lead to death. In autumn, aurochs fed up for the winter and got fatter and shinier than during the rest of the year, according to Schneeberger. Calves were born in spring. According to Schneeberger the mother stayed at the calf's side until it was strong enough to join and keep up with the herd on the feeding grounds.
Calves were vulnerable to wolves and to an extent, bears, while healthy adult aurochs probably did not have to fear these predators. In prehistoric Europe, North Africa and Asia, big cats, like lions and tigers, and hyenas were additional predators that probably preyed on aurochs.
Historical descriptions, like Caesar’s "De Bello Gallico" or Schneeberger, tell that aurochs were swift and fast, and could be very aggressive. According to Schneeberger, aurochs were not concerned when a man approached. But, teased or hunted, an aurochs could get very aggressive and dangerous, and throw the teasing person into the air, as he described in a 1602 letter to Gesner.
Habitat and distribution.
There is no consensus concerning the habitat of the aurochs. While some authors think that the habitat selection of the aurochs was comparable to the African forest buffalo, others describe the species as inhabiting open grassland and helping maintain open areas by grazing, together with other large herbivores. With its hypsodont jaw, the aurochs was probably a grazer and had a food selection very similar to domestic cattle. It was not a browser like many deer species, nor a semi-intermediary feeder like the wisent. Comparisons of the isotope levels of Mesolithic aurochs and domestic cattle bones showed that aurochs probably inhabited wetter areas than domestic cattle. Schneeberger describes that, during winter, the aurochs ate twigs and acorns in addition to grasses.
After the beginning of the Common Era, the habitat of aurochs became more fragmented because of the steadily growing human population. During the last centuries of its existence, the aurochs was limited to remote regions, such as floodplain forests or marshes, where there were no competing domestic herbivores and less hunting pressure.
Relationship with humans.
Domestication.
The aurochs, which ranged throughout much of Eurasia and Northern Africa during the late Pleistocene and early Holocene, is the wild ancestor of modern cattle. Archaeological evidence shows that domestication occurred independently in the Near East and the Indian subcontinent between 10,000–8,000 years ago, giving rise to the two major domestic taxa observed today: humpless "Bos taurus" (taurine) and humped "Bos indicus" (zebu), respectively. This is confirmed by genetic analyses of matrilineal mitochondrial DNA sequences, which reveal a marked differentiation between modern "Bos taurus" and "Bos indicus" haplotypes, demonstrating their derivation from two geographically and genetically divergent wild populations. It is possible that there was a third domestication event from another form of the aurochs in Africa. The Sanga cattle, a zebu-like cattle with no back hump, is commonly believed to originate from crosses between humped-zebus with taurine cattle breeds. However, there is archaeological evidence that these cattle were domesticated independently in Africa and that bloodlines of taurine and zebu cattle were introduced only within the last few hundreds years.
Domestication of the aurochs began in the southern Caucasus and northern Mesopotamia from about the 6th millennium BC. Genetic evidence suggests that aurochs were independently domesticated in India and possibly also in northern Africa. Domesticated cattle and aurochs are so different in size that they have been regarded as separate species; however, large ancient cattle and aurochs "are difficult to classify because morphological traits have overlapping distributions in cattle and aurochs and diagnostic features are identified only in horn and some cranial elements."
A Mitochondrial DNA study suggests that all domesticated taurine cattle originated from about 80 wild female aurochs in the Near East.
Comparison of aurochs bones with those of modern cattle has provided many insights about the aurochs. Remains of the beast, from specimens believed to have weighed more than a ton, have been found in Mesolithic sites around Goldcliff, Wales.
Though aurochs became extinct in Britain during the Bronze Age, analysis of bones from aurochs that lived at about the same time as domesticated cattle showed no genetic contribution to modern breeds. As a result of this study, modern European cattle were thought to have descended directly from the Near East domestication. Another study found distinct similarities between modern breeds and Italian aurochs specimens, which suggested that the previously tested British aurochs were not a good model of the diversity of aurochs genetics. It also suggests possible North African and European aurochs contributions to domestic breeds. Further genetic tests have shown that domestic cattle in Europe are of Near Eastern origin. This indicates that the European aurochs was not domesticated, nor did it interbreed with the imported Near Eastern cattle.
Indian cattle (zebu), although domesticated eight to ten thousand years ago, are related to aurochs that diverged from the Near Eastern ones some 200,000 years ago. African cattle are thought to have descended from aurochs more closely related to the Near Eastern ones. The Near East and African aurochs groups are thought to have split some 25,000 years ago, probably 15,000 years before domestication. The "Turano-Mongolian" type of cattle now found in Northern China, Mongolia, Korea and Japan may represent a fourth domestication event (and a third event among "Bos taurus"–type aurochs). This group may have diverged from the Near East group some 35,000 years ago. Whether these separate genetic populations would have equated to separate subspecies is unclear.
The maximum range of the aurochs was from Europe (excluding Ireland and northern Scandinavia), to northern Africa, the Middle East, India and Central Asia. Until at least 3,000 years ago, the aurochs was also found in Eastern China, where it is recorded at the Dingjiabao Reservoir in Yangyuan County. Most remains in China are known from the area east of 105° E, but the species has also been reported from the eastern margin of the Tibetan plateau, close to the Heihe River.
Extinction.
Already in the times of Herodotus (5th century BC) aurochs had disappeared from southern Greece but remained common in the area north and east of Echedorus river close to modern Thessaloniki. Last reports of the species in the southern tip of the Balkans date to the 1st century BC when Varro reports that fierce wild oxen live in Dardania (southern Serbia) and Thrace. By the 13th century AD, the aurochs' range was restricted to Poland, Lithuania, Moldavia, Transylvania and East Prussia. The right to hunt large animals on any land was restricted first to nobles and then, gradually, to only the royal households. As the population of aurochs declined, hunting ceased, and the royal court used gamekeepers to provide open fields for grazing for the aurochs. The gamekeepers were exempted from local taxes in exchange for their service. Poaching aurochs was punishable by death.
According to the royal survey in 1564, the gamekeepers knew of 38 animals. The last recorded live aurochs, a female, died in 1627 in the Jaktorów Forest, Poland, from natural causes. The causes of extinction were unrestricted hunting, a narrowing of habitat due to the development of farming, and diseases transmitted by domestic cattle.
Cattle resembling the aurochs.
While all the wild subspecies are extinct, "Bos primigenius" lives on in domesticated cattle and attempts are being made to breed similar types suitable for filling the extinct subspecies' role in the wild.
Less-derived cattle breeds.
Because some cattle breeds have been changed more than other breeds, certain breeds (all belonging to the so-called landraces) bear a greater resemblance to the aurochs. These breeds are not very productive from the economical point of view, as they do not give as much milk or meat as others. Most of the ″primitive″ phenotypes are facing extinction, because farmers give them up for economic reasons or crossbreed them with more productive dairy and meat cattle. Very hardy and robust, the primitive breeds are sometimes used in nature conservation programs, where they can fill the place of their wild ancestor in the ecology. Primitive breeds include, for example: Caldela, Limia Cattle, Maremmana primitivo, Maronesa, Pajuna Cattle, Rhodopian Shorthorn, Sayaguesa Cattle, Spanish Fighting Bull, and Tudanca Cattle.
Breeding of aurochs-like cattle.
The idea of breeding back the aurochs was first proposed in the 19th century by Feliks Paweł Jarocki. In the 1920s a first attempt was undertaken by the Heck brothers in Germany with the aim of breeding an effigy (a look-alike) of the aurochs. Starting in the 1990s, grazing and rewilding projects brought new impetus to the idea and new breeding-back efforts came underway, this time with the aim of recreating an animal not only with the looks but also with the behaviour and the ecological impact of the aurochs, in order to be able to fill the ecological role of the aurochs.
Heck cattle.
In the 1920s, two German zoo directors (in Berlin and Munich), the brothers Heinz and Lutz Heck, began a selective breeding program to breed back the aurochs into existence from the descendant domestic cattle. Their plan was based on the concept that a species is not extinct as long as all its genes are still present in a living population. The result is the breed called Heck cattle. It resembles what is known about the appearance of the aurochs in colour and, in some cases, also horn shape.
Taurus Project.
The ABU ("Arbeitsgemeinschaft Biologischer Umweltschutz"), a conservation group in Germany, started to crossbreed Heck cattle with southern-European primitive breeds in 1996, with the goal to increase the aurochs-likeness of certain Heck cattle herds. These crossbreeds are called Taurus cattle. It is aimed to bring in aurochs-like features that are supposedly missing in Heck cattle, using Sayaguesa Cattle, Chianina and, to a lesser extent, Spanish Fighting Cattle (Lidia). The same is done in the Hungarian national park Hortobágy National Park, additionally using Hungarian Grey cattle and Watusi, in Lille Vildmose National Park in Denmark, using only Chianina and Sayaguesa so far, and in Latvia.
Tauros Programme.
The Dutch-based Tauros Programme, (initially TaurOs Project) is trying to DNA-sequence breeds of primitive cattle to find gene sequences that match those found in "ancient DNA" from aurochs samples. The modern cattle would be selectively bred to try to produce the aurochs-type genes in a single animal. Starting around 2007, Tauros programme selected a number of primitive breeds mainly from Iberia and Italy, such as Sayaguesa Cattle, Maremmana primitivo, Pajuna Cattle, Limia Cattle, Maronesa, Tudanca Cattle and others, which already bear considerable resemblance to the aurochs in certain features. Numerous crossbreed calves have been born already.
Uruz Project.
The newest of the back-breeding efforts, the Uruz Project, was started in 2013 by the initiator of the Tauros Project and the True Nature Foundation, an organization that wants to create a sound economic foundation underneath ecological restoration and rewilding. It differs from the other projects in that it will use a limited set of cattle breeds with known aurochs characteristics, it will adhere to a strict crossbreeding strategy following the rules of Mendelian inheritance, and when needed the project will make use of genome editing. By doing this the project hopes to reduce the amount of generations needed and reduce the amount of unwanted recessive genes and throwbacks. Its preliminary plans called for the use of Sayaguesa, Maremmana primitivo or Hungarian steppe cattle, Chianina and Watusi. A first breeding herd so far consisting of three Chianina cows and a Watusi bull was started at Kloster Lorsch in Germany in December 2013, another herd using Barrosã is being set up in northern Portugal.
Other projects.
Scientists of the Polish Foundation for Recreating the Aurochs (PFOT) in Poland hope to use DNA from bones in museums to recreate the aurochs. They plan to return this animal to the forests of Poland. The project has gained the support of the Polish Ministry of the Environment. They plan research on ancient preserved DNA. Other research projects have extracted "ancient" DNA over the past twenty years and their results have been published in such periodicals as "Nature" and "PNAS". Polish scientists Ryszard Słomski and Jacek A. Modliński believe that modern genetics and biotechnology make it possible to recreate an animal almost identical to the aurochs. They say this research will lead to examining the causes of the extinction of the aurochs, and help prevent a similar occurrence with domestic cattle.
Cultural significance.
 
The aurochs was an important game animal appearing in both Paleolithic European and Mesopotamian cave paintings, such as those found at Lascaux and Livernon in France. Aurochs existed into the Iron Age in Anatolia and the Near East, where it was worshiped as a sacred animal, the Lunar Bull, associated with the Great Goddess and later with Mithras. In 2012, an archaeological mission of the British Museum, led by Lebanese archaeologist Claude Doumet Serhal, discovered at the site of the old American school in Sidon, Lebanon, the remains of wild animal bones, including those of an aurochs, dating from the late fourth-early third millennium. A 1999 archaeological dig in Peterborough, England, uncovered the skull of an aurochs. The front part of the skull had been removed but the horns remained attached. The supposition is that the killing of the aurochs in this instance was a sacrificial act.
Also during antiquity, the aurochs was regarded as an animal of cultural value. Aurochs are depicted on the Ishtar Gate. Greeks and Paeonians were hunting aurochs (wild oxen/bulls) and used their huge horns as trophies, cups for wine and offers to the gods and heroes. For example, as mentioned by Samus, Philippus of Thessalonica and Antipater when Philip V of Macedon killed an aurochs on the foothills of mountain Orvilos he offered the horns which were 105 cm long and the skin to a temple of Hercules. Aurochs horns were often used by Romans as hunting horns. Aurochs were among those wild animals caught for fights ("venationes") in arenas. Julius Caesar wrote about aurochs in "Gallic War" Chapter 6.28:
"...those animals which are called uri. These are a little below the elephant in size, and of the appearance, colour, and shape of a bull. Their strength and speed are extraordinary; they spare neither man nor wild beast which they have espied. These the Germans take with much pains in pits and kill them. The young men harden themselves with this exercise, and practice themselves in this sort of hunting, and those who have slain the greatest number of them, having produced the horns in public, to serve as evidence, receive great praise. But not even when taken very young can they be rendered familiar to men and tamed. The size, shape, and appearance of their horns differ much from the horns of our oxen. These they anxiously seek after, and bind at the tips with silver, and use as cups at their most sumptuous entertainments."
The Hebrew Bible contains numerous references to the untameable strength of "re'em", translated as "bullock" or "wild-ox" in Jewish translations and translated rather poorly in the King James Version as "unicorn" but recognized from the last century by Hebrew scholars as the aurochs.
When the aurochs became rarer, hunting it became a privilege of the nobility and a sign of a high social status. In the Nibelungenlied, the killing of aurochs by Siegfried is described: "Darnach schlug er schiere einen Wisent und einen Elch, starker Ure viere und einen grimmen Schelch", meaning "After that, he defeated one wisent and one elk, four aurochs and one Schelch" - the background of the "Schelch" is dubious. Aurochs horns were commonly used as drinking horns by the nobility, which led to the fact that many aurochs horn sheaths are preserved today (albeit often discoloured). The drinking horn at Corpus Christi College, Cambridge, given to the college on its foundation in 1352, probably by the college's founders, the Guilds of Corpus Christi and the Blessed Virgin Mary, is thought to come from an aurochs. Furthermore, there is a painting by Willem Kalf depicting an aurochs horn. The horns of the last aurochs bulls, which died in 1620, were ornamented with gold and are located at the Livrustkammaren in Stockholm today.
Schneeberger writes that aurochs were hunted with arrows, nets and hunting dogs. With immobilized aurochs, a ritual was practised that might be regarded as cruel nowadays: the curly hair on the forehead was cut from the skull of the living animal. Belts were made out of this hair and were believed to increase the fertility of women. When the aurochs was slaughtered, a cross-like bone was extracted from the heart. This bone, which is also present in domestic cattle, contributed to the mystique of the animal and magical powers have been attributed to it.
In eastern Europe, where the aurochs survived until nearly 400 years ago, the aurochs has left traces in fixed expressions. In Russia, a drunken person behaving badly was described as "behaving like an aurochs", whereas in Poland, big strong people were characterized as being "a bloke like an aurochs".
In Central Europe the aurochs features in toponyms and heraldic coats of arms. For example, the names Ursenbach and Aurach am Hongar are derived from the aurochs. An aurochs head, the traditional arms of the German region Mecklenburg, figures in the coat of arms of Mecklenburg-Vorpommern. The aurochs (Romanian "bour", from Latin "būbalus") was also the symbol of Moldavia; nowadays they can be found in the coat of arms of both Romania and Moldova. In modern-day Romania, there are villages named Boureni. The horn of the aurochs is a charge of the coat of arms of Tauragė, Lithuania, (the name itself of Tauragė is a compound of "taũras" "auroch" and "ragas" "horn"). It is also present in the emblem of Kaunas, Lithuania, and was part of the emblem of Bukovina during its time as an Austro-Hungarian "Kronland". The Swiss Canton of Uri is named after the aurochs; its yellow flag shows a black aurochs head. East Slavic surnames Turenin, Turishchev, Turov, Turovsky originate from the Slavic name of the species "tur". In Slovakia there are toponyms like Turany, Turíčky, Turie, Turie Pole, Turík, Turová (villages), Turiec (river and region), Turská dolina (valley) and others. Turopolje, a large lowland floodplain south of the Sava river in Croatia, got its name from the once-abundant aurochs (Croatian: ). The ancient name of the Estonian town of Rakvere, "Tarwanpe" or "Tarvanpea", probably derives from "Auroch's head" ("Tarvan pea") in ancient Estonian.
In 2002, a 3.5-m-high and 7.1-m-long statue of an aurochs was erected in Rakvere, Estonia for the town's 700th birthday. The sculpture, made by artist Tauno Kangro, has become a symbol of the town.
Aurochs are frequently mentioned in the "A Song of Ice and Fire" series of fantasy novels by George R. R. Martin, in which roasted aurochs are sometimes served at banquets.
In the 2012 movie "Beasts of the Southern Wild", the six-year-old main character imagines aurochs, though the fantasy creatures are portrayed by "costumed" Vietnamese Pot-Bellied piglets.
Notes.
This article incorporates Creative Commons license CC BY-2.5 text from reference.

</doc>
<doc id="2499" url="https://en.wikipedia.org/wiki?curid=2499" title="Asynchronous Transfer Mode">
Asynchronous Transfer Mode

Asynchronous Transfer Mode (ATM) is, according to the ATM Forum, "a telecommunications concept defined by ANSI and ITU (formerly CCITT) standards for carriage of a complete range of user traffic, including voice, data, and video signals". ATM was developed to meet the needs of the Broadband Integrated Services Digital Network, as defined in the late 1980s, and designed to unify telecommunication and computer networks. It was designed for a network that must handle both traditional high-throughput data traffic (e.g., file transfers), and real-time, low-latency content such as voice and video. The reference model for ATM approximately maps to the three lowest layers of the ISO-OSI reference model: network layer, data link layer, and physical layer. ATM is a core protocol used over the SONET/SDH backbone of the public switched telephone network (PSTN) and Integrated Services Digital Network (ISDN), but its use is declining in favour of all IP.
ATM provides functionality that is similar to both circuit switching and packet switching networks: ATM uses asynchronous time-division multiplexing, and encodes data into small, fixed-sized packets (ISO-OSI frames) called "cells." This differs from approaches such as the Internet Protocol or Ethernet that use variable sized packets and frames. ATM uses a connection-oriented model in which a virtual circuit must be established between two endpoints before the actual data exchange begins. These virtual circuits may be “permanent”, i.e. dedicated connections that are usually preconfigured by the service provider, or “switched”, i.e. set up on a per-call basis using signalling and disconnected when the call is terminated.
ATM eventually became dominated by Internet Protocol (IP) only technology (and Wireless or Mobile ATM never got any foothold).
Layer 2 – Datagrams.
In the ISO-OSI reference model data link layer (layer 2), the basic transfer units are generically called frames. In ATM these frames are of a fixed (53 octets or bytes) length and specifically called "cells".
Cell size.
If a speech signal is reduced to packets, and it is forced to share a link with bursty data traffic (traffic with some large data packets) then no matter how small the speech packets could be made, they would always encounter full-size data packets. Under normal queuing conditions the cells might experience maximum queuing delays. To avoid this issue, all ATM packets, or "cells," are the same small size. In addition, the fixed cell structure means that ATM can be readily switched by hardware without the inherent delays introduced by software switched and routed frames.
Thus, the designers of ATM utilized small data cells to reduce jitter (delay variance, in this case) in the multiplexing of data streams. Reduction of jitter (and also end-to-end round-trip delays) is particularly important when carrying voice traffic, because the conversion of digitized voice into an analogue audio signal is an inherently real-time process, and to do a good job, the decoder (codec) that does this needs an evenly spaced (in time) stream of data items. If the next data item is not available when it is needed, the codec has no choice but to produce silence or guess — and if the data is late, it is useless, because the time period when it should have been converted to a signal has already passed.
At the time of the design of ATM, 155 Mbit/s Synchronous Digital Hierarchy (SDH) with 135 Mbit/s payload was considered a fast optical network link, and many plesiochronous digital hierarchy (PDH) links in the digital network were considerably slower, ranging from 1.544 to 45 Mbit/s in the USA, and 2 to 34 Mbit/s in Europe.
At this rate, a typical full-length 1500 byte (12000-bit) data packet would take 77.42 µs to transmit. In a lower-speed link, such as a 1.544 Mbit/s T1 line, a 1500 byte packet would take up to 7.8 milliseconds.
A queuing delay induced by several such data packets might exceed the figure of 7.8 ms several times over, in addition to any packet generation delay in the shorter speech packet. This was clearly unacceptable for speech traffic, which needs to have low jitter in the data stream being fed into the codec if it is to produce good-quality sound. A packet voice system can produce this low jitter in a number of ways:
The design of ATM aimed for a low-jitter network interface. However, "cells" were introduced into the design to provide short queuing delays while continuing to support datagram traffic. ATM broke up all packets, data, and voice streams into 48-byte chunks, adding a 5-byte routing header to each one so that they could be reassembled later. The choice of 48 bytes was political rather than technical. When the CCITT (now ITU-T) was standardizing ATM, parties from the United States wanted a 64-byte payload because this was felt to be a good compromise in larger payloads optimized for data transmission and shorter payloads optimized for real-time applications like voice; parties from Europe wanted 32-byte payloads because the small size (and therefore short transmission times) simplify voice applications with respect to echo cancellation. Most of the European parties eventually came around to the arguments made by the Americans, but France and a few others held out for a shorter cell length. With 32 bytes, France would have been able to implement an ATM-based voice network with calls from one end of France to the other requiring no echo cancellation. 48 bytes (plus 5 header bytes = 53) was chosen as a compromise between the two sides. 5-byte headers were chosen because it was thought that 10% of the payload was the maximum price to pay for routing information. ATM multiplexed these 53-byte cells instead of packets which reduced worst-case cell contention jitter by a factor of almost 30, reducing the need for echo cancellers.
The structure of an ATM cell.
An ATM cell consists of a 5-byte header and a 48-byte payload. The payload size of 48 bytes was chosen as described above.
ATM defines two different cell formats: UNI (User-Network Interface) and NNI (Network-Network Interface). Most ATM links use UNI cell format.
ATM uses the PT field to designate various special kinds of cells for operations, administration and management (OAM) purposes, and to delineate packet boundaries in some ATM adaptation layers (AAL). If the most significant bit of the PT field is 0, this is a user data cell, and the other two bits are used to indicate network congestion and as a general purpose header bit available for ATM adaptation layers.
If the msbit of the PT bit is 1, this is a management cell, and the other two bits indicate the type. (Network management segment, network management end-to-end, resource management, and reserved for future use.)
Several ATM link protocols use the HEC field to drive a CRC-based framing algorithm, which allows locating the ATM cells with no overhead beyond what is otherwise needed for header protection. The 8-bit CRC is used to correct single-bit header errors and detect multi-bit header errors. When multi-bit header errors are detected, the current and subsequent cells are dropped until a cell with no header errors is found.
A UNI cell reserves the GFC field for a local flow control/submultiplexing system between users. This was intended to allow several terminals to share a single network connection, in the same way that two Integrated Services Digital Network (ISDN) phones can share a single basic rate ISDN connection. All four GFC bits must be zero by default.
The NNI cell format replicates the UNI format almost exactly, except that the 4-bit GFC field is re-allocated to the VPI field, extending the VPI to 12 bits. Thus, a single NNI ATM interconnection is capable of addressing almost 2 VPs of up to almost 2 VCs each (in practice some of the VP and VC numbers are reserved).
Cells in practice.
ATM supports different types of services via AALs. Standardized AALs include AAL1, AAL2, and AAL5, and the rarely used AAL3 and AAL4. AAL1 is used for constant bit rate (CBR) services and circuit emulation. Synchronization is also maintained at AAL1. AAL2 through AAL4 are used for variable bit rate (VBR) services, and AAL5 for data. Which AAL is in use for a given cell is not encoded in the cell. Instead, it is negotiated by or configured at the endpoints on a per-virtual-connection basis.
Following the initial design of ATM, networks have become much faster. A 1500 byte (12000-bit) full-size Ethernet frame takes only 1.2 µs to transmit on a 10 Gbit/s network, reducing the need for small cells to reduce jitter due to contention. Some consider that this makes a case for replacing ATM with Ethernet in the network backbone. However, it should be noted that the increased link speeds by themselves do not alleviate jitter due to queuing. Additionally, the hardware for implementing the service adaptation for IP packets is expensive at very high speeds. Specifically, at speeds of OC-3 and above, the cost of segmentation and reassembly (SAR) hardware makes ATM less competitive for IP than Packet Over SONET (POS); because of its fixed 48-byte cell payload, ATM is not suitable as a data link layer "directly" underlying IP (without the need for SAR at the data link level) since the OSI layer on which IP operates must provide a maximum transmission unit (MTU) of at least 576 bytes. SAR performance limits mean that the fastest IP router ATM interfaces are STM16 - STM64 which actually compares, while POS can operate at OC-192 (STM64) with higher speeds expected in the future.
On slower or congested links (622 Mbit/s and below), ATM does make sense, and for this reason most asymmetric digital subscriber line (ADSL) systems use ATM as an intermediate layer between the physical link layer and a Layer 2 protocol like PPP or Ethernet.
At these lower speeds, ATM provides a useful ability to carry multiple logical circuits on a single physical or virtual medium, although other techniques exist, such as Multi-link PPP and Ethernet VLANs, which are optional in VDSL implementations. DSL can be used as an access method for an ATM network, allowing a DSL termination point in a telephone central office to connect to many internet service providers across a wide-area ATM network. In the United States, at least, this has allowed DSL providers to provide DSL access to the customers of many internet service providers. Since one DSL termination point can support multiple ISPs, the economic feasibility of DSL is substantially improved.
Why virtual circuits?
ATM operates as a channel-based transport layer, using virtual circuits (VCs). This is encompassed in the concept of the Virtual Paths (VP) and Virtual Channels. Every ATM cell has an 8- or 12-bit Virtual Path Identifier (VPI) and 16-bit Virtual Channel Identifier (VCI) pair defined in its header. The VCI, together with the VPI, is used to identify the next destination of a cell as it passes through a series of ATM switches on its way to its destination. The length of the VPI varies according to whether the cell is sent on the user-network interface (on the edge of the network), or if it is sent on the network-network interface (inside the network).
As these cells traverse an ATM network, switching takes place by changing the VPI/VCI values (label swapping). Although the VPI/VCI values are not necessarily consistent from one end of the connection to the other, the concept of a circuit "is" consistent (unlike IP, where any given packet could get to its destination by a different route than the others). ATM switches use the VPI/VCI fields to identify the Virtual Channel Link (VCL) of the next network that a cell needs to transit on its way to its final destination. The function of the VCI is similar to that of the data link connection identifier (DLCI) in frame relay and the Logical Channel Number & Logical Channel Group Number in X.25.
Another advantage of the use of virtual circuits comes with the ability to use them as a multiplexing layer, allowing different services (such as voice, Frame Relay, n* 64 channels, IP). The VPI is useful for reducing the switching table of some virtual circuits which have common paths.
Using cells and virtual circuits for traffic engineering.
Another key ATM concept involves the traffic contract. When an ATM circuit is set up each switch on the circuit is informed of the traffic class of the connection.
ATM traffic contracts form part of the mechanism by which "quality of service" (QoS) is ensured. There are four basic types (and several variants) which each have a set of parameters describing the connection.
VBR has real-time and non-real-time variants, and serves for "bursty" traffic. Non-real-time is sometimes abbreviated to vbr-nrt.
Most traffic classes also introduce the concept of Cell Delay Variation Tolerance (CDVT), which defines the "clumping" of cells in time.
Traffic policing.
To maintain network performance, networks may apply traffic policing to virtual circuits to limit them to their traffic contracts at the entry points to the network, i.e. the user–network interfaces (UNIs) and network-to-network interfaces (NNIs): Usage/Network Parameter Control (UPC and NPC). The reference model given by the ITU-T and ATM Forum for UPC and NPC is the generic cell rate algorithm (GCRA), which is a version of the leaky bucket algorithm. CBR traffic will normally be policed to a PCR and CDVt alone, whereas VBR traffic will normally be policed using a dual leaky bucket controller to a PCR and CDVt and an SCR and Maximum Burst Size (MBS). The MBS will normally be the packet (SAR-SDU) size for the VBR VC in cells.
If the traffic on a virtual circuit is exceeding its traffic contract, as determined by the GCRA, the network can either drop the cells or mark the Cell Loss Priority (CLP) bit (to identify a cell as potentially redundant). Basic policing works on a cell by cell basis, but this is sub-optimal for encapsulated packet traffic (as discarding a single cell will invalidate the whole packet). As a result, schemes such as Partial Packet Discard (PPD) and Early Packet Discard (EPD) have been created that will discard a whole series of cells until the next packet starts. This reduces the number of useless cells in the network, saving bandwidth for full packets. EPD and PPD work with AAL5 connections as they use the end of packet marker: the ATM User-to-ATM User (AUU) Indication bit in the Payload Type field of the header, which is set in the last cell of a SAR-SDU.
Traffic shaping.
Traffic shaping usually takes place in the network interface card (NIC) in user equipment, and attempts to ensure that the cell flow on a VC will meet its traffic contract, i.e. cells will not be dropped or reduced in priority at the UNI. Since the reference model given for traffic policing in the network is the GCRA, this algorithm is normally used for shaping as well, and single and dual leaky bucket implementations may be used as appropriate.
Types of virtual circuits and paths.
ATM can build virtual circuits and virtual paths either statically or dynamically. Static circuits (permanent virtual circuits or PVCs) or paths (permanent virtual paths or PVPs) require that the circuit is composed of a series of segments, one for each pair of interfaces through which it passes.
PVPs and PVCs, though conceptually simple, require significant effort in large networks. They also do not support the re-routing of service in the event of a failure. Dynamically built PVPs (soft PVPs or SPVPs) and PVCs (soft PVCs or SPVCs), in contrast, are built by specifying the characteristics of the circuit (the service "contract") and the two end points.
Finally, ATM networks create and remove switched virtual circuits (SVCs) on demand when requested by an end piece of equipment. One application for SVCs is to carry individual telephone calls when a network of telephone switches are inter-connected using ATM. SVCs were also used in attempts to replace local area networks with ATM.
Virtual circuit routing.
Most ATM networks supporting SPVPs, SPVCs, and SVCs use the Private Network Node Interface or the Private Network-to-Network Interface (PNNI) protocol. PNNI uses the same shortest-path-first algorithm used by OSPF and IS-IS to route IP packets to share topology information between switches and select a route through a network. PNNI also includes a very powerful summarization mechanism to allow construction of very large networks, as well as a call admission control (CAC) algorithm which determines the availability of sufficient bandwidth on a proposed route through a network in order to satisfy the service requirements of a VC or VP.
Call admission and connection establishment.
A network must establish a connection before two parties can send cells to each other. In ATM this is called a virtual circuit (VC). It can be a permanent virtual circuit (PVC), which is created administratively on the end points, or a switched virtual circuit (SVC), which is created as needed by the communicating parties. SVC creation is managed by signaling, in which the requesting party indicates the address of the receiving party, the type of service requested, and whatever traffic parameters may be applicable to the selected service. "Call admission" is then performed by the network to confirm that the requested resources are available and that a route exists for the connection.
Reference model.
ATM defines three layers:
Deployment.
ATM became popular with telephone companies and many computer makers in the 1990s. However, even by the end of the decade, the better price/performance of Internet Protocol-based products was competing with ATM technology for integrating real-time and bursty network traffic. Companies such as FORE Systems focused on ATM products, while other large vendors such as Cisco Systems provided ATM as an option. After the burst of the dot-com bubble, some still predicted that "ATM is going to dominate". However, in 2005 the ATM Forum, which had been the trade organization promoting the technology, merged with groups promoting other technologies, and eventually became the Broadband Forum.
Wireless ATM or Mobile ATM.
Wireless ATM, or Mobile ATM, consists of an ATM core network with a wireless access network. ATM cells are transmitted from base stations to mobile terminals. Mobility functions are performed at an ATM switch in the core network, known as "crossover switch", which is similar to the MSC (mobile switching center) of GSM Networks. The advantage of Wireless ATM is its high bandwidth and high speed handoffs done at Layer 2. In the early 1990s, Bell Labs and NEC Research Labs worked actively in this field. Andy Hopper from Cambridge University Computer Laboratory also worked in this area. There was a Wireless ATM Forum formed to standardize the technology behind Wireless ATM Networks. The forum was supported by several telecommunication companies, including NEC, Fujitsu, AT&T, etc. Mobile
ATM aimed to provide high speed multimedia communications technology, capable of delivering broadband mobile communications beyond that of GSM and WLANs.

</doc>
<doc id="2500" url="https://en.wikipedia.org/wiki?curid=2500" title="Anus">
Anus

The anus (, which is from Proto-Indo-European "*h₁eh₂no-", meaning "ring") is an opening at the opposite end of an animal's digestive tract from the mouth. Its function is to control the expulsion of feces, unwanted semi-solid matter produced during digestion, which, depending on the type of animal, may include: matter which the animal cannot digest, such as bones; food material after all the nutrients have been extracted, for example cellulose or lignin; ingested matter which would be toxic if it remained in the digestive tract; and dead or excess gut bacteria and other endosymbionts.
Amphibians, reptiles, and birds use the same orifice (known as the cloaca) for excreting liquid and solid wastes, for copulation and egg-laying. Monotreme mammals also have a cloaca, which is thought to be a feature inherited from the earliest amniotes via the therapsids. Marsupials have a single orifice for excreting both solids and liquids and, in females, a separate vagina for reproduction. Female placental mammals have completely separate orifices for defecation, urination, and reproduction; males have one opening for defecation and another for both urination and reproduction, although the channels flowing to that orifice are almost completely separate.
The development of the anus was an important stage in the evolution of multicellular animals. It appears to have happened at least twice, following different paths in protostomes and deuterostomes. This accompanied or facilitated other important evolutionary developments: the bilaterian body plan, the coelom, and metamerism, in which the body was built of repeated "modules" which could later specialize, such as the heads of most arthropods, which are composed of fused, specialized segments.
Development.
In animals at least as complex as an earthworm, the embryo forms a dent on one side, the blastopore, which deepens to become the archenteron, the first phase in the growth of the gut. In deuterostomes, the original dent becomes the anus while the gut eventually tunnels through to make another opening, which forms the mouth. The protostomes were so named because it was thought that in their embryos the dent formed the mouth first ("proto–" meaning "first") and the anus was formed later at the opening made by the other end of the gut. More recent research, however, shows that in protostomes the edges of the dent close up in the middle, leaving openings at the ends which become the mouth and anus.

</doc>
<doc id="2501" url="https://en.wikipedia.org/wiki?curid=2501" title="Appendix">
Appendix

Appendix may refer to:
In documents:
In anatomy:
In music:
In journalism:

</doc>
<doc id="2502" url="https://en.wikipedia.org/wiki?curid=2502" title="Acantharea">
Acantharea

The Acantharea (Acantharia) are a group of radiolarian protozoa, distinguished mainly by their skeletons.
Structure.
These are composed of strontium sulfate crystals, which do not fossilize, and take the form of either ten diametric or twenty radial spines. The central capsule is made up of microfibrils arranged into twenty plates, each with a hole through which one spine projects, and there is also a microfibrillar cortex linked to the spines by myonemes. These assist in flotation, together with the vacuoles in the ectoplasm, which often contain zooxanthellae.
Classification by spine arrangement.
The arrangement of the spines is very precise, and is described by what is called the Müllerian law. This is easiest to describe in terms of lines of latitude and longitude - the spines lie on the intersections between five of the former, symmetric about an equator, and eight of the latter, spaced uniformly. Each line of longitude carries either two "tropical" spines or one "equatorial" and two "polar" spines, in alternation. The way that the spines are joined together at the center of the cell varies and is one of the primary characteristics by which acanthareans are classified.
The axopods are fixed in number.
Life cycle.
Adults are usually multinucleated. Reproduction is thought to take place by formation of swarmer cells (formerly referred to as "spores"), which may be flagellate. Not all life cycle stages have been observed, and study of these organisms has been hampered mainly by an inability to maintain these organisms in culture through successive generations.

</doc>
<doc id="2503" url="https://en.wikipedia.org/wiki?curid=2503" title="African National Congress">
African National Congress

The African National Congress (ANC) is the Republic of South Africa's governing social democratic political party. It has been the ruling party of post-apartheid South Africa on the national level since 1994, including the election of Nelson Mandela as president from 1994-1999. In the 2004 general election the ANC won 69.7% of the votes, in the 2009 general election it won 65.9% of the votes, and in 2014 it won 62.15% of the votes.
The ANC defines itself as a "disciplined force of the left", and it has been supported by the Tripartite Alliance with the Congress of South African Trade Unions (COSATU) and the South African Communist Party (SACP), since the establishment of a non-racial democracy in April 1994.
Members founded the organisation as the South African Native National Congress (SANNC) on 8 January 1912 at the Waaihoek Wesleyan Church in Bloemfontein to work for the rights of the black South African population. John Dube, its first president, and poet and author Sol Plaatje were among its founding members. The organisation became the ANC in 1923 and formed a military wing, the Umkhonto we Sizwe (Spear of the Nation) in 1961.
History.
The founding of the SANNC was in direct response to injustice against black South Africans at the hands of the government then in power. It can be said that the SANNC had its origins in a pronouncement by Pixley ka Isaka Seme who said in 1911, "Forget all the past differences among Africans and unite in one national organisation." The SANNC was founded the following year on 8 January 1912.
The government of the newly formed Union of South Africa began a systematic oppression of black people in South Africa. The Land Act was promulgated in 1913 forcing many non-whites from their farms into the cities and towns to work, and to restrict their movement within South Africa.
By 1919, the SANNC was leading a campaign against passes (an ID which non-whites had to possess). However, it then became dormant in the mid-1920s. During that time, black people were also represented by the ICU and the previously white-only Communist party. In 1923, the organisation became the African National Congress, and in 1929 the ANC supported a militant mineworkers' strike.
By 1927, J.T. Gumede (president of the ANC) proposed co-operation with the Communists in a bid to revitalise the organisation, but he was voted out of power in the 1930s. This led to the ANC becoming largely ineffectual and inactive, until the mid-1940s when the ANC was remodelled as a mass movement.
The ANC responded to attacks on the rights of black South Africans, as well as calling for strikes, boycotts, and defiance. This led to a later Defiance Campaign in the 1950s, a mass movement of resistance to apartheid. The government tried to stop the ANC by banning party leaders and enacting new laws to stop the ANC, however these measures ultimately proved to be ineffective.
In 1955, the Congress of the People officially adopted the Freedom Charter, stating the core principles of the South African Congress Alliance, which consisted of the African National Congress and its allies the South African Communist Party (SACP), the South African Indian Congress, the South African Congress of Democrats (COD) and the Coloured People's Congress. The government claimed that this was a communist document, and consequently leaders of the ANC and Congress were arrested. 1960 saw the Sharpeville massacre, in which 69 people were killed when police opened fire on anti-apartheid protesters.
The ANC and its members were officially removed from the United States terrorism watch list in 2008.
Umkhonto we Sizwe.
Umkhonto we Sizwe or MK, translated "The Spear of the Nation", was the military wing of the ANC. Partly in response to the Sharpeville massacre of 1960, individual members of the ANC found it necessary to consider violence to combat what passive protest had failed to quell. MK commenced the military struggle against apartheid with acts of sabotage aimed at the installations of the state, and in the early stages was reluctant to target civilian targets.
MK was responsible for the deaths of both civilians and members of the military. Acts committed by MK include the Church Street bombing and the Magoo's Bar bombing.
In co-operation with the South African Communist Party, MK was founded in 1961 and integrated into the South African National Defence Force by 1994.
Ideology.
The ANC deems itself a force of national liberation in the post-apartheid era; it officially defines its agenda as the "National Democratic Revolution". The ANC is a member of the Socialist International. It also sets forth the redressing of socio-economic differences stemming from colonial- and apartheid-era policies as a central focus of ANC policy.
The National Democratic Revolution (NDR) is described as a process through which the National Democratic Society (NDS) is achieved; a society in which people are intellectually, socially, economically and politically empowered. The drivers of the NDR are also called the motive forces and are defined as the elements within society that gain from the success of the NDR. Using contour plots or concentric circles the centre represents the elements in society that gain the most out of the success of the NDR. Moving away from the centre results in the reduction of the gains that those elements derive. It is generally believed that the force that occupies the centre of those concentric circles in countries with low unemployment is the working class while in countries with higher levels of unemployment it is the unemployed. Some of the many theoreticians that have written about the NDR include Joe Slovo, Joel Netshitenzhe and Tshilidzi Marwala.
In 2004 the ANC declared itself to be a social democratic party.
The 53rd National Conference of the ANC, held in 2015, stated in its "Discussion Document" that "China economic development trajectory remains a leading example of the triumph of humanity over adversity. The exemplary role of the collective leadership of the Communist Party of China in this regard should be a guiding lodestar of our own struggle." It went on to state that "The collapse of the Berlin Wall and socialism in the Soviet Union and Eastern European States influenced our transition towards the negotiated political settlement in our country. The cause of events in the world changed tremendously in favour of the US led imperialism."
Tripartite Alliance.
The ANC holds a historic alliance with the South African Communist Party (SACP) and Congress of South African Trade Unions (COSATU), known as the "Tripartite Alliance". The SACP and COSATU have not contested any election in South Africa, but field candidates through the ANC, hold senior positions in the ANC, and influence party policy and dialogue. During Mbeki's presidency, the government took a more pro-capitalist stance, often running counter to the demands of the SACP and COSATU.
2008 schism.
Following Zuma's accession to the ANC leadership in 2007 and Mbeki's resignation as president in 2008, a number of former ANC leaders led by Mosiuoa Lekota split away from the ANC to form the Congress of the People.
2013 NUMSA split from Cosatu.
On 20 December 2013, a special congress of the National Union of Metalworkers of South Africa (NUMSA), the country's biggest trade union with 338,000 members, voted to withdraw support from the ANC and SACP, and form a socialist party to protect the interests of the working class. NUMSA secretary general Irvin Jim condemned the ANC and SACP's support for big business and stated: "It is clear that the working class cannot any longer see the ANC or the SACP as its class allies in any meaningful sense."
ANC flag.
The ANC flag is composed of three stripes – black, green and gold. Black symbolises the native people of South Africa, green represents the land and gold represents the mineral and other natural wealth of South Africa. This flag was also the battle flag of Umkhonto we Sizwe. An unrelated but identical flag was used by the Grand Duchy of Saxe-Weimar-Eisenach from 1813.
Party list.
Politicians in the party win a place in parliament by being on the "Party List", which is drawn up before the elections and enumerates, in order, the party's preferred MPs. The number of seats allocated is proportional to the popular national vote, and this determines the cut-off point.
The ANC has also gained members through the controversial floor crossing process.
Although most South African parties announced their candidate list for provincial premierships in the 2009 election, the ANC did not, as it is not required for parties to do so.
"ANC Today".
In 2001, the ANC launched an online weekly web-based newsletter, "ANC Today – Online Voice of the African National Congress" to offset the alleged bias of the press. It consists mainly of updates on current programmes and initiatives of the ANC. It is one of the few major online publications from an African political party.
Role of the ANC in resolving the conflict.
The ANC represented the main opposition to the government during apartheid and therefore they played a major role in resolving the conflict through participating in the peacemaking and peace-building processes. Initially intelligence agents of the National Party met in secret with ANC leaders, including Nelson Mandela, to judge whether conflict resolution was possible. Discussions and negotiations took place leading to the eventual unbanning of the ANC and other opposing political parties by then President de Klerk on 2 February 1990. These initial meetings were the first crucial steps towards resolution.
The next official step towards rebuilding South Africa was the Groote Schuur Minute where the government and the ANC agreed on a common commitment towards the resolution of the existing climate of violence and intimidation, as well as a commitment to stability and to a peaceful process of negotiations. The ANC negotiated the release of political prisoners and the indemnity from prosecution for returning exiles and moreover channels of communication were established between the Government and the ANC.
Later the Pretoria Minute represented another step towards resolution where agreements at Groote Schuur were reconsolidated and steps towards setting up an interim government and drafting a new constitution were established as well as suspension of the military wing of the ANC – the Umkhonto we Sizwe. This step helped end much of the violence within South Africa. Another agreement that came out of the Pretoria Minute was that both parties would try and raise awareness that a new way of governance was being created for South Africa, and that further violence would only hinder this process. However, violence still continued in Kwazulu-Natal, which violated the trust between Mandela and de Klerk. Moreover, internal disputes in the ANC prolonged the war as consensus on peace was not reached.
The next significant steps towards resolution were the Repeal of the Population Registration Act, the repeal of the Group Areas and the Native Land Acts and a catch-all Abolition of Racially Based Land Measures Act was passed. These measures ensured no one could claim, or be deprived of, any land rights on the basis of race.
In December 1991 the Convention for a Democratic South Africa (CODESA) was held with the aim of establishing an interim government. However, a few months later in June 1992 the Boipatong massacre occurred and all negotiations crumbled as the ANC pulled out. After this negotiations proceeded between two agents, Cyril Ramaphosa of the ANC, and Roelf Meyer of the National Party. In over 40 meetings the two men discussed and negotiated over many issues including the nature of the future political system, the fate of over 40,000 government employees and if/how the country would be divided. The result of these negotiations was an interim constitution that meant the transition from apartheid to democracy was a constitutional continuation and that the rule of law and state sovereignty remained intact during the transition, which was vital for stability within the country. A date was set for the first democratic elections on 27 April 1994. The ANC won 62.5% of the votes and has been in power ever since.
Criticism.
Controversy over corrupt members.
The most prominent corruption case involving the ANC relates to a series of bribes paid to companies involved in the ongoing R55 billion Arms Deal saga, which resulted in a long term jail sentence to then Deputy President Jacob Zuma's legal adviser Schabir Shaik. Shaik was released after nearly two years imprisonment after being diagnosed with a terminal illness. Zuma, now the State president, was charged with fraud, bribery and corruption in the Arms Deal, but the charges were subsequently withdrawn by the National Prosecuting Authority of South Africa due to their delay in prosecution. The ANC has also been criticised for its subsequent abolition of the Scorpions, the multidisciplinary agency that investigated and prosecuted organised crime and corruption, and was heavily involved in the investigation into Zuma and Shaik. Tony Yengeni, in his then position as chief whip of the ANC and also head of the Parliaments defence committee has recently been named as being involved in a R6 million bribe with the German company ThyssenKrupp over the purchase of four corvettes for the SANDF.
Other recent corruption issues include the sexual misconduct and criminal charges of Beaufort West municipal manager Truman Prince, and the Oilgate scandal, in which millions of Rand in funds from a state-owned company were allegedly funnelled into ANC coffers.
The ANC has also been accused of using government and civil society to fight its political battles against opposition parties such as the Democratic Alliance. The result has been a number of complaints and allegations that none of the political parties truly represent the interests of the poor. This has resulted in the "No Land! No House! No Vote!" Campaign which became very prominent during elections.
Controversy over wasteful expenditure.
The ANC spent over 1 billion of taxpayers' money on luxury vehicles, expensive hotels, banquets, advertising and other "wasteful expenditure" between August 2009 and April 2010. The main thrust behind this reporting is the official opposition in the country, the Democratic Alliance (DA), which kept a tally of the expenditure called "The Wasteful Expenditure Monitor".
According to the DA, this money could have:
The ANC Northern Cape premier, Sylvia Lucas, in her first 10 weeks in office, spent R53,159.00 of taxpayers money on "fast food" at outlets such as Spur, Nandos, KFC and Wimpy.
Condemnation over Secrecy Bill.
In late 2011 the ANC was heavily criticised over the passage of the Protection of State Information Bill, which opponents claimed would improperly restrict the freedom of the press. Opposition to the bill included otherwise ANC-aligned groups such as COSATU. Notably, Nelson Mandela and other Nobel laureates Nadine Gordimer, Archbishop Desmond Tutu, and F. W. de Klerk have expressed disappointment with the bill for not meeting standards of constitutionality and aspirations for freedom of information and expression.
Role in the Marikana killings.
The ANC have been criticised for its role in failing to prevent 16 August 2012 massacre of Lonmin miners at Marikana in the North West. Some allege that Police Commissioner Riah Phiyega and Police Minister Nathi Mthethwa, a close confidant of Jacob Zuma, may have given the go ahead for the police action against the miners on that day.
Commissioner Phiyega of the ANC came under further criticism as being insensitive and uncaring when she was caught smiling and laughing during the Farlam Commission's video playback of the 'massacre'. Archbishop Desmond Tutu has announced that he no longer can bring himself to exercise a vote for the ANC as it is no longer the party that he and Nelson Mandela fought for, and that the party has now lost its way, and is in danger of becoming a corrupt entity in power.
There is also controversy over an email that the ANC deputy president and shareholder in Lonmin, Cyril Ramaphosa sent where he asked for "decisive action" to be taken against the striking miners.

</doc>
<doc id="2504" url="https://en.wikipedia.org/wiki?curid=2504" title="Amphetamine">
Amphetamine

Amphetamine (contracted from ) is a potent central nervous system (CNS) stimulant that is used in the treatment of attention deficit hyperactivity disorder (ADHD), narcolepsy, and obesity. Amphetamine was discovered in 1887 and exists as two enantiomers: levoamphetamine and dextroamphetamine. "Amphetamine" properly refers to a specific chemical, the racemic free base, which is equal parts of the two enantiomers, levoamphetamine and dextroamphetamine, in their pure amine forms. However, the term is frequently used informally to refer to any combination of the enantiomers, or to either of them alone. Historically, it has been used to treat nasal congestion and depression. Amphetamine is also used as a performance and cognitive enhancer, and recreationally as an aphrodisiac and euphoriant. It is a prescription drug in many countries, and unauthorized possession and distribution of amphetamine are often tightly controlled due to the significant health risks associated with recreational use.
The first pharmaceutical amphetamine was Benzedrine, a brand of inhalers used to treat a variety of conditions. Currently, pharmaceutical amphetamine is prescribed as racemic amphetamine, Adderall, dextroamphetamine, or the inactive prodrug lisdexamfetamine. Amphetamine, through activation of a trace amine receptor, increases monoamine and excitatory neurotransmitter activity in the brain, with its most pronounced effects targeting the catecholamine neurotransmitters norepinephrine and dopamine. At therapeutic doses, this causes emotional and cognitive effects such as euphoria, change in libido, increased wakefulness, and improved cognitive control. It induces physical effects such as decreased reaction time, fatigue resistance, and increased muscle strength.
Much larger doses of amphetamine may impair cognitive function and induce rapid muscle breakdown. Drug addiction is a serious risk with large recreational doses, but rarely arises from medical use. Very high doses can result in psychosis (e.g., delusions and paranoia) which rarely occurs at therapeutic doses even during long-term use. Recreational doses are generally much larger than prescribed therapeutic doses and carry a far greater risk of serious side effects.
Amphetamine belongs to the phenethylamine class. It is also the parent compound of its own structural class, the substituted amphetamines, which includes prominent substances such as bupropion, cathinone, MDMA (ecstasy), and methamphetamine. As a member of the phenethylamine class, amphetamine is also chemically related to the naturally occurring trace amine neuromodulators, specifically phenethylamine and , both of which are produced within the human body. Phenethylamine is the parent compound of amphetamine, while is a constitutional isomer that differs only in the placement of the methyl group.
Uses.
Medical.
<onlyinclude></onlyinclude>
Enhancing performance.
<onlyinclude></onlyinclude>
Contraindications.
<onlyinclude></onlyinclude>
Side effects.
The side effects of amphetamine are varied, and the amount of amphetamine used is the primary factor in determining the likelihood and severity of side effects. Amphetamine products such as Adderall, Dexedrine, and their generic equivalents are currently approved by the USFDA for long-term therapeutic use. Recreational use of amphetamine generally involves much larger doses, which have a greater risk of serious side effects than dosages used for therapeutic reasons.
<onlyinclude></onlyinclude>
Overdose.
<onlyinclude></onlyinclude>
Interactions.
Many types of substances are known to interact with amphetamine, resulting in altered drug action or metabolism of amphetamine, the interacting substance, or both. Inhibitors of the enzymes that metabolize amphetamine (e.g., CYP2D6 and flavin-containing monooxygenase 3) will prolong its elimination half-life, meaning that its effects will last longer. Amphetamine also interacts with , particularly monoamine oxidase A inhibitors, since both MAOIs and amphetamine increase plasma catecholamines (i.e., norepinephrine and dopamine); therefore, concurrent use of both is dangerous. Amphetamine modulates the activity of most psychoactive drugs. In particular, amphetamine may decrease the effects of sedatives and depressants and increase the effects of stimulants and antidepressants. Amphetamine may also decrease the effects of antihypertensives and antipsychotics due to its effects on blood pressure and dopamine respectively. In general, there is no significant interaction when consuming amphetamine with food, but the pH of gastrointestinal content and urine affects the absorption and excretion of amphetamine, respectively. Acidic substances reduce the absorption of amphetamine and increase urinary excretion, and alkaline substances do the opposite. Due to the effect pH has on absorption, amphetamine also interacts with gastric acid reducers such as proton pump inhibitors and H antihistamines, which increase gastrointestinal pH (i.e., make it less acidic).
Pharmacology.
Pharmacodynamics.
Amphetamine exerts its behavioral effects by altering the use of monoamines as neuronal signals in the brain, primarily in catecholamine neurons in the reward and executive function pathways of the brain. The concentrations of the main neurotransmitters involved in reward circuitry and executive functioning, dopamine and norepinephrine, increase dramatically in a dose-dependent manner by amphetamine due to its effects on monoamine transporters. The reinforcing and task saliency effects of amphetamine are mostly due to enhanced dopaminergic activity in the mesolimbic pathway.
Amphetamine has been identified as a potent full agonist of trace amine-associated receptor 1 (TAAR1), a and G protein-coupled receptor (GPCR) discovered in 2001, which is important for regulation of brain monoamines. Activation of increases production via adenylyl cyclase activation and inhibits monoamine transporter function. Monoamine autoreceptors (e.g., D short, presynaptic α, and presynaptic 5-HT) have the opposite effect of TAAR1, and together these receptors provide a regulatory system for monoamines. Notably, amphetamine and trace amines bind to TAAR1, but not monoamine autoreceptors. Imaging studies indicate that monoamine reuptake inhibition by amphetamine and trace amines is site specific and depends upon the presence of TAAR1 in the associated monoamine neurons. of TAAR1 and the dopamine transporter (DAT) has been visualized in rhesus monkeys, but of TAAR1 with the norepinephrine transporter (NET) and the serotonin transporter (SERT) has only been evidenced by messenger RNA (mRNA) expression.
In addition to the neuronal monoamine transporters, amphetamine also inhibits both vesicular monoamine transporters, VMAT1 and VMAT2, as well as SLC1A1, SLC22A3, and SLC22A5. SLC1A1 is excitatory amino acid transporter 3 (EAAT3), a glutamate transporter located in neurons, SLC22A3 is an extraneuronal monoamine transporter that is present in astrocytes, and SLC22A5 is a high-affinity carnitine transporter. Amphetamine is known to strongly induce cocaine- and amphetamine-regulated transcript (CART) gene expression, a neuropeptide involved in feeding behavior, stress, and reward, which induces observable increases in neuronal development and survival "in vitro". The CART receptor has yet to be identified, but there is significant evidence that CART binds to a unique . Amphetamine also inhibits monoamine oxidase at very high doses, resulting in less dopamine and phenethylamine metabolism and consequently higher concentrations of synaptic monoamines. In humans, the only post-synaptic receptor at which amphetamine is known to bind is the receptor, where it acts as an agonist with micromolar affinity.
The full profile of amphetamine's short-term drug effects in humans is mostly derived through increased cellular communication or neurotransmission of dopamine, serotonin, norepinephrine, epinephrine, histamine, CART peptides, acetylcholine, endogenous opioids, adrenocorticotropic hormone, corticosteroids, and glutamate, which it effects through interactions with , , , , , , and possibly other biological targets.
Dextroamphetamine is a more potent agonist of than levoamphetamine. Consequently, dextroamphetamine produces greater stimulation than levoamphetamine, roughly three to four times more, but levoamphetamine has slightly stronger cardiovascular and peripheral effects.
Dopamine.
In certain brain regions, amphetamine increases the concentration of dopamine in the synaptic cleft. Amphetamine can enter the presynaptic neuron either through or by diffusing across the neuronal membrane directly. As a consequence of DAT uptake, amphetamine produces competitive reuptake inhibition at the transporter. Upon entering the presynaptic neuron, amphetamine activates which, through protein kinase A (PKA) and protein kinase C (PKC) signaling, causes DAT phosphorylation. Phosphorylation by either protein kinase can result in DAT internalization ( reuptake inhibition), but phosphorylation alone induces reverse transporter function (dopamine efflux). Amphetamine is also known to increase intracellular calcium, an effect which is associated with DAT phosphorylation through an unidentified Ca2+/calmodulin-dependent protein kinase (CAMK)-dependent pathway, in turn producing dopamine efflux. Through direct activation of G protein-coupled inwardly-rectifying potassium channels, reduces the firing rate of postsynaptic dopamine neurons, preventing a hyper-dopaminergic state.
Amphetamine is also a substrate for the presynaptic vesicular monoamine transporter, . Following amphetamine uptake at VMAT2, the synaptic vesicle releases dopamine molecules into the cytosol in exchange. Subsequently, the cytosolic dopamine molecules exit the presynaptic neuron via reverse transport at .
Norepinephrine.
Similar to dopamine, amphetamine dose-dependently increases the level of synaptic norepinephrine, the direct precursor of epinephrine. Based upon neuronal expression, amphetamine is thought to affect norepinephrine analogously to dopamine. In other words, amphetamine induces TAAR1-mediated efflux and reuptake inhibition at phosphorylated , competitive NET reuptake inhibition, and norepinephrine release from .
Serotonin.
Amphetamine exerts analogous, yet less pronounced, effects on serotonin as on dopamine and norepinephrine. Amphetamine affects serotonin via and, like norepinephrine, is thought to phosphorylate via . Like dopamine, amphetamine has low, micromolar affinity at the human 5-HT1A receptor.
Other neurotransmitters, peptides, and hormones.
Amphetamine has no direct effect on acetylcholine neurotransmission, but several studies have noted that acetylcholine release increases after its use. In lab animals, amphetamine increases acetylcholine levels in certain brain regions as a downstream effect. In humans, a similar phenomenon occurs via the ghrelin-mediated cholinergic–dopaminergic reward link in the ventral tegmental area. Acute amphetamine administration in humans also increases endogenous opioid release in several brain structures in the reward system. Extracellular levels of glutamate, the primary excitatory neurotransmitter in the brain, have been shown to increase upon exposure to amphetamine. This cotransmission effect was found in the mesolimbic pathway, an area of the brain implicated in reward, where amphetamine is known to affect dopamine neurotransmission.
Amphetamine also induces the selective release of histamine from mast cells and efflux from histaminergic neurons through . Acute amphetamine administration can also increase adrenocorticotropic hormone and corticosteroid levels in blood plasma by stimulating the hypothalamic–pituitary–adrenal axis.
Pharmacokinetics.
<onlyinclude></onlyinclude>
Related endogenous compounds.
<onlyinclude></onlyinclude>
Physical and chemical properties.
Amphetamine is a methyl homolog of the mammalian neurotransmitter phenethylamine with the chemical formula . The carbon atom adjacent to the primary amine is a stereogenic center, and amphetamine is composed of a racemic 1:1 mixture of two enantiomeric mirror images. This racemic mixture can be separated into its optical isomers: levoamphetamine and dextroamphetamine. Physically, at room temperature, the pure free base of amphetamine is a mobile, colorless, and volatile liquid with a characteristically strong amine odor, and acrid, burning taste. Frequently prepared solid salts of amphetamine include amphetamine aspartate, hydrochloride, phosphate, saccharate, and sulfate, the last of which is the most common amphetamine salt. Amphetamine is also the parent compound of its own structural class, which includes a number of psychoactive derivatives. In organic chemistry, amphetamine is an excellent chiral ligand for the stereoselective synthesis of .
Substituted derivatives.
The substituted derivatives of amphetamine, sometimes referred to as "amphetamines" or "substituted amphetamines", are a broad range of chemicals that contain amphetamine as a "backbone"; specifically, this chemical class includes derivative compounds that are formed by replacing one or more hydrogen atoms in the amphetamine core structure with substituents. The class includes stimulants like methamphetamine, serotonergic empathogens like MDMA, and decongestants like ephedrine, among other subgroups.
Synthesis.
Since the first preparation was reported in 1887, numerous synthetic routes to amphetamine have been developed. The most common route of both legal and illicit amphetamine synthesis employs a non-metal reduction known as the Leuckart reaction (method 1). In the first step, a reaction between phenylacetone and formamide, either using additional formic acid or formamide itself as a reducing agent, yields . This intermediate is then hydrolyzed using hydrochloric acid, and subsequently basified, extracted with organic solvent, concentrated, and distilled to yield the free base. The free base is then dissolved in an organic solvent, sulfuric acid added, and amphetamine precipitates out as the sulfate salt.
A number of chiral resolutions have been developed to separate the two enantiomers of amphetamine. For example, racemic amphetamine can be treated with to form a diastereoisomeric salt which is fractionally crystallized to yield dextroamphetamine. Chiral resolution remains the most economical method for obtaining optically pure amphetamine on a large scale. In addition, several enantioselective syntheses of amphetamine have been developed. In one example, optically pure is condensed with phenylacetone to yield a chiral Schiff base. In the key step, this intermediate is reduced by catalytic hydrogenation with a transfer of chirality to the carbon atom alpha to the amino group. Cleavage of the benzylic amine bond by hydrogenation yields optically pure dextroamphetamine.
A large number of alternative synthetic routes to amphetamine have been developed based on classic organic reactions. One example is the Friedel–Crafts alkylation of chlorobenzene by allyl chloride to yield beta chloropropylbenzene which is then reacted with ammonia to produce racemic amphetamine (method 2). Another example employs the Ritter reaction (method 3). In this route, allylbenzene is reacted acetonitrile in sulfuric acid to yield an organosulfate which in turn is treated with sodium hydroxide to give amphetamine via an acetamide intermediate. A third route starts with which through a double alkylation with methyl iodide followed by benzyl chloride can be converted into acid. This synthetic intermediate can be transformed into amphetamine using either a Hofmann or Curtius rearrangement (method 4).
A significant number of amphetamine syntheses feature a reduction of a nitro, imine, oxime or other nitrogen-containing functional groups. In one such example, a Knoevenagel condensation of benzaldehyde with nitroethane yields . The double bond and nitro group of this intermediate is reduced using either catalytic hydrogenation or by treatment with lithium aluminium hydride (method 5). Another method is the reaction of phenylacetone with ammonia, producing an imine intermediate that is reduced to the primary amine using hydrogen over a palladium catalyst or lithium aluminum hydride (method 6).
Detection in body fluids.
Amphetamine is frequently measured in urine or blood as part of a drug test for sports, employment, poisoning diagnostics, and forensics. Techniques such as immunoassay, which is the most common form of amphetamine test, may cross-react with a number of sympathomimetic drugs. Chromatographic methods specific for amphetamine are employed to prevent false positive results. Chiral separation techniques may be employed to help distinguish the source of the drug, whether prescription amphetamine, prescription amphetamine prodrugs, (e.g., selegiline), over-the-counter drug products (e.g., the American version of Vicks VapoInhaler, which contains levomethamphetamine) or illicitly obtained substituted amphetamines. Several prescription drugs produce amphetamine as a metabolite, including benzphetamine, clobenzorex, famprofazone, fenproporex, lisdexamfetamine, mesocarb, methamphetamine, prenylamine, and selegiline, among others. These compounds may produce positive results for amphetamine on drug tests. Amphetamine is generally only detectable by a standard drug test for approximately 24 hours, although a high dose may be detectable for two to four days.
For the assays, a study noted that an enzyme multiplied immunoassay technique (EMIT) assay for amphetamine and methamphetamine may produce more false positives than liquid chromatography–tandem mass spectrometry. Gas chromatography–mass spectrometry (GC–MS) of amphetamine and methamphetamine with the derivatizing agent chloride allows for the detection of methamphetamine in urine. GC–MS of amphetamine and methamphetamine with the chiral derivatizing agent Mosher's acid chloride allows for the detection of both dextroamphetamine and dextromethamphetamine in urine. Hence, the latter method may be used on samples that test positive using other methods to help distinguish between the various sources of the drug.
History, society, and culture.
Amphetamine was first synthesized in 1887 in Germany by Romanian chemist Lazăr Edeleanu who named it "phenylisopropylamine"; its stimulant effects remained unknown until 1927, when it was independently resynthesized by Gordon Alles and reported to have sympathomimetic properties. Amphetamine had no pharmacological use until 1934, when Smith, Kline and French began selling it as an inhaler under the trade name Benzedrine as a decongestant. During World War II, amphetamine and methamphetamine were used extensively by both the Allied and Axis forces for their stimulant and performance-enhancing effects. As the addictive properties of the drug became known, governments began to place strict controls on the sale of amphetamine. For example, during the early 1970s in the United States, amphetamine became a schedule II controlled substance under the Controlled Substances Act. In spite of strict government controls, amphetamine has been used legally or illicitly by people from a variety of backgrounds, including authors, musicians, mathematicians, and athletes.
Amphetamine is still illegally synthesized today in clandestine labs and sold on the black market, primarily in European countries. Among European Union (EU) member states, 1.2 million young adults used illicit amphetamine or methamphetamine in 2013. During 2012, approximately 5.9 metric tons of illicit amphetamine were seized within EU member states; the "street price" of illicit amphetamine within the EU ranged from €6–38 per gram during the same period. Outside Europe, the illicit market for amphetamine is much smaller than the market for methamphetamine and MDMA.
Legal status.
As a result of the United Nations 1971 Convention on Psychotropic Substances, amphetamine became a schedule II controlled substance, as defined in the treaty, in all (183) state parties. Consequently, it is heavily regulated in most countries. Some countries, such as South Korea and Japan, have banned substituted amphetamines even for medical use. In other nations, such as Canada (schedule I drug), the Netherlands (List I drug), the United States (schedule II drug), Australia (schedule 8), Thailand (category 1 narcotic), and United Kingdom (class B drug), amphetamine is in a restrictive national drug schedule that allows for its use as a medical treatment.
Pharmaceutical products.
Three currently prescribed amphetamine formulations that contain both enantiomers are Adderall, Dyanavel XR, and Evekeo, the last of which is racemic amphetamine sulfate. Amphetamine is also prescribed in enantiopure and prodrug form as dextroamphetamine and lisdexamfetamine respectively. Lisdexamfetamine is structurally different from amphetamine, and is inactive until it metabolizes into dextroamphetamine. The free base of racemic amphetamine was previously available as Benzedrine, Psychedrine, and Sympatedrine. Levoamphetamine was previously available as Cydril. Many current amphetamine pharmaceuticals are salts due to the comparatively high volatility of the free base. Some of the current brands and their generic equivalents are listed below.

</doc>
<doc id="2506" url="https://en.wikipedia.org/wiki?curid=2506" title="Asynchronous communication">
Asynchronous communication

In telecommunications, asynchronous communication is transmission of data, generally without the use of an external clock signal, where data can be transmitted intermittently rather than in a steady stream. Any timing required to recover data from the communication symbols is encoded within the symbols. A notable exception is the RS-232 port, and some derivatives, which are asynchronous, but still have an external clock signal available, although not commonly used. The most significant aspect of asynchronous communications is that data is not transmitted at regular intervals, thus making possible variable bit rate, and that the transmitter and receiver clock generators do not have to be exactly synchronized all the time.
Physical layer.
In asynchronous serial communication the physical protocol layer, the data blocks are code words of a certain word length, for example octets (bytes) or ASCII characters, delimited by start bits and stop bits. A variable length space can be inserted between the code words. No bit synchronization signal is required. This is sometimes called character oriented communication. Examples are the RS-232C serial standard, and MNP2 and V.2 modems and older.
Data link layer and higher.
Asynchronous communication at the data link layer or higher protocol layers is known as statistical multiplexing, for example asynchronous transfer mode (ATM). In this case the asynchronously transferred blocks are called data packets, for example ATM cells. The opposite is circuit switched communication, which provides constant bit rate, for example ISDN and SONET/SDH.
The packets may be encapsulated in a data frame, with a frame synchronization bit sequence indicating the start of the frame, and sometimes also a bit synchronization bit sequence, typically 01010101, for identification of the bit transition times. Note that at the physical layer, this is considered as synchronous serial communication. Examples of packet mode data link protocols that can be/are transferred using synchronous serial communication are the HDLC, Ethernet, PPP and USB protocols.
Application layer.
An asynchronous communication service or application does not require a constant bit rate. Examples are file transfer, email and the World Wide Web. An example of the opposite, a synchronous communication service, is realtime streaming media, for example IP telephony, IP-TV and video conferencing.
Electronically mediated communication.
Electronically mediated communication often happens asynchronously in that the participants do not communicate concurrently. Examples include email
and bulletin-board systems, where participants send or post messages at different times. The term "asynchronous communication" acquired currency in the field of online learning, where teachers and students often exchange information asynchronously instead of synchronously (that is, simultaneously), as they would in face-to-face or in telephone conversations.

</doc>
<doc id="2508" url="https://en.wikipedia.org/wiki?curid=2508" title="Artillery">
Artillery

Artillery is a class of large military weapons built to fire munitions far beyond the range and power of infantry's small arms. Early artillery development focused on the ability to breach fortifications, and led to heavy, fairly immobile siege engines. As technology improved, lighter, more mobile field artillery developed for battlefield use. This development continues today; modern self-propelled artillery vehicles are highly mobile weapons of great versatility providing the largest share of an army's total firepower.
In its earliest sense, the word artillery referred to any group of soldiers primarily armed with some form of manufactured weapon or armour. Since the introduction of gunpowder and cannon, the word "artillery" has largely meant cannon, and in contemporary usage, it usually refers to shell-firing guns, howitzers, mortars, rockets and guided missiles. In common speech, the word artillery is often used to refer to individual devices, along with their accessories and fittings, although these assemblages are more properly called "equipments". However, there is no generally recognised generic term for a gun, howitzer, mortar, and so forth: the United States uses "artillery piece", but most English-speaking armies use "gun" and "mortar". The projectiles fired are typically either "shot" (if solid) or "shell" (if not). "Shell" is a widely used generic term for a projectile, which is a component of munitions.
By association, artillery may also refer to the arm of service that customarily operates such engines. In some armies one arm has operated field, coast, anti-aircraft artillery and some anti-tank artillery, in others these have been separate arms and in some nations coast has been a naval or marine responsibility. In the 20th Century technology based target acquisition devices, such as radar, and systems, such as sound ranging and flash spotting, emerged to acquire targets, primarily for artillery. These are usually operated by one or more of the artillery arms. The widespread adoption of indirect fire in the early 20th Century introduced the need for specialist data for field artillery, notably survey and meteorological, in some armies provision of these are the responsibility of the artillery arm.
Artillery originated for use against ground targets—against infantry, cavalry and other artillery. An early specialist development was coast artillery for use against enemy ships. The early 20th Century saw the development of a new class of artillery for use against aircraft: anti-aircraft guns.
Artillery is arguably the most lethal form of land-based armament currently employed, and has been since at least the early Industrial Revolution. The majority of combat deaths in the Napoleonic Wars, World War I, and World War II were caused by artillery. In 1944, Joseph Stalin said in a speech that artillery was "the God of War".
Artillery piece.
Although not called as such, machines performing the role recognizable as artillery have been employed in warfare since antiquity. The first references in the western historical tradition begin at Syracuse in 399 BC, and these devices were widely employed by the Roman legions in Republican times well before the Christian era. Until the introduction of gunpowder into western warfare, artillery depended upon mechanical energy to operate, and this severely limited the kinetic energy of the projectiles, while also requiring the construction of very large apparatus to store sufficient energy. For comparison, a Roman 1st-century BC catapult using stones of 6.55 kg fired with a kinetic energy of 16,000 joules, while a mid-19th-century 12-pounder gun firing projectiles of 4.1 kg fired the projectile with a kinetic energy of 240,000 joules.
From the Middle Ages through most of the modern era, artillery pieces on land were moved by horse-drawn gun carriages. In the contemporary era, the artillery and crew rely on wheeled or tracked vehicles as transportation, though some of the largest were railway guns. Artillery used by naval forces has changed significantly also, with missiles replacing guns in surface warfare.
Over the course of military history, projectiles were manufactured from a wide variety of materials, made in a wide variety of shapes, and used different means of inflicting physical damage and casualties to defeat specific types of targets. The engineering designs of the means of delivery have likewise changed significantly over time, and have become some of the most complex technological application today.
In some armies, the weapon of artillery is the projectile, not the equipment that fires it. The process of delivering fire onto the target is called gunnery. The actions involved in operating the piece are collectively called "serving the gun" by the "detachment" or gun crew, constituting either direct or indirect artillery fire. The manner in which artillery units or formations are employed is called artillery support, and may at different periods in history refer to weapons designed to be fired from ground-, sea-, and even air-based weapons platforms.
Crew.
The term "gunner" is used in some armed forces for the soldiers and sailors with the primary function of using artillery. 
The gunners and their guns are usually grouped in teams called either "crews" or "detachments". Several such crews and teams with other functions are combined into a unit of artillery, usually called a battery, although sometimes called a company. In gun detachments, each role is numbered, starting with "1" the Detachment Commander, and the highest number being the Coverer, the second-in-command. "Gunner" is also the lowest rank and junior non-commissioned officers are "Bombardiers" in some artillery arms.
Batteries are roughly equivalent to a company in the infantry, and are combined into larger military organizations for administrative and operational purposes, either battalions or regiments, depending on the army. These may be grouped into brigades; the Russian army also groups some brigades into artillery divisions, and the People's Liberation Army has artillery corps.
The term "artillery" is also applied to a combat arm of most military services when used organizationally to describe units and formations of the national armed forces that operate the weapons.
During military operations, the role of field artillery is to provide support to other arms in combat or to attack targets, particularly in depth. Broadly, these effects fall into two categories, either to suppress or neutralize the enemy, or to cause casualties, damage, and destruction. This is mostly achieved by delivering high-explosive munitions to suppress, or inflict casualties on the enemy from casing fragments and other debris and blast, or by destroying enemy positions, equipment, and vehicles. Non-lethal munitions, notably smoke, can also be used to suppress or neutralize the enemy by obscuring their view.
Fire may be directed by an artillery observer or other observer, including manned and unmanned aircraft pilots, or called onto map coordinates.
Military doctrine has played a significant influence on the core engineering design considerations of artillery ordnance through its history, in seeking to achieve a balance between delivered volume of fire with ordnance mobility. However, during the modern period, the consideration of protecting the gunners also arose due to the late-19th-century introduction of the new generation of infantry weapons using conoidal bullet, better known as the Minié ball, with a range almost as long as that of field artillery.
The gunners' increasing proximity to and participation in direct combat against other combat arms and attacks by aircraft made the introduction of a gun shield necessary. The problems of how to employ a fixed or horse-towed gun in mobile warfare necessitated the development of new methods of transporting the artillery into combat. Two distinct forms of artillery developed: the towed gun, which was used primarily to attack or defend a fixed line; and the self-propelled gun, which was designed to accompany a mobile force and provide continuous fire support. These influences have guided the development of artillery ordnance, systems, organisations, and operations until the present, with artillery systems capable of providing support at ranges from as little as 100 m to the intercontinental ranges of ballistic missiles. The only combat in which artillery is unable to take part in is close quarters combat, with the possible exception of artillery reconnaissance teams.
Etymology.
The word as used in the current context originated in the Middle Ages. One suggestion is that it comes from the Old French "atellier", meaning "to arrange", and "attillement", meaning "equipment".
From the 13th century, an "artillier" referred to a builder of any war equipment; and, for the next 250 years, the sense of the word "artillery" covered all forms of military weapons. Hence, the naming of the Honourable Artillery Company an essentially infantry unit until the 19th century. Another suggestion is that it comes from the Italian "arte de tirare" (art of shooting), coined by one of the first theorists on the use of artillery, Niccolò Tartaglia.
History.
Mechanical systems used for throwing ammunition in ancient warfare, also known as "engines of war", like the catapult, onager, trebuchet, and ballista, are also referred to by military historians as artillery.
Invention of gunpowder.
The first documented record of artillery with gunpowder propellant used on the battlefield was on January 28, 1132, when General Han Shizhong of the Song dynasty used escalade and Huochong to capture a city in Fujian. Early Chinese artillery had vase-like shapes. This includes the "long range awe inspiring" cannon dated from 1350 and found in the 14th century Ming Dynasty treatise "Huolongjing". With the development of better metallurgy techniques, later cannons abandoned the vase shape of early Chinese artillery. This change can be seen in the bronze "thousand ball thunder cannon," an early example of field artillery. These small, crude weapons diffused into the Middle East (the "madfaa", see also and "midfa") and reached Europe in the 13th century, in a very limited manner.
In Asia, Mongols adopted the Chinese artillery and used it effectively in the great conquest. By the late 14th century, Chinese rebels used organized artillery and cavalry to push Mongols out.
As small smooth-bore tubes these were initially cast in iron or bronze around a core, with the first drilled bore ordnance recorded in operation near Seville in 1247. They fired lead, iron, or stone balls, sometimes large arrows and on occasions simply handfuls of whatever scrap came to hand. During the Hundred Years' War, these weapons became more common, initially as the bombard and later the cannon. Cannon were always muzzle-loaders. While there were many early attempts at breech-loading designs, a lack of engineering knowledge rendered these even more dangerous to use than muzzle-loaders.
Expansion of artillery use.
In 1415, the Portuguese invaded the Mediterranean port town of Ceuta. While it is difficult to confirm the use of firearms in the siege of the city, it is known the Portuguese defended it thereafter with firearms, namely "bombardas", "colebratas", and "falconetes". In 1419, Sultan Abu Sa'id led an army to reconquer the fallen city, and Moroccans brought cannons and used them in the assault on Ceuta. Finally, hand-held firearms and riflemen appear in Morocco, in 1437, in an expedition against the people of Tangiers. It is clear these weapons had developed into several different forms, from small guns to large artillery pieces.
The artillery revolution in Europe caught on during the Hundred Years' War and changed the way that battles were fought. In the preceding decades, the English had even used a gunpowder-like weapon in military campaigns against the Scottish. However, at this time, the cannons used in battle were very small and not particularly powerful. Cannons were only useful for the defense of a castle, as demonstrated at Breteuil in 1356, when the besieged English used a cannon to destroy an attacking French assault tower. By the end of the 14th century, cannon were only powerful enough to knock in roofs, and could not penetrate castle walls.
However, a major change occurred between 1420 and 1430, when artillery became much more powerful and could now batter strongholds and fortresses quite efficiently. The English, French, and Burgundians all advanced in military technology, and as a result the traditional advantage that went to the defense in a siege was lost. The cannon during this period were elongated, and the recipe for gunpowder was improved to make it three times as powerful as before. These changes led to the increased power in the artillery weapons of the time.
Joan of Arc encountered gunpowder weaponry several times. When she led the French against the English at the Battle of Tourelles, in 1430, she faced heavy gunpowder fortifications, and yet her troops prevailed in that battle. In addition, she led assaults against the English-held towns of Jargeau, Meung, and Beaugency, all with the support of large artillery units. When she led the assault on Paris, Joan faced stiff artillery fire, especially from the suburb of St. Denis, which ultimately led to her defeat in this battle. In April 1430, she went to battle against the Burgundians, whose support was purchased by the English. At this time, the Burgundians had the strongest and largest gunpowder arsenal among the European powers, and yet the French, under Joan of Arc's leadership, were able to beat back the Burgundians and defend themselves. As a result, most of the battles of the Hundred Years' War that Joan of Arc participated in were fought with gunpowder artillery.
The army of Mehmet the Conqueror, which conquered Constantinople in 1453, included both artillery and foot soldiers armed with gunpowder weapons. The Ottomans brought to the siege sixty-nine guns in fifteen separate batteries and trained them at the walls of the city. The barrage of Ottoman cannon fire lasted forty days, and they are estimated to have fired 19,320 times. Artillery also played a decisive role in the Battle of St. Jakob an der Birs of 1444.
The new Ming Dynasty established the "Divine Engine Battalion" (神机营), which specialized in various types of artillery. Light cannons and cannons with multiple volleys were developed. In a campaign to suppress a local minority rebellion near today's Burmese border, the Ming army used a 3-line formation of arquebuses/muskets to destroy an elephant formation.
Between 1593 and 1597, about 200,000 Korean and Chinese troops which fought against Japan in Korea actively used heavy artillery in both siege and field combat. Korean forces mounted artillery in ships as naval guns, providing an advantage against Japanese navy which used "Kunikuzushi" (国崩し – Japanese breech-loading swivel gun) and "Ōzutsu" (大筒 – large size Tanegashima) as their largest firearms.
Smoothbores.
Bombards were of value mainly in sieges. A famous Turkish example used at the siege of Constantinople in 1453 weighed 19 tons, took 200 men and sixty oxen to emplace, and could fire just seven times a day. The Fall of Constantinople was perhaps "the first event of supreme importance whose result was determined by the use of artillery" when the huge bronze cannons of Mehmed II breached the city's walls, ending the Byzantine Empire, according to Sir Charles Oman.
Bombards developed in Europe were massive smoothbore weapons distinguished by their lack of a field carriage, immobility once emplaced, highly individual design, and noted unreliability (in 1460 James II, King of Scots, was killed when one exploded at the siege of Roxburgh). Their large size precluded the barrels being cast and they were constructed out of metal staves or rods bound together with hoops like a barrel, giving their name to the gun barrel.
The use of the word "cannon" marks the introduction in the 15th century of a dedicated field carriage with axle, trail and animal-drawn limber—this produced mobile field pieces that could move and support an army in action, rather than being found only in siege and static defences. The reduction in the size of the barrel was due to improvements in both iron technology and gunpowder manufacture, while the development of trunnions – projections at the side of the cannon as an integral part of the cast – allowed the barrel to be fixed to a more movable base, and also made raising or lowering the barrel much easier.
The first land-based mobile weapon is usually credited to Jan Žižka, who deployed his oxen-hauled cannon during the Hussite Wars of Bohemia (1418–1424). However cannons were still large and cumbersome. With the rise of musketry in the 16th century, cannon were largely (though not entirely) displaced from the battlefield—the cannon were too slow and cumbersome to be used and too easily lost to a rapid enemy advance.
The combining of shot and powder into a single unit, a cartridge, occurred in the 1620s with a simple fabric bag, and was quickly adopted by all nations. It speeded loading and made it safer, but unexpelled bag fragments were an additional fouling in the gun barrel and a new tool—a worm—was introduced to remove them. Gustavus Adolphus is identified as the general who made cannon an effective force on the battlefield—pushing the development of much lighter and smaller weapons and deploying them in far greater numbers than previously. The outcome of battles was still determined by the clash of infantry.
Shells, explosive-filled fused projectiles, were also developed in the 17th century. The development of specialized pieces—shipboard artillery, howitzers and mortars—was also begun in this period. More esoteric designs, like the multi-barrel "ribauldequin" (known as "organ guns"), were also produced.
The 1650 book by Kazimierz Siemienowicz "Artis Magnae Artilleriae pars prima" was one of the most important contemporary publications on the subject of artillery. For over two centuries this work was used in Europe as a basic artillery manual.
One of the most significant effects of artillery during this period was however somewhat more indirect – by easily reducing to rubble any medieval-type fortification or city wall (some which had stood since Roman times), it abolished millennia of siege-warfare strategies and styles of fortification building. This led, among other things, to a frenzy of new bastion-style fortifications to be built all over Europe and in its colonies, but also had a strong integrating effect on emerging nation-states, as kings were able to use their newfound artillery superiority to force any local dukes or lords to submit to their will, setting the stage for the absolutist kingdoms to come.
Modern rocket artillery can trace its heritage back to the Mysorean rockets of India. Their first recorded use was in 1780 during the battles of the Second, Third and Fourth Mysore Wars. The wars fought between the British East India Company and the Kingdom of Mysore in India made use of the rockets as a weapon. In the Battle of Pollilur (1780), the Siege of Seringapatam (1792) and in Battle of Seringapatam in 1799 these rockets were used with considerable effect against the British." After the wars, several Mysore rockets were sent to England, and from 1801, William Congreve copied the rockets with minor modifications as the Congreve rocket which were used effectively during the Napoleonic Wars and the War of 1812.
Napoleonic artillery.
Cannons continued to become smaller and lighter—Frederick II of Prussia deployed the first genuine light artillery during the Seven Years' War.
Jean-Baptiste de Gribeauval, a French artillery engineer, introduced the standardization of cannon design in the mid-18th century. He developed a field howitzer whose gun barrel, carriage assembly and ammunition specifications were made uniform for all French cannons. The standardized interchangeable parts of these cannons down to the nuts, bolts and screws made their mass production and repair much easier. Another major change at this time was the development of a flintlock firing mechanism for the cannons to replace the old method of igniting powder in the cannon touchhole. The flintlock was a far more reliable (and safe) mechanism.
These improvements in the French artillery were essential for the later military successes of Napoleon. Napoleon, himself a former artillery officer, perfected the tactic of massed artillery batteries unleashed upon a critical point in his enemies' line as a prelude to a decisive infantry and cavalry assault.
Modern artillery.
The development of modern artillery occurred in the mid to late 19th century as a result of the convergence of various improvements in the underlying technology. Advances in metallurgy allowed for the construction of breech-loading rifled guns that could fire at a much greater muzzle velocity.
After the British artillery was shown up in the Crimean War as having barely changed since the Napoleonic Wars the industrialist William Armstrong was awarded a contract by the government to design a new piece of artillery. Production started in 1855 at the Elswick Ordnance Company and the Royal Arsenal at Woolwich, and the outcome was the revolutionary Armstrong Gun, which marked the birth of modern artillery. Three of its features particularly stand out.
First, the piece was rifled, which allowed for a much more accurate and powerful action. Although rifling had been tried on small arms since the 15th century, the necessary machinery to accurately rifle artillery was only available by the mid-19th century. Martin von Wahrendorff, and Joseph Whitworth independently produced rifled cannon in the 1840s, but it was Armstrong's gun that was first to see widespread use during the Crimean War. The cast iron shell of the Armstrong gun was similar in shape to a Minié ball and had a thin lead coating which made it fractionally larger than the gun's bore and which engaged with the gun's rifling grooves to impart spin to the shell. This spin, together with the elimination of windage as a result of the tight fit, enabled the gun to achieve greater range and accuracy than existing smooth-bore muzzle-loaders with a smaller powder charge.
His gun was also a breech-loader. Although attempts at breech-loading mechanisms had been made since medieval times, the essential engineering problem was that the mechanism couldn't withstand the explosive charge. It was only with the advances in metallurgy and precision engineering capabilities during the Industrial Revolution that Armstrong was able to construct a viable solution. The gun combined all the properties that make up an effective artillery piece. The gun was mounted on a carriage in such a way as to return the gun to firing position after the recoil.
What made the gun really revolutionary lay in the technique of the construction of the gun barrel that allowed it to withstand much more powerful explosive forces. The "built-up" method involved assembling the barrel with wrought-iron (later mild steel was used) tubes of successively smaller diameter. The tube would then be heated to allow it to expand and fit over the previous tube. When it cooled the gun would contract although not back to its original size, which allowed an even pressure along the walls of the gun which was directed inward against the outward forces that the gun firing exerted on the barrel.
Another innovative feature, more usually associated with 20th-century guns, was what Armstrong called its "grip", which was essentially a squeeze bore; the 6 inches of the bore at the muzzle end was of slightly smaller diameter, which centered the shell before it left the barrel and at the same time slightly swaged down its lead coating, reducing its diameter and slightly improving its ballistic qualities.
Armstrong's system was adopted in 1858, initially for "special service in the field" and initially he only produced smaller artillery pieces, 6-pounder (2.5 in/64 mm) mountain or light field guns, 9-pounder (3 in/76 mm) guns for horse artillery, and 12-pounder (3 inches /76 mm) field guns.
The first cannon to contain all 'modern' features is generally considered to be the French 75 of 1897. It was the first field gun to include a hydro-pneumatic recoil mechanism, which kept the gun's trail and wheels perfectly still during the firing sequence. Since it did not need to be re-aimed after each shot, the crew could fire as soon as the barrel returned to its resting position. In typical use, the French 75 could deliver fifteen rounds per minute on its target, either shrapnel or melinite high-explosive, up to about 5 miles (8,500 m) away. Its firing rate could even reach close to 30 rounds per minute, albeit only for a very short time and with a highly experienced crew. These were rates that contemporary bolt action rifles could not match. The gun used cased ammunition, was breech-loading, had modern sights, a self-contained firing mechanism and hydro-pneumatic recoil dampening.
Indirect fire.
Indirect fire, the firing of a projectile without relying on direct line of sight between the gun and the target, possibly dates back to the 16th century. Early battlefield use of indirect fire may have occurred at Paltzig in July 1759, when the Russian artillery fired over the tops of trees, and at the Battle of Waterloo, where a battery of the Royal Horse Artillery fired Shrapnel indirectly against advancing French troops.
In 1882, Russian Lieutenant Colonel KG Guk published "Indirect Fire for Field Artillery", which provided a practical method of using aiming points for indirect fire by describing, "all the essentials of aiming points, crest clearance, and corrections to fire by an observer".
A few years later, the Richtfläche (lining-plane) sight was invented in Germany and provided a means of indirect laying in azimuth, complementing the clinometers for indirect laying in elevation which already existed. Despite conservative opposition within the German army, indirect fire was adopted as doctrine by the 1890s. In the early 1900s, Goertz in Germany developed an optical sight for azimuth laying. It quickly replaced the lining-plane; in English, it became the 'Dial Sight' (UK) or 'Panoramic Telescope' (US).
The British halfheartedly experimented with indirect fire techniques since the 1890s, but with the onset of the Boer War, they were the first to apply the theory in practice in 1899, although they had to improvise without a lining-plane sight.
In the next 15 years leading up to World War I, the techniques of indirect fire became available for all types of artillery. Indirect fire was the defining characteristic of 20th-century artillery and led to undreamt of changes in the amount of artillery, its tactics, organisation, and techniques, most of which occurred during World War I.
An implication of indirect fire and improving guns was increasing range between gun and target, this increased the time of flight and the vertex of the trajectory. The result was decreasing accuracy (the increasing distance between the target and the mean point of impact of the shells aimed at it) caused by the increasing effects of non-standard conditions. Indirect firing data was based on standard conditions including a specific muzzle velocity, zero wind, air temperature and density, and propellant temperature. In practice, this standard combination of conditions almost never existed, they varied throughout the day and day to day, and the greater the time of flight, the greater the inaccuracy. An added complication was the need for survey to accurately fix the coordinates of the gun position and provide accurate orientation for the guns. Of course, targets had to be accurately located, but by 1916, air photo interpretation techniques enabled this, and ground survey techniques could sometimes be used.
In 1914, the methods of correcting firing data for the actual conditions were often convoluted, and the availability of data about actual conditions was rudimentary or non-existent, the assumption was that fire would always be ranged (adjusted). British heavy artillery worked energetically to progressively solve all these problems from late 1914 onwards, and by early 1918, had effective processes in place for both field and heavy artillery. These processes enabled 'map-shooting', later called 'predicted fire'; it meant that effective fire could be delivered against an accurately located target without ranging. Nevertheless, the mean point of impact was still some tens of yards from the target-centre aiming point. It was not precision fire, but it was good enough for concentrations and barrages. These processes remain in use into the 21st Century with refinements to calculations enabled by computers and improved data capture about non-standard conditions.
The British major-general Henry Hugh Tudor pioneered armour and artillery cooperation at the breakthrough Battle of Cambrai. The improvements in providing and using data for non-standard conditions (propellant temperature, muzzle velocity, wind, air temperature, and barometric pressure) were developed by the major combatants throughout the war and enabled effective predicted fire. The effectiveness of this was demonstrated by the British in 1917 (at Cambrai) and by Germany the following year (Operation Michael).
Major General J. B. A. Bailey, British Army (retired) wrote:
An estimated 75,000 French soldiers were casualties of friendly artillery in the four years of World War I.
Precision artillery.
Modern artillery is most obviously distinguished by its long range, firing an explosive shell or rocket and a mobile carriage for firing and transport. However, its most important characteristic is the use of indirect fire, whereby the firing equipment is aimed without seeing the target through its sights. Indirect fire emerged at the beginning of the 20th century and was greatly enhanced by the development of predicted fire methods in World War I. However, indirect fire was area fire; it was and is not suitable for destroying point targets; its primary purpose is area suppression. Nevertheless, by the late 1970s precision munitions started to appear, notably the US 155 mm Copperhead and its Soviet 152 mm equivalent that had success in Indian service. These relied on laser designation to 'illuminate' the target that the shell homed onto. However, in the early 21st Century, the Global Positioning System (GPS) enabled relatively cheap and accurate guidance for shells and missiles, notably the US 155 mm Excalibur and the 227 mm GMLRS rocket. The introduction of these led to a new issue, the need for very accurate three dimensional target coordinates – the mensuration process.
Weapons covered by the term 'modern artillery' include "cannon" artillery (such as howitzer, mortar, and field gun) and rocket artillery. Certain smaller-caliber mortars are more properly designated small arms rather than artillery, albeit indirect-fire small arms. This term also came to include coastal artillery which traditionally defended coastal areas against seaborne attack and controlled the passage of ships. With the advent of powered flight at the start of the 20th century, artillery also included ground-based anti-aircraft batteries.
The term "artillery" has traditionally not been used for projectiles with internal guidance systems, preferring the term "missilery", though some modern artillery units employ surface-to-surface missiles. Advances in terminal guidance systems for small munitions has allowed large-caliber guided projectiles to be developed, blurring this distinction.
Ammunition.
One of the most important roles of logistics is the supply of munitions as a primary type of artillery consumable, their storage and the provision of fuses, detonators and warheads at the point where artillery troops will assemble the charge, projectile, bomb or shell.
A round of artillery ammunition comprises four components:
Fuzes.
Fuzes are the devices that initiate an artillery projectile, either to detonate its high explosive (HE) filling or eject its cargo (illuminating flare or smoke canisters being examples). The official military spelling is "fuze". Broadly there are four main types:
Most artillery fuzes are nose fuzes. However, base fuzes have been used with armour piercing shells and for squash head (HESH or HEP) anti-tank shells. At least one nuclear shell and its non-nuclear spotting version also used a multi-deck mechanical time fuze fitted into its base.
Impact fuzes were, and in some armies remain, the standard fuze for HE projectiles. Their default action is normally 'superquick', some have had a 'graze' action which allows them to penetrate light cover and others have 'delay'. Delay fuzes allow the shell to penetrate the ground before exploding. Armor- or concrete-piercing fuzes are specially hardened. During World War I and later, ricochet fire with delay or graze fuzed HE shells, fired with a flat angle of descent, was used to achieve airburst.
HE shells can be fitted with other fuzes. Airburst fuzes usually have a combined airburst and impact function. However, until the introduction of proximity fuzes, the airburst function was mostly used with cargo munitions—for example, shrapnel, illumination, and smoke. The larger calibers of anti-aircraft artillery are almost always used airburst. Airburst fuzes have to have the fuze length (running time) set on them. This is done just before firing using either a wrench or a fuze setter pre-set to the required fuze length.
Early airburst fuzes used igniferous timers which lasted into the second half of the 20th century. Mechanical time fuzes appeared in the early part of the century. These required a means of powering them. The Thiel mechanism used a spring and escapement (i.e. 'clockwork'), Junghans used centrifugal force and gears, and Dixi used centrifugal force and balls. From about 1980, electronic time fuzes started replacing mechanical ones for use with cargo munitions.
Proximity fuzes have been of two types: photo-electric or radar. The former was not very successful and seems only to have been used with British anti-aircraft artillery 'unrotated projectiles' (rockets) in World War II. Radar proximity fuzes were a big improvement over the mechanical (time) fuzes which they replaced. Mechanical time fuzes required an accurate calculation of their running time, which was affected by non-standard conditions. With HE (requiring a burst 20 to above the ground), if this was very slightly wrong the rounds would either hit the ground or burst too high. Accurate running time was less important with cargo munitions that burst much higher.
The first radar proximity fuzes (codenamed 'VT') were initially used against aircraft in World War II. Their ground use was delayed for fear of the enemy recovering 'blinds' (artillery shells which failed to detonate) and copying the fuze. The first proximity fuzes were designed to detonate about above the ground. These air-bursts are much more lethal against personnel than ground bursts because they deliver a greater proportion of useful fragments and deliver them into terrain where a prone soldier would be protected from ground bursts.
However, proximity fuzes can suffer premature detonation because of the moisture in heavy rain clouds. This led to 'controlled variable time' (CVT) after World War II. These fuzes have a mechanical timer that switched on the radar about 5 seconds before expected impact, they also detonated on impact.
The proximity fuze emerged on the battlefields of Europe in late December 1944. They have become known as the U.S. Artillery's "Christmas present", and were much appreciated when they arrived during the Battle of the Bulge. They were also used to great effect in anti-aircraft projectiles in the Pacific against "kamikaze" as well as in Britain against V-1 flying bombs.
Electronic multi-function fuzes started to appear around 1980. Using solid-state electronics they were relatively cheap and reliable, and became the standard fitted fuze in operational ammunition stocks in some western armies. The early versions were often limited to proximity airburst, albeit with height of burst options, and impact. Some offered a go/no-go functional test through the fuze setter.
Later versions introduced induction fuze setting and testing instead of physically placing a fuze setter on the fuze. The latest, such as Junghan's DM84U provide options giving, superquick, delay, a choice of proximity heights of burst, time and a choice of foliage penetration depths.
A new type of artillery fuze will appear soon. In addition to other functions these offer some course correction capability, not full precision but sufficient to significantly reduce the dispersion of the shells on the ground.
Projectiles.
The projectile is the munition or "bullet" fired downrange. This may or may not be an explosive device. Traditionally, projectiles have been classified as "shot" or "shell", the former being solid and the latter having some form of "payload".
Shells can also be divided into three configurations: bursting, base ejection or nose ejection. The latter is sometimes called the shrapnel configuration. The most modern is base ejection, which was introduced in World War I. Both base and nose ejection are almost always used with airburst fuzes. Bursting shells use various types of fuze depending on the nature of the payload and the tactical need at the time.
Payloads have included:
Propellant.
Most forms of artillery require a propellant to propel the projectile at the target. Propellant is always a low explosive, this means it deflagrates instead of detonating, as with high explosives. The shell is accelerated to a high velocity in a very short time by the rapid generation of gas from the burning propellant. This high pressure is achieved by burning the propellant in a contained area, either the chamber of a gun barrel or the combustion chamber of a rocket motor.
Until the late 19th century, the only available propellant was black powder. Black powder had many disadvantages as a propellant; it has relatively low power, requiring large amounts of powder to fire projectiles, and created thick clouds of white smoke that would obscure the targets, betray the positions of guns, and make aiming impossible. In 1846, nitrocellulose (also known as guncotton) was discovered, and the high explosive nitroglycerin was discovered at much the same time. Nitrocellulose was significantly more powerful than black powder, and was smokeless. Early guncotton was unstable, however, and burned very fast and hot, leading to greatly increased barrel wear. Widespread introduction of smokeless powder would wait until the advent of the double-base powders, which combine nitrocellulose and nitroglycerin to produce powerful, smokeless, stable propellant.
Many other formulations were developed in the following decades, generally trying to find the optimum characteristics of a good artillery propellant; low temperature, high energy, non corrosive, highly stable, cheap, and easy to manufacture in large quantities. Broadly, modern gun propellants are divided into three classes: single-base propellants which are mainly or entirely nitrocellulose based, double-base propellants composed of a combination of nitrocellulose and nitroglycerin, and triple base composed of a combination of nitrocellulose and nitroglycerin and Nitroguanidine.
Artillery shells fired from a barrel can be assisted to greater range in three ways:
Propelling charges for tube artillery can be provided in one of two ways: either as cartridge bags or in metal cartridge cases. Generally, anti-aircraft artillery and smaller-caliber (up to 3" or 76.2 mm) guns use metal cartridge cases that include the round and propellant, similar to a modern rifle cartridge. This simplifies loading and is necessary for very high rates of fire. Bagged propellant allows the amount of powder to be raised or lowered, depending on the range to the target. It also makes handling of larger shells easier. Each requires a totally different type of breech to the other. A metal case holds an integral primer to initiate the propellant and provides the gas seal to prevent the gases leaking out of the breech; this is called obturation. With bagged charges, the breech itself provides obturation and holds the primer. In either case, the primer is usually percussion, but electrical is also used, and laser ignition is emerging. Modern 155 mm guns have a primer magazine fitted to their breech.
Artillery ammunition has four classifications according to use:
Field artillery system.
Because field artillery mostly uses indirect fire the guns have to be part of a system that enables them to attack targets invisible to them in accordance with the combined arms plan.
The main functions in the field artillery system are:
Organisationally and spatially, these functions can be arranged in many ways. Since the creation of modern indirect fire, different armies have done it differently at different times and in different places. Technology is often a factor, but so are military–social issues, the relationships between artillery and other arms, and the criteria by which military capability, efficiency, and effectiveness are judged. Cost is also an issue because artillery is expensive due to the large quantities of ammunition that it uses and its level of manpower.
Communications underpin the artillery system, as it must be reliable and available in real-time. During the 20th century communications often used flags, morse code by radio, line and lights (which could include voice and teleprinter, to name a few contrivances). Radio has included HF, VHF, satellite and radio relay as well as modern tactical trunk systems. In western armies radio communications are now usually encrypted.
The emergence of mobile and man-portable radios after World War I had a major impact on artillery because it enabled fast and mobile operations with observers accompanying the infantry or armoured troops. In World War II, some armies fitted their self-propelled guns with radios. However, sometimes in the first half of the 20th century, hardcopy artillery fire plans and map traces were distributed.
Data communications can be especially important for artillery because by using structured messages and defined data types fire control messages can be automatically routed and processed by computers. For example, a target acquisition element can send a message with target details which is automatically routed through the tactical and technical fire control elements to deliver firing data to the gun's laying system and the gun automatically laid. As tactical data networks become pervasive, they will provide any connected soldier with a means for reporting target information and requesting artillery fire.
Command is the authority to allocate resources, typically by assigning artillery formations or units. Terminology and its implications vary widely. However, very broadly, artillery units are assigned in direct support or in general support. Typically, the former mostly provide close support to manoeuvre units, while the latter may provide close support and or depth fire, notably counter-battery. Generally, 'direct support' also means that the artillery unit provides artillery observation and liaison teams to the supported units. Sometimes, direct support units are placed under command of the regiment/brigade they support. General support units may be grouped into artillery formations; for example, brigades, even divisions, or multi-battalion regiments, and usually under command of division, corps, or higher HQs. General support units tend to be moved to where they are most required at any particular time. Artillery command may impose priorities and constraints to support their combined arms commander's plans.
Target acquisition can take many forms, it is usually observation in real time, but may be the product of analysis. Artillery observation teams are the most common means of target acquisition. However, air observers have been use since the beginning of indirect fire and were quickly joined by air photography. Target acquisition may also be by anyone that can get the information into the artillery system. Targets may be visible to forward troops or in depth and invisible to them.
Observation equipment can vary widely in its complexity.
Control, sometimes called tactical fire control, is primarily concerned with 'targeting' and the allotment of fire units to targets. This is vital when a target is within range of many fire units and the number of fire units needed depends on the nature of the target, and the circumstances and purpose of its engagement. Targeting is concerned with selecting the right weapons in the right quantities to achieve the required effects on the target. Allotment attempts to address the artillery dilemma—important targets are rarely urgent and urgent targets are rarely important. Of course importance is a matter of perspective; what is important to a divisional commander is rarely the same as what is important to an infantry platoon commander.
Broadly, there are two situations: fire against opportunity targets, and targets whose engagement is planned as part of a particular operation. In the latter situation, command assigns fire units to the operation and an overall artillery fire planner makes a plan, possibly delegating resources for some parts of it to other planners. Fire plans may also involve use of non-artillery assets, such as mortars and aircraft.
Control of fire against opportunity targets is an important differentiator between different types of artillery system. In some armies, only designated artillery HQs have the tactical fire control authority to order fire units to engage a target, all 'calls for fire' being requests to these HQs. This authority may also extend to deciding the type and quantity of ammunition to be used. In other armies, an 'authorised observer' (for example, artillery observation team or other target acquisition element) can order fire units to engage. In the latter case, a battery observation team can order fire to their own battery and may be authorised to order fire to their own battalion, and sometimes to many battalions. For example, a divisional artillery commander may authorise selected observers to order fire to the entire divisional artillery. When observers or cells are not authorised, they can still request fire.
Armies that apply forward tactical control generally put the majority of the more senior officers of artillery units forward in command observation posts or with the supported arm. Those that do not use this approach tend to put these officers close to the guns. In either case, the observation element usually controls fire in detail against the target, such as adjusting it onto the target, moving it, and coordinating it with the supported arm as necessary to achieve the required effects.
Firing data has to be calculated and is the key to indirect fire; the arrangements for this have varied widely. In the end, firing data has two components: quadrant elevation and azimuth, to these may be added the size of propelling charge and the fuze setting. The process to produce firing data is sometimes called technical fire control. Before computers, some armies set the range on the gun's sights, which mechanically corrected it for the gun's muzzle velocity. For the first few decades of indirect fire, the firing data were often calculated by the observer, who then adjusted the fall of shot onto the target.
However, the need to engage targets at night, in depth, or hit the target with the first rounds quickly led to predicted fire being developed in World War I. Predicted fire existed alongside the older method. After World War II, predicted methods were invariably applied, but the fall of shot usually needed adjustment because of inaccuracy in locating the target, the proximity of friendly troops, or the need to engage a moving target. Target location errors were significantly reduced once laser rangefinders, orientation, and navigation devices were issued to observation parties.
In predicted fire, the basic geospatial data of range, angle of sight, and azimuth between a fire unit and its target was produced and corrected for variations from the 'standard conditions'. These variations included barrel wear, propellant temperature, different projectiles weights that all affected the muzzle velocity, and air temperature, density, wind speed & direction, and rotation of the Earth that affect the shell in flight. The net effect of variations can also be determined by shooting at an accurately known point, a process called 'registration'.
All these calculations to produce a quadrant elevation (or range) and azimuth were done manually by highly trained soldiers using instruments, tabulated data, data of the moment, and approximations until battlefield computers started appearing in the 1960s and '70s. While some early calculators copied the manual method (typically substituting polynomials for tabulated data), computers use a different approach. They simulate a shell's trajectory by 'flying' it in short steps and applying data about the conditions affecting the trajectory at each step. This simulation is repeated until it produces a quadrant elevation and azimuth that lands the shell within the required 'closing' distance of the target coordinates. NATO has a standard ballistic model for computer calculations and has expanded the scope of this into the NATO Armaments Ballistic Kernel (NABK) within the SG2 Shareable (Fire Control) Software Suite (S4).
Technical fire control has been performed in various places, but mostly in firing batteries. However, in the 1930s, the French moved it to battalion level and combined it with some tactical fire control. This was copied by the US. Nevertheless, most armies seemed to have retained it within firing batteries, and some duplicated the technical fire control teams in a battery to give operational resilience and tactical flexibility. Computers reduced the number of men needed and enabled decentralisation of technical fire control to autonomous sub-battery fire units, such as platoons, troops, or sections, although some armies had sometimes done this with their manual methods. Computation on the gun or launcher, integrated with their laying system, is also possible. MLRS led the way in this.
A fire unit is the smallest artillery or mortar element, consisting of one or more weapon systems, capable of being employed to execute a fire assigned by a tactical fire controller. Generally it is a battery, but sub-divided batteries are quite common, and in some armies very common. On occasions a battery of 6 guns has been 6 fire units. Fire units may or may not occupy separate positions. Geographically dispersed fire units may or may not have an integral capability for technical fire control.
Specialist services provide data need for predicted fire. Increasingly, they are provided from within firing units. These services include:
Logistic services, supply of artillery ammunition has always been a major component of military logistics. Up until World War I some armies made artillery responsible for all forward ammunition supply because the load of small arms ammunition was trivial compared to artillery. Different armies use different approaches to ammunition supply, which can vary with the nature of operations. Differences include where the logistic service transfers artillery ammunition to artillery, the amount of ammunition carried in units and extent to which stocks are held at unit or battery level. A key difference is whether supply is 'push' or 'pull'. In the former the 'pipeline' keeps pushing ammunition into formations or units at a defined rate. In the latter units fire as tactically necessary and replenish to maintain or reach their authorised holding (which can vary), so the logistic system has to be able to cope with surge and slack.
Artillery has always been equipment intensive and for centuries artillery provided its own artificers to maintain and repair their equipment. Most armies now place these services in specialist branches with specialist repair elements in batteries and units.
Classification of artillery.
Artillery types can be categorised in several ways, for example by type or size of weapon or ordnance, by role or by organizational arrangements.
Types of ordnance.
The types of cannon artillery are generally distinguished by the velocity at which they fire projectiles.
Types of artillery:
Modern field artillery can also be split into two other categories: towed and self-propelled. As the name suggests, towed artillery has a prime mover, usually a jeep or truck, to move the piece, crew, and ammunition around. Self-propelled howitzers are permanently mounted on a carriage or vehicle with room for the crew and ammunition and are thus capable of moving quickly from one firing position to another, both to support the fluid nature of modern combat and to avoid counter-battery fire. There are also mortar carrier vehicles, many of which allow the mortar to be removed from the vehicle and be used dismounted, potentially in terrain in which the vehicle cannot navigate, or in order to avoid detection.
Organizational types.
At the beginning of the modern artillery period, the late 19th century, many armies had three main types of artillery, in some case they were sub-branches within the artillery branch in others they were separate branches or corps. There were also other types excluding the armament fitted to warships:
After World War I many nations merged these different artillery branches, in some cases keeping some as sub-branches. Naval artillery disappeared apart from that belonging to marines. However, two new branches of artillery emerged during that war and its aftermath, both used specialised guns (and a few rockets) and used direct not indirect fire, in the 1950s and 1960s both started to make extensive use of missiles:
However, the general switch by artillery to indirect fire before and during World War I led to a reaction in some armies. The result was accompanying or infantry guns. These were usually small, short range guns, that could be easily man-handled and used mostly for direct fire but some could use indirect fire. Some were operated by the artillery branch but under command of the supported unit. In World War II they were joined by self-propelled assault guns, although other armies adopted infantry or close support tanks in armoured branch units for the same purpose, subsequently tanks generally took on the accompanying role.
Equipment types.
The three main types of artillery "gun" are guns, howitzers and mortars. During the 20th century, guns and howitzers have steadily merged in artillery use, making a distinction between the terms somewhat meaningless. By the end of the 20th century, true guns with calibers larger than about 60 mm had become very rare in artillery use, the main users being tanks, ships, and a few residual anti-aircraft and coastal guns. The term "cannon" is a United States generic term that includes guns, howitzers and mortars; it is not used in other English speaking armies.
The traditional definitions differentiated between guns and howitzers in terms of maximum elevation (well less than 45° as opposed to close to or greater than 45°), number of charges (one or more than one charge), and having higher or lower muzzle velocity, sometimes indicated by barrel length. These three criteria give eight possible combinations, of which guns and howitzers are but two. However, modern "howitzers" have higher velocities and longer barrels than the equivalent "guns" of the first half of the 20th century.
True guns are characterized by long range, having a maximum elevation significantly less than 45°, a high muzzle velocity and hence a relatively long barrel, smooth bore (no rifling) and a single charge. The latter often led to fixed ammunition where the projectile is locked to the cartridge case. There is no generally accepted minimum muzzle velocity or barrel length associated with a gun. 
Howitzers can fire at maximum elevations at least close to 45°; elevations up to about 70° are normal for modern howitzers. Howitzers also have a choice of charges, meaning that the same elevation angle of fire will achieve a different range depending on the charge used. They have rifled bores, lower muzzle velocities and shorter barrels than equivalent guns. All this means they can deliver fire with a steep angle of descent. Because of their multi-charge capability, their ammunition is mostly separate loading (the projectile and propellant are loaded separately).
That leaves six combinations of the three criteria, some of which have been termed gun howitzers. A term first used in the 1930s when howitzers with a relatively high maximum muzzle velocities were introduced, it never became widely accepted, most armies electing to widen the definition of "gun" or "howitzer". By the 1960s, most equipments had maximum elevations up to about 70°, were multi-charge, had quite high maximum muzzle velocities and relatively long barrels.
Mortars are simpler. The modern mortar originated in World War I and there were several patterns. After that war, most mortars settled on the Stokes pattern, characterized by a short barrel, smooth bore, low muzzle velocity, elevation angle of firing generally greater than 45°, and a very simple and light mounting using a "baseplate" on the ground. The projectile with its integral propelling charge was dropped down the barrel from the muzzle to hit a fixed firing pin. Since that time, a few mortars have become rifled and adopted breech loading.
There are other recognized typifying characteristics for artillery. One such characteristic is the type of obturation used to seal the chamber and prevent gases escaping through the breech. This may use a metal cartridge case that also holds the propelling charge, a configuration called "QF" or "quickfiring" by some nations. The alternative does not use a metal cartridge case, the propellant being merely bagged or in combustible cases with the breech itself providing all the sealing. This is called "BL" or "breech loading" by some nations.
A second characteristic is the form of propulsion. Modern equipment can either be towed or self-propelled (SP). A towed gun fires from the ground and any inherent protection is limited to a gun shield. Towing by horse teams lasted throughout World War II in some armies, but others were fully mechanized with wheeled or tracked gun towing vehicles by the outbreak of that war. The size of a towing vehicle depends on the weight of the equipment and the amount of ammunition it has to carry.
A variation of towed is portee, where the vehicle carries the gun which is dismounted for firing. Mortars are often carried this way. A mortar is sometimes carried in an armored vehicle and can either fire from it or be dismounted to fire from the ground. Since the early 1960s it has been possible to carry lighter towed guns and most mortars by helicopter. Even before that, they were parachuted or landed by glider from the time of the first airborne trials in the USSR in the 1930s.
In an SP equipment, the gun is an integral part of the vehicle that carries it. SPs first appeared during World War I, but did not really develop until World War II. They are mostly tracked vehicles, but wheeled SPs started to appear in the 1970s. Some SPs have no armor and carry little or no ammunition. Armoured SPs usually carry a useful ammunition load. Early armoured SPs were mostly a "casemate" configuration, in essence an open top armored box offering only limited traverse. However, most modern armored SPs have a full enclosed armored turret, usually giving full traverse for the gun. Many SPs cannot fire without deploying stabilizers or spades, sometimes hydraulic. A few SPs are designed so that the recoil forces of the gun are transferred directly onto the ground through a baseplate. A few towed guns have been given limited self-propulsion by means of an auxiliary engine.
Two other forms of tactical propulsion were used in the first half of the 20th century: Railways or transporting the equipment by road, as two or three separate loads, with disassembly and re-assembly at the beginning and end of the journey. Railway artillery took two forms, railway mountings for heavy and super-heavy guns and howitzers and armored trains as "fighting vehicles" armed with light artillery in a direct fire role. Disassembled transport was also used with heavy and super heavy weapons and lasted into the 1950s.
Caliber categories.
A third form of artillery typing is to classify it as "light", "medium", "heavy" and various other terms. It appears to have been introduced in World War I, which spawned a very wide array of artillery in all sorts of sizes so a simple categorical system was needed. Some armies defined these categories by bands of calibers. Different bands were used for different types of weapons—field guns, mortars, anti-aircraft guns and coast guns.
Modern operations.
List of countries in order of amount of artillery:
Artillery is used in a variety of roles depending on its type and caliber. The general role of artillery is to provide "fire support"—"the application of fire, coordinated with the manoeuvre of forces to destroy, "neutralize" or "suppress" the enemy". This NATO definition, of course, makes artillery a supporting arm although not all NATO armies agree with this logic. The "italicised" terms are NATO's.
Unlike rockets, guns (or howitzers as some armies still call them) and mortars are suitable for delivering "close supporting fire". However, they are all suitable for providing "deep supporting fire" although the limited range of many mortars tends to exclude them from the role. Their control arrangements and limited range also mean that mortars are most suited to "direct supporting fire". Guns are used either for this or "general supporting fire" while rockets are mostly used for the latter. However, lighter rockets may be used for direct fire support. These rules of thumb apply to NATO armies.
Modern mortars, because of their lighter weight and simpler, more transportable design, are usually an integral part of infantry and, in some armies, armor units. This means they generally do not have to "concentrate" their fire so their shorter range is not a disadvantage. Some armies also consider infantry operated mortars to be more responsive than artillery, but this is a function of the control arrangements and not the case in all armies. However, mortars have always been used by artillery units and remain with them in many armies, including a few in NATO.
In NATO armies artillery is usually assigned a tactical mission that establishes its relationship and responsibilities to the formation or units it is assigned to. It seems that not all NATO nations use the terms and outside NATO others are probably used. The standard terms are: "direct support", "general support", "general support reinforcing" and "reinforcing". These tactical missions are in the context of the command authority: "operational command", "operational control", "tactical command" or "tactical control".
In NATO direct support generally means that the directly supporting artillery unit provides observers and liaison to the manoeuvre troops being supported, typically an artillery battalion or equivalent is assigned to a brigade and its batteries to the brigade's battalions. However, some armies achieve this by placing the assigned artillery units under command of the directly supported formation. Nevertheless, the batteries' fire can be "concentrated" onto a single target, as can the fire of units in range and with the other tactical missions.
Application of fire.
There are several dimensions to this subject. The first is the notion that fire may be against an "opportunity" target or may be "prearranged". If it is the latter it may be either "on-call" or "scheduled". Prearranged targets may be part of a "fire plan". Fire may be either "observed" or "unobserved", if the former it may be "adjusted", if the latter then it has to be "predicted". Observation of adjusted fire may be directly by a forward observer or indirectly via some other "target acquisition" system.
NATO also recognises several different types of fire support for tactical purposes:
These purposes have existed for most of the 20th century, although their definitions have evolved and will continue to do so, lack of "suppression" in "counterbattery" is an omission. Broadly they can be defined as either:
Two other NATO terms also need definition:
The tactical purposes also include various "mission verbs", a rapidly expanding subject with the modern concept of "effects based operations".
"Targeting" is the process of selecting target and matching the appropriate response to them taking account of operational requirements and capabilities. It requires consideration of the type of fire support required and the extent of coordination with the supported arm. It involves decisions about:
The "targeting" process is the key aspect of tactical fire control. Depending on the circumstances and national procedures it may all be undertaken in one place or may be distributed. In armies practicing control from the front, most of the process may be undertaken by a forward observer or other target acquirer. This is particularly the case for a smaller target requiring only a few fire units. The extent to which the process is formal or informal and makes use of computer based systems, documented norms or experience and judgement also varies widely armies and other circumstances.
Surprise may be essential or irrelevant. It depends on what effects are required and whether or not the target is likely to move or quickly improve its protective posture. During World War II UK researchers concluded that for impact fuzed munitions the relative risk were as follows:
Airburst munitions significantly increase the relative risk for lying men, etc. Historically most casualties occur in the first 10–15 seconds of fire, i.e. the time needed to react and improve protective posture, however, this is less relevant if airburst is used.
There are several ways of making best use of this brief window of maximum vulnerability:
Counter-battery fire.
Modern counter-battery fire developed in World War I, with the objective of defeating the enemy's artillery. Typically such fire was used to suppress enemy batteries when they were or were about to interfere with the activities of friendly forces (such as to prevent enemy defensive artillery fire against an impending attack) or to systematically destroy enemy guns. In World War I the latter required air observation. The first indirect counter-battery fire was in May 1900 by an observer in a balloon.
Enemy artillery can be detected in two ways, either by direct observation of the guns from the air or by ground observers (including specialist reconnaissance), or from their firing signatures. This includes radars tracking the shells in flight to determine their place of origin, sound ranging detecting guns firing and resecting their position from pairs of microphones or cross-observation of gun flashes using observation by human observers or opto-electronic devices, although the widespread adoption of 'flashless' propellant limited the effectiveness of the latter.
Once hostile batteries have been detected they may be engaged immediately by friendly artillery or later at an optimum time, depending on the tactical situation and the counter-battery policy. Air strike is another option. In some situations the task is to locate all active enemy batteries for attack using a counter-battery fire at the appropriate moment in accordance with a plan developed by artillery intelligence staff. In other situations counter-battery fire may occur whenever a battery is located with sufficient accuracy.
Modern counter-battery target acquisition uses unmanned aircraft, counter-battery radar, ground reconnaissance and sound-ranging. Counter-battery fire may be adjusted by some of the systems, for example the operator of an unmanned aircraft can 'follow' a battery if it moves. Defensive measures by batteries include frequently changing position or constructing defensive earthworks, the tunnels used by North Korea being an extreme example. Counter-measures include air defence against aircraft and attacking counter-battery radars physically and electronically.
Field artillery team.
'Field Artillery Team' is a US term and the following description and terminology applies to the US, other armies are broadly similar but differ in significant details. The 'Field Artillery System' above gives a more comprehensive description. Modern field artillery (post–World War I) has three distinct parts: the forward observer (or FO), the fire direction center (FDC) and the actual guns themselves. The forward observer observes the target using tools such as binoculars, laser rangefinders, designators and call back fire missions on his radio, or relays the data through a portable computer via an encrypted digital radio connection protected from jamming by computerized frequency hopping. A lesser known part of the team is the FAS or Field Artillery Survey team which setups up the "Gun Line" for the cannons. Today most artillery battalions use a(n) "Aiming Circle" which allows for faster setup and more mobility. FAS teams are still used for checks and balances purposes and if a gun battery has issues with the "Aiming Circle" a FAS team will do it for them.
The FO can communicate directly with the battery FDC, of which there is one per each battery of 4–8 guns. Otherwise the several FOs communicate with a higher FDC such as at a Battalion level, and the higher FDC prioritizes the targets and allocates fires to individual batteries as needed to engage the targets that are spotted by the FOs or to perform preplanned fires.
The Battery FDC computes firing data—ammunition to be used, powder charge, fuse settings, the direction to the target, and the quadrant elevation to be fired at to reach the target, what gun will fire any rounds needed for adjusting on the target, and the number of rounds to be fired on the target by each gun once the target has been accurately located—to the guns. Traditionally this data is relayed via radio or wire communications as a warning order to the guns, followed by orders specifying the type of ammunition and fuse setting, direction, and the elevation needed to reach the target, and the method of adjustment or orders for fire for effect (FFE). However, in more advanced artillery units, this data is relayed through a digital radio link.
Other parts of the field artillery team include meteorological analysis to determine the temperature, humidity and pressure of the air and wind direction and speed at different altitudes. Also radar is used both for determining the location of enemy artillery and mortar batteries and to determine the precise actual strike points of rounds fired by battery and comparing that location with what was expected to compute a registration allowing future rounds to be fired with much greater accuracy.
Time on Target.
A technique called Time on Target was developed by the British Army in North Africa at the end of 1941 and early 1942 particularly for counter-battery fire and other concentrations, it proved very popular. It relied on BBC time signals to enable officers to synchronize their watches to the second because this avoided the need to use military radio networks and the possibility of losing surprise, and the need for field telephone networks in the desert. With this technique the time of flight from each fire unit (battery or troop) to the target is taken from the range or firing tables, or the computer and each engaging fire unit subtracts its time of flight from the TOT to determine the time to fire. An executive order to fire is given to all guns in the fire unit at the correct moment to fire. When each fire unit fires their rounds at their individual firing time all the opening rounds will reach the target area almost simultaneously. This is especially effective when combined with techniques that allow fires for effect to be made without preliminary adjusting fires.
MRSI.
This is a modern version of the earlier "time on target" concept in which fire from different weapons was timed to arrive on target at the same time. It is possible for artillery to fire several shells per gun at a target and have all of them arrive simultaneously, which is called MRSI (Multiple Rounds Simultaneous Impact). This is because there is more than one trajectory for the rounds to fly to any given target: typically one is below 45 degrees from horizontal and the other is above it, and by using different size propelling charges with each shell, it is possible to create multiple trajectories. Because the higher trajectories cause the shells to arc higher into the air, they take longer to reach the target and so if the shells are fired on these trajectories for the first volleys (starting with the shell with the most propellant and working down) and then after the correct pause more volleys are fired on the lower trajectories, the shells will all arrive at the same time. This is useful because many more shells can land on the target with no warning. With traditional volleys along the same trajectory, anybody at the target area may have time (however long it takes to reload and re-fire the guns) to take cover between volleys. However, guns capable of burst fire can deliver several rounds in 10 seconds if they use the same firing data for each, and if guns in more than one location are firing on one target they can use Time on Target procedures so that all their shells arrive at the same time and target.
To engage targets using MRSI requires two things, firstly guns with the requisite rate of fire and sufficiently different size propelling charges, secondly a fire control computer that has been designed to compute such missions and the data handling capability that allows all the firing data to be produced, sent to each gun and then presented to the gun commander in the correct order. The number of rounds that can be delivered in MRSI depends primarily on the range to the target and the rate of fire, for maximum rounds the range is limited to that of lowest propelling charge that will reach the target.
Examples of guns with a rate of fire that makes them suitable for MRSI includes UK's AS-90, South Africa's Denel G6-52 (which can land six rounds simultaneously at targets at least away), Germany's Panzerhaubitze 2000 (which can land five rounds simultaneously at targets at least away) Slovakia's 155 mm SpGH ZUZANA model 2000. The Archer project (developed by BAE-Systems in Sweden) is a 155 mm howitzer on a wheeled chassis which is claimed to be able to deliver up to six shells on target simultaneously from the same gun. The 120 mm twin barrel AMOS mortar system, joint developed by Hägglunds (Sweden) and Patria (Finland), is capable of 7 + 7 shells MRSI. The United States Crusader program (now cancelled) was slated to have MRSI capability. It is unclear how many fire control computers have the necessary capabilities.
Two-round MRSI firings were a popular artillery demonstration in the 1960s, where well trained detachments could show off their skills for spectators.
Air burst.
The destructiveness of artillery bombardments can be enhanced when some or all of the shells are set for airburst, meaning that they explode in the air above the target instead of upon impact. This can be accomplished either through time fuses or proximity fuses. Time fuses use a precise timer to detonate the shell after a preset delay. This technique is tricky and slight variations in the functioning of the fuse can cause it to explode too high and be ineffective, or to strike the ground instead of exploding above it. Since December 1944 (Battle of the Bulge), proximity fuzed artillery shells have been available that take the guesswork out of this process. These embody a miniature, low powered radar transmitter in the fuse to detect the ground and explode them at a predetermined height above it. The return of the weak radar signal completes an electrical circuit in the fuze which explodes the shell. The proximity fuse itself was developed by the British to increase the effectiveness of anti-aircraft warfare.
This is a very effective tactic against infantry and light vehicles, because it scatters the fragmentation of the shell over a larger area and prevents it from being blocked by terrain or entrenchments that do not include some form of robust overhead cover. Combined with TOT or MRSI tactics that give no warning of the incoming rounds, these rounds are especially devastating because many enemy soldiers are likely to be caught in the open. This is even more so if the attack is launched against an assembly area or troops moving in the open rather than a unit in an entrenched tactical position.

</doc>
<doc id="2510" url="https://en.wikipedia.org/wiki?curid=2510" title="Arnulf of Carinthia">
Arnulf of Carinthia

Arnulf of Carinthia (c. 850 – 8 December 899) was the Carolingian King of East Francia from 887, the disputed King of Italy from 894 and the disputed Holy Roman Emperor from 22 February 896 until his death at Regensburg, Bavaria.
Career.
Birth and illegitimacy.
Arnulf was the son of Carloman, King of Bavaria, and his wife Liutswind, who maybe was the sister of Ernst, Count of the Bavarian Nordgau Margraviate in the area of the Upper Palatinate, or perhaps the burgrave of Passau, as some sources say. After Arnulf's birth, Carloman married, before 861, a daughter of that same Count Ernst, who died after 8 August 879. As it is mainly West-Franconian historiography that speaks of Arnulf's illegitimacy, it is quite feasible that the two females are one and the same person and that Carloman later on actually married Liutswind, thus legitimizing his son.
Early years.
Arnulf was granted the Duchy of Carinthia, a Frankish vassal state and successor of the ancient Principality of Carantania, by his father Carloman, after Carloman had become reconciled with his own father Louis the German and was created King of Bavaria. Arnulf spent his childhood on the "Mosaburch" or Mosapurc, which is widely believed to be Moosburg in Carinthia, only a few miles away from one of the Imperial residences, the Carolingian Kaiserpfalz at Karnburg, which before as "Krnski grad" had been the residence of the Carantanian princes. Arnulf kept his seat here and from later events it may be inferred that the Carantanians, from an early time, treated him as their own Duke. Later, after he had been crowned King of East Francia, Arnulf turned his old territory of Carinthia into the March of Carinthia, a part of the Duchy of Bavaria.
After Carloman was incapacitated by a stroke in 879, Louis the Younger inherited Bavaria, Charles the Fat was given the Kingdom of Italy and Arnulf was confirmed in Carinthia by an agreement with Carloman. Bavaria, however, was ruled more or less by Arnulf. Arnulf had in fact ruled Bavaria during the summer and autumn of 879 while his father arranged his succession and he himself was granted "Pannonia," in the words of the "Annales Fuldenses", or "Carantanum," in the words of Regino of Prüm. The division of the realm was confirmed in 880 on Carloman’s death.
When, in 882, Engelschalk II rebelled against the Margrave of Pannonia, Aribo, and ignited the so-called Wilhelminer War, Arnulf supported him and even accepted his and his brother's homage. This ruined Arnulf's relationship with his uncle the Emperor and put him at war with Svatopluk of Moravia. Pannonia was invaded, but Arnulf refused to give up the young Wilhelminers. Arnulf did not make peace with Svatopluk until late 885, by which time the Moravian was a man of the emperor. Some scholars see this war as destroying Arnulf's hopes at succeeding Charles.
King of East Francia.
Arnulf took the leading role in the deposition of his uncle, the Emperor Charles the Fat. With the support of the nobles, Arnulf held a Diet at Tribur and deposed Charles in November 887, under threat of military action. Charles peacefully went into his involuntary retirement, but not without first chastising his nephew for his treachery and asking only for a few royal villas in Swabia, which Arnulf mercifully granted him, on which to live out his final months. Arnulf, having distinguished himself in the war against the Slavs was elected by the nobles of the realm (only the eastern realm, though Charles had ruled the whole of the Frankish lands) and assumed his title of King of East Francia.
Arnulf took advantage of the problems in West Francia upon the death of Charles The Fat to secure the territory of Lorraine, which he converted into a kingdom for his son, Zwentibold. In addition, in 889, Arnulf supported the claim of Louis the Blind to the kingdom of Provence, after receiving a personal appeal from Louis’ mother, Ermengard, who came to see Arnulf at Forchheim in May 889. Recognising the superiority of Arnulf’s position, in 888 Odo of France formally admitted the suzerainty of Arnulf. In 893, Arnulf switched his support from Odo to Charles the Simple after being persuaded by Fulk (Archbishop of Reims) that it was in his best interests. Arnulf then took advantage of the fighting that followed between Odo and Charles in 894, taking territory from West Francia and transferring it to his dominion. At one point, Charles was forced to flee to Arnulf and ask for his protection. His intervention forced Pope Formosus to get involved, as he was worried that a divided and war weary West Francia would be easy prey for the Normans.
In 895, Arnulf summoned both Charles and Odo to his presence at Worms. Charles’s advisers convinced him not to go, and he sent a representative in his place. Odo, on the other hand, personally attended, together with a large retinue, bearing many gifts for Arnulf. Angered by the non-appearance of Charles, he welcomed Odo at the Diet of Worms in May 895, and again supported Odo's claim to the West Francian throne. In this same assembly, he bestowed upon his illegitimate son Zwentibold, a crown as the King of Lotharingia.
Arnulf was not a negotiator, but a fighter. In 890 he was successfully battling the Slavs in Pannonia. In 891, the Danes invaded Lotharingia, and crushed an East Frankish army at Maastricht. At the decisive Battle of Leuven in September 891 in Lorraine, he repelled an invasion by the Normans (Northmen or Vikings), essentially ending their invasions on that front. The "Annales Fuldenses" report that the bodies of dead Northmen blocked the run of the river. After his victory, Arnulf built a new castle on an island in the Dijle river (Dutch: Dijle, English and French: Dyle).
As early as 880, Arnulf had designs on Great Moravia, and had the Frankish bishop Wiching of Nitra interfere with the missionary activities of Methodius, with the aim of preventing any potential for creating a unified Moravian nation. In 893 or 894, Great Moravia probably lost a part of its territory — present-day Western Hungary — to him. As a reward, Wiching became Arnulf’s chancellor in 892. Arnulf, however, failed to conquer the whole of Great Moravia when he attempted it in 892, 893, and 899. Yet Arnulf did achieve some successes, in particular in 895, when Bohemia broke away from Great Moravia and became his vassal. An accord was made between him and the Bohemian Duke Borivoj I (reigned 870-95); Bohemia was thus freed from the dangers of invasion. However, in his attempts to conquer Moravia, in 899 Arnulf invited across the Magyars who had settled in Pannonia, and with their help he imposed a measure of control on Moravia.
Like all early Germanic rulers, he was heavily involved in ecclesiastical disputes; in 895, at the Diet of Tribur, he presided over a dispute between the Episcopal sees of Bremen, Hamburg and Cologne over jurisdictional authority, which saw Bremen and Hamburg remain a combined see, independent of the see of Cologne.
King of Italy and Holy Roman Emperor.
In Italy, the Iron Crown of Lombardy was being fought over between Guy III of Spoleto and Berengar of Friuli. Berengar had been crowned king in 887, but Guy was in his turn crowned in 889. While Pope Stephen V supported Guy, crowning him Roman Emperor in 891, Arnulf threw his support behind Berengar.
In 893, a new pope, Formosus, not trusting the newly crowned co-emperors Guy and Lambert, sent an embassy to Omuntesberch, where Arnulf was holding a Diet with Svatopluk, to request Arnulf come and liberate Italy, where he would be crowned in Rome. Arnulf met the "Primores" of the Kingdom of Italy, dismissed them with gifts and promised to enter Italy. Arnulf sent his son Zwentibold with a Bavarian army to join Berengar of Friuli. They defeated Guy, but were bought off and left in autumn. Arnulf then personally led an army across the Alps early in 894. In January 894 Bergamo fell, and Count Ambrose, Guy’s representative in the city, was hung from a tree by the city’s gate.
Conquering all of the territory north of the Po, he forced the surrender of Milan and then drove Guy out of Pavia, where he was crowned King of Italy, but went no further before Guy died suddenly in late autumn, and fever incapacitated his troops. His march northward through the Alps was interrupted by Rudolph, King of Transjurane Burgundy, and it was only with great difficulty that Arnulf crossed the mountain range. In retaliation, Arnulf ordered his illegitimate son Zwentibold to ravage Burgundy. In the meantime, Lambert and his mother Ageltrude travelled to Rome to receive papal confirmation of his imperial succession, but Formosus, still desiring to crown Arnulf, was imprisoned in Castel Sant'Angelo.
In September 895, a new embassy arrived in Regensburg beseeching Arnulf's aid. In October, Arnulf undertook his second campaign into Italy. He crossed the Alps quickly and took Pavia, but then he continued slowly, garnering support among the nobility of Tuscany. First Maginulf, Count of Milan, and then Walfred, Count of Pavia, joined him. Eventually even the Margrave Adalbert II abandoned Lambert. Finding Rome locked against him and held by Ageltrude, he had to take the city by force on 21 February 896, freeing the pope. Arnulf was greeted at the Ponte Milvio by the Roman Senate who escorted him into the Leonine City, where he was received by Pope Formosus on the steps of the Santi Apostoli.
On 22 February 896, Formosus led the king into the church, anointed and crowned him, and saluted him as "Augustus". Arnulf then proceeded to the Basilica of Saint Paul Outside the Walls, where he received the homage of the Roman people, who swore “never to hand over the city to Lambert or his mother Ageltrude”. Arnulf then proceeded to exile to Bavaria two leading senators, Constantine and Stephen, who had helped Ageltrude seize the city. Leaving one of his vassals, Farold, to hold Rome, Arnulf marched on Spoleto, where Ageltrude had fled to join Lambert. On his way down, Arnulf suffered a stroke, forcing him to call off his campaign and return to Bavaria.
Arnulf only retained power in Italy as long as he was personally there. On his way north, he stopped at Pavia where he crowned his illegitimate son Ratold, sub-King of Italy, after which he left Ratold in Milan in an attempt to preserve his hold on Italy. That same year, Formosus died, leaving Lambert once again in power, and both he and Berengar killed any officials who had been put in place by Arnulf, as Ratold also fled from Milan to Bavaria. Rumours of the time made Arnulf's condition to be a result of poisoning at the hand of Ageltrude. On his return to Germany, he exercised very little further control in Italy for the rest of his life, although his agents in Rome did not prevent the accession of Pope Stephen VI in 896. Although he eventually became a supporter of the claims of Lambert, he initially gave his support to Arnulf.
Final years.
With his return to Germany in 896, Arnulf found that his physical ill health - he suffered from (morbus pediculosis – infestation of lice under the eyelid) - meant he was unable to deal with the problems besetting his reign. Italy was lost, raiders from Moravia and Hungary were continually raiding his lands, and Lotharingia was in revolt against Zwentibold. He was also plagued by escalating violence and power struggles between the lower German nobility.
On 8 December 899 Arnulf of Carinthia died at Ratisbon (today known as Regensburg), in present-day Bavaria, Germany.
On Arnulf's death, he was succeeded as a king of the East Franks by his son by his wife Ota (died 903), Louis the Child. When his only legitimate son, Louis the Child, died in 911 at age 17 or 18, the eastern (German) branch of the house of Charlemagne ceased to exist. Arnulf had the nobility also recognize the rights of his illegitimate sons Zwentibold and Ratold as his successors. Zwentibold, whom he had made King of Lotharingia in 895, continued to rule there until the next year (900).
Arnulf is entombed in St. Emmeram's Basilica at Regensburg, which is now known as Schloss Thurn und Taxis, the palace of the Princes of Thurn und Taxis.

</doc>
<doc id="2511" url="https://en.wikipedia.org/wiki?curid=2511" title="Alexanderplatz">
Alexanderplatz

Alexanderplatz () is a large public square and transport hub in the central Mitte district of Berlin, near the Fernsehturm. Berliners often call it simply Alex, referring to a larger neighbourhood stretching from "Mollstraße" in the northeast to "Spandauer Straße" and the Rotes Rathaus in the southwest.
History.
Early history.
Originally a cattle market outside the city fortifications, it was named in honor of a visit of the Russian Emperor Alexander I to Berlin on 25 October 1805 by order of King Frederick William III of Prussia. The square gained a prominent role in the late 19th century with the construction of the Stadtbahn station of the same name and a nearby market hall, followed by the opening of a department store of Hermann Tietz in 1904, becoming a major commercial centre. The U-Bahn station of the present-day U2 line opened on 1 July 1913.
Its heyday was in the 1920s, when together with Potsdamer Platz it was at the heart of Berlin's nightlife, inspiring the 1929 novel "Berlin Alexanderplatz" (see 1920s Berlin) and the two films based thereon, Piel Jutzi's 1931 film and Rainer Werner Fassbinder's 15½ hour second adaptation, released in 1980. About 1920 the city's authorities started a rearrangement of the increasing traffic flows laying out a roundabout, accompanied by two buildings along the Stadtbahn viaduct, "Alexanderhaus" and "Berolinahaus" finished in 1932 according to plans designed by Peter Behrens.
East Germany.
Alexanderplatz has been subject to redevelopment several times in its history, most recently during the 1960s, when it was turned into a pedestrian zone and enlarged as part of the German Democratic Republic's redevelopment of the city centre. It is surrounded by several notable structures including the Fernsehturm (TV Tower), the second tallest structure in the European Union.
"Alex" also accommodates the Park Inn Berlin and the World Time Clock, a continually rotating installation that shows the time throughout the globe, and Hermann Henselmann's "Haus des Lehrers". During the Peaceful Revolution of 1989, the Alexanderplatz demonstration on 4 November was the largest demonstration in the history of East Germany.
After German reunification.
Since German reunification, Alexanderplatz has undergone a gradual process of change with many of the surrounding buildings being renovated. Despite the reconstruction of the tram line crossing, it has retained its socialist character, including the much-graffitied "Fountain of Friendship between Peoples" ("Brunnen der Völkerfreundschaft"), a popular venue.
In 1993, architect Hans Kollhoff's master plan for a major redevelopment including the construction of several skyscrapers was published. Due to a lack of demand it is unlikely these will be constructed, with the exception (announced January 2014) of a 39-storey residential tower designed by Frank Gehry on which work is expected to begin in 2015. However, beginning with the reconstruction of the "Kaufhof" department store in 2004, and the biggest underground railway station of Berlin, some buildings will be redesigned and new structures built on the square's south-eastern side. Sidewalks were expanded to shrink one of the avenues, a new underground garage was built, and commuter tunnels meant to keep pedestrians off the streets were removed. The surrounding buildings now house chain stores, fast-food restaurants, and fashion discounters. The "Alexa" shopping mall, with approximately 180 stores opened nearby during 2007 and a large "Saturn" electronic store was built and is open on Alexanderplatz since 2008. The CUBIX multiplex cinema, which opened in November 2000, joined in 2007 the team of Berlinale cinemas and the festival is showing films on three of its screens.
Many historic buildings are located in the vicinity of Alexanderplatz. The traditional seat of city government, the Rotes Rathaus, or "Red City Hall", is located nearby, as was the former East German parliament building, the Palast der Republik, demolition of which began in February 2006 and has been completed. The reconstruction of the Baroque Stadtschloss near Alexanderplatz has been in planning for several years.
Alexanderplatz is also the name of the S-Bahn and U-Bahn stations there. It is one of Berlin's largest and most important transportation hubs, being a cross of 3 subway (U-Bahn) lines, 3 S-Bahn lines, and many tram and bus lines, as well as regional trains.

</doc>
<doc id="2512" url="https://en.wikipedia.org/wiki?curid=2512" title="Asian Development Bank">
Asian Development Bank

The Asian Development Bank (ADB) is a regional development bank established on 19 December 1966 which is headquartered in Ortigas Center located in Mandaluyong, Metro Manila, Philippines, and maintains 31 field offices around the world, to promote social and economic development in Asia. The bank admits the members of the United Nations Economic and Social Commission for Asia and the Pacific (UNESCAP, formerly the Economic Commission for Asia and the Far East or ECAFE) and non-regional developed countries. From 31 members at its establishment, ADB now has 67 members, of which 48 are from within Asia and the Pacific and 19 outside. The ADB was modeled closely on the World Bank, and has a similar weighted voting system where votes are distributed in proportion with members' capital subscriptions. ADB releases an annual report that summarizes its operations, budget and other materials for review by the public.
At the end of 2014, Japan holds the largest proportion of shares at 15.7%. The United States holds 15.6%, China holds 6.5%, India holds 6.4%, and Australia holds 5.8%.
Organization.
The highest policy-making body of the bank is the Board of Governors, composed of one representative from each member state. The Board of Governors, in turn, elect among themselves the twelve members of the Board of Directors and their deputy. Eight of the twelve members come from regional (Asia-Pacific) members while the others come from non-regional members.
The Board of Governors also elect the bank's president, who is the chairperson of the Board of Directors and manages ADB. The president has a term of office lasting five years, and may be reelected. Traditionally, and because Japan is one of the largest shareholders of the bank, the president has always been Japanese.
The most recent president was Takehiko Nakao, who succeeded Haruhiko Kuroda in 2013.
The headquarters of the bank is at 6 ADB Avenue, Mandaluyong, Metro Manila, Philippines, and it has 25 field offices in Asia and the Pacific and representative offices in Washington, Frankfurt, Tokyo and Sydney. The bank employs about 3,000 people, representing 60 of its 67 members.
History.
1960s.
The concept of a regional bank was formally proposed, as an institution for developing intra-regional trade, at a trade conference organized by the Economic Commission for Asia and the Far East (ECAFE) in 1963 by a young Thai banker, Paul Sithi-Amnuai. (ESCAP, United Nations Publication March 2007, "The first parliament of Asia" pp. 65). The United States was initially opposed to the creation of another regional development bank following the establishment of the Inter-American Development Bank in 1959. However, with the escalation if the Vietnam War, President Lyndon Johnson was persuaded to support the establishment of the ADB in 1964 in an effort the mollify Senator J. William Fulbright (Chairman of the Senate Foreign Relations Committee) who argued that the War would not only bleed American blood and treasure, but would also be very bad for America's image in Asia. President Johnson pressed retired World Bank President Eugene Black into organizing and establishing the new institution.
In the process, Secretary of State Dean Rusk urged that Japan play an important role in the ADB. He argued that the biggest danger to American foreign policy in Asia was Japan's inability to integrate into the Asian society of nations following the animosities of WWII. Indeed, there was sharp Asian opposition to Japan's participation in the institution. After considerable diplomatic effort, Japan was eventually accepted into the organization by the majority of the participating nations, and Tokyo was selected as the site of the bank's headquarters. The Presidency was to rotate between the various countries of Asia. However, at the eleventh hour in a meeting of the delegates in Manila, President Ferdinand Marcos delivered a stinging tirade against the establishment of the ADB with Japanese participation. He threatened to personally travel to every Asian capitol and scuttle the project. Eugene Black, with the assistance of President Johnson was finally able to mollify President Ramos with the promise to locate the ADB in Manila. (In fact, Ramos eagerly volunteered to house the ADB in the newly constructed building on prestigious Roxas Boulevard, which had been designated for the Foreign Ministry.) As a concession to the Japanese, they were given the inaugural Presidency of the institution - a position they have tenaciously held onto ever since.
Once the ADB was founded in 1966, Japan took up the Presidency and some other crucial "reserve positions" such as the directorship of the all powerful administration department known as BPMSD (Budget, Personnel, and Management Systems Department) through which they manage the institution. By the end of 1972, Japan had contributed $173.7 million (22.6% of the total) to the ordinary capital resources and $122.6 million (59.6% of the total) to the special funds. In contrast, the United States contributed only $1.25 million to the special fund.
After its creation in the 1960s, ADB focused much of its assistance on food production and rural development. At the time, Asia was one of the poorest regions in the world.
Early loans went largely to Indonesia, Thailand, Malaysia, Republic of Korea and the Philippines, the countries with which Japan had crucial trading ties; these nations accounted for 78.48% of the total ADB loans between 1967 and 1972. Moreover, Japan received tangible benefits, 41.67% of the total procurements between 1967 and 1976. Japan tied its special funds contributions to its preferred sectors and regions and procurements of its goods and services, as reflected in its $100 million donation for the Agricultural Special Fund in April 1968.
Takeshi Watanabe served as the first ADB president from 1966 to 1972.
1970s–1980s.
In the 1970s, ADB's assistance to developing countries in Asia expanded into education and health, and then to infrastructure and industry. The gradual emergence of Asian economies in the latter part of the decade spurred demand for better infrastructure to support economic growth. ADB focused on improving roads and providing electricity. When the world suffered its first oil price shock, ADB shifted more of its assistance to support energy projects, especially those promoting the development of domestic energy sources in member countries.
Following considerable pressure from the Reagan Administration in the 1980s, ADB reluctantly began working with the private sector in an attempt to increase the impact of its development assistance to poor countries in Asia and the Pacific. In the wake of the second oil crisis, ADB expanded its assistance to energy projects. In 1982, ADB opened its first field office, in Bangladesh, and later in the decade it expanded its work with non-government organizations (NGOs).
Japanese presidents Inoue Shiro (1972–76) and Yoshida Taroichi (1976–81) took the spotlight in the 1970s. Fujioka Masao, the fourth president (1981–90), adopted an assertive leadership style, launching an ambitious plan to expand the ADB into a high-impact development agency.
1990s.
In the 1990s, ADB began promoting regional cooperation by helping the countries on the Mekong River to trade and work together. The decade also saw an expansion of ADB's membership with the addition of several Central Asian countries following the end of the Cold War.
In mid-1997, ADB responded to the financial crisis that hit the region with projects designed to strengthen financial sectors and create social safety nets for the poor. During the crisis, ADB approved its largest single loan – a $4 billion emergency loan to the Republic of Korea. In 1999, ADB adopted poverty reduction as its overarching goal.
2000s.
In 2003, severe acute respiratory syndrome (SARS) epidemic hit the region and ADB responded with programs to help the countries in the region work together to address infectious diseases, including avian influenza and HIV/AIDS. ADB also responded to a multitude of natural disasters in the region, committing more than $850 million for recovery in areas of India, Indonesia, Maldives, and Sri Lanka hit by the December 2004 Asian tsunami. In addition, $1 billion in loans and grants was provided to the victims of the October 2005 earthquake in Pakistan.
In March of 2008, the Board of Directors formally adopted the Long Term Strategic Framework (LTSF) which stated that assistance to private sector development was the lead priority of the ADB and that it should constitute 50% of the bank's lending by 2020. 
In 2009, ADB's Board of Governors agreed to triple ADB's capital base from $55 billion to $165 billion, giving it much-needed resources to respond to the global economic crisis. The 200% increase is the largest in ADB's history, and the first since 1994.
2010s.
Asia moved beyond the economic crisis and by 2010 had emerged as a new engine of global economic growth though it remained home to two thirds of the world’s poor. In addition, the increasing prosperity of many people in the region created a widening income gap that left many people behind. ADB responded to this with loans and grants that encouraged economic growth.
In early 2012, began to re-engage with Myanmar in response to reforms initiated by the government. In April 2014, ADB opened an office in Myanmar and resumed making loans and grants to the country.
In 2017, ADB will combine the lending operations of its Asian Development Fund (ADF) with its ordinary capital resources (OCR), which will increased its annual lending and grants to as high as $20 billion—50% more than the previous level.
Objectives and activities.
Aim.
The ADB defines itself as a social development organization that is dedicated to reducing poverty in Asia and the Pacific through inclusive economic growth, environmentally sustainable growth, and regional integration. This is carried out through investments – in the form of loans, grants and information sharing – in infrastructure, health care services, financial and public administration systems, helping nations prepare for the impact of climate change or better manage their natural resources, as well as other areas.
Focus areas.
Eighty percent of ADB’s lending is concentrated public sector lending in five operational areas. 
Lending.
The ADB offers "hard" loans on commercial terms primarily to middle income countries in Asia and "soft" loans with lower interest rates to poorer countries in the region. Based on a new policy, both types of loans will be sourced starting January 2017 from the bank’s ordinary capital resources (OCR), which functions as its general operational fund.
In 2014, ADB lent $11.2 billion to its member governments – known as "sovereign" lending – and invested another $1.7 billion in private enterprises, as part of its "nonsovereign" operations. ADB’s operations in 2014, including grants and other assistance, totaled $22.93 billion.
ADB obtains its funding by issuing bonds on the world's capital markets. It also relies on the contributions of member countries, retained earnings from lending operations, and the repayment of loans.
Private sector investments.
ADB provides direct financial assistance, in the form of debt, equity and mezzanine finance to private sector companies, for projects that have clear social benefits beyond the financial rate of return. ADB’s participation is usually limited but it leverages a large amount of funds from commercial sources to finance these projects by holding no more than 25% of any given transaction.
Cofinancing.
ADB partners with other development organizations on some projects to increase the amount of funding available. In 2014, $9.2 billion—or nearly half—of ADB’s $22.9 billion in operations were financed by other organizations. According to Jason Rush, Principal Communication Specialist, the Bank communicates with many other multilateral organizations.
Funds and resources.
More than 50 financing partnership facilities, trust funds, and other funds – totalling several billion each year – are administered by ADB and put toward projects that promote social and economic development in Asia and the Pacific.
Access to information.
ADB has an information disclosure policy that presumes all information that is produced by the institution should be disclosed to the public unless there is a specific reason to keep it confidential. The police calls for accountability and transparency in operations and the timely response to requests for information and documents. ADB does not disclose information that jeopardizes personal privacy, safety and security, certain financial and commercial information, as well as other exceptions.
Criticism.
Since the ADB's early days, critics have charged that the two major donors, Japan and the United States, have had extensive influence over lending, policy and staffing decisions.
Oxfam Australia has criticized the Asian Development Bank for insensitivity to local communities. "Operating at a global and international level, these banks can undermine people's human rights through projects that have detrimental outcomes for poor and marginalized communities." The bank also received criticism from the United Nations Environmental Program, stating in a report that "much of the growth has bypassed more than 70 percent of its rural population, many of whom are directly dependent on natural resources for livelihoods and incomes."
There had been criticism that ADB's large scale projects cause social and environmental damage due to lack of oversight. One of the most controversial ADB-related projects is Thailand's Mae Moh coal-fired power station. Environmental and human rights activists say ADB's environmental safeguards policy as well as policies for indigenous peoples and involuntary resettlement, while usually up to international standards on paper, are often ignored in practice, are too vague or weak to be effective, or are simply not enforced by bank officials.
The bank has been criticized over its role and relevance in the food crisis.The ADB has been accused by civil society of ignoring warnings leading up the crisis and also contributing to it by pushing loan conditions that many say unfairly pressure governments to deregulate and privatize agriculture, leading to problems such as the rice supply shortage in Southeast Asia.
The bank has also been criticized by Vietnam War veterans for funding projects in Laos, because of the United States' 15% stake in the bank, underwritten by taxes. Laos became a communist country after the U.S. withdrew from Vietnam, and the Laotian Civil War was won by the Pathet Lao, which is widely understood to have been supported by the North Vietnamese Army.
In 2009, the bank endorsed a $2.9 billion funding strategy for proposed projects in India. The projects in this strategy were only indicative and still needed to be further approved by the bank's board of directors; however, PRC Foreign Ministry spokesman Qin Gang claimed, "The Asian Development Bank, regardless of the major concerns of China, approved the India Country Partnership strategy which involves the territorial dispute between China and India. China expresses its strong dissatisfaction over this... The bank's move not only seriously tarnishes its own name, but also undermines the interests of its members."
There has been considerable criticism of management for its reluctance to implement the Long Term Strategic Framework (LTSF) which (as noted above) was formally adopted in March of 2008. Indeed, whereas the Private Sector Operations Department (PSOD)closed out that year with financings of $2.4 billion, the ADB has significantly dropped below that level in the years since and is clearly not on the path to achieving its stated goal of 50% of financings to the private sector by 2020. Critics also point out that the PSOD is the only Department that actually makes money for the ADB. Hence, with the vast majority of loans going to concessionary (sub-market) loans to the public sector, the ADB is facing considerable financial difficulty and continuous operating losses.
List of 20 largest countries and regions by subscribed capital and voting power.
The following table are amounts for 20 largest countries by subscribed capital and voting power at the Asian Development Bank as of December 2014.
Members.
ADB has 67 members (as of 2 February 2007): 48 members from the Asian and Pacific Region, 19 members from Other Regions. Notable non-members are Russia, Bahrain, Iran, Iraq, Jordan, Kuwait, Lebanon, North Korea, Oman, Qatar, Saudi Arabia, the United Arab Emirates, and Yemen. "Names are as recognized by ADB." The year after a member's name indicates the year of membership. At the time a country ceases to be a member, the Bank shall arrange for the repurchase of such country's shares by the Bank as a part of the settlement of accounts with such country in accordance with the provisions of paragraphs 3 and 4 of Article 43.
References.
63) http://devpolicy.org/aiibe-20150810/

</doc>
<doc id="2514" url="https://en.wikipedia.org/wiki?curid=2514" title="Aswan">
Aswan

Aswan (; ' ; Ancient Egyptian: '; '; '), formerly spelled "Assuan", is a city in the south of Egypt, the capital of the Aswan Governorate.
Aswan is a busy market and tourist centre located just north of the Aswan Dams on the east bank of the Nile at the first cataract. The modern city has expanded and includes the formerly separate community on the island of Elephantine.
History.
Aswan is the ancient city of Swenett, which in antiquity was the frontier town of Ancient Egypt facing the south. Swenett is supposed to have derived its name from an Egyptian goddess with the same name. This goddess later was identified as Eileithyia by the Greeks and Lucina by the Romans during their occupation of Ancient Egypt because of the similar association of their goddesses with childbirth, and of which the import is "the opener". The ancient name of the city also is said to be derived from the Egyptian symbol for "trade", or "market".
Because the Ancient Egyptians oriented toward the origin of the life-giving waters of the Nile in the south, Swenett was the first town in the country, and Egypt always was conceived to "open" or begin at Swenett. The city stood upon a peninsula on the right (east) bank of the Nile, immediately below (and north of) the first cataract of the flowing waters, which extend to it from Philae. Navigation to the delta was possible from this location without encountering a barrier.
The stone quarries of ancient Egypt located here were celebrated for their stone, and especially for the granitic rock called Syenite. They furnished the colossal statues, obelisks, and monolithal shrines that are found throughout Egypt, including the pyramids; and the traces of the quarrymen who wrought in these 3,000 years ago are still visible in the native rock. They lie on either bank of the Nile, and a road, in length, was cut beside them from Syene to Philae.
Swenett was equally important as a military station as that of a place of traffic. Under every dynasty it was a garrison town; and here tolls and customs were levied on all boats passing southwards and northwards. Around AD 330, the legion stationed here received a bishop from Alexandria; this later became the Coptic Diocese of Syene. The city is mentioned by numerous ancient writers, including Herodotus, Strabo, Stephanus of Byzantium, Ptolemy, Pliny the Elder, Vitruvius, and it appears on the Antonine Itinerary. It also is mentioned in the Book of Ezekiel and the Book of Isaiah.
The latitude of the city that would become Aswan – located at 24° 5′ 23″ – was an object of great interest to the ancient geographers. They believed that it was seated immediately under the tropic, and that on the day of the summer solstice, a vertical staff cast no shadow. They noted that the sun's disc was reflected in a well at noon. This statement is only approximately correct; at the summer solstice, the shadow was only /th of the staff, and so could scarcely be discerned, and the northern limb of the Sun's disc would be nearly vertical.
However, Eratosthenes used this information together with measurements of the shadow length on the solstice at Alexandria to perform the first known calculation of the circumference of the Earth.
The Nile is nearly wide above Aswan. From this frontier town to the northern extremity of Egypt, the river flows for more than without bar or cataract. The voyage from Aswan to Alexandria usually took 21 to 28 days in favourable weather.
Climate.
Aswan has a hot desert climate (Köppen climate classification "BWh") like almost all of Egypt outside the Mediterranean coast and high altitude mountains. Aswan and Luxor have the hottest summer days of any city in Egypt. Aswan is one of the hottest, sunniest and driest cities in the world. Averages high temperatures are consistently above during summer (June, July, August and also September) while averages low temperatures remain above . Summers are long, prolonged and extremely hot. Averages high temperatures remain above during the coldest month of the year while averages low temperatures remain above . Winters are short, brief and extremely warm. Wintertime is very pleasant and enjoyable while summertime is unbearably hot with blazing sunshine although desert heat is dry.
The climate of Aswan is extremely dry year-round, with less than of average annual precipitation. The desert city is one of the driest ones in the world, and rainfall doesn't occur every year, as of early 2001, the last rain there was seven years earlier. Aswan is one of the least humid cities on the planet, with an average relative humidity of only 26%, with a maximum mean of 42% during winter and a minimum mean of 16% during summer.
The climate of Aswan is extremely clear, bright and sunny year-round, in all seasons, with a low seasonal variation, with about some 4,000 hours of annual sunshine, very close of the maximum theoretical sunshine duration. Aswan is one of the sunniest places on Earth.
The highest record temperature was on May 22, 1973 and the lowest record temperature was on January 6, 1989.
Education.
In 1999, South Valley University was inaugurated and it has three branches; Aswan, Qena and Hurghada. The university grew steadily and now it is firmly established as a major institution of higher education in Upper Egypt. Aswan branch of Assiut University began in 1973 with the Faculty of Education and in 1975 the Faculty of Science was opened. Aswan branch has five faculties namely; Science, Education, Engineering, Arts, Social Works and Institute of Energy. The Faculty of Science in Aswan has six departments. Each department has one educational programme: Chemistry, Geology, Physics and Zoology. Except Botany Department, which has three educational programmes: Botany, Environmental Sciences and Microbiology; and Mathematics Department, which has two educational programmes: Mathematics and Computer Science. The Faculty of Science awards the following degrees: Bachelor of Science in nine educational programmes, Higher Diploma, Master of Science and Philosophy Doctor of Science. Over 100 academic staff members are employed in.
Transport.
Aswan is served by the Aswan International Airport. Train and bus service is also available. Taxi and rickshaw are used for transport here.

</doc>
<doc id="2519" url="https://en.wikipedia.org/wiki?curid=2519" title="Adelaide of Italy">
Adelaide of Italy

Adelaide of Italy (931 – 16 December 999), also called Adelaide of Burgundy, was the second wife of Holy Roman Emperor Otto the Great and was crowned as the Holy Roman Empress with him by Pope John XII in Rome on February 2, 962. Empress Adelaide was perhaps the most prominent European woman of the 10th century; she was regent of the Holy Roman Empire as the guardian of her grandson in 991-995.
Life.
Born in Orbe Castle, Orbe, Kingdom of Upper Burgundy (now in modern-day Switzerland), she was the daughter of Rudolf II of Burgundy, a member of the Elder House of Welf, and Bertha of Swabia. Her first marriage, at the age of fifteen, was to the son of her father's rival in Italy, Lothair II, the nominal King of Italy; the union was part of a political settlement designed to conclude a peace between her father and Hugh of Provence, the father of Lothair. Their daughter, Emma of Italy, was born about 948.
Marriage to Otto I.
The Calendar of Saints states that her first husband was poisoned by the holder of real power, his successor, Berengar of Ivrea, who attempted to cement his political power by forcing her to marry his son, Adalbert; when she refused and fled, she was tracked down and imprisoned for four months at Como.
According to Adelaide's contemporary biographer, Odilo of Cluny, she managed to escape from captivity. After a time spent in the marshes nearby, she was rescued and taken to a "certain impregnable fortress," likely the fortified town of Canossa near Reggio. She managed to send an emissary to Otto I, and asked the East Frankish king for his protection. The widow met Otto at the old Lombard capital of Pavia and they married in 951. Pope John XII crowned Otto Holy Roman Emperor in Rome on February 2, 962, and, breaking tradition, also crowned Adelaide as Holy Roman Empress.
In Germany, the crushing of a revolt in 953 by Liudolf, Otto's son by his first marriage, cemented Adelaide's position, for she retained all her dower lands. She and their eleven-year-old son, the crown prince who became Otto II, accompanied Otto in 966 on his third expedition to Italy, where Otto restored the newly elected Pope John XIII to his throne (and executed some of the Roman rioters who had deposed him). Adelaide remained in Rome for six years while Otto ruled his kingdom from Italy. Their son Otto II was crowned co-emperor in 967, then married the Byzantine princess Theophanu in April 972, resolving the conflict between the two empires in southern Italy, as well as ensuring the imperial succession. Adelaide and her husband then returned to Germany, where Otto died in May 973, at the same Memleben palace where his father had died 37 years earlier. 
Retirement.
Adelaide had long entertained close relations with Cluny, then the center of the movement for ecclesiastical reform, and in particular with its abbots Majolus and Odilo. She retired to a nunnery she had founded in c. 991 at Selz in Alsace. Though she never became a nun, she spent the rest of her days there in prayer. On her way to Burgundy to support her nephew Rudolf III against a rebellion, she died at Selz Abbey on December 16, 999, days short of the millennium she thought would bring the Second Coming of Christ. She had constantly devoted herself to the service of the church and peace, and to the empire as guardian of both; she also interested herself in the conversion of the Slavs. She was thus a principal agent—almost an embodiment—of the work of the pre-schism Orthodox Catholic Church at the end of the Early Middle Ages in the construction of the religious culture of Central Europe.
Some of her relics are preserved in a shrine in Hanover. Her feast day, December 16, is still kept in many German dioceses.
Issue.
In 947, Adelaide was married to King Lothair II of Italy. The union produced one child:
In 951, Adelaide was married to King Otto I, the future Holy Roman Emperor. The union produced four children:
|-

</doc>
<doc id="2524" url="https://en.wikipedia.org/wiki?curid=2524" title="Airbus A300">
Airbus A300

The Airbus A300 is a short- to medium-range wide-body twin-engine jet airliner that was developed and manufactured by Airbus. Formally announced in 1969 and first flying in October 1972, it holds the distinction of being the world's first twin-engined widebody airliner; it was also the first product of Airbus Industrie, a consortium of European aerospace manufacturers, now a subsidiary of Airbus Group. The A300 can typically seat 266 passengers in a two-class layout, with a maximum range of when fully loaded, depending on model.
Air France, the launch customer for the A300, introduced the type into service on 30 May 1974. Production of the A300 ceased in July 2007, along with its smaller A310 derivative. The freighter sales for which the A300 had previously competed for in later life are instead fulfilled by a A330-200F derivative.
Development.
Origins.
During the 1960s, European aircraft manufacturers such as Hawker Siddeley and the British Aircraft Corporation, based in the UK, and Sud Aviation of France, had ambitions to build a new 200-seat airliner for the growing civil aviation market. While studies were performed and considered, such as a stretched twin-engine of the Hawker Siddeley Trident and an expanded development of the BAC 1-11, designated BAC 2-11, it was recognized that if each of the European manufacturers were to launch quite similar aircraft onto the market at the same time, none of them would achieve the volume sales needed to make them viable. In 1965, a government study, known as the Plowden Report, had found British aircraft production costs to between 10 and 20 per cent higher than American counterparts due to shorter production runs, which in turn was due to the fractured European market. To overcome this factor, the report recommended the pursuit of multinational collaborative projects between the region's leading aircraft manufacturers.
European manufacturers were keen to explore prospective programs; the proposed 260-seat wide-body "HBN 100" between Hawker Siddeley, Nord Aviation, and Breguet Aviation being one such example. National governments were also keen to support such efforts amid a belief that American manufacturers could dominate the European Economic Community; in particular, Germany had ambitions for a multinational airliner project to invigorate its aircraft industry, which had declined considerably following the Second World War. During the mid-1960s, both Air France and American Airlines had expressed interest in a twin-engine wide-body aircraft, indicating a market demand for such an aircraft to be produced. In July 1967, during a high profile meeting between French, German, and British ministers, an agreement was made for greater cooperation between European nations in the field of aviation technology, and "for the joint development and production of an airbus".
Shortly after the July 1967 meeting, French engineer Roger Béteille was appointed as the technical director of what would become the A300 program, while Henri Ziegler, chief operating office of Sud Aviation, was appointed as the general manager of the organization and German politician Franz Josef Strauss became the chairman of the supervisory board. Béteille drew up an initial work share plan for the project, under which French firms would produce the aircraft's cockpit, the control systems, and lower-center portion of the fuselage, Hawker Siddeley would manufacture the wings, while German companies would produce the forward, rear and upper part of the center fuselage sections. Addition work included moving elements of the wings being produced in the Netherlands, and Spain producing the horizontal tail plane.
An early design goal for the A300 that Béteille had stressed the importance of was the incorporation of a high level of technology, which would serve as a decisive advantage over prospective competitors. As such, the A300 would feature the first use of composite materials of any passenger aircraft, the leading and trailing edges of the tail fin being composed of glass fibre reinforced plastic. Béteille opted for English as the working language for the developing aircraft, as well against using Metric instrumentation and measurements, as most airlines already had US-built aircraft. These decisions were partially influenced by feedback from various airlines, such as Air France and Lufthansa, as an emphasis had been placed on determining the specifics of what kind of aircraft that potential operators were seeking. According to Airbus, this cultural approach to market research had been crucial to the company's long term success.
On 26 September 1967, the British, French, and West German governments signed a Memorandum of Understanding to start development of the 300-seat Airbus A300. Under the terms of the memorandum, Britain and France were each to receive a 37.5 per cent work share on the project, while Germany received a 25 per cent share. Sud Aviation was recognized as the lead company for A300, with Hawker Siddeley being selected as the British partner company. At the time, the news of the announcement had been clouded by the British Government's support for the Airbus, which coincided with its refusal to back BAC's proposed competitor, the BAC 2-11, despite a preference for the latter expressed by British European Airways (BEA). Another problem was the requirement for a new engine to be developed by Rolls-Royce to power the proposed airliner; a derivative of the in-development Rolls-Royce RB211, the triple-spool RB207, capable of producing of 47,500 lbf.
In December 1968, the French and British partner companies (Sud Aviation and Hawker Siddeley) proposed a revised configuration, the 250-seat Airbus A250. It had been feared that the original 300-seat proposal was too large for the market, thus it had been scaled down to produce the A250. The dimensional changes involved in the shrink reduced the length of the fuselage by 5.62 meters and the diameter by 0.8 meters, reducing the overall weight by 25 tonnes. For increased flexibility, the cabin floor was raised so that standard LD3 freight containers could be accommodated side-by-side, allowing more cargo to be carried. Refinements made by Hawker Siddeley to the wing's design provided for greater lift and overall performance; this have the aircraft the ability to climb faster and attain a level cruising altitude sooner than any other passenger aircraft. It was later renamed the A300B.
Perhaps the most significant change of the A300B was that it would not require new engines to be developed, being of a suitable size to be powered by Rolls-Royce's RB211, or alternatively the American Pratt & Whitney JT9D and General Electric CF6 powerplants; this switch was recognized as considerably reducing the project's development costs. To attract potential customers in the US market, it was decided that General Electric CF6-50 engines would power the A300 in place of the British RB207; these engines would be produced in co-operation with French firm Snecma. By this time, Rolls-Royce had been concentrating their efforts upon developing their RB211 turbofan engine instead and progress on the RB207s development had been slow for some time, which had been a factor in the engine switch decision.
On 10 April 1969, a few months after the decision to drop the RB207 had been announced, the British government announced that they would withdraw from the Airbus venture. In response, West Germany proposed to France that they would be willing to contribute up to 50% of the project's costs if France was prepared to do the same. Additionally, the managing director of Hawker Siddeley, Sir Arnold Alexander Hall, decided that his company would remain in the project as a favoured sub-contractor, developing and manufacturing the wings for the A300, which would later become pivotal in later versions' impressive performance from short domestic to long intercontinental flights. Hawker Siddeley spent £35 million of its own funds, along with a further £35 million loan from the West German government, on the machine tooling to design and produce the wings.
Project launch.
On 29 May 1969, during the Paris Air Show, French transport minister Jean Chamant and German economics minister Karl Schiller signed an agreement officially launching the Airbus A300, the world's first twin-engine widebody airliner. The intention of the project was to produce an aircraft that was smaller, lighter, and more economical than its three-engine American rivals, the McDonnell Douglas DC-10 and the Lockheed L-1011 TriStar. In order to meet Air France's demands for a aircraft larger than 250-seat A300B, it was decided to stretch the fuselage to create a new variant, designated as the A300B2, which would be offered alongside the original 250-seat A300B, henceforth referred to as the A300B1. On 3 September 1970, Air France signed a letter of intent for six A300s, marking the first order to be won for the new airliner.
In the aftermath of the Paris Air Show agreement, it was decided that, in order to provide effective management of responsibilities, a Groupement d'intérêt économique would be established, allowing the various partners to work together on the project while remaining separate business entities. On 18 December 1970, Airbus Industrie was formally established following an agreement between Aérospatiale (the newly merged Sud Aviation and Nord Aviation) of France and the antecedents to Deutsche Aerospace of Germany, each receiving a 50 per cent stake in the newly-formed company. In 1971, the consortium was joined by a third full partner, the Spanish firm CASA, who received a 4.2 per cent stake, the other two members reducing their stakes to 47.9 per cent each. In 1979, Britain joined the Airbus consortium via British Aerospace, which Hawker Siddeley had merged into, who acquired a 20 per cent stake in Airbus Industrie with France and Germany each reducing their stakes to 37.9 per cent.
Airbus Industrie was initially headquartered in Paris, which is where design, development, flight testing, sales, marketing, and customer support activities were centered; the headquarters was relocated to Toulouse in January 1974. The final assembly line for the A300 was located adjacent to Toulouse Blagnac International Airport, the manufacturing process necessitated transporting each aircraft section being produced the partner companies scattered across Europe to this one location. The combined use of ferries and roads were used for the assembly of the first A300, however this was time-consuming and not viewed as ideal by Felix Kracht, Airbus Industrie's production director. Felix's solution was to have the various A300 sections brought to Toulouse by a fleet of Boeing 377-derived Aero Spacelines Super Guppy aircraft, by which means none of the manufacturing sites were more than two hours away. Having the sections airlifted in this manner made the A300 the first airliner to use just-in-time manufacturing techniques, and allowed each company to manufacture its sections as fully-equipped, ready-to-fly assemblies.
On 28 September 1972, the first prototype A300 was unveiled to the public, it conducted its maiden flight from Toulouse–Blagnac International Airport on 28 October that year. This maiden flight, which was performed a month ahead of schedule, lasted for one hour and 25 minutes; the captain was Max Fischl and the first officer was Bernard Ziegler, son of Henri Ziegler. On 5 February 1973, the second prototype performed its maiden flight. In September 1973, as part of promotional efforts for the A300, the new aircraft was taken on a six-week tour around North America and South America, to demonstrate it to airline executives, pilots, and would-be customers. Amongst the consequences of this expedition, it had allegedly brought the A300 to the attention of Frank Borman of Eastern Airlines, one of the "big four" U.S. airlines.
In March 1974, type certificates were granted for the A300 from both German and French authorities, clearing the way for its entry into revenue service. The first production model, the A300B2, entered service in 1974, followed by the A300B4 one year later. Initially, the success of the consortium was poor, in part due to the economic consiquences of the 1973 oil crisis, but by 1979 there were 81 A300 passenger liners in service with 14 airlines, alongside 133 firm orders and 88 options. Ten years after the official launch of the A300, the company had achieved a 26 per cent market share in terms of dollar value, enabling Airbus Industries to proceed with the development of its second aircraft, the Airbus A310. It was the launch of the Airbus A320 in 1987 that firmly established Airbus as a major player in the aircraft market – over 400 orders were placed before the narrow-body airliner had flown its first flight, compared to 15 for the A300 in 1972.
Design.
The Airbus A300 is a wide-body medium-to-long range airliner; it has the distinction of being the first twin-engine wide-body aircraft in the world. In 1977, the A300 became the first ETOPS-compliant aircraft, due to its high performance and safety standards. Another world-first of the A300 is the use of composite materials on a commercial aircraft, which was used on both secondary and later primary airframe structures, decreasing overall weight and improving cost-effectiveness. Other firsts included the pioneering use of center-of-gravity control, achieved by transferring fuel between various locations across the aircraft, and electrically-signaled secondary flight controls.
The A300 is powered by a pair of underwing turbofan engines, either General Electric CF6 and Pratt & Whitney JT9D engines; the sole use of underwing engine pods allowed for any suitable turbofan engine to be more readily used. The lack of a third tail-mounted engine, as per the trijet configuration used by some competing airliners, allowed for the wings to be located further forwards and to reduce the size of the vertical stabilizer and elevator, which had the effect of increasing the aircraft's fuel efficiency.
Airbus partners had employed the latest technology, some of which having been derived from Concorde, on the A300. According to Airbus, new technologies adopted for the airliner were selected principally for increased safety, operational capability, and profitability. Upon entry into service in 1974, the A300 was a very advanced plane, which went on to influence later airliner designs. The technological highlights include:
Later A300s incorporated other advanced features such as the Forward-Facing Crew Cockpit, which enabled a two-pilot flight crew to fly the aircraft alone without the need for a flight engineer, the functions of which were automated; this two-man cockpit concept was a world-first for a wide-body aircraft. Glass cockpit flight instrumentation, which used cathode ray tube (CRT) monitors to display flight, navigation, and warning information, along with fully-digital dual autopilots and digital flight control computers for controlling the spoilers, flaps, and leading-edge slats, were also adopted upon later-built models. Additional composites were also made use of, such as carbon-fiber-reinforced polymer (CFRP), as well as their presence in an increasing proportion of the aircraft's components, including the spoilers, rudder, air brakes, and landing gear doors. Another feature of later aircraft were the addition of wingtip fences, which generated greater aerodynamic performance (first introduced on the A310-300).
In addition to passenger duties, the A300 became widely used by air freight operators; according to Airbus, it is the best selling freight aircraft of all time. Various variants of the A300 were built to meet customer demands, often for diverse roles such as aerial refueling tankers, freighter models (new-build and conversions), combi aircraft, military airlifter, and VIP transport. Perhaps the most visually unique of the variants is the A300-600ST Beluga, a oversize cargo-carrying model operated by Airbus to carry aircraft sections between their manufacturing facilities. The A300 was the basis for, and retained a high level of commonality with, the second airliner produced by Airbus, the smaller Airbus A310.
Operational history.
On 23 May 1974, the first A300 to enter service performed the first commercial flight of the type, flying from Paris to London, for Air France.
Immediately after the launch, sales of the A300 were weak for some years, with most orders going to airlines that had an obligation to favor the domestically made product – notably Air France and Lufthansa, the first two airlines to place orders for the type. Following the appointment of Bernard Lathière as Henri Ziegler's replacement, an aggressive sales approach was adopted. Germanwings was the world's first international airline to purchase the A300. Indian Airlines was the world's first domestic airline to purchase the A300, ordering three aircraft with three options. However, between December 1975 and May 1977, there were no sales for the type. During this period a number of "whitetail" A300s – completed but unsold aircraft – were completed and stored at Toulouse, and production fell to half an aircraft per month amid calls to pause production completely.
During the flight testing of the A300B2, Airbus held a series of talks with Korean Air on the topic of developing a longer-range version of the A300, which would become the A300B4. In September 1974, Korean Air placed an order for 4 A300B4s with options for 2 further aircraft; this sale was viewed as significant as it was the first non-European international airline to order Airbus aircraft. Airbus had viewed South-East Asia as a vital market that was ready to be opened up and believed that Korean Air to be the 'key'.
It was becoming clear that the whole concept of a short haul widebody was flawed. Airlines operating the A300 on short haul routes were forced to reduce frequencies to try and fill the aircraft. As a result, they lost passengers to airlines operating more frequent narrow body flights. The supposed widebody comfort which it was assumed passengers would demand was illusory. Eventually, Airbus had to build its own narrowbody aircraft (the A320) to compete with the Boeing 737 and McDonnell Douglas DC-9/MD-80. The savior of the A300 was the advent of Extended Range Twin Operations (ETOPS), a revised FAA rule which allows twin-engine jets to fly long-distance routes that were previously off-limits to them. This enabled Airbus to develop the aircraft as a medium/long range airliner.
In 1977, US carrier Eastern Air Lines leased four A300s as an in-service trial. Frank Borman, ex-astronaut and the then CEO of the airline, was impressed that the A300 consumed 30% less fuel, even more economical than expected, in contrast to his fleet of Tristars and proceeded to order 23 A300s, becoming the first U.S. customer for the type (This order is often cited as the point at which Airbus came to be seen as a serious competitor to the large American aircraft-manufacturers Boeing and McDonnell Douglas). Aviation author John Bowen alleged that various concessions, such as loan guarentees from European governments and compensation payments, were a factor in the decision as well. The Eastern Air Lines breakthrough was shortly followed by an order from Pan Am. From then on, the A300 family sold well, eventually reaching a total of 878 delivered aircraft.
In December 1977, AeroCóndor Colombia became the first Airbus operator in Latin America, leasing one Airbus A300, named "Ciudad de Barranquilla".
During the late 1970s, Airbus adopted a so-called 'Silk Road' strategy, targeting airlines in the Far East. As a result, The aircraft found particular favor with Asian airlines, being bought by Japan Air System, Korean Air, China Eastern Airlines, Thai Airways International, Singapore Airlines, Malaysia Airlines, Philippine Airlines, Garuda Indonesia, China Airlines, Pakistan International Airlines, Indian Airlines, Trans Australia Airlines and many others. As Asia did not have restrictions similar to the FAA 60-minutes rule for twin-engine airliners which existed at the time, Asian airlines used A300s for routes across the Bay of Bengal and South China Sea.
In 1977, the A300B4 became the first ETOPS compliant aircraft – its high performance and safety standards qualified it for Extended Twin Engine Operations over water, providing operators with more versatility in routing. In 1982 Garuda Indonesia became the first airline to fly the A300B4-200FF. By 1981, Airbus was growing rapidly, with over 300 aircraft sold and options for 200 more planes for over forty airlines. Alarmed by the success of the A300, Boeing responded with the new Boeing 767.
In 1989, Chinese operator China Eastern Airlines received its first A300; by 2006, the airline operated around 18 A300s, making it the largest operator of both the A300 and the A310 at that time. On 31 May 2014, China Eastern officially retired the last A300-600 in its fleet, having begun drawing down the type in 2010.
From 1997 to 2014, a single A300, designated A300 Zero-G, was operated by the European Space Agency (ESA), centre national d'études spatiales (CNES) and the German Aerospace Center (DLR) as a reduced-gravity aircraft for conducting research into microgravity; the A300 is the largest aircraft to ever have been used in this capacity. A typical flight would last for two and a half hours, enabling up to 30 parabolas to be performed per flight.
The A300 provided Airbus the experience of manufacturing and selling airliners competitively. The basic fuselage of the A300 was later stretched (A330 and A340), shortened (A310), or modified into derivatives (A300-600ST "Beluga" Super Transporter).The largest freight operator of the A300 is FedEx Express, which, as of January 2012, had 71 A300 aircraft in service. UPS Airlines also operates freighter versions of the A300. The final version was the A300-600R and is rated for 180-minute ETOPS. The A300 has enjoyed renewed interest in the secondhand market for conversion to freighters. The freighter versions – either new-build A300-600s or converted ex-passenger A300-600s, A300B2s and B4s – account for most of the world freighter fleet after the Boeing 747 freighter.
In March 2006, Airbus announced the impending closure of the A300/A310 final assembly line, making them the first Airbus aircraft to be discontinued. The final production A300, an A300F freighter, performed its initial flight on 18 April 2007, and was delivered to FedEx Express on 12 July 2007. Airbus has announced a support package to keep A300s flying commercially until at least 2025.
Variants.
A300B1.
Only two were built: the first prototype, registered F-WUAB, then F-OCAZ, and a second aircraft, F-WUAC, which was leased in November 1974 to Trans European Airways (TEA) and re-registered OO-TEF. TEA instantly subleased the aircraft for six weeks to Air Algérie, but continued to operate the aircraft until 1990. It had accommodation for 300 passengers (TEA) or 323 passengers (Air Algérie) with a maximum weight of 132,000 kg and two General Electric CF6-50A engines of 220 kN thrust. The A300B1 was five frames shorter than the later production versions, being only 50.97 m (167.2 ft) in length.
A300B2.
The first production version. Powered by General Electric CF6 or Pratt & Whitney JT9D engines (the same engines that powered the Boeing 747-100, "the original jumbo jet") of between 227 and 236 kN thrust, it entered service with Air France in May 1974. The prototype A300B2 made its first flight on 28 June 1973 and was certificated by the French and German authorities on 15 March 1974 and FAA approval followed on 30 May 1974. The first production A300B2 (A300 number 5) made its maiden flight on 15 April 1974 and was handed over to Air France a few weeks later on 10 May 1974. The A300B2 entered revenue service on 23 May 1974 between Paris and London.
A300B4.
The major production version. Features a centre fuel tank for increased fuel capacity (47,500 kg) and new wing root Krüger flaps which were later made available as an option for the B2. Production of the B2 and B4 totalled 248. The first A300B4 (the 9th A300) flew on 25 December 1974 and was certified on 26 March 1975. The first delivery was made to Germanair (which later merged into Hapag Lloyd) on 23 May 1975.
A300-600.
Officially designated A300B4-600, this version is nearly the same length as the B2 and B4 but has increased space because it uses the A310 rear fuselage and horizontal tail. It has higher power CF6-80 or Pratt & Whitney PW4000 engines and uses the Honeywell 331-250 auxiliary power unit (APU). Other changes include an improved wing featuring a recambered trailing edge, the incorporation of simpler single-slotted Fowler flaps, the deletion of slat fences, and the removal of the outboard ailerons after they were deemed unnecessary on the A310. The A300-600 made its first flight on 8 July 1983 and entered service later that year with Saudi Arabian Airlines. A total of 313 A300-600s (all versions) have been sold. The A300-600 also has a similar cockpit to the A310, eliminating the need for a flight engineer. The FAA issues a single type rating which allows operation of both the A310 and A300-600.
A300B10 (A310).
Introduced a shorter fuselage, a new, higher aspect ratio wing, smaller horizontal tail and two crew operation. It was available in standard −200 and the Extended range −300 with range in both passenger and full cargo versions.
It was also available as a military tanker/transport serving the Canadian Forces and German Air Force. Sales total 260, although five of these (ordered by Iraqi Airways) were never built.
A300-600ST.
Commonly referred to as the Airbus Beluga or "Airbus Super Transporter," these five airframes are used by Airbus to ferry parts between the company's disparate manufacturing facilities, thus enabling workshare distribution. They replaced the four Aero Spacelines Super Guppys previously used by Airbus.
Incidents and accidents.
As of September 2015, the A300 has been involved in 70 accidents and incidents, including 32 hull-losses and 1,435 fatalities.
Aircraft on display.
Four A300s are currently preserved:
Specifications.
Sources:

</doc>
<doc id="2526" url="https://en.wikipedia.org/wiki?curid=2526" title="Agostino Carracci">
Agostino Carracci

Agostino Carracci (or Caracci) (16 August 1557 – 22 March 1602) was an Italian painter and printmaker. He was the brother of the more famous Annibale and cousin of Lodovico Carracci.
He posited the ideal in nature, and was the founder of the competing school to the more gritty view of nature as expressed by Caravaggio. He was one of the founders of the Accademia degli Incamminati along with his brother, Annibale Carracci, and cousin, Ludovico Carracci. The academy helped propel painters of the School of Bologna to prominence.
Life.
Agostino Carracci was born in Bologna, and trained at the workshop of the architect Domenico Tibaldi. Starting from 1574 he worked as a reproductive engraver, copying works of 16th century masters such as Federico Barocci, Tintoretto, Antonio Campi, Veronese and Correggio. He also produced some original prints, including two etchings.
He travelled to Venice (1582, 1587–1589) and Parma (1586–1587). Together with Annibale and Ludovico he worked in Bologna on the fresco cycles in Palazzo Fava ("Histories of Jason and Medea", 1584) and Palazzo Magnani ("Histories of Romulus", 1590–1592). In 1592 he also painted the "Communion of St. Jerome", now in the Pinacoteca di Bologna and considered his masterwork. From 1586 is his altarpiece of the "Madonna with Child and Saints", in the National Gallery of Parma. In 1598 Carracci joined his brother Annibale in Rome, to collaborate on the decoration of the Gallery in Palazzo Farnese. From 1598–1600 is a "triple Portrait", now in Naples, an example of genre painting. In 1600 he was called to Parma by Duke Ranuccio I Farnese to began the decoration of the Palazzo del Giardino, but he died before it was finished.
Agostino's son Antonio Carracci was also a painter, and attempted to compete with his father's Academy.
An engraving by Agostino Carraci after the painting "Love in the Golden Age" by the 16th-century Flemish painter Paolo Fiammingo was the inspiration for Matisse's "Le bonheur de vivre" (Joy of Life).

</doc>
<doc id="2528" url="https://en.wikipedia.org/wiki?curid=2528" title="Adenylyl cyclase">
Adenylyl cyclase

Adenylyl cyclase (, also commonly known as adenyl cyclase and adenylate cyclase, abbreviated AC) is an enzyme with key regulatory roles in essentially all cells. It is the most polyphyletic known enzyme: six distinct classes have been described, all catalyzing the same reaction but representing unrelated gene families with no known sequence or structural homology. The best known class of adenylyl cyclases is class III or AC-III (Roman numerals are used for classes). AC-III occurs widely in eukaryotes and has important roles in many human tissues.
All classes of adenylyl cyclases catalyse the conversion of adenosine triphosphate (ATP) to 3',5'-cyclic AMP (cAMP) and pyrophosphate. Magnesium ions are generally required and appears to be closely involved in the enzymatic mechanism. The cAMP produced by AC then serves as a regulatory signal via specific cAMP-binding proteins, either transcription factors, enzymes (e.g., cAMP-dependent kinases), or ion transporters.
Classes.
Class I.
The first class of adenylyl cyclases occur in many bacteria including "E. coli". This was the first class of AC to be characterized. It was observed that "E. coli" deprived of glucose produce cAMP that serves as an internal signal to activate expression of genes for importing and metabolizing other sugars. cAMP exerts this effect by binding the transcription factor CRP, also known as CAP. Class I AC's are large cytosolic enzymes (~100 kDa) with a large regulatory domain (~50 kDa) that indirectly senses glucose levels. , no crystal structure is available for class I AC.
Class II.
These adenylyl cyclases are toxins secreted by pathogenic bacteria such as "Bacillus anthracis" and "Bordetella pertussis" during infection. These bacteria also secrete proteins that enable the AC-II to enter host cells, where the exogenous AC activity undermines normal cellular processes. The genes for Class II AC's are known as cyaA. Several crystal structures are known for AC-II enzymes.
Class III.
These adenylyl cyclases are the most familiar based on extensive study due to their important roles in human health. They are also found in some bacteria, notably "Mycobacterium tuberculosis" where they appear to have a key role in pathogenesis. Most AC-III's are integral membrane proteins involved in transducing extracellular signals into intracellular responses. A Nobel Prize was awarded to Earl Sutherland in 1971 for discovering the key role of AC-III in human liver, where adrenaline indirectly stimulates AC to mobilize stored energy in the "fight or flight" response. The effect of adrenaline is via a G protein signaling cascade, which transmits chemical signals from outside the cell across the membrane to the inside of the cell (cytoplasm). The outside signal (in this case, adrenaline) binds to a receptor, which transmits a signal to the G protein, which transmits a signal to adenylyl cyclase, which transmits a signal by converting adenosine triphosphate to cyclic adenosine monophosphate (cAMP). cAMP is known as a second messenger.
Cyclic AMP is an important molecule in eukaryotic signal transduction, a so-called second messenger. Adenylyl cyclases are often activated or inhibited by G proteins, which are coupled to membrane receptors and thus can respond to hormonal or other stimuli. Following activation of adenylyl cyclase, the resulting cAMP acts as a second messenger by interacting with and regulating other proteins such as protein kinase A and cyclic nucleotide-gated ion channels.
Photoactivatable adenylyl cyclase (PAC) was discovered in "E. gracilis" and can be expressed in other organisms through genetic manipulation. Shining blue light on a cell containing PAC activates it and abruptly increases the rate of conversion of ATP to cAMP. This is a useful technique for researchers in neuroscience because it allows them to quickly increase the intracellular cAMP levels in particular neurons, and to study the effect of that increase in neural activity on the behavior of the organism. For example, PAC expression in certain neurons has been shown to alter the grooming behavior in fruit flies exposed to blue light. Channelrhodopsin-2 is also used in a similar fashion.
Structure.
Most class III adenylyl cyclases are transmembrane proteins with 12 transmembrane segments. The protein is organized with 6 transmembrane segments, then the C1 cytoplasmic domain, then another 6 membrane segments, and then a second cytoplasmic domain called C2. The important parts for function are the N-terminus and the C1 and C2 regions. The C1a and C2a subdomains are homologous and form an intramolecular 'dimer' that forms the active site. In "Mycobacterium tuberculosis", the AC-III polypeptide is only half as long, comprising one 6-transmembrane domain followed by a cytoplasmic domain, but two of these form a functional homodimer that resembles the mammalian architecture.
Types.
There are ten known isoforms of adenylyl cyclases in mammals:
These are also sometimes called simply AC1, AC2, etc., and, somewhat confusingly, sometimes Roman numerals are used for these isoforms that all belong to the overall AC class III. They differ mainly in how they are regulated, and are differentially expressed in various tissues throughout mammalian development.
Regulation.
Adenylyl cyclase is dually regulated by G proteins (Gs stimulating activity and Gi inhibiting it), and by forskolin, as well as other isoform-specific effectors:
In neurons, calcium-sensitive adenylyl cyclases are located next to calcium ion channels for faster reaction to Ca influx; they are suspected of playing an important role in learning processes. This is supported by the fact that adenylyl cyclases are "coincidence detectors", meaning that they are activated only by several different signals occurring together. In peripheral cells and tissues adenylyl cyclases appear to form molecular complexes with specific receptors and other signaling proteins in an isoform-specific manner.
Class IV.
AC-IV was first reported in the bacterium "Aeromonas hydrophila", and the structure of the AC-IV from "Yersinia pestis" has been reported. These are the smallest of the AC enzyme classes; the AC-IV from "Yersinia" is a dimer of 19 kDa subunits with no known regulatory components.
Classes V and VI.
These forms of AC have been reported in specific bacteria ("Prevotella ruminicola" and "Rhizobium etti", respectively) and have not been extensively characterized.

</doc>
<doc id="2529" url="https://en.wikipedia.org/wiki?curid=2529" title="Alexandra">
Alexandra

Alexandra (Greek: ) is the feminine form of the given name Alexander, which is a romanization of the Greek name ("Alexandros"). Etymologically, the name is a compound of the Greek verb ("alexein") "to defend" and ("anēr") "man" ( "andros"). Thus it may be roughly translated as "defender of man" or "protector of man". The name was one of the titles or epithets given to the Greek goddess Hera and as such is usually taken to mean "one who comes to save warriors". The earliest attested form of the name is the Mycenaean Greek , "a-re-ka-sa-da-ra", written in the Linear B syllabic script.

</doc>
<doc id="2536" url="https://en.wikipedia.org/wiki?curid=2536" title="Articolo 31">
Articolo 31

Articolo 31 was a band from Milan, Italy, melting hip hop, funk, pop and traditional Italian musical forms. They are one of the most popular Italian hip hop groups.
Band history.
Articolo 31 were formed by rapper J-Ax (real name Alessandro Aleotti) and DJ Jad (Vito Luca Perrini).
In the spoken intro of "Strade di Città", it is stated that the band is named after the article of the Irish constitution guaranteeing freedom of the press, although it must be noted that article 31 of the Irish constitution is not about the freedom of the press. They probably meant the Section 31 of the Broadcasting Authority Act.
Articolo 31 released one of the first Italian hip hop records, "Strade di città", in 1993. Soon, they signed with BMG Ricordi and started to mix rap with pop music - a move that earned them great commercial success but that alienated the underground hip hop scene, who perceived them as traitors. 
In 1997, DJ Gruff dissed Articolo 31 in a track titled "1 vs 2" on the first album of the beatmaker Fritz da Cat, starting a feud that would go on for years. 
In 2001, Articolo 31 collaborated with the American old school rapper Kurtis Blow on the album "XChé SI!". In the same year, they made the movie "Senza filtro" (in English, "Without filter"). Their producer was Franco Godi, who also produced the music for the "Signor Rossi" animated series.
Their 2002 album "Domani smetto" represented a further departure from hip hop, increasingly relying on the formula of rapping over pop music samples.
Following their 2003 album "Italiano medio", the band took a break. Both J Ax and DJ Jad have been involved with solo projects. In 2006, the group declared an indefinite hiatus.
Their posse, "Spaghetti Funk", includes other popular performers like Space One and pop rappers Gemelli DiVersi.

</doc>
<doc id="2543" url="https://en.wikipedia.org/wiki?curid=2543" title="Alexander Kerensky">
Alexander Kerensky

Alexander Fyodorovich Kerensky (, ; 2 May 1881 – 11 June 1970) was a Russian lawyer and politician who served as the second Minister-Chairman of the Russian Provisional Government between July and November 1917. A leader of the moderate-socialist Trudoviks faction of the Socialist Revolutionary Party, Kerensky was a key political figure in the Russian Revolution of 1917. On 7 November, his government was overthrown by the Vladimir Lenin-led Bolsheviks in the October Revolution. He spent the remainder of his life in exile, dying in New York City at the age of 89.
Early life and activism.
Alexander Kerensky was born in Simbirsk (now Ulyanovsk) on the Volga River on 2 May 1881. His father, Fyodor Kerensky, was a teacher and director of the local gymnasium. His mother, Nadezhda (née Adler), was born to a Russian German, Alexander Adler, who was head of the Topographical Bureau of the Kazan Military District, and Nadezhda Adlerova (née Kalmykova), a daughter of a former serf who had bought his freedom before serfdom was abolished in 1861, which enabled him to become a wealthy Moscow merchant. Later rumours emerged about Alexander Kerensky having Jewish roots, but these seem to be untrue.
Kerensky's father was the teacher of Vladimir Ulyanov (Lenin), and members of the Kerensky and Ulyanov families were friends. In 1889, when Kerensky was eight, the family moved to Tashkent, where his father had been appointed the main inspector of public schools (superintendent). Alexander graduated with honours in 1899. The same year he entered St. Petersburg University, where he studied history and philology. The next year he switched to law. He earned his law degree in 1904 and married Olga Lvovna Baranovskaya, the daughter of a Russian general, the same year. Kerensky joined the Narodnik movement and worked as a legal counsel to victims of Revolution of 1905. At the end of 1904, he was jailed on suspicion of belonging to a militant group. Afterwards he gained a reputation for his work as a defence lawyer in a number of political trials of revolutionaries.
In 1912, Kerensky became widely known when he visited the goldfields at the Lena River and published material about the Lena Minefields incident. In the same year, Kerensky was elected to the Fourth Duma as a member of the Trudoviks, a moderate, non-Marxist labour party that was associated with the Socialist-Revolutionary Party, and joined a freemason society uniting the anti-monarchy forces that strived for the democratic renewal of Russia. He was a brilliant orator and skilled parliamentary leader of the socialist opposition to the government of Tsar Nicholas II.
Rasputin.
In response to bitter resentments held against the imperial favourite Grigori Rasputin in the midst of Russia's failing effort in World War I, Kerensky, at the opening of the Duma on 2 November 1916, called the imperial ministers "hired assassins" and "cowards," and alleged that they were "guided by the contemptible Grishka Rasputin!" Grand Duke Nikolai Mikhailovich, Prince Lvov and general Mikhail Alekseyev attempted to persuade the emperor Nicholas II to send away the empress Alexandra Feodorovna, Rasputin's steadfast patron, either to the Livadia Palace in Yalta or to England. Mikhail Rodzianko, Zinaida Yusupova, Alexandra's sister Elisabeth, Grand Duchess Victoria and the empress's mother-in-law Maria Feodorovna also tried to influence the imperial couple to remove Rasputin from his position of influence within the imperial household, but without success. According to Kerensky, Rasputin had terrorised the empress by threatening to return to his native village.
Rasputin was murdered in December 1916 and buried in the imperial residence of Tsarskoye Selo. Shortly after the February Revolution of 1917, a group of soldiers were ordered by Kerensky to re-bury the corpse at an unmarked spot in the countryside. But the truck broke down or was forced to stop because of the snow on Lesnoe Road outside of St. Petersburg.
February Revolution of 1917.
When the February Revolution broke out in 1917, Kerensky was one of its most prominent leaders: he was a member of the Provisional Committee of the State Duma and was elected vice-chairman of the Petrograd Soviet. Simultaneously, he became the first Minister of Justice in the newly formed Russian Provisional Government. When the Soviet passed a resolution prohibiting its leaders from joining the government, Kerensky delivered a stirring speech at a Soviet meeting. Although the decision was never formalised, he was granted a "de facto" exemption and continued acting in both capacities.
After the first government crisis over Pavel Milyukov's secret note re-committing Russia to its original war aims on 2–4 May, Kerensky became the Minister of War and the dominant figure in the newly formed socialist-liberal coalition government. On 10 May (Julian calendar), Kerensky started for the front and visited one division after another, urging the men to do their duty. His speeches were impressive and convincing for the moment, but had little lasting effect. Under Allied pressure to continue the war, he launched what became known as the Kerensky Offensive against the Austro-Hungarian/German South Army on 17 June (Julian calendar). At first successful, the offensive was soon stopped and then thrown back by a strong counter-attack. The Russian army suffered heavy losses, and it was clear from the many incidents of desertion, sabotage, and mutiny that the army was no longer willing to attack.
Kerensky was heavily criticised by the military for his liberal policies, which included stripping officers of their mandates and handing over control to revolutionary inclined "soldier committees" instead; the abolition of the death penalty; and allowing revolutionary agitators to be present at the front. Many officers jokingly referred to commander-in-chief Kerensky as "persuader-in-chief."
On 2 July 1917, the first coalition collapsed over the question of Ukraine's autonomy. Following the July Days unrest in Petrograd and suppression of the Bolsheviks, Kerensky succeeded Prince Lvov as Russia's Prime Minister. Following the Kornilov Affair, an attempted military coup d'état at the end of August, and the resignation of the other ministers, he appointed himself Supreme Commander-in-Chief as well.
Kerensky's next move, on 15 September, was to proclaim Russia a republic, which was contrary to the non-socialists' understanding that the Provisional Government should hold power only until a Constituent Assembly should meet to decide Russia's form of government, but which was in line with the long proclaimed aim of the Socialist Revolutionary Party. He formed a five-member Directory, which consisted of himself, minister of foreign affairs Mikhail Tereshchenko, minister of war General Verkhovsky, minister of the navy Admiral Dmitry Verderevsky and minister of post and telegraph Nikitin. He retained his post in the final coalition government in October 1917 until it was overthrown by the Bolsheviks.
Kerensky's major challenge was that Russia was exhausted after three years of its participation in World War I, while the provisional government offered little motivation for a victory outside of continuing Russia's obligations towards its allies. Russia's continued involvement in the war was not popular among the lower and middle classes, and especially not popular among the soldiers. They had all believed that Russia would stop fighting when the Provisional Government took power, and now they felt deceived. Furthermore, Vladimir Lenin and his Bolshevik party were promising "peace, land, and bread" under a communist system. The army was disintegrating owing to a lack of discipline, leading to desertion in large numbers. By autumn 1917, an estimated two million men had unofficially left the army.
Kerensky and the other political leaders continued their obligation to Russia's allies by continuing involvement in World War I, fearing that the economy, already under huge stress from the war effort, might become increasingly unstable if vital supplies from France and the United Kingdom were cut off. The dilemma of whether to withdraw was a great one, and Kerensky's inconsistent and impractical policies further destabilised the army and the country at large.
Furthermore, Kerensky adopted a policy that isolated the right-wing conservatives, both democratic and monarchist-oriented. His philosophy of "no enemies to the left" greatly empowered the Bolsheviks and gave them a free hand, allowing them to take over the military arm or "voyenka" of the Petrograd and Moscow Soviets. His arrest of Lavr Kornilov and other officers left him without strong allies against the Bolsheviks, who ended up being Kerensky's strongest and most determined adversaries, as opposed to the right wing, which evolved into the White movement.
October Revolution of 1917.
During the Kornilov Affair, Kerensky had distributed arms to the Petrograd workers, and by November most of these armed workers had gone over to the Bolsheviks. On 1917, the Bolsheviks launched the second Russian revolution of the year. Kerensky's government in Petrograd had almost no support in the city. Only one small force, a subdivision of the 2nd company of the First Petrograd Women's Battalion, also known as The Women's Death Battalion, was willing to fight for the government against the Bolsheviks, but this force was overwhelmed by the numerically superior pro-Bolshevik forces and defeated and captured. It took less than 20 hours before the Bolsheviks had taken over the government.
Kerensky escaped the Bolsheviks and fled to Pskov, where he rallied some loyal troops for an attempt to re-take the city. His troops managed to capture Tsarskoe Selo, but were beaten the next day at Pulkovo. Kerensky narrowly escaped, and he spent the next few weeks in hiding before fleeing the country, eventually arriving in France. During the Russian Civil War he supported neither side, as he opposed both the Bolshevik regime and the White Movement.
Life in exile.
Kerensky lived in Paris until 1940, engaged in the endless splits and quarrels of the exiled Russian politicians. In 1939, Kerensky married the former Australian journalist Lydia "Nell" Tritton. When Germany invaded France in 1940, they emigrated to the United States.
When his wife became terminally ill in 1945, Kerensky travelled with her to Brisbane, Australia, and lived there with her family. She suffered a stroke in February 1946, and he remained there until her death on 10 April 1946. Kerensky returned to the United States, where he spent the rest of his life.
After the German-led invasion of the Soviet Union in 1941, Kerensky offered his support to Joseph Stalin.
Kerensky eventually settled in New York City, but spent much of his time at the Hoover Institution at Stanford University in California, where he both used and contributed to the Institution's huge archive on Russian history, and where he taught graduate courses. He wrote and broadcast extensively on Russian politics and history. His last public speech was delivered at Kalamazoo College in Kalamazoo, Michigan.
Kerensky died at his home in New York City in 1970, one of the last surviving major participants in the turbulent events of 1917. The local Russian Orthodox Churches in New York refused to grant Kerensky burial, because of his association with Freemasonry and because it saw him as largely responsible for Russia falling to the Bolsheviks. A Serbian Orthodox Church also refused burial. Kerensky's body was flown to London where he was buried at Putney Vale's non-denominational cemetery.
Personal life.
Kerensky was married to Olga Lvovna Baranovskaya and they had two sons, Oleg and Gleb, who both went on to become engineers. Kerensky and Olga were divorced in 1939 and soon after he married Lydia Ellen (Nell) Tritton (1899–1946).

</doc>
<doc id="2544" url="https://en.wikipedia.org/wiki?curid=2544" title="Ansgar">
Ansgar

Saint Ansgar (8 September 801 – 3 February 865), also known as Anskar or Saint Anschar, was a Germanic Archbishop of Hamburg-Bremen. The See of Hamburg was designated a mission to bring Christianity to Northern Europe, and Ansgar became known as the "Apostle of the North".
Life.
Ansgar was the son of a noble Frankish family, born near Amiens. After his mother's early death, Ansgar was brought up in Corbie Abbey, and was educated at the Benedectine monastery in Picardy. According to the "Vita Ansgarii" ("Life of Ansgar"), when the little boy learned in a vision that his mother was in the company of Saint Mary, his careless attitude toward spiritual matters changed to seriousness ("Life of Ansgar", 1). His pupil, successor, and eventual biographer Rimbert considered the visions of which this was the first to be the main motivation of the saint's life.
Ansgar was a product of the phase of Christianization of Saxony (present day Northern Germany) begun by Charlemagne and continued by his son and successor, Louis the Pious. A group of monks including Ansgar were sent back to Jutland with the baptized exiled king Harald Klak. Ansgar returned two years later after educating young boys who had been purchased because Harald had possibly been driven out of his kingdom. In 822 Ansgar was one of a number of missionaries sent to found the abbey of Corvey (New Corbie) in Westphalia, and there became a teacher and preacher. Then in 829 in response to a request from the Swedish king Björn at Hauge for a mission to the Swedes, Louis appointed Ansgar missionary. With an assistant, the friar Witmar, he preached and made converts for six months at Birka, on Lake Mälaren. They organized a small congregation there with the king's steward, Hergeir, and Mor Frideborg as its most prominent members. In 831 he returned to Louis' court at Worms and was appointed to the Archbishopric of Hamburg. This was a new archbishopric with a see formed from those of Bremen and Verden, plus the right to send missions into all the northern lands and to consecrate bishops for them. He was given the mission of evangelizing Denmark, Norway, and Sweden. The King of Sweden decided to cast lots as to whether the Christian missionaries should be admitted into his kingdom. Ansgar recommended the issue to the care of God, and the lot was favorable.
Ansgar was consecrated in November 831, and, the arrangements having been at once approved by Gregory IV, he went to Rome to receive the pallium directly from the hands of the pope and to be named legate for the northern lands. This commission had previously been bestowed upon Ebbo, Archbishop of Reims, but the jurisdiction was divided by agreement, with Ebbo retaining Sweden for himself. For a time Ansgar devoted himself to the needs of his own diocese, which was still missionary territory with but a few churches. He founded a monastery and a school in Hamburg; the school was intended to serve the Danish mission, but accomplished little.
After Louis died in 840, his empire was divided and Ansgar lost the abbey of Turholt, which had been given as an endowment for his work. Then in 845, the Danes unexpectedly raided Hamburg, destroying all the church's treasures and books and leaving the entire diocese unrestorable. Ansgar now had neither see nor revenue. Many of his helpers deserted him, but the new king, Louis the German, came to his aid; after failing to recover Turholt for him, in 847 he awarded him the vacant diocese of Bremen, where he took up residence in 848. However, since Hamburg had been an archbishopric, the sees of Bremen and Hamburg were combined for him. This presented canonical difficulties and also aroused the anger of the Bishop of Cologne, to whom Bremen had been suffragan, but after prolonged negotiations, Pope Nicholas I approved the union of the two dioceses in 864.
Through all this political turmoil, Ansgar continued his mission to the northern lands. The Danish civil war compelled him to establish good relations with two kings, Horik the Elder and his son, Horik II. Both assisted him until his death (Wood, 124–125). He was able to secure recognition of Christianity as a tolerated religion and permission to build a church in Sleswick. He did not forget the Swedish mission, and spent two years there in person (848–850), at the critical moment when a pagan reaction was threatened, which he succeeded in averting. In 854, Ansgar returned to Sweden when king Olof ruled in Birka. According to Rimbert, he was well disposed to Christianity. On a Viking raid to Apuole (current village in Lithuania) in Courland, the Swedes plundered the Curonians.
Ansgar wore a rough hair shirt, lived on bread and water, and showed great charity to the poor. Being the first missionary in Sweden and the organiser of the hierarchy in the Nordic countries, he was declared Patron of Scandinavia. Ansgar was buried in Bremen in 865.
His life story was written by his successor as archbishop, Rimbert, in the "Vita Ansgarii".
Visions.
Although a historical document and primary source written by a man whose existence can be proven historically, the "Vita Ansgarii" ("The Life of Ansgar") aims above all to demonstrate Ansgar's sanctity. It is partly concerned with Ansgar's visions, which, according to the author Rimbert, encouraged and assisted Ansgar's remarkable missionary feats.
Through the course of this work, Ansgar repeatedly embarks on a new stage in his career following a vision. According to Rimbert, his early studies and ensuing devotion to the ascetic life of a monk were inspired by a vision of his mother in the presence of Saint Mary. Again, when the Swedish people were left without a priest for some time, he begged King Horik to help him with this problem; then after receiving his consent, consulted with Bishop Gautbert to find a suitable man. The two together sought the approval of King Louis, which he granted when he learned that they were in agreement on the issue. Ansgar was convinced he was commanded by heaven to undertake this mission, and was influenced by a vision he received when he was concerned about the journey, in which he met a man who reassured him of his purpose and informed him of a prophet that he would meet, the abbot Adalhard, who would instruct him in what was to happen. In the vision, he searched for and found Adalhard, who commanded, "Islands, listen to me, pay attention, remotest peoples", which Ansgar interpreted as God's will that he go to the Scandinavian countries as "most of that country consisted of islands, and also, when 'I will make you the light of the nations so that my salvation may reach to the ends of the earth' was added, since the end of the world in the north was in Swedish territory".
Statues dedicated to him stand in Hamburg, Copenhagen, Ribe as well as a stone cross at Birka. A crater on the Moon, Ansgarius, has been named for him. His feast day is 3 February.
<br>

</doc>
<doc id="2546" url="https://en.wikipedia.org/wiki?curid=2546" title="Automated theorem proving">
Automated theorem proving

Automated theorem proving (also known as ATP or automated deduction) is a subfield of automated reasoning and mathematical logic dealing with proving mathematical theorems by computer programs. Automated reasoning over mathematical proof was a major impetus for the development of computer science.
Logical foundations.
While the roots of formalised logic go back to Aristotle, the end of the 19th and early 20th centuries saw the development of modern logic and formalised mathematics. Frege's "Begriffsschrift" (1879) introduced both a complete propositional calculus and what is essentially modern predicate logic. His "Foundations of Arithmetic", published 1884, expressed (parts of) mathematics in formal logic. This approach was continued by Russell and Whitehead in their influential "Principia Mathematica", first published 1910–1913, and with a revised second edition in 1927. Russell and Whitehead thought they could derive all mathematical truth using axioms and inference rules of formal logic, in principle opening up the process to automatisation. In 1920, Thoralf Skolem simplified a previous result by Leopold Löwenheim, leading to the Löwenheim–Skolem theorem and, in 1930, to the notion of a Herbrand universe and a Herbrand interpretation that allowed (un)satisfiability of first-order formulas (and hence the validity of a theorem) to be reduced to (potentially infinitely many) propositional satisfiability problems.
In 1929, Mojżesz Presburger showed that the theory of natural numbers with addition and equality (now called Presburger arithmetic in his honor) is decidable and gave an algorithm that could determine if a given sentence in the language was true or false.
However, shortly after this positive result, Kurt Gödel published "On Formally Undecidable Propositions of Principia Mathematica and Related Systems" (1931), showing that in any sufficiently strong axiomatic system there are true statements which cannot be proved in the system. This topic was further developed in the 1930s by Alonzo Church and Alan Turing, who on the one hand gave two independent but equivalent definitions of computability, and on the other gave concrete examples for undecidable questions.
First implementations.
Shortly after World War II, the first general purpose computers became available. In 1954, Martin Davis programmed Presburger's algorithm for a JOHNNIAC vacuum tube computer at the Princeton Institute for Advanced Study. According to Davis, "Its great triumph was to prove that the sum of two even numbers is even". More ambitious was the Logic Theory Machine, a deduction system for the propositional logic of the "Principia Mathematica", developed by Allen Newell, Herbert A. Simon and J. C. Shaw. Also running on a JOHNNIAC, the Logic Theory Machine constructed proofs from a small set of propositional axioms and three deduction rules: modus ponens, (propositional) variable substitution, and the replacement of formulas by their definition. The system used heuristic guidance, and managed to prove 38 of the first 52 theorems of the "Principia".
The "heuristic" approach of the Logic Theory Machine tried to emulate human mathematicians, and could not guarantee that a proof could be found for every valid theorem even in principle. In contrast, other, more systematic algorithms achieved, at least theoretically, completeness for first-order logic. Initial approaches relied on the results of Herbrand and Skolem to convert a first-order formula into successively larger sets of propositional formulae by instantiating variables with terms from the Herbrand universe. The propositional formulas could then be checked for unsatisfiability using a number of methods. Gilmore's program used conversion to disjunctive normal form, a form in which the satisfiability of a formula is obvious.
Decidability of the problem.
Depending on the underlying logic, the problem of deciding the validity of a formula varies from trivial to impossible. For the frequent case of propositional logic, the problem is decidable but Co-NP-complete, and hence only exponential-time algorithms are believed to exist for general proof tasks. For a first order predicate calculus, Gödel's completeness theorem states that the theorems (provable statements) are exactly the logically valid well-formed formulas, so identifying valid formulas is recursively enumerable: given unbounded resources, any valid formula can eventually be proven. However, "invalid" formulas (those that are "not" entailed by a given theory), cannot always be recognized.
The above applies to first order theories, such as Peano Arithmetic. However, for a specific model that may be described by a first order theory, some statements may be true but undecidable in the theory used to describe the model. For example, by Gödel's incompleteness theorem, we know that any theory whose proper axioms are true for the natural numbers cannot prove all first order statements true for the natural numbers, even if the list of proper axioms is allowed to be infinite enumerable. It follows that an automated theorem prover will fail to terminate while searching for a proof precisely when the statement being investigated is undecidable in the theory being used, even if it is true in the model of interest. Despite this theoretical limit, in practice, theorem provers can solve many hard problems, even in models that are not fully described by any first order theory (such as the integers).
Related problems.
A simpler, but related, problem is proof verification, where an existing proof for a theorem is certified valid. For this, it is generally required that each individual proof step can be verified by a primitive recursive function or program, and hence the problem is always decidable.
Since the proofs generated by automated theorem provers are typically very large, the problem of proof compression is crucial and various techniques aiming at making the prover's output smaller, and consequently more easily understandable and checkable, have been developed.
Proof assistants require a human user to give hints to the system. Depending on the degree of automation, the prover can essentially be reduced to a proof checker, with the user providing the proof in a formal way, or significant proof tasks can be performed automatically. Interactive provers are used for a variety of tasks, but even fully automatic systems have proved a number of interesting and hard theorems, including at least one that has eluded human mathematicians for a long time, namely the Robbins conjecture. However, these successes are sporadic, and work on hard problems usually requires a proficient user.
Another distinction is sometimes drawn between theorem proving and other techniques, where a process is considered to be theorem proving if it consists of a traditional proof, starting with axioms and producing new inference steps using rules of inference. Other techniques would include model checking, which, in the simplest case, involves brute-force enumeration of many possible states (although the actual implementation of model checkers requires much cleverness, and does not simply reduce to brute force).
There are hybrid theorem proving systems which use model checking as an inference rule. There are also programs which were written to prove a particular theorem, with a (usually informal) proof that if the program finishes with a certain result, then the theorem is true. A good example of this was the machine-aided proof of the four color theorem, which was very controversial as the first claimed mathematical proof which was essentially impossible to verify by humans due to the enormous size of the program's calculation (such proofs are called non-surveyable proofs). Another example would be the proof that the game Connect Four is a win for the first player.
Industrial uses.
Commercial use of automated theorem proving is mostly concentrated in integrated circuit design and verification. Since the Pentium FDIV bug, the complicated floating point units of modern microprocessors have been designed with extra scrutiny. AMD, Intel and others use automated theorem proving to verify that division and other operations are correctly implemented in their processors.
First-order theorem proving.
In the late 1960s agencies funding research in automated deduction began to emphasize the need for practical applications. One of the first fruitful areas was that of program verification whereby first-order theorem provers were applied to the problem of verifying the correctness of computer programs in languages such as Pascal, Ada, Java etc. Notable among early program verification systems was the Stanford Pascal Verifier developed by David Luckham at Stanford University. This was based on the Stanford Resolution Prover also developed at Stanford using J.A. Robinson's resolution Principle. This was the first automated deduction system to demonstrate an ability to solve mathematical problems that were announced in the Notices of the American Mathematical Society before solutions were formally published.
First-order theorem proving is one of the most mature subfields of automated theorem proving. The logic is expressive enough to allow the specification of arbitrary problems, often in a reasonably natural and intuitive way. On the other hand, it is still semi-decidable, and a number of sound and complete calculi have been developed, enabling "fully" automated systems. More expressive logics, such as higher order logics, allow the convenient expression of a wider range of problems than first order logic, but theorem proving for these logics is less well developed.
Benchmarks and competitions.
The quality of implemented systems has benefited from the existence of a large library of standard benchmark examples — the Thousands of Problems for Theorem Provers (TPTP) Problem Library — as well as from the CADE ATP System Competition (CASC), a yearly competition of first-order systems for many important classes of first-order problems.
Some important systems (all have won at least one CASC competition division) are listed below.

</doc>
<doc id="2547" url="https://en.wikipedia.org/wiki?curid=2547" title="Agent Orange">
Agent Orange

Agent Orange—or Herbicide Orange (HO)—is one of the herbicides and defoliants used by the U.S. military as part of its herbicidal warfare program, Operation Ranch Hand, during the Vietnam War from 1961 to 1971. It was a mixture of equal parts of two herbicides, 2,4,5-T and 2,4-D.
During the late 1940s and 1950s, the US and Britain collaborated on development of herbicides with potential applications in warfare. Some of those products were brought to market as herbicides. The British were the first to employ herbicides and defoliants to destroy the crops, bushes, and trees of communist insurgents in Malaya during the Malayan Emergency. These operations laid the groundwork for the subsequent use of Agent Orange and other defoliant formulations by the US.
In mid-1961, President Ngo Dinh Diem of South Vietnam asked the United States to conduct aerial herbicide spraying in his country. In August of that year, the South Vietnamese Air Force initiated herbicide operations with American help. But Diem's request launched a policy debate in the White House and the State and Defense Departments. However, U.S. officials considered using it, pointing out that the British had already used herbicides and defoliants during the Malayan Emergency in the 1950s. In November 1961, President John F. Kennedy authorized the start of Operation Ranch Hand, the codename for the U.S. Air Force's herbicide program in Vietnam.
Agent Orange was manufactured for the U.S. Department of Defense primarily by Monsanto Corporation and Dow Chemical. It was given its name from the color of the orange-striped barrels in which it was shipped, and was by far the most widely used of the so-called "Rainbow Herbicides". The 2,4,5-T used to produce Agent Orange was contaminated with 2,3,7,8-Tetrachlorodibenzodioxin (TCDD), an extremely toxic dioxin compound. In some areas, TCDD concentrations in soil and water were hundreds of times greater than the levels considered safe by the U.S. Environmental Protection Agency.
In the absence of specific customary or positive international humanitarian law regarding herbicidal warfare, a draft convention, prepared by a Working Group set up within the Conference of the Committee on Disarmament (CCD), was submitted to the UN General Assembly in 1976. In that same year, the First Committee of the General Assembly decided to send the text of the draft convention to the General Assembly, which adopted Resolution 31/72 on December 10, 1976, with the text of the Convention attached as an annex thereto. The convention, namely the Environmental Modification Convention, was opened for signature and ratification on May 18, 1977, and entered into force in October 5, 1978. The convention prohibits the military or other hostile use of environmental modification techniques having widespread, long-lasting or severe effects. Many states do not regard this as a complete ban on the use of herbicides and defoliants in warfare but it does require case-by-case consideration.
Although in the Geneva Disarmament Convention of 1978, Article 2(4) Protocol III to the weaponry convention has "The Jungle Exception", which prohibits states from attacking forests or jungles "except if such natural elements are used to cover, conceal or camouflage combatants or military objectives or are military objectives themselves" this voids any protection of any military or civilians from a napalm attack or something like agent Orange and is clear that it was designed to cater to situations like U.S. tactics in Vietnam. This clause has yet to be revised.
Chemical description and toxicology.
Chemically, Agent Orange is an approximately 1:1 mixture of two phenoxyl herbicides – 2,4-dichlorophenoxyacetic acid (2,4-D) and 2,4,5-trichlorophenoxyacetic acid (2,4,5-T) – in iso-octyl ester form.
Numerous studies have examined health effects linked to Agent Orange, its component compounds, and its manufacturing byproducts.
A 1969 report authored by K. Diane Courtney and others found 2,4,5-T could cause birth defects and stillbirths in mice. Several studies have shown an increased rate of cancer mortality for workers exposed to 2,4,5-T. In one such study, from Hamburg, Germany, the risk of cancer mortality increased by 170% after working for 10 years at the 2,4,5-T-producing section of a Hamburg manufacturing plant. Three studies have suggested prior exposure to Agent Orange poses an increased risk of acute myelogenous leukemia in the children of Vietnam veterans.
In 1969 it was also revealed to the public that the 2,4,5-T was contaminated with a dioxin, 2,3,7,8-tetrachlorodibenzodioxin (TCDD), and that the TCDD was causing many of the previously unexplained adverse health effects which were correlated with Agent Orange exposure. TCDD has been described as "perhaps the most toxic molecule ever synthesized by man".(Galston 1979, cited in)
It has often been claimed that the contamination with dioxin was discovered only "later". However, prior to Operation Ranch Hand (1962-1971), health-risks had become apparent, from several accidents in 2,4,5-T-production in the U.S. and in Europe. The causes had been investigated, and results published in 1957, specifically stating "tetrachlordibenzodioxine proved very active". Additionally "Boehringer, which used the relatively safer low-temperature-process since 1957, in the same year warned the other producers of 2,4,5-TCP, which were using the high-temperature-process, pointing out the risk and providing suggestions how to avoid them."
Internal memoranda revealed that Monsanto (a major manufacturer of 2,4,5-T) had informed the U.S. government in 1952 that its 2,4,5-T was contaminated. In the manufacture of 2,4,5-T, accidental overheating of the reaction mixture easily causes the product to condense into the toxic self-condensation product TCDD. At the time, precautions were not taken against this unintended side reaction, which also caused the Seveso disaster in Italy in 1976.
The employment of 2,4,5-T by the military rapidly ended, according to the American Cancer Society, following the convincing results of a study in 1970 that found 2,4,5-T could cause birth defects in lab animals.
In 1979, Yale biologist Arthur Galston, who specialized in herbicide research, published a review of what was known at the time about the toxicity of TCDD. Even "vanishingly small" quantities of dioxin in the diet caused adverse health effects when tested on animals. Since then, TCDD has been comprehensively studied. It has been associated with increased neoplasms in every animal bioassay reported in the scientific literature. The National Toxicology Program has classified TCDD as "known to be a human carcinogen", frequently associated with soft-tissue sarcoma, non-Hodgkin's lymphoma, Hodgkin's lymphoma and chronic lymphocytic leukemia (CLL).
Starting in 1991, Congress asked the Institute of Medicine to review the scientific literature on Agent Orange and the other herbicides used in Vietnam, including their active ingredients and the dioxin contaminant. The IOM found an association between dioxin exposure and diabetes.
Of the two herbicides that make up Agent Orange, 2,4-D and 2,4,5-T, the latter is considered to be less biodegradable.
While degradation of 2,4,5-T with a half-life on a scale of days can be achieved by adding bacteria of a special strain, "no substantial degradation" was observed in the same soil without addition of bacteria. The half-life of dioxins in soil is more than 10 years, and that of TCDD in human fat tissue is about 7 years.
Discovery of herbicides and defoliants and first use in war.
Several herbicides were discovered as part of efforts by the US and the British to develop herbicidal weapons for use during WWII. These included 2,4-D (2,4-dichlorophenoxyacetic acid), 2,4,5-T (coded LN-14, and also known as trioxone), MCPA (2-methyl-4-chlorophenoxyacetic acid, 1414B and 1414A, recoded LN-8 and LN-32), and isopropyl phenylcarbamate (1313, recoded LN-33).
In 1943, the U.S. Department of the Army contracted the University of Chicago to study the effects of 2,4-D and 2,4,5-T on cereal grains (including rice) and broadleaf crops. From these studies arose the concept of using aerial applications of herbicides to destroy enemy crops to disrupt their food supply. In early 1945, the U.S. Army ran tests of various 2,4-D and 2,4,5-T mixtures at the Bushnell Army Airfield in Florida, which is now listed as a Formerly Used Defense Site (FUDS). As a result, the U.S. began a full-scale production of 2,4-D and 2,4,5-T and would have used it against Japan in 1946 during Operation Downfall if the war had continued.
By the end of the war, the relationship between the two countries was well established. In the years after the war, the U.S. tested 1100 compounds and field trials of the more promising ones were done at British stations in India and Australia, to establish their effects in tropical conditions, as well as at the U.S.'s testing ground in Florida.
Between 1950 and 1952, for example, trials were conducted in Tanganyika, at Kikore and Stunyansa, to test arboricides and defoliants under tropical conditions. The chemicals involved were 2,4-D, 2,4,5-T, and endothall (3,6-endoxohexahydrophthalic acid). During 1952/53, the unit supervised the aerial spraying of 2,4,5-T over the Waturi peninsula in Kenya to assess the value of defoliants in the eradication of tsetse fly.
During the Malayan Emergency, Britain was the first nation to employ the use of herbicides and defoliants to destroy bushes, trees, and vegetation to deprive insurgents of cover and targeting food crops as part of a starvation campaign in the early 1950s. A detailed account of how the British experimented with the spraying of herbicides was written by two scientists, E.K. Woodford of Agricultural Research Council's Unit of Experimental Agronomy and H.G.H. Kearns of the University of Bristol.
After the Malayan conflict ended in 1960, the U.S. considered the British precedent in deciding that the use of defoliants was a legal tactic of warfare. Secretary of State Dean Rusk advised President John F. Kennedy that the British had established a precedent for warfare with herbicides in Malaya.
Use in the Vietnam War.
During the Vietnam War, between 1962 and 1971, the United States military sprayed nearly of chemical herbicides and defoliants in Vietnam, eastern Laos, and parts of Cambodia as part of the aerial defoliation program known as Operation Ranch Hand, reaching its peak from 1967 to 1969. Like the British did in Malaya, the goal was to defoliate rural/forested land, depriving guerrillas of food and cover and clearing sensitive areas such as around base perimeters. The program was also a part of a general policy of forced draft urbanization, which aimed to destroy the ability of peasants to support themselves in the countryside, forcing them to flee to the U.S. dominated cities, depriving the guerrillas of their rural support base.
Spraying was usually done either from helicopters or from low-flying C-123 Provider aircraft, fitted with sprayers and "MC-1 Hourglass" pump systems and chemical tanks. Spray runs were also conducted from trucks, boats, and backpack sprayers.
The first batch of herbicides was unloaded at Tan Son Nhut Air Base in South Vietnam, on January 9, 1962. U.S. Air Force records show at least 6,542 spraying missions took place over the course of Operation Ranch Hand. By 1971, 12 percent of the total area of South Vietnam had been sprayed with defoliating chemicals, at an average concentration of 13 times the recommended U.S. Department of Agriculture application rate for domestic use. In South Vietnam alone, an estimated 10 million hectares of agricultural land was ultimately destroyed. In some areas, TCDD concentrations in soil and water were hundreds of times greater than the levels considered safe by the U.S. Environmental Protection Agency.
The campaign destroyed of upland and mangrove forests and millions of acres of crops. Overall, more than 20% of South Vietnam's forests were sprayed at least once over a nine-year period.
In 1965, members of the U.S. Congress were told "crop destruction is understood to be the more important purpose ... but the emphasis is usually given to the jungle defoliation in public mention of the program." Military personnel were told they were destroying crops because they were going to be used to feed guerrillas. They later discovered nearly all of the food they had been destroying was not being produced for guerrillas; it was, in reality, only being grown to support the local civilian population. For example, in Quang Ngai province, 85% of the crop lands were scheduled to be destroyed in 1970 alone. This contributed to widespread famine, leaving hundreds of thousands of people malnourished or starving.
The U.S. military began targeting food crops in October 1962, primarily using Agent Blue; the American public was not made aware of the crop destruction programs until 1965 (and it was then believed that crop spraying had begun that spring). In 1965, 42 percent of all herbicide spraying was dedicated to food crops. The first official acknowledgement of the programs came from the State Department in March 1966.
Many experts at the time, including Arthur Galston, the biologist who developed and intensively studied 2,4,5-T and TCDD, opposed herbicidal warfare, due to concerns about the side effects to humans and the environment by indiscriminately spraying the chemical over a wide area. As early as 1966, resolutions were introduced to the United Nations charging that the U.S. was violating the 1925 Geneva Protocol, which regulated the use of chemical and biological weapons. The U.S. defeated most of the resolutions, arguing that Agent Orange was not a chemical or a biological weapon as it was considered a herbicide and a defoliant and it was used in effort to destroy plant crops to deprive the enemy of cover and not meant to target human beings. A weapon, by definition, is any device used to injure, defeat, or destroy living beings, structures, or systems, and Agent Orange did not qualify under that definition. It also argued that if the U.S. were to be charged for using Agent Orange, then Britain and its Commonwealth nations should be charged since they also used it widely during the Malayan Emergency in the 1950s. In 1969, during a debate in the First Committee of the UN General Assembly on the question of chemical and bacteriological (biological) weapons, Britain stated with respect to the then still draft Resolution 2603 (XXIV): “The evidence seems to us to be notably inadequate for the assertion that the use in war of chemical substances specifically toxic to plants is prohibited by international law.”
Effects on the Vietnamese people.
Health effects.
The government of Vietnam says that 4 million of its citizens were exposed to Agent Orange, and as many as 3 million have suffered illnesses because of it; these figures include the children of people who were exposed. The Red Cross of Vietnam estimates that up to 1 million people are disabled or have health problems due to contaminated Agent Orange. The United States government has challenged these figures as being unreliable.
According to a study by a sole Vietnamese scientist, Dr Nguyen Viet Nhan, children in the areas where Agent Orange was used have been affected and have multiple health problems, including cleft palate, mental disabilities, hernias, and extra fingers and toes. In the 1970s, high levels of dioxin were found in the breast milk of South Vietnamese women, and in the blood of U.S. military personnel who had served in Vietnam. The most affected zones are the mountainous area along Truong Son (Long Mountains) and the border between Vietnam and Cambodia. The affected residents are living in substandard conditions with many genetic diseases.
The scientific data supporting a causal link between Agent Orange/dioxin exposure and birth defects is controversial and weak, in part due to poor methodology. In 2006 Anh Duc Ngo and colleagues, of the University of Texas Health Science Center in Austin, published a meta-analysis that exposed a large amount of heterogeneity/(different findings) between studies, a finding consistent with a lack of consensus on the issue. Despite this, statistical analysis of the studies they examined resulted in data that the increase in birth defects/relative risk(RR) from exposure to agent orange/dioxin "appears" to be on the order of 3 in Vietnamese funded studies but 1.29 in the rest of the world. With a casual relationship near the threshold of statistical significance in still-births, cleft palate, and neural tube defects, with spina bifida being the most statistically significant defect. The large discrepancy in RR between Vietnamese studies and those in the rest of the world have been suggested to be due to bias in the Vietnamese studies.
About 28 of the former U.S. military bases in Vietnam where the herbicides were stored and loaded onto airplanes may still have high level of dioxins in the soil, posing a health threat to the surrounding communities. Extensive testing for dioxin contamination has been conducted at the former U.S. airbases in Da Nang, Phu Cat and Bien Hoa. Some of the soil and sediment on the bases have extremely high levels of dioxin requiring remediation. The Da Nang Airbase has dioxin contamination up to 350 times higher than international recommendations for action. The contaminated soil and sediment continue to affect the citizens of Vietnam, poisoning their food chain and causing illnesses, serious skin diseases and a variety of cancers in the lungs, larynx, and prostate.
Ecological effects.
About 17.8 percent——of the total forested area of Vietnam was sprayed during the war, which disrupted the ecological equilibrium. The persistent nature of dioxins, erosion caused by loss of tree cover and loss of seedling forest stock meant that reforestation was difficult (or impossible) in many areas. Many defoliated forest areas were quickly invaded by aggressive pioneer species (such as bamboo and cogon grass), making forest regeneration difficult and unlikely. Animal-species diversity was also impacted; in one study a Harvard biologist found 24 species of birds and five species of mammals in a sprayed forest, while in two adjacent sections of unsprayed forest there were 145 and 170 species of birds and 30 and 55 species of mammals.
Dioxins from Agent Orange have persisted in the Vietnamese environment since the war, settling in the soil and sediment and entering the food chain through animals and fish which feed in the contaminated areas. The movement of dioxins through the food web has resulted in bioconcentration and biomagnification. The areas most heavily contaminated with dioxins are former U.S. air bases.
Sociopolitical effects.
The RAND Corporation's "Memorandum 5446-ISA/ARPA" states: "the fact that the VC obtain most of their food from the neutral rural population dictates the destruction of civilian crops ... if they (the VC) are to be hampered by the crop destruction program, it will be necessary to destroy large portions of the rural economy – probably 50% or more".
Rural-to-urban migration rates dramatically increased in South Vietnam, as peasants escaped the war in the countryside by fleeing to the cities. The urban population in South Vietnam nearly tripled, growing from 2.8 million people in 1958 to 8 million by 1971. The rapid flow of people led to a fast-paced and uncontrolled urbanization; an estimated 1.5 million people were living in Saigon slums.
Effects on U.S. veterans.
Studies have shown that veterans have increased rates of cancer, and nerve, digestive, skin, and respiratory disorders, in particular, higher rates of acute/chronic leukemia, Hodgkin's lymphoma and non-Hodgkin's lymphoma, throat cancer, prostate cancer, lung cancer, colon cancer, Ischemic heart disease, soft tissue sarcoma and liver cancer. With the exception of liver cancer, these are the same conditions the U.S. Veterans Administration has determined may be associated with exposure to Agent Orange/dioxin, and are on the list of conditions eligible for compensation and treatment.
Military personnel who loaded airplanes and helicopters used in Ranch Hand probably sustained some of the heaviest exposures. Members of the Army Chemical Corps, who stored and mixed herbicides and defoliated the perimeters of military bases, and mechanics who worked on the helicopters and planes, are also thought to have had some of the heaviest exposures. However, this same group of individuals has not shown remarkably higher incidences of the associated diseases, leading to disagreement within certain circles of just how much effect the defoliants actually have on the health of those exposed. Others with potentially heavy exposures included members of U.S. Army Special Forces units who defoliated remote campsites, and members of U.S. Navy river units who cleared base perimeters. Military members who served on Okinawa also claim to have been exposed to the chemical but there is no verifiable evidence to corroborate these claims. 
More recent research established that veterans exposed to Agent Orange suffer more than twice the rate of highly aggressive prostate cancers Additionally, recent reports from the Institute of Medicine of the National Academy of Sciences show that Agent Orange exposure also doubles the risk of invasive skin cancers.
While in Vietnam, the veterans were told not to worry, and were persuaded the chemical was harmless. After returning home, Vietnam veterans began to suspect their ill health or the instances of their wives having miscarriages or children born with birth defects might be related to Agent Orange and the other toxic herbicides to which they were exposed in Vietnam. Veterans began to file claims in 1977 to the Department of Veterans Affairs for disability payments for health care for conditions they believed were associated with exposure to Agent Orange, or more specifically, dioxin, but their claims were denied unless they could prove the condition began when they were in the service or within one year of their discharge.
By April 1993, the Department of Veterans Affairs had compensated only 486 victims, although it had received disability claims from 39,419 soldiers who had been exposed to Agent Orange while serving in Vietnam.
Legal and diplomatic proceedings.
U.S. veterans class action lawsuit against manufacturers.
Since at least 1978, several lawsuits have been filed against the companies which produced Agent Orange, among them Dow Chemical, Monsanto, and Diamond Shamrock.
Hy Mayerson of The Mayerson Law Offices, P.C. was an early pioneer in Agent Orange litigation, working with environmental attorney Victor Yannacone in 1980 on the first class-action suits against wartime manufacturers of Agent Orange. In meeting Dr. Ronald A. Codario, one of the first civilian doctors to see afflicted patients, Mayerson, so impressed by the fact a physician would show so much interest in a Vietnam veteran, forwarded more than a thousand pages of information on Agent Orange and the effects of dioxin on animals and humans to Codario's office the day after he was first contacted by the doctor. The corporate defendants sought to escape culpability by blaming everything on the U.S. government.
The Mayerson law firm, with Sgt. Charles E. Hartz as their principal client, filed the first U.S. Agent Orange class-action lawsuit, in Pennsylvania in 1980, for the injuries military personnel in Vietnam suffered through exposure to toxic dioxins in the defoliant. Attorney Hy Mayerson co-wrote the brief that certified the Agent Orange Product Liability action as a class action, the largest ever filed as of its filing. Hartz's deposition was one of the first ever taken in America, and the first for an Agent Orange trial, for the purpose of preserving testimony at trial, as it was understood that Hartz would not live to see the trial because of a brain tumor that began to develop while he was a member of Tiger Force, Special Forces, and LRRPs in Vietnam. The firm also located and supplied critical research to the Veterans' lead expert, Dr. Ronald A. Codario, M.D., including about 100 articles from toxicology journals dating back more than a decade, as well as data about where herbicides had been sprayed, what the effects of dioxin had been on animals and humans, and every accident in factories where herbicides were produced or dioxin was a contaminant of some chemical reaction.
The chemical companies involved denied that there was a link between Agent Orange and the veterans' medical problems. However, on May 7, 1984, seven chemical companies settled the class-action suit out of court just hours before jury selection was to begin. The companies agreed to pay $180 million as compensation if the veterans dropped all claims against them. Slightly over 45% of the sum was ordered to be paid by Monsanto alone. Many veterans who were victims of Agent Orange exposure were outraged the case had been settled instead of going to court, and felt they had been betrayed by the lawyers. "Fairness Hearings" were held in five major American cities, where veterans and their families discussed their reactions to the settlement, and condemned the actions of the lawyers and courts, demanding the case be heard before a jury of their peers. Federal Judge Julius Weinstein refused the appeals, claiming the settlement was "fair and just". By 1989, the veterans' fears were confirmed when it was decided how the money from the settlement would be paid out. A totally disabled Vietnam veteran would receive a maximum of $12,000 spread out over the course of 10 years. Furthermore, by accepting the settlement payments, disabled veterans would become ineligible for many state benefits that provided far more monetary support than the settlement, such as food stamps, public assistance, and government pensions. A widow of a Vietnam veteran who died of Agent Orange exposure would only receive $3700.
In 2004, Monsanto spokesman Jill Montgomery said Monsanto should not be liable at all for injuries or deaths caused by Agent Orange, saying: "We are sympathetic with people who believe they have been injured and understand their concern to find the cause, but reliable scientific evidence indicates that Agent Orange is not the cause of serious long-term health effects."
New Jersey Agent Orange Commission.
In 1980, New Jersey created the New Jersey Agent Orange Commission, the first state commission created to study its effects. The commission's research project in association with Rutgers University was called "The Pointman Project". It was disbanded by Governor Christine Todd Whitman in 1996.
During Pointman I, commission researchers devised ways to determine small dioxin levels in blood. Prior to this, such levels could only be found in the adipose (fat) tissue. The project studied dioxin (TCDD) levels in blood as well as in adipose tissue in a small group of Vietnam veterans who had been exposed to Agent Orange and compared them to those of a matched control group; the levels were found to be higher in the former group.
The second phase of the project continued to examine and compare dioxin levels in various groups of Vietnam veterans, including Army, Marines and brown water riverboat Navy personnel.
U.S. Congress.
In 1991, Congress enacted the Agent Orange Act, giving the Department of Veterans Affairs the authority to declare certain conditions 'presumptive' to exposure to Agent Orange/dioxin, making these veterans who served in Vietnam eligible to receive treatment and compensation for these conditions. The same law required the National Academy of Sciences to periodically review the science on dioxin and herbicides used in Vietnam to inform the Secretary of Veterans Affairs about the strength of the scientific evidence showing association between exposure to Agent Orange/dioxin and certain conditions. The authority for the National Academy of Sciences reviews and addition of any new diseases to the presumptive list by the VA is expiring in 2015 under the sunset clause of the Agent Orange Act of 1991. Through this process, the list of 'presumptive' conditions has grown since 1991, and currently the U.S. Department of Veterans Affairs has listed prostate cancer, respiratory cancers, multiple myeloma, type II diabetes mellitus, Hodgkin's disease, non-Hodgkin's lymphoma, soft tissue sarcoma, chloracne, porphyria cutanea tarda, peripheral neuropathy, chronic lymphocytic leukemia, and spina bifida in children of veterans exposed to Agent Orange as conditions associated with exposure to the herbicide. This list now includes B cell leukemias, such as hairy cell leukemia, Parkinson's disease and ischemic heart disease, these last three having been added on August 31, 2010. Several highly placed individuals in government are voicing concerns about whether some of the diseases on the list should, in fact, actually have been included.
In 2011 an appraisal of the 20 year long "Air Force Health Study" that began in 1982 indicates that the results of the AFHS as they pretain to Agent Orange, do not provide evidence of disease in the Ranch Hand veterans due to "their elevated levels of exposure to Agent Orange".
The VA denied the applications of post-Vietnam C-123 aircrew veterans because as veterans without "boots on the ground" service in Vietnam, they were not covered under VA's interpretation of "exposed.". At the request of the VA, the Institute Of Medicine evaluated whether or not service in these C-123 aircrafts could have plausibly exposed soldiers and been detrimental to their health. Their report "Post-Vietnam Dioxin Exposure in Agent Orange-Contaminated C-123 Aircraft" confirmed it. In June 2015 the Secretary of Veterans Affairs issued an Interim final rule providing presumptive service connection for post-Vietnam C-123 aircrews, maintenance staff and aeromedical evacuation crews. VA now provides medical care and disability compensation for the recognized list of Agent Orange illnesses.,
U.S.–Vietnamese government negotiations.
In 2002, Vietnam and the U.S. held a joint conference on Human Health and Environmental Impacts of Agent Orange. Following the conference, the U.S. National Institute of Environmental Health Sciences (NIEHS) began scientific exchanges between the U.S. and Vietnam, and began discussions for a joint research project on the human health impacts of Agent Orange.
These negotiations broke down in 2005, when neither side could agree on the research protocol and the research project was cancelled. More progress has been made on the environmental front. In 2005, the first U.S.-Vietnam workshop on remediation of dioxin was held.
Starting in 2005, the U.S. Environmental Protection Agency (EPA) began to work with the Vietnamese government to measure the level of dioxin at the Da Nang Airbase. Also in 2005, the Joint Advisory Committee on Agent Orange, made up of representatives of Vietnamese and U.S. government agencies, was established. The committee has been meeting yearly to explore areas of scientific cooperation, technical assistance and environmental remediation of dioxin.
A breakthrough in the diplomatic stalemate on this issue occurred as a result of United States President George W. Bush's state visit to Vietnam in November 2006. In the joint statement, President Bush and President Triet agreed "further joint efforts to address the environmental contamination near former dioxin storage sites would make a valuable contribution to the continued development of their bilateral relationship."
On May 25, 2007, President Bush signed the U.S. Troop Readiness, Veterans' Care, Katrina Recovery, and Iraq Accountability Appropriations Act, 2007 into law for the wars in Iraq and Afghanistan that included an earmark of $3 million specifically for funding for programs for the remediation of dioxin 'hotspots' on former U.S. military bases, and for public health programs for the surrounding communities; some authors consider this to be completely inadequate, pointing out that the U.S. airbase in Da Nang, alone, will cost $14 million to clean up, and that three others are estimated to require $60 million for cleanup. The appropriation was renewed in the fiscal year 2009 and again in FY 2010. An additional $12 million was appropriated in the fiscal year 2010 in the Supplemental Appropriations Act and a total of $18.5 million appropriated for fiscal year 2011.
Secretary of State Hillary Clinton stated during a visit to Hanoi in October 2010 that the U.S. government would begin work on the clean-up of dioxin contamination at the Da Nang airbase.
In June 2011, a ceremony was held at Da Nang airport to mark the start of U.S.-funded decontamination of dioxin hotspots in Vietnam. $32 million has so far been allocated by the U.S. Congress to fund the program.
A $43 million project began in the summer of 2012, as Vietnam and the U.S. forge closer ties to boost trade and counter China's rising influence in the disputed South China Sea.
Vietnamese victims class action lawsuit in U.S. courts.
On January 31, 2004, a victim's rights group, the Vietnam Association for Victims of Agent Orange/dioxin (VAVA), filed a lawsuit in the United States District Court for the Eastern District of New York in Brooklyn, against several U.S. companies for liability in causing personal injury, by developing, and producing the chemical, and claimed that the use of Agent Orange violated the 1907 Hague Convention on Land Warfare, 1925 Geneva Protocol, and the 1949 Geneva Conventions. Dow Chemical and Monsanto were the two largest producers of Agent Orange for the U.S. military, and were named in the suit, along with the dozens of other companies (Diamond Shamrock, Uniroyal, Thompson Chemicals, Hercules, etc.). On March 10, 2005, Judge Jack B. Weinstein of the Eastern District – who had presided over the 1984 U.S. veterans class-action lawsuit – dismissed the lawsuit, ruling there was no legal basis for the plaintiffs' claims. He concluded Agent Orange was not considered a poison under international law at the time of its use by the U.S.; the U.S. was not prohibited from using it as a herbicide; and the companies which produced the substance were not liable for the method of its use by the government. Weinstein used the British example to help dismiss the claims of people exposed to Agent Orange in their suit against the chemical companies that had supplied it.
George Jackson stated that "if the Americans were guilty of war crimes for using Agent Orange in Vietnam, then the British would be also guilty of war crimes as well since they were the first nation to deploy the use of herbicides and defoliants in warfare and used them on a large scale throughout the Malayan Emergency. Not only was there no outcry by other states in response to Britain's use, but the U.S. viewed it as establishing a precedent for the use of herbicides and defoliants in jungle warfare." The U.S. government was also not a party in the lawsuit, due to sovereign immunity, and the court ruled the chemical companies, as contractors of the U.S. government, shared the same immunity.
The case was appealed and heard by the Second Circuit Court of Appeals in Manhattan on June 18, 2007. Three judges on the Second Circuit Court of Appeals upheld Weinstein's ruling to dismiss the case. They ruled that, though the herbicides contained a dioxin (a known poison), they were not intended to be used as a poison on humans. Therefore, they were not considered a chemical weapon and thus not a violation of international law. A further review of the case by the whole panel of judges of the Court of Appeals also confirmed this decision. The lawyers for the Vietnamese filed a petition to the U.S. Supreme Court to hear the case. On March 2, 2009, the Supreme Court denied certiorari and refused to reconsider the ruling of the Court of Appeals.
In a November 2004 Zogby International poll of 987 people, 79% of respondents thought the U.S. chemical companies which produced Agent Orange defoliant should compensate U.S. soldiers who were affected by the toxic chemical used during the war in Vietnam. Also, 51% said they supported compensation for Vietnamese Agent Orange victims.
Help for those affected in Vietnam.
To assist those who have been affected by Agent Orange/dioxin, the Vietnamese have established "peace villages", which each host between 50 and 100 victims, giving them medical and psychological help. As of 2006, there were 11 such villages, thus granting some social protection to fewer than a thousand victims. U.S. veterans of the war in Vietnam and individuals who are aware and sympathetic to the impacts of Agent Orange have supported these programs in Vietnam. An international group of veterans from the U.S. and its allies during the Vietnam War working with their former enemy — veterans from the Vietnam Veterans Association — established the Vietnam Friendship Village outside of Hanoi.
The center provides medical care, rehabilitation and vocational training for children and veterans from Vietnam who have been affected by Agent Orange. In 1998, The Vietnam Red Cross established the Vietnam Agent Orange Victims Fund to provide direct assistance to families throughout Vietnam that have been affected. In 2003, the Vietnam Association of Victims of Agent Orange (VAVA) was formed. In addition to filing the lawsuit against the chemical companies, VAVA provides medical care, rehabilitation services and financial assistance to those injured by Agent Orange.
The Vietnamese government provides small monthly stipends to more than 200,000 Vietnamese believed affected by the herbicides; this totaled $40.8 million in 2008 alone. The Vietnam Red Cross has raised more than $22 million to assist the ill or disabled, and several U.S. foundations, United Nations agencies, European governments and nongovernmental organizations have given a total of about $23 million for site cleanup, reforestation, health care and other services to those in need.
Vuong Mo of the Vietnam News Agency described one of centers:
On June 16, 2010, members of the U.S.-Vietnam Dialogue Group on Agent Orange/Dioxin unveiled a comprehensive 10-year Declaration and Plan of Action to address the toxic legacy of Agent Orange and other herbicides in Vietnam. The Plan of Action was released as an Aspen Institute publication and calls upon the U.S. and Vietnamese governments to join with other governments, foundations, businesses, and nonprofits in a partnership to clean up dioxin "hot spots" in Vietnam and to expand humanitarian services for people with disabilities there. On September 16, 2010, Senator Patrick Leahy (D-VT) acknowledged the work of the Dialogue Group by releasing a statement on the floor of the United States Senate. The statement urges the U.S. government to take the Plan of Action's recommendations into account in developing a multi-year plan of activities to address the Agent Orange/dioxin legacy.
Use outside Malaya and Vietnam.
Australia.
In 2008, Australian researcher Jean Williams claimed that cancer rates in the town of Innisfail, Queensland were 10 times higher than the state average due to secret testing of Agent Orange by the Australian military scientists during the Vietnam War. Williams, who had won the Order of Australia medal for her research on the effects of chemicals on U.S. war veterans, based her allegations on Australian government reports found in the Australian War Memorial's archives. A former soldier, Ted Bosworth, backed up the claims, saying that he had been involved in the secret testing. Neither Williams or Bosworth have produced verifiable evidence to support their claims. The Queensland health department determined that cancer rates in Innisfail were no higher than those in other parts of the state.
Brazil.
The Brazilian government used herbicides to defoliate a large section of the Amazon rainforest so that Alcoa could build the Tucuruí dam to power mining operations. Large areas of rainforest were destroyed, along with the homes and livelihoods of thousands of rural peasants and indigenous tribes.
Cambodia.
Agent Orange was used as a defoliant in eastern Cambodia during the Vietnam War, but its impacts are difficult to assess due to the chaos caused by the Khmer Rouge regime.
Canada.
New Brunswick
The U.S. military, with the permission of the Canadian government, tested herbicides, including Agent Orange, in the forests near the Canadian Forces Base Gagetown in New Brunswick for three days in 1966 and four days in 1967. Soldiers working on the base at that time were advised that the chemicals would have no harmful effects on them, to the point they would spray each other with the chemical to cool off. This inaccuracy led many to later seek compensation for medical bills. Veteran John Chisholm worked on behalf of fellow veterans to help with claims for the compensation package. On September 12, 2007, Greg Thompson, Minister of Veterans Affairs, announced that the government of Canada was offering a one-time ex gratia payment of $20,000 as the compensation package for Agent Orange exposure at CFB Gagetown.
On July 12, 2005, Merchant Law Group LLP on behalf of over 1,100 Canadian veterans and civilians who were living in and around the CFB Gagetown filed a lawsuit to pursue class action litigation concerning Agent Orange and Agent Purple with the Federal Court of Canada. On August 4, 2009, the case was rejected by the court due to lack of evidence. The ruling was appealed.
In 2007 the Canadian government announced that a research and fact-finding program initiated in 2005 had found the base was safe.
Ontario
On February 17, 2011, the Toronto Star revealed that the same chemicals used to strip the jungles of Vietnam were also employed to clear extensive plots of Crown land in Northern Ontario. The Toronto Star reported that, "records from the 1950s, 1960s and 1970s show forestry workers, often students and junior rangers, spent weeks at a time as human markers holding red, helium-filled balloons on fishing lines while low-flying planes sprayed toxic herbicides including an infamous chemical mixture known as Agent Orange on the brush and the boys below." The same day, in response to the Toronto Star article, the Ontario provincial government launched a probe into the use of Agent Orange. On February 18, 2011, the next day, Ontario's Ministry of Natural Resources widened the probe of Agent Orange spraying to include all areas of the province where government managed forests on Crown land.
British Columbia
Records show tens of thousands of gallons of the toxic mixture were applied to clear brush near highways and along power lines in the late 1960s and early 1970s – and in some cases the substance was sprayed next to homes. In B.C., the mix of 2-4-D and 2-4-5-T was called "Type B Weed and Brush Killer" in government invoices. Sometimes, the engineers ordered 2-4-5-T by itself, and dubbed it "Type C Weed and Brush Killer."
In total, about 26,000 gallons of Type B Weed and Brush Killer were ordered between 1965 and 1972. About 10,000 gallons of Type C Weed and Brush Killer were ordered in the same time period. The barrels were shipped to all four of the regions of B.C. as designated by the Ministry of Highways: Kamloops, Nelson, Prince George and Vancouver.
In 1976, documents from BC Hydro show that 2-4-5-T and 2-4-D were sprayed along Hydro lines Vernon-Monashee and Nicola-Brenda circuits. The documents also say "brushkiller" was sprayed in Pemberton and Daisy Lake.
Guam.
An analysis of chemicals present in the island’s soil, together with resolutions passed by Guam’s legislature, suggest that Agent Orange was among the herbicides routinely used on and around military bases Anderson Air Force Base, Naval Air Station Agana, Guam. Despite the evidence, the Department of Defense continues to deny that Agent Orange was ever stored or used on Guam. Several Guam veterans have collected an enormous amount of evidence to assist in their disability claims for direct exposure to dioxin containing herbicides such as 2,4,5-T which are similar to the illness associations and disability coverage that has become standard for those who were harmed by the same chemical contaminant of Agent Orange used in Vietnam.
Korea.
Agent Orange was used in Korea in the late 1960s. Republic of Korea troops were the only personnel involved in the spraying, which occurred along the Korean Demilitarized Zone (DMZ). "Citing declassified U.S. Department of Defense documents, Korean officials fear thousands of its soldiers may have come into contact with the herbicide in the late 1960s and early 1970s. According to one top government official, as many as '30,000 Korean veterans are suffering from illness related to their exposure'. The exact number of GIs who may have been exposed is unknown. But C. David Benbow, a North Carolina attorney who served as a sergeant with Co. C, 3rd Battalion, 23rd Infantry Regiment, 2nd Infantry Division, along the DMZ in 1968–69, estimates as many as '4,000 soldiers at any given time' could have been affected.".
In 1999, about 20,000 South Koreans filed two separated lawsuits against U.S. companies, seeking more than $5 billion in damages. After losing a decision in 2002, they filed an appeal.
In January 2006, the South Korean Appeals Court ordered Dow Chemical and Monsanto to pay $62 million in compensation to about 6,800 people. The ruling acknowledged that "the defendants failed to ensure safety as the defoliants manufactured by the defendants had higher levels of dioxins than standard", and, quoting the U.S. National Academy of Science report, declared that there was a "causal relationship" between Agent Orange and 11 diseases, including cancers of the lung, larynx and prostate. The judges failed to acknowledge "the relationship between the chemical and peripheral neuropathy, the disease most widespread among Agent Orange victims" according to the "Mercury News".
The United States local press KPHO-TV in Phoenix, Arizona alleged that the United States Army had buried Agent Orange in Camp Carroll, the U.S. Army base located in Gyeongsangbuk-do, Korea. It is based on the claim of three U.S. Army veterans. They claimed approximately 250 drums of Agent Orange were buried at Camp Carroll in 1978. The South Korean Ministry of Environment announced that they will request cooperative investigation at Camp Carroll officially. The USFK issued a statement that confirmed that barrels were buried there, but all (plus an additional 60 tons of soil) were removed in 1996.
Currently, veterans who provide evidence meeting VA requirements for service in Vietnam, and who can medically establish that anytime after this 'presumptive exposure' they developed any medical problems on the list of presumptive diseases, may receive compensation from the VA. Certain veterans who served in Korea and are able to prove they were assigned to certain specified around the DMZ during a specific time frame are afforded similar presumption. The differences in requirements between Vietnam and Korea service stem from the fact that congress has not made any laws to provide for the same sweeping presumption of exposure similar to the Agent Orange Act of 1991 for Korean veterans.
Laos.
Parts of Laos were sprayed with Agent Orange during the Vietnam War.
New Zealand.
The use of Agent Orange has been controversial in New Zealand, because of the exposure of New Zealand troops in Vietnam and because of the production of Agent Orange for Vietnam and other users at an Ivon Watkins-Dow chemical plant in Paritutu, New Plymouth. There have been continuing claims, as yet unproven, that the suburb of Paritutu has also been polluted; see New Zealand in the Vietnam War.
There are cases of New Zealand soldiers developing cancers such as bone cancer but none has been scientifically connected to exposure to herbicides.
Philippines.
Herbicide persistence studies of Agents Orange and White were conducted in the Philippines. The Philippine herbicide test program was conducted in cooperation with the University of the Philippines, College of Forestry and was described in a 1969 issue of The Philippine Collegian.
Johnston Atoll.
The U.S. Air Force operation to remove Herbicide Orange from Vietnam in 1972 was named Operation Pacer IVY, while the operation to destroy the Agent Orange stored at Johnston Atoll in 1977 was named Operation Pacer HO. Operation Pacer IVY (InVentorY) collected Agent Orange in South Vietnam and removed it in 1972 aboard the ship for storage on Johnston Atoll. The Environmental Protection Agency (EPA) reports that 1,800,000 gallons of Herbicide Orange was stored at Johnston Island in the Pacific and 480,000 gallons at Gulfport Mississippi.
Research and studies were initiated to find a safe method to destroy the materials and it was discovered they could be incinerated safely under special conditions of temperature and dwell time. However, these herbicides were expensive and the Air Force wanted to resell its surplus instead of dumping it at sea. Among many methods tested, a possibility of salvaging the herbicides by reprocessing and filtering out the 2,3,7,8-tetrachlorodibenzo-p-dioxin (TCDD) contaminant with carbonized (charcoaled) coconut fibers. This concept was then tested in 1976 and a pilot plant constructed at Gulfport, Mississippi.
From July to September 1977 during Operation Pacer HO (Herbicide Orange), the entire stock of Agent Orange from both Herbicide Orange storage sites at Gulfport, Mississippi and Johnston Atoll was subsequently incinerated in four separate burns in the vicinity of Johnson Island aboard the Dutch-owned waste incineration ship .
As of 2004, some records of the storage and disposition of Agent Orange at Johnston Atoll have been associated with the historical records of Operation Red Hat.
Okinawa, Japan.
There have been dozens of reports in the press about use and/or storage of military formulated herbicides on Okinawa that are based upon statements by former U.S. service members that had been stationed on the island, photographs, government records, and unearthed storage barrels. The U.S. Department of Defense (DoD) has denied these allegations with statements by military officials and spokespersons, as well as a January 2013 report authored by Dr. Alvin Young that was released in April 2013. Dr. Young has long argued against Agent Orange harmful effects. As reported in court documents covering the Agent Orange lawsuits, in May 1985 "White House scientist Alvin L. Young, a toxicologist, recommends that no further research on dioxin should be funded, "because research has failed to show it causes cancer or birth defects in humans."
In particular, the 2013 report refuted articles written by journalist Jon Mitchell as well as a statement from "An Ecological Assessment of Johnston Atoll" a 2003 publication produced by the United States Army Chemical Materials Agency that states, "in 1972, the U.S. Air Force also brought about 25,000 55-gallon (208L) drums of the chemical, Herbicide Orange (HO) to Johnston Island that originated from Vietnam and was stored on Okinawa." The 2013 report stated: "The authors of the 00 report were not DoD employees, nor were they likely familiar with the issues surrounding Herbicide Orange or its actual history of transport to the Island." and detailed the transport phases and routes of Agent Orange from Vietnam to Johnston Atoll, none of which included Okinawa.
Further official confirmation of restricted (dioxin containing) herbicide storage on Okinawa appeared in a 1971 Fort Detrick report titled "Historical, Logistical, Political and Technical Aspects of the Herbicide/Defoliant Program", which mentioned that the environmental statement should consider "Herbicide stockpiles elsewhere in PACOM (Pacific Command) U.S. Government restricted materials Thailand and Okinawa (Kadena AFB)." The 2013 DoD report says that the environmental statement urged by the 1971 report was published in 1974 as "The Department of Air Force Final Environmental Statement", and that the latter did not find Agent Orange was held in either Thailand or Okinawa.
Thailand.
Agent Orange was tested by the United States in Thailand during the war in Southeast Asia. Buried drums were uncovered and confirmed to be Agent Orange in 1999. Workers who uncovered the drums fell ill while upgrading the airport near Hua Hin, 100 km south of Bangkok.
Vietnam-era Veterans whose service involved duty on or near the perimeters of military bases in Thailand anytime between February 28, 1961 and May 7, 1975 may have been exposed to herbicides and may qualify for VA benefits. A claim for direct exposure is possible if the individual can verify that they worked or lived in close proximity to the affected areas of the bases in Thailand.
A declassified Department of Defense report written in 1973, suggests that there was a significant use of herbicides on the fenced-in perimeters of military bases in Thailand to remove foliage that provided cover for enemy forces.
In 2013 VA determined that herbicides used on the Thailand base perimeters may have been tactical and procured from Vietnam, or a strong, commercial type resembling tactical herbicides.
United States.
The University of Hawaii has acknowledged extensive testing of Agent Orange on behalf of the United States Department of Defense in Hawaii along with mixtures of Agent Orange on Kaua'i Island in 1967-68 and on Hawaii Island in 1966; testing and storage in other U.S. locations has been documented by the United States Department of Veterans Affairs.
In 1971, the C-123 aircraft used for spraying Agent Orange were returned to the United States and assigned various East Coast USAF Reserve squadrons, and then employed in traditional airlift missions between 1972 and 1982. In 1994, testing by the Air Force identified some former spray aircraft as "heavily contaminated" with dioxin residue. Inquiries by aircrew veterans in 2011 brought a decision by the U.S. Department of Veterans Affairs opining that not enough dioxin residue remained to injure these post-Vietnam War veterans. On 26 January 2012, the U.S. Center For Disease Control's Agency for Toxic Substances and Disease Registry challenged this with their finding that former spray aircraft were indeed contaminated and the aircrews exposed to harmful levels of dioxin. In response to veterans' concerns, the VA in February 2014 referred the C-123 issue to the Institute of Medicine for a special study, with results released on January 9, 2015.
In 1978, the U.S. Environmental Protection Agency suspended spraying of Agent Orange in National Forests.
A December 2006 Department of Defense report listed Agent Orange testing, storage, and disposal sites at 32 locations throughout the United States, as well as in Canada, Thailand, Puerto Rico, Korea, and in the Pacific Ocean. The Veteran Administration has also acknowledged that Agent Orange was used domestically by U.S. forces in test sites throughout the United States. Eglin Air Force Base in Florida was one of the primary testing sites throughout the 1960s.
Cleanup programs.
In February 2012, Monsanto agreed to settle a case covering Dioxin contamination around a plant in Nitro, West Virginia that had made Agent Orange. Monsanto agreed to pay up to $9 million for cleanup of affected homes, $84 million for medical monitoring of people affected, and the community’s legal fees.
On the 9th of August 2012, the United States and Vietnam began a cooperative cleaning up of the toxic chemical on part of Danang International Airport, marking the first time Washington has been involved in cleaning up Agent Orange in Vietnam. Danang was the primary storage site of the chemical. Two other cleanup sites the United States and Vietnam are looking at is Biên Hòa, in the southern province of Đồng Nai - a 'hotspot' for dioxin - and Phù Cát airport in the central province of Bình Định, says U.S. Ambassador to Vietnam David Shear. According to the Vietnamese newspaper Nhân Dân, the U.S. government provided $41 million to the project, which will reduce the contamination level in 73,000 m³ of soil by late 2016.

</doc>
<doc id="2551" url="https://en.wikipedia.org/wiki?curid=2551" title="Astronomical year numbering">
Astronomical year numbering

Astronomical year numbering is based on AD/CE year numbering, but follows normal decimal integer numbering more strictly. Thus, it has a year 0, the years before that are designated with negative numbers and the years after that are designated with positive numbers. Astronomers use the Julian calendar for years before 1582, including this year 0, and the Gregorian calendar for years after 1582 as exemplified by Jacques Cassini (1740), Simon Newcomb (1898) and Fred Espenak (2007).
The prefix AD and the suffixes CE, BC or BCE (Common Era, Before Christ or Before Common Era) are dropped. The year 1 BC/BCE is numbered 0, the year 2 BC is numbered −1, and in general the year "n" BC/BCE is numbered "−("n" − 1)" (a negative number equal to 1 − "n"). The numbers of AD/CE years are not changed and are written with either no sign or a positive sign; thus in general "n" AD/CE is simply "n" or +"n". For normal calculation a number zero is often needed, here most notably when calculating the number of years in a period that spans the epoch; the end years need only be subtracted from each other.
The system is so named due to its use in astronomy. Few other disciplines outside history deal with the time before year 1, some exceptions being dendrochronology, archaeology and geology, the latter two of which use 'years before the present'. Although the absolute numerical values of astronomical and historical years only differ by one before year 1, this difference is critical when calculating astronomical events like eclipses or planetary conjunctions to determine when historical events which mention them occurred.
Year zero usage.
In his Rudolphine Tables (1627), Johannes Kepler used a prototype of year zero which he labeled "Christi" (Christ's) between years labeled "Ante Christum" (Before Christ) and "Post Christum" (After Christ) on the mean motion tables for the Sun, Moon, Saturn, Jupiter, Mars, Venus and Mercury. Then in 1702 the French astronomer Philippe de la Hire used a year he labeled at the end of years labeled "ante Christum" (BC), and immediately before years labeled "post Christum" (AD) on the mean motion pages in his "Tabulæ Astronomicæ", thus adding the designation "0" to Kepler's "Christi". Finally, in 1740 the French astronomer Jacques Cassini , who is traditionally credited with the invention of year zero, completed the transition in his "Tables astronomiques", simply labeling this year "0", which he placed at the end of Julian years labeled "avant Jesus-Christ" (before Jesus Christ or BC), and immediately before Julian years labeled "après Jesus-Christ" (after Jesus Christ or AD).
Cassini gave the following reasons for using a year 0:
Fred Espanak of NASA lists 50 phases of the moon within year 0, showing that it is a full year, not an instant in time. <br> Jean Meeus gives the following explanation:
Signed years without year 0.
Although he used the usual French terms "avant J.-C." (before Jesus Christ) and "après J.-C." (after Jesus Christ) to label years elsewhere in his book, the Byzantine historian Venance Grumel used negative years (identified by a minus sign, −) to label BC years and unsigned positive years to label AD years in a table, possibly to save space, without a year 0 between them.
Version 1.0 of the XML Schema language, often used to describe data interchanged between computers in XML, includes built-in primitive datatypes date and dateTime. Although these are defined in terms of ISO 8601 which uses the proleptic Gregorian calendar and therefore should include a year 0, the XML Schema specification states that there is no year zero. Version 1.1 of the defining recommendation realigned the specification with ISO 8601 by including a year zero, despite
the problems arising from the lack of backwards compatibility.

</doc>
<doc id="2552" url="https://en.wikipedia.org/wiki?curid=2552" title="Adam of Bremen">
Adam of Bremen

Adam of Bremen (also: Adamus Bremensis) was a German medieval chronicler. He lived and worked in the second half of the eleventh century. He is most famous for his chronicle "Gesta Hammaburgensis Ecclesiae Pontificum" ("Deeds of Bishops of the Hamburg Church").
Little is known of his life other than hints from his own chronicles. He is believed to have come from Meissen (Latin "Misnia") in Saxony. The dates of his birth and death are uncertain, but he was probably born before 1050 and died on 12 October of an unknown year (possibly 1081, at the latest 1085). From his chronicles it is apparent that he was familiar with a number of authors. The honorary name of "Magister Adam" shows that he had passed through all the stages of a higher education. It is probable that he was taught at the "Magdeburger Domschule".
In 1066 or 1067 he was invited by archbishop Adalbert of Hamburg to join the Church of Bremen. Adam was accepted among the capitulars of Bremen, and by 1069 he appeared as director of the cathedral's school. Soon thereafter he began to write the history of Bremen/Hamburg and of the northern lands in his "Gesta".
His position and the missionary activity of the church of Bremen allowed him to gather information on the history and the geography of Northern Germany. A stay at the court of Svend Estridson gave him the opportunity to find information about the history and geography of Denmark and the other Scandinavian countries.

</doc>
<doc id="2553" url="https://en.wikipedia.org/wiki?curid=2553" title="Ab urbe condita">
Ab urbe condita

"ab urbe condita" (related to "anno urbis conditae"; A. U. C., AUC, a.u.c.; also "anno urbis", short a.u.) is a Latin phrase meaning "from the founding of the City (Rome)", traditionally dated to 753 BC. AUC is a year-numbering system used by some ancient Roman historians to identify particular Roman years. Renaissance editors sometimes added AUC to Roman manuscripts they published, giving the false impression that the Romans usually numbered their years using the AUC system. The dominant method of identifying Roman years in Roman times was to name the two consuls who held office that year. The regnal year of the emperor was also used to identify years, especially in the Byzantine Empire after 537 when Justinian required its use.
Significance.
The traditional date for the founding of Rome of 21 April 753 BC, was initiated by 1st century BC scholar Marcus Terentius Varro. Varro may have used the consular list with its mistakes, and called the year of the first consuls "245 "ab urbe condita"", accepting the 244-year interval from Dionysius of Halicarnassus for the kings after the foundation of Rome. The correctness of Varro's calculation has not been proven scientifically but is still used worldwide.
From Emperor Claudius (reigned 41–54 AD) onwards, Varro's calculation superseded other contemporary calculations. Celebrating the anniversary of the city became part of imperial propaganda. Claudius was the first to hold magnificent celebrations in honour of the city's anniversary, in 48 AD, 800 years after the founding of the city. Hadrian and Antoninus Pius held similar celebrations, in 121 AD and 147/148 AD respectively.
During 248 AD, Philip the Arab celebrated Rome's first millennium, together with Ludi saeculares for Rome's alleged tenth saeculum. Coins from his reign commemorate the celebrations. A coin by a contender for the imperial throne, Pacatianus, explicitly states "Year one thousand and first", which is an indication that the citizens of the Empire had a sense of the beginning of a new era, a "Saeculum Novum".
When the Roman Empire turned Christian during the following century, this imagery came to be used in a more metaphysical sense, and removed legal impediments to the development and public use of the "Anno Domini" dating system, which came into general use during the reign of Charlemagne.
Relationship with Anno Domini.
The Anno Domini (AD) year numbering was developed by a monk named Dionysius Exiguus in Rome during 525, as a result of his work on calculating the date of Easter. In his Easter table the year 532 AD was equated with the regnal year 248 of Emperor Diocletian. The table counted the years starting from the presumed birth of Christ, rather than the accession of the emperor Diocletian on 20 November 284, or as stated by Dionysius: "sed magis elegimus ab incarnatione Domini nostri Jesu Christi annorum tempora praenotare..." Blackburn and Holford-Strevens review interpretations of Dionysius which place the Incarnation in 2 BC, 1 BC, or 1 AD. It was later calculated (from the historical record of the succession of Roman consuls) that the year 1 AD corresponds to the Roman year 754 AUC, based on Varro's epoch. This however resulted in that year not corresponding with the lifetimes of historical figures reputed to be alive, or otherwise mentioned in connection with the Christian incarnation, e.g. Herod the Great or Quirinius.

</doc>
<doc id="2558" url="https://en.wikipedia.org/wiki?curid=2558" title="ARY Group">
ARY Group

The ARY Group is a Dubai-based holding company founded by a Pakistani businessman, Haji Abdul Razzak Yaqoob. Abdul is the Chief Executive Officer and owner of the Group. ARY is a diversified group with interests in several sectors, though it is most famous for its contribution to Pakistani television.
Companies.
Companies under the ARY Group:
Take over of BOL Network.
BOL Network and its defunct BOL News channel have been taken over in August 2015 by ARY Digital Network CEO Salman Iqbal, who said that the decision is taken in view to provide career protection to media industry and its workers. The founder of ARY Group said the his media group would launch the transmission of the channel within three weeks.

</doc>
<doc id="2559" url="https://en.wikipedia.org/wiki?curid=2559" title="Arapaoa Island">
Arapaoa Island

Arapaoa Island, formerly known as Arapawa Island, is a small island located in the Marlborough Sounds, at the north east tip of the South Island of New Zealand.
The island has a land area of . Queen Charlotte Sound defines its western side, while to the south lies Tory Channel, which is on the sea route from Wellington in the North Island to Picton. Cook Strait's narrowest point is between Arapaoa Island's Perano Head and Cape Terawhiti in the North Island.
History.
According to Māori oral tradition, the island was where the great navigator Kupe killed the octopus Te Wheke-a-Muturangi.
It was from a hill on Arapaoa Island in 1770 that Captain James Cook first saw the sea passage from the Pacific Ocean to the Tasman Sea, which was named Cook Strait. This discovery banished the fond notion of geographers that there existed a great southern continent, Terra Australis. A monument at Cook's Lookout was erected in 1970.
From the late 1820s until the mid-1960s, Arapaoa Island was a base for whaling in the Sounds. John Guard established a shore station at Te Awaiti in 1827 for right whales. Later, the station at Perano Head on the east coast of the island was used to hunt humpback whales from 1911 to 1964 (see Whaling in New Zealand). The houses built by the Perano family are now operated as tourist accommodations.
In August 2014, the spelling of the island's name was officially altered from "Arapawa" to "Arapaoa".
Aircraft accident.
An elevated power cable from the mainland to Arapaoa Island over Tory Channel was struck by an Air Albatross Cessna 402 commuter aircraft in 1985. The crash was witnessed by many passengers on an inter-island Cook Strait ferry. The ferry immediately stopped to dispatch a rescue lifeboat. Along with the two pilots, one entire family was lost, and all but a young girl from the other. No bodies were ever found. The sole survivor (Cindy Mosey) was travelling with her family and the other from Nelson to Wellington to attend a gymnastics competition. The Arapaoa Island crash caused public confidence in Air Albatross to falter, contributing to the company going into liquidation in December of that year.
Conservation.
Parts of the island have been heavily cleared of native vegetation in the past through burning and logging, A number of pine forests were planted on the island. Wilding pines, an invasive species in some parts of New Zealand, are being poisoned on the island to allow the regenerating native vegetation to grow. About at Ruaomoko Point on the south-eastern portion of the island will be killed by drilling holes into the trees and injecting poison.
Arapaoa Island is known for the breeds of pigs, sheep and goats found only on the island. These became established in the 19th century, but the origin of these breeds is uncertain, and a matter of some speculation. Common suggestions are that they are old English breeds introduced by the early whalers, or by Captain Cook or other early explorers. These breeds are now extinct in England, and the goats surviving in a sanctuary on the island are now also bred in other parts of New Zealand and in the northern hemisphere.

</doc>
<doc id="2560" url="https://en.wikipedia.org/wiki?curid=2560" title="Administrative law">
Administrative law

Administrative law is the body of law that governs the activities of administrative agencies of government. Government agency action can include rulemaking, adjudication, or the enforcement of a specific regulatory agenda. Administrative law is considered a branch of public law. As a body of law, administrative law deals with the decision-making of administrative units of government (for example, tribunals, boards or commissions) that are part of a national regulatory scheme in such areas as police law, international trade, manufacturing, the environment, taxation, broadcasting, immigration and transport. Administrative law expanded greatly during the twentieth century, as legislative bodies worldwide created more government agencies to regulate the social, economic and political spheres of human interaction.
Civil law countries often have specialized courts, administrative courts, that review these decisions.
Administrative law in common law countries.
Generally speaking, most countries that follow the principles of common law have developed procedures for judicial review that limit the reviewability of decisions made by administrative law bodies. Often these procedures are coupled with legislation or other common law doctrines that establish standards for proper rulemaking. Administrative law may also apply to review of decisions of so-called semi-public bodies, such as non-profit corporations, disciplinary boards, and other decision-making bodies that affect the legal rights of members of a particular group or entity.
While administrative decision-making bodies are often controlled by larger governmental units, their decisions could be reviewed by a court of general jurisdiction under some principle of judicial review based upon due process (United States) or fundamental justice (Canada). Judicial review of administrative decisions is different from an administrative appeal. When sitting in review of a decision, the Court will only look at the method in which the decision was arrived at, whereas in an administrative appeal the correctness of the decision itself will be examined, usually by a higher body in the agency. This difference is vital in appreciating administrative law in common law countries.
The scope of judicial review may be limited to certain questions of fairness, or whether the administrative action is "ultra vires". In terms of ultra vires actions in the broad sense, a reviewing court may set aside an administrative decision if it is unreasonable (under Canadian law, following the rejection of the "Patently Unreasonable" standard by the Supreme Court in Dunsmuir v. New Brunswick), "Wednesbury" unreasonable (under British law), or arbitrary and capricious (under U.S. Administrative Procedure Act and New York State law). Administrative law, as laid down by the Supreme Court of India, has also recognized two more grounds of judicial review which were recognized but not applied by English Courts, namely legitimate expectation and proportionality.
The powers to review administrative decisions are usually established by statute, but were originally developed from the royal prerogative writs of English law, such as the writ of mandamus and the writ of certiorari. In certain Common Law jurisdictions, such as India or Pakistan, the power to pass such writs is a Constitutionally guaranteed power. This power is seen as fundamental to the power of judicial review and an aspect of the independent judiciary.
United States.
In the United States, many government agencies are organized under the executive branch of government, although a few are part of the judicial or legislative branches.
In the federal government, the executive branch, led by the president, controls the federal executive departments, which are led by secretaries who are members of the United States Cabinet. The many independent agencies of the United States government created by statutes enacted by Congress exist outside of the federal executive departments but are still part of the executive branch.
Congress has also created some special judicial bodies known as Article I tribunals to handle some areas of administrative law.
The actions of executive agencies and independent agencies are the main focus of American administrative law. In response to the rapid creation of new independent agencies in the early twentieth century (see discussion below), Congress enacted the Administrative Procedure Act (APA) in 1946. Many of the independent agencies operate as miniature versions of the tripartite federal government, with the authority to "legislate" (through rulemaking; see Federal Register and Code of Federal Regulations), "adjudicate" (through administrative hearings), and to "execute" administrative goals (through agency enforcement personnel). Because the United States Constitution sets no limits on this tripartite authority of administrative agencies, Congress enacted the APA to establish fair administrative law procedures to comply with the constitutional requirements of due process. Agency procedures are drawn from four sources of authority: the APA, organic statutes, agency rules, and informal agency practice.
The American Bar Association's official journal concerning administrative law is the "Administrative Law Review", a quarterly publication that is managed and edited by students at the Washington College of Law.
Historical development.
Stephen Breyer, a U.S. Supreme Court Justice since 1994, divides the history of administrative law in the United States into six discrete periods, according to his book, "Administrative Law & Regulatory Policy" (3d Ed., 1992):
Agriculture.
The agricultural sector is one of the most heavily regulated sectors in the U.S. economy, as it is regulated in various ways at the international, federal, state, and local levels. Consequently, administrative law is a significant component of the discipline of Agricultural Law. The United States Department of Agriculture and its myriad agencies such as the Agricultural Marketing Service are the primary sources of regulatory activity, although other administrative bodies such as the Environmental Protection Agency play a significant regulatory role as well.
Administrative law in civil law countries.
Unlike most Common-law jurisdictions, the majority of civil law jurisdictions have specialized courts or sections to deal with administrative cases which, as a rule, will apply procedural rules specifically designed for such cases and different from that applied in private-law proceedings, such as contract or tort claims.
France.
In France, most claims against the national or local governments are handled by administrative courts, which use the "Conseil d'État" (Council of State) as a court of last resort. The main administrative courts are the "tribunaux administratifs" and appeal courts are the "cours administratives d'appel". The French body of administrative law is called "droit administratif".
French administrative law which is the founder of Continental administrative law has a huge effect on other administrative laws of several countries such as Belgium, Greece, Turkey and Tunisia.
Germany.
Administrative law in Germany, called “Verwaltungsrecht”, generally rules the relationship between authorities and the citizens and therefore, it establishes citizens’ rights and obligations against the
authorities. It is a part of the public law, which deals with the organization, the tasks and the acting of the public administration. It also contains rules, regulations, orders and decisions created by and related to administrative agencies, such as federal agencies, federal state authorities, urban administrations, but also admission offices and fiscal authorities etc. Administrative law in Germany follows three basic principles.
Administrative law in Germany can be divided into general administrative law and special administrative law.
General administrative law.
The general administration law is basically ruled in the Administrative Procedures Law (Verwaltungsverfahrensgesetz wVf). Other legal sources are the Rules of the Administrative Courts (Verwaltungsgerichtsordnung wG), the social security code (Sozialgesetzbuch G) and the general fiscal law (Abgabenordnung ).
Administrative Procedures Law.
The Verwaltungsverfahrensgesetz (VwVfG), which was enacted in 1977, regulates the main administrative procedures of the federal government. It serves the purpose to ensure a treatment in accordance with the rule of law by the public authority. Furthermore, it contains the regulations for mass processes and expands the legal protection against the authorities. The VwVfG basically applies for the entire public administrative activities of federal agencies as well as federal state authorities, in case of making federal law. One of the central clause is § 35 VwVfG. It defines the administrative act, the most common form of action in which the public administration occurs against a citizen. The definition in § 35 says, that an administration act is characterized by the following features:
It is an official act of an authority in the field of public law to resolve an individual case with effect to the outside.
§§ 36 – 39, §§ 58 – 59 and § 80 VwV––fG rule the structure and the necessary elements of the
administrative act. § 48 and § 49 VwVfG have a high relevance in practice, as well. In these
paragraphs, the prerequisites for redemption of an unlawful administration act (§ 48 VwVfG ) and
withdrawal of a lawful administration act (§ 49 VwVfG ), are listed.
Other legal sources.
Administration procedural law (Verwaltungsgerichtsordnung wG), which was enacted in 1960, rules the court procedures at the administrative court. The VwGO is divided into five parts, which are the constitution of the courts, action, remedies and retrial, costs and enforcement15 and final clauses and temporary arrangements.
In absence of a rule, the VwGO is supplemented by the code of civil procedure (Zivilprozessordnung P) and the judicature act (Gerichtsverfassungsgesetz V). In addition to the regulation of the administrative procedure, the VwVfG also constitutes the legal protection in administrative law beyond the court procedure. § 68 VwVGO rules the preliminary proceeding, called “Vorverfahren” or “Widerspruchsverfahren”, which is a stringent prerequisite for the administrative procedure, if an action for rescission or a writ of mandamus against an authority is aimed. The preliminary proceeding gives each citizen, feeling unlawfully mistreated by an authority, the possibility to object and to force a review of an administrative act without going to court. The prerequisites to open the public law remedy are listed in § 40 I VwGO. Therefore, it is necessary to have the existence of a conflict in public law without any constitutional aspects and no assignment to another jurisdiction.
The social security code (Sozialgesetzbuch G) and the general fiscal law are less important for the administrative law. They supplement the VwVfG and the VwGO in the fields of taxation and social legislation, such as social welfare or financial support for students (BaFÖG) etc.
Special administrative law.
The special administrative law consists of various laws. Each special sector has its own law. The most important ones are the
In Germany, the highest administrative court for most matters is the federal administrative court Bundesverwaltungsgericht. There are federal courts with special jurisdiction in the fields of social security law (Bundessozialgericht) and tax law (Bundesfinanzhof).
Italy.
Administrative law in Italy, known as “Diritto amministrativo”, is a branch of public law, whose rules govern the organization of the public administration and the activities of the pursuit of the public interest of the public administration and the relationship between this and the citizens.
Its genesis is related to the principle of division of powers of the State. The administrative power, originally called "executive", is to organize resources and people whose function is devolved to achieve the public interest objectives as defined by the law.
The Netherlands.
In The Netherlands, administrative law provisions are usually contained in separate laws. There is however a single General Administrative Law Act ("Algemene wet bestuursrecht" or Awb) that applies both to the making of administrative decisions and the judicial review of these decisions in courts. On the basis of the Awb, citizens can oppose a decision ('besluit') made by an administrative agency ('bestuursorgaan') within the administration and apply for judicial review in courts if unsuccessful.
Unlike France or Germany, there are no special administrative courts of first instance in the Netherlands, but regular courts have an administrative "chamber" which specializes in administrative appeals. The courts of appeal in administrative cases however are specialized depending on the case, but most administrative appeals end up in the judicial section of the Council of State (Raad van State).
Before going to court, citizens must usually first object to the decision with the administrative body who made it. This is called "bezwaar". This procedure allows for the administrative body to correct possible mistakes themselves and is used to filter cases before going to court. Sometimes, instead of bezwaar, a different system is used called "administratief beroep" (administrative appeal). The difference with bezwaar is that administratief beroep is filed with a different administrative body, usually a higher ranking one, than the administrative body that made the primary decision. Administratief beroep is available only if the law on which the primary decision is based specifically provides for it. An example involves objecting to a traffic ticket with the district attorney ("officier van justitie"), after which the decision can be appealed in court.
In addition, Netherlands General Administrative Law Act (GALA) is a rather good sample of procedural laws in Europe
Turkey.
In Turkey, the lawsuits against the acts and actions of the national or local governments and public bodies are handled by administrative courts which are the main administrative courts. The decisions of the administrative courts are checked by the Regional Administrative Courts and Council of State. Council of State as a court of last resort is exactly similar to Conseil d'État in France.
Sweden.
In Sweden, there is a system of administrative courts that considers only administrative law cases, and is completely separate from the system of general courts. This system has three tiers, with 12 county administrative courts ("förvaltningsrätt") as the first tier, four administrative courts of appeal ("kammarrätt") as the second tier, and the Supreme Administrative Court of Sweden ("Högsta Förvaltningsdomstolen") as the third tier.
Migration cases are handled in a two-tier system, effectively within the system general administrative courts. Three of the administrative courts serve as migration courts ("migrationsdomstol") with the Administrative Court of Appeal in Stockholm serving as the Migration Court of Appeal ("Migrationsöverdomstolen").
Brazil.
In Brazil, unlike most Civil-law jurisdictions, there is no specialized court or section to deal with administrative cases. In 1998, a constitutional reform, led by the government of the President Fernando Henrique Cardoso, introduced regulatory agencies as a part of the executive branch. Since 1988, Brazilian administrative law has been strongly influenced by the judicial interpretations of the constitutional principles of public administration (art. 37 of Federal Constitution): legality, impersonality, publicity of administrative acts, morality and efficiency.
Chile.
The President of the Republic exercises the administrative function, in collaboration with several Ministries or other authorities with "ministerial rank". Each Ministry has one or more under-secretary that performs through public services the actual satisfaction of public needs. There is not a single specialized court to deal with actions against the Administrative entities, but instead there are several specialized courts and procedures of review.
People's Republic of China.
Administrative law in the People's Republic of China was virtually non-existent before the economic reform era initiated by Deng Xiaoping. Since the 1980s, the People's Republic of China has constructed a new legal framework for administrative law, establishing control mechanisms for overseeing the bureaucracy and disciplinary committees for the Communist Party of China. However, many have argued that the usefulness of these laws is vastly inadequate in terms of controlling government actions, largely because of institutional and systemic obstacles like a weak judiciary, poorly trained judges and lawyers, and corruption.
In 1990, the Administrative Supervision Regulations (行政检查条例) and the Administrative Reconsideration Regulations (行政复议条例) were passed. The 1993 State Civil Servant Provisional Regulations (国家公务员暂行条例) changed the way government officials were selected and promoted, requiring that they pass exams and yearly appraisals, and introduced a rotation system. The three regulations have been amended and upgraded into laws. In 1994, the State Compensation Law (国家赔偿法) was passed, followed by the Administrative Penalties Law (行政处罚法) in 1996. Administrative Compulsory Law was enforced in 2012. Adiministrative Litigation Law was amended in 2014.The General Administrative Procedure Law is under way .
Ukraine.
As a homogeneous legal substance isolated in a system of jurisprudence, the administrative law of Ukraine is characterized as: (1) a branch of law; (2) a science; (3) a discipline.

</doc>
<doc id="2563" url="https://en.wikipedia.org/wiki?curid=2563" title="Arthur Phillip">
Arthur Phillip

Admiral Arthur Phillip (11 October 173831 August 1814) was a Royal Navy officer, the first Governor of New South Wales and led the colonisation of what is now Australia and founded the British penal colony that later became the city of Sydney, Australia.
After much experience at sea, Phillip sailed with the First Fleet as Governor-designate of the proposed British penal colony of New South Wales. In January 1788, he selected its location to be Port Jackson (encompassing Sydney Harbour).
Phillip was a far-sighted governor who soon saw that New South Wales would need a civil administration and a system for emancipating the convicts. But his plan to bring skilled tradesmen on the voyage had been rejected, and he faced immense problems of labour, discipline and supply. Also his friendly attitude towards the aborigines was sorely tested when they killed his gamekeeper, and he was not able to assert a clear policy about them.
The arrival of the Second and Third Fleets placed new pressures on the scarce local resources, but by the time Phillip sailed home in December 1792, the colony was taking shape, with official land-grants and systematic farming and water-supply.
Phillip retired in 1805, but continued to correspond with his friends in New South Wales and to promote the colony's interests.
Early life.
Arthur Phillip was born on 11 October 1738, the younger of two children to Jacob Phillip and Elizabeth Breach. His father Jacob was born in Frankfurt, Germany. He was a languages teacher who may also have served in the Royal Navy as an able seaman and purser's steward. His mother Elizabeth was the widow of an ordinary seaman, John Herbert, who had served in Jamaica aboard HMS "Tartar" and died of disease on 13 August 1732. At the time of Arthur Phillip's birth, his family maintained a modest existence as tenants near Cheapside in the City of London.
There are no surviving records of Phillip's early childhood. His father Jacob died in 1739, after which the Phillip family may have fallen on hard times. On 22 June 1751 he was accepted into the Greenwich Hospital School, a charity school for the sons of indigent seafarers. In keeping with the school's curriculum, his education was focused on literacy, arithmetic and navigational skills, including cartography. He was a competent student and something of a perfectionist. His headmaster, Rev. Francis Swinden observed that in personality, Phillip was "unassuming, reasonable, business-like to the smallest degree in everything he undertakes".
Phillip remained at the Greenwich School for two and a half years, considerably longer than the average student stay of twelve months. At the end of 1753 he was granted a seven-year indenture as an apprentice aboard "Fortune", a 210-ton whaling boat commanded by merchant mariner Wiliam Readhead. He left the Greenwich School on 1 December and spent the winter aboard "Fortune" awaiting the commencement of the 1754 whaling season.
First voyages.
Phillip spent the summer of 1754 hunting whales near Svalbard in the Barents Sea. As an apprentice, his responsibilities included stripping blubber from whale carcasses and helping to pack it into barrels. Food was scarce and "Fortune"s thirty crew members supplemented their diet with bird's eggs, scurvy grass and where possible, reindeer. The ship returned to England on 20 July 1754. The whaling crew were paid off and replaced with twelve sailors for a winter voyage to the Mediterranean. As an apprentice, Phillip remained aboard as "Fortune" undertook an outward trading voyage to Barcelona and Livorno carrying salt and raisins, returning via Rotterdam with a cargo of grains and citrus. The ship returned to England in April 1755 and sailed immediately for Svalbard for that year's whale hunt. Phillip was still a member of the crew, but abandoned his apprenticeship when the ship returned to England on 27 July. On 16 October he enlisted in the Royal Navy and was assigned the rank of ordinary seaman aboard the 68-gun .
As a member of "Buckingham"s crew, Phillip saw action in the Seven Years' War, including the Battle of Minorca in 1756. By 1762 he had transferred to , and was promoted to Lieutenant in recognition of active service in the Battle of Havana. The War ended in 1763 and Phillip returned to England on half pay. In July 1763 he married Margaret Denison, a widow 16 years his senior, and established a farm in Lyndhurst, Hampshire. The marriage was unhappy, and the couple separated in 1769 when Phillip returned to the Navy. The following year he was posted as second lieutenant aboard , a newly built 74-gun ship of the line.
In 1774 Phillip joined the Portuguese Navy as a captain, serving in the War against Spain. While with the Portuguese Navy, Phillip commanded a frigate, the "Nossa Senhora do Pilar." On this ship he took a detachment of troops from Rio de Janeiro to Colonia do Sacramento on the Rio de la Plata (opposite Buenos Aires) to relieve the garrison there. This voyage also conveyed a consignment of convicts assigned to carry out work at Colonia. During a storm encountered in the course of the voyage, the convicts assisted in working the ship and, on arrival at Colonia, Phillip recommended that they be rewarded for saving the ship by remission of their sentences. A garbled version of this eventually found its way into the English press when Phillip was appointed in 1786 to lead the expedition to Sydney. Phillip played a leading part in the capture of the Spanish ship San Agustín, on 19 April 1777, off Santa Catarina. The "San Agustin" was commissioned into the Portuguese Navy as the "Santo Agostinho", and command of her was given to Phillip. The action was reported in the English press: 
Madrid, Aug. 28. Letters from Lisbon bring the following Account from Rio Janeiro: That the St. Augustine, of 70 Guns, having been separated from the Squadron of M. Casa Tilly, was attacked by two Portugueze Ships, against which they defended themselves for a Day and a Night, but being next Day surrounded by the Portugueze Fleet, was obliged to surrender.
In 1778 Britain was again at war, and Phillip was recalled to active service, and in 1779 obtained his first command, HMS "Basilisk". He was promoted to post-captain on 30 November 1781 and given command of .
In July 1782, in a change of government, Thomas Townshend became Secretary of State for Home and American Affairs, and assumed responsibility for organising an expedition against Spanish America. Like his predecessor, Lord Germain, he turned for advice to Arthur Phillip. A letter from Phillip to Sandwich of 17 January 1781 records Phillip's loan to Sandwich of his charts of the Plata and Brazilian coasts for use in organising the expedition. Phillip's plan was for a squadron of three ships of the line and a frigate to mount a raid on Buenos Aires and Monte Video, then to proceed to the coasts of Chile, Peru and Mexico to maraud, and ultimately to cross the Pacific to join the British Navy's East India squadron for an attack on Manila. The expedition, consisting of the "Grafton," 70 guns, "Elizabeth," 74 guns, "Europe," 64 guns, and the frigate "Iphigenia", sailed on 16 January 1783, under the command of Commodore Robert Kingsmill. Phillip was given command of the 64-gun , or "Europe". Shortly after sailing, an armistice was concluded between Great Britain and Spain. Phillip learnt of this in April when he put in for storm repairs at Rio de Janeiro. Phillip wrote to Townshend from Rio de Janeiro on 25 April 1783, expressing his disappointment that the ending of the American War had robbed him of the opportunity for naval glory in South America.
After his return to England from India in April 1784, Phillip remained in close contact with Townshend, now Lord Sydney, and the Home Office Under Secretary, Evan Nepean. From October 1784 to September 1786 he was employed by Nepean, who was in charge of the Secret Service relating to the Bourbon Powers, France and Spain, to spy on the French naval arsenals at Toulon and other ports. There was fear that Britain would soon be at war with these powers as a consequence of the Batavian Revolution in the Netherlands.
Portraits of the time depict Phillip as shorter than average, with an olive complexion, dark eyes and a "smooth pear of a skull." His features were dominated by a large and fleshy nose, and by a pronounced lower lip.
Colonial service.
At this time, Lord Sandwich, together with the President of the Royal Society, Sir Joseph Banks, was advocating establishment of a British colony in New South Wales. A colony there would be of great assistance to the British Navy in facilitating attacks on the Spanish possessions in Chile and Peru, as Banks's collaborators, James Matra, Captain Sir George Young and Sir John Call pointed out in written proposals on the subject. The British Government took the decision to settle what is now Australia and found the Botany Bay colony in mid-1786. Lord Sydney, as Secretary of State for the Home Office, was the minister in charge, and in September 1786 he appointed Phillip commodore of the fleet which was to transport the convicts and soldiers who were to be the new settlers to Botany Bay. Upon arrival there, Phillip was to assume the powers of Captain General and Governor in Chief of the new colony. A subsidiary colony was to be founded on Norfolk Island, as recommended by Sir John Call, to take advantage for naval purposes of that island's native flax and timber.
In October 1786, Phillip was appointed captain of and named Governor-designate of New South Wales, the proposed British colony on the east coast of Australia, by Lord Sydney, the Home Secretary.
Phillip had a very difficult time assembling the fleet which was to make the eight-month sea voyage to Australia. Everything a new colony might need had to be taken, since Phillip had no real idea of what he might find when he got there. There were few funds available for equipping the expedition. His suggestion that people with experience in farming, building and crafts be included was rejected. Most of the 772 convicts (of whom 732 survived the voyage) were petty thieves from the London slums. Phillip was accompanied by a contingent of marines and a handful of other officers who were to administer the colony.
The 11 ships of the First Fleet set sail on 13 May 1787. The leading ship, reached Botany Bay setting up camp on the Kurnell Peninsula, on 18 January 1788. Phillip soon decided that this site, chosen on the recommendation of Sir Joseph Banks, who had accompanied James Cook in 1770, was not suitable, since it had poor soil, no secure anchorage and no reliable water source. After some exploration Phillip decided to go on to Port Jackson, and on 26 January the marines and convicts landed at Sydney Cove, which Phillip named after Lord Sydney.
Governor of New South Wales.
Shortly after landing and establishing the settlement at Port Jackson, on 15 February 1788, Phillip sent Lieutenant Philip Gidley King with 8 free men and a number of convicts to establish the second British colony in the Pacific at Norfolk Island. This was partly in response to a perceived threat of losing Norfolk Island to the French and partly to establish an alternative food source for the new colony.
The early days of the settlement were chaotic and difficult. With limited supplies, the cultivation of food was imperative, but the soils around Sydney were poor, the climate was unfamiliar, and moreover very few of the convicts had any knowledge of agriculture. The colony was on the verge of outright starvation for an extended period. The marines, poorly disciplined themselves in many cases, were not interested in convict discipline. Almost at once, therefore, Phillip had to appoint overseers from among the ranks of the convicts to get the others working. This was the beginning of the process of convict emancipation which was to culminate in the reforms of Lachlan Macquarie after 1811.
Phillip showed in other ways that he recognised that New South Wales could not be run simply as a prison camp. Lord Sydney, often criticised as an ineffectual incompetent, had made one fundamental decision about the settlement that was to influence it from the start. Instead of just establishing it as a military prison, he provided for a civil administration, with courts of law. Two convicts, Henry and Susannah Kable, sought to sue Duncan Sinclair, the captain of "Alexander", for stealing their possessions during the voyage. Convicts in Britain had no right to sue, and Sinclair had boasted that he could not be sued by them. Despite this, the court found for the plaintiffs and ordered the captain to make restitution for the loss of their possessions.
Further, soon after Lord Sydney appointed him governor of New South Wales Arthur Phillip drew up a detailed memorandum of his plans for the proposed new colony. In one paragraph he wrote: "The laws of this country nglan will of course, be introduced in e South Wales, and there is one that I would wish to take place from the moment his Majesty's forces take possession of the country: That there can be no slavery in a free land, and consequently no slaves." Nevertheless, Phillip believed in discipline; floggings and hangings were commonplace, although Philip commuted many death sentences.
Phillip also had to adopt a policy towards the Eora Aboriginal people, who lived around the waters of Sydney Harbour. Phillip ordered that they must be well-treated, and that anyone killing Aboriginal people would be hanged. Phillip befriended an Eora man called Bennelong, and later took him to England. On the beach at Manly, a misunderstanding arose and Phillip was speared in the shoulder: but he ordered his men not to retaliate. Phillip went some way towards winning the trust of the Eora, although they remained wary of the settlers. Soon, a virulent disease, smallpox that was believed to be on account of the white settlers, and other European-introduced epidemics, ravaged the Eora population.
The Governor's main problem was with his own military officers, who wanted large grants of land, which Phillip had not been authorised to grant. Scurvy broke out, and in October 1788 Phillip had to send "Sirius" to Cape Town for supplies, and strict rationing was introduced, with thefts of food punished by hanging. Arthur Phillip quoted "The living conditions need to improve or my men won't work as hard, so I have come to a conclusion that I must hire surgeons to fix the convicts."
Stabilising the colony.
Phillip insisted that no retaliation be taken to avenge his own non-fatal spearing. Convict John MacIntyre had been fatally speared during a hunting expedition by unknown aborigines apparently without provocation. MacIntyre swore on his death bed that he had done them no harm, but marine officer Watkin Tench was suspicious of the claim. Tench was sent on a punitive expedition but finding no Aborigines other than Bennelong took no action.
Phillip, growing frustrated with the burdens of upholding a colony and his health suffering, resigned soon after this episode.
By 1790 the situation had stabilised. The population of about 2,000 was adequately housed and fresh food was being grown. Phillip assigned a convict, James Ruse, land at Rose Hill (now Parramatta) to establish proper farming, and when Ruse succeeded he received the first land grant in the colony. Other convicts followed his example. "Sirius" was wrecked in March 1790 at the satellite settlement of Norfolk Island, depriving Phillip of vital supplies. In June 1790 the Second Fleet arrived with hundreds more convicts, most of them too sick to work.
By December 1790 Phillip was ready to return to England, but the colony had largely been forgotten in London and no instructions reached him, so he carried on. In 1791 he was advised that the government would send out two convoys of convicts annually, plus adequate supplies. But July, when the vessels of the Third Fleet began to arrive, with 2,000 more convicts, food again ran short, and he had to send the ship "Atlantic" to Calcutta for supplies.
By 1792 the colony was well established, though Sydney remained an unplanned huddle of wooden huts and tents. The whaling industry was established, ships were visiting Sydney to trade, and convicts whose sentences had expired were taking up farming. John Macarthur and other officers were importing sheep and beginning to grow wool. The colony was still very short of skilled farmers, craftsmen and tradesmen, and the convicts continued to work as little as possible, even though they were working mainly to grow their own food.
In late 1792 Phillip, whose health was suffering, relinquished his governorship and sailed for England on the ship "Atlantic", taking with him many specimens of plants and animals. He also took Bennelong and his friend Yemmerrawanne, another young Indigenous Australian who, unlike Bennelong, would succumb to English weather and disease and not live to make the journey home. The European population of New South Wales at his departure was 4,221, of whom 3,099 were convicts. The early years of the colony had been years of struggle and hardship, but the worst was over, and there were no further famines in New South Wales. Phillip arrived in London in May 1793. He tendered his formal resignation and was granted a pension of £500 a year.
Later life.
Phillip's estranged wife, Margaret, had died in 1792 and was buried in St Beuno's Churchyard, Llanycil, Bala, Merionethshire. In 1794 Phillip married Isabella Whitehead, and lived for a time at Bath. His health gradually recovered and in 1796 he went back to sea, holding a series of commands and responsible posts in the wars against the French. In January 1799 he became a Rear-Admiral. In 1805, aged 67, he retired from the Navy with the rank of Admiral of the Blue, and spent most of the rest of his life at Bath. He continued to correspond with friends in New South Wales and to promote the colony's interests with government officials. He died in Bath in 1814.
Phillip was buried in St Nicholas's Church, Bathampton. Forgotten for many years, the grave was discovered in 1897 and the Premier of New South Wales, Sir Henry Parkes, had it restored. An annual service of remembrance is held here around Phillip's birthdate by the Britain–Australia Society to commemorate his life.
In 2007, Geoffrey Robertson QC alleged that Phillip's remains are no longer in St Nicholas Church, Bathampton and have been lost: "...Captain Arthur Phillip is not where the ledger stone says he is: it may be that he is buried somewhere outside, it may simply be that he is simply lost. But he is not where Australians have been led to believe that he now lies."
Legacy.
A number of places in Australia bear Phillip's name, including Port Phillip, Phillip Island (Victoria), Phillip Island (Norfolk Island), the federal electorate of Phillip (1949–1993), the suburb of Phillip in Canberra, the Governor Phillip Tower building in Sydney, and many streets, parks and schools including a state high school in Parramatta.
A monument to Phillip in Bath Abbey Church was unveiled in 1937. Another was unveiled at St Mildred's Church, Bread St, London, in 1932; that church was destroyed in the London Blitz in 1940, but the principal elements of the monument were re-erected at the west end of Watling Street, near Saint Paul's Cathedral, in 1968. A different bust and memorial is inside the nearby church of St Mary-le-Bow. There is a statue of him in the Botanic Gardens, Sydney. There is a portrait of him by Francis Wheatley in the National Portrait Gallery, London.
Percival Serle wrote of Phillip in his "Dictionary of Australian Biography": 
200th anniversary.
As part of a series of events on the bicentenary of his death, a memorial was dedicated in Westminster Abbey on 9 July 2014. In the service the Dean of Westminster, Very Reverend Dr John Hall, described Phillip as: "This modest, yet world-class seaman, linguist, and patriot, whose selfless service laid the secure foundations on which was developed the Commonwealth of Australia, will always be remembered and honoured alongside other pioneers and inventors here in the Nave: David Livingstone, Thomas Cochrane, and Isaac Newton."
A similar memorial was unveiled by the outgoing 37th Governor of New South Wales, Marie Bashir, in St James' Church, Sydney on 31 August 2014.
A bronze bust was installed at the Museum of Sydney and a full-day symposium planned on his contributions to the founding of modern Australia.
Popular culture.
Phillip is a prominent character in Timberlake Wertenbaker's play "Our Country's Good", in which he commissions Lieutenant Ralph Clark to stage a production of "The Recruiting Officer". He is shown as compassionate and just, but receives little support from his fellow officers. He is also prominent in "Banished" and is played by David Wenham.
Phillip is referred to in the John Williamson song "Chains around my ankle".
Kate Grenville's 2008 novel "The Lieutenant" portrays Phillip through the character Commodore James Gilbert.

</doc>
<doc id="2564" url="https://en.wikipedia.org/wiki?curid=2564" title="April 10">
April 10


</doc>
<doc id="2573" url="https://en.wikipedia.org/wiki?curid=2573" title="Angus">
Angus

Angus () is one of the 32 local government council areas of Scotland, a registration county and a lieutenancy area. The council area borders Aberdeenshire, Dundee City and Perth and Kinross. Main industries include agriculture and fishing. Global pharmaceuticals company GSK has a significant presence in Montrose in the north of the county.
Angus was historically a county, known officially as Forfarshire from the 18th century until 1928. It remains a registration county and a lieutenancy area. In 1975 its administrative functions were transferred to the council district of the Tayside Region, and in 1995 further reform resulted in the establishment of the unitary Angus Council.
History.
Prehistory.
The area that now comprises Angus has been occupied since at least the Neolithic period. Material taken from postholes from an enclosure at Douglasmuir, near Friockheim, about five miles north of Arbroath has been radiocarbon dated to around 3500 BC. The function of the enclosure is unknown, but may have been for agriculture or for ceremonial purposes.
Bronze age archaeology is to be found in abundance in the area. Examples include the short-cist burials found near West Newbigging, about a mile to the North of the town. These burials included pottery urns, a pair of silver discs and a gold armlet. Iron Age archaeology is also well represented, for example in the souterrain nearby Warddykes cemetery and at West Grange of Conan, as well as the better-known examples at Carlungie and Ardestie.
Mediaeval history.
The county is traditionally associated with the Pictish kingdom of Circinn, which is thought to have encompassed Angus and the Mearns. Bordering it were the kingdoms of Ce (Mar and Buchan) to the North, Fotla (Atholl) to the West, and Fib (Fife) to the South.
The most visible remnants of the Pictish age are the numerous sculptured stones that can be found throughout Angus. Of particular note are the collections found at Aberlemno, St Vigeans, Kirriemuir and Monifieth.
Angus shares borders with Kincardineshire to the north-east, Aberdeenshire to the north and Perthshire to the west. Southwards, it faces Fife across the Firth of Tay.
Angus is marketed as the birthplace of Scotland. The signing of the Declaration of Arbroath at Arbroath Abbey in 1320 marked Scotland's establishment as an independent nation. It is an area of rich history from Pictish times onwards. Notable historic sites in addition to Arbroath Abbey include Glamis Castle, Arbroath Signal Tower museum and the Bell Rock Light House.
Main industries include agriculture and fishing. Global pharmaceuticals company GSK has a significant presence in Montrose in the north of the county.
Demography.
Population structure.
In the 2001 census the population of Angus was recorded as 108,400. 20.14% were under the age of 16, 63.15% were between 16 and 65 and 18.05% were aged 65 or above.
Of the 16 to 74 age group, 32.84% had no formal qualifications, 27.08% were educated to 'O' Grade/Standard Grade level, 14.38% to Higher level, 7.64% to HND or equivalent level and 18.06% to degree level.
Language in Angus.
The most recent available census results (2001) show that Gaelic is spoken by 0.45% of the Angus population. This, similar to other lowland areas, is lower than the national average of 1.16%. These figures are self-reported and are not broken down into levels of fluency.
Historically, the dominant language in Angus was Pictish until the sixth to seventh centuries AD when the area became progressively gaelicised, with Pictish extinct by the mid-ninth century. Gaelic/Middle Irish began to retreat from lowland areas in the late-eleventh century and was absent from the Eastern lowlands by the fourteenth century. It was replaced there by Middle Scots, the contemporary local South Northern dialect of Modern Scots, while Gaelic persisted as a majority language in the highland Glens until the 19th century. Scottish English is now increasingly replacing Scots.
Angus Council are planning to raise the status of Gaelic in the county by adopting a series of measures, including bilingual road signage, communications, vehicle livery and staffing.
Government.
Local government.
Angus () is one of the 32 local government council areas of Scotland. In 1996, two-tier local government council was abolished and Angus was established as one of the replacement single-tier Council Areas.
The boundaries of the present council area are the same as those of the county minus the City of Dundee.
The council area borders Aberdeenshire, Dundee City and Perth and Kinross.
Parliamentary representation.
Areas similar to that of the council area are covered by the Angus Westminster constituency for the UK Parliament, and the area is represented at the Scottish Parliament by the Angus and North Tayside Holyrood constituencies.
Geography.
Angus can be split into three geographic areas. To the north and west, the topography is mountainous. This is the area of the five Angus Glens, which is sparsely populated and where the main industry is hill-farming. To the south and east the topography consists of rolling hills bordering the sea. This area is well populated, with the larger towns. In between lies Strathmore ("the Great Valley"), which is a fertile agricultural area noted for the growing of potatoes, soft fruit and the raising of Angus cattle. Montrose in the north east of the county is notable for its tidal basin.
Surnames.
Most common surnames in Angus (Forfarshire) at the time of the United Kingdom Census of 1881:

</doc>
