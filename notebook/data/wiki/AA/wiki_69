<doc id="9510" url="https://en.wikipedia.org/wiki?curid=9510" title="Electronic music">
Electronic music

Electronic music is music that employs electronic musical instruments and electronic music technology in its production, an electronic musician being a musician who composes and/or performs such music. In general a distinction can be made between sound produced using electromechanical means and that produced using electronic technology. Examples of electromechanical sound producing devices include the telharmonium, Hammond organ, and the electric guitar. Purely electronic sound production can be achieved using devices such as the theremin, sound synthesizer, and computer.
The first electronic devices for performing music were developed at the end of the 19th century, and shortly afterward Italian Futurists explored sounds that had previously not been considered musical. During the 1920s and 1930s, electronic instruments were introduced and the first compositions for electronic instruments were composed. By the 1940s, magnetic audio tape allowed musicians to tape sounds and then modify them by changing the tape speed or direction, leading to the development of electroacoustic tape music in the 1940s, in Egypt and France. Musique concrète, created in Paris in 1948, was based on editing together recorded fragments of natural and industrial sounds. Music produced solely from electronic generators was first produced in Germany in 1953. Electronic music was also created in Japan and the United States beginning in the 1950s. An important new development was the advent of computers for the purpose of composing music. Algorithmic composition was first demonstrated in Australia in 1951.
In America and Europe, live electronics were pioneered in the early 1960s. During the 1970s to early 1980s, the monophonic Minimoog became once the most widely used synthesizer at that time in both popular and electronic art music.
In the 1970s, electronic music began having a significant influence on popular music, with the adoption of polyphonic synthesizers such as the Yamaha GX-1 and Prophet-5, electronic drums, and drum machines such as the Roland CR-78, through the emergence of genres such as krautrock, disco, new wave and synthpop. In the 1980s, electronic music became more dominant in popular music, with a greater reliance on synthesizers, and the adoption of programmable drum machines such as the Roland TR-808 and TR-909 and the Linn LM-1, and bass synthesizers such as the Roland TB-303. In the early 1980s, a group of musicians and music merchants developed the Musical Instrument Digital Interface (MIDI), and Yamaha released the first FM digital synthesizer, the DX7.
Electronically produced music became prevalent in the popular domain by the 1990s, because of the advent of affordable music technology. Contemporary electronic music includes many varieties and ranges from experimental art music to popular forms such as electronic dance music.Today, pop electronic music is most recognizable in its 4/4 form and vastly more connected with the mainstream culture as opposed to its preceding forms which were specialized to niche markets. 
Origins: late 19th century to early 20th century.
Lee de Forest's 1906 invention, the triode audion tube, later had a profound effect on electronic music. It was the first thermionic valve, or vacuum tube, and led to circuits that could create and amplify audio signals, broadcast radio waves, compute values, and perform many other functions.
Before electronic music, there was a growing desire for composers to use emerging technologies for musical purposes. Several instruments were created that employed electromechanical designs and they paved the way for the later emergence of electronic instruments. An electromechanical instrument called the Telharmonium (sometimes Teleharmonium or Dynamophone) was developed by Thaddeus Cahill in the years 1898 to 1912. However, simple inconvenience hindered the adoption of the Telharmonium, due to its immense size. One early electronic instrument often mentioned may be the theremin, invented by Professor Léon Theremin circa 1919–1920. Other early electronic instruments include the Audion Piano invented in 1915 by Lee de Forest who was inventor of triode audion as mentioned above, the "Croix Sonore", invented in 1926 by Nikolai Obukhov, and the ondes Martenot, which was most famously used in the "Turangalîla-Symphonie" by Olivier Messiaen as well as other works by him. The ondes Martenot was also used by other, primarily French, composers such as André Jolivet.
"Sketch of a New Esthetic of Music".
In 1907, just a year after the invention of the triode audion, Ferruccio Busoni published "Sketch of a New Esthetic of Music", which discussed the use of electrical and other new sound sources in future music. He wrote of the future of microtonal scales in music, made possible by Cahill's Dynamophone: "Only a long and careful series of experiments, and a continued training of the ear, can render this unfamiliar material approachable and plastic for the coming generation, and for Art."
Also in the "Sketch of a New Esthetic of Music", Busoni states:
Through this writing, as well as personal contact, Busoni had a profound effect on many musicians and composers, perhaps most notably on his pupil, Edgard Varèse, who said: 
Futurists.
In Italy, the Futurists approached the changing musical aesthetic from a different angle. A major thrust of the Futurist philosophy was to value "noise," and to place artistic and expressive value on sounds that had previously not been considered even remotely musical. Balilla Pratella's "Technical Manifesto of Futurist Music" (1911) states that their credo is: "To present the musical soul of the masses, of the great factories, of the railways, of the transatlantic liners, of the battleships, of the automobiles and airplanes. To add to the great central themes of the musical poem the domain of the machine and the victorious kingdom of Electricity."
On 11 March 1913, futurist Luigi Russolo published his manifesto "The Art of Noises". In 1914, he held the first "art-of-noises" concert in Milan on April 21. This used his Intonarumori, described by Russolo as "acoustical noise-instruments, whose sounds (howls, roars, shuffles, gurgles, etc.) were hand-activated and projected by horns and megaphones."
In June, similar concerts were held in Paris.
The 1920s to 1930s.
This decade brought a wealth of early electronic instruments and the first compositions for electronic instruments. The first new electronic musical instrument of the decade, the etherphone ( or ætherphone ), was created by Léon Theremin (born Lev Termen) between 1919 and 1920 in Leningrad, though it was eventually renamed the theremin ( though sometimes also known as the thereminophone or termenvox/thereminvox ). This led to the first compositions for electronic instruments, as opposed to noisemakers and re-purposed machines. In 1929, Joseph Schillinger composed "First Airphonic Suite for Theremin and Orchestra", premièred with the Cleveland Orchestra with Leon Theremin as soloist.
In addition to the theremin, the ondes Martenot was invented in 1928 by Maurice Martenot, who debuted it in Paris.
Recording of sounds made a leap in 1927, when American inventor J. A. O'Neill developed a recording device that used magnetically coated ribbon. However, this was a commercial failure. Two years later, Laurens Hammond established his company for the manufacture of electronic instruments. He went on to produce the Hammond organ, which was based on the principles of the Telharmonium, along with other developments including early reverberation units. Hammond (along with John Hanert and C. N. Williams) would also go on to invent another electronic instrument, the Novachord, which Hammond's company manufactured from 1939–1942.
The method of photo-optic sound recording used in cinematography made it possible to obtain a visible image of a sound wave, as well as to realize the opposite goal—synthesizing a sound from an artificially drawn sound wave.
In this same period, experiments began with sound art, early practitioners of which include Tristan Tzara, Kurt Schwitters, Filippo Tommaso Marinetti, and others. The animation film "L'Idee" (1932) by Berthold Bartosch featured a score composed by Arthur Honegger with ondes Martenot and chamber orchestra.
Development: 1940s to 1950s.
Electroacoustic tape music.
Low-fidelity magnetic wire recorders had been in use since around 1900 and in the early 1930s the movie industry began to convert to the new optical sound-on-film recording systems based on the photoelectric cell. It was around this time that the German electronics company AEG developed the first practical audio tape recorder, the "Magnetophon" K-1, which was unveiled at the Berlin Radio Show in August 1935.
During World War II, Walter Weber rediscovered and applied the AC biasing technique, which dramatically improved the fidelity of magnetic recording by adding an inaudible high-frequency tone. It extended the 1941 'K4' Magnetophon frequency curve to 10 kHz and improved the dynamic range up to 60 dB, surpassing all known recording systems at that time.
As early as 1942 AEG was making test recordings in stereo. However these devices and techniques remained a secret outside Germany until the end of WWII, when captured Magnetophon recorders and reels of Farben ferric-oxide recording tape were brought back to the United States by Jack Mullin and others. These captured recorders and tapes were the basis for the development of America's first commercially made professional tape recorder, the Model 200, manufactured by the American Ampex company with support from entertainer Bing Crosby, who became one of the first performers to record radio broadcasts and studio master recordings on tape.
Magnetic audio tape opened up a vast new range of sonic possibilities to musicians, composers, producers and engineers. Audio tape was relatively cheap and very reliable, and its fidelity of reproduction was better than any audio medium to date. Most importantly, unlike discs, it offered the same plasticity of use as film. Tape can be slowed down, sped up or even run backwards during recording or playback, with often startling effect. It can be physically edited in much the same way as film, allowing for unwanted sections of a recording to be seamlessly removed or replaced; likewise, segments of tape from other sources can be edited in. Tape can also be joined to form endless loops that continually play repeated patterns of pre-recorded material. Audio amplification and mixing equipment further expanded tape's capabilities as a production medium, allowing multiple pre-taped recordings (and/or live sounds, speech or music) to be mixed together and simultaneously recorded onto another tape with relatively little loss of fidelity. Another unforeseen windfall was that tape recorders can be relatively easily modified to become echo machines that produce complex, controllable, high-quality echo and reverberation effects (most of which would be practically impossible to achieve by mechanical means).
The spread of tape recorders eventually led to the development of electroacoustic tape music. The first known example was composed in 1944 by Halim El-Dabh, a student at Cairo, Egypt. He recorded the sounds of an ancient "zaar" ceremony using a cumbersome wire recorder and at the Middle East Radio studios processed the material using reverberation, echo, voltage controls, and re-recording. The resulting work was entitled "The Expression of Zaar" and it was presented in 1944 at an art gallery event in Cairo. While his initial experiments in tape based composition were not widely known outside of Egypt at the time, El-Dabh is also notable for his later work in electronic music at the Columbia-Princeton Electronic Music Center in the late 1950s.
Musique concrète.
It wasn't long before composers in Paris also began using the tape recorder to develop a new technique for composition called "musique concrète". This technique involved editing together recorded fragments of natural and industrial sounds. The first pieces of "musique concrète" in Paris were assembled by Pierre Schaeffer, who went on to collaborate with Pierre Henry.
On 5 October 1948, Radiodiffusion Française (RDF) broadcast composer Pierre Schaeffer's "Etude aux chemins de fer". This was the first "movement" of "Cinq études de bruits", and marked the beginning of studio realizations and musique concrète (or acousmatic art). Schaeffer employed a disk-cutting lathe, four turntables, a four-channel mixer, filters, an echo chamber, and a mobile recording unit. Not long after this, Henry began collaborating with Schaeffer, a partnership that would have profound and lasting effects on the direction of electronic music. Another associate of Schaeffer, Edgard Varèse, began work on "Déserts", a work for chamber orchestra and tape. The tape parts were created at Pierre Schaeffer's studio, and were later revised at Columbia University.
In 1950, Schaeffer gave the first public (non-broadcast) concert of musique concrète at the École Normale de Musique de Paris. "Schaeffer used a PA system, several turntables, and mixers. The performance did not go well, as creating live montages with turntables had never been done before." Later that same year, Pierre Henry collaborated with Schaeffer on "Symphonie pour un homme seul" (1950) the first major work of musique concrete. In Paris in 1951, in what was to become an important worldwide trend, RTF established the first studio for the production of electronic music. Also in 1951, Schaeffer and Henry produced an opera, "Orpheus", for concrete sounds and voices.
Elektronische Musik.
Karlheinz Stockhausen worked briefly in Schaeffer's studio in 1952, and afterward for many years at the WDR Cologne's Studio for Electronic Music.
In Cologne, what would become the most famous electronic music studio in the world was officially opened at the radio studios of the NWDR in 1953, though it had been in the planning stages as early as 1950 and early compositions were made and broadcast in 1951. The brain child of Werner Meyer-Eppler, Robert Beyer, and Herbert Eimert (who became its first director), the studio was soon joined by Karlheinz Stockhausen and Gottfried Michael Koenig. In his 1949 thesis "Elektronische Klangerzeugung: Elektronische Musik und Synthetische Sprache", Meyer-Eppler conceived the idea to synthesize music entirely from electronically produced signals; in this way, "elektronische Musik" was sharply differentiated from French "musique concrète", which used sounds recorded from acoustical sources.
"With Stockhausen and Mauricio Kagel in residence, it became a year-round hive of charismatic avante-gardism " on two occasions combining electronically generated sounds with relatively conventional orchestras—in "Mixtur" (1964) and "Hymnen, dritte Region mit Orchester" (1967). Stockhausen stated that his listeners had told him his electronic music gave them an experience of "outer space," sensations of flying, or being in a "fantastic dream world".
More recently, Stockhausen turned to producing electronic music in his own studio in Kürten, his last work in the medium being "Cosmic Pulses" (2007).
Japanese electronic music.
While early electric instruments such as the ondes Martenot, theremin and trautonium were little known in Japan prior to World War II, certain composers such as Minao Shibata had known about them at the time. Several years after the end of World War II, musicians in Japan began experimenting with electronic music, resulting in some of the most dedicated efforts due to institutional sponsorship enabling composers to experiment with the latest audio recording and processing equipment. These efforts represented an infusion of Asian music into the emerging genre and would eventually pave the way for Japan's domination in the development of music technology several decades later.
Following the foundation of electronics company Sony (then called Tokyo Tsushin Kogyo K.K.) in 1946, two Japanese composers, Toru Takemitsu and Minao Shibata, independently wrote about the possible use of electronic technology to produce music during the late 1940s. In 1948, Takemitsu conceived of a technology that would "bring noise into tempered musical tones inside a busy small tube," an idea similar to Pierre Schaeffer's musique concrète the same year, which Takemitsu was unaware of until several years later. In 1949, Shibata wrote about his concept of "a musical instrument with very high performance" that can "synthesize any kind of sound waves" and is "operated very easily," predicting that with such an instrument, "the music scene will be changed drastically." The same year, Sony developed the magnetic tape recorder G-Type, which became a popular recording device in courtrooms and government offices, leading to Sony releasing the H-Type for home use by 1951.
In 1950, the Jikken Kōbō (Experimental Workshop) electronic music studio would be founded by a group of musicians in order to produce experimental electronic music using Sony tape recorders. It included musicians such as Toru Takemitsu, Kuniharu Akiyama, and Joji Yuasa, and was supported by Sony, which offered them access to the latest audio technology, hired Takemitsu to compose electronic tape music to demonstrate their tape recorders, and sponsored concerts. The first electronic tape music from the group were "Toraware no Onna" ("Imprisoned Woman") and "Piece B", completed in 1951 by Kuniharu Akiyama. Many of the electroacoustic tape pieces they produced were usually used as incidental music for radio, film, and theatre. They also held concerts such as 1953's "Experimental Workshop, 5th Exhibition", which employed an 'auto-slide', a machine developed by Sony that made it possible to synchronize a slide show with a soundtrack recorded on tape; they used the same device to produce the concert's tape music at the Sony studio. The concert, along with the experimental electroacoustic tape music they produced, anticipated the introduction of musique concrète in Japan later that year. Beyond the Jikken Kobo, several other composers such as Yasushi Akutagawa, Saburo Tominaga and Shiro Fukai were also experimenting with producing radiophonic tape music between 1952 and 1953.
Japan was introduced to musique concrète through Toshiro Mayuzumi, who in 1952 attended a Schaeffer concert in Paris. On his return to Japan, he experimented with a short tape music piece for the 1952 comedy film "Carmen Jyunjyosu" ("Carmen With Pure Heart") and then produced "X, Y, Z for Musique Concrète", broadcast by the JOQR radio station in 1953. Mayuzumi also composed another musique concrète piece for Yukio Mishima's 1954 radio drama "Boxing". Schaeffer's French concept of "objet sonore" ("sound object"), however, was not influential among Japanese composers, whose main interest in music technology was instead to, according to Mayuzumi, overcome the restrictions of "the materials or the boundary of human performance." This led to several Japanese electroacoustic musicians making use of serialism and twelve-tone techniques, evident in Yoshirō Irino's 1951 dodecaphonic piece "Concerto da
Camera", in the organization of electronic sounds in Mayuzumi's "X, Y, Z for Musique Concrète", and later in Shibata's electronic music by 1956.
Following as a model the NWDR Cologne studio, Japan's NHK company established an electronic-music studio in Tokyo in 1955, which became one of the world's leading electronic music facilities. The NHK Studio was equipped with technologies such as tone-generating and audio processing equipment, recording and radiophonic equipment, ondes Martenot, Monochord and Melochord, sine-wave oscillators, tape recorders, ring modulators, band-pass filters, and four- and eight-channel mixers. Musicians associated with the studio included Toshiro Mayuzumi, Minao Shibata, Joji Yuasa, Toshi Ichiyanagi, and Toru Takemitsu. The studio's first electronic compositions were completed in 1955, including Mayuzumi's five-minute pieces "Studie I: Music for Sine Wave by Proportion of Prime Number", "Music for Modulated Wave by Proportion of Prime Number" and "Invention for Square Wave and Sawtooth Wave" produced using the studio's various tone-generating capabilities, and Shibata's 20-minute stereo piece "Musique Concrète for Stereophonic Broadcast".
Ikutaro Kakehashi founded a repair shop called Kakehashi Watch Shop in the late 1940s repairing watches and radios, and then in 1954 founded Kakehashi Musen ("Kakehashi Radio"), which by 1960 grew into the company Ace Tone, and by 1972 became the Roland Corporation. Kakehashi began producing electronic musical instruments in 1955, with the aim of creating devices that could produce monophonic melodies. During the late 1950s, he produced theremins, ondes Martenots, and electronic keyboards, and by 1959, a Hawaiian guitar amplifier and electronic organs.
In the 1970's the composer Isao Tomita realized and arranged the music of Debussy, Ravel and Mussorgsky in electronic arrangements. He nominated three Grammys for his album "Snowflakes Are Dancing" in 1974.
American electronic music.
In the United States, electronic music was being created as early as 1939, when John Cage published "Imaginary Landscape, No. 1", using two variable-speed turntables, frequency recordings, muted piano, and cymbal, but no electronic means of production. Cage composed five more "Imaginary Landscapes" between 1942 and 1952 (one withdrawn), mostly for percussion ensemble, though No. 4 is for twelve radios and No. 5, written in 1952, uses 42 recordings and is to be realized as a magnetic tape. According to Otto Luening, Cage also performed a "William Mix" at Donaueschingen in 1954, using eight loudspeakers, three years after his alleged collaboration. "Williams Mix" was a success at the Donaueschingen Festival, where it made a "strong impression".
The Music for Magnetic Tape Project was formed by members of the New York School (John Cage, Earle Brown, Christian Wolff, David Tudor, and Morton Feldman), and lasted three years until 1954. Cage wrote of this collaboration: "In this social darkness, therefore, the work of Earle Brown, Morton Feldman, and Christian Wolff continues to present a brilliant light, for the reason that at the several points of notation, performance, and audition, action is provocative."
Cage completed "Williams Mix" in 1953 while working with the Music for Magnetic Tape Project. The group had no permanent facility, and had to rely on borrowed time in commercial sound studios, including the studio of Louis and Bebe Barron.
Columbia-Princeton Center.
In the same year Columbia University purchased its first tape recorder—a professional Ampex machine—for the purpose of recording concerts. Vladimir Ussachevsky, who was on the music faculty of Columbia University, was placed in charge of the device, and almost immediately began experimenting with it.
Herbert Russcol writes: "Soon he was intrigued with the new sonorities he could achieve by recording musical instruments and then superimposing them on one another." Ussachevsky said later: "I suddenly realized that the tape recorder could be treated as an instrument of sound transformation." On Thursday, May 8, 1952, Ussachevsky presented several demonstrations of tape music/effects that he created at his Composers Forum, in the McMillin Theatre at Columbia University. These included "Transposition, Reverberation, Experiment, Composition", and "Underwater Valse". In an interview, he stated: "I presented a few examples of my discovery in a public concert in New York together with other compositions I had written for conventional instruments."
Otto Luening, who had attended this concert, remarked: "The equipment at his disposal consisted of an Ampex tape recorder . . . and a simple box-like device designed by the brilliant young engineer, Peter Mauzey, to create feedback, a form of mechanical reverberation. Other equipment was borrowed or purchased with personal funds."
Just three months later, in August 1952, Ussachevsky traveled to Bennington, Vermont at Luening's invitation to present his experiments. There, the two collaborated on various pieces. Luening described the event: "Equipped with earphones and a flute, I began developing my first tape-recorder composition. Both of us were fluent improvisors and the medium fired our imaginations."
They played some early pieces informally at a party, where "a number of composers almost solemnly congratulated us saying, 'This is it' ('it' meaning the music of the future)."
Word quickly reached New York City. Oliver Daniel telephoned and invited the pair to "produce a group of short compositions for the October concert sponsored by the American Composers Alliance and Broadcast Music, Inc., under the direction of Leopold Stokowski at the Museum of Modern Art in New York. After some hesitation, we agreed. . . . Henry Cowell placed his home and studio in Woodstock, New York, at our disposal. With the borrowed equipment in the back of Ussachevsky's car, we left Bennington for Woodstock and stayed two weeks. . . . In late September, 1952, the travelling laboratory reached Ussachevsky's living room in New York, where we eventually completed the compositions."
Two months later, on October 28, Vladimir Ussachevsky and Otto Luening presented the first Tape Music concert in the United States. The concert included Luening's "Fantasy in Space" (1952)—"an impressionistic virtuoso piece" using manipulated recordings of flute—and "Low Speed" (1952), an "exotic composition that took the flute far below its natural range." Both pieces were created at the home of Henry Cowell in Woodstock, NY. After several concerts caused a sensation in New York City, Ussachevsky and Luening were invited onto a live broadcast of NBC's Today Show to do an interview demonstration—the first televised electroacoustic performance. Luening described the event: "I improvised some lut sequences for the tape recorder. Ussachevsky then and there put them through electronic transformations."
1954 saw the advent of what would now be considered authentic electric plus acoustic compositions—acoustic instrumentation augmented/accompanied by recordings of manipulated and/or electronically generated sound. Three major works were premiered that year: Varèse's "Déserts", for chamber ensemble and tape sounds, and two works by Luening and Ussachevsky: "Rhapsodic Variations for the Louisville Symphony" and "A Poem in Cycles and Bells", both for orchestra and tape. Because he had been working at Schaeffer's studio, the tape part for Varèse's work contains much more concrete sounds than electronic. "A group made up of wind instruments, percussion and piano alternates with the mutated sounds of factory noises and ship sirens and motors, coming from two loudspeakers."
At the German premiere of "Déserts" in Hamburg, which was conducted by Bruno Maderna, the tape controls were operated by Karlheinz Stockhausen. The title "Déserts", suggested to Varèse not only, "all physical deserts (of sand, sea, snow, of outer space, of empty streets), but also the deserts in the mind of man; not only those stripped aspects of nature that suggest bareness, aloofness, timelessness, but also that remote inner space no telescope can reach, where man is alone, a world of mystery and essential loneliness."
In 1958, Columbia-Princeton developed the RCA Mark II Sound Synthesizer, the first programmable synthesizer. This device was actually a special-purpose, digitally controlled analogue computer, it was the first electronic music synthesizer in which a large range of sounds could not only be produced and sequenced but also be programmed by the user. This programming feature had a profound influence on the nature of Babbitt's electronic music. Prominent composers such as Vladimir Ussachevsky, Otto Luening, Milton Babbitt, Charles Wuorinen, Halim El-Dabh, Bülent Arel and Mario Davidovsky used the RCA Synthesizer extensively in various compositions. One of the most influential composers associated with the early years of the studio was Egypt's Halim El-Dabh who, after having developed the earliest known electronic tape music in 1944, became more famous for "Leiyla and the Poet", a 1959 series of electronic compositions that stood out for its immersion and seamless fusion of electronic and folk music, in contrast to the more mathematical approach used by serial composers of the time such as Babbitt. El-Dabh's "Leiyla and the Poet", released as part of the album "Columbia-Princeton Electronic Music Center" in 1961, would be cited as a strong influence by a number of musicians, ranging from Neil Rolnick, Charles Amirkhanian and Alice Shields to rock musicians Frank Zappa and The West Coast Pop Art Experimental Band.
Stochastic music.
An important new development was the advent of computers for the purpose of composing music, as opposed to manipulating or creating sounds. Iannis Xenakis began what is called "musique stochastique," or "stochastic music," which is a composing method that uses mathematical probability systems. Different probability algorithms were used to create a piece under a set of parameters. Xenakis used computers to compose pieces like "ST/4" for string quartet and "ST/48" for orchestra (both 1962), "Morsima-Amorsima", "ST/10", and "Atrées". He developed the computer system UPIC for translating graphical images into musical results and composed "Mycènes Alpha" (1978) with it.
Mid-to-late 1950s.
In 1954, Stockhausen composed his "Elektronische Studie II"—the first electronic piece to be published as a score. In 1955, more experimental and electronic studios began to appear. Notable were the creation of the Studio di fonologia musicale di Radio Milano, a studio at the NHK in Tokyo founded by Toshiro Mayuzumi, and the Philips studio at Eindhoven, the Netherlands, which moved to the University of Utrecht as the Institute of Sonology in 1960.
The score for "Forbidden Planet", by Louis and Bebe Barron, was entirely composed using custom built electronic circuits and tape recorders in 1956.
The world's first computer to play music was CSIRAC, which was designed and built by Trevor Pearcey and Maston Beard. Mathematician Geoff Hill programmed the CSIRAC to play popular musical melodies from the very early 1950s. In 1951 it publicly played the Colonel Bogey March, of which no known recordings exist. However, CSIRAC played standard repertoire and was not used to extend musical thinking or composition practice. CSIRAC was never recorded, but the music played was accurately reconstructed. The oldest known recordings of computer generated music were played by the Ferranti Mark 1 computer, a commercial version of the Baby Machine from the University of Manchester in the autumn of 1951. The music program was written by Christopher Strachey.
The impact of computers continued in 1956. Lejaren Hiller and Leonard Isaacson composed "Illiac Suite" for string quartet, the first complete work of computer-assisted composition using algorithmic composition. "... Hiller postulated that a computer could be taught the rules of a particular style and then called on to compose accordingly." Later developments included the work of Max Mathews at Bell Laboratories, who developed the influential MUSIC I program in 1957, one of the first computer programs to play electronic music. Vocoder technology was also a major development in this early era. In 1956, Stockhausen composed "Gesang der Jünglinge", the first major work of the Cologne studio, based on a text from the "Book of Daniel". An important technological development of that year was the invention of the Clavivox synthesizer by Raymond Scott with subassembly by Robert Moog.
Also in 1957, Kid Baltan (Dick Raaymakers) and Tom Dissevelt released their debut album, "Song Of The Second Moon", recorded at the Philips studio. The public remained interested in the new sounds being created around the world, as can be deduced by the inclusion of Varèse's "Poème électronique", which was played over four hundred loudspeakers at the Philips Pavilion of the 1958 Brussels World Fair. That same year, Mauricio Kagel, an Argentine composer, composed "Transición II". The work was realized at the WDR studio in Cologne. Two musicians performed on a piano, one in the traditional manner, the other playing on the strings, frame, and case. Two other performers used tape to unite the presentation of live sounds with the future of prerecorded materials from later on and its past of recordings made earlier in the performance.
Expansion: 1960s.
 These were fertile years for electronic music—not just for academia, but for independent artists as synthesizer technology became more accessible. By this time, a strong community of composers and musicians working with new sounds and instruments was established and growing. 1960 witnessed the composition of Luening's "Gargoyles" for violin and tape as well as the premiere of Stockhausen's "Kontakte" for electronic sounds, piano, and percussion. This piece existed in two versions—one for 4-channel tape, and the other for tape with human performers. "In "Kontakte", Stockhausen abandoned traditional musical form based on linear development and dramatic climax. This new approach, which he termed 'moment form,' resembles the 'cinematic splice' techniques in early twentieth century film."
The theremin had been in use since the 1920s but it attained a degree of popular recognition through its use in science-fiction film soundtrack music in the 1950s (e.g., Bernard Herrmann's classic score for "The Day the Earth Stood Still").
In the UK in this period, the BBC Radiophonic Workshop (established in 1958) came to prominence, thanks in large measure to their work on the BBC science-fiction series "Doctor Who". One of the most influential British electronic artists in this period was Workshop staffer Delia Derbyshire, who is now famous for her 1963 electronic realisation of the iconic "Doctor Who" theme, composed by Ron Grainer.
In 1961 Josef Tal established the "Centre for Electronic Music in Israel" at The Hebrew University, and in 1962 Hugh Le Caine arrived in Jerusalem to install his "Creative Tape Recorder" in the centre. In the 1990s Tal conducted, together with Dr Shlomo Markel, in cooperation with the Technion – Israel Institute of Technology, and VolkswagenStiftung a research project (Talmark) aimed at the development of a novel musical notation system for electronic music.
Milton Babbitt composed his first electronic work using the synthesizer—his "Composition for Synthesizer" (1961)—which he created using the RCA synthesizer at the Columbia-Princeton Electronic Music Center.
The collaborations also occurred across oceans and continents. In 1961, Ussachevsky invited Varèse to the Columbia-Princeton Studio (CPEMC). Upon arrival, Varese embarked upon a revision of "Déserts". He was assisted by Mario Davidovsky and Bülent Arel.
The intense activity occurring at CPEMC and elsewhere inspired the establishment of the San Francisco Tape Music Center in 1963 by Morton Subotnick, with additional members Pauline Oliveros, Ramon Sender, Anthony Martin, and Terry Riley.
Later, the Center moved to Mills College, directed by Pauline Oliveros, where it is today known as the Center for Contemporary Music.
Simultaneously in San Francisco, composer Stan Shaff and equipment designer Doug McEachern, presented the first “Audium” concert at San Francisco State College (1962), followed by a work at the San Francisco Museum of Modern Art (1963), conceived of as in time, controlled movement of sound in space. Twelve speakers surrounded the audience, four speakers were mounted on a rotating, mobile-like construction above. In an SFMOMA performance the following year (1964), "San Francisco Chronicle" music critic Alfred Frankenstein commented, "the possibilities of the space-sound continuum have seldom been so extensively explored". In 1967, the first Audium, a "sound-space continuum" opened, holding weekly performances through 1970. In 1975, enabled by seed money from the National Endowment for the Arts, a new Audium opened, designed floor to ceiling for spatial sound composition and performance. “In contrast, there are composers who manipulated sound space by locating multiple speakers at various locations in a performance space and then switching or panning the sound between the sources. In this approach, the composition of spatial manipulation is dependent on the location of the speakers and usually exploits the acoustical properties of the enclosure. Examples include Varese's "Poeme Electronique" (tape music performed in the Philips Pavilion of the 1958 World Fair, Brussels) and Stanley Schaff's "Audium" installation, currently active in San Francisco” Through weekly programs (over 4,500 in 40 years), Shaff “sculpts” sound, performing now-digitized spatial works live through 176 speakers.
A well-known example of the use of Moog's full-sized Moog modular synthesizer is the "Switched-On Bach" album by Wendy Carlos, which triggered a craze for synthesizer music.
Along with the Moog modular synthesizer, other makes of this period included ARP and Buchla.
Pietro Grossi was an Italian pioneer of computer composition and tape music, who first experimented with electronic techniques in the early sixties. Grossi was a cellist and composer, born in Venice in 1917. He founded the S 2F M (Studio de Fonologia Musicale di Firenze) in 1963 in order to experiment with electronic sound and composition.
Computer music.
CSIRAC, the first computer to play music, did so publicly in August 1951. One of the first large-scale public demonstrations of computer music was a pre-recorded national radio broadcast on the NBC radio network program Monitor on February 10, 1962. In 1961, LaFarr Stuart programmed Iowa State University's CYCLONE computer (a derivative of the Illiac) to play simple, recognizable tunes through an amplified speaker that had been attached to the system originally for administrative and diagnostic purposes. An interview with Mr. Stuart accompanied his computer music.
Laurie Spiegel is also notable for her development of "Music Mouse—an Intelligent Instrument" (1986) for Macintosh, Amiga, and Atari computers. The intelligent-instrument name refers to the program's built-in knowledge of chord and scale convention and stylistic constraints. She continued to update the program through Macintosh OS 9, and , it remained available for purchase or demo download from her Web site.
The late 1950s, 1960s and 1970s also saw the development of large mainframe computer synthesis. Starting in 1957, Max Mathews of Bell Labs developed the MUSIC programs, culminating in MUSIC V, a direct digital synthesis language
Live electronics.
In Europe in 1964, Karlheinz Stockhausen composed "Mikrophonie I" for tam-tam, hand-held microphones, filters, and potentiometers, and "Mixtur" for orchestra, four sine-wave generators, and four ring modulators. In 1965 he composed "Mikrophonie II" for choir, Hammond organ, and ring modulators.
In 1966–67, Reed Ghazala discovered and began to teach "circuit bending"—the application of the creative short circuit, a process of chance short-circuiting, creating experimental electronic instruments, exploring sonic elements mainly of timbre and with less regard to pitch or rhythm, and influenced by John Cage’s aleatoric music concept.
Popularization: 1970s to early 1980s.
Synthesizers.
Released in 1970 by Moog Music, the Mini-Moog was among the first widely available, portable and relatively affordable synthesizers. It became once the most widely used synthesizer at that time in both popular and electronic art music.
Patrick Gleeson, playing live with Herbie Hancock in the beginning of the 1970s, pioneered the use of synthesizers in a touring context, where they were subject to stresses the early machines were not designed for.
In 1974, the WDR studio in Cologne acquired an EMS Synthi 100 synthesizer, which a number of composers used to produce notable electronic works—including Rolf Gehlhaar's "Fünf deutsche Tänze" (1975), Karlheinz Stockhausen's "Sirius" (1975–76), and John McGuire's "Pulse Music III" (1978).
The early 1980s saw the rise of bass synthesizers, the most influential being the Roland TB-303, a bass synthesizer and sequencer released in late 1981 that later became a fixture in electronic dance music, particularly acid house. One of the first to use it was Charanjit Singh in 1982, though it wouldn't be popularized until Phuture's "Acid Tracks" in 1987.
IRCAM, STEIM, and Elektronmusikstudion.
IRCAM in Paris became a major center for computer music research and realization and development of the Sogitec 4X computer system, featuring then revolutionary real-time digital signal processing. Pierre Boulez's "Répons" (1981) for 24 musicians and 6 soloists used the 4X to transform and route soloists to a loudspeaker system.
STEIM is a center for research and development of new musical instruments in the electronic performing arts, located in Amsterdam, Netherlands. STEIM has existed since 1969. It was founded by Misha Mengelberg, Louis Andriessen, Peter Schat, Dick Raaymakers, , Reinbert de Leeuw, and Konrad Boehmer. This group of Dutch composers had fought for the reformation of Amsterdam's feudal music structures; they insisted on Bruno Maderna's appointment as musical director of the Concertgebouw Orchestra and enforced the first public fundings for experimental and improvised electronic music in The Netherlands.
Elektronmusikstudion (EMS), formerly known as Electroacoustic Music in Sweden, is the Swedish national centre for electronic music and sound art. The research organisation started in 1964 and is based in Stockholm.
Rise of popular electronic music.
In the late 1960s, pop and rock musicians, including The Beach Boys and The Beatles, began to use electronic instruments, like the theremin and Mellotron, to supplement and define their sound. By the end of the decade, the Moog synthesizer took a leading place in the sound of emerging progressive rock with bands including Pink Floyd, Yes, Emerson, Lake & Palmer, and Genesis making them part of their sound. Instrumental prog rock was particularly significant in continental Europe, allowing bands like Kraftwerk, Tangerine Dream, Can, and Faust to circumvent the language barrier. Their synthesiser-heavy "krautrock", along with the work of Brian Eno (for a time the keyboard player with Roxy Music), would be a major influence on subsequent electronic rock.
Electronic rock was also produced by several Japanese musicians, including Isao Tomita's "Electric Samurai: Switched on Rock" (1972), which featured Moog synthesizer renditions of contemporary pop and rock songs, and Osamu Kitajima's progressive rock album "Benzaiten" (1974). The mid-1970s saw the rise of electronic art music musicians such as Jean Michel Jarre, Vangelis, and Tomita, who with Brian Eno were a significant influence on the development of new-age music.
After the arrival of punk rock, a form of basic electronic rock emerged, increasingly using new digital technology to replace other instruments. Pioneering bands included Ultravox with their 1977 single "Hiroshima Mon Amour", Yellow Magic Orchestra from Japan, Gary Numan, Depeche Mode, and The Human League. Yellow Magic Orchestra in particular helped pioneer synthpop with their self-titled album (1978) and "Solid State Survivor" (1979). The definition of MIDI and the development of digital audio made the development of purely electronic sounds much easier. These developments led to the growth of synthpop, which after it was adopted by the New Romantic movement, allowed synthesizers to dominate the pop and rock music of the early 80s. Key acts included Duran Duran, Spandau Ballet, A Flock of Seagulls, Culture Club, Talk Talk, Japan and the Eurythmics. Synthpop sometimes used synthesizers to replace all other instruments, until the style began to fall from popularity in the mid-1980s.
Sequencers and drum machines.
 Music sequencers began being used around the mid 20th century, and Tomita's albums in mid-1970s being later examples. In 1978, Yellow Magic Orchestra were using computer-based technology in conjunction with a synthesiser to produce popular music, making their early use of the microprocessor-based Roland MC-8 Microcomposer sequencer.
Drum machines, also known as rhythm machines, also began being used around the late-1950s, with a later example being Osamu Kitajima's progressive rock album "Benzaiten" (1974), which used a rhythm machine along with electronic drums and a synthesizer. In 1977, Ultravox's "Hiroshima Mon Amour" was one of the first singles to use the metronome-like percussion of a Roland TR-77 drum machine. In 1980, Roland Corporation released the TR-808, one of the first and most popular programmable drum machines. The first band to use it was Yellow Magic Orchestra in 1980, and it would later gain widespread popularity with the release of Marvin Gaye's "Sexual Healing" and Afrika Bambaataa's "Planet Rock" in 1982. The TR-808 was a fundamental tool in the later Detroit techno scene of the late 1980s, and was the drum machine of choice for Derrick May and Juan Atkins.
Birth of MIDI.
In 1980, a group of musicians and music merchants met to standardize an interface that new instruments could use to communicate control instructions with other instruments and computers. This standard was dubbed Musical Instrument Digital Interface (MIDI) and resulted from a collaboration between leading manufacturers, initially Sequential Circuits, Oberheim, Roland—and later, other participants that included Yamaha, Korg, and Kawai. A paper was authored by Dave Smith of Sequential Circuits and proposed to the Audio Engineering Society in 1981. Then, in August 1983, the MIDI Specification 1.0 was finalized.
MIDI technology allows a single keystroke, control wheel motion, pedal movement, or command from a microcomputer to activate every device in the studio remotely and in synchrony, with each device responding according to conditions predetermined by the composer.
MIDI instruments and software made powerful control of sophisticated instruments easily affordable by many studios and individuals. Acoustic sounds became reintegrated into studios via sampling and sampled-ROM-based instruments.
Miller Puckette developed graphic signal-processing software for 4X called Max (after Max Mathews) and later ported it to Macintosh (with Dave Zicarelli extending it for Opcode) for real-time MIDI control, bringing algorithmic composition availability to most composers with modest computer programming background.
Digital synthesis.
In 1975, the Japanese company Yamaha licensed the algorithms for frequency modulation synthesis (FM synthesis) from John Chowning, who had experimented with it at Stanford University since 1971. Yamaha's engineers began adapting Chowning's algorithm for use in a digital synthesizer, adding improvements such as the "key scaling" method to avoid the introduction of distortion that normally occurred in analog systems during frequency modulation. However, the first commercial digital synthesizer to be released would be the Australian Fairlight company's Fairlight CMI (Computer Musical Instrument) in 1979, as the first practical polyphonic digital synthesizer/sampler system.
In 1980, Yamaha eventually released the first FM digital synthesizer, the Yamaha GS-1, but at an expensive price. In 1983, Yamaha introduced the first stand-alone digital synthesizer, the DX-7, which also used FM synthesis and would become one of the best-selling synthesizers of all time. The DX-7 was known for its recognizable bright tonalities that was partly due to an overachieving sampling rate of 57 kHz.
Barry Vercoe describes one of his experiences with early computer sounds: 
Chiptunes.
The characteristic lo-fi sound of chip music was initially the result of early sound cards' technical limitations; however, the sound has since become sought after in its own right.
Late 1980s to 1990s.
Rise of dance music.
The trend has continued to the present day with modern nightclubs worldwide regularly playing electronic dance music (EDM). Nowadays, electronic dance music has radio stations, websites, and publications like "Mixmag" dedicated solely to the genre. Moreover, the genre has found commercial and cultural significance in the United States and North America, thanks to the wildly popular big room house/EDM sound that has been incorporated into U.S. pop music and the rise of large scale commercial raves such as Electric Daisy Carnival, Tomorrowland (festival) and Ultra Music Festival.
Advancements.
Other recent developments included the Tod Machover (MIT and IRCAM) composition "Begin Again Again" for "hypercello", an interactive system of sensors measuring physical movements of the cellist. Max Mathews developed the "Conductor" program for real-time tempo, dynamic and timbre control of a pre-input electronic score. Morton Subotnick released a multimedia CD-ROM "All My Hummingbirds Have Alibis".
2000s and 2010s.
In recent years, as computer technology has become more accessible and music software has advanced, interacting with music production technology is now possible using means that bear no relationship to traditional musical performance practices: for instance, laptop performance ("laptronica"), live coding and Algorave. In general, the term Live PA refers to any live performance of electronic music, whether with laptops, synthesizers, or other devices.
In the last decade, a number of software-based virtual studio environments have emerged, with products such as Propellerhead's Reason and Ableton Live finding popular appeal. Such tools provide viable and cost-effective alternatives to typical hardware-based production studios, and thanks to advances in microprocessor technology, it is now possible to create high quality music using little more than a single laptop computer. Such advances have democratized music creation, leading to a massive increase in the amount of home-produced electronic music available to the general public via the internet.
Artists can now also individuate their production practice by creating personalized software synthesizers, effects modules, and various composition environments. Devices that once existed exclusively in the hardware domain can easily have virtual counterparts. Some of the more popular software tools for achieving such ends are commercial releases such as Max/Msp and Reaktor and open source packages such as Csound, Pure Data, SuperCollider, and ChucK.
Circuit bending.
Circuit bending is the creative customization of the circuits within electronic devices such as low voltage, battery-powered guitar effects, children's toys and small digital synthesizers to create new musical or visual instruments and sound generators. Emphasizing spontaneity and randomness, the techniques of circuit bending have been commonly associated with noise music, though many more conventional contemporary musicians and musical groups have been known to experiment with "bent" instruments. Circuit bending usually involves dismantling the machine and adding components such as switches and potentiometers that alter the circuit. With the revived interest for analogue synthesizers, circuit bending became a cheap solution for many experimental musicians to create their own individual analogue sound generators. Nowadays many schematics can be found to build noise generators such as the Atari Punk Console or the Dub Siren as well as simple modifications for children toys such as the famous Speak & Spells that are often modified by circuit benders. Reed Ghazala has explored circuit bending with the Speak & Spell toy, and has held apprenticeships and workshops on circuit bending.

</doc>
<doc id="9514" url="https://en.wikipedia.org/wiki?curid=9514" title="Edvard Grieg">
Edvard Grieg

Edvard Hagerup Grieg (; 15 June 18434 September 1907) was a Norwegian composer and pianist. He is widely considered one of the leading Romantic era composers, and his music is part of the standard classical repertoire worldwide. His use and development of Norwegian folk music in his own compositions put the music of Norway in the international spectrum, as well as helping to develop a national identity, much as Jean Sibelius and Antonín Dvořák did in Finland and Bohemia, respectively. Grieg is regarded as simultaneously nationalistic and cosmopolitan in his orientation, for although born in Bergen and buried there, he traveled widely throughout Europe, and considered his music to express both the beauty of Norwegian rural life and the culture of Europe as a whole. He is the most celebrated person from the city of Bergen, with numerous statues depicting his image, and many cultural entities named after him: the city's largest building (Grieg Hall), its most advanced music school (Grieg Academy), its professional choir (Edvard Grieg Kor), and even some private companies that include its largest hotel (Quality Hotel Edvard Grieg), and a music technology developer (Grieg Music). The Edvard Grieg Museum in Troldhaugen (Grieg's former home in Bergen) is dedicated to his legacy.
Biography.
Edvard Hagerup Grieg was born in Bergen, Norway, on 15 June 1843. His parents were Alexander Grieg (1806–1875), a merchant and vice-consul in Bergen; and Gesine Judithe Hagerup (1814–1875), a music teacher and daughter of Edvard Hagerup. The family name, originally spelled Greig, has Scottish origins. After the Battle of Culloden in 1746, Grieg's great-grandfather, Alexander Greig, traveled widely, settling in Norway about 1770, and establishing business interests in Bergen.
Edvard Grieg was raised in a musical milieu. His mother was his first piano teacher and taught him to play at the age of six. Grieg studied in several schools, including Tanks Upper School, Tanks School and the N.P.S, Norwegian Private School.
In the summer of 1858, Grieg met the eminent Norwegian violinist Ole Bull, who was a family friend; Bull's brother was married to Grieg's aunt. Bull recognized the 15-year-old boy's talent and persuaded his parents to send him to the Leipzig Conservatory, the piano department of which was directed by Ignaz Moscheles.
Grieg enrolled in the conservatory, concentrating on the piano, and enjoyed the many concerts and recitals given in Leipzig. He disliked the discipline of the conservatory course of study. An exception was the organ, which was mandatory for piano students. In the spring of 1860, he survived a life-threatening lung disease, pleurisy and tuberculosis. Throughout his life, Grieg's health was impaired by a destroyed left lung and considerable deformity of his thoracic spine. He suffered from numerous respiratory infections, and ultimately developed combined lung and heart failure. Grieg was admitted many times to spas and sanatoria both in Norway and abroad. Several of his doctors became his personal friends.
In 1861, Grieg made his debut as a concert pianist in Karlshamn, Sweden. In 1862, he finished his studies in Leipzig and held his first concert in his home town, where his programme included Beethoven's "Pathétique" sonata.
In 1863, Grieg went to Copenhagen, Denmark, and stayed there for three years. He met the Danish composers J. P. E. Hartmann and Niels Gade. He also met his fellow Norwegian composer Rikard Nordraak (composer of the Norwegian national anthem), who became a good friend and source of inspiration. Nordraak died in 1866, and Grieg composed a funeral march in his honor.
On 11 June 1867, Grieg married his first cousin, Nina Hagerup, a lyric soprano. The next year, their only child, Alexandra, was born. Alexandra died in 1869 from meningitis. In the summer of 1868, Grieg wrote his Piano Concerto in A minor while on holiday in Denmark. Edmund Neupert gave the concerto its premiere performance on 3 April 1869 in the Casino Theater in Copenhagen. Grieg himself was unable to be there due to conducting commitments in Christiania (as Oslo was then named).
In 1868, Franz Liszt, who had not yet met Grieg, wrote a testimonial for him to the Norwegian Ministry of Education, which led to Grieg's obtaining a travel grant. The two men met in Rome in 1870. On Grieg's first visit, they went over Grieg's Violin Sonata No. 1, which pleased Liszt greatly. On his second visit, in April, Grieg brought with him the manuscript of his Piano Concerto, which Liszt proceeded to sightread (including the orchestral arrangement). Liszt's rendition greatly impressed his audience, although Grieg gently pointed out to him that he played the first movement too quickly. Liszt also gave Grieg some advice on orchestration (for example, to give the melody of the second theme in the first movement to a solo trumpet).
In 1874–76, Grieg composed incidental music for the premiere of Henrik Ibsen's play "Peer Gynt", at the request of the author.
Grieg had close ties with the Bergen Philharmonic Orchestra (Harmonien), and later became Music Director of the orchestra from 1880 to 1882. In 1888, Grieg met Tchaikovsky in Leipzig. Grieg was struck by the sadness in Tchaikovsky. Tchaikovsky thought very highly of Grieg's music, praising its beauty, originality and warmth.
Grieg was awarded two honorary doctorates, first by Cambridge University in 1894 and the next from Oxford University in 1906.
Later years.
The Norwegian government provided him with a pension. In the spring 1903, Grieg made nine 78-rpm gramophone recordings of his piano music in Paris; all of these historic discs have been reissued on both LPs and CDs, despite limited fidelity. Grieg also made live-recording player piano music rolls for the Hupfeld Phonola piano-player system and Welte-Mignon reproducing system, all of which survive today and can be heard. He also worked with the Aeolian Company for its 'Autograph Metrostyle' piano roll series wherein he indicated the tempo mapping for many of his pieces.
In 1906, he met the composer and pianist Percy Grainger in London. Grainger was a great admirer of Grieg's music and a strong empathy was quickly established. In a 1907 interview, Grieg stated: “I have written Norwegian Peasant Dances that no one in my country can play, and here comes this Australian who plays them as they ought to be played! He is a genius that we Scandinavians cannot do other than love.”
Edvard Grieg died in Municipal Hospital, in Bergen, Norway, in the late summer of 1907, aged 64, from heart failure. He had suffered a long period of illness. His final words were "Well, if it must be so." The funeral drew between 30,000 and 40,000 people out on the streets of his home town to honor him. Following his wish, his own "Funeral March in Memory of Rikard Nordraak" was played in an orchestration by his friend Johan Halvorsen, who had married Grieg's niece. In addition, the "Funeral March" movement from Chopin's Piano Sonata No. 2 was played. Grieg was cremated, and his ashes were entombed in a mountain crypt near his house, Troldhaugen. The ashes of his wife were later placed with his.
Edvard Grieg and his wife considered themselves Unitarians and Nina went to the Unitarian church in Copenhagen after his death.
Music.
Some of Grieg's early works include a symphony (which he later suppressed) and a piano sonata. He also wrote three violin sonatas and a cello sonata.
Grieg also composed the incidental music for Henrik Ibsen's play, Peer Gynt - which includes the famous excerpt entitled, "In the Hall of the Mountain King". In this piece of music, the adventures of the anti-hero, Peer Gynt, are related, including the episode in which he steals a bride at her wedding. The angry guests chase him, and Peer falls, hitting his head on a rock. He wakes up in a mountain surrounded by trolls. The music of "In the Hall of the Mountain King" represents the angry trolls taunting Peer and gets louder each time the theme repeats. The music ends with Peer escaping from the mountain.
In an 1874 letter to his friend Frants Beyer, Grieg expressed his unhappiness with Dance of the Mountain King's Daughter, one of the movements he composed for Peer Gynt, writing "I have also written something for the scene in the hall of the mountain King – something that I literally can't bear listening to because it absolutely reeks of cow-pies, exaggerated Norwegian nationalism, and trollish self-satisfaction! But I have a hunch that the irony will be discernible."
Grieg's "Holberg Suite" was originally written for the piano, and later arranged by the composer for string orchestra. Grieg wrote songs in which he set lyrics by poets Heinrich Heine, Johann Wolfgang Goethe, Henrik Ibsen, Hans Christian Andersen, Rudyard Kipling and others. Russian composer Nikolai Myaskovsky used a theme by Grieg for the variations with which he closed his Third String Quartet. Norwegian pianist Eva Knardahl recorded the composer's complete piano music during 1978 and 1980. The recordings were reissued in 2006 on 12 compact discs by BIS Records. Grieg himself recorded many of these piano works before his death in 1907.

</doc>
<doc id="9515" url="https://en.wikipedia.org/wiki?curid=9515" title="Emancipation Proclamation">
Emancipation Proclamation

The Emancipation Proclamation was a presidential proclamation and executive order issued by President Abraham Lincoln on January 1, 1863. In a single stroke, it changed the federal legal status of more than 3 million enslaved persons in the designated areas of the South from "slave" to "free". It had the practical effect that as soon as a slave escaped the control of the Confederate government, by running away or through advances of federal troops, the slave became legally free. Eventually it reached and liberated all of the designated slaves. It was issued as a war measure during the American Civil War, directed to all of the areas in rebellion and all segments of the executive branch (including the Army and Navy) of the United States.
It proclaimed the freedom of slaves in ten states (excluding Tennessee). Because it was issued under the President's war powers, it necessarily excluded areas not in rebellion - it applied to more than 3 million of the 4 million slaves at the time. The Proclamation was based on the president's constitutional authority as commander in chief of the armed forces; it was not a law passed by Congress. The Proclamation also ordered that suitable persons among those freed could be enrolled into the paid service of United States' forces, and ordered the Union Army (and all segments of the Executive branch) to "recognize and maintain the freedom of" the ex-slaves. The Proclamation did not compensate the owners, did not outlaw slavery, and did not grant citizenship to the ex-slaves (called freedmen). It made the eradication of slavery an explicit war goal, in addition to the goal of reuniting the Union.
Around 20,000 to 50,000 slaves in regions where rebellion had already been subdued were immediately emancipated. It could not be enforced in areas still under rebellion, but as the Union army took control of Confederate regions, the Proclamation provided the legal framework for freeing more than 3 million slaves in those regions. Prior to the Proclamation, in accordance with the Fugitive Slave Act of 1850, escaped slaves were either returned to their masters or held in camps as contraband for later return. The Proclamation applied only to slaves in Confederate-held lands; it did not apply to those in the four slave states that were not in rebellion (Kentucky, Maryland, Delaware, and Missouri, which were unnamed), nor to Tennessee (unnamed but occupied by Union troops since 1862) and lower Louisiana (also under occupation), and specifically excluded those counties of Virginia soon to form the state of West Virginia. Also specifically excluded (by name) were some regions already controlled by the Union army. Emancipation in those places would come after separate state actions and/or the December 1865 ratification of the Thirteenth Amendment, which made slavery and indentured servitude, except for those duly convicted of a crime, illegal everywhere subject to United States jurisdiction.
On September 22, 1862, Lincoln had issued a preliminary proclamation warning that he would order the emancipation of all slaves in any state that did not end its rebellion against the Union by January 1, 1863. None of the Confederate states restored themselves to the Union, and Lincoln's order, signed and issued January 1, 1863, took effect. The Emancipation Proclamation outraged white Southerners (and their sympathizers) who envisioned a race war, angered some Northern Democrats, energized anti-slavery forces, and undermined forces in Europe that wanted to intervene to help the Confederacy. The Proclamation lifted the spirits of African Americans both free and slave. It led many slaves to escape from their masters and get to Union lines to obtain their freedom.
The Emancipation Proclamation broadened the goals of the Civil War. While slavery had been a major issue that led to the war, Lincoln's only mission at the start of the war was to maintain the Union. The Proclamation made freeing the slaves an explicit goal of the Union war effort. Establishing the abolition of slavery as one of the two primary war goals served to deter intervention by Britain and France. The Emancipation Proclamation was never challenged in court. To ensure the abolition of slavery in all of the U.S., Lincoln pushed for passage of the Thirteenth Amendment. Congress passed it by the necessary two-thirds vote on January 31, 1865, and it was ratified by the states on December 6, 1865.
Authority.
The United States Constitution of 1787 did not use the word "slavery" but included several provisions about unfree persons. The Three-Fifths Compromise (in Article I, Section 2) allocated Congressional representation based "on the whole Number of free Persons" and "three fifths of all other Persons". Under the Fugitive Slave Clause (Article IV, Section 2), "o person held to service or labour in one state" would be freed by escaping to another. allowed Congress to pass legislation to outlaw the "Importation of Persons", but not until 1808. However, for purposes of the Fifth Amendment—which states that, "No person shall ... be deprived of life, liberty, or property, without due process of law"—slaves were understood as property. Although abolitionists used the Fifth Amendment to argue against slavery, it became part of the legal basis for treating slaves as property with "Dred Scott v. Sandford" (1857). Socially, slavery was also supported in law and in practice by a pervasive culture of white supremacy. Nonetheless, between 1777 and 1804, every Northern state provided for the immediate or gradual abolition of slavery. No Southern state did so, and the slave population of the South continued to grow, peaking at almost 4 million people at the beginning of the American Civil War, in which most slave states sought to break away from the United States.
Lincoln understood that the Federal government's power to end slavery in peacetime was limited by the Constitution which before 1865, committed the issue to individual states. Against the background of the American Civil War, however, Lincoln issued the Proclamation under his authority as "Commander in Chief of the Army and Navy" under Article II, section 2 of the United States Constitution. As such, he claimed to have the martial power to free persons held as slaves in those states that were in rebellion "as a fit and necessary war measure for suppressing said rebellion". He did not have Commander-in-Chief authority over the four slave-holding states that were not in rebellion: Missouri, Kentucky, Maryland and Delaware, and so those states were not named in the Proclamation. The fifth border jurisdiction, West Virginia, where slavery remained legal but was in the process of being abolished, was, in January 1863, still part of the legally recognized "reorganized" state of Virginia, based in Alexandria, which was in the Union (as opposed to the Confederate state of Virginia, based in Richmond).
Coverage.
The Proclamation applied in the eleven states that were still in rebellion in 1863, and thus did not cover the nearly 500,000 slaves in the slave-holding border states (Missouri, Kentucky, Maryland or Delaware) which were Union states. Those slaves were freed by later separate state and federal actions.
The state of Tennessee had already mostly returned to Union control, under a recognized Union government, so it was not named and was exempted. Virginia was named, but exemptions were specified for the 48 counties then in the process of forming the new state of West Virginia, and seven additional counties and two cities in the Union-controlled Tidewater region. Also specifically exempted were New Orleans and 13 named parishes of Louisiana, which were mostly under federal control at the time of the Proclamation. These exemptions left unemancipated an additional 300,000 slaves.
The Emancipation Proclamation has been ridiculed, notably in an influential passage by Richard Hofstadter for "freeing" only the slaves over which the Union had no power. These slaves were freed due to Lincoln's "war powers". This act cleared up the issue of contraband slaves. It automatically clarified the status of over 100,000 now-former slaves. Some 20,000 to 50,000 slaves were freed the day it went into effect in parts of nine of the ten states to which it applied (Texas being the exception). In every Confederate state (except Tennessee and Texas), the Proclamation went into immediate effect in Union-occupied areas and at least 20,000 slaves were freed at once on January 1, 1863.
Additionally, the Proclamation provided the legal framework for the emancipation of nearly all four million slaves as the Union armies advanced, and committed the Union to ending slavery, which was a controversial decision even in the North. Hearing of the Proclamation, more slaves quickly escaped to Union lines as the Army units moved South. As the Union armies advanced through the Confederacy, thousands of slaves were freed each day until nearly all (approximately 3.9 million, according to the 1860 Census) were freed by July 1865.
While the Proclamation had freed most slaves as a war measure, it had not made slavery illegal. Of the states that were exempted from the Proclamation, Maryland, Missouri, Tennessee, and West Virginia prohibited slavery before the war ended. In 1863, President Lincoln proposed a moderate plan for the Reconstruction of the captured Confederate State of Louisiana. Only 10% of the state's electorate had to take the loyalty oath. The state was also required to abolish slavery in its new constitution. Identical Reconstruction plans would be adopted in Arkansas and Tennessee. By December 1864, the Lincoln plan abolishing slavery had been enacted in Louisiana. However, in Delaware and Kentucky, slavery continued to be legal until December 18, 1865, when the Thirteenth Amendment went into effect.
Background.
Military action prior to emancipation.
The Fugitive Slave Act of 1850 required individuals to return runaway slaves to their owners. During the war, Union generals such as Benjamin Butler declared that slaves in occupied areas were contraband of war and accordingly refused to return them. This decision was controversial because it implied recognition of the Confederacy as a separate nation under international law, a notion that Lincoln steadfastly denied. As a result, he did not promote the contraband designation. In addition, as contraband, these people were legally designated as "property" when they crossed Union lines and their ultimate status was uncertain.
Governmental action towards emancipation.
In December 1861, Lincoln sent his first annual message to Congress (the State of the Union Address, but then typically given in writing and not referred to as such). In it he praised the free labor system, as respecting human rights over property rights; he endorsed legislation to address the status of contraband slaves and slaves in loyal states, possibly through buying their freedom with federal taxes, and also the funding of strictly voluntary colonization efforts. In January 1862, Thaddeus Stevens, the Republican leader in the House, called for total war against the rebellion to include emancipation of slaves, arguing that emancipation, by forcing the loss of enslaved labor, would ruin the rebel economy. On March 13, 1862, Congress approved a "Law Enacting an Additional Article of War", which stated that from that point onward it was forbidden for Union Army officers to return fugitive slaves to their owners. On April 10, 1862, Congress declared that the federal government would compensate slave owners who freed their slaves. Slaves in the District of Columbia were freed on April 16, 1862, and their owners were compensated.
On June 19, 1862, Congress prohibited slavery in all current and future United States territories (though not in the states), and President Lincoln quickly signed the legislation. By this act, they repudiated the 1857 opinion of the Supreme Court of the United States in the "Dred Scott Case" that Congress was powerless to regulate slavery in U.S. territories.
This joint action by Congress and President Lincoln also rejected the notion of popular sovereignty that had been advanced by Stephen A. Douglas as a solution to the slavery controversy, while completing the effort first legislatively proposed by Thomas Jefferson in 1784 to confine slavery within the borders of existing states.
In July, Congress passed and Lincoln signed the Confiscation Act of 1862, containing provisions for court proceedings to liberate slaves held by convicted "rebels", or of slaves of rebels that had escaped to Union lines. The Act applied in cases of criminal convictions and to those who were slaves of "disloyal" masters. However, Lincoln's position continued to be that Congress lacked power to free all slaves within the borders of rebel held states, but Lincoln as commander in chief could do so if he deemed it a proper military measure, and that Lincoln had already drafted plans to do.
Public opinion of emancipation.
Abolitionists had long been urging Lincoln to free all slaves. In the summer of 1862, Republican editor Horace Greeley of the highly influential New York Tribune wrote a famous editorial entitled "The Prayer of Twenty Millions" demanding a more aggressive attack on the Confederacy and faster emancipation of the slaves: "On the face of this wide earth, Mr. President, there is not one ... intelligent champion of the Union cause who does not feel ... that the rebellion, if crushed tomorrow, would be renewed if slavery were left in full vigor ... and that every hour of deference to slavery is an hour of added and deepened peril to the Union." Lincoln responded in his from August 22, 1862, in terms of the limits imposed by his duty as president to save the Union:
Lincoln scholar Harold Holzer wrote in this context about Lincoln's letter: "Unknown to Greeley, Lincoln composed this after he had already drafted a preliminary Emancipation Proclamation, which he had determined to issue after the next Union military victory. Therefore, this letter, was in truth, an attempt to position the impending announcement in terms of saving the Union, not freeing slaves as a humanitarian gesture. It was one of Lincoln's most skillful public relations efforts, even if it has cast longstanding doubt on his sincerity as a liberator." Historian Richard Striner argues that "for years" Lincoln's letter has been misread as "Lincoln only wanted to save the Union." However, within the context of Lincoln's entire career and pronouncements on slavery this interpretation is wrong, according to Striner. Rather, Lincoln was softening the strong Northern white supremacist opposition to his imminent emancipation by tying it to the cause of the Union. This opposition would fight for the Union but not to end slavery, so Lincoln gave them the means and motivation to do both, at the same time. In his 2014 book, "Lincoln's Gamble", journalist and historian Todd Brewster asserted that Lincoln's desire to reassert the saving of the Union as his sole war goal was in fact crucial to his claim of legal authority for emancipation. Since slavery was protected by the Constitution, the only way that he could free the slaves was as a tactic of war—not as the mission itself. But that carried the risk that when the war ended, so would the justification for freeing the slaves. Late in 1862, Lincoln asked his Attorney General, Edward Bates, for an opinion as to whether slaves freed through a war-related proclamation of emancipation could be re-enslaved once the war was over. Bates had to work through the language of the Dred Scott decision to arrive at an answer, but he finally concluded that they could indeed remain free. Still, a complete end to slavery would require a constitutional amendment. Conflicting advice, to free all slaves, or not free them at all, was presented to Lincoln in public and private. Thomas Nast, a cartoon artist during the Civil War and the late 1800s considered "Father of the American Cartoon", composed many works including a two-sided spread that showed the transition from slavery into civilization after President Lincoln signed the Proclamation. Nast believed in equal opportunity and equality for all people, including enslaved Africans or free blacks. A mass rally in Chicago on September 7, 1862, demanded an immediate and universal emancipation of slaves. A delegation headed by William W. Patton met the President at the White House on September 13. Lincoln had declared in peacetime that he had no constitutional authority to free the slaves. Even used as a war power, emancipation was a risky political act. Public opinion as a whole was against it. There would be strong opposition among Copperhead Democrats and an uncertain reaction from loyal border states. Delaware and Maryland already had a high percentage of free blacks: 91.2% and 49.7%, respectively, in 1860.
Drafting and issuance of the proclamation.
Lincoln first discussed the proclamation with his cabinet in July 1862. He drafted his "preliminary proclamation" and read it to Secretary of State William Seward, and Secretary of Navy Gideon Welles, on July 13. Seward and Welles were at first speechless, then Seward referred to possible anarchy throughout the South and resulting foreign intervention; Welles apparently said nothing. On July 22, Lincoln presented it to his entire cabinet as something he had determined to do and he asked their opinion on wording. Although Secretary of War Edwin Stanton supported it, Seward advised Lincoln to issue the proclamation after a major Union victory, or else it would appear as if the Union was giving "its last shriek of retreat".
In September 1862, the Battle of Antietam gave Lincoln the victory he needed to issue the Emancipation. In the battle, though General McClellan allowed the escape of Robert E. Lee's retreating troops, Union forces turned back a Confederate invasion of Maryland. On September 22, 1862, five days after Antietam occurred, Lincoln called his cabinet into session and issued the Preliminary Emancipation Proclamation. According to Civil War historian James M. McPherson, Lincoln told Cabinet members that he had made a covenant with God, that if the Union drove the Confederacy out of Maryland, he would issue the Emancipation Proclamation. Lincoln had first shown an early draft of the proclamation to Vice President Hannibal Hamlin, an ardent abolitionist, who was more often kept in the dark on presidential decisions. The final proclamation was issued January 1, 1863. Although implicitly granted authority by Congress, Lincoln used his powers as Commander-in-Chief of the Army and Navy, "as a necessary war measure" as the basis of the proclamation, rather than the equivalent of a statute enacted by Congress or a constitutional amendment. Some days after issuing the final Proclamation, Lincoln wrote to Major General John McClernand: "After the commencement of hostilities I struggled nearly a year and a half to get along without touching the "institution"; and when finally I conditionally determined to touch it, I gave a hundred days fair notice of my purpose, to all the States and people, within which time they could have turned it wholly aside, by simply again becoming good citizens of the United States. They chose to disregard it, and I made the peremptory proclamation on what appeared to me to be a military necessity. And being made, it must stand."
Initially, the Emancipation Proclamation effectively freed only a small percentage of the slaves, those who were behind Union lines in areas not exempted. Most slaves were still behind Confederate lines or in exempted Union-occupied areas. Secretary of State William H. Seward commented, "We show our sympathy with slavery by emancipating slaves where we cannot reach them and holding them in bondage where we can set them free." Had any slave state ended its secession attempt before January 1, 1863, it could have kept slavery, at least temporarily. The Proclamation only gave the Lincoln Administration the legal basis to free the slaves in the areas of the South that were still in rebellion on January 1, 1863. It effectively destroyed slavery as the Union armies advanced south and conquered the entire Confederacy.
The Emancipation Proclamation also allowed for the enrollment of freed slaves into the United States military. During the war nearly 200,000 blacks, most of them ex-slaves, joined the Union Army. Their contributions gave the North additional manpower that was significant in winning the war. The Confederacy did not allow slaves in their army as soldiers until the last month before its defeat.
Though the counties of Virginia that were soon to form West Virginia were specifically exempted from the Proclamation (Jefferson County being the only exception), a condition of the state's admittance to the Union was that its constitution provide for the gradual abolition of slavery (an immediate emancipation of all slaves was also adopted there in early 1865). Slaves in the border states of Maryland and Missouri were also emancipated by separate state action before the Civil War ended. In Maryland, a new state constitution abolishing slavery in the state went into effect on November 1, 1864. The Union-occupied counties of eastern Virginia and parishes of Louisiana, which had been exempted from the Proclamation, both adopted state constitutions that abolished slavery in April 1864. In early 1865, Tennessee adopted an amendment to its constitution prohibiting slavery. Slaves in Kentucky and Delaware were not emancipated until the Thirteenth Amendment was ratified.
Implementation.
The Proclamation was issued in two parts. The first part, issued on September 22, 1862, was a preliminary announcement outlining the intent of the second part, which officially went into effect 100 days later on January 1, 1863, during the second year of the Civil War. It was Abraham Lincoln's declaration that all slaves would be permanently freed in all areas of the Confederacy that had not already returned to federal control by January 1863. The ten affected states were individually named in the second part (South Carolina, Mississippi, Florida, Alabama, Georgia, Louisiana, Texas, Virginia, Arkansas, North Carolina). Not included were the Union slave states of Maryland, Delaware, Missouri and Kentucky. Also not named was the state of Tennessee, in which a Union-controlled military government had already been set up, based in the capital, Nashville. Specific exemptions were stated for areas also under Union control on January 1, 1863, namely 48 counties that would soon become West Virginia, seven other named counties of Virginia including Berkeley and Hampshire counties, which were soon added to West Virginia, New Orleans and 13 named parishes nearby.
Union-occupied areas of the Confederate states where the proclamation was put into immediate effect by local commanders included Winchester, Virginia, Corinth, Mississippi, the Sea Islands along the coasts of the Carolinas and Georgia, Key West, Florida, and Port Royal, South Carolina.
Immediate impact.
It has been inaccurately claimed that the Emancipation Proclamation did not free a single slave; black author Lerone Bennett, Jr. alleged that the proclamation was a hoax deliberately designed not to free any slaves. However, as a result of the Proclamation, many slaves were freed during the course of the war, beginning with the day it took effect; eyewitness accounts at places such as Hilton Head, South Carolina, and Port Royal, South Carolina record celebrations on January 1 as thousands of blacks were informed of their new legal status of freedom. Estimates of how many thousands of slaves were freed immediately by the Emancipation Proclamation are varied. One contemporary estimate put the 'contraband' population of Union-occupied North Carolina at 10,000, and the Sea Islands of South Carolina also had a substantial population. Those 20,000 slaves were freed immediately by the Emancipation Proclamation." This Union-occupied zone where freedom began at once included parts of eastern North Carolina, the Mississippi Valley, northern Alabama, the Shenandoah Valley of Virginia, a large part of Arkansas, and the Sea Islands of Georgia and South Carolina. Although some counties of Union-occupied Virginia were exempted from the Proclamation, the lower Shenandoah Valley, and the area around Alexandria were covered. Emancipation was immediately enforced as Union soldiers advanced into the Confederacy. Slaves fled their masters and were often assisted by Union soldiers.
Booker T. Washington, as a boy of 9 in Virginia, remembered the day in early 1865:
Emancipation took place without violence by masters or ex-slaves. The Proclamation represented a shift in the war objectives of the North—reuniting the nation was no longer the only goal. It represented a major step toward the ultimate abolition of slavery in the United States and a "new birth of freedom".
Runaway slaves who had escaped to Union lines had previously been held by the Union Army as "contraband of war" under the Confiscation Acts; when the proclamation took effect, they were told at midnight that they were free to leave. The Sea Islands off the coast of Georgia had been occupied by the Union Navy earlier in the war. The whites had fled to the mainland while the blacks stayed. An early program of Reconstruction was set up for the former slaves, including schools and training. Naval officers read the proclamation and told them they were free.
Slaves had been part of the "engine of war" for the Confederacy. They produced and prepared food; sewed uniforms; repaired railways; worked on farms and in factories, shipping yards, and mines; built fortifications; and served as hospital workers and common laborers. News of the Proclamation spread rapidly by word of mouth, arousing hopes of freedom, creating general confusion, and encouraging thousands to escape to Union lines. George Washington Albright, a teenage slave in Mississippi, recalled that like many of his fellow slaves, his father escaped to join Union forces. According to Albright, plantation owners tried to keep the Proclamation from slaves but news of it came through the "grapevine". The young slave became a "runner" for an informal group they called the "4Ls" ("Lincoln's Legal Loyal League") bringing news of the proclamation to secret slave meetings at plantations throughout the region.
Robert E. Lee saw the Emancipation Proclamation as a way for the Union to bolster the number of soldiers it could place on the field, making it imperative for the Confederacy to increase their own numbers.
Writing on the matter after the sack of Fredericksburg, Lee wrote "In view of the vast increase of the forces of the enemy, of the savage and brutal policy he has proclaimed, which leaves us no alternative but success or degradation worse than death, if we would save the honor of our families from pollution, our social system from destruction, let every effort be made, every means be employed, to fill and maintain the ranks of our armies, until God, in his mercy, shall bless us with the establishment of our independence." Lee's request for a drastic increase of troops would go unfulfilled.
Political impact.
The Proclamation was immediately denounced by Copperhead Democrats who opposed the war and advocated restoring the union by allowing slavery. Horatio Seymour, while running for the governorship of New York, cast the Emancipation Proclamation as a call for slaves to commit extreme acts of violence on all white southerners, saying it was "a proposal for the butchery of women and children, for scenes of lust and rapine, and of arson and murder, which would invoke the interference of civilized Europe". The Copperheads also saw the Proclamation as an unconstitutional abuse of Presidential power. Editor Henry A. Reeves wrote in Greenport's "Republican Watchman" that "In the name of freedom of Negroes, he proclamatio imperils the liberty of white men; to test a utopian theory of equality of races which Nature, History and Experience alike condemn as monstrous, it overturns the Constitution and Civil Laws and sets up Military Usurpation in their Stead."
Racism remained pervasive on both sides of the conflict and many in the North supported the war only as an effort to force the South to stay in the Union. The promises of many Republican politicians that the war was to restore the Union and not about black rights or ending slavery, were now declared lies by their opponents citing the Proclamation. Copperhead David Allen spoke to a rally in Columbiana, Ohio, stating "I have told you that this war is carried on for the Negro. There is the proclamation of the President of the United States. Now fellow Democrats I ask you if you are going to be forced into a war against your Brithren of the Southern States for the Negro. I answer No!" The Copperheads saw the Proclamation as irrefutable proof of their position and the beginning of a political rise for their members; in Connecticut, H. B. Whiting wrote that the truth was now plain even to "those stupid thick-headed persons who persisted in thinking that the President was a conservative man and that the war was for the restoration of the Union under the Constitution".
War Democrats who rejected the Copperhead position within their party, found themselves in a quandary. While throughout the war they had continued to espouse the racist positions of their party and their disdain of the concerns of slaves, they did see the Proclamation as a viable military tool against the South, and worried that opposing it might demoralize troops in the Union army. The question would continue to trouble them and eventually lead to a split within their party as the war progressed.
Lincoln further alienated many in the Union two days after issuing the preliminary copy of the Emancipation Proclamation by suspending habeas corpus. His opponents linked these two actions in their claims that he was becoming a despot. In light of this and a lack of military success for the Union armies, many War Democrat voters who had previously supported Lincoln turned against him and joined the Copperheads in the off-year elections held in October and November.
In the 1862 elections, the Democrats gained 28 seats in the House as well as the governorship of New York. Lincoln's friend Orville Hickman Browning told the President that the Proclamation and the suspension of habeas corpus had been "disastrous" for his party by handing the Democrats so many weapons. Lincoln made no response. Copperhead William Javis of Connecticut pronounced the election the "beginning of the end of the utter downfall of Abolitionism in the United States".
Historians James M. McPherson and Allan Nevins state that though the results looked very troubling, they could be seen favorably by Lincoln; his opponents did well only in their historic strongholds and "at the national level their gains in the House were the smallest of any minority party's in an off-year election in nearly a generation. Michigan, California, and Iowa all went Republican... Moreover, the Republicans picked up five seats in the Senate." McPherson states "If the election was in any sense a referendum on emancipation and on Lincoln's conduct of the war, a majority of Northern voters endorsed these policies."
The initial Confederate response was one of expected outrage. The Proclamation was seen as vindication for the rebellion, and proof that Lincoln would have abolished slavery even if the states had remained in the Union.
International impact.
As Lincoln had hoped, the Proclamation turned foreign popular opinion in favor of the Union by gaining the support of anti-slavery countries and countries that had already abolished slavery (especially the developed countries in Europe). This shift ended the Confederacy's hopes of gaining official recognition.
Since the Emancipation Proclamation made the eradication of slavery an explicit Union war goal, it linked support for the South to support for slavery. Public opinion in Britain would not tolerate direct support for slavery. British companies, however, continued to build and operate blockade runners for the South. As Henry Adams noted, "The Emancipation Proclamation has done more for us than all our former victories and all our diplomacy." In Italy, Giuseppe Garibaldi hailed Lincoln as "the heir of the aspirations of John Brown". On August 6, 1863, Garibaldi wrote to Lincoln: "Posterity will call you the great emancipator, a more enviable title than any crown could be, and greater than any merely mundane treasure".
Alan Van Dyke, a representative for workers from Manchester, England, wrote to Lincoln saying, "We joyfully honor you for many decisive steps toward practically exemplifying your belief in the words of your great founders: 'All men are created free and equal.'" The Emancipation Proclamation served to ease tensions with Europe over the North's conduct of the war, and combined with the recent failed Southern offensive at Antietam, to cut off any practical chance for the Confederacy to receive international support in the war.
Gettysburg Address.
Lincoln's Gettysburg Address in November 1863 made indirect reference to the Proclamation and the ending of slavery as a war goal with the phrase "new birth of freedom". The Proclamation solidified Lincoln's support among the rapidly growing abolitionist element of the Republican Party and ensured they would not block his re-nomination in 1864.
Postbellum.
Near the end of the war, abolitionists were concerned that the Emancipation Proclamation would be construed solely as a war measure, Lincoln's original intent, and would no longer apply once fighting ended. They were also increasingly anxious to secure the freedom of all slaves, not just those freed by the Emancipation Proclamation. Thus pressed, Lincoln staked a large part of his 1864 presidential campaign on a constitutional amendment to abolish slavery uniformly throughout the United States. Lincoln's campaign was bolstered by separate votes in both Maryland and Missouri to abolish slavery in those states. Maryland's new constitution abolishing slavery took effect in November 1864. Slavery in Missouri was ended by executive proclamation of its governor, Thomas C. Fletcher, on January 11, 1865.
Winning re-election, Lincoln pressed the lame duck 38th Congress to pass the proposed amendment immediately rather than wait for the incoming 39th Congress to convene. In January 1865, Congress sent to the state legislatures for ratification what became the Thirteenth Amendment, banning slavery in all U.S. states and territories. The amendment was ratified by the legislatures of enough states by December 6, 1865, and proclaimed 12 days later. There were about 40,000 slaves in Kentucky and 1,000 in Delaware who were liberated then.
Critiques.
As the years went on and American life continued to be deeply unfair towards blacks, cynicism towards Lincoln and the Emancipation Proclamation increased. Perhaps the strongest attack was Lerone Bennett's "" (2000), which claimed that Lincoln was a white supremacist who issued the Emancipation Proclamation in lieu of the real racial reforms for which radical abolitionists pushed. In his "Lincoln's Emancipation Proclamation", Allen C. Guelzo noted the professional historians' lack of substantial respect for the document, since it has been the subject of few major scholarly studies. He argued that Lincoln was America's "last Enlightenment politician" and as such was dedicated to removing slavery strictly within the bounds of law.
Other historians have given more credit to Lincoln for what he accomplished within the tensions of his cabinet and a society at war, for his own growth in political and moral stature, and for the promise he held out to the slaves. More might have been accomplished if he had not been assassinated. As Eric Foner wrote:
Lincoln was not an abolitionist or Radical Republican, a point Bennett reiterates innumerable times. He did not favor immediate abolition before the war, and held racist views typical of his time. But he was also a man of deep convictions when it came to slavery, and during the Civil War displayed a remarkable capacity for moral and political growth.
Kal Ashraf wrote:
Perhaps in rejecting the critical dualism–Lincoln as individual emancipator pitted against collective self-emancipators–there is an opportunity to recognise the greater persuasiveness of the combination. In a sense, yes: a racist, flawed Lincoln did something heroic, and not in lieu of collective participation, but next to, and enabled, by it. To venerate a singular –Great Emancipator' may be as reductive as dismissing the significance of Lincoln's actions. Who he was as a man, no one of us can ever really know. So it is that the version of Lincoln we keep is also the version we make.
Legacy in the Civil Rights Era.
Dr. Martin Luther King Jr..
Dr. Martin Luther King Jr. made many references to the Emancipation Proclamation during the Civil Rights Movement. These include a speech made at an observance of the hundredth anniversary of the issuing of the Proclamation made in New York City on September 12, 1962 where he placed it alongside the Declaration of Independence as an "imperishable" contribution to civilization, and "All tyrants, past, present and future, are powerless to bury the truths in these declarations". He lamented that despite a history where America "proudly professed the basic principles inherent in both documents", it "sadly practiced the antithesis of these principles". He concluded "There is but one way to commemorate the Emancipation Proclamation. That is to make its declarations of freedom real; to reach back to the origins of our nation when our message of equality electrified an unfree world, and reaffirm democracy by deeds as bold and daring as the issuance of the Emancipation Proclamation."
King's most famous invocation of the Emancipation Proclamation was in a speech from the steps of the Lincoln Memorial at the 1963 March on Washington for Jobs and Freedom (often referred to as the "I Have a Dream" speech). King began the speech saying "Five score years ago, a great American, in whose symbolic shadow we stand, signed the Emancipation Proclamation. This momentous decree came as a great beacon light of hope to millions of Negro slaves who had been seared in the flames of withering injustice. It came as a joyous daybreak to end the long night of captivity. But one hundred years later, we must face the tragic fact that the Negro is still not free. One hundred years later, the life of the Negro is still sadly crippled by the manacles of segregation and the chains of discrimination."
The "Second Emancipation Proclamation".
In the early 1960s, Dr. Martin Luther King Jr. and his associates developed a strategy to call on President John F. Kennedy to bypass a Southern segregationist opposition in the Congress by issuing an executive order to put an end to segregation. This envisioned document was referred to as the "Second Emancipation Proclamation".
President John F. Kennedy.
On June 11, 1963, President Kennedy appeared on national television to address the issue of civil rights. Kennedy, who had been routinely criticized as timid by some of the leaders of the civil rights movement, told Americans that two black students had been peacefully enrolled in the University of Alabama with the aid the National Guard despite the opposition of Governor George Wallace.
John Kennedy called it a "moral issue" Invoking the centennial of the Emancipation Proclamation he said "One hundred years of delay have passed since President Lincoln freed the slaves, yet their heirs, their grandsons, are not fully free. They are not yet freed from the bonds of injustice. They are not yet freed from social and economic oppression. And this Nation, for all its hopes and all its boasts, will not be fully free until all its citizens are free. We preach freedom around the world, and we mean it, and we cherish our freedom here at home, but are we to say to the world, and much more importantly, to each other that this is a land of the free except for the Negroes; that we have no second-class citizens except Negroes; that we have no class or caste system, no ghettoes, no master race except with respect to Negroes? Now the time has come for this Nation to fulfill its promise. The events in Birmingham and elsewhere have so increased the cries for equality that no city or State or legislative body can prudently choose to ignore them."
In the same speech Kennedy announced he would introduce comprehensive civil rights legislation to the United States Congress which he did a week later (he continued to push for its passage until his assassination in November 1963). Historian Peniel E. Joseph holds Lyndon Johnson's ability to get that bill, the Civil Rights Act of 1964, passed on July 2, 1964 was aided by "the moral forcefulness of the June 11 speech" which turned "the narrative of civil rights from a regional issue into a national story promoting racial equality and democratic renewal".
President Lyndon B. Johnson.
During the American Civil Rights movement of the 1960s Lyndon B. Johnson invoked the Emancipation Proclamation holding it up as a promise yet to be fully implemented.
As Vice President while speaking from Gettysburg on May 30, 1963 (Memorial Day), at the centennial of the Emancipation Proclamation, Johnson connected it directly with the ongoing Civil Rights struggles of the time saying "One hundred years ago, the slave was freed. One hundred years later, the Negro remains in bondage to the color of his skin... In this hour, it is not our respective races which are at stake—it is our nation. Let those who care for their country come forward, North and South, white and Negro, to lead the way through this moment of challenge and decision... Until justice is blind to color, until education is unaware of race, until opportunity is unconcerned with color of men's skins, emancipation will be a proclamation but not a fact. To the extent that the proclamation of emancipation is not fulfilled in fact, to that extent we shall have fallen short of assuring freedom to the free."
As President, Johnson again invoked the proclamation in a speech presenting the Voting Rights Act at a joint session of Congress on Monday, March 15, 1965. This was one week after violence had been inflicted on peaceful civil rights marchers during the Selma to Montgomery marches. Johnson said "... it's not just Negroes, but really it's all of us, who must overcome the crippling legacy of bigotry and injustice. And we shall overcome. As a man whose roots go deeply into Southern soil, I know how agonizing racial feelings are. I know how difficult it is to reshape the attitudes and the structure of our society. But a century has passed—more than 100 years—since the Negro was freed. And he is not fully free tonight. It was more than 100 years ago that Abraham Lincoln—a great President of another party—signed the Emancipation Proclamation. But emancipation is a proclamation and not a fact. A century has passed—more than 100 years—since equality was promised, and yet the Negro is not equal. A century has passed since the day of promise, and the promise is unkept. The time of justice has now come, and I tell you that I believe sincerely that no force can hold it back. It is right in the eyes of man and God that it should come, and when it does, I think that day will brighten the lives of every American."
In popular culture.
In episode 86 of "The Andy Griffith Show", Andy asks Barney to explain the Emancipation Proclamation to Opie who is struggling with history at school. Barney brags about his history expertise, yet it is apparent he cannot answer Andy's question. He finally becomes frustrated and explains it is a proclamation for certain people who wanted emancipation.
The Emancipation Proclamation is celebrated around the world including on stamps of nations such as the Republic of Togo. The United States commemorative was issued on August 16, 1963, the opening day of the Century of Negro Progress Exposition in Chicago, Illinois. Designed by Georg Olden, an initial printing of 120 million stamps was authorized.

</doc>
<doc id="9516" url="https://en.wikipedia.org/wiki?curid=9516" title="Erwin Rommel">
Erwin Rommel

Erwin Johannes Eugen Rommel (15 November 1891 – 14 October 1944), popularly known as the Desert Fox, was a German field marshal of World War II. He earned the respect of both his own troops and his enemies.
Rommel was a highly decorated officer in World War I and was awarded the Pour le Mérite for his exploits on the Italian Front. In World War II, he further distinguished himself as the commander of the 7th Panzer Division during the 1940 invasion of France. His leadership of German and Italian forces in the North African campaign established him as one of the most able commanders of the war, and earned him the appellation of the Desert Fox. He is regarded as one of the most skilled commanders of desert warfare in the conflict. He later commanded the German forces opposing the Allied cross-channel invasion of Normandy.
Rommel is regarded as having been a humane and professional officer. Orders to kill Jewish soldiers, civilians and captured commandos were ignored. Later in the war, Rommel was indirectly linked to the conspiracy to assassinate Adolf Hitler. Because Rommel was a national hero, Hitler desired to eliminate him quietly. He forced Rommel to commit suicide with a cyanide pill, in return for assurances that Rommel's family would not be persecuted following his death. He was given a state funeral, and it was announced that Rommel had succumbed to his injuries from an earlier strafing of his staff car in Normandy.
Early life and career.
Rommel was born on 15 November 1891 in Southern Germany at Heidenheim, from Ulm, in the Kingdom of Württemberg, then part of the German Empire. He was the second of four children of Erwin Rommel Senior (1860–1913), a teacher and school administrator, and his wife Helene von Lutz, who headed the local government council. As a young man Rommel's father had been a lieutenant in the artillery. Rommel had one older sister and three younger brothers, one of whom died in infancy.
At the age of 14, Rommel and a friend built a full-scale glider and were able to fly it short distances. He later purchased a motorcycle, and upon getting home immediately set about taking it apart and putting it back together. Rommel considered becoming an aeronautical engineer, but at age 18 he acceded to his father's wishes and joined the local 124th Württemberg Infantry Regiment as a "Fähnrich" (ensign), in 1910, studying at the Officer Cadet School in Danzig. He graduated in November 1911 and was commissioned as a lieutenant in January 1912 and was assigned to the 124th Infantry in Weingarten. He was posted to Ulm in March 1914 to command the No.4 Battery, 46th Field Artillery Regiment, XIII (Royal Württemberg) Corps. Until the start of the First World War he trained new recruits and gave refresher courses to reserve officers, and then returned to the 124th when war was declared. While at Cadet School, Rommel met his future wife, 17-year-old Lucia (Lucie) Maria Mollin (1894–1971). They married in November 1916 in Danzig.
World War I.
During World War I, Rommel fought in France as well as in the Romanian and Italian Campaigns. He gained success leading small groups of men, using tactics such as infiltrating through enemy lines under cover of darkness, moving forward rapidly to a flanking position to arrive at their rear, and attacking defenders using the element of surprise. Arriving at the front near Verdun on 22 August 1914, Rommel initially commanded a platoon in 2nd Battalion, 124th Regiment. They were assigned to reconnaissance and courier tasks. His first combat experience was on 22 August 1914, when – catching the French garrison unprepared at the village of Bleid – he and three men engaged the enemy without waiting for the rest of their platoon to arrive. Rommel was often ill while on active duty, particularly with stomach troubles and exhaustion, a problem that manifested itself from the beginning of his career. He was appointed Battalion Adjutant in September. The armies continued to skirmish in open engagements throughout September, as the static trench warfare typical of the First World War was still in the future. On 24 September Rommel was shot in the leg when he engaged several French soldiers armed only with his bayonet (he had run out of ammunition). For this action, he was awarded the Iron Cross, Second Class.
On his return in January 1915, Rommel was assigned to command 9th Company, 124th Regiment, stationed in the trenches near Argonne. On 29 January, he and his platoon crawled through of barbed wire to engage the French, who were positioned in blockhouses and earthworks. His company was running low on ammunition and were ordered to retreat. Rommel ordered an attack on one of the blockhouses to keep the enemy from opening fire on the withdrawing men. For his work that day, he was awarded the Iron Cross, First Class.
He continued to soldier in the trenches of France for another nine months, and received a minor shrapnel wound to the leg on 29 June 1915. He was promoted to "Oberleutnant" (first lieutenant) and transferred to the newly created "Königliche Wurttemberg Gebirgsbataillon" (Royal Wurttemberg Mountain Battalion) of the "Alpenkorps" in September. He was commander of 2nd Company, which trained in mountain warfare in Austria until December, when they were posted on a stretch of front in the Vosges mountains of Alsace. They remained there, seeing action in reconnaissance work and raids on enemy positions until October 1916, when they were moved to the Southern Carpathians to fight the Romanians, who had joined the conflict in August. In August 1917, his unit was involved in the battle for Mount Cosna, a heavily fortified objective on the border between Hungary and Romania. They succeeded after nearly two weeks of difficult uphill fighting and were withdrawn to reserve on 25 August. The unit spent six weeks recuperating in Carinthia, and Rommel received leave to return to Danzig to see his wife and recover from a gunshot wound to the arm that he had received in the fight at Mount Cosna.
The Mountain Battalion was next assigned to fight on the Isonzo front, a mountainous area which had been the scene of near-constant fighting since the entry of Italy into the war on the Allied side on 23 May 1915. The offensive known as the Twelfth Battle of the Isonzo, or the Battle of Caporetto, began on 24 October 1917 with a four-hour artillery barrage. Rommel's "Abteilung", consisting of three rifle companies and a machine gun unit, was part of an attempt to take enemy positions on three mountains: Kolovrat, Matajur, and Stol. Beginning at dawn on 25 October, Rommel took advantage of the terrain to outflank the Italians and gain control of the ridge on Kolovrat, taking 1,500 prisoners in the first three hours. Noticing that there was no field of fire on a supply road leading down to the village of Luico (now Livek), Rommel and 150 of his men proceeded down and captured the town, behind enemy lines. Believing the presence of Rommels' group to be proof that their lines had collapsed, a column of Italian light infantry, 2,000 strong, surrendered after a brief firefight. Before dawn on the 26th, Rommel led his "Abteilung", now reinforced with two additional machine gun companies, toward Matajur. They took the village of Jevszek without a fight, capturing another 1,600 men. In spite of orders not to attack, they assaulted Matajur from an unexpected direction from behind the Italian lines, arriving at the summit shortly before noon on 27 October. In two and a half days, he and his small contingent of men had captured 81 guns and 9,000 men (including 150 officers), at the loss of six dead and 30 wounded. Acting as advance guard in the capture of Longarone on 9 November, he again decided to attack with a much smaller force. Reinforcements continued to arrive, and fighting continued through the night. Convinced that they were surrounded by an entire German division, the 1st Italian Infantry Division – 10,000 men – surrendered to Rommel at dawn. For this and his work at Matajur, he (and his battalion commander, Major ) received the order of Pour le Mérite. After a week on leave in January 1918, Rommel was promoted to "Hauptmann" (captain) and assigned to a staff position with XLIV Army Corps, where he served for the remainder of the war. 
Between the wars.
Rommel remained with the 124th Regiment until 1 October 1920, when he was named company commander of the 13th Infantry Regiment in Stuttgart, a post he held with the rank of captain for the next nine years. His regiment was involved in quelling riots and civil disturbances that were occurring throughout Germany at this time. Wherever possible, he avoided the use of force in these confrontations. He was assigned as an instructor at the Dresden Infantry School from 1929 to 1933, and was promoted to major in April 1932. While at Dresden, he wrote "Gefechts-Aufgaben für Zug und Kompanie : Ein Handbuch für den Offizierunterricht" ("Combat tasks for platoon and company: A manual for the officer instruction in infantry training", published in 1934) and his book "Infanterie Greift An" ("Infantry Attacks"), a description of his wartime experiences along with his analysis, published in 1937. It became a bestseller; Adolf Hitler was one of many people who owned a copy. During this period he indulged his interest in engineering and mechanics by learning about the inner workings and maintenance of internal combustion engines and heavy machine guns. He memorized logarithm tables in his spare time, and enjoyed skiing and other outdoor sports.
Rommel was promoted to "Oberstleutnant" (lieutenant colonel) in October 1933 given his next command, the 3rd "Jäger" Battalion, 17th Infantry Regiment, stationed at Goslar. Here he first met Hitler, who inspected his troops on 30 September 1934. On this occasion the SS major in charge of Hitler's bodyguard tried to place a row of his men in front of Rommel's men, ostensibly to protect the Führer. Insulted, Rommel refused to turn out his battalion. The S.S. were ordered to stand down.
In September 1935 Rommel was moved to the War Academy at Potsdam as an instructor, a post he held for the next three years. Hearing of Rommel's reputation as an outstanding military instructor, in February 1937 Hitler assigned him as the War Ministry liaison officer to the Hitler Youth, in charge of military training. Here he clashed with Baldur von Schirach, the Hitler Youth leader, over the amount of military training that the boys should receive. Accounts differ: Rommel himself said that he wished to concentrate on basic education and minimize the military aspects, while Schirach's version was that Rommel wished to focus on nothing else. Rommel left the programme in 1938.
In 1938 Rommel, who had been promoted to "Oberst" (colonel) on 1 August 1937, was appointed commandant of the Theresian Military Academy War Academy at Wiener Neustadt. In October 1938 Hitler specially requested that Rommel be seconded to command the "Führerbegleitbrigade" (his escort battalion) for his entry into Prague during the German occupation of Czechoslovakia. This unit accompanied him whenever he traveled outside of Germany. While Rommel developed an admiration for Hitler, he never joined the Nazi Party. He refused to permit his son to join the SS.
World War II.
Poland 1939.
Rommel was promoted to "Generalmajor" on 23 August 1939 and assigned as commander of the "Führerbegleitbrigade", tasked with guarding Hitler and his field headquarters during the invasion of Poland, which began on 1 September. The invasion is considered as the starting point of the Second World War. Hitler took a personal interest in the campaign, often moving close to the front in the "Führersonderzug" (headquarters train). Rommel attended Hitler's daily war briefings and accompanied him everywhere, making use of the opportunity to observe first-hand the use of tanks and other motorized units. On 26 September Rommel returned to Berlin to set up a new headquarters for his unit in the Reich Chancellery, and returned briefly to Warsaw on 5 October for the German victory parade in that city.
France 1940.
Panzer commander.
Following the campaign in Poland, Rommel made it known that charge of a guard detail was not the best use of his services and began lobbying for command of one of Germany's panzer divisions, of which there were then only ten. Rommel's successes in World War I were based on surprise and maneuver, two elements for which the new panzer units were ideally suited. With Hitler's support and in spite of his lack of experience commanding mechanized units, Rommel was given command of 7th Panzer Division on 10 February 1940. The unit had been recently converted to a fully armoured division consisting of 218 tanks in three battalions, with two rifle regiments, a motorcycle battalion, an engineer battalion, and an anti-tank battalion. Upon taking command he quickly set his unit to practicing the maneuvers they would need in the upcoming campaign.
Invasion of France and Belgium.
The invasion began on 10 May 1940. By the third day Rommel, along with three panzer divisions commanded by "Generalleutnant" Heinz Guderian, had reached the River Meuse, where they found the bridges had already been destroyed. Rommel was active in the forward areas, directing the efforts to make a crossing, which were initially unsuccessful due to suppressive fire by the French on the other side of the river. Rommel brought up tanks and flak units to provide counter-fire and had nearby houses set on fire to create a smokescreen. He sent infantry across in rubber boats, appropriated the bridging tackle of the 5th Panzer Division, and went into the water himself, encouraging the sappers and helping lash together the pontoons. By 16 May Rommel had reached his assigned objective at Avesnes, where the original plan called for him to stop and await further orders. But Rommel pressed on.
Battle of Arras.
On 20 May Rommel reached Arras. General Hermann Hoth received orders that the town should be bypassed and its British garrison thus isolated. He ordered the 5th Panzer Division to move to the west and 7th Panzer Division to the east, flanked by the 3rd SS Panzer Division "Totenkopf". The following day the British launched a counterattack, meeting the SS "Totenkopf" with two infantry battalions supported by heavily armoured Matilda Mk I and Matilda II tanks in the Battle of Arras. The German 37 mm anti-tank gun proved ineffective against the heavily-armoured Matildas. The 25th Panzer Regiment and a battery of anti-aircraft guns were called in to support, and the British withdrew.
On 24 May, Hitler issued a halt order. The reason for this decision is still a matter of debate. He may have overestimated the size of the British forces in the area, or he may have wished to reserve the bulk of the armour for the drive on Paris. The halt order was lifted on 26 May. 7th Panzer continued its advance, reaching Lille on 27 May. For the assault, Hoth placed the 5th Panzer Division under Rommel's command. The Siege of Lille continued until 31 May, when the French garrison of 40,000 men surrendered. 7th Panzer was given six days leave, during which Rommel was summoned to Berlin to meet with Hitler. He was the only divisional commander present at the planning session for "Fall Rot" (Case Red), the second phase of the conquest of France. By this time the evacuation of the BEF was complete; over 338,000 Allied troops had been evacuated across the Channel, though they had to leave behind all their heavy equipment and vehicles.
Drive for the Channel.
Rommel, resuming his advance on 5 June, drove for the River Seine to secure the bridges near Rouen. Advancing in two days, the division reached Rouen only to find the bridges destroyed. On 10 June, Rommel reached the coast near Dieppe, sending Hoth the laconic message "Am at coast". On 17 June, 7th Panzer was ordered to advance on Cherbourg, where additional British evacuations were underway. The Division advanced in 24 hours, and after two days of shelling, the French garrison surrendered on 19 June. The speed and surprise it was consistently able to achieve, to the point where both the enemy and the "Oberkommando des Heeres" (OKH; German High Command) at times lost track of its whereabouts, earned the 7th Panzers the nickname "Gespensterdivision" (Ghost Division).
After the armistice with the French was signed on 22 June, the division was placed in reserve, being sent first to the Somme and then to Bordeaux to re-equip and prepare for "Unternehmen Seelöwe" (Operation Sea Lion), the planned invasion of Britain. This invasion was later cancelled as Germany was not able to acquire the air superiority deemed a necessity for a successful outcome.
North Africa 1941–43.
On 6 February 1941, Rommel was appointed commander of the newly created "Deutsches Afrika Korps" (DAK), consisting of the 5th Light Division (later redesignated 21st Panzer Division) and of the 15th Panzer Division. He was promoted to the rank of "Generalleutnant" three days later and flew to Tripoli on 12 February. The DAK had been sent to Libya in Operation Sonnenblume, to support Italian troops that had been severely defeated by British Commonwealth forces in Operation Compass. His efforts in the Western Desert Campaign earned Rommel the nickname the "Desert Fox" from British journalists. Allied troops in Africa were commanded by General Archibald Wavell, Commander-in-Chief Middle East Command.
First Axis offensive.
Rommel and his troops were technically subordinate to Italian commander-in-chief General Italo Gariboldi. Disagreeing with the Oberkommando der Wehrmacht (OKW)'s orders to assume a defensive posture along the front line at Sirte, Rommel resorted to subterfuge and insubordination to take the war to the British. He took advantage of his connections with Hitler to obtain approval to launch a limited offensive on 24 March with 5th Light Division, supported by two Italian divisions. This thrust was not anticipated by the British, who had Ultra intelligence showing that Rommel had orders to remain on the defense until at least May, when the 15th Panzers were due to arrive.
The British Western Desert Force had meanwhile been weakened by the transfer in mid-February of three divisions to help defend Greece. They fell back to Mersa El Brega and started constructing defensive works. Rommel continued his attack against these positions to prevent the British from building up their fortifications. After a day of fierce fighting on 31 March, the Germans captured Mersa El Brega. Splitting his force into three groups, Rommel resumed the advance on 3 April. Benghazi fell that night as the British pulled out of the city. Gariboldi, who had ordered Rommel to stay in Mersa El Brega, was furious. Rommel was equally forceful in his response, telling Gariboldi: "One cannot permit unique opportunities to slip by for the sake of trifles." At that point a signal arrived from General Franz Halder reminding Rommel that he was to halt in Mersa El Brega. Knowing Gariboldi could not speak German, Rommel told him the message gave him complete freedom of action. Gariboldi backed down.
On 4 April Rommel was advised by his supply officers that fuel was running short, which could result in a delay of up to four days. The problem was ultimately Rommel's fault, as he had not advised his supply officers of his intentions, and no fuel dumps had been set up. Rommel ordered the 5th Light Division to unload all their lorries and return to El Agheila to collect fuel and ammunition. Driving through the night, they were able to reduce the halt to a single day. Fuel supply was problematic throughout the campaign, as no petrol was available locally; it had to be brought from Europe via tanker and then carried by road to where it was needed. Food and fresh water were also in short supply, and it was difficult to move tanks and other equipment off-road through the sand. In spite of these problems, Cyrenaica was captured by 8 April, except for the port city of Tobruk, which was surrounded on the landward sides on 11 April.
Siege of Tobruk.
The siege of Tobruk was not technically a siege, as the defenders were still able to move supplies and reinforcements into the city via the port. Rommel knew that by capturing the port he could greatly reduce the length of his supply lines and increase his overall port capacity, which was insufficient even for day-to-day operations and only half that needed for offensive operations. The city, which had been heavily fortified by the Italians during their 30-year occupation, was garrisoned by the 18th Infantry Brigade of the Australian 7th Division, the Australian 9th Division, HQ 3rd Armoured Brigade, several thousand British infantrymen, and one regiment of Indian infantry, for a total of 36,000 men. The commanding officer was Australian Lieutenant General Leslie Morshead. Hoping to catch the defenders off-guard, Rommel launched a failed attack on 14 April.
Rommel requested reinforcements, but the OKW, then completing preparations for Operation Barbarossa, refused. General Friedrich Paulus, head of the Operations Branch of OKH, arrived on 25 April to review the situation. He was present for a second failed attack on the city on 30 April. On 4 May Paulus ordered that no further attempts should be made to take Tobruk via a direct assault. This order was not open to interpretation, and Rommel had no choice but to comply. Aware of this order from intelligence reports, Churchill urged Wavell to seize the initiative. While awaiting further reinforcements and a shipment of 300 tanks that were already on their way, Wavell launched a limited offensive code named Operation Brevity on 15 May. The British briefly seized Sollum, Fort Capuzzo, and the important Halfaya Pass, a bottleneck along the coast near the border between Libya and Egypt. Rommel soon forced them to withdraw. On 15 June Wavell launched Operation Battleaxe. The attack was defeated in a four-day battle at Sollum and Halfaya Pass, resulting in the loss of 98 British tanks. The Germans lost 12 tanks, while capturing and putting into service over 20 British tanks. The defeat resulted in Churchill replacing Wavell with General Claude Auchinleck as theatre commander. Rommel appointed Heinrich Kirchheim as commander of 5th Light Division on 16 May, became displeased and replaced him with Johann von Ravenstein on 30 May 1941.
In August, Rommel was appointed commander of the newly created Panzer Group Africa, with Fritz Bayerlein as his chief of staff. The Afrika Korps, comprising the 15th Panzer Division and the 5th Light Division, now reinforced and redesignated 21st Panzer Division, was put under command of Generalleutnant Ludwig Crüwell. In addition to the Afrika Korps, Rommel's Panzer Group had the 90th Light Division and four Italian divisions, three infantry divisions investing Tobruk, and one holding Bardia. The two Italian armoured divisions, "Ariete" and "Trieste", were still under Italian control. They formed the Italian XX Motorized Corps under the command of General Gastone Gambara. Two months later Hitler decided he must have German officers in better control of the Mediterranean theatre, and appointed Field Marshal Albert Kesselring as Commander in Chief, South. Kesselring was ordered to get control of the air and sea between Africa and Italy.
Operation Crusader.
Following his success in Battleaxe, Rommel returned his attention to the capture of Tobruk. He made preparations for a new offensive, to be launched between 15 and 20 November. Meanwhile, Auchinleck reorganised Allied forces and strengthened them to two corps, XXX and XIII, which formed the British Eighth Army, which was placed under the command of Alan Cunningham. Auchinleck had 770 tanks and double the number of Axis aircraft. Rommel opposed him with the 15th and 21st Panzer Divisions with a total of 260 tanks, the 90th Light Infantry division, five Italian infantry divisions, and one Italian armoured division of 278 tanks.
Auchinleck launched Operation Crusader, a major offensive to relieve Tobruk, on 18 November 1941. The XIII Corps on the right were assigned to attack Sidi Omar, Capuzzo, Sollum, and Bardia; the XXX Corps (which included most of the armour) were to move on the left southern flank to a position about south of Tobruk, with the expectation that Rommel would find this move so threatening that he would move his armour there in response. Once Rommel's tanks were written down, the British 70th Infantry Division would break out of Tobruk to link up with XXX Corps. Rommel reluctantly decided on 20 November to call off his planned attack on Tobruk.
Some elements of the 7th Armoured Division were stopped on the 19th by the Italian "Ariete" Armoured Division at Bir el Gobi, but they also managed to capture the airfields at Sidi Rezegh, from Tobruk. Engaging the Allied tanks located there became Rommel's primary objective. Noting that the British armour was split into three discontiguous groups, he concentrated his Panzers so as to gain local superiority. The expected breakout from Tobruk, which took place on 20 November, was stopped by the Italians. The airfield at Sidi Rezegh was retaken by 21st Panzer on 22 November. In four days of fighting, the Eighth Army lost 530 tanks and Rommel only 100. The German forces near Halfaya Pass were cut off on 23 November.
Wanting to exploit the British halt and their apparent disorganisation, on 24 November Rommel counterattacked near the Egyptian border in an operation that became known as the "dash to the wire". Unknown to Rommel, his troops passed within of a major British supply dump. Cunningham asked Auchinleck for permission to withraw into Egypt, but Auchinleck refused, and soon replaced Cunningham as commander of Eighth Army with Major General Neil Ritchie. The German counterattack stalled as it outran its supplies and met stiffening resistance, and was criticised by the German High Command and some of Rommel's staff officers.
While Rommel drove into Egypt, the remaining Commonwealth forces east of Tobruk threatened the weak Axis lines there. Unable to reach Rommel for several days, Rommel's Chief of Staff, Siegfried Westphal, ordered the 21st Panzer Division withdrawn to support the siege of Tobruk. On 27 November the British attack on Tobruk linked up with the defenders, and Rommel, having suffered losses that could not easily be replaced, had to concentrate on regrouping the divisions that had attacked into Egypt. By 7 December Rommel fell back to a defensive line at Gazala, just west of Tobruk, all the while under heavy attack from the Desert Air Force. The Bardia garrison surrendered on 2 January and Halfaya on 17 January 1942. The Allies kept up the pressure, and Rommel was forced to retreat all the way back to the starting positions he had held in March, reaching El Agheila in December 1941. The British had retaken almost all of Cyrenaica, but Rommel's retreat dramatically shortened his supply lines.
Battle of Gazala and capture of Tobruk.
On 5 January 1942 the Afrika Korps received 55 tanks and new supplies and Rommel started planning a counterattack. On 21 January, Rommel launched the attack. Caught by surprise by the Afrika Korps, the Allies lost over 110 tanks and other heavy equipment. The Axis forces retook Benghazi on 29 January and Timimi on 3 February, with the Allies pulling back to a defensive line just before the Tobruk area south of the coastal town of Gazala. Rommel placed a thin screen of mobile forces before them, and held the main force of the Panzerarmee well back near Antela and Mersa Brega. Between December 1941 and June 1942, Rommel had excellent information about the disposition and intentions of the Commonwealth forces. Bonner Fellers, the US diplomat in Egypt, was sending detailed reports to the US State Department using a compromised code.
Following Kesselring's successes in creating local air superiority around the British naval and air bases at Malta in April 1942, an increased flow of supplies reached the Axis forces in Africa. With his forces strengthened, Rommel contemplated a major offensive operation for the end of May. He knew the British were planning offensive operations as well, and he hoped to pre-empt them. While out on reconnaissance on 6 April, he was severely bruised in the abdomen when his vehicle was the target of artillery fire.
The British had 900 tanks in the area, 200 of which were new Grant tanks. Unlike the British, the Axis forces had no armoured reserve; all operable equipment was put into immediate service. Rommel's Panzer Army Africa had a force of 320 German tanks; 50 of these were the light Panzer II model. In addition, 240 Italian tanks were in service, but these were also under-gunned and poorly armoured.
Early in the afternoon of 26 May 1942, Rommel attacked first and the Battle of Gazala commenced. Italian infantry supplemented with small numbers of armoured forces assaulted the centre of the Gazala fortifications. To give the impression that this was the main assault, spare aircraft engines mounted on trucks were used to create huge clouds of dust. Ritchie was not convinced by this display, and left the 4th and 22nd Armoured Brigades in position at the south end of the Commonwealth position. Under the cover of darkness, the bulk of Rommel's motorized and armoured forces (15th and 21st Panzers, 90th Light Division, and the Italian Ariete and Trieste Divisions) drove south to skirt the left flank of the British, coming up behind them and attacking to the north the following morning. Throughout the day a running armour battle occurred, where both sides took heavy losses. The Grant tanks proved to be impossible to knock out except at close range. Renewing the attack on the morning of 28 May, Rommel concentrated on encircling and destroying separate units of the British armour. Repeated British counterattacks threatened to cut off and destroy the Afrika Korps. Running low on fuel, Rommel assumed a defensive posture, forming "the Cauldron". He made use of the extensive British minefields to shield his western flank. Meanwhile, Italian infantry cleared a path through the mines to provide supplies. On 30 May Rommel resumed the offensive, attacking westwards to link with elements of Italian X Corps, which had cleared a path through the Allied minefields to establish a supply line. On 1 June, Rommel accepted the surrender of some 3,000 soldiers of the 150th Brigade. On 6 June, 90th Light Division and the "Trieste" Division assaulted the Free French strongpoint in the Battle of Bir Hakeim, but the defenders continued to thwart the attack until finally evacuating on 10 June. With his communications and the southern strongpoint of the British line thus secured, Rommel shifted his attack north again, relying on the British minefields of the Gazala lines to protect his left flank. Threatened with being completely cut off, the British began a retreat eastward toward Egypt on 14 June, the so-called "Gazala Gallop."
On 15 June Axis forces reached the coast, cutting off the escape for the Commonwealth forces still occupying the Gazala positions. With this task completed, Rommel struck for Tobruk while the enemy was still confused and disorganised. Tobruk's defenders were at this point the 2nd South African Infantry Division, 4th Antiaircraft Brigade, 11th Indian Infantry, 32nd Army Tank, and 201st Guards Brigades, all under command of Generalmajor (major general) Hendrik Klopper. The assault on Tobruk began at dawn on 20 June, and Klopper surrendered at dawn the following day. With Tobruk, Rommel achieved the capture of 32,000 defenders, the port, and huge quantities of supplies. Only at the fall of Singapore, earlier that year, had more British Commonwealth troops been captured at one time. On 22 June, Hitler promoted Rommel to Generalfeldmarschall for this victory.
Following his success at Gazala and Tobruk, Rommel wanted to seize the moment and not allow 8th Army a chance to regroup. He strongly argued that the Panzerarmee should advance into Egypt and drive on to Alexandria and the Suez Canal, as this would place almost all Mediterranean coastline in Axis hands, ease conditions on the Eastern Front, and potentially lead to the capture from the south of the oil fields in the Caucasus and Middle East. However, Hitler viewed the North African campaign primarily as a way to assist his Italian allies, not as an objective in and of itself. He would not consider sending Rommel the reinforcements and supplies he needed to take and hold Egypt, as this would have required diverting men and supplies from his primary focus: the Eastern Front.
Rommel's success at Tobruk worked against him, as Hitler no longer felt it was necessary to proceed with Operation Herkules, the proposed attack on Malta. Auchinleck relieved Ritchie of command of the Eighth Army on 25 June, and temporarily took command himself. Rommel knew that delay would only benefit the British, who continued to receive supplies at a faster rate than Rommel could hope to achieve. He pressed an attack on the heavily fortified town of Mersa Matruh, which Auchinleck had designated as the fall-back position, surrounding it on 28 June. The 2nd New Zealand Division and 50th (Northumbrian) Infantry Division were almost caught, with 50th Division fleeing on the 27th and 2nd Division escaping after a short engagement during the pre-dawn hours of 28 June. The four divisions of X Corps were caught in the encirclement, and were ordered by Auchinleck to attempt a breakout. The 29th Indian Infantry Brigade was nearly destroyed, losing 6,000 troops and 40 tanks. The fortress fell on 29 June. In addition to stockpiles of fuel and other supplies, the British abandoned hundreds of tanks and trucks. Those that were functional were put into service by the Panzerarmee.
El Alamein.
First Battle of El Alamein.
Rommel continued his pursuit of the Eighth Army, which had fallen back to heavily prepared defensive positions at El Alamein. This region is a natural choke point, where the Qattara Depression creates a relatively short line to defend that could not be outflanked to the south because of the steep escarpment. On 1 July the First Battle of El Alamein began. Rommel had around 100 available tanks. The Allies were able to achieve local air superiority, with heavy bombers attacking the 15th and 21st Panzers, who had also been delayed by a sandstorm. The 90th Light Division veered off course and were pinned down by South African artillery fire. Rommel continued to attempt to advance for two more days, but repeated sorties by the RAF meant he could make no progress. On 3 July, he wrote in his diary that his strength had "faded away". Attacks by 21st Panzer on 13 and 14 July were repulsed, and an Australian attack on 16–17 July was held off with difficulty. Throughout the first half of July, Auchinleck concentrated attacks on the Italian 60th Infantry Division Sabratha at Tel el Eisa. The ridge was captured by the 26th Australian Brigade on 16 July. Both sides suffered similar losses throughout the month, but the Axis supply situation remained less favourable. Rommel realised that the tide was turning. A break in the action took place at the end of July as both sides rested and regrouped.
Preparing for a renewed drive, the British replaced Auchinleck with General Harold Alexander on 8 August. Bernard Montgomery was made the new commander of Eighth Army that same day. The Eighth Army had initially been assigned to General William Gott, but he was killed when his plane was shot down on 7 August. Rommel knew that a British convoy carrying over 100,000 tons of supplies was due to arrive in September. He decided to launch an attack at the end of August with the 15th and 21st Panzer Division, 90th Light Division, and the Italian XX Motorized Corps in a drive through the southern flank of the El Alamein lines. Expecting an attack sooner rather than later, Montgomery fortified the Alam el Halfa ridge with the 44th Division, and positioned the 7th Armoured Division about to the south.
Battle of Alam El Halfa.
The Battle of Alam el Halfa was launched on 30 August. The terrain left Rommel with no choice but to follow a similar tactic as he had at previous battles: the bulk of the forces attempted to sweep around from the south while secondary attacks were launched on the remainder of the front. It took much longer than anticipated to get through the minefields in the southern sector, and the tanks got bogged down in unexpected patches of quicksand (Montgomery had arranged for Rommel to acquire a falsified map of the terrain). Under heavy fire from British artillery and aircraft, and in the face of well prepared positions that Rommel could not hope to outflank due to lack of fuel, the attack stalled. By 2 September, Rommel realized the battle was unwinnable, and decided to withdraw.
Montgomery had made preparations to cut the Germans off in their retreat, but in the afternoon of 2 September he visited Corps commander Brian Horrocks and gave orders to allow the Germans to retire. This was to preserve his own strength intact for the main battle which was to come. On the night of 3 September the 2nd New Zealand Division and 7th Armoured Division positioned to the north engaged in an assault, but they were repelled in a fierce rearguard action by the 90th Light Division. Montgomery called off further action to preserve his strength and allow for further desert training for his forces. In the attack Rommel had suffered 2,940 casualties and lost 50 tanks, a similar number of guns, and 400 lorries, vital for supplies and movement. The British losses, except tank losses of 68, were much less, further adding to the numerical inferiority of Panzer Army Afrika. The Desert Air Force inflicted the highest proportions of damage to Rommel's forces. He now realized the war in Africa could not be won. Physically exhausted and suffering from a liver infection and low blood pressure, Rommel flew home to Germany to recover his health. General Georg Stumme was left in command in Rommel's absence.
Second Battle of El Alamein.
Improved decoding by British intelligence meant that the Allies had advance knowledge of virtually every Mediterranean convoy, and only 30 per cent of shipments were getting through. In addition, Mussolini diverted supplies intended for the front to his garrison at Tripoli, and refused to release any additional troops to Rommel. The increasing Allied air superiority and lack of fuel meant Rommel was forced to take a more defensive posture than he would have liked for the second Battle of El Alamein. The German defences to the west of the town included a minefield deep with the main defensive line – itself several thousand yards deep – to its west. This, Rommel hoped, would allow his infantry to hold the line at any point until motorized and armoured units in reserve could move up and counterattack any Allied breaches. The British offensive began on 23 October. Stumme, in command in Rommel's absence, died of an apparent heart attack while examining the front on 24 October, and Rommel was ordered to return from his medical leave, arriving on the 25th. Montgomery's intention was to clear a narrow path through the minefield at the northern part of the defenses, at the area called Kidney Ridge, with a feint to the south. By the end of 25 October, 15th Panzers, the defenders in this sector, had only 31 serviceable tanks remaining of their initial force of 119. Rommel brought north the 21st Panzer and Ariete Divisions on 26 October to bolster the sector. On the 28th, Montgomery shifted his focus to the coast, ordering his 1st and 10th Armoured Divisions to attempt to swing around and cut off Rommel's line of retreat. Meanwhile, Rommel concentrated his attack on the Allied salient at Kidney Ridge, inflicting heavy losses. However, Rommel had only 150 operational tanks remaining, and Montgomery had 800, many of them Shermans.
Montgomery, seeing his armoured brigades losing tanks at an alarming rate, stopped major attacks until the early hours of 2 November, when he opened "Operation "Supercharge"" with a massive artillery barrage. This was followed by penetration at the salient by two armoured and two infantry divisions. Rommel's counterattack at 11:00 inflicted severe casualties on the Commonwealth troops, but by 20:00, with only 35 tanks remaining, he ordered his forces to disengage and begin to withdraw. At midnight, he informed the OKW of his decision, and received a reply directly from Hitler the following afternoon: he ordered Rommel and his troops to hold their position to the last man. Rommel, who believed that the lives of his soldiers should never be squandered needlessly, was stunned. While he (like all members of the Wehrmacht) had pledged an oath of absolute obedience to Hitler, he thought this order was pointless, even madness, and had to be disobeyed. Rommel initially complied with the order, but after discussions with Kesselring and others, he issued orders for a retreat on 4 November. The delay proved costly in terms of his ability to get his forces out of Egypt. He later said the decision to delay was what he most regretted from his time in Africa. Meanwhile, the British 1st and 7th Armoured Division had broken through the German defences and were preparing to swing north and surround the Axis forces. On the evening of the 4th, Rommel finally received word from Hitler authorizing the withdrawal. By this time it was impossible for Rommel to save his non-motorized units.
End of Africa campaign.
Retreat across Africa.
As Rommel attempted to withdraw his forces before the British could cut off his retreat, he was forced to fight a series of delaying actions. Heavy rains slowed movements and grounded the Desert Air Force, which aided the withdrawal. Those parts of Panzerarmee Africa that were motorized slipped away from El Alamein, but were under pressure from the pursuing Eighth Army. A series of short delaying actions were fought over the coastal highway, but no line could be held for any length of time, as Rommel lacked the armour and fuel to defend his open southern flank. Rommel continued to do the only thing sensible, and moved his army west, abandoning Halfaya Pass, Sollum, Mersa Brega and El Agheila. Tripolitania, with its many steep scarps cut in places by dried-up watercourses, made for useful defensive terrain, but the line Rommel was aiming for was 'Gabes gap' in Tunisia. Luftwaffe Field Marshal Kesselring strongly criticized Rommel's decision to retreat all the way to Tunisia, as each airfield the Germans abandoned extended the range of the Allied bombers and fighters. Rommel defended his decision, pointing out that if he tried to assume a defensive position the Allies would destroy his forces and take the airfields anyway; the retreat saved the lives of his remaining men and shortened his supply lines. By now, Rommel's remaining forces fought in reduced strength combat groups, whereas the Allied forces had great numerical superiority and control of the air. Upon his arrival in Tunisia, Rommel noted with some bitterness the reinforcements, including the 10th Panzer Division, arriving in Tunisia following the Allied invasion of Morocco. He felt these could have made all the difference at El Alamein. Their arrival in Tunisia was to a position which he knew Germany ultimately could not hold.
Tunisia.
Having reached Tunisia, Rommel launched an attack against the U.S. II Corps which was threatening to cut his lines of supply north to Tunis. Rommel inflicted a sharp defeat on the American forces at the Kasserine Pass in February—what proved to be his last battlefield victory of the war, as well as his first battle against the United States Army.
Rommel immediately turned back against the British forces, occupying the Mareth Line (old French defences on the Libyan border). But Rommel could only delay the inevitable. While Rommel was at Kasserine at the end of January 1943, the Italian General Giovanni Messe was appointed commander of Panzer Army Africa, renamed the Italo-German Panzer Army in recognition of the fact that it consisted of one German and three Italian corps. Though Messe replaced Rommel, he diplomatically deferred to him, and the two coexisted in what was theoretically the same command. On 23 February "Armeegruppe Afrika" was created with Rommel in command. It included the Italo-German Panzer Army under Messe (renamed 1st Italian Army) and the German 5th Panzer Army in the north of Tunisia under General Hans-Jürgen von Arnim.
The last Rommel offensive in North Africa was on 6 March 1943, when he attacked Eighth Army at the Battle of Medenine. The attack was made with 10th, 15th, and 21st Panzer Divisions. Warned by Ultra intercepts, Montgomery deployed large numbers of anti-tank guns in the path of the offensive. After losing 52 tanks, Rommel called off the assault. On 9 March he returned to Germany in an effort to get Hitler to comprehend the reality of the changing situation. In this he was unsuccessful. Command was handed over to General Hans-Jürgen von Arnim. Rommel never returned to Africa. The fighting there continued on for another two months, until 13 May 1943, when General Messe surrendered the exhausted remnants of "Armeegruppe Afrika" to the Allies.
Italy 1943.
On 23 July 1943 Rommel was moved to Greece as commander of Army Group E to counter a possible British invasion of the Greek coast. This was an idea which Churchill advocated, but ultimately never occurred. British intelligence, however, used the idea as part of their ongoing efforts to mislead and extend the German army, this aspect being known as "Operation Mincemeat". Rommel returned to Germany upon the overthrow of Mussolini, and on 17 August 1943 was sent to Northern Italy to prepare a northern line of defense. Rommel was headquartered in Lake Garda as commander of the newly formed Army Group B.
In Italy, Rommel and Hitler's disagreement reached a new level. Rommel had wrongly predicted that the collapse of the German line in Italy would be fast. Hitler in 1944 expressed these doubts: "In Italy too he predicted our collapse as being just around the corner. It still has not occurred. Events have proved him completely wrong, and thoroughly justified my decision to leave Field Marshal Kesselring there. My view is that without optimism you cannot be a military commander".
On 21 November Hitler gave Kesselring overall command of the Italian theater, moving Rommel and Army Group B to Normandy in France with responsibility for defending the French coast against the long anticipated Allied invasion.
Atlantic Wall 1944.
There was broad disagreement in the German High Command as to how best to meet the expected allied invasion of Northern France. The Commander-in-Chief West, Gerd von Rundstedt, believed there was no way to stop the invasion near the beaches due to the firepower possessed by the Allied navies, as had been experienced at Salerno. He argued that the German armour should be held in reserve well inland near Paris where they could be used to counter-attack in force in a more traditional military doctrine. The allies could be allowed to extend themselves deep into France where a battle for control would be fought, allowing the Germans to envelop the allied forces in a pincer movement, cutting off their avenue of retreat. These ideas were supported by other officers, most notably Heinz Guderian and Panzer Group West commander Leo Geyr. They feared the piecemeal commitment of their armoured forces would cause them to become caught in a battle of attrition which they could not hope to win. The notion of holding the armour inland to use as a mobile reserve force from which they could mount a powerful counterattack applied the classic use of armoured formations as seen in France 1940. These tactics were still effective on the Eastern Front, where control of the air was important but did not dominate the action. Rommel's own experiences at the end of the North African campaign revealed to him that the Germans would not be allowed to preserve their armour from air attack for this type of massed assault. Rommel believed their only opportunity would be to oppose the landings directly at the beaches, and to counterattack there before the invaders could become well established. Though there had been some defensive positions established and gun emplacements made, the Atlantic Wall was a token defensive line. Rommel believed if the Wehrmacht would have any chance, beach defenses would have to be created and the forces available brought close enough to the allied invaders as to make airstrikes against them difficult.
Upon arriving in Northern France Rommel was dismayed by the lack of completed works and the slow building pace. He feared he had just a few months before an invasion. His presence greatly invigorated the fortification effort along the Atlantic Wall. He had millions of mines laid and thousands of tank traps and obstacles set up on the beaches and throughout the countryside, including in fields suitable for glider aircraft landings, the so-called "Rommelspargel" ("Rommel's asparagus"). Rommel's arrival in Northern France instilled a great deal of purpose to the demoralized units that were simply waiting for the inevitable attack. His efforts to buttress the Atlantic Wall went a long way in improving their effectiveness. If given more time, he may have succeeded. U.S. Navy Commander Edward Ellsberg said of the various Atlantic Wall obstacles, "Rommel had thoroughly muddled our plans. Attacking at high tide as we had intended, we'd never get enough troops in over those obstacles..." The obstructions compelled the Allies to land at low tide, which narrowed the time frames they could land and increased the length of the beach to be crossed, but uncovered and revealed the obstacles, reducing their effectiveness.
Rundstedt expected the Allies to invade in the Pas-de-Calais because it was the shortest crossing point from Britain, its port facilities were essential to supplying a large invasion force, and the distance from Calais to Germany was relatively short. Hitler and his various intelligence services largely agreed with this assessment. Rommel, believing that Normandy was indeed a likely landing ground, argued that it did not matter to the Allies where they landed, just that the landing was successful.
Hitler vacillated between the two strategies. In late April, he ordered the 1st SS Panzer Corps placed near Paris, far enough inland to be useless to Rommel, but not far enough for Rundstedt. Rommel moved those armoured formations under his command as far forward as possible, ordering General Erich Marcks, commanding the 84th Corps defending the Normandy section, to move his reserves into the frontline. Rommel's strategy of an armor-supported defense line was scoffed at by most of his fellow commanders including Rundstedt, but his support from Hitler and Goebbels meant he could put all of it into effect except the Panzer divisions; however, these were, in his view, the most critical parts of the plan.
The Allies staged elaborate deceptions for D-Day (see Operation Fortitude), giving the impression that the landings would be at Calais. Although Hitler himself expected a Normandy invasion for a while, Rommel and most Army commanders in France believed there would be two invasions, with the main invasion coming at the Pas-de-Calais. Rommel drove defensive preparations all along the coast of Northern France, particularly concentrating fortification building in the River Somme estuary. By D-Day on 6 June 1944 nearly all the German staff officers, including Hitler's staff, believed that Pas-de-Calais was going to be the main invasion site, and continued to believe so even after the landings in Normandy had occurred.
A part of the difficulty in the German response to the landings in Northern France was a split command structure. Anxious of the power of the regular army, Hitler had created a second service, the Waffen-SS, which was not under command of the regular army but under his own direct command. In addition, a great number of the land forces included units under the control of the Luftwaffe, including the paratrooper forces and various flak units, while others were under command of the Kriegsmarine. 14 of the 62 divisions in the west, and 7 of the 25 first grade formations were not part of the army. This weakened the ability of the army to control and respond to the battle. To make matters worse for the Germans, the 5 June storm in the channel seemed to make a landing very unlikely, and a number of the senior officers were away from their units for training exercises and various other efforts. All this made the German command structure in France in disarray during the opening hours of the D-Day invasion. On 4 June the chief meteorologist of the 3 Air Fleet reported that weather in the channel was so poor there could be no landing attempted for two weeks. On 5 June Rommel set out to visit his family on 6 June, planning to then go on to meet with Hitler at the Berchtesgaden to persuade him that the 12th SS Panzer Division should be moved forward to the St. Lo-Carantan area.. Several units, notably the 12th SS Panzer Division and Panzer-Lehr-Division, were near enough that they could have caused serious havoc. However Hitler refused to release these units over his continued concern over a second landing at the Pas de Calais. Facing relatively small-scale German counterattacks, the Allies quickly secured all beachheads except Omaha.
The Allies pushed ashore and expanded their beachhead despite strong German resistance. Rommel believed that if his armies pulled out of range of Allied naval fire, it would give them a chance to regroup and re-engage them later with a better chance of success. While he managed to convince Rundstedt, they still needed to win over Hitler. However, at a meeting with Hitler in Margival on 17 June—in the bunker Hitler had intended to use for Operation Sea Lion—Hitler would not even consider a withdrawal. Undaunted, Rommel told Hitler that given the near-certainty that both the Normandy and Eastern fronts would collapse. Under the circumstances, he felt Hitler should sue for peace. Hitler would not hear of it.
By mid-July the German position was crumbling. On 17 July 1944, Rommel was returning from visiting the headquarters of Sepp Dietrich, the commander of 1st SS Panzer Corps, being driven back to Army Group B headquarters in his staff car. According to a widely accepted version of events, an RCAF Spitfire of 412 Squadron piloted by Charley Fox strafed the car near Sainte-Foy-de-Montgommery. The car sped up and attempted to get off the main roadway, but a 20 mm round shattered the driver's left arm, causing the vehicle to come off the road and crash into some trees. Rommel was thrown from the car, suffering injuries to the left side of his face from glass shards and three fractures to his skull. He was hospitalised with major head injuries.
Plot against Hitler.
There had always been opposition to Hitler in conservative circles and in the Army, the Schwarze Kapelle (Black Orchestra), but Hitler's dazzling successes in 1938–1941 had stifled it. However, after the Soviet campaign failed, and the Axis suffered more defeats, this opposition underwent a revival.
Early in 1944, three of Rommel's closest friends—the "Oberbürgermeister" of Stuttgart, Karl Strölin (who had served with Rommel in the First World War), Alexander von Falkenhausen, and Carl Heinrich von Stülpnagel—began efforts to bring Rommel into the anti-Hitler conspiracy. They felt that as by far the most popular officer in Germany, he would lend their cause badly needed credibility with the populace. Meetings between Rommel and them were organized by Rommel's chief of staff Hans Speidel. Additionally, the conspirators felt they needed the support of a field marshal on active duty. Erwin von Witzleben, who would have become commander-in-chief of the Wehrmacht if Hitler had been overthrown, was a field marshal, but had not been on active duty since 1942. Sometime in February, Rommel agreed to lend his support to the conspiracy in order to, as he put it, "come to the rescue of Germany." At a meeting between Spiedel, former foreign minister Konstantin von Neurath, and Stuttgart Lord Mayor Karl Strölin, it was agreed that Rommel should be ready at some point to become either interim head of state or commander-in-chief—posts that Rommel never actively sought.
Rommel opposed assassinating Hitler. After the war, his widow—among others—maintained that Rommel believed an assassination attempt would spark civil war in Germany and Austria, and Hitler would have become a martyr for a lasting cause. Instead, Rommel insisted that Hitler be arrested and brought to trial for his crimes. Two days before Rommel's car was fired upon by the Allies, Spiedel persuaded Rommel to write a letter to Hitler telling him that the troops in the west were hopelessly overmatched despite "fighting heroically." He urged Hitler to "draw the proper conclusions without delay." The message was clear—Rommel felt the war must end. He later told Spiedel that this letter was Hitler's "last chance" to save himself, and "if he does not take it, we will act." Later, he told fellow conspirator Caesar von Hofacker, "Tell the people in Berlin they can count on me."
After the failed bomb attack of 20 July, many conspirators were arrested and the dragnet expanded to anyone even suspected of participating. This led Rommel to tell Spiedel that Hitler had "gone completely mad." However, it did not take long for Rommel's involvement to come to light. Rommel's name was first mentioned when Stülpnagel blurted it out during an interrogation after he failed in an attempt at suicide. Later, Hofacker admitted under particularly severe Gestapo interrogation that Rommel was actively involved.
Additionally, Carl Goerdeler, the main civilian leader of the Resistance, wrote on several letters and other documents that Rommel was a potential supporter and an acceptable military leader to be placed in a position of responsibility should their coup succeed. Nazi party officials in France reported that Rommel extensively and scornfully criticised Nazi incompetence and crimes. Gestapo agents went to Rommel's house in Ulm and placed him under partial house arrest.
Death.
The "Court of Military Honour"—a drumhead court-martial convened to decide the fate of officers involved in the conspiracy—included three men with whom Rommel had crossed swords before: Heinz Guderian, Gerd von Rundstedt and Heinrich Kirchheim (the latter whom Rommel had fired after Tobruk in 1941). The Court decided that Rommel should be expelled from the Army in disgrace and brought before Roland Freisler's People's Court, a kangaroo court that always decided in favour of the prosecution. However, Hitler knew that having Rommel branded as a traitor would severely damage morale on the home front. He and Wilhelm Keitel thus decided to offer Rommel the chance to take his own life.
Two generals from Hitler's headquarters, Wilhelm Burgdorf and Ernst Maisel, visited Rommel at his home on 14 October 1944. Burgdorf informed him of the charges and offered him a choice: he could either face the People's Court—which would have been tantamount to a death sentence—or choose a quiet suicide. In the former case, his family would have suffered even before the all-but-certain conviction and execution, and his staff would have been arrested and executed as well. In the latter case, the government would claim that he died a hero and bury him with full military honours, and his family would receive full pension payments. Burgdorf had brought a cyanide capsule. Rommel retired to think the matter over, during which he learned that SS detachments had surrounded his village. He realized that if he agreed to face the People's Court, he would have likely been "shot while trying to escape" before he even made it to Berlin. With this in mind, Rommel opted to commit suicide, and explained his decision to his wife and son. Wearing his Afrika Korps jacket and carrying his field marshal's baton, Rommel went to Burgdorf's Opel, driven by SS Master Sergeant Heinrich Doose, and was driven out of the village. After stopping, Doose and Maisel walked away from the car, leaving Rommel with Burgdorf. Five minutes later Burgdorf gestured to the two men to return to the car, and Doose noticed that Rommel was slumped over, having taken the cyanide. Ten minutes later the group phoned Rommel's wife to inform her of Rommel's death.
The official story of Rommel's death, as initially reported to the public, stated that Rommel had died of a cerebral embolism—a complication of the skull fractures he'd suffered in the earlier strafing of his staff car. To further strengthen the story, Hitler ordered an official day of mourning in commemoration. As previously promised, Rommel was given a state funeral. The fact that his state funeral was held in Ulm instead of Berlin had, according to his son, been stipulated by Rommel. Hitler sent Field Marshal von Rundstedt, who was unaware that Rommel had died as a result of Hitler's orders, as his representative at Rommel's funeral. Rommel had specified that no political paraphernalia be displayed on his corpse, but the Nazis made sure his coffin was festooned with swastikas. The truth behind Rommel's death became known to the Allies when intelligence officer Charles Marshall interviewed Rommel's widow, Lucia Rommel, as well as from a letter by Rommel's son Manfred in April 1945.
Following the war, Rommel's diary and letters were edited by military historian B. H. Liddell Hart and published as "The Rommel Papers". His grave can be found in Herrlingen, a short distance west of Ulm. For decades after the war on the anniversary of his death, veterans of the Africa campaign, including former opponents, would gather at Rommel's tomb in Herrlingen. He is the only member of the Third Reich establishment to have a museum dedicated to him.
In 2013, it was revealed that Friedrich Breiderhoff wrote a report for the Cologne police on 22 July 1960, describing the circumstances which forced him to falsify Rommel's death certificate in 1944.
Rommel's style as military commander.
Manoeuvre warfare.
Taking his opponent by surprise and creating uncertainty in the mind of the adversarial commander were key elements in Rommel's thinking on offensive warfare. Rommel understood the impact of striking quickly, and his offensive campaigns are noted for his ability to arrive in force where his opponents did not expect him. Rommel would take advantage of sand storms and the dark of night to conceal the movement of his forces. In France and later in Africa, Rommel made use of the Luftwaffe as a forward, mobile artillery to support the advance and help overcome difficult obstacles. He viewed the essential aspect of successful use of armour was the ability to concentrate all available strength at one point and then hit that point with everything at hand to force a breakthrough. Maintaining momentum was critical. He was willing to trade the tenuous logistical support of such moves for the advantage in creating havoc and confusion in the enemy. A former Afrika Korps soldier recalled: "When the kampfgruppe leader would say 'Jawohl Herr Feldmarschall. According to my estimates the proposed drive behind the lines to encircle the enemy would require a drive of . Our fuel supply is barely enough for .' Rommel would reply in his Schwaebisch dialect, 'Fahren Sie, fahren Sie, dann brauchen Sie keinen Treibstoff' (Drive, drive, then you do not need fuel), which was understood to mean 'Get there quickly, take the enemy by surprise, then use the fuel available from the enemy's supply.'"
Leadership.
The 7th Panzer's drive through the Belgian, French and British lines in 1940 succeeded to a remarkable degree from Rommel's driving presence with his forces. The boldness of his attacks often led larger enemy formations to surrender, as they were overwhelmed by the pace of the action and became unsure of themselves. This was even more evident in North Africa. A central aspect of his thinking on command was the high value he placed on a commander being physically present at the point of contact. Rommel's experiences in the First World War of successes gained by rapid forward movement, flanking opponents and attacking their rear areas, and catching the defenders by surprise were amplified with the mobility afforded to armoured formations. To augment his force at the point of attack he made use of the Luftwaffe as a forward mobile artillery. A major aspect of his success was his grasp of the psychological shock such attacks had upon the morale and fighting spirit of the enemy forces. When the British mounted a commando raid deep behind German lines in an effort to kill Rommel and his staff on the eve of their Crusader offensive, Rommel was indignant, not that the British had singled him out to be killed, but that the British could believe his headquarters would be found 250 miles behind his front. In terms of making tactical decisions quickly, he believed the commander needed to be at the crucial place at the crucial time. If Rommel "did" find it necessary to keep his headquarters well behind the lines, he would often personally pilot a reconnaissance aircraft over the battle lines to get a view of the situation. Although Rommel did not have a pilot's license, he was a competent pilot, and none of the Luftwaffe officers had the nerve to stop him.
Rommel led by example. In 1933 when he became commander of a Hanoverian Jaeger battalion, which was composed of soldiers with skiing expertise, its officers gave him the mandatory test on the snow slopes. No lift was present, and the men had to climb to ski down the hillside. They trudged to the top and descended, and honour was satisfied, but the 41-year-old commander led his officers up and down the slope twice more before he let them fall out. He felt a commander should be physically more robust than the troops he led, and should always show them an example. He expected his subordinate commanders to do the same. They had to live hard. He felt it the obligation of a commander to be willing to suffer whatever hardships the soldier in the line was facing, and he understood the effect of this on the morale of his men.
Rommel received both praise and criticism for his tactics during the French campaign. Many, such as General Georg Stumme, who had previously commanded 7th Panzer Division, were impressed with the speed and success of Rommel's drive. Others, however, were more reserved, some out of envy, others over concerns about risks Rommel was willing to accept, and others in the German High Command out of their limited appreciation and acceptance of maneuver warfare. With Rommel's campaign in North Africa to view in retrospect, Hoth's reservations can be seen as unfounded. Commented Georg Ralf: ""Wegen seiner steilen Karriere, seiner Popularität und vor allem aufgrund der Gunst, die er bei Hitler genoss, hatte er viele Feinde in der Wehrmacht"," which can be translated: "Because of his stellar career, his popularity, and especially because of the favor he enjoyed with Hitler, he had many enemies in the armed forces."
The respect afforded Rommel by his soldiers was the result of their observation of him. Said staff officer Friedrich von Mellenthin: "The Afrika Korps followed Rommel wherever he led, however hard he drove them. ... the men knew that Rommel was the last man to spare Rommel." Hard on his officers, he demanded they take proper care of their men and materiel. Once he saw things were properly attended to he could be easy and comfortable, but if unhappy with the way an officer was applying himself he could be very severe, being quick to fire officers who did not maintain standards or dithered over his commands. Mellenthin said: "While very popular with young soldiers and N.C.O.s, with whom he cracked many a joke, he could be most outspoken and offensive to commanders of troops if he did not approve of their measures." According to Mellenthin's memoirs, James Mason's portrayed Rommel as "altogether too polite" in the film "The Desert Fox".
Rommel spoke German with a pronounced southern German or Swabian accent. He was not a part of the Prussian aristocracy that dominated the German high command, and as such was looked upon somewhat suspiciously by the Wehrmacht's traditional power structure. His successes caused a certain amount of resentment among headquarters staff officers, who criticized him for failing to keep them in contact and properly informed of his intentions. For Rommel this was not always an oversight, but was sometimes preferred.
Personality.
In battle, Rommel was often directing fire or leading an assault in the hottest point of decision. Wounded multiple times in both world wars, his notoriety was partly the result of his having the luck to survive long enough to become prominent. In addition, Rommel was also the possessor of a great deal of moral courage. German historian Hans-Adolf Jacobson commented: "Rommel was one of the few generals who had the strength to refuse to carry out one of Hitler's orders." He could be difficult on his subordinate commanders and superiors. He expected a great deal of himself and much the same for them. He had little patience for junior officers who did not do their jobs properly. He was not open to objections to his plans, and he did not tolerate incompetence.
Friedrich von Mellenthin, who was a key aide on Rommel's staff during the Africa campaign, wrote that Rommel was willing to take chances and preferred to be at the forefront of the battle. But at times in North Africa his absence from a position of communication made command of the battles of the Afrika Korps difficult. According to Mellenthin, Rommel's counterattack during Operation "Crusader" is one such instance. It should be noted though, that throughout the desert war Rommel was acting from a position of relative weakness. To succeed he had to accept risks that commanders like Montgomery were never forced to take. General Fritz Bayerlein, Rommel's chief of staff through much of the campaign, noted that risks taken were made only after carefully weighing the potential dangers and rewards.
According to Mellenthin's post-war memoirs, long absences from contact with headquarters meant that at times subordinate commanders had to make decisions without first consulting Rommel. Even when Rommel was present at headquarters, his impatient personality made it difficult for his subordinates—and sometimes his superiors—to work with him.
Relations with the Italians.
Rommel's contemptuous opinion of the Italian military stemmed initially from his experiences fighting against them in the mountains of Northern Italy in the First World War. His initial disdain was tempered when he came to realise their lack of success was principally due to poor leadership and equipment, remarking succinctly in his typical fashion: "Good soldiers, bad officers." When these difficulties were overcome, he found them equal to German soldiers. Rommel's relationship with the Italian High Command in North Africa was generally poor. Rommel was sent to Africa to shore up a crumbling situation created under the direction of the Italian command, and though he was nominally subordinate to the Italians for much of the campaign, he was under no illusions as to why he was there. Further, he enjoyed direct access with the highest German political authority, which allowed him a certain degree of autonomy from his Italian counterparts; since he was directing their troops in battle as well as his own, this was bound to cause hostility among Italian commanders. Conversely, as the Italian command had control over the supplies of the forces in Africa, they resupplied Italian units preferentially, which was a source of resentment for Rommel and his staff. Rommel's direct and abrasive manner did nothing to smooth these issues.
While certainly much less proficient than Rommel in their leadership, aggressiveness, tactical outlook and mobile warfare skills, Italian commanders were competent in logistics, strategy and artillery doctrine: their troops were ill-equipped but well-trained. As such, the Italian commanders were repeatedly at odds with Rommel over concerns with issues of supply. Field Marshal Kesselring was assigned Supreme Commander Mediterranean, at least in part to alleviate command problems between Rommel and the Italians. This effort does not seem to have succeeded, Kesselring claiming Rommel ignored him as easily as he ignored the Italians.
Very different, however, was the perception of Rommel by Italian common soldiers and NCOs, who, like the German field troops, had the deepest trust and respect for him.
Views on the conduct of war.
Rommel understood and accepted that with war would come casualties, but sought to minimise the number of people killed and wounded during battles he was involved in. Writing in 1950, Desmond Young stated that the Afrika Korps was never accused of any war crimes. However, in 2009 Richard J. Evans stated that German soldiers in Tunisia had raped Jewish women, and the success of Rommel's forces in capturing or securing Allied, Italian and Vichy French territory in North Africa led to many Jews in these areas being killed by other German institutions as part of the Holocaust. Similarly, in 2007 several German historians argued that while Rommel did not have strong racial views, if he had succeeded in his goal of invading the Middle East during 1942 large numbers of Jews in Palestine would have been murdered by an SS unit which had been deployed to North Africa in July 1942 to operate behind the lines of the Afrika Korps. During the desert campaign, interactions between German and British troops encountering each other between battles were sometimes openly friendly. The same was not true in the Normandy Campaign, however, where both Allied and German troops murdered prisoners of war on occasion during June and July 1944. Rommel defied Hitler's order to execute captured commandos. After the capture of commandos Lieutenant Roy Wooldridge and Lieutenant George Lane following Operation Fortitude, he placed them in a POW camp.
During Rommel's time in France, Hitler ordered him to deport the country's Jewish population; Rommel disobeyed. Several times he wrote letters protesting against the treatment of the Jews. He also refused to comply with Hitler's order to execute Jewish POWs. At his 17 June 1944 meeting with Hitler at Margival, he protested against the atrocity committed by the 2nd SS Panzer division "Das Reich", which in retribution had massacred the citizens of the French town of Oradour-sur-Glane. Rommel asked to be allowed to punish the division. While he implemented the construction of the many obstacles to strengthen the Atlantic Wall, Rommel directed that French workers were to be paid for their labour, and were not to be used as slave labourers. However, French civilians and Italian prisoners of war held by the Germans were forced to work on building some of the defences Rommel ordered constructed.
Popular perception.
Rommel had been extraordinarily well known in his lifetime, not only by the German people, but also by his adversaries. His tactical prowess and consistent decency in the treatment of allied prisoners earned him the respect of many opponents, including Claude Auchinleck, Winston Churchill, George S. Patton, and Bernard Montgomery. Rommel reciprocated their respect. He at one time said Montgomery "never made a serious strategic mistake" and credited Patton with "the most astounding achievement in mobile warfare". Rommel's admiration of the British was particularly notable; while having tea with George Lane, a captured British commando, he expressed regret that Germany and Britain had not been allies during both world wars.
Rommel was among the few Axis commanders (the others being Isoroku Yamamoto and Reinhard Heydrich) who were directly targeted for assassination by Allied planners. At least two attempts were made against Rommel's life, the first being Operation Flipper, which attempted to kill Rommel in North Africa on the eve of Operation Crusader in 1941, and the second being Operation Gaff, undertaken shortly after the invasion of Normandy in 1944.
When Rommel's involvement in the plot to kill Hitler became known after the war, his stature was enhanced in the eyes of his former adversaries. Rommel was often cited in Western sources as a loyal German willing to stand up to Hitler. The release of the film "" (1951) increased his fame and furthered his standing as the most widely-known and well-regarded leader in the German Army. In 1970, a "Lütjens"-class destroyer was named the "Rommel" in his honour.
In the course of the war, during parliamentary debate following the fall of Tobruk, Prime Minister Winston Churchill spoke of Rommel as a "daring and skillful opponent ... a great general". Writing about him years later, Churchill offered the following:
Family life.
While at Cadet School in 1911, Rommel met and became engaged to 17-year-old Lucia (Lucie) Maria Mollin (1894–1971). While stationed in Weingarten in 1913, Rommel developed a relationship with Walburga Stemmer, which produced a daughter, Gertrude, born 8 December 1913. Because of elitism in the officer corps, Stemmer's working-class background made her unsuitable as an officer's wife, and Rommel felt honour-bound to uphold his previous commitment to Mollin. With Lucie's cooperation, he accepted financial responsibility for the child.
Rommel and Mollin were married in November 1916 in Danzig. After the end of the First World War, the couple settled initially in Stuttgart, and Stemmer and her child lived with them. Gertrude was referred to as Rommel's niece, a fiction that went unquestioned due to the enormous number of women widowed during the war. Walburga died suddenly in October 1928, and Gertrude remained a member of the household until Rommel's death in 1944.
Rommel's marriage was a happy one, and he wrote his wife at least one letter every day while he was in the field. Their son Manfred, born 24 December 1928, served as "Oberbürgermeister" of Stuttgart from 1974 to 1996. He died on 7 November 2013, survived by a daughter, Catherine.

</doc>
<doc id="9518" url="https://en.wikipedia.org/wiki?curid=9518" title="Edmund Husserl">
Edmund Husserl

Edmund Gustav Albrecht Husserl (; ; 8 April 1859 – 27 April 1938) was a German philosopher who established the school of phenomenology. In his early work, he elaborated critiques of historicism and of psychologism in logic based on analyses of intentionality. In his mature work, he sought to develop a systematic foundational science based on the so-called phenomenological reduction. Arguing that transcendental consciousness sets the limits of all possible knowledge, Husserl re-defined phenomenology as a transcendental-idealist philosophy. Husserl's thought profoundly influenced the landscape of twentieth-century philosophy and he remains a notable figure in contemporary philosophy and beyond.
Husserl studied mathematics under Karl Weierstrass and Leo Königsberger, and philosophy under Franz Brentano and Carl Stumpf. He taught philosophy as a "Privatdozent" at Halle from 1887, then as professor, first at Göttingen from 1901, then at Freiburg from 1916 until he retired in 1928, after which he remained highly productive. Following an illness, he died at Freiburg in 1938.
Life and career.
Youth and education.
Husserl was born in 1859 in Prostějov (), a town in the Margraviate of Moravia, which was then in the Austrian Empire, and now belongs to the Czech Republic. He was born into a Jewish family, the second of four children (boy, boy, girl, boy). His father was a milliner. His childhood was spent in Prostějov, where he attended the elementary school. Then Husserl traveled to Vienna to study at the "Realgymnasium" there, followed next by the "Staatsgymnasium" in Olomouc (Ger.: Olmütz).
At the University of Leipzig from 1876 to 1878, Husserl studied mathematics, physics, and astronomy. At Leipzig he was inspired by philosophy lectures given by Wilhelm Wundt, one of the founders of modern psychology. Then he moved to the Frederick William University of Berlin in 1878 where he continued his study of mathematics under Leopold Kronecker and the renowned Karl Weierstrass. In Berlin he found a mentor in Thomas Masaryk, then a former philosophy student of Franz Brentano and later the first president of Czechoslovakia. There Husserl also attended Friedrich Paulsen's philosophy lectures. In 1881 he left for the University of Vienna to complete his mathematics studies under the supervision of Leo Königsberger (a former student of Weierstrass). At Vienna in 1883 he obtained his Ph.D. with the work "Beiträge zur Variationsrechnung" ("Contributions to the Calculus of Variations").
Evidently as a result of his becoming familiar with the New Testament during his twenties, he asked to be baptized into the Lutheran Church in 1886. Husserl's father Adolf had died in 1884. Herbert Spiegelberg writes, "While outward religious practice never entered his life any more than it did that of most academic scholars of the time, his mind remained open for the religious phenomenon as for any other genuine experience." At times Husserl saw his goal as one of moral "renewal". Although a steadfast proponent of a radical and rational "autonomy" in all things, Husserl could also speak "about his vocation and even about his mission under God's will to find new ways for philosophy and science," observes Spiegelberg.
Following his doctorate in mathematics, he returned to Berlin to work as the assistant to Karl Weierstrass. Yet already Husserl had felt the desire to pursue philosophy. Then professor Weierstrass became very ill. Husserl became free to return to Vienna where, after serving a short military duty, he devoted his attention to philosophy. In 1884 at the University of Vienna he attended the lectures of Franz Brentano on philosophy and philosophical psychology. Brentano introduced him to the writings of Bernard Bolzano, Hermann Lotze, J. Stuart Mill, and David Hume. Husserl was so impressed by Brentano that he decided to dedicate his life to philosophy; indeed, Franz Brentano is often credited as being his most important influence, e.g., with regard to intentionality. Following academic advice, two years later in 1886 Husserl followed Carl Stumpf, a former student of Brentano, to the University of Halle, seeking to obtain his Habilitation which would qualify him to teach at the university level. There, under Stumpf's supervision, he wrote "Über den Begriff der Zahl" ("On the Concept of Number") in 1887, which would serve later as the basis for his first important work, "Philosophie der Arithmetik" (1891).
In 1887 he married Malvine Steinschneider, a union that would last over fifty years. In 1892 their daughter Elizabeth was born, in 1893 their son Gerhard, and in 1894 their son Wolfgang. Elizabeth would marry in 1922, and Gerhard in 1923; Wolfgang, however, became a casualty of the First World War. Gerhard would become a philosopher of law, contributing to the subject of comparative law, teaching in the USA and after the war in Austria.
Professor of philosophy.
Following his marriage Husserl began his long teaching career in philosophy. He started where he was in 1887 as a "Privatdozent" at the University of Halle. In 1891 he published his "Philosophie der Arithmetik. Psychologische und logische Untersuchungen" which, drawing on his prior studies in mathematics and philosophy, proposed a psychological context as the basis of mathematics. It drew the adverse notice of Gottlob Frege, who criticized its psychologism.
In 1901 Husserl with his family moved to the Georg-August University of Göttingen where he taught as "extraordinarius professor". Just prior to this a major work of his, "Logische Untersuchungen" (Halle 1900–1901), was published. Volume One contains seasoned reflections on "pure logic" in which he carefully refutes "psychologism". This work was well received and became the subject of a seminar given by Wilhelm Dilthey; Husserl in 1905 traveled to Berlin to visit Dilthey. Two years later in Italy he paid a visit to Franz Brentano his inspiring old teacher and to Constantin Carathéodory the mathematician. Kant and Descartes were also now influencing his thought. In 1910 he became joint editor of the journal "Logos". During this period Husserl had delivered lectures on "internal time consciousness", which several decades later his former student Heidegger edited for publication.
In 1912 at Freiburg the journal "Jahrbuch für Philosophie und Phänomenologische Forschung" ("Yearbook for Philosophy and Phenomenological Research") was founded by Husserl and his school, and which published articles of their phenomenological movement from 1913 to 1930. His important work "Ideen" was published in its first issue. Before beginning "Ideen" Husserl's thought had reached the stage where "each subject is 'presented' to itself, and to each all others are 'presentiated' ("Vergegenwärtigung"), not as parts of nature but as pure consciousness." "Ideen" advanced his transition to a "transcendental interpretation" of phenomenology, a view later criticized by, among others, Jean-Paul Sartre. In "Ideen" Paul Ricœur sees the development of Husserl's thought as leading "from the psychological cogito to the transcendental cogito." As phenomenology further evolves, it leads (when viewed from another vantage point in Husserl's 'labyrinth') to "transcendental subjectivity". Also in "Ideen" Husserl explicitly elaborates the eidetic and phenomenological reductions. In 1913 Karl Jaspers visited Husserl at Göttingen.
In October 1914 both his sons were sent to fight on the Western Front of World War I and the following year one of them, Wolfgang Husserl, was badly injured. On 8 March 1916, on the battlefield of Verdun, Wolfgang was killed in action. The next year his other son Gerhard Husserl was wounded in the war but survived. His own mother Julia died. In November 1917 one of his outstanding students and later a noted philosophy professor in his own right, Adolf Reinach, was killed in the war while serving in Flanders.
Husserl had transferred in 1916 to the Albert Ludwigs University of Freiburg (Freiburg im Breisgau) where he continued bringing his work in philosophy to fruition, now as a full professor. Edith Stein served as his personal assistant during his first few years in Freiburg, followed later by Martin Heidegger from 1920 to 1923. The mathematician Hermann Weyl began corresponding with him in 1918. Husserl gave four lectures on Phenomenological method at University College, London in 1922. The University of Berlin in 1923 called on him to relocate there, but he declined the offer. In 1926 Heidegger dedicated his book "Sein und Zeit" ("Being and Time") to him "in grateful respect and friendship." Husserl remained in his professorship at Freiburg until he requested retirement, teaching his last class on 25 July 1928. A "Festschrift" to celebrate his seventieth birthday was presented to him on 8 April 1929.
For Husserl 1933 was an ugly year, when the racial laws of the new Nazi regime were enacted. On 6 April Husserl was suspended from the University of Freiburg by the Badische Ministry of Culture; the following week he was disallowed any university activities. Yet his colleague Heidegger was elected Rector of the university on 21–22 April, and joined the Nazi Party. By contrast, in July Husserl resigned from the "Deutsche Academie".
Despite retirement, Husserl gave several notable lectures. The first, at Paris in 1929, led to "Méditations cartésiennes" (Paris 1931). Husserl here reviews the "epoché" and transcendental reduction, presented earlier in his pivotal "Ideen" (1913), in terms of a further reduction of experience to what he calls a 'sphere of ownness.' From within this sphere, which Husserl enacts in order to show the impossibility of solipsism, the transcendental ego finds itself always already paired with the lived body of another ego, another monad. This 'a priori' interconnection of bodies, given in perception, is what founds the interconnection of consciousnesses known as transcendental intersubjectivity, which Husserl would go on to describe at length in volumes of unpublished writings. There has been a debate over whether or not Husserl's description of ownness and its movement into intersubjectivity is sufficient to reject the charge of solipsism, to which Descartes, for example, was subject. One argument against Husserl's description works this way: instead of infinity and the Deity being the ego's gateway to the Other, as in Descartes, Husserl's ego in the "Cartesian Meditations" itself becomes transcendent. It remains, however, alone (unconnected). Only the ego's grasp "by analogy" of the Other (e.g., by conjectural reciprocity) allows the possibility for an 'objective' intersubjectivity, and hence for community.
Later Husserl lectured at Prague in 1935 and Vienna in 1936, which resulted in a very differently styled work that, while innovative, is no less problematic: "Die Krisis" (Belgrade 1936). Husserl describes here the cultural crisis gripping Europe, then approaches a philosophy of history, discussing Galileo, Descartes, several British philosophers, and Kant. The apolitical Husserl before had specifically avoided such historical discussions, pointedly preferring to go directly to an investigation of consciousness. Merleau-Ponty and others question whether Husserl here does not undercut his own position, in that Husserl had attacked in principle historicism, while specifically designing his phenomenology to be rigorous enough to transcend the limits of history. On the contrary, Husserl may be indicating here that historical traditions are merely features given to the pure ego's intuition, like any other. A longer section follows on the "life world" 'Lebenswelt', one not observed by the objective logic of science, but a world seen in our subjective experience. Yet a problem arises similar to that dealing with 'history' above, a chicken-and-egg problem. Does the life world contextualize and thus compromise the gaze of the pure ego, or does the phenomenological method nonetheless raise the ego up transcendent? These last writings presented the fruits of his professional life. Since his university retirement Husserl had "worked at a tremendous pace, producing several major works."
After suffering a fall the autumn of 1937, the philosopher became ill with pleurisy. Edmund Husserl died at Freiburg on 27 April 1938, having just turned 79. His wife Malvine survived him. Eugen Fink, his research assistant, delivered his eulogy. Gerhard Ritter was the only Freiburg faculty member to attend the funeral, as an anti-Nazi protest.
Heidegger and the Nazi era.
Husserl was incorrectly rumoured to have been denied the use of the library at Freiburg as a result of the anti-Jewish legislation of April 1933. However, among other disabilities Husserl was unable to publish his works in Nazi Germany; cf., above footnote to "Die Krisis" (1936). It was also rumoured that his former pupil and Nazi Party member, Martin Heidegger, informed Husserl that he was discharged, but it was actually the former rector.
Apparently Husserl and Heidegger had moved apart during the 1920s, which became clearer after 1928 when Husserl retired and Heidegger succeeded to his University chair. In the summer of 1929 Husserl had studied carefully selected writings of Heidegger, coming to the conclusion that on several of their key positions they differed, e.g., Heidegger substituted "Dasein" quot;Being-there&quot for the pure ego, thus transforming phenomenology into an anthropology, a type of psychologism strongly disfavored by Husserl. Such observations of Heidegger, along with a critique of Max Scheler, were put into a lecture Husserl gave to various "Kant Societies" in Frankfurt, Berlin, and Halle during 1931 entitled "Phänomenologie und Anthropologie".
In the war-time 1941 edition of Heidegger's primary work, "Being and Time" (first published in 1927), the original dedication to Husserl was removed. This was not due to a negation of the relationship between the two philosophers, however, but rather was the result of a suggested censorship by Heidegger's publisher who feared that the book might otherwise be banned by the Nazi regime. The dedication can still be found in a footnote on page 38, thanking Husserl for his guidance and generosity. Husserl, of course, had died several years earlier. In post-war editions of "Sein und Zeit" the dedication to Husserl is restored. The complex, troubled, and sundered philosophical relationship between Husserl and Heidegger has been widely discussed.
On 4 May 1933, Professor Edmund Husserl addressed the recent regime change in Germany and its consequences:
"The future alone will judge which was the true Germany in 1933, and who were the true Germans—those who subscribe to the more or less materialistic-mythical racial prejudices of the day, or those Germans pure in heart and mind, heirs to the great Germans of the past whose tradition they revere and perpetuate."
After his death, Husserl's manuscripts, amounting to approximately 40,000 pages of "Gabelsberger" stenography and his complete research library, were in 1939 smuggled to Belgium by the Franciscan priest Herman Van Breda. There they were deposited at Leuven to form the "Husserl-Archives" of the Higher Institute of Philosophy. Much of the material in his research manuscripts has since been published in the Husserliana critical edition series.
Development of his thought.
Several early themes.
In his first works Husserl tries to combine mathematics, psychology and philosophy with a main goal to provide a sound foundation for mathematics. He analyzes the psychological process needed to obtain the concept of number and then tries to build up a systematical theory on this analysis. To achieve this he uses several methods and concepts taken from his teachers. From Weierstrass he derives the idea that we generate the concept of number by counting a certain collection of objects.
From Brentano and Stumpf he takes over the distinction between "proper" and "improper" presenting. In an example Husserl explains this in the following way: if you are standing in front of a house, you have a proper, direct presentation of that house, but if you are looking for it and ask for directions, then these directions (e.g. the house on the corner of this and that street) are an indirect, improper presentation. In other words, you can have a proper presentation of an object if it is actually present, and an improper (or symbolic as he also calls it) if you only can indicate that object through signs, symbols, etc. Husserl's "Logical Investigations" (1900–1901) is considered the starting point for the formal theory of wholes and their parts known as mereology.
Another important element that Husserl took over from Brentano is intentionality, the notion that the main characteristic of consciousness is that it is always intentional. While often simplistically summarised as "aboutness" or the relationship between mental acts and the external world, Brentano defined it as the main characteristic of "mental phenomena", by which they could be distinguished from "physical phenomena". Every mental phenomenon, every psychological act, has a content, is directed at an object (the "intentional object"). Every belief, desire, etc. has an object that it is about: the believed, the wanted. Brentano used the expression "intentional inexistence" to indicate the status of the objects of thought in the mind. The property of being intentional, of having an intentional object, was the key feature to distinguish mental phenomena and physical phenomena, because physical phenomena lack intentionality altogether.
The elaboration of phenomenology.
Some years after the 1900–1901 publication of his main work, the "Logische Untersuchungen" ("Logical Investigations"), Husserl made some key conceptual elaborations which led him to assert that in order to study the structure of consciousness, one would have to distinguish between the act of consciousness and the phenomena at which it is directed (the objects as intended). Knowledge of essences would only be possible by "bracketing" all assumptions about the existence of an external world. This procedure he called "epoché". These new concepts prompted the publication of the "Ideen" ("Ideas") in 1913, in which they were at first incorporated, and a plan for a second edition of the "Logische Untersuchungen".
From the "Ideen" onward, Husserl concentrated on the ideal, essential structures of consciousness. The metaphysical problem of establishing the reality of what we perceive, as distinct from the perceiving subject, was of little interest to Husserl in spite of his being a transcendental idealist. Husserl proposed that the world of objects and ways in which we direct ourselves toward and perceive those objects is normally conceived of in what he called the "natural standpoint", which is characterized by a belief that objects exist distinct from the perceiving subject and exhibit properties that we see as emanating from them. Husserl proposed a radical new phenomenological way of looking at objects by examining how we, in our many ways of being intentionally directed toward them, actually "constitute" them (to be distinguished from materially creating objects or objects merely being figments of the imagination); in the Phenomenological standpoint, the object ceases to be something simply "external" and ceases to be seen as providing indicators about what it is, and becomes a grouping of perceptual and functional aspects that imply one another under the idea of a particular object or "type". The notion of objects as real is not expelled by phenomenology, but "bracketed" as a way in which we regard objectsinstead of a feature that inheres in an object's essence founded in the relation between the object and the perceiver. In order to better understand the world of appearances and objects, phenomenology attempts to identify the invariant features of how objects are perceived and pushes attributions of reality into their role as an attribution about the things we perceive (or an assumption underlying how we perceive objects). The major dividing line in Husserl's thought is the turn to transcendental idealism.
In a later period, Husserl began to wrestle with the complicated issues of intersubjectivity, specifically, how communication about an object can be assumed to refer to the same ideal entity ("Cartesian Meditations", Meditation V). Husserl tries new methods of bringing his readers to understand the importance of phenomenology to scientific inquiry (and specifically to psychology) and what it means to "bracket" the natural attitude. "The Crisis of the European Sciences" is Husserl's unfinished work that deals most directly with these issues. In it, Husserl for the first time attempts a historical overview of the development of Western philosophy and science, emphasizing the challenges presented by their increasingly (one-sidedly) empirical and naturalistic orientation. Husserl declares that mental and spiritual reality possess their own reality independent of any physical basis, and that a science of the mind (") must be established on as scientific a foundation as the natural sciences have managed:
Thought.
Husserl's thought is revolutionary in several ways, most notably in the distinction between 'natural' and 'phenomenological' modes of understanding. In the former, sense-perception in correspondence with the material realm constitutes the known reality, and understanding is premised on the accuracy of the perception and the objective knowability of what is called the 'real world'. Phenomenological understanding strives to be rigorously 'presuppositionless' by means of what Husserl calls 'phenomenological reduction'. This reduction is not conditioned but rather transcendental: in Husserl's terms, pure consciousness of absolute Being. In Husserl's work, consciousness of any given thing calls for discerning its meaning as an 'intentional object'. Such an object does not simply strike the senses, to be interpreted or misinterpreted by mental reason; it has already been selected and grasped, grasping being an etymological connotation, of "percipere", the root of 'perceive.
Meaning and object.
From "Logical Investigations" (1900/1901) to "Experience and Judgment" (published in 1939), Husserl expressed clearly the difference between meaning and object. He identified several different kinds of names. For example, there are names that have the role of properties that uniquely identify an object. Each of these names expresses a meaning and designates the same object. Examples of this are "the victor in Jena" and "the loser in Waterloo", or "the equilateral triangle" and "the equiangular triangle"; in both cases, both names express different meanings, but designate the same object. There are names which have no meaning, but have the role of designating an object: "Aristotle", "Socrates", and so on. Finally, there are names which designate a variety of objects. These are called "universal names"; their meaning is a "concept" and refers to a series of objects (the extension of the concept). The way we know sensible objects is called "sensible intuition".
Husserl also identifies a series of "formal words" which are necessary to form sentences and have no sensible correlates. Examples of formal words are "a", "the", "more than", "over", "under", "two", "group", and so on. Every sentence must contain formal words to designate what Husserl calls "formal categories". There are two kinds of categories: meaning categories and formal-ontological categories. Meaning categories relate judgments; they include forms of conjunction, disjunction, forms of plural, among others. Formal-ontological categories relate objects and include notions such as set, cardinal number, ordinal number, part and whole, relation, and so on. The way we know these categories is through a faculty of understanding called "categorial intuition".
Through sensible intuition our consciousness constitutes what Husserl calls a "situation of affairs" ("Sachlage"). It is a passive constitution where objects themselves are presented to us. To this situation of affairs, through categorial intuition, we are able to constitute a "state of affairs" ("Sachverhalt"). One situation of affairs through objective acts of consciousness (acts of constituting categorially) can serve as the basis for constituting multiple states of affairs. For example, suppose "a" and "b" are two sensible objects in a certain situation of affairs. We can use it as basis to say, ""a"<"b" and "b">"a"", two judgments which designate the same state of affairs. For Husserl a sentence has a proposition or judgment as its meaning, and refers to a state of affairs which has a situation of affairs as a reference base.
Philosophy of logic and mathematics.
Husserl believed that "truth-in-itself" has as ontological correlate "being-in-itself", just as meaning categories have formal-ontological categories as correlates. Logic is a formal theory of judgment, that studies the formal "a priori" relations among judgments using meaning categories. Mathematics, on the other hand, is formal ontology; it studies all the possible forms of being (of objects). Hence for both logic and mathematics, the different formal categories are the objects of study, not the sensible objects themselves. The problem with the psychological approach to mathematics and logic is that it fails to account for the fact that this approach is about formal categories, and not simply about abstractions from sensibility alone. The reason why we do not deal with sensible objects in mathematics is because of another faculty of understanding called "categorial abstraction." Through this faculty we are able to get rid of sensible components of judgments, and just focus on formal categories themselves.
Thanks to "eidetic intuition" (or "essential intuition"), we are able to grasp the possibility, impossibility, necessity and contingency among concepts and among formal categories. Categorial intuition, along with categorial abstraction and eidetic intuition, are the basis for logical and mathematical knowledge.
Husserl criticized the logicians of his day for not focusing on the relation between subjective processes that give us objective knowledge of pure logic. All subjective activities of consciousness need an ideal correlate, and objective logic (constituted noematically) as it is constituted by consciousness needs a noetic correlate (the subjective activities of consciousness).
Husserl stated that logic has three strata, each further away from consciousness and psychology than those that precede it.
The ontological correlate to the third stratum is the "theory of manifolds". In formal ontology, it is a free investigation where a mathematician can assign several meanings to several symbols, and all their possible valid deductions in a general and indeterminate manner. It is, properly speaking, the most universal mathematics of all. Through the posit of certain indeterminate objects (formal-ontological categories) as well as any combination of mathematical axioms, mathematicians can explore the apodeictic connections between them, as long as consistency is preserved.
According to Husserl, this view of logic and mathematics accounted for the objectivity of a series of mathematical developments of his time, such as "n"-dimensional manifolds (both Euclidean and non-Euclidean), Hermann Grassmann's theory of extensions, William Rowan Hamilton's Hamiltonians, Sophus Lie's theory of transformation groups, and Cantor's set theory.
Jacob Klein was one student of Husserl who pursued this line of inquiry, seeking to "desedimentize" mathematics and the mathematical sciences.
Husserl and psychologism.
Philosophy of arithmetic and Frege.
After obtaining his PhD in mathematics, Husserl began analyzing the foundations of mathematics from a psychological point of view. In his habilitation thesis, "On the Concept of Number" (1886) and in his "Philosophy of Arithmetic" (1891), Husserl sought, by employing Brentano's descriptive psychology, to define the natural numbers in a way that advanced the methods and techniques of Karl Weierstrass, Richard Dedekind, Georg Cantor, Gottlob Frege, and other contemporary mathematicians. Later, in the first volume of his "Logical Investigations", the "Prolegomena of Pure Logic", Husserl, while attacking the psychologistic point of view in logic and mathematics, also appears to reject much of his early work, although the forms of psychologism analysed and refuted in the "Prolegomena" did not apply directly to his "Philosophy of Arithmetic". Some scholars question whether Frege's negative review of the "Philosophy of Arithmetic" helped turn Husserl towards Platonism, but he had already discovered the work of Bernhard Bolzano independently around 1890/91 and explicitly mentioned Bernard Bolzano, Gottfried Leibniz and Hermann Lotze as inspirations for his newer position.
Husserl's review of Ernst Schröder, published before Frege's landmark 1892 article, clearly distinguishes sense from reference; thus Husserl's notions of noema and object also arose independently. Likewise, in his criticism of Frege in the "Philosophy of Arithmetic", Husserl remarks on the distinction between the content and the extension of a concept. Moreover, the distinction between the subjective mental act, namely the content of a concept, and the (external) object, was developed independently by Brentano and his school, and may have surfaced as early as Brentano's 1870's lectures on logic.
Scholars such as J. N. Mohanty, Claire Ortiz Hill, and Guillermo E. Rosado Haddock, among others, have argued that Husserl's so-called change from psychologism to Platonism came about independently of Frege's review.
For example, the review falsely accuses Husserl of subjectivizing everything, so that no objectivity is possible, and falsely attributes to him a notion of abstraction whereby objects disappear until we are left with numbers as mere ghosts. Contrary to what Frege states, in Husserl's "Philosophy of Arithmetic" we already find two different kinds of representations: subjective and objective. Moreover, objectivity is clearly defined in that work. Frege's attack seems to be directed at certain foundational doctrines then current in Weierstrass's Berlin School, of which Husserl and Cantor cannot be said to be orthodox representatives.
Furthermore, various sources indicate that Husserl changed his mind about psychologism as early as 1890, a year before he published the "Philosophy of Arithmetic". Husserl stated that by the time he published that book, he had already changed his mind—that he had doubts about psychologism from the very outset. He attributed this change of mind to his reading of Leibniz, Bolzano, Lotze, and David Hume. Husserl makes no mention of Frege as a decisive factor in this change. In his "Logical Investigations", Husserl mentions Frege only twice, once in a footnote to point out that he had retracted three pages of his criticism of Frege's "The Foundations of Arithmetic", and again to question Frege's use of the word "Bedeutung" to designate "reference" rather than "meaning" (sense).
In a letter dated 24 May 1891, Frege thanked Husserl for sending him a copy of the "Philosophy of Arithmetic" and Husserl's review of Ernst Schröder's "Vorlesungen über die Algebra der Logik". In the same letter, Frege used the review of Schröder's book to analyze Husserl's notion of the sense of reference of concept words. Hence Frege recognized, as early as 1891, that Husserl distinguished between sense and reference. Consequently, Frege and Husserl independently elaborated a theory of sense and reference before 1891.
Commentators argue that Husserl's notion of noema has nothing to do with Frege's notion of sense, because "noemata" are necessarily fused with noeses which are the conscious activities of consciousness. "Noemata" have three different levels:
Consequently, in intentional activities, even non-existent objects can be constituted, and form part of the whole noema. Frege, however, did not conceive of objects as forming parts of senses: If a proper name denotes a non-existent object, it does not have a reference, hence concepts with no objects have no truth value in arguments. Moreover, Husserl did not maintain that predicates of sentences designate concepts. According to Frege the reference of a sentence is a truth value; for Husserl it is a "state of affairs." Frege's notion of "sense" is unrelated to Husserl's noema, while the latter's notions of "meaning" and "object" differ from those of Frege.
In detail, Husserl's conception of logic and mathematics differs from that of Frege, who held that arithmetic could be derived from logic. For Husserl this is not the case: mathematics (with the exception of geometry) is the ontological correlate of logic, and while both fields are related, neither one is strictly reducible to the other.
Husserl's criticism of psychologism.
Reacting against authors such as J. S. Mill, Christoph von Sigwart and his own former teacher Brentano, Husserl criticised their psychologism in mathematics and logic, i.e. their conception of these abstract and "a priori" sciences as having an essentially empirical foundation and a prescriptive or descriptive nature. According to psychologism, logic would not be an autonomous discipline, but a branch of psychology, either proposing a prescriptive and practical "art" of correct judgement (as Brentano and some of his more orthodox students did) or a description of the factual processes of human thought. Husserl pointed out that the failure of anti-psychologists to defeat psychologism was a result of being unable to distinguish between the foundational, theoretical side of logic, and the applied, practical side. Pure logic does not deal at all with "thoughts" or "judgings" as mental episodes but about "a priori" laws and conditions for any theory and any judgments whatsoever, conceived as propositions in themselves.
Since "truth-in-itself" has "being-in-itself" as ontological correlate, and since psychologists reduce truth (and hence logic) to empirical psychology, the inevitable consequence is scepticism. Psychologists have also not been successful in showing how from induction or psychological processes we can justify the absolute certainty of logical principles, such as the principles of identity and non-contradiction. It is therefore futile to base certain logical laws and principles on uncertain processes of the mind.
This confusion made by psychologism (and related disciplines such as biologism and anthropologism) can be due to three specific prejudices:
1. The first prejudice is the supposition that logic is somehow normative in nature. Husserl argues that logic is theoretical, i.e., that logic itself proposes "a priori" laws which are themselves the basis of the normative side of logic. Since mathematics is related to logic, he cites an example from mathematics: If we have a formula like "(a + b)(a – b) = a² – b²" it does not tell us how to think mathematically. It just expresses a truth. A proposition that says: "The product of the sum and the difference of a and b "should" give us the difference of the squares of a and b" does express a normative proposition, but this normative statement "is based on" the theoretical statement "(a + b)(a – b) = a² – b²".
2. For psychologists, the acts of judging, reasoning, deriving, and so on, are all psychological processes. Therefore, it is the role of psychology to provide the foundation of these processes. Husserl states that this effort made by psychologists is a "metábasis eis állo génos" (Gr. μετάβασις εἰς ἄλλο γένος, "a transgression to another field"). It is a metábasis because psychology cannot possibly provide any foundations for "a priori" laws which themselves are the basis for all the ways we should think correctly. Psychologists have the problem of confusing intentional activities with the object of these activities. It is important to distinguish between the act of judging and the judgment itself, the act of counting and the number itself, and so on. Counting five objects is undeniably a psychological process, but the number 5 is not.
3. Judgments can be true or not true. Psychologists argue that judgments are true because they become "evidently" true to us. This evidence, a psychological process that "guarantees" truth, is indeed a psychological process. Husserl responds by saying that truth itself as well as logical laws always remain valid regardless of psychological "evidence" that they are true. No psychological process can explain the "a priori" objectivity of these logical truths.
From this criticism to psychologism, the distinction between psychological acts and their intentional objects, and the difference between the normative side of logic and the theoretical side, derives from a platonist conception of logic. This means that we should regard logical and mathematical laws as being independent of the human mind, and also as an autonomy of meanings. It is essentially the difference between the real (everything subject to time) and the ideal or irreal (everything that is atemporal), such as logical truths, mathematical entities, mathematical truths and meanings in general.
Influence.
David Carr of Yale University commented in 1970 on Husserl's following: "It is well known that Husserl was always disappointed at the tendency of his students to go their own way, to embark upon fundamental revisions of phenomenology rather than engage in the communal task" as originally intended by the radical new science. Notwithstanding, he did attract philosophers to phenomenology.
Martin Heidegger is the best known of Husserl's students, the one whom Husserl chose as his successor at Freiburg. Heidegger's magnum opus "Being and Time" was dedicated to Husserl. They shared their thoughts and worked alongside each other for over a decade at the University of Freiburg, Heidegger being Husserl's assistant during 1920–1923. Heidegger's early work followed his teacher, but with time he began to develop new insights distinctively variant. Husserl became increasingly critical of Heidegger's work, especially in 1929, and included pointed criticism of Heidegger in lectures he gave during 1931. Heidegger, while acknowledging his debt to Husserl, followed a political position offensive and harmful to Husserl after the Nazis came to power in 1933, Husserl being of Jewish origin and Heidegger infamously being then a Nazi proponent. Academic discussion of Husserl and Heidegger is extensive.
At Göttingen in 1913 Adolf Reinach (1884–1917) "was now Husserl's right hand. He was above all the mediator between Husserl and the students, for he understood extremely well how to deal with other persons, whereas Husserl was pretty much helpless in this respect." He was an original editor of Husserl's new journal, "Jahrbuch"; one of his works (giving a phenomenological analysis of the law of obligations) appeared in its first issue. Reinach was widely admired and a remarkable teacher. Husserl, in his 1917 obituary, wrote, "He wanted to draw only from the deepest sources, he wanted to produce only work of enduring value. And through his wise restrain he succeeded in this."
Edith Stein was Husserl's student at Göttingen while she wrote her "On the Problem of Empathy" (1916). She then became his assistant at Freiburg 1916–1918. She later adapted her phenomenology to the modern school of Thomas Aquinas".
Ludwig Landgrebe became assistant to Husserl in 1923. From 1939 he collaborated with Eugen Fink at the Husserl-Archives in Leuven. In 1954 he became leader of the Husserl-Archives. Landgrebe is known as one of Husserl's closest associates, but also for his independent views relating to history, religion and politics as seen from the viewpoints of existentialist philosophy and metaphysics.
Eugen Fink was a close associate of Husserl during the 1920s and 1930s. He wrote the "Sixth Cartesian Meditation" which Husserl said was the truest expression and continuation of his own work. Fink delivered the eulogy for Husserl in 1938.
Roman Ingarden, an early student of Husserl at Freiburg, corresponded with Husserl into the mid-1930s. Ingarden did not accept, however, the later transcendental idealism of Husserl which he thought would lead to relativism. Ingarden has written his work in German and Polish. In his "Spór o istnienie świata" (Ger.: "Der Streit um die Existenz der Welt", Eng.: "Dispute about existence of the world") he created his own realistic position, which also helped to spread phenomenology in Poland.
Max Scheler met Husserl in Halle in 1901 and found in his phenomenology a methodological breakthrough for his own philosophy. Scheler, who was at Göttingen when Husserl taught there, was one of the original few editors of the journal "Jahrbuch für Philosophie und Phänomenologische Forschung" (1913). Scheler's work "Formalism in Ethics and Nonformal Ethics of Value" appeared in the new journal (1913 and 1916) and drew acclaim. The personal relationship between the two men, however, became strained, due to Scheler's legal troubles, and Scheler returned to Munich. Although Scheler later criticised Husserl's idealistic logical approach and proposed instead a "phenomenology of love", he states that he remained "deeply indebted" to Husserl throughout his work.
Nicolai Hartmann was once thought to be at the center of phenomenology, but perhaps no longer. In 1921 the prestige of Hartmann the Neo-Kantian, who was Professor of Philosophy at Marburg, was added to the Movement; he "publicly declared his solidarity with the actual work of "die Phänomenologie"." Yet Hartmann's connections were with Max Scheler and the Munich circle; Husserl himself evidently did not consider him as a phenomenologist. His philosophy, however, is said to include an innovative use of the method.
Emmanuel Levinas in 1929 gave a presentation at one of Husserl's last seminars in Freiburg. Also that year he wrote on Husserl's "Ideen" (1913) a long review published by a French journal. With Gabrielle Peiffer, Levinas translated into French Husserl's "Méditations cartésiennes" (1931). He was at first impressed with Heidegger and began a book on him, but broke off the project when Heidegger became involved with the Nazis. After the war he wrote on Jewish spirituality; most of his family had been murdered by the Nazis in Lithuania. Levinas then began to write works that would become widely known and admired.
Jean-Paul Sartre was also largely influenced by Husserl, although he later came to disagree with key points in his analyses. Sartre rejected Husserl's transcendental interpretations begun in his "Ideen" (1913) and instead followed Heidegger's ontology.
Maurice Merleau-Ponty's "Phenomenology of Perception" is influenced by Edmund Husserl's work on perception, intersubjectivity, intentionality, space, and temporality, including Husserl's theory of retention and protention. Merleau-Ponty's description of 'motor intentionality' and sexuality, for example, retain the important structure of the noetic-noematic correlation of Ideas I, yet furher concretize what it means for Husserl when consciousness particularizes itself into modes of intuition. Merleau-Ponty's most clearly Husserlian work is, perhaps, "the Philosopher and His Shadow." Depending on the interpretation of Husserl's accounts of eidetic intuition, given in Husserl's "Phenomenological Psychology" and "Experience and Judgment", it may be that Merleau-Ponty did not accept the "eidetic reduction" nor the "pure essence" said to result. Merleau-Ponty was the first student to study at the Husserl-archives in Leuven.
Gabriel Marcel explicitly rejected existentialism, due to Sartre, but not phenomenology, which has enjoyed a wide following among French Catholics. He appreciated Husserl, Scheler, and (but with apprehension) Heidegger. His expressions like "ontology of sensability" when referring to the body, indicate influence by phenomenological thought.
Kurt Gödel is known to have read "Cartesian Meditations". He expressed very strong appreciation for Husserl's work, especially with regard to "bracketing" or epoché.
Hermann Weyl's interest in intuitionistic logic and impredicativity appears to have resulted from his reading of Husserl. He was introduced to Husserl's work through his wife, Helene Joseph, herself a student of Husserl at Göttingen.
Colin Wilson has used Husserl's ideas extensively in developing his "New Existentialism," particularly in regards to his "intentionality of consciousness," which he mentions in a number of his books.
Rudolf Carnap was also influenced by Husserl, not only concerning Husserl's notion of essential insight that Carnap used in his "Der Raum", but also his notion of "formation rules" and "transformation rules" is founded on Husserl's philosophy of logic.
Karol Wojtyla, who would later become became Pope John Paul II was influenced by Husserl. Phenomenology appears in his major work, "The Acting Person" (1969). Originally published in Polish, it was translated by Andrzej Potocki and edited by Anna-Teresa Tymieniecka in the Analecta Husserliana. "The Acting Person" combines phenomenological work with Thomistic Ethics.
Paul Ricœur has translated many works of Husserl into French and has also written many of his own studies of the philosopher. Among other works, Ricœur employed phenomenology in his "Freud and Philosophy" (1965).
Jacques Derrida wrote several critical studies of Husserl early in his academic career. These included his dissertation, "The Problem of Genesis in Husserl's Philosophy," and also his introduction to "The Origin of Geometry". Derrida continued to make reference to Husserl in works such as "Of Grammatology".
Stanisław Leśniewski and Kazimierz Ajdukiewicz were inspired by Husserl's formal analysis of language. Accordingly, they employed phenomenology in the development of categorial grammar.
Ortega y Gasset visited Husserl at Freiburg in 1934. He credited phenomenology for having 'liberated him' from a narrow neo-Kantian thought. While perhaps not a phenomenologist himself, he introduced the philosophy to Iberia and Latin America.
Wilfrid Sellars, an influential figure in the so-called "Pittsburgh School" (Robert Brandom, John McDowell) had been a student of Marvin Farber, a pupil of Husserl, and was influenced by phenomenology through him:
Hans Blumenberg received his postdoctoral qualification in 1950, with a dissertation on 'Ontological distance', an inquiry into the crisis of Husserl's phenomenology.
The influence of the Husserlian phenomenological tradition in the 21st century extends beyond the confines of the European and North American legacies. It has already started to impact (indirectly) scholarship in Eastern and Oriental thought, including research on the impetus of philosophical thinking in the history of ideas in Islam.
Anthologies:

</doc>
<doc id="9531" url="https://en.wikipedia.org/wiki?curid=9531" title="Electrical engineering">
Electrical engineering

Electrical engineering is a field of engineering that generally deals with the study and application of electricity, electronics, and electromagnetism. This field first became an identifiable occupation in the latter half of the 19th century after commercialization of the electric telegraph, the telephone, and electric power distribution and use. Subsequently, broadcasting and recording media made electronics part of daily life. The invention of the transistor, and later the integrated circuit, brought down the cost of electronics to the point they can be used in almost any household object.
Electrical engineering has now subdivided into a wide range of subfields including electronics, digital computers, power engineering, telecommunications, control systems, radio-frequency engineering, signal processing, instrumentation, and microelectronics. The subject of electronic engineering is often treated as its own subfield but it intersects with all the other subfields, including the power electronics of power engineering.
Electrical engineers typically hold a degree in electrical engineering or electronic engineering. Practicing engineers may have professional certification and be members of a professional body. Such bodies include the Institute of Electrical and Electronics Engineers (IEEE) and the Institution of Engineering and Technology (professional society) (IET).
Electrical engineers work in a very wide range of industries and the skills required are likewise variable. These range from basic circuit theory to the management skills required of a project manager. The tools and equipment that an individual engineer may need are similarly variable, ranging from a simple voltmeter to a top end analyzer to sophisticated design and manufacturing software.
History.
Electricity has been a subject of scientific interest since at least the early 17th century. The first electrical engineer was probably William Gilbert who designed the versorium: a device that detected the presence of statically charged objects. He was also the first to draw a clear distinction between magnetism and static electricity and is credited with establishing the term electricity. Then in 1762 Swedish professor Johan Carl Wilcke invented, and in 1775 Alessandro Volta improved, a device (for which Volta coined the name electrophorus) that produced a static electric charge, and by 1800 Volta had developed the voltaic pile, a forerunner of the electric battery.
19th century.
In 19th century, research into the subject started to intensify. Notable developments in this century include the work of Georg Ohm, who in 1827 quantified the relationship between the electric current and potential difference in a conductor, of Michael Faraday, the discoverer of electromagnetic induction in 1831, and of James Clerk Maxwell, who in 1873 published a unified theory of electricity and magnetism in his treatise "Electricity and Magnetism".
Beginning in the 1830s, efforts were made to apply electricity to practical use in the telegraph. By the end of the 19th century, the world had been forever changed by the rapid communication made possible by the engineering development of land-lines, submarine cables, and, from about 1890, wireless telegraphy.
Practical applications and advances in such fields created an increasing need for standardized units of measure. They led to the international standardization of the units volt, ampere, coulomb, ohm, farad, and henry. This was achieved at an international conference in Chicago in 1893. The publication of these standards formed the basis of future advances in standardisation in various industries, and in many countries the definitions were immediately recognised in relevant legislation.
During these years, the study of electricity was largely considered to be a subfield of physics. That's because early electrical technology was electromechanical in nature. The Technische Universität Darmstadt founded the world's first department of electrical engineering in 1882. The first electrical engineering degree program was started at Massachusetts Institute of Technology (MIT) in the physics department under Professor Charles Cross, though it was Cornell University to produce the world's first electrical engineering graduates in 1885. The first course in electrical engineering was taught in 1883 in Cornell’s Sibley College of Mechanical Engineering and Mechanic Arts. It was not until about 1885 that Cornell President Andrew Dickson White established the first Department of Electrical Engineering in the United States. In the same year, University College London founded the first chair of electrical engineering in Great Britain. Professor Mendell P. Weinbach at University of Missouri soon followed suit by establishing the electrical engineering department in 1886. Afterwards, universities and institutes of technology gradually started to offer electrical engineering programs to their students all over the world.
During these decades use of electrical engineering increased dramatically. In 1882, Thomas Edison switched on the world's first large-scale electric power network that provided 110 volts — direct current (DC) — to 59 customers on Manhattan Island in New York City. In 1884, Sir Charles Parsons invented the steam turbine allowing for more efficient electric power generation. Alternating current, with its ability to transmit power more efficiently over long distances via the use of transformers, developed rapidly in the 1880s and 1890s with transformer designs by Károly Zipernowsky, Ottó Bláthy and Miksa Déri (later called ZBD transformers), Lucien Gaulard, John Dixon Gibbs and William Stanley, Jr.. Practical AC motor designs including induction motors were independently invented by Galileo Ferraris and Nikola Tesla and further developed into a practical three-phase form by Mikhail Dolivo-Dobrovolsky and Charles Eugene Lancelot Brown. Charles Steinmetz and Oliver Heaviside contributed to the theoretical basis of alternating current engineering. The spread in the use of AC set off in the United States what has been called the "War of Currents" between a George Westinghouse backed AC system and a Thomas Edison backed DC power system, with AC being adopted as the overall standard.
More modern developments.
During the development of radio, many scientists and inventors contributed to radio technology and electronics. The mathematical work of James Clerk Maxwell during the 1850s had shown the relationship of different forms of electromagnetic radiation including possibility of invisible airborne waves (later called "radio waves"). In his classic physics experiments of 1888, Heinrich Hertz proved Maxwell's theory by transmitting radio waves with a spark-gap transmitter, and detected them by using simple electrical devices. Other physicists experimented with these new waves and in the process developed devices for transmitting and detecting them. In 1895, Guglielmo Marconi began work on a way to adapt the known methods of transmitting and detecting these "Hertzian waves" into a purpose built commercial wireless telegraphic system. Early on, he sent wireless signals over a distance of one and a half miles. In December 1901, he sent wireless waves that were not affected by the curvature of the Earth. Marconi later transmitted the wireless signals across the Atlantic between Poldhu, Cornwall, and St. John's, Newfoundland, a distance of .
In 1897, Karl Ferdinand Braun introduced the cathode ray tube as part of an oscilloscope, a crucial enabling technology for electronic television. John Fleming invented the first radio tube, the diode, in 1904. Two years later, Robert von Lieben and Lee De Forest independently developed the amplifier tube, called the triode.
In 1920, Albert Hull developed the magnetron which would eventually lead to the development of the microwave oven in 1946 by Percy Spencer. In 1934, the British military began to make strides toward radar (which also uses the magnetron) under the direction of Dr Wimperis, culminating in the operation of the first radar station at Bawdsey in August 1936.
In 1941, Konrad Zuse presented the Z3, the world's first fully functional and programmable computer using electromechanical parts. In 1943, Tommy Flowers designed and built the Colossus, the world's first fully functional, electronic, digital and programmable computer. In 1946, the ENIAC (Electronic Numerical Integrator and Computer) of John Presper Eckert and John Mauchly followed, beginning the computing era. The arithmetic performance of these machines allowed engineers to develop completely new technologies and achieve new objectives, including the Apollo program which culminated in landing astronauts on the Moon.
Solid-state transistors.
The invention of the transistor in late 1947 by William B. Shockley, John Bardeen, and Walter Brattain of the Bell Telephone Laboratories opened the door for more compact devices and led to the development of the integrated circuit in 1958 by Jack Kilby and independently in 1959 by Robert Noyce. Starting in 1968, Ted Hoff and a team at the Intel Corporation invented the first commercial microprocessor, which foreshadowed the personal computer. The Intel 4004 was a four-bit processor released in 1971, but in 1973 the Intel 8080, an eight-bit processor, made the first personal computer, the Altair 8800, possible.
Subdisciplines.
Electrical engineering has many subdisciplines, the most common of which are listed below. Although there are electrical engineers who focus exclusively on one of these subdisciplines, many deal with a combination of them. Sometimes certain fields, such as electronic engineering and computer engineering, are considered separate disciplines in their own right.
Power.
Power engineering deals with the generation, transmission, and distribution of electricity as well as the design of a range of related devices. These include transformers, electric generators, electric motors, high voltage engineering, and power electronics. In many regions of the world, governments maintain an electrical network called a power grid that connects a variety of generators together with users of their energy. Users purchase electrical energy from the grid, avoiding the costly exercise of having to generate their own. Power engineers may work on the design and maintenance of the power grid as well as the power systems that connect to it. Such systems are called "on-grid" power systems and may supply the grid with additional power, draw power from the grid, or do both. Power engineers may also work on systems that do not connect to the grid, called "off-grid" power systems, which in some cases are preferable to on-grid systems. The future includes Satellite controlled power systems, with feedback in real time to prevent power surges and prevent blackouts.
Control.
Control engineering focuses on the modeling of a diverse range of dynamic systems and the design of controllers that will cause these systems to behave in the desired manner. To implement such controllers, electrical engineers may use electronic circuits, digital signal processors, microcontrollers, and programmable logic controls (PLCs). Control engineering has a wide range of applications from the flight and propulsion systems of commercial airliners to the cruise control present in many modern automobiles. It also plays an important role in industrial automation.
Control engineers often utilize feedback when designing control systems. For example, in an automobile with cruise control the vehicle's speed is continuously monitored and fed back to the system which adjusts the motor's power output accordingly. Where there is regular feedback, control theory can be used to determine how the system responds to such feedback.
Electronics.
Electronic engineering involves the design and testing of electronic circuits that use the properties of components such as resistors, capacitors, inductors, diodes, and transistors to achieve a particular functionality. The tuned circuit, which allows the user of a radio to filter out all but a single station, is just one example of such a circuit. Another example (of a pneumatic signal conditioner) is shown in the adjacent photograph.
Prior to the Second World War, the subject was commonly known as "radio engineering" and basically was restricted to aspects of communications and radar, commercial radio, and early television. Later, in post war years, as consumer devices began to be developed, the field grew to include modern television, audio systems, computers, and microprocessors. In the mid-to-late 1950s, the term "radio engineering" gradually gave way to the name "electronic engineering".
Before the invention of the integrated circuit in 1959, electronic circuits were constructed from discrete components that could be manipulated by humans. These discrete circuits consumed much space and power and were limited in speed, although they are still common in some applications. By contrast, integrated circuits packed a large number—often millions—of tiny electrical components, mainly transistors, into a small chip around the size of a coin. This allowed for the powerful computers and other electronic devices we see today.
Microelectronics.
Microelectronics engineering deals with the design and microfabrication of very small electronic circuit components for use in an integrated circuit or sometimes for use on their own as a general electronic component. The most common microelectronic components are semiconductor transistors, although all main electronic components (resistors, capacitors etc.) can be created at a microscopic level. Nanoelectronics is the further scaling of devices down to nanometer levels. Modern devices are already in the nanometer regime, with below 100 nm processing having been standard since about 2002.
Microelectronic components are created by chemically fabricating wafers of semiconductors such as silicon (at higher frequencies, compound semiconductors like gallium arsenide and indium phosphide) to obtain the desired transport of electronic charge and control of current. The field of microelectronics involves a significant amount of chemistry and material science and requires the electronic engineer working in the field to have a very good working knowledge of the effects of quantum mechanics.
Signal processing.
Signal processing deals with the analysis and manipulation of signals. Signals can be either analog, in which case the signal varies continuously according to the information, or digital, in which case the signal varies according to a series of discrete values representing the information. For analog signals, signal processing may involve the amplification and filtering of audio signals for audio equipment or the modulation and demodulation of signals for telecommunications. For digital signals, signal processing may involve the compression, error detection and error correction of digitally sampled signals.
Signal Processing is a very mathematically oriented and intensive area forming the core of digital signal processing and it is rapidly expanding with new applications in every field of electrical engineering such as communications, control, radar, audio engineering, broadcast engineering, power electronics, and biomedical engineering as many already existing analog systems are replaced with their digital counterparts. Analog signal processing is still important in the design of many control systems.
DSP processor ICs are found in every type of modern electronic systems and products including, SDTV | HDTV sets, radios and mobile communication devices, Hi-Fi audio equipment, Dolby noise reduction algorithms, GSM mobile phones, mp3 multimedia players, camcorders and digital cameras, automobile control systems, noise cancelling headphones, digital spectrum analyzers, intelligent missile guidance, radar, GPS based cruise control systems, and all kinds of image processing, video processing, audio processing, and speech processing systems.
Telecommunications.
Telecommunications engineering focuses on the transmission of information across a channel such as a coax cable, optical fiber or free space. Transmissions across free space require information to be encoded in a carrier signal to shift the information to a carrier frequency suitable for transmission; this is known as modulation. Popular analog modulation techniques include amplitude modulation and frequency modulation. The choice of modulation affects the cost and performance of a system and these two factors must be balanced carefully by the engineer.
Once the transmission characteristics of a system are determined, telecommunication engineers design the transmitters and receivers needed for such systems. These two are sometimes combined to form a two-way communication device known as a transceiver. A key consideration in the design of transmitters is their power consumption as this is closely related to their signal strength. If the signal strength of a transmitter is insufficient the signal's information will be corrupted by noise.
Instrumentation.
Instrumentation engineering deals with the design of devices to measure physical quantities such as pressure, flow, and temperature. The design of such instrumentation requires a good understanding of physics that often extends beyond electromagnetic theory. For example, flight instruments measure variables such as wind speed and altitude to enable pilots the control of aircraft analytically. Similarly, thermocouples use the Peltier-Seebeck effect to measure the temperature difference between two points.
Often instrumentation is not used by itself, but instead as the sensors of larger electrical systems. For example, a thermocouple might be used to help ensure a furnace's temperature remains constant. For this reason, instrumentation engineering is often viewed as the counterpart of control engineering.
Computers.
Computer engineering deals with the design of computers and computer systems. This may involve the design of new hardware, the design of PDAs, tablets, and supercomputers, or the use of computers to control an industrial plant. Computer engineers may also work on a system's software. However, the design of complex software systems is often the domain of software engineering, which is usually considered a separate discipline. Desktop computers represent a tiny fraction of the devices a computer engineer might work on, as computer-like architectures are now found in a range of devices including video game consoles and DVD players.
Related disciplines.
Mechatronics is an engineering discipline which deals with the convergence of electrical and mechanical systems. Such combined systems are known as electromechanical systems and have widespread adoption. Examples include automated manufacturing systems, heating, ventilation and air-conditioning systems, and various subsystems of aircraft and automobiles.
The term "mechatronics" is typically used to refer to macroscopic systems but futurists have predicted the emergence of very small electromechanical devices. Already, such small devices, known as Microelectromechanical systems (MEMS), are used in automobiles to tell airbags when to deploy, in digital projectors to create sharper images, and in inkjet printers to create nozzles for high definition printing. In the future it is hoped the devices will help build tiny implantable medical devices and improve optical communication.
Biomedical engineering is another related discipline, concerned with the design of medical equipment. This includes fixed equipment such as ventilators, MRI scanners, and electrocardiograph monitors as well as mobile equipment such as cochlear implants, artificial pacemakers, and artificial hearts.
Aerospace engineering and robotics an example is the most recent electric propulsion and ion propulsion.
Education.
Electrical engineers typically possess an academic degree with a major in electrical engineering, electronics engineering, electrical engineering technology, or electrical and electronic engineering. The same fundamental principles are taught in all programs, though emphasis may vary according to title. The length of study for such a degree is usually four or five years and the completed degree may be designated as a Bachelor of Science in Electrical/Electronics Engineering Technology, Bachelor of Engineering, Bachelor of Science, Bachelor of Technology, or Bachelor of Applied Science depending on the university. The bachelor's degree generally includes units covering physics, mathematics, computer science, project management, and a variety of topics in electrical engineering. Initially such topics cover most, if not all, of the subdisciplines of electrical engineering. At some schools, the students can then choose to emphasize one or more subdisciplines towards the end of their courses of study.
At many schools, electronic engineering is included as part of an electrical award, sometimes explicitly, such as a Bachelor of Engineering (Electrical and Electronic), but in others electrical and electronic engineering are both considered to be sufficiently broad and complex that separate degrees are offered.
Some electrical engineers choose to study for a postgraduate degree such as a Master of Engineering/Master of Science (M.Eng./M.Sc.), a Master of Engineering Management, a Doctor of Philosophy (Ph.D.) in Engineering, an Engineering Doctorate (Eng.D.), or an Engineer's degree. The master's and engineer's degrees may consist of either research, coursework or a mixture of the two. The Doctor of Philosophy and Engineering Doctorate degrees consist of a significant research component and are often viewed as the entry point to academia. In the United Kingdom and some other European countries, Master of Engineering is often considered to be an undergraduate degree of slightly longer duration than the Bachelor of Engineering rather than postgraduate.
Practicing engineers.
In most countries, a bachelor's degree in engineering represents the first step towards professional certification and the degree program itself is certified by a professional body. After completing a certified degree program the engineer must satisfy a range of requirements (including work experience requirements) before being certified. Once certified the engineer is designated the title of Professional Engineer (in the United States, Canada and South Africa), Chartered Engineer or Incorporated Engineer (in India, Pakistan, the United Kingdom, Ireland and Zimbabwe), Chartered Professional Engineer (in Australia and New Zealand) or European Engineer (in much of the European Union).
The advantages of certification vary depending upon location. For example, in the United States and Canada "only a licensed engineer may seal engineering work for public and private clients". This requirement is enforced by state and provincial legislation such as Quebec's Engineers Act. In other countries, no such legislation exists. Practically all certifying bodies maintain a code of ethics that they expect all members to abide by or risk expulsion. In this way these organizations play an important role in maintaining ethical standards for the profession. Even in jurisdictions where certification has little or no legal bearing on work, engineers are subject to contract law. In cases where an engineer's work fails he or she may be subject to the tort of negligence and, in extreme cases, the charge of criminal negligence. An engineer's work must also comply with numerous other rules and regulations such as building codes and legislation pertaining to environmental law.
Professional bodies of note for electrical engineers include the Institute of Electrical and Electronics Engineers (IEEE) and the Institution of Engineering and Technology (IET). The IEEE claims to produce 30% of the world's literature in electrical engineering, has over 360,000 members worldwide and holds over 3,000 conferences annually. The IET publishes 21 journals, has a worldwide membership of over 150,000, and claims to be the largest professional engineering society in Europe. Obsolescence of technical skills is a serious concern for electrical engineers. Membership and participation in technical societies, regular reviews of periodicals in the field and a habit of continued learning are therefore essential to maintaining proficiency. An MIET(Member of the Institution of Engineering and Technology) is recognised in Europe as an Electrical and computer (technology) engineer.
In Australia, Canada, and the United States electrical engineers make up around 0.25% of the labor force (see note).
Tools and work.
From the Global Positioning System to electric power generation, electrical engineers have contributed to the development of a wide range of technologies. They design, develop, test, and supervise the deployment of electrical systems and electronic devices. For example, they may work on the design of telecommunication systems, the operation of electric power stations, the lighting and wiring of buildings, the design of household appliances, or the electrical control of industrial machinery.
Fundamental to the discipline are the sciences of physics and mathematics as these help to obtain both a qualitative and quantitative description of how such systems will work. Today most engineering work involves the use of computers and it is commonplace to use computer-aided design programs when designing electrical systems. Nevertheless, the ability to sketch ideas is still invaluable for quickly communicating with others.
Although most electrical engineers will understand basic circuit theory (that is the interactions of elements such as resistors, capacitors, diodes, transistors, and inductors in a circuit), the theories employed by engineers generally depend upon the work they do. For example, quantum mechanics and solid state physics might be relevant to an engineer working on VLSI (the design of integrated circuits), but are largely irrelevant to engineers working with macroscopic electrical systems. Even circuit theory may not be relevant to a person designing telecommunication systems that use off-the-shelf components. Perhaps the most important technical skills for electrical engineers are reflected in university programs, which emphasize strong numerical skills, computer literacy, and the ability to understand the technical language and concepts that relate to electrical engineering.
A wide range of instrumentation is used by electrical engineers. For simple control circuits and alarms, a basic multimeter measuring voltage, current, and resistance may suffice. Where time-varying signals need to be studied, the oscilloscope is also an ubiquitous instrument. In RF engineering and high frequency telecommunications, spectrum analyzers and network analyzers are used. In some disciplines, safety can be a particular concern with instrumentation. For instance, medical electronics designers must take into account that much lower voltages than normal can be dangerous when electrodes are directly in contact with internal body fluids. Power transmission engineering also has great safety concerns due to the high voltages used; although voltmeters may in principle be similar to their low voltage equivalents, safety and calibration issues make them very different. Many disciplines of electrical engineering use tests specific to their discipline. Audio electronics engineers use audio test sets consisting of a signal generator and a meter, principally to measure level but also other parameters such as harmonic distortion and noise. Likewise, information technology have their own test sets, often specific to a particular data format, and the same is true of television broadcasting.
For many engineers, technical work accounts for only a fraction of the work they do. A lot of time may also be spent on tasks such as discussing proposals with clients, preparing budgets and determining project schedules. Many senior engineers manage a team of technicians or other engineers and for this reason project management skills are important. Most engineering projects involve some form of documentation and strong written communication skills are therefore very important.
The workplaces of engineers are just as varied as the types of work they do. Electrical engineers may be found in the pristine lab environment of a fabrication plant, the offices of a consulting firm or on site at a mine. During their working life, electrical engineers may find themselves supervising a wide range of individuals including scientists, electricians, computer programmers, and other engineers.
Electrical engineering has an intimate relationship with the physical sciences. For instance, the physicist Lord Kelvin played a major role in the engineering of the first transatlantic telegraph cable. Conversely, the engineer Oliver Heaviside produced major work on the mathematics of transmission on telegraph cables. Electrical engineers are often required on major science projects. For instance, large particle accelerators such as CERN need electrical engineers to deal with many aspects of the project: from the power distribution, to the instrumentation, to the manufacture and installation of the superconducting electromagnets.
Notes.
Note I - In May 2014 there were around 175,000 people working as electrical engineers in the US. In 2012, Australia had around 19,000 while in Canada, there were around 37,000 (), constituting about 0.2% of the labour force in each of the three countries. Australia and Canada reported that 96% and 88% of their electrical engineers respectively are male.

</doc>
<doc id="9532" url="https://en.wikipedia.org/wiki?curid=9532" title="Electromagnetism">
Electromagnetism

Electromagnetism is a branch of physics which involves the study of the electromagnetic force, a type of physical interaction that occurs between electrically charged particles. The electromagnetic force usually shows electromagnetic fields, such as electric fields, magnetic fields, and light. The electromagnetic force is one of the four fundamental interactions in nature. The other three fundamental interactions are the strong interaction, the weak interaction, and gravitation.
The word "electromagnetism" is a compound form of two Greek terms, ἤλεκτρον, "ēlektron", "amber", and μαγνῆτις λίθος "magnētis lithos", which means "magnesian stone", a type of iron ore. The science of electromagnetic phenomena is defined in terms of the electromagnetic force, sometimes called the Lorentz force, which includes both electricity and magnetism as elements of one phenomenon.
The electromagnetic force plays a major role in determining the internal properties of most objects encountered in daily life. Ordinary matter takes its form as a result of intermolecular forces between individual molecules in matter. Electrons are bound by electromagnetic wave mechanics into orbitals around atomic nuclei to form atoms, which are the building blocks of molecules. This governs the processes involved in chemistry, which arise from interactions between the electrons of neighboring atoms, which are in turn determined by the interaction between electromagnetic force and the momentum of the electrons.
There are numerous mathematical descriptions of the electromagnetic field. In classical electrodynamics, electric fields are described as electric potential and electric current. In Faraday's law, magnetic fields are associated with electromagnetic induction and magnetism, and Maxwell's equations describe how electric and magnetic fields are generated and altered by each other and by charges and currents.
The theoretical implications of electromagnetism, in particular the establishment of the speed of light based on properties of the "medium" of propagation (permeability and permittivity), led to the development of special relativity by Albert Einstein in 1905.
Although electromagnetism is considered one of the four fundamental forces, at high energy the weak force and electromagnetism are unified. In the history of the universe, during the quark epoch, the electroweak force split into the electromagnetic and weak forces.
History of the theory.
Originally, electricity and magnetism were thought of as two separate forces. This view changed, however, with the publication of James Clerk Maxwell's 1873 "A Treatise on Electricity and Magnetism" in which the interactions of positive and negative charges were shown to be regulated by one force. There are four main effects resulting from these interactions, all of which have been clearly demonstrated by experiments:
While preparing for an evening lecture on 21 April 1820, Hans Christian Ørsted made a surprising observation. As he was setting up his materials, he noticed a compass needle deflected away from magnetic north when the electric current from the battery he was using was switched on and off. This deflection convinced him that magnetic fields radiate from all sides of a wire carrying an electric current, just as light and heat do, and that it confirmed a direct relationship between electricity and magnetism.
At the time of discovery, Ørsted did not suggest any satisfactory explanation of the phenomenon, nor did he try to represent the phenomenon in a mathematical framework. However, three months later he began more intensive investigations. Soon thereafter he published his findings, proving that an electric current produces a magnetic field as it flows through a wire. The CGS unit of magnetic induction (oersted) is named in honor of his contributions to the field of electromagnetism.
His findings resulted in intensive research throughout the scientific community in electrodynamics. They influenced French physicist André-Marie Ampère's developments of a single mathematical form to represent the magnetic forces between current-carrying conductors. Ørsted's discovery also represented a major step toward a unified concept of energy.
This unification, which was observed by Michael Faraday, extended by James Clerk Maxwell, and partially reformulated by Oliver Heaviside and Heinrich Hertz, is one of the key accomplishments of 19th century mathematical physics. It had far-reaching consequences, one of which was the understanding of the nature of light. Unlike what was proposed in Electromagnetism, light and other electromagnetic waves are at the present seen as taking the form of quantized, self-propagating oscillatory electromagnetic field disturbances which have been called photons. Different frequencies of oscillation give rise to the different forms of electromagnetic radiation, from radio waves at the lowest frequencies, to visible light at intermediate frequencies, to gamma rays at the highest frequencies.
Ørsted was not the only person to examine the relationship between electricity and magnetism. In 1802, Gian Domenico Romagnosi, an Italian legal scholar, deflected a magnetic needle using electrostatic charges. Actually, no galvanic current existed in the setup and hence no electromagnetism was present. An account of the discovery was published in 1802 in an Italian newspaper, but it was largely overlooked by the contemporary scientific community.
Fundamental forces.
The electromagnetic force is one of the four known fundamental forces. The other fundamental forces are:
All other forces (e.g., friction) are derived from these four fundamental forces (including momentum which is carried by the movement of particles).
The electromagnetic force is the one responsible for practically all the phenomena one encounters in daily life above the nuclear scale, with the exception of gravity. Roughly speaking, all the forces involved in interactions between atoms can be explained by the electromagnetic force acting on the electrically charged atomic nuclei and electrons inside and around the atoms, together with how these particles carry momentum by their movement. This includes the forces we experience in "pushing" or "pulling" ordinary material objects, which come from the intermolecular forces between the individual molecules in our bodies and those in the objects. It also includes all forms of chemical phenomena.
A necessary part of understanding the intra-atomic to intermolecular forces is the effective force generated by the momentum of the electrons' movement, and that electrons move between interacting atoms, carrying momentum with them. As a collection of electrons becomes more confined, their minimum momentum necessarily increases due to the Pauli exclusion principle. The behaviour of matter at the molecular scale including its density is determined by the balance between the electromagnetic force and the force generated by the exchange of momentum carried by the electrons themselves.
Classical electrodynamics.
The scientist William Gilbert proposed, in his "De Magnete" (1600), that electricity and magnetism, while both capable of causing attraction and repulsion of objects, were distinct effects. Mariners had noticed that lightning strikes had the ability to disturb a compass needle, but the link between lightning and electricity was not confirmed until Benjamin Franklin's proposed experiments in 1752. One of the first to discover and publish a link between man-made electric current and magnetism was Romagnosi, who in 1802 noticed that connecting a wire across a voltaic pile deflected a nearby compass needle. However, the effect did not become widely known until 1820, when Ørsted performed a similar experiment. Ørsted's work influenced Ampère to produce a theory of electromagnetism that set the subject on a mathematical foundation.
A theory of electromagnetism, known as classical electromagnetism, was developed by various physicists over the course of the 19th century, culminating in the work of James Clerk Maxwell, who unified the preceding developments into a single theory and discovered the electromagnetic nature of light. In classical electromagnetism, the electromagnetic field obeys a set of equations known as Maxwell's equations, and the electromagnetic force is given by the Lorentz force law.
One of the peculiarities of classical electromagnetism is that it is difficult to reconcile with classical mechanics, but it is compatible with special relativity. According to Maxwell's equations, the speed of light in a vacuum is a universal constant, dependent only on the electrical permittivity and magnetic permeability of free space. This violates Galilean invariance, a long-standing cornerstone of classical mechanics. One way to reconcile the two theories (electromagnetism and classical mechanics) is to assume the existence of a luminiferous aether through which the light propagates. However, subsequent experimental efforts failed to detect the presence of the aether. After important contributions of Hendrik Lorentz and Henri Poincaré, in 1905, Albert Einstein solved the problem with the introduction of special relativity, which replaces classical kinematics with a new theory of kinematics that is compatible with classical electromagnetism. (For more information, see History of special relativity.)
In addition, relativity theory shows that in moving frames of reference a magnetic field transforms to a field with a nonzero electric component and vice versa; thus firmly showing that they are two sides of the same coin, and thus the term "electromagnetism". (For more information, see Classical electromagnetism and special relativity and Covariant formulation of classical electromagnetism.
Quantum mechanics.
Photoelectric effect.
In another paper published in 1905, Albert Einstein undermined the very foundations of classical electromagnetism. In his theory of the photoelectric effect (for which he won the Nobel prize in physics) and inspired by the idea of Max Planck's "quanta", he posited that light could exist in discrete particle-like quantities as well, which later came to be known as photons. Einstein's theory of the photoelectric effect extended the insights that appeared in the solution of the ultraviolet catastrophe presented by Max Planck in 1900. In his work, Planck showed that hot objects emit electromagnetic radiation in discrete packets ("quanta"), which leads to a finite total energy emitted as black body radiation. Both of these results were in direct contradiction with the classical view of light as a continuous wave. Planck's and Einstein's theories were progenitors of quantum mechanics, which, when formulated in 1925, necessitated the invention of a quantum theory of electromagnetism. This theory, completed in the 1940s-1950s, is known as quantum electrodynamics (or "QED"), and, in situations where perturbation theory is applicable, is one of the most accurate theories known to physics.
Quantum electrodynamics.
All electromagnetic phenomena are underpinned by quantum mechanics, specifically by quantum electrodynamics (which includes classical electrodynamics as a limiting case) and this accounts for almost all physical phenomena observable to the unaided human senses, including light and other electromagnetic radiation, all of chemistry, most of mechanics (excepting gravitation), and, of course, magnetism and electricity.
Electroweak interaction.
The electroweak interaction is the unified description of two of the four known fundamental interactions of nature: electromagnetism and the weak interaction. Although these two forces appear very different at everyday low energies, the theory models them as two different aspects of the same force. Above the unification energy, on the order of 100 GeV, they would merge into a single electroweak force. Thus if the universe is hot enough (approximately 10 K, a temperature exceeded until shortly after the Big Bang) then the electromagnetic force and weak force merge into a combined electroweak force. During the electroweak epoch, the electroweak force separated from the strong force. During the quark epoch, the electroweak force split into the electromagnetic and weak force.
Quantities and units.
Electromagnetic units are part of a system of electrical units based primarily upon the magnetic properties of electric currents, the fundamental SI unit being the ampere. The units are:
In the electromagnetic cgs system, electric current is a fundamental quantity defined via Ampère's law and takes the permeability as a dimensionless quantity (relative permeability) whose value in a vacuum is unity. As a consequence, the square of the speed of light appears explicitly in some of the equations interrelating quantities in this system.
Formulas for physical laws of electromagnetism (such as Maxwell's equations) need to be adjusted depending on what system of units one uses. This is because there is no one-to-one correspondence between electromagnetic units in SI and those in CGS, as is the case for mechanical units. Furthermore, within CGS, there are several plausible choices of electromagnetic units, leading to different unit "sub-systems", including Gaussian, "ESU", "EMU", and Heaviside–Lorentz. Among these choices, Gaussian units are the most common today, and in fact the phrase "CGS units" is often used to refer specifically to CGS-Gaussian units.

</doc>
<doc id="9534" url="https://en.wikipedia.org/wiki?curid=9534" title="Euphemism">
Euphemism

A euphemism is a generally innocuous word or expression used in place of one that may be found offensive or suggest something unpleasant. Some euphemisms are intended to amuse; while others use bland, inoffensive terms for things the user wishes to downplay. Euphemisms are used to refer to taboo topics (such as disability, sex, excretion, and death) in a polite way, or to mask profanity.
There are three antonyms of euphemism: "dysphemism", "cacophemism", and "loaded language". Dysphemism can be either offensive or merely ironic; cacophemism is deliberately offensive. Loaded language evokes a visceral response beyond the meaning of the words.
Etymology.
"Euphemism" comes from the Greek word ("euphemia"), meaning "the use of words of good omen", which in turn is derived from the Greek root-words "eû" (), "good, well" and "phḗmē" (φήμη) "prophetic speech; rumour, talk". Etymologically, the "eupheme" is the opposite of the "blaspheme" "evil-speaking." The term "euphemism" itself was used as a euphemism by the ancient Greeks, meaning "to keep a holy silence" (speaking well by not speaking at all).
Purpose.
Euphemism use ranges from a polite concern for propriety, to attempting to escape responsibility for war crimes. For instance one reason for the comparative scarcity of written evidence documenting the exterminations at Auschwitz (at least given the scale) is "directives for the extermination process obscured in bureaucratic euphemisms." Columnist David Brooks called the euphemisms for torture at Abu Ghraib, Guantánamo and elsewhere an effort to "dull the moral sensibility."
Formation.
Phonetic modification.
Phonetic euphemism is used to replace profanities, giving them the intensity of a mere interjection.
"NOTE: Contrary to popular belief, the words "crap" and "freaking" are not euphemisms for "shit" and "fucking." However, crap is a synonym and is nearly as vulgar as shit, and freaking can refer to obscene and sexual dancing. Crap, on the other hand, can refer to a dice game; to which the word "shit" isn't associated with."
Rhetoric.
Euphemism may be used as a rhetorical strategy, in which case its goal is to change the valence of a description from positive to negative.
Slang.
Using a less harsh term with similar meaning. For instance, "screwed up" is a euphemism for "fucked up"; "hook-up", "we hooked up", or "laid" for sexual intercourse
There is some disagreement over whether certain terms are or are not euphemisms. For example, sometimes the phrase "visually impaired" is labeled as a politically correct euphemism for "blind". However, visual impairment can be a broader term, including, for example, people who have partial sight in one eye, those with uncorrectable mild to moderate poor vision, or even those who wear glasses, a group that would be excluded by the word "blind".
Evolution.
Euphemisms may be formed in a number of ways. Periphrasis, or circumlocution, is one of the most common: to "speak around" a given word, implying it without saying it. Over time, circumlocutions become recognized as established euphemisms for particular words or ideas.
To alter the pronunciation or spelling of a taboo word (such as a swear word) to form a euphemism is known as "taboo deformation", or "minced oath". In American English, words that are unacceptable on television such as "fuck", may be represented by deformations such as "freak", even in children's cartoons. Some examples of rhyming slang may serve the same purpose: to call a person a "berk" sounds less offensive than to call a person a "cunt", though "berk" is short for "Berkeley Hunt", which rhymes with "cunt".
Bureaucracies frequently spawn euphemisms of a more deliberate nature, "doublespeak" expressions. For example, in the past the US military called contamination by radioactive isotopes "sunshine units". A practical death sentence in the Soviet Union during the Great Purge often used the clause "imprisonment "without right to correspondence":" the person sentenced never had a chance to correspond because soon after imprisonment he would be shot. As early as 1939, Nazi official Reinhard Heydrich used the term Sonderbehandlung (translated into English as "special treatment") to mean summary execution, most likely by hanging, of persons who proved 'disciplinary problems' to the Nazis even before the Nazis began the systematic extermination of the Jews. Heinrich Himmler, aware that the word had come to be known to mean murder, replaced that euphemism with one in which Jews would be "guided" (to their deaths) through the slave-labor and extermination camps after having been "evacuated" to their doom. Such was part of the superficially innocuous formulation Endlösung der Judenfrage (the "Final Solution to the Jewish Question"), which became infamous to the entire world during the Nuremberg Trials.
A euphemism may often devolve into a taboo word itself, through the linguistic process known as "pejoration" or "semantic change" described by W.V.O. Quine, and more recently dubbed the "euphemism treadmill" by Harvard professor Steven Pinker. For instance, "Toilet" is an 18th-century euphemism, replacing the older euphemism "House-of-Office", which in turn replaced the even older euphemisms "privy-house" or "bog-house". In the 20th century, where the words "lavatory" or "toilet" were deemed inappropriate (e.g. in the United States), they were sometimes replaced with "bathroom" or "water closet," which in turn became "restroom", "W.C.", or "washroom". The word "shit" appears to have originally been a euphemism for defecation in Pre-Germanic, as the Proto-Indo-European root *"skined-", from which it was derived, meant to cut off.
In popular culture.
Doublespeak is a term sometimes used for deliberate euphemistic misuse of incorrect words to disguise unacceptable meaning, as in a "Ministry of Peace" which wages war, a "Ministry of Love" which imprisons and tortures. It is a portmanteau of the terms "newspeak" and "doublethink", which originate from George Orwell's novel "1984".
The "Dead Parrot sketch" from "Monty Python's Flying Circus" contains an extensive list of euphemisms for death, referring to the deceased parrot that the character played by John Cleese had purchased.
The word euphemism itself can be used as a euphemism. In the animated TV special "Halloween Is Grinch Night" (See Dr. Seuss), a child asks to go to the "euphemism", where "euphemism" is being used as a euphemism for "outhouse". This euphemistic use of "euphemism" also occurred in the play "Who's Afraid of Virginia Woolf?" where a character requests, "Martha, will you show her where we keep the, uh, euphemism?"
In Wes Anderson's film "Fantastic Mr. Fox", the replacement of swear words by the word "cuss" became a humorous motif throughout the film.
In Tom Hanks' web series "Electric City", the use of profanity has been censored by the word "expletive".
In Isaac Asimov's Foundation series, the curses of the scientist Ebling Mis have all been replaced with the word "unprintable". In fact, there is only one case of his curses being referred to as such, leading some readers to mistakenly assume that the euphemism is Ebling's, rather than Asimov's. The same word has also been used in his short story "Flies".

</doc>
<doc id="9536" url="https://en.wikipedia.org/wiki?curid=9536" title="Edmund Spenser">
Edmund Spenser

Edmund Spenser (; 1552/1553 – 13 January 1599) was an English poet best known for "The Faerie Queene", an epic poem and fantastical allegory celebrating the Tudor dynasty and Elizabeth I. He is recognized as one of the premier craftsmen of nascent Modern English verse, and is often considered one of the greatest poets in the English language.
Life.
Edmund Spenser was born in East Smithfield, London, around the year 1552, though there is some ambiguity as to the exact date of his birth. As a young boy, he was educated in London at the Merchant Taylors' School and matriculated as a sizar at Pembroke College, Cambridge. While at Cambridge he became a friend of Gabriel Harvey and later consulted him, despite their differing views on poetry. In 1578, he became for a short time secretary to John Young, Bishop of Rochester. In 1579, he published "The Shepheardes Calender" and around the same time married his first wife, Machabyas Childe. They had two children, Sylvanus (d.1638) and Katherine.
In July 1580, Spenser went to Ireland in service of the newly appointed Lord Deputy, Arthur Grey, 14th Baron Grey de Wilton. Spenser served under Lord Gray with Walter Raleigh at the Siege of Smerwick massacre. When Lord Grey was recalled to England, Spenser stayed on in Ireland, having acquired other official posts and lands in the Munster Plantation. Raleigh acquired other nearby Munster estates confiscated in the Second Desmond Rebellion. Some time between 1587 and 1589, Spenser acquired his main estate at Kilcolman, near Doneraile in North Cork. He later bought a second holding to the south, at Rennie, on a rock overlooking the river Blackwater in North Cork. Its ruins are still visible today. A short distance away grew a tree, locally known as "Spenser's Oak" until it was destroyed in a lightning strike in the 1960s. Local legend has it that he penned some of "The Faerie Queene" under this tree.
In 1590, Spenser brought out the first three books of his most famous work, "The Faerie Queene", having travelled to London to publish and promote the work, with the likely assistance of Raleigh. He was successful enough to obtain a life pension of £50 a year from the Queen. He probably hoped to secure a place at court through his poetry, but his next significant publication boldly antagonised the queen's principal secretary, Lord Burghley (William Cecil), through its inclusion of the satirical "Mother Hubberd's Tale". He returned to Ireland.
By 1594, Spenser's first wife had died, and in that year he married Elizabeth Boyle, to whom he addressed the sonnet sequence "Amoretti". The marriage itself was celebrated in "Epithalamion". They had a son named Peregrine.
In 1596, Spenser wrote a prose pamphlet titled "A View of the Present State of Ireland". This piece, in the form of a dialogue, circulated in manuscript, remaining unpublished until the mid-seventeenth century. It is probable that it was kept out of print during the author's lifetime because of its inflammatory content. The pamphlet argued that Ireland would never be totally 'pacified' by the English until its indigenous language and customs had been destroyed, if necessary by violence.
In 1598, during the Nine Years War, Spenser was driven from his home by the native Irish forces of Aodh Ó Néill. His castle at Kilcolman was burned, and Ben Jonson, who may have had private information, asserted that one of his infant children died in the blaze. 
In the year after being driven from his home, 1599, Spenser travelled to London, where he died at the age of forty-six – "for want of bread", according to Ben Jonson, which is ironic considering Spenser's approving writing on the scorched-earth policy that caused famine in Ireland. His coffin was carried to his grave in Poets' Corner in Westminster Abbey by other poets, who threw many pens and pieces of poetry into his grave with many tears. His second wife survived him and remarried twice. His sister Sarah, who had accompanied him to Ireland, married into the Travers family, and her descendants were prominent landowners in Cork for centuries.
Rhyme and reason.
Thomas Fuller, in "Worthies of England", included a story where the Queen told her treasurer, William Cecil, to pay Spenser one hundred pounds for his poetry. The treasurer, however, objected that the sum was too much. She said, "Then give him what is reason". Without receiving his payment in due time, Spenser gave the Queen this quatrain on one of her progresses:
<poem>
I was promis'd on a time,
To have a reason for my rhyme:
From that time unto this season,
I receiv'd nor rhyme nor reason.
</poem>
She immediately ordered the treasurer pay Spenser the original £100.
This story seems to have attached itself to Spenser from Thomas Churchyard, who apparently had difficulty in getting payment of his pension, the only other pension Elizabeth awarded to a poet. Spenser seems to have had no difficulty in receiving payment when it was due as the pension was being collected for him by his publisher, Ponsonby.
The Shepherd's Calendar.
The Shepherd's Calendar is Edmund Spenser's first major work, which appeared in 1579. It emulates Virgil's Eclogues of the first century BCE and the Eclogues of Mantuan by Baptista Mantuanus, a late medieval, early renaissance poet. An eclogue is a short pastoral poem that is in the form of a dialogue or soliloquy. Although all the months together form an entire year, each month stands alone as a separate poem. Editions of the late 16th and early 17th centuries include woodcuts for each month/poem, and thereby have a slight similarity to an emblem book which combines a number of self-contained pictures and texts, usually a short vignette, saying, or allegory with an accompanying illustration.
The Faerie Queene.
Spenser's masterpiece is the epic poem "The Faerie Queene". The first three books of "The Faerie Queene" were published in 1590, and a second set of three books were published in 1596. Spenser originally indicated that he intended the poem to consist of twelve books, so the version of the poem we have today is incomplete. Despite this, it remains one of the longest poems in the English language. It is an allegorical work, and can be read (as Spenser presumably intended) on several levels of allegory, including as praise of Queen Elizabeth I. In a completely allegorical context, the poem follows several knights in an examination of several virtues. In Spenser's "A Letter of the Authors," he states that the entire epic poem is "cloudily enwrapped in allegorical devises," and that the aim behind "The Faerie Queene" was to "fashion a gentleman or noble person in virtuous and gentle discipline.”
Shorter poems.
Spenser published numerous relatively short poems in the last decade of the sixteenth century, almost all of which consider love or sorrow. In 1591, he published "Complaints", a collection of poems that express complaints in mournful or mocking tones. Four years later, in 1595, Spenser published "Amoretti and Epithalamion". This volume contains eighty-nine sonnets commemorating his courtship of Elizabeth Boyle. In "Amoretti," Spenser uses subtle humour and parody while praising his beloved, reworking Petrarchism in his treatment of longing for a woman. "Epithalamion," similar to "Amoretti," deals in part with the unease in the development of a romantic and sexual relationship. It was written for his wedding to his young bride, Elizabeth Boyle. The poem consists of 365 long lines, corresponding to the days of the year; 68 short lines, claimed to represent the sum of the 52 weeks, 12 months, and 4 seasons of the annual cycle; and 24 stanzas, corresponding to the diurnal and sidereal hours. Some have speculated that the attention to disquiet in general reflects Spenser's personal anxieties at the time, as he was unable to complete his most significant work, "The Faerie Queene". In the following year Spenser released "Prothalamion", a wedding song written for the daughters of a duke, allegedly in hopes to gain favour in the court.
The Spenserian stanza and sonnet.
Spenser used a distinctive verse form, called the Spenserian stanza, in several works, including "The Faerie Queene". The stanza's main meter is iambic pentameter with a final line in iambic hexameter (having six feet or stresses, known as an Alexandrine), and the rhyme scheme is ababbcbcc. He also used his own rhyme scheme for the sonnet. In a Spenserian sonnet, the last line of every stanza is linked with the first line of the next one, yielding the rhyme scheme ababbcbccdcdee.
Influences.
Though Spenser was well read in classical literature, scholars have noted that his poetry does not rehash tradition, but rather is distinctly his. This individuality may have resulted, to some extent, from a lack of comprehension of the classics. Spenser strove to emulate such ancient Roman poets as Virgil and Ovid, whom he studied during his schooling, but many of his best-known works are notably divergent from those of his predecessors. The language of his poetry is purposely archaic, reminiscent of earlier works such as "The Canterbury Tales" of Geoffrey Chaucer and "Il Canzoniere" of Francesco Petrarca, whom Spenser greatly admired.
Spenser was called a Poets' Poet and was admired by John Milton, William Blake, William Wordsworth, John Keats, Lord Byron, and Alfred Lord Tennyson, among others. Walter Raleigh wrote a dedicatory poem to "The Faerie Queene" in 1590, in which he claims to admire and value Spenser's work more so than any other in the English language. John Milton in his Areopagitica mentions "our sage and serious poet Spenser, whom I dare be known to think a better teacher than Scotus or Aquinas". In the eighteenth century, Alexander Pope compared Spenser to "a mistress, whose faults we see, but love her with them all."
A View of the Present State of Ireland.
In his work "A Veue of the present state of Irelande" (1596), Spenser discussed future plans to subjugate Ireland, the most recent rising, led by Hugh O'Neill, having demonstrated the futility of previous efforts. The work is partly a defence of Lord Arthur Grey de Wilton, who was appointed Lord Deputy of Ireland in 1580, and who greatly influenced Spenser's thinking on Ireland.
The goal of this piece was to show that Ireland was in great need of reform. Spenser believed that "Ireland is a diseased portion of the State, it must first be cured and reformed, before it could be in a position to appreciate the good sound laws and blessings of the nation". In "A View of the Present State of Ireland", Spenser categorises the “evils” of the Irish people into three prominent categories: laws, customs, and religion. These three elements work together in creating the disruptive and degraded people. One example given in the work is the native law system called "Brehon Law" which trumps the established law given by the English monarchy. This system has its own court and way of dealing with infractions. It has been passed down through the generations and Spenser views this system as a native backward custom which must be destroyed. (Brehon Law methods of dealing with murder by imposing an "éraic", or fine, on the murderer's whole family particularly horrified the English, in whose Protestant view a murderer should die for his act.)
Spenser wished devoutly that the Irish language should be eradicated, writing that if children learn Irish before English, "Soe that the speach being Irish, the hart must needes be Irishe; for out of the aboundance of the hart, the tonge speaketh".
He pressed for a scorched earth policy in Ireland, noting that the destruction of crops and animals had been successful in crushing the Second Desmond Rebellion (1579–83), when, despite the rich and bountiful land:
"'Out of everye corner of the woode and glenns they came creepinge forth upon theire handes, for theire legges could not beare them; they looked Anatomies death, they spake like ghostes, crying out of theire graves; they did eate of the carrions, happye wheare they could find them, yea, and one another soone after, in soe much as the verye carcasses they spared not to scrape out of theire graves; and if they found a plott of water-cresses or shamrockes, theyr they flocked as to a feast… in a shorte space there were none almost left, and a most populous and plentyfull countrye suddenly lefte voyde of man or beast: yett sure in all that warr, there perished not manye by the sworde, but all by the extreamytie of famine ... they themselves had wrought'"
1590:
1591:
1592:
1595:
1596:
Posthumous:
|- style="text-align:center;" 

</doc>
<doc id="9540" url="https://en.wikipedia.org/wiki?curid=9540" title="Electricity generation">
Electricity generation

Electricity generation is the process of generating electric power from other sources of primary energy.
The fundamental principles of electricity generation were discovered during the 1820s and early 1830s by the British scientist Michael Faraday. His basic method is still used today: electricity is generated by the movement of a loop of wire, or disc of copper between the poles of a magnet.
For electric utilities, it is the first process in the delivery of electricity to consumers. The other processes, electricity transmission, distribution, and electrical power storage and recovery using pumped-storage methods are normally carried out by the electric power industry.
Electricity is most often generated at a power station by electromechanical generators, primarily driven by heat engines fueled by chemical combustion or nuclear fission but also by other means such as the kinetic energy of flowing water and wind. Other energy sources include solar photovoltaics and geothermal power and electrochemical batteries.
History.
Central power stations became economically practical with the development of alternating current power transmission, using power transformers to transmit power at high voltage and with low loss. Electricity has been generated at central stations since 1882. The first power plants were run on water power or coal, and today, rely mainly on coal, nuclear, natural gas, hydroelectric, wind generators, and petroleum, with a small amount from solar energy, tidal power, and geothermal sources.
The use of power-lines and power-poles have been significantly important in the distribution of electricity.
Methods of generating electricity.
There are seven fundamental methods of directly transforming other forms of energy into electrical energy: 
Static electricity was the first form discovered and investigated, and the electrostatic generator is still used even in modern devices such as the Van de Graaff generator and MHD generators. Charge carriers are separated and physically transported to a position of increased electric potential.
Almost all commercial electrical generation is done using electromagnetic induction, in which mechanical energy forces an electrical generator to rotate. There are many different methods of developing the mechanical energy, including heat engines, hydro, wind and tidal power.
The direct conversion of nuclear potential energy to electricity by beta decay is used only on a small scale. In a full-size nuclear power plant, the heat of a nuclear reaction is used to run a heat engine. This drives a generator, which converts mechanical energy into electricity by magnetic induction.
Most electric generation is driven by heat engines. The combustion of fossil fuels
supplies most of the heat to these engines, with a significant fraction from nuclear fission and some from renewable sources. The modern steam turbine (invented by Sir Charles Parsons in 1884) currently generates about 80% of the electric power in the world using a variety of heat sources.
Turbines.
Almost all electrical power on Earth is generated with a turbine of some type. Turbines are commonly driven by wind, water, steam or burning gas. The turbine drives an electric generator.
Power sources include:
Reciprocating engines.
Small electricity generators are often powered by reciprocating engines burning diesel, biogas or natural
gas. Diesel engines are often used for back up generation, usually at low voltages. However most large power grids also use diesel generators, originally provided as emergency back up for a specific facility such as a hospital, to feed power into the grid during certain circumstances. Biogas is often combusted where it is produced, such as a
landfill or wastewater treatment plant, with a reciprocating engine or a microturbine, which is a small gas turbine.
Photovoltaic panels.
Unlike the solar heat concentrators mentioned above, photovoltaic panels convert sunlight directly to electricity. Although sunlight is free and abundant, solar electricity is still usually more expensive to produce than large-scale mechanically generated power due to the cost of the panels. Low-efficiency silicon solar cells have been decreasing in cost and multijunction cells with close to 30% conversion efficiency are now commercially available. Over 40% efficiency has been demonstrated in experimental systems. Until recently, photovoltaics were most commonly used in remote sites where there is no access to a commercial power grid, or as a supplemental electricity source for individual homes and businesses. Recent advances in manufacturing efficiency and photovoltaic technology, combined with subsidies driven by environmental concerns, have dramatically accelerated the deployment of solar panels. Installed capacity is growing by 40% per year led by increases in Germany, Japan, and the United States.
Electrochemical.
Electrochemical electricity generation is important in portable and mobile applications. Currently, most electrochemical power comes from closed electrochemical cells ("batteries").
Primary cells, such as the common zinc-carbon batteries, act as power sources directly, but many types of cells are used as storage systems rather than primary generation systems.
Open electrochemical systems, known as fuel cells, have been undergoing a great deal of research and development in the last few years. Fuel cells can be used to extract power either from natural fuels or from synthesized fuels (mainly electrolytic hydrogen) and so can be viewed as either generation systems or storage systems depending on their use.
Other generation methods.
Various other technologies have been studied and developed for power generation.
Solid-state generation (without moving parts) is of particular interest in portable applications. This area is largely dominated by thermoelectric (TE) devices, though thermionic (TI) and thermophotovoltaic (TPV) systems have been developed as well. Typically, TE devices are used at lower temperatures than TI and TPV systems.
Piezoelectric devices are used for power generation from mechanical strain, particularly in power harvesting.
Betavoltaics are another type of solid-state power generator which produces electricity from radioactive decay.
Fluid-based magnetohydrodynamic (MHD) power generation has been studied as a method for extracting electrical power from nuclear reactors and also from more conventional fuel combustion systems. Osmotic power finally is another possibility at places where salt and fresh water merges (e.g. deltas, ...).
The Perth Wave Energy Project is an early production, submerged buoy, electrical power and direct desalination installation supplying power to HMAS Stirling in Western Australia.
Economics of generation and production of electricity.
The selection of electricity production modes and their economic viability varies in accordance with demand and region. The economics vary considerably around the world, resulting in widespread selling prices, e.g. the price in Venezuela is 3 cents per kWh while in Denmark it is 40 cents per kWh. Hydroelectric plants, nuclear power plants, thermal power plants and renewable sources have their own pros and cons, and selection is based upon the local power requirement and the fluctuations in demand. All power grids have varying loads on them but the daily minimum is the base load, supplied by plants which run continuously. Nuclear, coal, oil and gas plants can supply base load.
Thermal energy is economical in areas of high industrial density, as the high demand cannot be met by renewable sources. The effect of localized pollution is also minimized as industries are usually located away from residential areas. These plants can also withstand variation in load and consumption by adding more units or temporarily decreasing the production of some units.
Nuclear power plants can produce a huge amount of power from a single unit. However, recent disasters in Japan have raised concerns over the safety of nuclear power, and the capital cost of nuclear plants is very high. 
Hydroelectric power plants are located in areas where the potential energy from falling water can be harnessed for moving turbines and the generation of power. It is not an economically viable source of production where the load varies too much during the annual production cycle and the ability to store the flow of water is limited.
Due to advancements in technology, and with mass production, renewable sources other than hydroelectricity (solar power, wind energy, tidal power, etc.) experienced decreases in cost of production, and the energy is now in many cases cost-comparative with fossil fuels. Many governments around the world provide subsidies to offset the higher cost of any new power production, and to make the installation of renewable energy systems economically feasible. However, their use is frequently limited by their intermittent nature.
If natural gas prices are below $3 per million British thermal units, generating electricity from natural gas is cheaper than generating power by burning coal.
Production.
The production of electricity in 2009 was 20,053TWh. Sources of electricity were fossil fuels 67%, renewable energy 16% (mainly hydroelectric, wind, solar and biomass), and nuclear power 13%, and other sources were 3%. The majority of fossil fuel usage for the generation of electricity was coal and gas. Oil was 5.5%, as it is the most expensive common commodity used to produce electrical energy. Ninety-two percent of renewable energy was hydroelectric followed by wind at 6% and geothermal at 1.8%. Solar photovoltaic was 0.06%, and solar thermal was 0.004%. Data are from OECD 2011-12 Factbook (2009 data).
Total energy consumed at all power plants for the generation of electricity was 4,398,768 ktoe (kilo ton of oil equivalent) which was 36% of the total for primary energy sources (TPES) of 2008. <br> 
Electricity output (gross) was 1,735,579 ktoe (20,185 TWh), efficiency was 39%, and the balance of 61% was generated heat. A small part (145,141 ktoe, which was 3% of the input total) of the heat was utilized at co-generation heat and power plants. The in-house consumption of electricity and power transmission losses were 289,681 ktoe.
The amount supplied to the final consumer was 1,445,285 ktoe (16,430 TWh) which was 33% of the total energy consumed at power plants and heat and power co-generation (CHP) plants.
Production by country.
The United States has long been the largest producer and consumer of electricity, with a global share in 2005 of at least 25%, followed by China, Japan, Russia, and India.
As of Jan-2010, total electricity generation for the 2 largest generators was as follows: USA: 3992 billion kWh (3992 TWh) and China: 3715 billion kWh (3715 TWh).
List of countries with source of electricity 2008.
Data source of values (electric power generated) is IEA/OECD.
Listed countries are top 20 by population or top 20 by GDP (PPP) and Saudi Arabia based on CIA World Factbook 2009.
Solar PV* is Photovoltaics
Bio other* = 198TWh (Biomass) + 69TWh (Waste) + 4TWh (other)
Cogeneration.
Co-generation is the practice of using exhaust or extracted steam from a turbine for heating purposes, such as drying paper, distilling petroleum in a refinery or for building heat. Before central power stations were widely introduced it was common for industries, large hotels and commercial buildings to generate their own power and use low pressure exhaust steam for heating. This practice carried on for many years after central stations became common and is still in use in many industries.
Environmental concerns.
Variations between countries generating electrical power affect concerns about the environment. In France only 10% of electricity is generated from fossil fuels, the US is higher at 70% and China is at 80%. The cleanliness of electricity depends on its source. Most scientists agree that emissions of pollutants and greenhouse gases from fossil fuel-based electricity generation account for a significant portion of world greenhouse gas emissions; in the United States, electricity generation accounts for nearly 40% of emissions, the largest of any source. Transportation emissions are close behind, contributing about one-third of U.S. production of carbon dioxide.
In the United States, fossil fuel combustion for electric power generation is responsible for 65% of all emissions of sulfur dioxide, the main component of acid rain. Electricity generation is the fourth highest combined source of NOx, carbon monoxide, and particulate matter in the US.
In July 2011, the UK parliament tabled a motion that "levels of (carbon) emissions from nuclear power were approximately three times lower per kilowatt hour than those of solar, four times lower than clean coal and 36 times lower than conventional coal".
Water consumption.
Most large scale thermoelectric power stations consume considerable amounts of water for cooling purposes and boiler water make up - 1 L/kWh for once through (e.g. river cooling), and 1.7 L/kWh for cooling tower cooling. Water abstraction for cooling water accounts for about 40% of European total water abstraction, although most of this water is returned to its source, albeit slightly warmer. Different cooling systems have different consumption vs. abstraction characteristics. Cooling towers withdraw a small amount of water from the environment and evaporate most of it. Once-through systems withdraw a large amount but return it to the environment immediately, at a higher temperature.

</doc>
<doc id="9541" url="https://en.wikipedia.org/wiki?curid=9541" title="Design of experiments">
Design of experiments

The design of experiments (DOE, DOX, or experimental design) is the design of any task that aims to describe or explain the variation of information under conditions that are hypothesized to reflect the variation. The term is generally associated with true experiments in which the design introduces conditions that directly affect the variation, but may also refer to the design of quasi-experiments, in which natural conditions that influence the variation are selected for observation.
In its simplest form, an experiment aims at predicting the outcome by introducing a change of the preconditions, which is reflected in a variable called the predictor. The change in the predictor is generally hypothesized to result in a change in the second variable, hence called the outcome variable. Experimental design involves not only the selection of suitable predictors and outcomes, but planning the delivery of the experiment under statistically optimal conditions given the constraints of available resources.
Main concerns in experimental design include the establishment of validity, reliability, and replicability. For example, these concerns can be partially addressed by carefully choosing the predictor, reducing the risk of measurement error, and ensuring that the documentation of the method is sufficiently detailed. Related concerns include achieving appropriate levels of statistical power and sensitivity.
Correctly designed experiments advance knowledge in the natural and social sciences and engineering. Other applications include marketing and policy making.
History.
Systematic clinical trials.
In 1747, while serving as surgeon on HMS "Salisbury", James Lind carried out a systematic clinical trial to compare remedies for scurvy. This systematic clinical trial constitutes a type of DOE.
Lind selected 12 men from the ship, all suffering from scurvy. Lind limited his subjects to men who "were as similar as I could have them", that is he provided strict entry requirements to reduce extraneous variation. He divided them into six pairs, giving each pair different supplements to their basic diet for two weeks. The treatments were all remedies that had been proposed:
The citrus treatment stopped after six days when they ran out of fruit, but by that time one sailor was fit for duty while the other had almost recovered. Apart from that, only group one (cider) showed some effect of its treatment. The remainder of the crew presumably served as a control, but Lind did not report results from any control (untreated) group.
Statistical experiments, following Charles S. Peirce.
A theory of statistical inference was developed by Charles S. Peirce in "Illustrations of the Logic of Science" (1877–1878) and "A Theory of Probable Inference" (1883), two publications that emphasized the importance of randomization-based inference in statistics.
Randomized experiments.
Charles S. Peirce randomly assigned volunteers to a blinded, repeated-measures design to evaluate their ability to discriminate weights.
Peirce's experiment inspired other researchers in psychology and education, which developed a research tradition of randomized experiments in laboratories and specialized textbooks in the 1800s.
Optimal designs for regression models.
Charles S. Peirce also contributed the first English-language publication on an optimal design for regression models in 1876. A pioneering optimal design for polynomial regression was suggested by Gergonne in 1815. In 1918 Kirstine Smith published optimal designs for polynomials of degree six (and less).
Sequences of experiments.
The use of a sequence of experiments, where the design of each may depend on the results of previous experiments, including the possible decision to stop experimenting, is within the scope of Sequential analysis, a field that was pioneered by Abraham Wald in the context of sequential tests of statistical hypotheses. Herman Chernoff wrote an overview of optimal sequential designs, while adaptive designs have been surveyed by S. Zacks. One specific type of sequential design is the "two-armed bandit", generalized to the multi-armed bandit, on which early work was done by Herbert Robbins in 1952.
Fisher's principles.
A methodology for designing experiments was proposed by Ronald Fisher, in his innovative books: "The Arrangement of Field Experiments" (1926) and "The Design of Experiments" (1935). Much of his pioneering work dealt with agricultural applications of statistical methods. As a mundane example, he described how to test the lady tasting tea hypothesis, that a certain lady could distinguish by flavour alone whether the milk or the tea was first placed in the cup. These methods have been broadly adapted in the physical and social sciences, are still used in agricultural engineering and differ from the design and analysis of computer experiments.
Example.
This example is attributed to Harold Hotelling. It conveys some of the flavor of those aspects of the subject that involve combinatorial designs.
Weights of eight objects are measured using a pan balance and set of standard weights. Each weighing measures the weight difference between objects in the left pan vs. any objects in the right pan by adding calibrated weights to the lighter pan until the balance is in equilibrium. Each measurement has a random error. The average error is zero; the standard deviations of the probability distribution of the errors is the same number σ on different weighings; and errors on different weighings are independent. Denote the true weights by
We consider two different experiments:
The question of design of experiments is: which experiment is better?
The variance of the estimate "X" of θ is σ if we use the first experiment. But if we use the second experiment, the variance of the estimate given above is σ/8. Thus the second experiment gives us 8 times as much precision for the estimate of a single item, and estimates all items simultaneously, with the same precision. What the second experiment achieves with eight would require 64 weighings if the items are weighed separately. However, note that the estimates for the items obtained in the second experiment have errors that correlate with each other.
Many problems of the design of experiments involve combinatorial designs, as in this example and others.
Avoiding false positives.
False positive conclusions, often resulting from the pressure to publish or the author's own confirmation bias, are an inherent hazard in many fields. A good way to prevent biases potentially leading to false positives in the data collection phase is to use a double-blind design. When a double-blind design is used, participants are randomly assigned to experimental groups but the researcher is unaware of what participants belong to which group. Therefore, the researcher can not affect the participants' response to the intervention. 
Experimental designs with undisclosed degrees of freedom are a problem. This can lead to conscious or unconscious "p-hacking": trying multiple things until you get the desired result. It typically involves the manipulation - perhaps unconsciously - of the process of statistical analysis and the degrees of freedom until they return a figure below the p<.05 level of statistical significance. So the design of the experiment should include a clear statement proposing the analyses to be undertaken. P-hacking can be prevented by preregistering researches, in which researchers have to send their data analysis plan to the journal they wish to publish their paper in before they even start their data collection, so no data mining is possible (https://osf.io). Another way to prevent this is taking the double-blind design to the data-analysis phase, where the data are sent to a data-analyst unrelated to the research who scrambles up the data so there is no way to know which participants belong to before they are potentially taken away as outliers.
Clear and complete documentation of the experimental methodology is also important in order to support replication of results.
Discussion topics when setting up an experimental design.
An experimental design or randomized clinical trial requires careful consideration of several factors before actually doing the experiment. An experimental design is the laying out of a detailed experimental plan in advance of doing the experiment. Some of the following topics have already been discussed in the principles of experimental design section:
The independent variable of a study often has many levels or different groups. In a true experiment, researchers can have an experimental group, which is where their intervention testing the hypothesis is implemented, and a control group, which has all the same element as the experimental group, without the interventional element. Thus, when everything else except for one intervention is held constant, researchers can certify with some certainty that this one element is what caused the observed change. In some instances, having a control group is not ethical. This is sometimes solved using two different experimental groups. In some cases, independent variables are not manipulable, for example when testing the difference between two groups who have a different disease, or testing the difference between genders (obviously variables that would be hard or unethical to assign participants to). In these cases, a quasi-experimental design may be used.
Causal attributions.
In the pure experimental design, the independent (predictor) variable is manipulated by the researcher - that is - every participant of the research is chosen randomly from the population, and each participant chosen is assigned randomly to conditions of the independent variable. Only when this is done is it possible to certify with high probability that the reason for the differences in the outcome variables are caused by the different conditions. Therefore, researchers should choose the experimental design over other design types whenever possible. However, the nature of the independent variable does not always allow for manipulation. In those cases, researchers must be aware of not certifying about causal attribution when their design doesn't allow for it. For example, in observational designs, participants are not assigned randomly to conditions, and so if there are differences found in outcome variables between conditions, it is likely that there is something other than the differences between the conditions that causes the differences in outcomes, that is - a third variable. The same goes for studies with correlational design. (Adér & Mellenbergh, 2008).
Statistical control.
It is best that a process be in reasonable statistical control prior to conducting designed experiments. When this is not possible, proper blocking, replication, and randomization allow for the careful conduct of designed experiments.
To control for nuisance variables, researchers institute control checks as additional measures. Investigators should ensure that uncontrolled influences (e.g., source credibility perception) do not skew the findings of the study. A manipulation check is one example of a control check. Manipulation checks allow investigators to isolate the chief variables to strengthen support that these variables are operating as planned.
One of the most important requirements of experimental research designs is the necessity of eliminating the effects of spurious, intervening, and antecedent variables. In the most basic model, cause (X) leads to effect (Y). But there could be a third variable (Z) that influences (Y), and X might not be the true cause at all. Z is said to be a spurious variable and must be controlled for. The same is true for intervening variables (a variable in between the supposed cause (X) and the effect (Y)), and anteceding variables (a variable prior to the supposed cause (X) that is the true cause). When a third variable is involved and has not been controlled for, the relation is said to be a zero order relationship. In most practical applications of experimental research designs there are several causes (X1, X2, X3). In most designs, only one of these causes is manipulated at a time.
Experimental designs after Fisher.
Some efficient designs for estimating several main effects were found independently and in near succession by Raj Chandra Bose and K. Kishen in 1940 at the Indian Statistical Institute, but remained little known until the Plackett-Burman designs were published in "Biometrika" in 1946. About the same time, C. R. Rao introduced the concepts of orthogonal arrays as experimental designs. This concept played a central role in the development of Taguchi methods by Genichi Taguchi, which took place during his visit to Indian Statistical Institute in early 1950s. His methods were successfully applied and adopted by Japanese and Indian industries and subsequently were also embraced by US industry albeit with some reservations.
In 1950, Gertrude Mary Cox and William Gemmell Cochran published the book "Experimental Designs," which became the major reference work on the design of experiments for statisticians for years afterwards.
Developments of the theory of linear models have encompassed and surpassed the cases that concerned early writers. Today, the theory rests on advanced topics in linear algebra, algebra and combinatorics.
As with other branches of statistics, experimental design is pursued using both frequentist and Bayesian approaches: In evaluating statistical procedures like experimental designs, frequentist statistics studies the sampling distribution while Bayesian statistics updates a probability distribution on the parameter space.
Some important contributors to the field of experimental designs are C. S. Peirce, R. A. Fisher, F. Yates, C. R. Rao, R. C. Bose, J. N. Srivastava, Shrikhande S. S., D. Raghavarao, W. G. Cochran, O. Kempthorne, W. T. Federer, V. V. Fedorov, A. S. Hedayat, J. A. Nelder, R. A. Bailey, J. Kiefer, W. J. Studden, A. Pázman, F. Pukelsheim, D. R. Cox, H. P. Wynn, A. C. Atkinson, G. E. P. Box and G. Taguchi. The textbooks of D. Montgomery and R. Myers have reached generations of students and practitioners.
Human participant experimental design constraints.
Laws and ethical considerations preclude some carefully designed 
experiments with human subjects. Legal constraints are dependent on 
jurisdiction. Constraints may involve 
institutional review boards, informed consent 
and confidentiality affecting both clinical (medical) trials and 
behavioral and social science experiments.
In the field of toxicology, for example, experimentation is performed 
on laboratory "animals" with the goal of defining safe exposure limits 
for "humans". Balancing
the constraints are views from the medical field. Regarding the randomization of patients, 
"... if no one knows which therapy is better, there is no ethical 
imperative to use one therapy or another." (p 380) Regarding 
experimental design, "...it is clearly not ethical to place subjects 
at risk to collect data in a poorly designed study when this situation 
can be easily avoided...". (p 393)

</doc>
<doc id="9545" url="https://en.wikipedia.org/wiki?curid=9545" title="Empirical research">
Empirical research

Empirical research is research using empirical evidence. It is a way of gaining knowledge by means of direct and indirect observation or experience. Empiricism values such research more than other kinds. Empirical evidence (the record of one's direct observations or experiences) can be analyzed quantitatively or qualitatively. Through quantifying the evidence or making sense of it in qualitative form, a researcher can answer empirical questions, which should be clearly defined and answerable with the evidence collected (usually called data). Research design varies by field and by the question being investigated. Many researchers combine qualitative and quantitative forms of analysis to better answer questions which cannot be studied in laboratory settings, particularly in the social sciences and in education.
In some fields, quantitative research may begin with a research question (e.g., "Does listening to vocal music during the learning of a word list have an effect on later memory for these words?") which is tested through experimentation. Usually, a researcher has a certain theory regarding the topic under investigation. Based on this theory some statements, or hypotheses, will be proposed (e.g., "Listening to vocal music has a negative effect on learning a word list."). From these hypotheses predictions about specific events are derived (e.g., "People who study a word list while listening to vocal music will remember fewer words on a later memory test than people who study a word list in silence."). These predictions can then be tested with a suitable experiment. Depending on the outcomes of the experiment, the theory on which the hypotheses and predictions were based will be supported or not, or may need to be modified and then subjected to further testing.
Terminology.
The term empirical was originally used to refer to certain ancient Greek practitioners of medicine who rejected adherence to the dogmatic doctrines of the day, preferring instead to rely on the observation of phenomena as perceived in experience. Later empiricism referred to a theory of knowledge in philosophy which adheres to the principle that knowledge arises from experience and evidence gathered specifically using the senses. In scientific use the term empirical refers to the gathering of data using only evidence that is observable by the senses or in some cases using calibrated scientific instruments. What early philosophers described as empiricist and empirical research have in common is the dependence on observable data to formulate and test theories and come to conclusions.
Usage.
The researcher attempts to describe accurately the interaction between the instrument (or the human senses) and the entity being observed. If instrumentation is involved, the researcher is expected to calibrate his/her instrument by applying it to known standard objects and documenting the results before applying it to unknown objects. In other words, it describes the research that has not taken place before and their results. 
In practice, the accumulation of evidence for or against any particular theory involves planned research designs for the collection of empirical data, and academic rigor plays a large part of judging the merits of research design. Several typologies for such designs have been suggested, one of the most popular of which comes from Campbell and Stanley (1963). They are responsible for popularizing the widely cited distinction among pre-experimental, experimental, and quasi-experimental designs and are staunch advocates of the central role of randomized experiments in educational research.
Scientific research.
Accurate analysis of data using standardized statistical methods in scientific studies is critical to determining the validity of empirical research. Statistical formulas such as regression, uncertainty coefficient, t-test, chi square, and various types of ANOVA (analyses of variance) are fundamental to forming logical, valid conclusions. If empirical data reach significance under the appropriate statistical formula, the research hypothesis is supported. If not, the null hypothesis is supported (or, more accurately, not rejected), meaning no effect of the independent variable(s) was observed on the dependent variable(s).
It is important to understand that the outcome of empirical research using statistical hypothesis testing is never "proof". It can only "support" a hypothesis, "reject" it, or do neither. These methods yield only probabilities.
Among scientific researchers, empirical "evidence" (as distinct from empirical "research") refers to objective evidence that appears the same regardless of the observer. For example, a thermometer will not display different temperatures for each individual who observes it. Temperature, as measured by an accurate, well calibrated thermometer, is empirical evidence. By contrast, non-empirical evidence is subjective, depending on the observer. Following the previous example, observer A might truthfully report that a room is warm, while observer B might truthfully report that the same room is cool, though both observe the same reading on the thermometer. The use of empirical evidence negates this effect of personal (i.e., subjective) experience or time.
Empirical cycle.
A.D. de Groot's empirical cycle:

</doc>
<doc id="9546" url="https://en.wikipedia.org/wiki?curid=9546" title="Engineering statistics">
Engineering statistics

Engineering statistics combines engineering and statistics:

</doc>
<doc id="9549" url="https://en.wikipedia.org/wiki?curid=9549" title="Edgar Allan Poe">
Edgar Allan Poe

Edgar Allan Poe (; born Edgar Poe; January 19, 1809 – October 7, 1849) was an American writer, editor, and literary critic. Poe is best known for his poetry and short stories, particularly his tales of mystery and the macabre. Widely regarded as a central figure of Romanticism in the United States and American literature as a whole, he was one of the country's earliest practitioners of the short story. Poe is generally considered the inventor of the detective fiction genre and is further credited with contributing to the emerging genre of science fiction. He was the first well-known American writer to try to earn a living through writing alone, resulting in a financially difficult life and career.
Born in Boston, Poe was the second child of two actors. His father abandoned the family in 1810, and his mother died the following year. Thus orphaned, the child was taken in by John and Frances Allan, of Richmond, Virginia. Although they never formally adopted him, Poe was with them well into young adulthood. Tension developed later as John Allan and Edgar repeatedly clashed over debts, including those incurred by gambling, and the cost of secondary education for the young man. Poe attended the University of Virginia for one semester but left due to lack of money. Poe quarreled with Allan over the funds for his education and enlisted in the Army in 1827 under an assumed name. It was at this time his publishing career began, albeit humbly, with an anonymous collection of poems, "Tamerlane and Other Poems" (1827), credited only to "a Bostonian". With the death of Frances Allan in 1829, Poe and Allan reached a temporary rapprochement. Later failing as an officer's cadet at West Point and declaring a firm wish to be a poet and writer, Poe parted ways with John Allan.
Poe switched his focus to prose and spent the next several years working for literary journals and periodicals, becoming known for his own style of literary criticism. His work forced him to move among several cities, including Baltimore, Philadelphia, and New York City. In Baltimore in 1835, he married Virginia Clemm, his 13-year-old cousin. In January 1845 Poe published his poem, "The Raven", to instant success. His wife died of tuberculosis two years after its publication. For years, he had been planning to produce his own journal, "The Penn" (later renamed "The Stylus"), though he died before it could be produced. On October 7, 1849, at age 40, Poe died in Baltimore; the cause of his death is unknown and has been variously attributed to alcohol, brain congestion, cholera, drugs, heart disease, rabies, suicide, tuberculosis, and other agents.
Poe and his works influenced literature in the United States and around the world, as well as in specialized fields, such as cosmology and cryptography. Poe and his work appear throughout popular culture in literature, music, films, and television. A number of his homes are dedicated museums today. The Mystery Writers of America present an annual award known as the Edgar Award for distinguished work in the mystery genre.
Life and career.
Early life.
He was born Edgar Poe in Boston, on January 19, 1809, the second child of English-born actress Elizabeth Arnold Hopkins Poe and actor David Poe, Jr. He had an elder brother, William Henry Leonard Poe, and a younger sister, Rosalie Poe. Their grandfather, David Poe, Sr., had emigrated from Cavan, Ireland, to America around the year 1750. Edgar may have been named after a character in William Shakespeare's "King Lear", a play the couple was performing in 1809. His father abandoned their family in 1810, and his mother died a year later from consumption (pulmonary tuberculosis). Poe was then taken into the home of John Allan, a successful Scottish merchant in Richmond, Virginia, who dealt in a variety of goods including tobacco, cloth, wheat, tombstones, and slaves. The Allans served as a foster family and gave him the name "Edgar Allan Poe", though they never formally adopted him.
The Allan family had Poe baptized in the Episcopal Church in 1812. John Allan alternately spoiled and aggressively disciplined his foster son. The family, including Poe and Allan's wife, Frances Valentine Allan, sailed to Britain in 1815. Poe attended the grammar school in Irvine, Scotland (where John Allan was born) for a short period in 1815, before rejoining the family in London in 1816. There he studied at a boarding school in Chelsea until summer 1817. He was subsequently entered at the Reverend John Bransby's Manor House School at Stoke Newington, then a suburb north of London.
Poe moved back with the Allans to Richmond, Virginia in 1820. In 1824 Poe served as the lieutenant of the Richmond youth honor guard as Richmond celebrated the visit of the Marquis de Lafayette. In March 1825, John Allan's uncle and business benefactor William Galt, said to be one of the wealthiest men in Richmond, died and left Allan several acres of real estate. The inheritance was estimated at $750,000. By summer 1825, Allan celebrated his expansive wealth by purchasing a two-story brick home named Moldavia.
Poe may have become engaged to Sarah Elmira Royster before he registered at the one-year-old University of Virginia in February 1826 to study ancient and modern languages. The university, in its infancy, was established on the ideals of its founder, Thomas Jefferson. It had strict rules against gambling, horses, guns, tobacco and alcohol, but these rules were generally ignored. Jefferson had enacted a system of student self-government, allowing students to choose their own studies, make their own arrangements for boarding, and report all wrongdoing to the faculty. The unique system was still in chaos, and there was a high dropout rate. During his time there, Poe lost touch with Royster and also became estranged from his foster father over gambling debts. Poe claimed that Allan had not given him sufficient money to register for classes, purchase texts, and procure and furnish a dormitory. Allan did send additional money and clothes, but Poe's debts increased. Poe gave up on the university after a year, and, not feeling welcome in Richmond, especially when he learned that his sweetheart Royster had married Alexander Shelton, he traveled to Boston in April 1827, sustaining himself with odd jobs as a clerk and newspaper writer. At some point he started using the pseudonym Henri Le Rennet.
Military career.
Unable to support himself, on May 27, 1827, Poe enlisted in the United States Army as a private. Using the name "Edgar A. Perry", he claimed he was even though he was 18. He first served at Fort Independence in Boston Harbor for five dollars a month. That same year, he released his first book, a 40-page collection of poetry, "Tamerlane and Other Poems", attributed with the byline "by a Bostonian". Only 50 copies were printed, and the book received virtually no attention. Poe's regiment was posted to Fort Moultrie in Charleston, South Carolina and traveled by ship on the brig "Waltham" on November 8, 1827. Poe was promoted to "artificer", an enlisted tradesman who prepared shells for artillery, and had his monthly pay doubled. After serving for two years and attaining the rank of Sergeant Major for Artillery (the highest rank a noncommissioned officer can achieve), Poe sought to end his five-year enlistment early. He revealed his real name and his circumstances to his commanding officer, Lieutenant Howard. Howard would only allow Poe to be discharged if he reconciled with John Allan and wrote a letter to Allan, who was unsympathetic. Several months passed and pleas to Allan were ignored; Allan may not have written to Poe even to make him aware of his foster mother's illness. Frances Allan died on February 28, 1829, and Poe visited the day after her burial. Perhaps softened by his wife's death, John Allan agreed to support Poe's attempt to be discharged in order to receive an appointment to the United States Military Academy at West Point.
Poe finally was discharged on April 15, 1829, after securing a replacement to finish his enlisted term for him. Before entering West Point, Poe moved back to Baltimore for a time, to stay with his widowed aunt Maria Clemm, her daughter, Virginia Eliza Clemm (Poe's first cousin), his brother Henry, and his invalid grandmother Elizabeth Cairnes Poe. Meanwhile, Poe published his second book, "Al Aaraaf, Tamerlane and Minor Poems", in Baltimore in 1829.
Poe traveled to West Point and matriculated as a cadet on July 1, 1830. In October 1830, John Allan married his second wife, Louisa Patterson. The marriage, and bitter quarrels with Poe over the children born to Allan out of affairs, led to the foster father finally disowning Poe. Poe decided to leave West Point by purposely getting court-martialed. On February 8, 1831, he was tried for gross neglect of duty and disobedience of orders for refusing to attend formations, classes, or church. Poe tactically pled not guilty to induce dismissal, knowing he would be found guilty.
He left for New York in February 1831, and released a third volume of poems, simply titled "Poems." The book was financed with help from his fellow cadets at West Point, many of whom donated 75 cents to the cause, raising a total of $170. They may have been expecting verses similar to the satirical ones Poe had been writing about commanding officers. Printed by Elam Bliss of New York, it was labeled as "Second Edition" and included a page saying, "To the U.S. Corps of Cadets this volume is respectfully dedicated." The book once again reprinted the long poems "Tamerlane" and "Al Aaraaf" but also six previously unpublished poems including early versions of "To Helen", "Israfel", and "The City in the Sea". He returned to Baltimore, to his aunt, brother and cousin, in March 1831. His elder brother Henry, who had been in ill health in part due to problems with alcoholism, died on August 1, 1831.
Publishing career.
After his brother's death, Poe began more earnest attempts to start his career as a writer. He chose a difficult time in American publishing to do so. He was the first well-known American to try to live by writing alone and was hampered by the lack of an international copyright law. Publishers often produced unauthorized copies of British works rather than paying for new work by Americans. The industry was also particularly hurt by the Panic of 1837. Despite a booming growth in American periodicals around this time period, fueled in part by new technology, many did not last beyond a few issues and publishers often refused to pay their writers or paid them much later than they promised. Poe, throughout his attempts to live as a writer, repeatedly had to resort to humiliating pleas for money and other assistance.
After his early attempts at poetry, Poe had turned his attention to prose. He placed a few stories with a Philadelphia publication and began work on his only drama, "Politian". The "Baltimore Saturday Visiter" awarded Poe a prize in October 1833 for his short story "MS. Found in a Bottle". The story brought him to the attention of John P. Kennedy, a Baltimorean of considerable means. He helped Poe place some of his stories, and introduced him to Thomas W. White, editor of the "Southern Literary Messenger" in Richmond. Poe became assistant editor of the periodical in August 1835, but was discharged within a few weeks for having been caught drunk by his boss. Returning to Baltimore, Poe secretly married Virginia, his cousin, on September 22, 1835. He was 26 and she was 13, though she is listed on the marriage certificate as being 21. Reinstated by White after promising good behavior, Poe went back to Richmond with Virginia and her mother. He remained at the "Messenger" until January 1837. During this period, Poe claimed that its circulation increased from 700 to 3,500. He published several poems, book reviews, critiques, and stories in the paper. On May 16, 1836, he had a second wedding ceremony in Richmond with Virginia Clemm, this time in public.
"The Narrative of Arthur Gordon Pym of Nantucket" was published and widely reviewed in 1838. In the summer of 1839, Poe became assistant editor of "Burton's Gentleman's Magazine". He published numerous articles, stories, and reviews, enhancing his reputation as a trenchant critic that he had established at the "Southern Literary Messenger". Also in 1839, the collection "Tales of the Grotesque and Arabesque" was published in two volumes, though he made little money off of it and it received mixed reviews. Poe left "Burton's" after about a year and found a position as assistant at "Graham's Magazine".
In June 1840, Poe published a prospectus announcing his intentions to start his own journal, "The Stylus". Originally, Poe intended to call the journal "The Penn", as it would have been based in Philadelphia. In the June 6, 1840 issue of Philadelphia's "Saturday Evening Post", Poe bought advertising space for his prospectus: "Prospectus of the Penn Magazine, a Monthly Literary journal to be edited and published in the city of Philadelphia by Edgar A. Poe." The journal was never produced before Poe's death. Around this time, he attempted to secure a position with the Tyler administration, claiming he was a member of the Whig Party. He hoped to be appointed to the Custom House in Philadelphia with help from president Tyler's son Robert, an acquaintance of Poe's friend Frederick Thomas. Poe failed to show up for a meeting with Thomas to discuss the appointment in mid-September 1842, claiming to have been sick, though Thomas believed he had been drunk. Though he was promised an appointment, all positions were filled by others.
The "Broadway Journal" failed in 1846. Poe moved to a cottage in Fordham, New York, in what is now the Bronx. That home, known today as the "Poe Cottage", is on the southeast corner of the Grand Concourse and Kingsbridge Road, where he befriended the Jesuits at St. John's College nearby (now Fordham University). Virginia died there on January 30, 1847. Biographers and critics often suggest that Poe's frequent theme of the "death of a beautiful woman" stems from the repeated loss of women throughout his life, including his wife.
Increasingly unstable after his wife's death, Poe attempted to court the poet Sarah Helen Whitman, who lived in Providence, Rhode Island. Their engagement failed, purportedly because of Poe's drinking and erratic behavior. There is also strong evidence that Whitman's mother intervened and did much to derail their relationship. Poe then returned to Richmond and resumed a relationship with his childhood sweetheart, Sarah Elmira Royster.
Death.
On October 3, 1849, Poe was found on the streets of Baltimore delirious, "in great distress, and... in need of immediate assistance", according to the man who found him, Joseph W. Walker. He was taken to the Washington Medical College, where he died on Sunday, October 7, 1849, at 5:00 in the morning. Poe was never coherent long enough to explain how he came to be in his dire condition, and, oddly, was wearing clothes that were not his own. Poe is said to have repeatedly called out the name "Reynolds" on the night before his death, though it is unclear to whom he was referring. Some sources say Poe's final words were "Lord help my poor soul." All medical records, including his death certificate, have been lost. Newspapers at the time reported Poe's death as "congestion of the brain" or "cerebral inflammation", common euphemisms for deaths from disreputable causes such as alcoholism. The actual cause of death remains a mystery. Speculation has included "delirium tremens", heart disease, epilepsy, syphilis, meningeal inflammation, cholera and rabies. One theory, dating from 1872, indicates that cooping—in which unwilling citizens who were forced to vote for a particular candidate were occasionally killed—was the cause of Poe's death.
Griswold's "Memoir".
The day Edgar Allan Poe was buried, a long obituary appeared in the "New York Tribune" signed "Ludwig". It was soon published throughout the country. The piece began, "Edgar Allan Poe is dead. He died in Baltimore the day before yesterday. This announcement will startle many, but few will be grieved by it." "Ludwig" was soon identified as Rufus Wilmot Griswold, an editor, critic and anthologist who had borne a grudge against Poe since 1842. Griswold somehow became Poe's literary executor and attempted to destroy his enemy's reputation after his death.
Rufus Griswold wrote a biographical article of Poe called "Memoir of the Author", which he included in an 1850 volume of the collected works. Griswold depicted Poe as a depraved, drunk, drug-addled madman and included Poe's letters as evidence. Many of his claims were either lies or distorted half-truths. For example, it is now known that Poe was not a drug addict. Griswold's book was denounced by those who knew Poe well, but it became a popularly accepted one. This occurred in part because it was the only full biography available and was widely reprinted and in part because readers thrilled at the thought of reading works by an "evil" man. Letters that Griswold presented as proof of this depiction of Poe were later revealed as forgeries.
Literary style and themes.
Genres.
Poe's best known fiction works are Gothic, a genre he followed to appease the public taste. His most recurring themes deal with questions of death, including its physical signs, the effects of decomposition, concerns of premature burial, the reanimation of the dead, and mourning. Many of his works are generally considered part of the dark romanticism genre, a literary reaction to transcendentalism, which Poe strongly disliked. He referred to followers of the latter movement as "Frog-Pondians", after the pond on Boston Common. and ridiculed their writings as "metaphor—run mad," lapsing into "obscurity for obscurity's sake" or "mysticism for mysticism's sake". Poe once wrote in a letter to Thomas Holley Chivers that he did not dislike Transcendentalists, "only the pretenders and sophists among them".
Beyond horror, Poe also wrote satires, humor tales, and hoaxes. For comic effect, he used irony and ludicrous extravagance, often in an attempt to liberate the reader from cultural conformity. "Metzengerstein", the first story that Poe is known to have published, and his first foray into horror, was originally intended as a burlesque satirizing the popular genre. Poe also reinvented science fiction, responding in his writing to emerging technologies such as hot air balloons in "The Balloon-Hoax".
Poe wrote much of his work using themes aimed specifically at mass-market tastes. To that end, his fiction often included elements of popular pseudosciences such as phrenology and physiognomy.
Literary theory.
Poe's writing reflects his literary theories, which he presented in his criticism and also in essays such as "The Poetic Principle". He disliked didacticism and allegory, though he believed that meaning in literature should be an undercurrent just beneath the surface. Works with obvious meanings, he wrote, cease to be art. He believed that work of quality should be brief and focus on a specific single effect. To that end, he believed that the writer should carefully calculate every sentiment and idea.
In "The Philosophy of Composition", an essay in which Poe describes his method in writing "The Raven", he claims to have strictly followed this method. It has been questioned whether he really followed this system. T. S. Eliot said: "It is difficult for us to read that essay without reflecting that if Poe plotted out his poem with such calculation, he might have taken a little more pains over it: the result hardly does credit to the method." Biographer Joseph Wood Krutch described the essay as "a rather highly ingenious exercise in the art of rationalization".
Legacy.
Literary influence.
During his lifetime, Poe was mostly recognized as a literary critic. Fellow critic James Russell Lowell called him "the most discriminating, philosophical, and fearless critic upon imaginative works who has written in America", suggesting—rhetorically—that he occasionally used prussic acid instead of ink. Poe's caustic reviews earned him the reputation of being a "tomahawk man". A favorite target of Poe's criticism was Boston's then-acclaimed poet, Henry Wadsworth Longfellow, who was often defended by his literary friends in what was later called "The Longfellow War". Poe accused Longfellow of "the heresy of the didactic", writing poetry that was preachy, derivative, and thematically plagiarized. Poe correctly predicted that Longfellow's reputation and style of poetry would decline, concluding that "We grant him high qualities, but deny him the Future".
Poe was also known as a writer of fiction and became one of the first American authors of the 19th century to become more popular in Europe than in the United States. Poe is particularly respected in France, in part due to early translations by Charles Baudelaire. Baudelaire's translations became definitive renditions of Poe's work throughout Europe.
Poe's early detective fiction tales featuring C. Auguste Dupin laid the groundwork for future detectives in literature. Sir Arthur Conan Doyle said, "Each f Poe's detective storie is a root from which a whole literature has developed... Where was the detective story until Poe breathed the breath of life into it?" The Mystery Writers of America have named their awards for excellence in the genre the "Edgars". Poe's work also influenced science fiction, notably Jules Verne, who wrote a sequel to Poe's novel "The Narrative of Arthur Gordon Pym of Nantucket" called "An Antarctic Mystery", also known as "The Sphinx of the Ice Fields". Science fiction author H. G. Wells noted, ""Pym" tells what a very intelligent mind could imagine about the south polar region a century ago."
Like many famous artists, Poe's works have spawned imitators. One trend among imitators of Poe has been claims by clairvoyants or psychics to be "channeling" poems from Poe's spirit. One of the most notable of these was Lizzie Doten, who in 1863 published "Poems from the Inner Life", in which she claimed to have "received" new compositions by Poe's spirit. The compositions were re-workings of famous Poe poems such as "The Bells", but which reflected a new, positive outlook.
Even so, Poe has received not only praise, but criticism as well. This is partly because of the negative perception of his personal character and its influence upon his reputation. William Butler Yeats was occasionally critical of Poe and once called him "vulgar". Transcendentalist Ralph Waldo Emerson reacted to "The Raven" by saying, "I see nothing in it", and derisively referred to Poe as "the jingle man". Aldous Huxley wrote that Poe's writing "falls into vulgarity" by being "too poetical"—the equivalent of wearing a diamond ring on every finger.
It is believed that only 12 copies of Poe's first book, "Tamerlane and Other Poems", have survived. In December 2009, one copy sold at Christie's, New York for $662,500, a record price paid for a work of American literature.
Physics and cosmology.
"", an essay written in 1848, included a cosmological theory that presaged the Big Bang theory by 80 years, as well as the first plausible solution to Olbers' paradox.
Poe eschewed the scientific method in "Eureka" and instead wrote from pure intuition. For this reason, he considered it a work of art, not science, but insisted that it was still true and considered it to be his career masterpiece. Even so, "Eureka" is full of scientific errors. In particular, Poe's suggestions ignored Newtonian principles regarding the density and rotation of planets.
Cryptography.
Poe had a keen interest in cryptography. He had placed a notice of his abilities in the Philadelphia paper "Alexander's Weekly (Express) Messenger", inviting submissions of ciphers, which he proceeded to solve. In July 1841, Poe had published an essay called "A Few Words on Secret Writing" in "Graham's Magazine". Capitalizing on public interest in the topic, he wrote "The Gold-Bug" incorporating ciphers as an essential part of the story. Poe's success with cryptography relied not so much on his deep knowledge of that field (his method was limited to the simple substitution cryptogram), as on his knowledge of the magazine and newspaper culture. His keen analytical abilities, which were so evident in his detective stories, allowed him to see that the general public was largely ignorant of the methods by which a simple substitution cryptogram can be solved, and he used this to his advantage. The sensation Poe created with his cryptography stunts played a major role in popularizing cryptograms in newspapers and magazines.
Poe had an influence on cryptography beyond increasing public interest during his lifetime. William Friedman, America's foremost cryptologist, was heavily influenced by Poe. Friedman's initial interest in cryptography came from reading "The Gold-Bug" as a child, an interest he later put to use in deciphering Japan's PURPLE code during World War II.
In popular culture.
As a character.
The historical Edgar Allan Poe has appeared as a fictionalized character, often representing the "mad genius" or "tormented artist" and exploiting his personal struggles. Many such depictions also blend in with characters from his stories, suggesting Poe and his characters share identities. Often, fictional depictions of Poe use his mystery-solving skills in such novels as "The Poe Shadow" by Matthew Pearl.
Preserved homes, landmarks, and museums.
No childhood home of Poe is still standing, including the Allan family's Moldavia estate. The oldest standing home in Richmond, the Old Stone House, is in use as the Edgar Allan Poe Museum, though Poe never lived there. The collection includes many items Poe used during his time with the Allan family and also features several rare first printings of Poe works. 13 West Range, the dorm room Poe is believed to have used while studying at the University of Virginia in 1826, is preserved and available for visits. Its upkeep is now overseen by a group of students and staff known as the Raven Society.
The earliest surviving home in which Poe lived is in Baltimore, preserved as the Edgar Allan Poe House and Museum. Poe is believed to have lived in the home at the age of 23 when he first lived with Maria Clemm and Virginia (as well as his grandmother and possibly his brother William Henry Leonard Poe). It is open to the public and is also the home of the Edgar Allan Poe Society. Of the several homes that Poe, his wife Virginia, and his mother-in-law Maria rented in Philadelphia, only the last house has survived. The Spring Garden home, where the author lived in 1843–1844, is today preserved by the National Park Service as the Edgar Allan Poe National Historic Site. Poe's final home is preserved as the Edgar Allan Poe Cottage in the Bronx.
In Boston, a commemorative plaque on Boylston Street is several blocks away from the actual location of Poe's birth. The house which was his birthplace at 62 Carver Street no longer exists; also, the street has since been renamed "Charles Street South". A "square" at the intersection of Broadway, Fayette, and Carver Streets had once been named in his honor, but it disappeared when the streets were rearranged. In 2009, the intersection of Charles and Boylston Streets (two blocks north of his birthplace) was newly designated "Edgar Allan Poe Square". In March 2014, fundraising was completed for construction of a permanent memorial sculpture at this location. The winning design, by Stefanie Rocknak, depicts a life-sized Poe striding against the wind, accompanied by a flying raven, and trailed by papers falling from his open suitcase. The public unveiling on October 5, 2014 was attended by former US poet laureate Robert Pinsky.
Other Poe landmarks include a building in the Upper West Side, where Poe temporarily lived when he first moved to New York. A plaque suggests that Poe wrote "The Raven" here. The bar where legend says Poe was last seen drinking before his death still stands in Fells Point in Baltimore. The drinking establishment is now known as "The Horse You Came In On", and local lore insists that a ghost they call "Edgar" haunts the rooms above.
Poe Toaster.
Adding to the mystery surrounding Poe's death, an unknown visitor affectionately referred to as the "Poe Toaster" paid homage at Poe's grave annually beginning in 1949. As the tradition carried on for more than 60 years, it is likely that the "Poe Toaster" was actually more than one individual, though the tribute was always the same. Every January 19, in the early hours of the morning, the person made a toast of cognac to Poe's original grave marker and left three roses. Members of the Edgar Allan Poe Society in Baltimore helped protect this tradition for decades.
On August 15, 2007, Sam Porpora, a former historian at the Westminster Church in Baltimore where Poe is buried, claimed that he had started the tradition. Porpora said that the tradition began in 1949 in order to raise money and enhance the profile of the church. His story has not been confirmed, and some details he gave to the press have been pointed out as factually inaccurate. The Poe Toaster's last appearance was on January 19, 2009, the day of Poe's bicentennial.
Selected list of works.
Tales
Poetry
Other works

</doc>
<doc id="9550" url="https://en.wikipedia.org/wiki?curid=9550" title="Electricity">
Electricity

Electricity is the set of physical phenomena associated with the presence and flow of electric charge. Electricity gives a wide variety of well-known effects, such as lightning, static electricity, electromagnetic induction and electric current. In addition, electricity permits the creation and reception of electromagnetic radiation such as radio waves.
In electricity, charges produce electromagnetic fields which act on other charges. Electricity occurs due to several types of physics:
In electrical engineering, electricity is used for:
Electrical phenomena have been studied since antiquity, though progress in theoretical understanding remained slow until the seventeenth and eighteenth centuries. Even then, practical applications for electricity were few, and it would not be until the late nineteenth century that engineers were able to put it to industrial and residential use. The rapid expansion in electrical technology at this time transformed industry and society. Electricity's extraordinary versatility means it can be put to an almost limitless set of applications which include transport, heating, lighting, communications, and computation. Electrical power is now the backbone of modern industrial society.
History.
Long before any knowledge of electricity existed people were aware of shocks from electric fish. Ancient Egyptian texts dating from 2750 BC referred to these fish as the "Thunderer of the Nile", and described them as the "protectors" of all other fish. Electric fish were again reported millennia later by ancient Greek, Roman and Arabic naturalists and physicians. Several ancient writers, such as Pliny the Elder and Scribonius Largus, attested to the numbing effect of electric shocks delivered by catfish and electric rays, and knew that such shocks could travel along conducting objects. Patients suffering from ailments such as gout or headache were directed to touch electric fish in the hope that the powerful jolt might cure them. Possibly the earliest and nearest approach to the discovery of the identity of lightning, and electricity from any other source, is to be attributed to the Arabs, who before the 15th century had the Arabic word for lightning ("raad") applied to the electric ray.
Ancient cultures around the Mediterranean knew that certain objects, such as rods of amber, could be rubbed with cat's fur to attract light objects like feathers. Thales of Miletus made a series of observations on static electricity around 600 BC, from which he believed that friction rendered amber magnetic, in contrast to minerals such as magnetite, which needed no rubbing. Thales was incorrect in believing the attraction was due to a magnetic effect, but later science would prove a link between magnetism and electricity. According to a controversial theory, the Parthians may have had knowledge of electroplating, based on the 1936 discovery of the Baghdad Battery, which resembles a galvanic cell, though it is uncertain whether the artifact was electrical in nature.
Electricity would remain little more than an intellectual curiosity for millennia until 1600, when the English scientist William Gilbert made a careful study of electricity and magnetism, distinguishing the lodestone effect from static electricity produced by rubbing amber. He coined the New Latin word "electricus" ("of amber" or "like amber", from ἤλεκτρον, "elektron", the Greek word for "amber") to refer to the property of attracting small objects after being rubbed. This association gave rise to the English words "electric" and "electricity", which made their first appearance in print in Thomas Browne's "Pseudodoxia Epidemica" of 1646.
Further work was conducted by Otto von Guericke, Robert Boyle, Stephen Gray and C. F. du Fay. In the 18th century, Benjamin Franklin conducted extensive research in electricity, selling his possessions to fund his work. In June 1752 he is reputed to have attached a metal key to the bottom of a dampened kite string and flown the kite in a storm-threatened sky. A succession of sparks jumping from the key to the back of his hand showed that lightning was indeed electrical in nature. He also explained the apparently paradoxical behavior of the Leyden jar as a device for storing large amounts of electrical charge in terms of electricity consisting of both positive and negative charges.
In 1791, Luigi Galvani published his discovery of bioelectromagnetics, demonstrating that electricity was the medium by which neurons passed signals to the muscles. Alessandro Volta's battery, or voltaic pile, of 1800, made from alternating layers of zinc and copper, provided scientists with a more reliable source of electrical energy than the electrostatic machines previously used. The recognition of electromagnetism, the unity of electric and magnetic phenomena, is due to Hans Christian Ørsted and
André-Marie Ampère in 1819-1820; Michael Faraday invented the electric motor in 1821, and Georg Ohm mathematically analysed the electrical circuit in 1827. Electricity and magnetism (and light) were definitively linked by James Clerk Maxwell, in particular in his "On Physical Lines of Force" in 1861 and 1862.
While the early 19th century had seen rapid progress in electrical science, the late 19th century would see the greatest progress in electrical engineering. Through such people as Alexander Graham Bell, Ottó Bláthy, Thomas Edison, Galileo Ferraris, Oliver Heaviside, Ányos Jedlik, William Thomson, 1st Baron Kelvin, Charles Algernon Parsons, Werner von Siemens, Joseph Swan, Nikola Tesla and George Westinghouse, electricity turned from a scientific curiosity into an essential tool for modern life, becoming a driving force of the Second Industrial Revolution.
In 1887, Heinrich Hertz discovered that electrodes illuminated with ultraviolet light create electric sparks more easily. In 1905 Albert Einstein published a paper that explained experimental data from the photoelectric effect as being the result of light energy being carried in discrete quantized packets, energising electrons. This discovery led to the quantum revolution. Einstein was awarded the Nobel Prize in Physics in 1921 for "his discovery of the law of the photoelectric effect". The photoelectric effect is also employed in photocells such as can be found in solar panels and this is frequently used to make electricity commercially.
The first solid-state device was the "cat's-whisker detector" first used in the 1900s in radio receivers. A whisker-like wire is placed lightly in contact with a solid crystal (such as a germanium crystal) in order to detect a radio signal by the contact junction effect. In a solid-state component, the current is confined to solid elements and compounds engineered specifically to switch and amplify it. Current flow can be understood in two forms: as negatively charged electrons, and as positively charged electron deficiencies called holes. These charges and holes are understood in terms of quantum physics. The building material is most often a crystalline semiconductor.
The solid-state device came into its own with the invention of the transistor in 1947. Common solid-state devices include transistors, microprocessor chips, and RAM. A specialized type of RAM called flash RAM is used in USB flash drives and more recently, solid-state drives to replace mechanically rotating magnetic disc hard disk drives. Solid state devices became prevalent in the 1950s and the 1960s, during the transition from vacuum tubes to semiconductor diodes, transistors, integrated circuit (IC) and the light-emitting diode (LED).
Concepts.
Electric charge.
The presence of charge gives rise to an electrostatic force: charges exert a force on each other, an effect that was known, though not understood, in antiquity. A lightweight ball suspended from a string can be charged by touching it with a glass rod that has itself been charged by rubbing with a cloth. If a similar ball is charged by the same glass rod, it is found to repel the first: the charge acts to force the two balls apart. Two balls that are charged with a rubbed amber rod also repel each other. However, if one ball is charged by the glass rod, and the other by an amber rod, the two balls are found to attract each other. These phenomena were investigated in the late eighteenth century by Charles-Augustin de Coulomb, who deduced that charge manifests itself in two opposing forms. This discovery led to the well-known axiom: "like-charged objects repel and opposite-charged objects attract".
The force acts on the charged particles themselves, hence charge has a tendency to spread itself as evenly as possible over a conducting surface. The magnitude of the electromagnetic force, whether attractive or repulsive, is given by Coulomb's law, which relates the force to the product of the charges and has an inverse-square relation to the distance between them. The electromagnetic force is very strong, second only in strength to the strong interaction, but unlike that force it operates over all distances. In comparison with the much weaker gravitational force, the electromagnetic force pushing two electrons apart is 10 times that of the gravitational attraction pulling them together.
Study has shown that the origin of charge is from certain types of subatomic particles which have the property of electric charge. Electric charge gives rise to and interacts with the electromagnetic force, one of the four fundamental forces of nature. The most familiar carriers of electrical charge are the electron and proton. Experiment has shown charge to be a conserved quantity, that is, the net charge within an isolated system will always remain constant regardless of any changes taking place within that system. Within the system, charge may be transferred between bodies, either by direct contact, or by passing along a conducting material, such as a wire. The informal term static electricity refers to the net presence (or 'imbalance') of charge on a body, usually caused when dissimilar materials are rubbed together, transferring charge from one to the other.
The charge on electrons and protons is opposite in sign, hence an amount of charge may be expressed as being either negative or positive. By convention, the charge carried by electrons is deemed negative, and that by protons positive, a custom that originated with the work of Benjamin Franklin. The amount of charge is usually given the symbol "Q" and expressed in coulombs; each electron carries the same charge of approximately −1.6022×10 coulomb. The proton has a charge that is equal and opposite, and thus +1.6022×10  coulomb. Charge is possessed not just by matter, but also by antimatter, each antiparticle bearing an equal and opposite charge to its corresponding particle.
Charge can be measured by a number of means, an early instrument being the gold-leaf electroscope, which although still in use for classroom demonstrations, has been superseded by the electronic electrometer.
Electric current.
The movement of electric charge is known as an electric current, the intensity of which is usually measured in amperes. Current can consist of any moving charged particles; most commonly these are electrons, but any charge in motion constitutes a current.
By historical convention, a positive current is defined as having the same direction of flow as any positive charge it contains, or to flow from the most positive part of a circuit to the most negative part. Current defined in this manner is called conventional current. The motion of negatively charged electrons around an electric circuit, one of the most familiar forms of current, is thus deemed positive in the "opposite" direction to that of the electrons. However, depending on the conditions, an electric current can consist of a flow of charged particles in either direction, or even in both directions at once. The positive-to-negative convention is widely used to simplify this situation.
The process by which electric current passes through a material is termed electrical conduction, and its nature varies with that of the charged particles and the material through which they are travelling. Examples of electric currents include metallic conduction, where electrons flow through a conductor such as metal, and electrolysis, where ions (charged atoms) flow through liquids, or through plasmas such as electrical sparks. While the particles themselves can move quite slowly, sometimes with an average drift velocity only fractions of a millimetre per second, the electric field that drives them itself propagates at close to the speed of light, enabling electrical signals to pass rapidly along wires.
Current causes several observable effects, which historically were the means of recognising its presence. That water could be decomposed by the current from a voltaic pile was discovered by Nicholson and Carlisle in 1800, a process now known as electrolysis. Their work was greatly expanded upon by Michael Faraday in 1833. Current through a resistance causes localised heating, an effect James Prescott Joule studied mathematically in 1840. One of the most important discoveries relating to current was made accidentally by Hans Christian Ørsted in 1820, when, while preparing a lecture, he witnessed the current in a wire disturbing the needle of a magnetic compass. He had discovered electromagnetism, a fundamental interaction between electricity and magnetics. The level of electromagnetic emissions generated by electric arcing is high enough to produce electromagnetic interference, which can be detrimental to the workings of adjacent equipment.
In engineering or household applications, current is often described as being either direct current (DC) or alternating current (AC). These terms refer to how the current varies in time. Direct current, as produced by example from a battery and required by most electronic devices, is a unidirectional flow from the positive part of a circuit to the negative. If, as is most common, this flow is carried by electrons, they will be travelling in the opposite direction. Alternating current is any current that reverses direction repeatedly; almost always this takes the form of a sine wave. Alternating current thus pulses back and forth within a conductor without the charge moving any net distance over time. The time-averaged value of an alternating current is zero, but it delivers energy in first one direction, and then the reverse. Alternating current is affected by electrical properties that are not observed under steady state direct current, such as inductance and capacitance. These properties however can become important when circuitry is subjected to transients, such as when first energised.
Electric field.
The concept of the electric field was introduced by Michael Faraday. An electric field is created by a charged body in the space that surrounds it, and results in a force exerted on any other charges placed within the field. The electric field acts between two charges in a similar manner to the way that the gravitational field acts between two masses, and like it, extends towards infinity and shows an inverse square relationship with distance. However, there is an important difference. Gravity always acts in attraction, drawing two masses together, while the electric field can result in either attraction or repulsion. Since large bodies such as planets generally carry no net charge, the electric field at a distance is usually zero. Thus gravity is the dominant force at distance in the universe, despite being much weaker.
An electric field generally varies in space, and its strength at any one point is defined as the force (per unit charge) that would be felt by a stationary, negligible charge if placed at that point. The conceptual charge, termed a 'test charge', must be vanishingly small to prevent its own electric field disturbing the main field and must also be stationary to prevent the effect of magnetic fields. As the electric field is defined in terms of force, and force is a vector, so it follows that an electric field is also a vector, having both magnitude and direction. Specifically, it is a vector field.
The study of electric fields created by stationary charges is called electrostatics. The field may be visualised by a set of imaginary lines whose direction at any point is the same as that of the field. This concept was introduced by Faraday, whose term 'lines of force' still sometimes sees use. The field lines are the paths that a point positive charge would seek to make as it was forced to move within the field; they are however an imaginary concept with no physical existence, and the field permeates all the intervening space between the lines. Field lines emanating from stationary charges have several key properties: first, that they originate at positive charges and terminate at negative charges; second, that they must enter any good conductor at right angles, and third, that they may never cross nor close in on themselves.
A hollow conducting body carries all its charge on its outer surface. The field is therefore zero at all places inside the body. This is the operating principal of the Faraday cage, a conducting metal shell which isolates its interior from outside electrical effects.
The principles of electrostatics are important when designing items of high-voltage equipment. There is a finite limit to the electric field strength that may be withstood by any medium. Beyond this point, electrical breakdown occurs and an electric arc causes flashover between the charged parts. Air, for example, tends to arc across small gaps at electric field strengths which exceed 30 kV per centimetre. Over larger gaps, its breakdown strength is weaker, perhaps 1 kV per centimetre. The most visible natural occurrence of this is lightning, caused when charge becomes separated in the clouds by rising columns of air, and raises the electric field in the air to greater than it can withstand. The voltage of a large lightning cloud may be as high as 100 MV and have discharge energies as great as 250 kWh.
The field strength is greatly affected by nearby conducting objects, and it is particularly intense when it is forced to curve around sharply pointed objects. This principle is exploited in the lightning conductor, the sharp spike of which acts to encourage the lightning stroke to develop there, rather than to the building it serves to protect
Electric potential.
The concept of electric potential is closely linked to that of the electric field. A small charge placed within an electric field experiences a force, and to have brought that charge to that point against the force requires work. The electric potential at any point is defined as the energy required to bring a unit test charge from an infinite distance slowly to that point. It is usually measured in volts, and one volt is the potential for which one joule of work must be expended to bring a charge of one coulomb from infinity. This definition of potential, while formal, has little practical application, and a more useful concept is that of electric potential difference, and is the energy required to move a unit charge between two specified points. An electric field has the special property that it is "conservative", which means that the path taken by the test charge is irrelevant: all paths between two specified points expend the same energy, and thus a unique value for potential difference may be stated. The volt is so strongly identified as the unit of choice for measurement and description of electric potential difference that the term voltage sees greater everyday usage.
For practical purposes, it is useful to define a common reference point to which potentials may be expressed and compared. While this could be at infinity, a much more useful reference is the Earth itself, which is assumed to be at the same potential everywhere. This reference point naturally takes the name earth or ground. Earth is assumed to be an infinite source of equal amounts of positive and negative charge, and is therefore electrically uncharged—and unchargeable.
Electric potential is a scalar quantity, that is, it has only magnitude and not direction. It may be viewed as analogous to height: just as a released object will fall through a difference in heights caused by a gravitational field, so a charge will 'fall' across the voltage caused by an electric field. As relief maps show contour lines marking points of equal height, a set of lines marking points of equal potential (known as equipotentials) may be drawn around an electrostatically charged object. The equipotentials cross all lines of force at right angles. They must also lie parallel to a conductor's surface, otherwise this would produce a force that will move the charge carriers to even the potential of the surface.
The electric field was formally defined as the force exerted per unit charge, but the concept of potential allows for a more useful and equivalent definition: the electric field is the local gradient of the electric potential. Usually expressed in volts per metre, the vector direction of the field is the line of greatest slope of potential, and where the equipotentials lie closest together.
Electromagnets.
Ørsted's discovery in 1821 that a magnetic field existed around all sides of a wire carrying an electric current indicated that there was a direct relationship between electricity and magnetism. Moreover, the interaction seemed different from gravitational and electrostatic forces, the two forces of nature then known. The force on the compass needle did not direct it to or away from the current-carrying wire, but acted at right angles to it. Ørsted's slightly obscure words were that "the electric conflict acts in a revolving manner." The force also depended on the direction of the current, for if the flow was reversed, then the force did too.
Ørsted did not fully understand his discovery, but he observed the effect was reciprocal: a current exerts a force on a magnet, and a magnetic field exerts a force on a current. The phenomenon was further investigated by Ampère, who discovered that two parallel current-carrying wires exerted a force upon each other: two wires conducting currents in the same direction are attracted to each other, while wires containing currents in opposite directions are forced apart. The interaction is mediated by the magnetic field each current produces and forms the basis for the international definition of the ampere.
This relationship between magnetic fields and currents is extremely important, for it led to Michael Faraday's invention of the electric motor in 1821. Faraday's homopolar motor consisted of a permanent magnet sitting in a pool of mercury. A current was allowed through a wire suspended from a pivot above the magnet and dipped into the mercury. The magnet exerted a tangential force on the wire, making it circle around the magnet for as long as the current was maintained.
Experimentation by Faraday in 1831 revealed that a wire moving perpendicular to a magnetic field developed a potential difference between its ends. Further analysis of this process, known as electromagnetic induction, enabled him to state the principle, now known as Faraday's law of induction, that the potential difference induced in a closed circuit is proportional to the rate of change of magnetic flux through the loop. Exploitation of this discovery enabled him to invent the first electrical generator in 1831, in which he converted the mechanical energy of a rotating copper disc to electrical energy. Faraday's disc was inefficient and of no use as a practical generator, but it showed the possibility of generating electric power using magnetism, a possibility that would be taken up by those that followed on from his work.
Electrochemistry.
The ability of chemical reactions to produce electricity, and conversely the ability of electricity to drive chemical reactions has a wide array of uses.
Electrochemistry has always been an important part of electricity. From the initial invention of the Voltaic pile, electrochemical cells have evolved into the many different types of batteries, electroplating and electrolysis cells. Aluminium is produced in vast quantities this way, and many portable devices are electrically powered using rechargeable cells.
Electric circuits.
An electric circuit is an interconnection of electric components such that electric charge is made to flow along a closed path (a circuit), usually to perform some useful task.
The components in an electric circuit can take many forms, which can include elements such as resistors, capacitors, switches, transformers and electronics. Electronic circuits contain active components, usually semiconductors, and typically exhibit non-linear behaviour, requiring complex analysis. The simplest electric components are those that are termed passive and linear: while they may temporarily store energy, they contain no sources of it, and exhibit linear responses to stimuli.
The resistor is perhaps the simplest of passive circuit elements: as its name suggests, it resists the current through it, dissipating its energy as heat. The resistance is a consequence of the motion of charge through a conductor: in metals, for example, resistance is primarily due to collisions between electrons and ions. Ohm's law is a basic law of circuit theory, stating that the current passing through a resistance is directly proportional to the potential difference across it. The resistance of most materials is relatively constant over a range of temperatures and currents; materials under these conditions are known as 'ohmic'. The ohm, the unit of resistance, was named in honour of Georg Ohm, and is symbolised by the Greek letter Ω. 1 Ω is the resistance that will produce a potential difference of one volt in response to a current of one amp.
The capacitor is a development of the Leyden jar and is a device that can store charge, and thereby storing electrical energy in the resulting field. It consists of two conducting plates separated by a thin insulating dielectric layer; in practice, thin metal foils are coiled together, increasing the surface area per unit volume and therefore the capacitance. The unit of capacitance is the farad, named after Michael Faraday, and given the symbol "F": one farad is the capacitance that develops a potential difference of one volt when it stores a charge of one coulomb. A capacitor connected to a voltage supply initially causes a current as it accumulates charge; this current will however decay in time as the capacitor fills, eventually falling to zero. A capacitor will therefore not permit a steady state current, but instead blocks it.
The inductor is a conductor, usually a coil of wire, that stores energy in a magnetic field in response to the current through it. When the current changes, the magnetic field does too, inducing a voltage between the ends of the conductor. The induced voltage is proportional to the time rate of change of the current. The constant of proportionality is termed the inductance. The unit of inductance is the henry, named after Joseph Henry, a contemporary of Faraday. One henry is the inductance that will induce a potential difference of one volt if the current through it changes at a rate of one ampere per second. The inductor's behaviour is in some regards converse to that of the capacitor: it will freely allow an unchanging current, but opposes a rapidly changing one.
Electric power.
Electric power is the rate at which electric energy is transferred by an electric circuit. The SI unit of power is the watt, one joule per second.
Electric power, like mechanical power, is the rate of doing work, measured in watts, and represented by the letter "P". The term "wattage" is used colloquially to mean "electric power in watts." The electric power in watts produced by an electric current "I" consisting of a charge of "Q" coulombs every "t" seconds passing through an electric potential (voltage) difference of "V" is
where
Electricity generation is often done with electric generators, but can also be supplied by chemical sources such as electric batteries or by other means from a wide variety of sources of energy. Electric power is generally supplied to businesses and homes by the electric power industry. Electricity is usually sold by the kilowatt hour (3.6 MJ) which is the product of power in kilowatts multiplied by running time in hours. Electric utilities measure power using electricity meters, which keep a running total of the electric energy delivered to a customer. Unlike fossil fuels, electricity is a low entropy form of energy and can be converted into motion or many other forms of energy with high efficiency.
Electronics.
Electronics deals with electrical circuits that involve active electrical components such as vacuum tubes, transistors, diodes and integrated circuits, and associated passive interconnection technologies. The nonlinear behaviour of active components and their ability to control electron flows makes amplification of weak signals possible and electronics is widely used in information processing, telecommunications, and signal processing. The ability of electronic devices to act as switches makes digital information processing possible. Interconnection technologies such as circuit boards, electronics packaging technology, and other varied forms of communication infrastructure complete circuit functionality and transform the mixed components into a regular working system.
Today, most electronic devices use semiconductor components to perform electron control. The study of semiconductor devices and related technology is considered a branch of solid state physics, whereas the design and construction of electronic circuits to solve practical problems come under electronics engineering.
Electromagnetic wave.
Faraday's and Ampère's work showed that a time-varying magnetic field acted as a source of an electric field, and a time-varying electric field was a source of a magnetic field. Thus, when either field is changing in time, then a field of the other is necessarily induced. Such a phenomenon has the properties of a wave, and is naturally referred to as an electromagnetic wave. Electromagnetic waves were analysed theoretically by James Clerk Maxwell in 1864. Maxwell developed a set of equations that could unambiguously describe the interrelationship between electric field, magnetic field, electric charge, and electric current. He could moreover prove that such a wave would necessarily travel at the speed of light, and thus light itself was a form of electromagnetic radiation. Maxwell's Laws, which unify light, fields, and charge are one of the great milestones of theoretical physics.
Thus, the work of many researchers enabled the use of electronics to convert signals into high frequency oscillating currents, and via suitably shaped conductors, electricity permits the transmission and reception of these signals via radio waves over very long distances.
Production and uses.
Generation and transmission.
In the 6th century BC, the Greek philosopher Thales of Miletus experimented with amber rods and these experiments were the first studies into the production of electrical energy. While this method, now known as the triboelectric effect, can lift light objects and generate sparks, it is extremely inefficient. It was not until the invention of the voltaic pile in the eighteenth century that a viable source of electricity became available. The voltaic pile, and its modern descendant, the electrical battery, store energy chemically and make it available on demand in the form of electrical energy. The battery is a versatile and very common power source which is ideally suited to many applications, but its energy storage is finite, and once discharged it must be disposed of or recharged. For large electrical demands electrical energy must be generated and transmitted continuously over conductive transmission lines.
Electrical power is usually generated by electro-mechanical generators driven by steam produced from fossil fuel combustion, or the heat released from nuclear reactions; or from other sources such as kinetic energy extracted from wind or flowing water. The modern steam turbine invented by Sir Charles Parsons in 1884 today generates about 80 percent of the electric power in the world using a variety of heat sources. Such generators bear no resemblance to Faraday's homopolar disc generator of 1831, but they still rely on his electromagnetic principle that a conductor linking a changing magnetic field induces a potential difference across its ends. The invention in the late nineteenth century of the transformer meant that electrical power could be transmitted more efficiently at a higher voltage but lower current. Efficient electrical transmission meant in turn that electricity could be generated at centralised power stations, where it benefited from economies of scale, and then be despatched relatively long distances to where it was needed.
Since electrical energy cannot easily be stored in quantities large enough to meet demands on a national scale, at all times exactly as much must be produced as is required. This requires electricity utilities to make careful predictions of their electrical loads, and maintain constant co-ordination with their power stations. A certain amount of generation must always be held in reserve to cushion an electrical grid against inevitable disturbances and losses.
Demand for electricity grows with great rapidity as a nation modernises and its economy develops. The United States showed a 12% increase in demand during each year of the first three decades of the twentieth century, a rate of growth that is now being experienced by emerging economies such as those of India or China. Historically, the growth rate for electricity demand has outstripped that for other forms of energy.
Environmental concerns with electricity generation have led to an increased focus on generation from renewable sources, in particular from wind and hydropower. While debate can be expected to continue over the environmental impact of different means of electricity production, its final form is relatively clean
Applications.
Electricity is a very convenient way to transfer energy, and it has been adapted to a huge, and growing, number of uses. The invention of a practical incandescent light bulb in the 1870s led to lighting becoming one of the first publicly available applications of electrical power. Although electrification brought with it its own dangers, replacing the naked flames of gas lighting greatly reduced fire hazards within homes and factories. Public utilities were set up in many cities targeting the burgeoning market for electrical lighting.
The Joule heating effect employed in the light bulb also sees more direct use in electric heating. While this is versatile and controllable, it can be seen as wasteful, since most electrical generation has already required the production of heat at a power station. A number of countries, such as Denmark, have issued legislation restricting or banning the use of electric heating in new buildings. Electricity is however a highly practical energy source for refrigeration, with air conditioning representing a growing sector for electricity demand, the effects of which electricity utilities are increasingly obliged to accommodate.
Electricity is used within telecommunications, and indeed the electrical telegraph, demonstrated commercially in 1837 by Cooke and Wheatstone, was one of its earliest applications. With the construction of first intercontinental, and then transatlantic, telegraph systems in the 1860s, electricity had enabled communications in minutes across the globe. Optical fibre and satellite communication have taken a share of the market for communications systems, but electricity can be expected to remain an essential part of the process.
The effects of electromagnetism are most visibly employed in the electric motor, which provides a clean and efficient means of motive power. A stationary motor such as a winch is easily provided with a supply of power, but a motor that moves with its application, such as an electric vehicle, is obliged to either carry along a power source such as a battery, or to collect current from a sliding contact such as a pantograph.
Electronic devices make use of the transistor, perhaps one of the most important inventions of the twentieth century, and a fundamental building block of all modern circuitry. A modern integrated circuit may contain several billion miniaturised transistors in a region only a few centimetres square.
Electricity is also used to fuel public transportation, including electric buses and trains.
Electricity and the natural world.
Physiological effects.
A voltage applied to a human body causes an electric current through the tissues, and although the relationship is non-linear, the greater the voltage, the greater the current. The threshold for perception varies with the supply frequency and with the path of the current, but is about 0.1 mA to 1 mA for mains-frequency electricity, though a current as low as a microamp can be detected as an electrovibration effect under certain conditions. If the current is sufficiently high, it will cause muscle contraction, fibrillation of the heart, and tissue burns. The lack of any visible sign that a conductor is electrified makes electricity a particular hazard. The pain caused by an electric shock can be intense, leading electricity at times to be employed as a method of torture. Death caused by an electric shock is referred to as electrocution. Electrocution is still the means of judicial execution in some jurisdictions, though its use has become rarer in recent times.
Electrical phenomena in nature.
Electricity is not a human invention, and may be observed in several forms in nature, a prominent manifestation of which is lightning. Many interactions familiar at the macroscopic level, such as touch, friction or chemical bonding, are due to interactions between electric fields on the atomic scale. The Earth's magnetic field is thought to arise from a natural dynamo of circulating currents in the planet's core. Certain crystals, such as quartz, or even sugar, generate a potential difference across their faces when subjected to external pressure. This phenomenon is known as piezoelectricity, from the Greek "piezein" (πιέζειν), meaning to press, and was discovered in 1880 by Pierre and Jacques Curie. The effect is reciprocal, and when a piezoelectric material is subjected to an electric field, a small change in physical dimensions takes place.
Some organisms, such as sharks, are able to detect and respond to changes in electric fields, an ability known as electroreception, while others, termed electrogenic, are able to generate voltages themselves to serve as a predatory or defensive weapon. The order Gymnotiformes, of which the best known example is the electric eel, detect or stun their prey via high voltages generated from modified muscle cells called electrocytes. All animals transmit information along their cell membranes with voltage pulses called action potentials, whose functions include communication by the nervous system between neurons and muscles. An electric shock stimulates this system, and causes muscles to contract. Action potentials are also responsible for coordinating activities in certain plants.
Cultural perception.
In 1850, William Gladstone asked the scientist Michael Faraday why electricity was valuable. Faraday answered, “One day sir, you may tax it.”
In the 19th and early 20th century, electricity was not part of the everyday life of many people, even in the industrialised Western world. The popular culture of the time accordingly often depicts it as a mysterious, quasi-magical force that can slay the living, revive the dead or otherwise bend the laws of nature. This attitude began with the 1771 experiments of Luigi Galvani in which the legs of dead frogs were shown to twitch on application of animal electricity. "Revitalization" or resuscitation of apparently dead or drowned persons was reported in the medical literature shortly after Galvani's work. These results were known to Mary Shelley when she authored "Frankenstein" (1819), although she does not name the method of revitalization of the monster. The revitalization of monsters with electricity later became a stock theme in horror films.
As the public familiarity with electricity as the lifeblood of the Second Industrial Revolution grew, its wielders were more often cast in a positive light, such as the workers who "finger death at their gloves' end as they piece and repiece the living wires" in Rudyard Kipling's 1907 poem "Sons of Martha". Electrically powered vehicles of every sort featured large in adventure stories such as those of Jules Verne and the "Tom Swift" books. The masters of electricity, whether fictional or real—including scientists such as Thomas Edison, Charles Steinmetz or Nikola Tesla—were popularly conceived of as having wizard-like powers.
With electricity ceasing to be a novelty and becoming a necessity of everyday life in the later half of the 20th century, it required particular attention by popular culture only when it "stops" flowing, an event that usually signals disaster. The people who "keep" it flowing, such as the nameless hero of Jimmy Webb’s song "Wichita Lineman" (1968), are still often cast as heroic, wizard-like figures.

</doc>
<doc id="9553" url="https://en.wikipedia.org/wiki?curid=9553" title="Empedocles">
Empedocles

Empedocles (; , "Empedoklēs"; c. 490 – c. 430 BC) was a Greek pre-Socratic philosopher and a citizen of Agrigentum, a Greek city in Sicily. Empedocles' philosophy is best known for being the originator of the cosmogenic theory of the four Classical elements. He also proposed powers called Love and Strife which would act as forces to bring about the mixture and separation of the elements. These physical speculations were part of a history of the universe which also dealt with the origin and development of life. Influenced by the Pythagoreans, he supported the doctrine of reincarnation. Empedocles is generally considered the last Greek philosopher to record his ideas in verse. Some of his work survives, more than in the case of any other Presocratic philosopher. Empedocles' death was mythologized by ancient writers, and has been the subject of a number of literary treatments.
Life.
Empedocles was born, c. 490 BC, at Agrigentum (Acragas) in Sicily to a distinguished family. Very little is known about his life. His father Meto seems to have been instrumental in overthrowing the tyrant of Agrigentum, presumably Thrasydaeus in 470 BC. Empedocles continued this tradition by helping to overthrow the succeeding oligarchic government. He is said to have been magnanimous in his support of the poor; severe in persecuting the overbearing conduct of the oligarchs; and he even declined the sovereignty of the city when it was offered to him.
His brilliant oratory, his penetrating knowledge of nature, and the reputation of his marvellous powers, including the curing of diseases, and averting epidemics, produced many myths and stories surrounding his name. He was said to have been a magician and controller of storms, and he himself, in his famous poem "Purifications" seems to have promised miraculous powers, including the destruction of evil, the curing of old age, and the controlling of wind and rain.
Empedocles was acquainted or connected by friendship with the physicians Pausanias (his eromenos) and Acron; with various Pythagoreans; and even, it is said, with Parmenides and Anaxagoras. The only pupil of Empedocles who is mentioned is the sophist and rhetorician Gorgias.
Timaeus and Dicaearchus spoke of the journey of Empedocles to the Peloponnese, and of the admiration, which was paid to him there; others mentioned his stay at Athens, and in the newly founded colony of Thurii, 446 BC; there are also fanciful reports of him travelling far to the east to the lands of the Magi.
According to Aristotle, he died at the age of sixty (c. 430 BC), even though other writers have him living up to the age of one hundred and nine. Likewise, there are myths concerning his death: a tradition, which is traced to Heraclides Ponticus, represented him as having been removed from the Earth; whereas others had him perishing in the flames of Mount Etna.
A contemporary "Life of Empedocles" by Xanthus has been lost.
Works.
Empedocles is considered the last Greek philosopher to write in verse and the surviving fragments of his teaching are from two poems, "Purifications" and "On Nature". Empedocles was undoubtedly acquainted with the didactic poems of Xenophanes and Parmenides – allusions to the latter can be found in the fragments, – but he seems to have surpassed them in the animation and richness of his style, and in the clearness of his descriptions and diction. Aristotle called him the father of rhetoric, and, although he acknowledged only the meter as a point of comparison between the poems of Empedocles and the epics of Homer, he described Empedocles as Homeric and powerful in his diction. Lucretius speaks of him with enthusiasm, and evidently viewed him as his model. The two poems together comprised 5000 lines. About 550 lines of his poetry survive, although because ancient writers rarely mentioned which poem they were quoting, it is not always certain to which poem the quotes belong. Some scholars now believe that there was only one poem, and that the "Purifications" merely formed the beginning of "On Nature".
"Purifications".
We possess only about 100 lines of his "Purifications". It seems to have given a mythical account of the world which may, nevertheless, have been part of Empedocles' philosophical system. The first lines of the poem are preserved by Diogenes Laërtius:
Friends who inhabit the mighty town by tawny Acragas<br>
which crowns the citadel, caring for good deeds,<br>
greetings; I, an immortal God, no longer mortal,<br>
wander among you, honoured by all,<br>
adorned with holy diadems and blooming garlands.<br>
To whatever illustrious towns I go,<br>
I am praised by men and women, and accompanied<br>
by thousands, who thirst for deliverance,<br>
some ask for prophecies, and some entreat,<br>
for remedies against all kinds of disease.
It was probably this work which contained a story about souls, where we are told that there were once spirits who lived in a state of bliss, but having committed a crime (the nature of which is unknown) they were punished by being forced to become mortal beings, reincarnated from body to body. Humans, animals, and even plants are such spirits. The moral conduct recommended in the poem may allow us to become like gods again.
"On Nature".
There are about 450 lines of his poem "On Nature" extant, including 70 lines which have been reconstructed from some papyrus scraps known as the "Strasbourg Papyrus". The poem originally consisted of 2000 lines of hexameter verse, and was addressed to Pausanias. It was this poem which outlined his philosophical system. In it, Empedocles explains not only the nature and history of the universe, including his theory of the four classical elements, but he describes theories on causation, perception, and thought, as well as explanations of terrestrial phenomena and biological processes.
Philosophy.
Although acquainted with the theories of the Eleatics and the Pythagoreans, Empedocles did not belong to any one definite school. An eclectic in his thinking, he combined much that had been suggested by Parmenides, Pythagoras and the Ionian schools. He was a firm believer in Orphic mysteries, as well as a scientific thinker and a precursor of physical science. Aristotle mentions Empedocles among the Ionic philosophers, and he places him in very close relation to the atomist philosophers and to Anaxagoras.
According to House (1956)
Empedocles, like the Ionian philosophers and the atomists, continued the tradition of tragic thought which tried to find the basis of the relationship of the one and many. Each of the various philosophers, following Parmenides, derived from the Eleatics the conviction that an existence could not pass into non-existence, and vice versa. Yet, each one had his peculiar way of describing this relation of Divine and mortal thought and thus of the relation of the One and the Many. In order to account for change in the world, in accordance with the ontological requirements of the Eleatics, they viewed changes as the result of mixture and separation of unalterable fundamental realities. Empedocles held that the four elements (Water, Air, Earth, and Fire) were those unchangeable fundamental realities, which were themselves transfigured into successive worlds by the powers of Love and Strife (Heraclitus had explicated the Logos or the "unity of opposites").
The four elements.
Empedocles established four ultimate elements which make all the structures in the world—fire, air, water, earth. Empedocles called these four elements "roots", which he also identified with the mythical names of Zeus, Hera, Nestis, and Aidoneus (e.g., "Now hear the fourfold roots of everything: enlivening Hera, Hades, shining Zeus. And Nestis, moistening mortal springs with tears.") Empedocles never used the term "element" (, "stoicheion"), which seems to have been first used by Plato. According to the different proportions in which these four indestructible and unchangeable elements are combined with each other the difference of the structure is produced. It is in the aggregation and segregation of elements thus arising, that Empedocles, like the atomists, found the real process which corresponds to what is popularly termed growth, increase or decrease. Nothing new comes or can come into being; the only change that can occur is a change in the juxtaposition of element with element. This theory of the four elements became the standard dogma for the next two thousand years.
Love and Strife.
The four elements, however, are simple, eternal, and unalterable, and as change is the consequence of their mixture and separation, it was also necessary to suppose the existence of moving powers to bring about mixture and separation. The four elements are both eternally brought into union and parted from one another by two divine powers, Love and . Love () is responsible for the attraction of different forms of matter, and Strife () is the cause for their separation. If these elements make up of the universe, then Love and Strife explain their variation and harmony. Love and Strife are attractive and repulsive forces, respectively, which is plainly observable in human behavior, but also pervade the universe. The two forces wax and wane their dominance but neither force ever wholly disappears from the imposition of the other.
The sphere of Empedocles.
As the best and original state, there was a time when the pure elements and the two powers co-existed in a condition of rest and inertness in the form of a sphere. The elements existed together in their purity, without mixture and separation, and the uniting power of Love predominated in the sphere: the separating power of Strife guarded the extreme edges of the sphere. Since that time, strife gained more sway and the bond which kept the pure elementary substances together in the sphere was dissolved. The elements became the world of phenomena we see today, full of contrasts and oppositions, operated on by both Love and Strife. The sphere being the embodiment of pure existence is the embodiment or representative of God. Empedocles assumed a cyclical universe whereby the elements return and prepare the formation of the sphere for the next period of the universe.
Cosmogony.
Since the time of the sphere, Strife has gained more sway; and the actual world is full of contrasts and oppositions, due to the combined action of both principles. Empedocles attempted to explain the separation of elements, the formation of earth and sea, of Sun and Moon, of atmosphere. He also dealt with the first origin of plants and animals, and with the physiology of humans. As the elements entered into combinations, there appeared strange results – heads without necks, arms without shoulders. Then as these fragmentary structures met, there were seen horned heads on human bodies, bodies of oxen with human heads, and figures of double sex. But most of these products of natural forces disappeared as suddenly as they arose; only in those rare cases where the parts were found to be adapted to each other, did the complex structures last. Thus the organic universe sprang from spontaneous aggregations, which suited each other as if this had been intended. Soon various influences reduced the creatures of double sex to a male and a female, and the world was replenished with organic life. It is possible to see this theory as an anticipation of Darwin's theory of natural selection, although Empedocles was not trying to explain evolution.
Perception and knowledge.
Empedocles is credited with the first comprehensive theory of light and vision. He put forward the idea that we see objects because light streams out of our eyes and touches them. While flawed in hindsight, this became the fundamental basis on which later Greek philosophers and mathematicians, such as Euclid, would construct some of the most important theories on light, vision and optics.
Knowledge is explained by the principle that the elements in the things outside us are perceived by the corresponding elements in ourselves. Like is known by like. The whole body is full of pores and hence respiration takes place over the whole frame. In the organs of sense these pores are specially adapted to receive the effluences which are continually rising from bodies around us; and in this way perception is explained. Thus in vision, certain particles go forth from the eye to meet similar particles given forth from the object, and the resultant contact constitutes vision. Perception is not merely a passive reflection of external objects.
Empedocles noted the limitation and narrowness of human perceptions. We see only a part, but fancy that we have grasped the whole. But the senses cannot lead to truth; thought and reflection must look at the thing on every side. It is the business of a philosopher, while laying bare the fundamental difference of elements, to display the identity that exists between what seem unconnected parts of the universe.
Respiration.
In a famous fragment, Empedocles attempted to explain the phenomena of respiration by means of an elaborate analogy with the clepsydra or water clock, an ancient device for transmitting liquids from one vessel to another. This fragment has sometimes been connected to a passage in Aristotle's "Physics" where Aristotle refers to people who twisted wineskins and captured air in clepsydras to demonstrate that void does not exist. There is however, no evidence that Empedocles performed any experiment with clepsydras. The fragment certainly implies that Empedocles knew about the corporeality of air, but he says nothing whatever about the void. The clepsydra was a common utensil and everyone who used it must have known, in some sense, that the invisible air could resist liquid.
Reincarnation.
Like Pythagoras, Empedocles believed in the transmigration of the soul, that souls can be reincarnated between humans, animals and even plants. For Empedocles, all living things were on the same spiritual plane; plants and animals are links in a chain where humans are a link too. Empedocles urged a vegetarian lifestyle, since the bodies of animals are the dwelling places of punished souls. Wise people, who have learned the secret of life, are next to the divine, and their souls, free from the cycle of reincarnations, are able to rest in happiness for eternity.
Death and literary treatments.
Diogenes Laërtius records the legend that Empedocles died by throwing himself into Mount Etna in Sicily, so that the people would believe his body had vanished and he had turned into an immortal god; the volcano, however, threw back one of his bronze sandals, revealing the deceit. Another legend maintains that he threw himself into the volcano to prove to his disciples that he was immortal; he believed he would come back as a god after being consumed by the fire. Horace also refers the death of Empedocles in his work ""Ars Poetica" and admits poets the right to destroy themselves.
In "Icaro-Menippus", a comedic dialogue written by the second century satirist Lucian of Samosata, Empedocles’ final fate is re-evaluated. Rather than being incinerated in the fires of Mount Etna, he was carried up into the heavens by a volcanic eruption. Although a bit singed by the ordeal, Empedocles survives and continues his life on the Moon, surviving by feeding on dew.
Empedocles' death has inspired two major modern literary treatments. Empedocles' death is the subject of Friedrich Hölderlin's play "Tod des Empedokles" ("The Death of Empedocles"), two versions of which were written between the years 1798 and 1800. A third version was made public in 1826. In Matthew Arnold's poem "Empedocles on Etna", a narrative of the philosopher's last hours before he jumps to his death in the crater first published in 1852, Empedocles predicts:
To the elements it came from <br>
Everything will return. <br>
Our bodies to earth,<br>
Our blood to water,<br>
Heat to fire,<br>
Breath to air.
In his "History of Western Philosophy", Bertrand Russell quotes an unnamed poet on the subject – "Great Empedocles, that ardent soul, Leapt into Etna, and was roasted whole."
In "J R" by William Gaddis, Karl Marx's famous dictum ("From each according to his abilities, to each according to his needs") is misattributed to Empedocles.
In 2006, a massive underwater volcano off the coast of Sicily was named Empedocles.

</doc>
<doc id="9555" url="https://en.wikipedia.org/wiki?curid=9555" title="Ericaceae">
Ericaceae

The Ericaceae are a family of flowering plants, commonly known as the heath or heather family, found most commonly in acid and infertile growing conditions. The family is large, with roughly 4000 species spread across 126 genera, making it the 14th-most-speciose family of flowering plants. The many well-known and economically important members of the Ericaceae include the cranberry, blueberry, huckleberry, rhododendron (including azaleas), and various common heaths and heathers ("Erica", "Cassiope", "Daboecia", and "Calluna" for example).
Description.
The Ericaceae contain a morphologically diverse range of taxa, including herbs, dwarf shrubs, shrubs, and trees. Their leaves are usually alternate or whorled, simple and without stipules. Their flowers are hermaphrodite and show considerable variability. The petals are often fused (sympetalous) with shapes ranging from narrowly tubular to funnelform or widely bowl-shaped. The corollas are usually radially symmetrical (actinomorphic), but many flowers of the genus "Rhododendron" are somewhat bilaterally symmetrical (zygomorphic).
Taxonomy.
Adanson used the term Vaccinia to describe a similar family, but Jussieu first used the term Ericaceae. The name comes from the type genus "Erica", which appears to be derived from the Greek word "ereike". The exact meaning is difficult to interpret, but some sources show it as meaning 'heather'. The name may have been used informally to refer to the plants before Linnaean times, and simply been formalised when Linnaeus described "Erica" in 1753, and then again when Jussieu described the Ericaceae in 1789.
Historically, the Ericaceae included both subfamilies and tribes. In 1971, Stevens, who outlined the history from 1876 and in some instances 1839, recognised six subfamilies (Rhododendroideae, Ericoideae, Vaccinioideae, Pyroloideae, Monotropoideae, and Wittsteinioideae), and further subdivided four of the subfamilies into tribes, the Rhododendroideae having seven tribes (Bejarieae, Rhodoreae, Cladothamneae, Epigaeae, Phyllodoceae, and Diplarcheae). Within tribe Rhodoreae, five genera were described, "Rhododendron" L. (including "Azalea" L. pro parte), "Therorhodion" Small, "Ledum" L., "Tsusiophyllum" Max., "Menziesia" J. E. Smith, that were eventually transferred into "Rhododendron", along with Diplarche from the monogeneric tribe Diplarcheae.
In 2002, systematic research resulted in the inclusion of the formerly recognised families Empetraceae, Epacridaceae, Monotropaceae, Prionotaceae, and Pyrolaceae into the Ericaceae based on a combination of molecular, morphological, anatomical, and embryological data, analysed within a phylogenetic framework. The move significantly increased the morphological and geographical range found within the group. One possible classification of the resulting family includes 9 subfamilies, 126 genera, and about 4000 species:
Distribution and ecology.
The Ericaceae have a nearly worldwide distribution. They are absent from continental Antarctica, parts of the high Arctic, central Greenland, northern and central Australia, and much of the lowland tropics and neotropics.
The family is largely composed of plants that can tolerate acidic, infertile conditions. Like other stress-tolerant plants, many Ericaceae have mycorrhizal fungi to assist with extracting nutrients from infertile soils, as well as evergreen foliage to conserve absorbed nutrients. This trait is not found in the Clethraceae and Cyrillaceae, the two families most closely related to the Ericaceae. Most Ericaceae (excluding the Monotropoideae, and some Styphelioideae) form a distinctive accumulation of mycorrhizae, in which fungi grow in and around the roots and provide the plant with nutrients. The Pyroloideae are mixotrophic and gain sugars from the mycorrhizae, as well as nutrients.
In many parts of the world, a "heath" or "heathland" is an environment characterised by an open dwarf-shrub community found on low-quality acidic soils, generally dominated by plants in the Ericaceae. A common example is "Erica tetralix". This plant family is also typical of peat bogs and blanket bogs; examples include "Rhododendron groenlandicum" and "Kalmia polifolia". In eastern North America, members of this family often grow in association with an oak canopy, in a habitat known as an oak-heath forest.
Some evidence suggests eutrophic rainwater can convert ericoid heaths with species such as "Erica tetralix" to grasslands. Nitrogen is particularly suspect in this regard, and may be causing measurable changes to the distribution and abundance of some ericaceous species.
Use in alternative medicine.
Heather has been listed as one of the 38 plants used to prepare Bach flower remedies, a kind of alternative medicine promoted for its effect on health. However, according to Cancer Research UK, "there is no scientific evidence to prove that flower remedies can control, cure, or prevent any type of disease, including cancer".

</doc>
<doc id="9559" url="https://en.wikipedia.org/wiki?curid=9559" title="Electrical network">
Electrical network

An electrical network is an interconnection of electrical components (e.g. batteries, resistors, inductors, capacitors, switches) or a model of such an interconnection, consisting of electrical elements (e.g. voltage sources, current sources, resistances, inductances, capacitances). An electrical circuit is a network consisting of a closed loop, giving a return path for the current. Linear electrical networks, a special type consisting only of sources (voltage or current), linear lumped elements (resistors, capacitors, inductors), and linear distributed elements (transmission lines), have the property that signals are linearly superimposable. They are thus more easily analyzed, using powerful frequency domain methods such as Laplace transforms, to determine DC response, AC response, and transient response.
A resistive circuit is a circuit containing only resistors and ideal current and voltage sources. Analysis of resistive circuits is less complicated than analysis of circuits containing capacitors and inductors. If the sources are constant (DC) sources, the result is a DC circuit.
A network that contains active electronic components is known as an "electronic circuit". Such networks are generally nonlinear and require more complex design and analysis tools.
Classification.
By Passivity.
An active network is a network that contains an active source - either a voltage source or current source.
A passive network is a network that does not contain an active source.
By linearity.
A network is linear if its signals obey the principle of superposition; otherwise it is non-linear.
Classification of sources.
Sources can be classified as independent sources and dependent sources
Independent Sources.
Ideal Independent Source maintains same voltage or current regardless of the other elements present in the circuit. Its value is either constant (DC) or sinusoidal (AC). The strength of voltage or current is not changed by any variation in connected network.
Dependent Sources.
Dependent Sources depend upon a particular element of the circuit for delivering the power or voltage or current depending upon the type of source it is.
Electrical laws.
A number of electrical laws apply to all electrical networks. These include:
Other more complex laws may be needed if the network contains nonlinear or reactive components. Non-linear self-regenerative heterodyning systems can be approximated. Applying these laws results in a set of simultaneous equations that can be solved either algebraically or numerically.
Design methods.
To design any electrical circuit, either analog or digital, electrical engineers need to be able to predict the voltages and currents at all places within the circuit. Simple linear circuits can be analyzed by hand using complex number theory. In more complex cases the circuit may be analyzed with specialized software programs or estimation techniques such as the piecewise-linear model.
Circuit simulation software, such as HSPICE (an analog circuit simulator), and languages such as VHDL-AMS and verilog-AMS allow engineers to design circuits without the time, cost and risk of error involved in building circuit prototypes.
Network simulation software.
More complex circuits can be analyzed numerically with software such as SPICE or GNUCAP, or symbolically using software such as SapWin.
Linearization around operating point.
When faced with a new circuit, the software first tries to find a steady state solution, that is, one where all nodes conform to Kirchhoff's current law "and" the voltages across and through each element of the circuit conform to the voltage/current equations governing that element.
Once the steady state solution is found, the operating points of each element in the circuit are known. For a small signal analysis, every non-linear element can be linearized around its operation point to obtain the small-signal estimate of the voltages and currents. This is an application of Ohm's Law. The resulting linear circuit matrix can be solved with Gaussian elimination.
Piecewise-linear approximation.
Software such as the PLECS interface to Simulink uses piecewise-linear approximation of the equations governing the elements of a circuit. The circuit is treated as a completely linear network of ideal diodes. Every time a diode switches from on to off or vice versa, the configuration of the linear network changes. Adding more detail to the approximation of equations increases the accuracy of the simulation, but also increases its running time.

</doc>
<doc id="9561" url="https://en.wikipedia.org/wiki?curid=9561" title="Euler (disambiguation)">
Euler (disambiguation)

Euler may also refer to:

</doc>
<doc id="9566" url="https://en.wikipedia.org/wiki?curid=9566" title="Empty set">
Empty set

In mathematics, and more specifically set theory, the empty set is the unique set having no elements; its size or cardinality (count of elements in a set) is zero. Some axiomatic set theories ensure that the empty set exists by including an axiom of empty set; in other theories, its existence can be deduced. Many possible properties of sets are trivially true for the empty set.
"Null set" was once a common synonym for "empty set", but is now a technical term in measure theory. The empty set may also be called the "void set".
Notation.
Common notations for the empty set include "{}", "∅", and "formula_1". The latter two symbols were introduced by the Bourbaki group (specifically André Weil) in 1939, inspired by the letter Ø in the Norwegian and Danish alphabets (and not related in any way to the Greek letter Φ).
The empty-set symbol is found at Unicode point U+2205. In TeX, it is coded as \emptyset or \varnothing.
Properties.
In standard axiomatic set theory, by the principle of extensionality, two sets are equal if they have the same elements; therefore there can be only one set with no elements. Hence there is but one empty set, and we speak of "the empty set" rather than "an empty set".
The mathematical symbols employed below are explained here.
For any set "A":
The empty set has the following properties:
The connection between the empty set and zero goes further, however: in the standard set-theoretic definition of natural numbers, we use sets to model the natural numbers. In this context, zero is modelled by the empty set.
For any property:
Conversely, if for some property and some set "V", the following two statements hold:
By the definition of subset, the empty set is a subset of any set "A". That is, "every" element "x" of formula_1 belongs to "A". Indeed, if it were not true that every element of formula_1 is in "A" then there would be at least one element of formula_1 that is not present in "A". Since there are "no" elements of formula_1 at all, there is no element of formula_1 that is not in "A". Any statement that begins "for every element of formula_1" is not making any substantive claim; it is a vacuous truth. This is often paraphrased as "everything is true of the elements of the empty set."
Operations on the empty set.
When speaking of the sum of the elements of a finite set, one is inevitably led to the convention that the sum of the elements of the empty set is zero. The reason for this is that zero is the identity element for addition. Similary, the product of the elements of the empty set should be considered to be one (see empty product), since one is the identity element for multiplication.
A disarrangement of a set is a permutation of the set that leaves no element in the same position. The empty set is a disarrangment of itself as no element can be found that retains its original position.
In other areas of mathematics.
Extended real numbers.
Since the empty set has no members, when it is considered as a subset of any ordered set, then every member of that set will be an upper bound and lower bound for the empty set. For example, when considered as a subset of the real numbers, with its usual ordering, represented by the real number line, every real number is both an upper and lower bound for the empty set. When considered as a subset of the extended reals formed by adding two "numbers" or "points" to the real numbers, namely negative infinity, denoted formula_18 which is defined to be less than every other extended real number, and positive infinity, denoted formula_19 which is defined to be greater than every other extended real number, then: 
and
That is, the least upper bound (sup or supremum) of the empty set is negative infinity, while the greatest lower bound (inf or infimum) is positive infinity. By analogy with the above, in the domain of the extended reals, negative infinity is the identity element for the maximum and supremum operators, while positive infinity is the identity element for minimum and infimum.
Topology.
Considered as a subset of the real number line (or more generally any topological space), the empty set is both closed and open; it is an example of a "clopen" set. All its boundary points (of which there are none) are in the empty set, and the set is therefore closed; while for every one of its points (of which there are again none), there is an open neighbourhood in the empty set, and the set is therefore open. Moreover, the empty set is a compact set by the fact that every finite set is compact.
The closure of the empty set is empty. This is known as "preservation of nullary unions."
Category theory.
If "A" is a set, then there exists precisely one function "f" from {} to "A", the empty function. As a result, the empty set is the unique initial object of the category of sets and functions.
The empty set can be turned into a topological space, called the empty space, in just one way: by defining the empty set to be open. This empty topological space is the unique initial object in the category of topological spaces with continuous maps. In fact, it is a strict initial object: only the empty set has a function to the empty set.
Questioned existence.
Axiomatic set theory.
In Zermelo set theory, the existence of the empty set is assured by the axiom of empty set, and its uniqueness follows from the axiom of extensionality. However, the axiom of empty set can be shown redundant in either of two ways:
Philosophical issues.
While the empty set is a standard and widely accepted mathematical concept, it remains an ontological curiosity, whose meaning and usefulness are debated by philosophers and logicians.
The empty set is not the same thing as "nothing"; rather, it is a set with nothing "inside" it and a set is always "something". This issue can be overcome by viewing a set as a bag—an empty bag undoubtedly still exists. Darling (2004) explains that the empty set is not nothing, but rather "the set of all triangles with four sides, the set of all numbers that are bigger than nine but smaller than eight, and the set of all opening moves in chess that involve a king."
The popular syllogism
is often used to demonstrate the philosophical relation between the concept of nothing and the empty set. Darling writes that the contrast can be seen by rewriting the statements "Nothing is better than eternal happiness" and " ham sandwich is better than nothing" in a mathematical tone. According to Darling, the former is equivalent to "The set of all things that are better than eternal happiness is formula_1" and the latter to "The set {ham sandwich} is better than the set formula_1". It is noted that the first compares elements of sets, while the second compares the sets themselves.
Jonathan Lowe argues that while the empty set:
it is also the case that:
George Boolos argued that much of what has been heretofore obtained by set theory can just as easily be obtained by plural quantification over individuals, without reifying sets as singular entities having other entities as members.

</doc>
<doc id="9567" url="https://en.wikipedia.org/wiki?curid=9567" title="Egoism">
Egoism

The terms "egoism" and "egotism" may refer to:

</doc>
<doc id="9569" url="https://en.wikipedia.org/wiki?curid=9569" title="Endomorphism">
Endomorphism

In mathematics, an endomorphism is a morphism (or homomorphism) from a mathematical object to itself. For example, an endomorphism of a vector space "V" is a linear map , and an endomorphism of a group "G" is a group homomorphism . In general, we can talk about endomorphisms in any category. In the category of sets, endomorphisms are functions from a set "S" to itself.
In any category, the composition of any two endomorphisms of "X" is again an endomorphism of "X". It follows that the set of all endomorphisms of "X" forms a monoid, denoted End("X") (or End("X") to emphasize the category "C").
Automorphisms.
An invertible endomorphism of "X" is called an automorphism. The set of all automorphisms is a subset of End("X") with a group structure, called the automorphism group of "X" and denoted Aut("X"). In the following diagram, the arrows denote implication:
Endomorphism ring.
Any two endomorphisms of an abelian group "A" can be added together by the rule . Under this addition, the endomorphisms of an abelian group form a ring (the endomorphism ring). For example, the set of endomorphisms of Z is the ring of all matrices with integer entries. The endomorphisms of a vector space or module also form a ring, as do the endomorphisms of any object in a preadditive category. The endomorphisms of a nonabelian group generate an algebraic structure known as a near-ring. Every ring with one is the endomorphism ring of its regular module, and so is a subring of an endomorphism ring of an abelian group, however there are rings which are not the endomorphism ring of any abelian group.
Operator theory.
In any concrete category, especially for vector spaces, endomorphisms are maps from a set into itself, and may be interpreted as unary operators on that set, acting on the elements, and allowing to define the notion of orbits of elements, etc.
Depending on the additional structure defined for the category at hand (topology, metric, ...), such operators can have properties like continuity, boundedness, and so on. More details should be found in the article about operator theory.
Endofunctions.
An endofunction is a function whose domain is equal to its codomain. A homomorphic endofunction is an endomorphism.
Let "S" be an arbitrary set. Among endofunctions on "S" one finds permutations of "S" and constant functions associating to each a given . Every permutation of "S" has the codomain equal to its domain and is bijective and invertible. A constant function on "S", if "S" has more than 1 element, has an image that is a proper subset of its codomain, is not bijective (and non invertible). The function associating to each natural integer "n" the floor of "n"/2 has its image equal to its codomain and is not invertible.
Finite endofunctions are equivalent to directed pseudoforests. For sets of size "n" there are "n" endofunctions on the set.
Particular bijective endofunctions are the involutions, i.e. the functions coinciding with their inverses.

</doc>
<doc id="9574" url="https://en.wikipedia.org/wiki?curid=9574" title="Eric Hoffer">
Eric Hoffer

Eric Hoffer (July 25, 1898 – May 21, 1983) was an American moral and social philosopher. He was the author of ten books and was awarded the Presidential Medal of Freedom in February 1983. His first book, "The True Believer" (1951), was widely recognized as a classic, receiving critical acclaim from both scholars and laymen, although Hoffer believed that "The Ordeal of Change" was his finest work.
Life and career.
Hoffer was born in 1898 in The Bronx, New York, to Knut and Elsa (Goebel) Hoffer. His parents were immigrants from Alsace, then part of Imperial Germany. By age five, Hoffer could already read in both English and his parents' native German. When he was five, his mother fell down the stairs with him in her arms. He later recalled, "I lost my sight at the age of seven. Two years before, my mother and I fell down a flight of stairs. She did not recover and died in that second year after the fall. I lost my sight and, for a time, my memory."
He was raised by a live-in relative or servant, a German immigrant named Martha. His eyesight inexplicably returned when he was 15. Fearing he might lose it again, he seized on the opportunity to read as much as he could. His recovery proved permanent, but Hoffer never abandoned his reading habit.
Hoffer was a young man when he also lost his father. The cabinetmaker's union paid for Knut Hoffer's funeral and gave Hoffer about $300 insurance money. He took a bus to Los Angeles and spent the next 10 years on Skid Row, reading, occasionally writing, and working at odd jobs.
In 1931, he considered suicide by drinking a solution of oxalic acid, but he could not bring himself to do it. He left Skid Row and became a migrant worker, following the harvests in California. He acquired a library card where he worked, dividing his time "between the books and the brothels." He also prospected for gold in the mountains. Snowed in for the winter, he read the "Essays" by Michel de Montaigne. Montaigne impressed Hoffer deeply, and Hoffer often made reference to him. He also developed a respect for America's underclass, which he said was "lumpy with talent."
He wrote a novel, "Four Years in Young Hank's Life," and a novella, "Chance and Mr. Kunze," both partly autobiographical. He also penned a long article based on his experiences in a federal work camp, "Tramps and Pioneers." It was never published, but a truncated version appeared in Harper's Magazine after he became well known.
Hoffer tried to enlist in the US Army at age 40 during World War II, but he was rejected because of a hernia. Instead, he worked as a longshoreman on the docks of San Francisco. At the same time, he began to write seriously.
Hoffer left the docks in 1967 and retired from public life in 1970. In 1970, he endowed the Lili Fabilli and Eric Hoffer Laconic Essay Prize for students, faculty, and staff at the University of California, Berkeley.
Hoffer called himself an atheist but had sympathetic views of religion and described it as a positive force.
He died at his home in San Francisco in 1983 at the age of 84.
Working class roots.
Hoffer was influenced by his modest roots and working-class surroundings, seeing in it vast human potential. In a letter to Margaret Anderson in 1941, he wrote:
He once remarked, "my writing grows out of my life just as a branch from a tree." When he was called an intellectual, he insisted that he was a longshoreman. Hoffer has been dubbed by some authors a "longshoreman philosopher."
Books and opinions.
"The True Believer".
Hoffer came to public attention with the 1951 publication of his first book, "The True Believer: Thoughts on the Nature of Mass Movements". Concerned about the rise of totalitarian governments, especially those of Adolf Hitler and Joseph Stalin, he tried to find the roots of these "madhouses" in human psychology.
Hoffer argued that fanatical and extremist cultural movements, whether religious or political, arose under predictable circumstances: when large numbers of people come to believe that their individual lives are worthless and ruined, that the modern world is irreparably corrupt, and that hope lies only in joining a larger group that demands radical changes. Hoffer believed that self-esteem and a sense of satisfaction with one's life was of central importance to psychological well-being. He thus focused on what he viewed as the consequences of a lack of self-esteem. For example, Hoffer noted that leaders of mass movements were often frustrated intellectuals, from Adolf Hitler in 20th century Europe to Hong Xiuquan's failure to advance in the Chinese bureaucracy of the 19th Century.
A core principle in the book is Hoffer's assertion that mass movements are interchangeable: in the Germany of the 1920s and the 1930s, the Communists and Nazis were ostensibly enemies but routinely swapped members as they competed for the same kind of marginalized, angry people, and fanatical Communists became Nazis and vice versa.
Almost 2000 years previously, Saul, a fanatical persecutor of Christians, became Paul, a Christian. For the "true believer," Hoffer argued that substance of any particular group is less important than being part of an energized movement.
Hoffer also claimed that a passionate obsession with the outside world or the private lives of others was an attempt to compensate for a lack of meaning in one's own life. The book discusses religious and political mass movements, and includes extensive discussions of Islam and Christianity.
Hoffer's work was non-Freudian at a time when much of American psychology was informed by the Freudian paradigm. Hoffer appeared on public television in 1964 and then in two one-hour conversations on CBS with Eric Sevareid in the late 1960s.
Later works.
Subsequent to the publication of "The True Believer" (1951), Eric Hoffer touched upon Asia and American interventionism in several of his essays. In “The Awakening of Asia” (1954), published in "The Reporter" and later his book "The Ordeal of Change" (1963), Hoffer discusses the reasons for unrest on the continent. In particular, he argues that the root cause of social discontent in Asia was not government corruption, "communist agitation," or the legacy of European colonial "oppression and exploitation." Rather a "craving for pride"/was the central problem in Asia, suggesting a problem that could not be relieved through typical American intervention.
For centuries, Hoffer notes that Asia had “submitted to one conqueror after another." Throughout these centuries, Asia had “been misruled, looted, and bled by both foreign and native oppressors without” so much as “a peep” from the general population. Though not without negative effect, corrupt governments and the legacy of European imperialism represented nothing new under the sun. Indeed, the European colonial authorities had been "fairly beneficent" in Asia.
To be sure, communism exerted an appeal of sorts. For the Asian "pseudo-intellectual," it promised elite status and the phony complexities of "doctrinaire double talk." For the ordinary Asian, it promised partnership with the seemingly emergent Soviet Union in a "tremendous, unprecedented undertaking" to build a better tomorrow.
According to Hoffer, however, communism in Asia was dwarfed by the desire for pride. To satisfy such desire, Asians would willingly and irrationally sacrifice their economic well-being and their lives as well.
Unintentionally, the West had created this appetite, causing "revolutionary unrest” in Asia. The West had done so by eroding the traditional communal bonds that once had woven the individual to the patriarchal family, clan, tribe, "cohesive rural or urban unit," and "religious or political body."
Without the security and spiritual meaning produced by such bonds, Asians had been liberated from tradition only to find themselves now atomized, isolated, exposed, and abandoned, “left orphaned and empty in a cold world."
Certainly, Europe had undergone a similar destruction of tradition, but it had occurred centuries earlier at the end of the medieval period and produced better results thanks to different circumstances.
For the Asians of the 1950s, the circumstances differed markedly. Most were illiterate and impoverished, living in a world that included no expansive physical or intellectual vistas. Dangerously, the "articulate minority" of the Asian population inevitably disconnected themselves from the ordinary people, thereby failing to acquire "a sense of usefulness and of worth" that came by "taking part in the world's work." As a result, they were "condemned to the life of chattering posturing pseudo-intellectuals" and coveted "the illusion of weight and importance."
Most significantly, Hoffer asserts that the disruptive awakening of Asia came about as a result of an unbearable sense of weakness. Indeed, Hoffer discusses the problem of weakness, asserting that while "power corrupts the few... weakness corrupts the many."
Hoffer notes that " the resentment of the weak does not spring from any injustice done them but from the sense of their w inadequacy and impotence." In short, the weak "hate not wickedness" but themselves for being weak. Consequently, self-loathing produces explosive effects that cannot be mitigated through social engineering schemes, such as programs of wealth redistribution. In fact, American "generosity" is counterproductive, perceived in Asia simply as an example of Western "oppression."
In the wake of the Korean War, Hoffer does not recommend exporting at gunpoint either American political institutions or mass democracy. In fact, Hoffer advances the possibility that winning over the multitudes of Asia may not even be desirable. If on the other hand, necessity truly dictates that for "survival" the United States must persuade the "weak" of Asia to "our side," Hoffer suggests the wisest course of action would be to master "the art or technique of sharing hope, pride, and as a last resort, hatred with others."
During the Vietnam War, despite his objections for the antiwar movement and acceptance of the notion that the war was somehow necessary to prevent a third world war, Hoffer remained skeptical concerning American interventionism, specifically the intelligence with which the war was being conducted in Southeast Asia. After the United States became involved in the war, Hoffer wished to avoid defeat in Vietnam because of his fear that such a defeat would transform American society for ill, opening the door to those who would preach a stab-in-the-back myth and allow for the rise of an American version of Hitler.
In "The Temper of Our Time" (1967), Hoffer implies that the United States as a rule should avoid interventions in the first place: "the better part of statesmanship might be to know clearly and precisely what not to do, and leave action to the improvisation of chance." In fact, Hoffer indicates that "it might be wise to wait for enemies to defeat themselves," as they might fall upon each other with the United States out of the picture. The view was somewhat borne out with the Cambodian-Vietnamese War and Chinese-Vietnamese War of the late 1970s.
In May 1968, about a year after the Six Day War, he wrote an article for the "Los Angeles Times" titled "Israel's Peculiar Position:"
Hoffer asks why "everyone expects the Jews to be the only real Christians in this world" and why Israel should sue for peace after its victory.
Hoffer believed that rapid change is not necessarily a positive thing for a society and that too rapid change can cause a regression in maturity for those who were brought up in a different society. He noted that in America in the 1960s, many young adults were still living in extended adolescence. Seeking to explain the attraction of the New Left protest movements, he characterized them as the result of widespread affluence, which "is robbing a modern society of whatever it has left of puberty rites to routinize the attainment of manhood." He saw the puberty rites as essential for self-esteem and noted that mass movements and juvenile mindsets tend to go together, to the point that anyone, no matter what age, who joins a mass movement immediately begins to exhibit juvenile behavior.
Hoffer further noted that working-class Americans rarely joined protest movements and subcultures since they had entry into meaningful labor as an effective rite of passage out of adolescence while both the very poor who lived on welfare and the affluent were, in his words, "prevented from having a share in the world's work, and of proving their manhood by doing a man's work and getting a man's pay" and thus remained in a state of extended adolescence. Lacking in necessary self-esteem, they were prone to joining mass movements as a form of compensation. Hoffer suggested that the need for meaningful work as a rite of passage into adulthood could be fulfilled with a two-year civilian national service program (like programs during the Great Depression such as the Civilian Conservation Corps): "The routinization of the passage from boyhood to manhood would contribute to the solution of many of our pressing problems. I cannot think of any other undertaking that would dovetail so many of our present difficulties into opportunities for growth."
Hoffer's papers.
Hoffer's papers, including 131 of the notebooks he carried in his pockets, were acquired in 2000 by the Hoover Institution Archives. The papers fill of shelf space. Because Hoffer cultivated an aphoristic style, the unpublished notebooks (dated from 1949 to 1977) contain very significant work. Available for scholarly study since at least 2003, little of their contents has yet been published. A selection of fifty aphorisms, focusing on the development of unrealized human talents through the creative process, appeared in the July 2005 issue of "Harper's Magazine".
Interviews.
"Conversations with Eric Hoffer," 12 part interview by James Day of KQED, San Franscisco, 1963.
"Eric Hoffer: The Passionate State of Mind" with Eric Sevareid, CBS, September 19, 1967 (rebroadcast on November 14, due to popular demand).
"The Savage Heart: A Conversation with Eric Hoffer," with Eric Sevareid, CBS, January 28, 1969.
Eric Hoffer Award.
On the 1st January, 2001, the Eric Hoffer Award for books and prose was launched internationally in his honor.
In 2005, the Eric Hoffer Estate granted its permission for the award. Also in that year Christopher Klim became the award's Chairperson.
Reception.
Australian foreign minister Julie Bishop extensively referred to Hoffer's book "The True Believer" when in a 2015 speech she closely compared the psychological underpinnings of ISIS with that of Nazism.
Further reading.
"American Iconoclast: The Life and Times of Eric Hoffer", Shachtman, Tom, Titusville, NJ, Hopewell Publications, 2011. ISBN 978-1-933435-38-1.

</doc>
<doc id="9577" url="https://en.wikipedia.org/wiki?curid=9577" title="European Coal and Steel Community">
European Coal and Steel Community

The European Coal and Steel Community (ECSC) was an international organization serving to unify European countries after World War II. It was formally established in 1951 by the Treaty of Paris, which was signed by Belgium, France, West Germany, Italy, the Netherlands and Luxembourg. The ECSC was the first international organisation to be based on the principles of supranationalism, and would ultimately lead the way to the founding of the European Union.
The ECSC was first proposed by French foreign minister Robert Schuman on 9 May 1950 as a way to prevent further war between France and Germany. He declared his aim was to "make war not only unthinkable but materially impossible" which was to be achieved by regional integration, of which the ECSC was the first step. The Treaty would create a common market for coal and steel among its member states which served to neutralise competition between European nations over natural resources, particularly in the Ruhr.
The ECSC was run by four institutions: a High Authority composed of independent appointees, a Common Assembly composed of national parliamentarians, a Special Council composed of nation ministers, and a Court of Justice. These would ultimately form the blueprint for today's European Commission, European Parliament, the Council of the European Union and the European Court of Justice.
The ECSC was joined by two other similar communities in 1957, the European Economic Community and European Atomic Energy Community, with whom it shared its membership and some institutions. In 1967 all its institutions were merged with that of the European Economic Community, but it retained its own independent legal personality. In 2002 the Treaty of Paris expired and all the ECSC activities and resources were absorbed by the European Community.
History.
As Prime Minister and Foreign Minister, Schuman was instrumental in turning French policy away from the Gaullist policy of permanent occupation or control of parts of German territory such as the Ruhr or the Saar. Despite stiff ultra-nationalist, Gaullist and communist opposition, the French Assembly voted a number of resolutions in favour of his new policy of integrating Germany into a community. The International Authority for the Ruhr changed in consequence. Schuman's guiding principles were moral, based on the equality of states (international democracy), not the power politics of domination.
Schuman declaration.
The Schuman Declaration of 9 May 1950 (later known as Europe Day) occurred after two Cabinet meetings, when the proposal became French government policy. France was thus the first government to agree to surrender sovereignty in a supranational Community. That decision was based on a text, written and edited by Schuman's friend and colleague, the Foreign Ministry lawyer, Paul Reuter with the assistance of Jean Monnet and Schuman's Directeur de Cabinet, Bernard Clappier. It laid out a plan for a European Community to pool the coal and steel of its members in a common market.
Schuman proposed that "Franco-German production of coal and steel as a whole be placed under a common High Authority, within the framework of an organisation open to the participation of the other countries of Europe." Such an act was intended to help economic growth and cement peace between France and Germany, who were historic enemies. Coal and steel were vital resources needed for a country to wage war, so pooling those resources between two such enemies was seen as more than symbolic. Schuman saw the decision of the French government on his proposal as the first example of a democratic and supranational Community, a new development in world history. The plan was also seen by some, like Monnet, who crossed out Reuter's mention of 'supranational' in the draft and inserted 'federation', as a first step to a "European federation".
The Schuman Declaration that created the ECSC had several distinct aims:
Firstly, it was intended to prevent further war between France and Germany and other states by tackling the root cause of war. The ECSC was primarily conceived with France and Germany in mind: "The coming together of the nations of Europe requires the elimination of the age-old opposition of France and Germany. Any action taken must in the first place concern these two countries." The coal and steel industries being essential for the production of munitions, Schuman believed that by uniting these two industries across France and Germany under an innovative supranational system that also included a European anti-cartel agency, he could "make war not only unthinkable but materially impossible." Schuman had another aim: "With increased resources Europe will be able to pursue the achievement of one of its essential tasks, namely, the development of the African continent." Industrial cartels tended to impose "restrictive practices" on national markets, whereas the ECSC would ensure the increased production necessary for their ambitions in Africa.
Political pressures.
In West Germany, Schuman kept the closest contacts with the new generation of democratic politicians. Karl Arnold, the Minister President of North Rhine-Westphalia, the province that included the coal and steel producing Ruhr, was initially spokesman for German foreign affairs. He gave a number of speeches and broadcasts on a supranational coal and steel community at the same time as Robert Schuman began to propose this Community in 1948 and 1949. The Social Democratic Party of Germany (, SPD), in spite of support from unions and other socialists in Europe, decided it would oppose the Schuman plan. Kurt Schumacher's personal distrust of France, capitalism, and Konrad Adenauer aside, he claimed that a focus on integrating with a "Little Europe of the Six" would override the SPD's prime objective of German reunification and thus empower ultra-nationalist and Communist movements in democratic countries. He also thought the ECSC would end any hopes of nationalising the steel industry and lock in a Europe of "cartels, clerics and conservatives." Younger members of the party like Carlo Schmid, were, however, in favor of the Community and pointed to the long socialist support for the supranational idea.
In France, Schuman had gained strong political and intellectual support from all sections of the nation and many noncommunist parties. Notable amongst these were ministerial colleague Andre Philip, president of the Foreign Relations Committee Edouard Bonnefous, and former prime minister, Paul Reynaud. Projects for a coal and steel authority and other supranational communities were formulated in specialist subcommittees of the Council of Europe in the period before it became French government policy. Charles de Gaulle, who was then out of power, had been an early supporter of "linkages" between economies, on French terms, and had spoken in 1945 of a "European confederation" that would exploit the resources of the Ruhr. However, he opposed the ECSC as a "faux" (false) pooling ("le pool, ce faux semblant") because he considered it an unsatisfactory "piecemeal approach" to European unity and because he considered the French government "too weak" to dominate the ECSC as he thought proper. De Gaulle also felt that the ECSC had insufficient supranational authority because the Assembly was not ratified by a European referendum and he did not accept Raymond Aron's contention that the ECSC was intended as a movement away from United States domination. Consequently, de Gaulle and his followers in the RPF voted against ratification in the lower house of the French Parliament.
Despite these attacks and those from the extreme left, the ECSC found substantial public support, and so it was established. It gained strong majority votes in all eleven chambers of the parliaments of the Six, as well as approval among associations and European public opinion. In 1950, many had thought another war was inevitable. The steel and coal interests, however, were quite vocal in their opposition. The Council of Europe, created by a proposal of Schuman's first government in May 1948, helped articulate European public opinion and gave the Community idea positive support.
Treaties.
The 100-article Treaty of Paris, which established the ECSC, was signed on 18 April 1951 by "the inner six": France, West Germany, Italy, Belgium, the Netherlands and Luxembourg (Benelux). The ECSC was the first international organisation to be based on supranational principles and was, through the establishment of a common market for coal and steel, intended to expand the economies, increase employment, and raise the standard of living within the Community. The market was also intended to progressively rationalise the distribution of high level production whilst ensuring stability and employment. The common market for coal was opened on 10 February 1953, and for steel on 1 May 1953. Upon taking effect, the ECSC gradually replaced the International Authority for the Ruhr.
On 11 August 1952, the United States was the first non-ECSC member to recognise the Community and stated it would now deal with the ECSC on coal and steel matters, establishing its delegation in Brussels. Monnet responded by choosing Washington, D. C. as the site of the ECSC's first external presence. The headline of the delegation's first bulletin read "Towards a Federal Government of Europe".
Six years after the Treaty of Paris, the Treaties of Rome were signed by the six ECSC members, creating the European Economic Community (EEC) and the European Atomic Energy Community (EAEC or 'Euratom'). These Communities were based, with some adjustments, on the ECSC. The Treaties of Rome were to be in force indefinitely, unlike the Treaty of Paris, which was to expire after fifty years. These two new Communities worked on the creation of a customs union and nuclear power community respectively. The Rome treaties were hurried through just before de Gaulle was given emergency powers and proclaimed the Fifth Republic. Despite his efforts to 'chloroform' the Communities, their fields rapidly expanded and the EEC became the most important tool for political unification, overshadowing the ECSC.
Merger and expiration.
Despite being separate legal entities, the ECSC, EEC and Euratom initially shared the Common Assembly and the European Court of Justice, although the Councils and the High Authority/Commissions remained separate. To avoid duplication, the Merger Treaty merged these separate bodies of the ECSC and Euratom with the EEC. The EEC later became one of the three pillars of the present day European Union.
The Treaty of Paris was frequently amended as the EC and EU evolved and expanded. With the treaty due to expire in 2002, debate began at the beginning of the 1990s on what to do with it. It was eventually decided that it should be left to expire. The areas covered by the ECSC's treaty were transferred to the Treaty of Rome and the financial loose ends and the ECSC research fund were dealt with via a protocol of the Treaty of Nice. The treaty finally expired on 23 July 2002. That day, the ECSC flag was lowered for the final time outside the European Commission in Brussels and replaced with the EU flag.
Institutions.
The institutions of the ECSC were the "High Authority", the "Common Assembly", the "Special Council of Ministers" and the "Court of Justice". A "Consultative Committee" was established alongside the High Authority, as a fifth institution representing civil society. This was the first international representation of consumers in history. These institutions were merged in 1967 with those of the European Community, which then governed the ECSC, except for the Committee, which continued to be independent until the expiration of the Treaty of Paris in 2002.
The Treaty stated that the location of the institutions would be decided by common accord of the members, yet the issue was hotly contested. As a temporary compromise, the institutions were provisionally located in the City of Luxembourg, despite the Assembly being based in Strasbourg.
High Authority.
The High Authority (the predecessor to the European Commission) was a nine-member executive body which governed the Community. The Authority consisted of nine members in office for a term of six years. Eight of these members were appointed by the governments of the six signatories. These eight members then themselves appointed a ninth person to be President of the High Authority.
Despite being appointed by agreement of national governments acting together, the members were to pledge not to represent their national interest, but rather took an oath to defend the general interests of the Community as a whole. Their independence was aided by members being barred from having any occupation outside the Authority or having any business interests (paid or unpaid) during their tenure and for three years after they left office. To further ensure impartiality, one third of the membership was to be renewed every two years (article 10).
The Authority's principal innovation was its supranational character. It had a broad area of competence to ensure the objectives of the treaty were met and that the common market functioned smoothly. The High Authority could issue three types of legal instruments: Decisions, which were entirely binding laws; Recommendations, which had binding aims but the methods were left to member states; and Opinions, which had no legal force.
Up to the merger in 1967, the authority had five Presidents followed by an interim President serving for the final days.
Other institutions.
The Common Assembly (which later became the European Parliament) was composed of 78 representatives and exercised supervisory powers over the executive High Authority. The Common Assembly representatives were to be national MPs delegated each year by their Parliaments to the Assembly or directly elected 'by universal suffrage' (article 21), though in practice it was the former, as there was no requirement for elections until the Treaties of Rome and no actual election until 1979, as Rome required agreement in the Council on the electoral system first. However, to emphasise that the chamber was not a traditional international organisation composed of representatives of national governments, the Treaty of Paris used the term "representatives of the peoples". The Assembly was not originally specified in the Schuman Plan because it was hoped the Community would use the institutions (Assembly, Court) of the Council of Europe. When this became impossible because of British objections, separate institutions had to be created. The Assembly was intended as a democratic counter-weight and check to the High Authority, to advise but also to have power to sack the Authority for incompetence, injustice, corruption or fraud. The first President (akin to a Speaker) was Paul-Henri Spaak.
The Special Council of Ministers (equivalent to the current Council of the European Union) was composed of representatives of national governments. The Presidency was held by each state for a period of three months, rotating between them in alphabetical order. One of its key aspects was the harmonisation of the work of the High Authority and that of national governments, which were still responsible for the state's general economic policies. The Council was also required to issue opinions on certain areas of work of the High Authority. Issues relating only to coal and steel were in the exclusive domain of the High Authority, and in these areas the Council (unlike the modern Council) could only act as a scrutiny on the Authority. However, areas outside coal and steel required the consent of the Council.
The Court of Justice was to ensure the observation of ECSC law along with the interpretation and application of the Treaty. The Court was composed of seven judges, appointed by common accord of the national governments for six years. There were no requirements that the judges had to be of a certain nationality, simply that they be qualified and that their independence be beyond doubt. The Court was assisted by two Advocates General.
The Consultative Committee (similar to the Economic and Social Committee) had between 30 and 50 members equally divided between producers, workers, consumers and dealers in the coal and steel sector. Again, there was no national quotas, and the treaty requires representatives of European associations to organise their own democratic procedures. They were to establish rules to make their membership fully 'representative' for democratic organised civil society. Members were appointed for two years and were not bound by any mandate or instruction of the organisations which appointed them. The Committee had a plenary assembly, bureau and president. Again, the required democratic procedures were not introduced and nomination of these members remained in the hands of national ministers. The High Authority was obliged to consult the Committee in certain cases where it was appropriate and to keep it informed. The Consultative Committee remained separate (despite the merger of the other institutions) until 2002, when the Treaty expired and its duties were taken over by the Economic and Social Committee (ESC). Despite its independence, the Committee did cooperate with the ESC when they were consulted on the same issue.
Achievements and failures.
Its mission (article 2) was general: to 'contribute to the expansion of the economy, the development of employment and the improvement of the standard of living' of its citizens. The Community had little effect on coal and steel "production", which was influenced more by global trends. Trade between members did increase (tenfold for steel) which saved members' money by not having to import resources from the United States. The High Authority also issued 280 modernization loans to the industry which helped the industry to improve output and reduce costs. Costs were further reduced by the abolition of tariffs at borders.
Among the ECSC's greatest achievements are those on welfare issues. Some mines, for example were clearly unsustainable without government subsidies. Some miners had extremely poor housing. Over 15 years it financed 112,500 flats for workers, paying US$1,770 per flat, enabling workers to buy a home they could not have otherwise afforded. The ECSC also paid half the occupational redeployment costs of those workers who have lost their jobs as coal and steel facilities began to close down. Combined with regional redevelopment aid the ECSC spent $150 million creating 100,000 jobs, a third of which were for unemployed coal and steel workers. The welfare guarantees invented by the ECSC were extended to workers outside the coal and steel sector by some of its members.
Far more important than creating Europe's first social and regional policy, it is argued that the ECSC introduced European peace. It involved the continent's first European tax. This was a flat tax, a levy on production with a maximum rate of one percent. Given that the European Community countries are now experiencing the longest period of peace in more than seventy years , this has been described as the cheapest tax for peace in history. Another world war, or 'world suicide' as Schuman called this threat in 1949, was avoided. In October 1953 Schuman said that the possibility of another European war had been eliminated. Reasoning had to prevail among member states.
However the ECSC failed to achieve several fundamental aims of the Treaty of Paris. It was hoped the ECSC would prevent a resurgence of large coal and steel groups such as the "Konzerne", which helped Adolf Hitler rise to power. In the Cold War trade-offs, the cartels and major companies re-emerged, leading to apparent price fixing (another element that was meant to be tackled). With a democratic supervisory system the worst aspects of past abuse were avoided with the anti-cartel powers of the Authority, the first international anti-cartel agency in the world. Efficient firms were allowed to expand into a European market without undue domination. Oil, gas, electricity became natural competitors to coal and also broke cartel powers. Furthermore, with the move to oil, the Community failed to define a proper energy policy. The Euratom treaty was largely stifled by de Gaulle and the European governments refused the suggestion of an Energy Community involving electricity and other vectors that was suggested at Messina in 1955. In a time of high inflation and monetary instability ECSC also fell short of ensuring an upward equalisation of pay of workers within the market. These failures could be put down to overambition in a short period of time, or that the goals were merely political posturing to be ignored. It has been argued that the greatest achievements of the European Coal and Steel Community lie in its revolutionary democratic concepts of a supranational Community.

</doc>
<doc id="9578" url="https://en.wikipedia.org/wiki?curid=9578" title="European Economic Community">
European Economic Community

The European Economic Community (EEC) was a regional organisation which aimed to bring about economic integration between its member states. It was created by the Treaty of Rome of 1957. Upon the formation of the European Union (EU) in 1993, the EEC was incorporated and renamed as the European Community (EC). In 2009 the EC's institutions were absorbed into the EU's wider framework and the community ceased to exist.
The Community's initial aim was to bring about economic integration, including a common market and customs union, among its six founding members: Belgium, France, Italy, Luxembourg, the Netherlands and West Germany. It gained a common set of institutions along with the European Coal and Steel Community (ECSC) and the European Atomic Energy Community (EURATOM) as one of the European Communities under the 1965 Merger Treaty (Treaty of Brussels). In 1993, a complete single market was achieved, known as the internal market, which allowed for the free movement of goods, capital, services, and people within the EEC. In 1994, the internal market was formalised by the EEA agreement. This agreement also extended the internal market to include most of the member states of the European Free Trade Association, forming the European Economic Area covering 15 countries.
Upon the entry into force of the Maastricht Treaty in 1993, the EEC was renamed the "European Community" to reflect that it covered a wider range than economic policy. This was also when the three European Communities, including the EC, were collectively made to constitute the first of the three pillars of the European Union, which the treaty also founded. The EC existed in this form until it was abolished by the 2009 Treaty of Lisbon, which incorporated the EC's institutions into the EU's wider framework and provided that the EU would "replace and succeed the European Community".
The EEC was also known as the "Common Market" in the English-speaking countries and sometimes referred to as the "European Community" even before it was officially renamed as such in 1993.
History.
Background.
In 1951, the Treaty of Paris was signed, creating the European Coal and Steel Community (ECSC). This was an international community based on supranationalism and international law, designed to help the economy of Europe and prevent future war by integrating its members.
In the aim of creating a federal Europe two further communities were proposed: a European Defence Community and a European Political Community. While the treaty for the latter was being drawn up by the Common Assembly, the ECSC parliamentary chamber, the proposed defence community was rejected by the French Parliament. ECSC President Jean Monnet, a leading figure behind the communities, resigned from the High Authority in protest and began work on alternative communities, based on economic integration rather than political integration. After the Messina Conference in 1955, Paul Henri Spaak was given the task to prepare a report on the idea of a customs union. The so-called Spaak Report of the Spaak Committee formed the cornerstone of the intergovernmental negotiations at Val Duchesse castle in 1956. Together with the Ohlin Report the Spaak Report would provide the basis for the Treaty of Rome.
In 1956, Paul Henri Spaak led the Intergovernmental Conference on the Common Market and Euratom at the Val Duchesse castle, which prepared for the Treaty of Rome in 1957. The conference led to the signature, on 25 March 1957, of the Treaty of Rome establishing a European Economic Community.
Creation and early years.
The resulting communities were the European Economic Community (EEC) and the European Atomic Energy Community (EURATOM or sometimes EAEC). These were markedly less supranational than the previous communities, due to protests from some countries that their sovereignty was being infringed (however there would still be concerns with the behaviour of the Hallstein Commission). The first formal meeting of the Hallstein Commission, was held on 16 January 1958 at the Chateau de Val-Duchesse. The EEC (direct ancestor of the modern Community) was to create a customs union while Euratom would promote co-operation in the nuclear power sphere. The EEC rapidly became the most important of these and expanded its activities. One of the first important accomplishments of the EEC was the establishment (1962) of common price levels for agricultural products. In 1968, internal tariffs (tariffs on trade between member nations) were removed on certain products.
Another crisis was triggered in regard to proposals for the financing of the Common Agricultural Policy, which came into force in 1962. The transitional period whereby decisions were made by unanimity had come to an end, and majority-voting in the Council had taken effect. Then-French President Charles de Gaulle's opposition to supranationalism and fear of the other members challenging the CAP led to an "empty chair policy" whereby French representatives were withdrawn from the European institutions until the French veto was reinstated. Eventually, a compromise was reached with the Luxembourg compromise on 29 January 1966 whereby a gentlemen's agreement permitted members to use a veto on areas of national interest.
On 1 July 1967 when the Merger Treaty came into operation, combining the institutions of the ECSC and Euratom into that of the EEC, they already shared a Parliamentary Assembly and Courts. Collectively they were known as the "European Communities". The Communities still had independent personalities although were increasingly integrated. Future treaties granted the community new powers beyond simple economic matters which had achieved a high level of integration. As it got closer to the goal of political integration and a peaceful and united Europe, what Mikhail Gorbachev described as a "Common European Home".
Enlargement and elections.
The 1960s saw the first attempts at enlargement. In 1961, Denmark, Ireland, Norway and the United Kingdom applied to join the three Communities. However, President Charles de Gaulle saw British membership as a Trojan horse for U.S. influence and vetoed membership, and the applications of all four countries were suspended.
The four countries resubmitted their applications on 11 May 1967 and with Georges Pompidou succeeding Charles de Gaulle as French president in 1969, the veto was lifted. Negotiations began in 1970 under the pro-European government of Edward Heath, who had to deal with disagreements relating to the Common Agricultural Policy and the UK's relationship with the Commonwealth of Nations. Nevertheless, two years later the accession treaties were signed so that Denmark, Ireland and the UK joined the Community from 1 January 1973. The Norwegian people had finally rejected membership in a referendum on 25 September 1972.
The Treaties of Rome had stated that the European Parliament must be directly elected, however this required the Council to agree on a common voting system first. The Council procrastinated on the issue and the Parliament remained appointed, French President Charles de Gaulle was particularly active in blocking the development of the Parliament, with it only being granted Budgetary powers following his resignation.
Parliament pressured for agreement and on 20 September 1976 the Council agreed part of the necessary instruments for election, deferring details on electoral systems which remain varied to this day. During the tenure of President Jenkins, in June 1979, the elections were held in all the then-members (see European Parliament election, 1979). The new Parliament, galvanised by direct election and new powers, started working full-time and became more active than the previous assemblies.
Shortly after its election, Parliament became the first Community institution to propose that the Community adopt the flag of Europe. The European Council agreed to this and adopted the Symbols of Europe as those of the Community in 1984. The European Council, or European summit, had developed since the 1960s as an informal meeting of the Council at the level of heads of state. It had originated from then-French President Charles de Gaulle's resentment at the domination of supranational institutions (e.g. the Commission) over the integration process. It was mentioned in the treaties for the first time in the Single European Act (see below).
Towards Maastricht.
Greece applied to join the community on 12 June 1975, following the restoration of democracy, and joined on 1 January 1981. Following on from Greece, and after their own democratic restoration, Spain and Portugal applied to the communities in 1977 and joined together on 1 January 1986. In 1987 Turkey formally applied to join the Community and began the longest application process for any country.
With the prospect of further enlargement, and a desire to increase areas of co-operation, the Single European Act was signed by the foreign ministers on the 17 and 28 February 1986 in Luxembourg and the Hague respectively. In a single document it dealt with reform of institutions, extension of powers, foreign policy cooperation and the single market. It came into force on 1 July 1987. The act was followed by work on what would be the Maastricht Treaty, which was agreed on 10 December 1991, signed the following year and coming into force on 1 November 1993 establishing the European Union.
European Community.
The EU absorbed the European Communities as one of its three pillars. The EEC's areas of activities were enlarged and were renamed the "European Community", continuing to follow the supranational structure of the EEC. The EEC institutions became those of the EU, however the Court, Parliament and Commission had only limited input in the new pillars, as they worked on a more intergovernmental system than the European Communities. This was reflected in the names of the institutions, the Council was formally the "Council of the "European Union"" while the Commission was formally the "Commission of the "European Communities"".
However, after the Treaty of Maastricht, Parliament gained a much bigger role. Maastricht brought in the codecision procedure, which gave it equal legislative power with the Council on Community matters. Hence, with the greater powers of the supranational institutions and the operation of Qualified Majority Voting in the Council, the Community pillar could be described as a far more federal method of decision making.
The Treaty of Amsterdam transferred responsibility for free movement of persons (e.g., visas, illegal immigration, asylum) from the Justice and Home Affairs (JHA) pillar to the European Community (JHA was renamed Police and Judicial Co-operation in Criminal Matters (PJCC) as a result). Both Amsterdam and the Treaty of Nice also extended codecision procedure to nearly all policy areas, giving Parliament equal power to the Council in the Community.
In 2002, the Treaty of Paris which established the ECSC expired, having reached its 50-year limit (as the first treaty, it was the only one with a limit). No attempt was made to renew its mandate; instead, the Treaty of Nice transferred certain of its elements to the Treaty of Rome and hence its work continued as part of the EC area of the European Community's remit.
After the entry into force of the Treaty of Lisbon in 2009 the pillar structure ceased to exist. The European Community, together with its legal personality, was absorbed into the newly consolidated European Union which merged in the other two pillars (however Euratom remained distinct). This was originally proposed under the European Constitution but that treaty failed ratification in 2005.
Aims and achievements.
The main aim of the EEC, as stated in its preamble, was to "preserve peace and liberty and to lay the foundations of an ever closer union among the peoples of Europe". Calling for balanced economic growth, this was to be accomplished through:
For the customs union, the treaty provided for a 10% reduction in custom duties and up to 20% of global import quotas. Progress on the customs union proceeded much faster than the twelve years planned. However, France faced some setbacks due to their war with Algeria.
Members.
The six states that founded the EEC and the other two Communities were known as the "inner six" (the "outer seven" were those countries who formed the European Free Trade Association). The six were France, West Germany, Italy and the three Benelux countries: Belgium, the Netherlands and Luxembourg. The first enlargement was in 1973, with the accession of Denmark, Ireland and the United Kingdom. Greece, Spain and Portugal joined in the 1980s. The former East Germany became part of the EEC upon German reunification in 1990. Following the creation of the EU in 1993, it has enlarged to include an additional sixteen countries by 2013.
Member states are represented in some form in each institution. The Council is also composed of one national minister who represents their national government. Each state also has a right to one European Commissioner each, although in the European Commission they are not supposed to represent their national interest but that of the Community. Prior to 2004, the larger members (France, Germany, Italy and the United Kingdom) have had two Commissioners. In the European Parliament, members are allocated a set number seats related to their population, however these (since 1979) have been directly elected and they sit according to political allegiance, not national origin. Most other institutions, including the European Court of Justice, have some form of national division of its members.
Institutions.
There were three political institutions which held the executive and legislative power of the EEC, plus one judicial institution and a fifth body created in 1975. These institutions (except for the auditors) were created in 1957 by the EEC but from 1967 onwards they applied to all three Communities. The Council represents governments, the Parliament represents citizens and the Commission represents the European interest. Essentially, the Council, Parliament or another party place a request for legislation to the Commission. The Commission then drafts this and presents it to the Council for approval and the Parliament for an opinion (in some cases it had a veto, depending upon the legislative procedure in use). The Commission's duty is to ensure it is implemented by dealing with the day-to-day running of the Union and taking others to Court if they fail to comply. After the Maastricht Treaty in 1993, these institutions became those of the European Union, though limited in some areas due to the pillar structure. Despite this, Parliament in particular has gained more power over legislation and security of the Commission. The Court was the highest authority in the law, settling legal disputes in the Community, while the Auditors had no power but to investigate.
Background.
The EEC inherited some of the Institutions of the ECSC in that the Common Assembly and Court of Justice of the ECSC had their authority extended to the EEC and Euratom in the same role. However the EEC, and Euratom, had different executive bodies to the ECSC. In place of the ECSC's Council of Ministers was the Council of the European Economic Community, and in place of the High Authority was the Commission of the European Communities.
There was greater difference between these than name: the French government of the day had grown suspicious of the supranational power of the High Authority and sought to curb its powers in favour of the intergovernmental style Council. Hence the Council had a greater executive role in the running of the EEC than was the situation in the ECSC. By virtue of the Merger Treaty in 1967, the executives of the ECSC and Euratom were merged with that of the EEC, creating a single institutional structure governing the three separate Communities. From here on, the term "European Communities" were used for the institutions (for example, from "Commission of the European Economic Community" to the "Commission of the European Communities".
Council.
The Council of the European Communities was a body holding legislative and executive powers and was thus the main decision making body of the Community. Its Presidency rotated between the member states every six months and it is related to the European Council, which was an informal gather of national leaders (started in 1961) on the same basis as the Council.
The Council was composed of one national minister from each member state. However the Council met in various forms depending upon the topic. For example, if agriculture was being discussed, the Council would be composed of each national minister for agriculture. They represented their governments and were accountable to their national political systems. Votes were taken either by majority (with votes allocated according to population) or unanimity. In these various forms they share some legislative and budgetary power of the Parliament. Since the 1960s the Council also began to meet informally at the level of national leaders; these European summits followed the same presidency system and secretariat as the Council but was not a formal formation of it.
Commission.
The Commission of the European Communities was the executive arm of the community, drafting Community law, dealing with the day to running of the Community and upholding the treaties. It was designed to be independent, representing the Community interest, but was composed of national representatives (two from each of the larger states, one from the smaller states). One of its members was the President, appointed by the Council, who chaired the body and represented it.
Parliament.
Under the Community, the European Parliament (formerly the European Parliamentary Assembly) had an advisory role to the Council and Commission. There were a number of Community legislative procedures, at first there was only the consultation procedure, which meant Parliament had to be consulted, although it was often ignored. The Single European Act gave Parliament more power, with the assent procedure giving it a right to veto proposals and the cooperation procedure giving it equal power with the Council if the Council was not unanimous.
In 1970 and 1975, the Budgetary treaties gave Parliament power over the Community budget. The Parliament's members, up-until 1980 were national MPs serving part-time in the Parliament. The Treaties of Rome had required elections to be held once the Council had decided on a voting system, but this did not happen and elections were delayed until 1979 (see European Parliament election, 1979). After that, Parliament was elected every five years. In the following 20 years, it gradually won co-decision powers with the Council over the adoption of legislation, the right to approve or reject the appointment of the Commission President and the Commission as a whole, and the right to approve or reject international agreements entered into by the Community.
Court.
The Court of Justice of the European Communities was the highest court of on matters of Community law and was composed of one judge per state with a president elected from among them. Its role was to ensure that Community law was applied in the same way across all states and to settle legal disputes between institutions or states. It became a powerful institution as Community law overrides national law.
Auditors.
The fifth institution is the "European Court of Auditors", which despite its name had no judicial powers like the Court of Justice. Instead, it ensured that taxpayer funds from the Community budget have been correctly spent. The court provided an audit report for each financial year to the Council and Parliament and gives opinions and proposals on financial legislation and anti-fraud actions. It is the only institution not mentioned in the original treaties, having been set up in 1975.
Policy areas.
At the time of its abolition, the European Community pillar covered the following areas;

</doc>
<doc id="9579" url="https://en.wikipedia.org/wiki?curid=9579" title="EFTA (disambiguation)">
EFTA (disambiguation)

EFTA most often refers to the European Free Trade Association, a European trade bloc which was established as an alternative for European states who did not join the then-European Economic Community.
EFTA may also refer to:

</doc>
<doc id="9580" url="https://en.wikipedia.org/wiki?curid=9580" title="European Free Trade Association">
European Free Trade Association

The European Free Trade Association (EFTA) is an intergovernmental trade organisation and free trade area consisting of four European states: Iceland, Liechtenstein, Norway, and Switzerland. The organisation operates in parallel with the European Union (EU), and all four member states participate in the EU's single market.
The EFTA was established on 3 May 1960 as a trade bloc-alternative for European states who were either unable or unwilling to join the then-European Economic Community (EEC) which has since become the EU. The Stockholm Convention, establishing the EFTA, was signed on 4 January 1960 in the Swedish capital by seven countries (known as the "outer seven"). Today only two founding members remain: Norway and Switzerland. The initial Stockholm Convention was superseded by the Vaduz Convention, with the aim of providing a successful framework for continued expansion and liberalization of trade among the organisation's member states and with the rest of the world.
While the EFTA is not a customs union, it does have a co-ordinated trade policy. As a result, its member states have jointly concluded free trade agreements with a number of other countries. To participate in the EU's single market, Iceland, Liechtenstein and Norway are party to the Agreement on a European Economic Area (EEA), with compliance regulated by the EFTA Surveillance Authority and the EFTA Court. Switzerland instead has a set of bilateral agreements with the EU.
History.
British reaction to the creation of the EEC was mixed and complex. Britain was also preoccupied with the Commonwealth, which, at the time of EFTA's formation, was in transition. Britain therefore brought together several countries, including some bordering the EEC, to form the European Free Trade Association soon after the establishment of the six-nation EEC (France, West Germany, Italy, Belgium, Luxembourg, and the Netherlands).
On 12 January 1960, the Treaty on European Free Trade Association was initialled in the Golden Hall of the Prince's Palace of Stockholm. This established the progressive elimination of customs duties on industrial products, but did not affect agricultural products or maritime trade.
The main difference between the early EEC and the EFTA was the absence of a common external customs tariff, and therefore each EFTA member was free to establish individual customs duties against, or individual free trade agreements with, non-EFTA countries. Despite this modest initiative, the financial results were excellent, as it stimulated an increase of foreign trade volume among its members from 3.5 to 8.2 billion US dollars between 1959 and 1967. This was rather less than the increase enjoyed by countries inside the EEC.
After the accession of Denmark, Ireland and the UK to the EEC in January 1973, EFTA began to falter. For this reason most countries eased or eliminated their trade tariffs in preparation to join the EEC, but experienced declining revenue which reduced the importance of EFTA. Four members remain: Switzerland, Norway, Liechtenstein and Iceland.
Membership.
History.
The founding members of EFTA were Austria, Denmark, Norway, Portugal, Sweden, Switzerland and the United Kingdom. During the 1960s these countries were often referred to as the Outer Seven, as opposed to the Inner Six of the then-European Economic Community (EEC).
Finland became an associate member in 1961 and a full member in 1986, and Iceland joined in 1970. The United Kingdom and Denmark joined the EEC in 1973, and hence ceased to be EFTA members. Portugal also left EFTA for the European Community in 1986. Liechtenstein joined EFTA in 1991 (previously its interests had been represented by Switzerland). Austria, Sweden and Finland joined the EU in 1995 and thus ceased to be EFTA members.
Twice, in 1973 and 1995, the Norwegian government has tried to join the EU (still the EEC in 1973) and by doing so, leave the EFTA. Both times, membership of the EU was rejected in national referendums, keeping Norway in the EFTA. Iceland applied for EU membership in 2009 due to the 2008–2011 Icelandic financial crisis, but has since dropped its bid.
Future.
Between 1994 and 2011, EFTA membership for Andorra, San Marino, Monaco, Isle of Man, Morocco, Turkey, Israel and other ENP partners was discussed.
Microstates.
In November 2012, after the Council of the European Union had called for an evaluation of the EU's relations with the sovereign European microstates of Andorra, Monaco and San Marino, which they described as "fragmented", the European Commission published a report outlining options for their further integration into the EU. Unlike Liechtenstein, which is a member of the EEA via the EFTA and the Schengen Agreement, relations with these three states are based on a collection of agreements covering specific issues. The report examined four alternatives to the current situation: 
The Commission argued that the sectoral approach did not address the major issues and was still needlessly complicated, while EU membership was dismissed in the near future because "the EU institutions are currently not adapted to the accession of such small-sized countries." The remaining options, EEA membership and a FAA with the states, were found to be viable and were recommended by the Commission. In response, the Council requested that negotiations with the three microstates on further integration continue, and that a report be prepared by the end of 2013 detailing the implications of the two viable alternatives and recommendations on how to proceed.
As EEA membership is currently only open to EFTA or EU members, the consent of existing EFTA member states is required for the microstates to join the EEA without becoming members of the EU. In 2011, Jonas Gahr Støre, the then Foreign Minister of Norway which is an EFTA member state, said that EFTA/EEA membership for the microstates was not the appropriate mechanism for their integration into the internal market due to their different requirements from those of large countries such as Norway, and suggested that a simplified association would be better suited for them. Espen Barth Eide, Støre's successor, responded to the Commission's report in late 2012 by questioning whether the microstates have sufficient administrative capabilities to meet the obligations of EEA membership. However, he stated that Norway was open to the possibility of EFTA membership for the microstates if they decide to submit an application, and that the country had not made a final decision on the matter. Pascal Schafhauser, the Counsellor of the Liechtenstein Mission to the EU, said that Liechtenstein, another EFTA member state, was willing to discuss EEA membership for the microstates provided their joining did not impede the functioning of the organization. However, he suggested that the option direct membership in the EEA for the microstates, outside of both the EFTA and the EU, should be given consideration.
On 18 November 2013 the EU Commission concluded that "the participation of the small-sized countries in the EEA is not judged to be a viable option at present due to the political and institutional reasons", and that Association Agreements were a more feasible mechanism to integrate the microstates into the internal market.
Norway.
The Norwegian electorate has rejected treaties of accession to the EU in two referendums. At the time of the first referendum in 1972, their neighbour Denmark joined. Following the second referendum in 1994, two other Nordic neighbours, Sweden and Finland, joined the EU. The last two governments of Norway have been unable and unwilling to advance the question positively, as they have both been coalition governments consisting of proponents and opponents.
Switzerland.
Since Switzerland rejected EEA membership in a referendum in 1992, more referendums on EU membership have been initiated, the last time in 2001. These were all rejected.
Iceland.
On 16 July 2009, the government of Iceland formally applied for EU membership, but the negotiation process has been halted since mid-2013.
Faroe Islands.
In mid-2005, representatives of the Faroe Islands raised the possibility of their territory joining the EFTA. According to Article 56 of the EFTA Convention, only states may become members of the EFTA. The Faroes are a constituent country of the Kingdom of Denmark, and not a sovereign state in their own right. Consequently, they considered the possibility that the "Kingdom of Denmark in respect of the Faroes" could join the EFTA, though the Danish Government has stated that this mechanism would not allow the Faroes to become a separate member of the EEA because Denmark was already a party to the EEA Agreement.
The Faroes already have an extensive bilateral free trade agreement with Iceland, known as the Hoyvík Agreement.
United Kingdom.
The United Kingdom is holding a referendum in 2016 on withdrawing from the European Union. Were the UK to withdraw from the EU, its new relationship with the organization could take several forms. A research paper presented to the Parliament of the United Kingdom proposed a number of alternatives to EU membership which would continue to allow it access to the EU's internal market, including continuing EEA membership as an EFTA member state, or the Swiss model of a number of bilateral treaties covering the provisions of the single market.
Organisation.
EFTA is governed by the EFTA Council and serviced by the EFTA Secretariat. In addition, in connection with the EEA Agreement of 1992, two other EFTA organisations were established, the EFTA Surveillance Authority and the EFTA Court.
Council.
The EFTA Council is the highest governing body of EFTA. The Council usually meets eight times a year at the ambassadorial level (heads of permanent delegations to EFTA) and twice a year at Ministerial level. In the Council meetings, the delegations consult with one another, negotiate and decide on policy issues regarding EFTA. Each Member State is represented and has one vote, though decisions are usually reached through consensus.
The Council discusses substantive matters, especially relating to the development of EFTA relations with third countries and the management of free trade agreements, and keeps under general review relations with the EU third-country policy and administration. It has a broad mandate to consider possible policies to promote the overall objectives of the Association and to facilitate the development of links with other states, unions of states or international organisations. The Council also manages relations between the EFTA States under the EFTA Convention. Questions relating to the EEA are dealt with by the Standing Committee in Brussels.
Secretariat.
The day-to-day running of the Secretariat is headed by the Secretary-General, Kristinn F. Árnason, who is assisted by two Deputy Secretaries-General, one based in Geneva and the other in Brussels. The three posts are shared between the Member State. The division of the Secretariat reflects the division of EFTA’s activities. The Secretariat employs approximately 100 staff members, of whom a third are based in Geneva and two thirds in Brussels and Luxembourg.
The Headquarters in Geneva deals with the management and negotiation of free trade agreements with non-EU countries, and provide support to the EFTA Council.
In Brussels, the Secretariat provides support for the management of the EEA Agreement and assists the Member States in the preparation of new legislation for integration into the EEA Agreement. The Secretariat also assists the Member States in the elaboration of input to EU decision making.
The two duty stations work together closely to implement the Vaduz Convention’s stipulations on the intra-EFTA Free Trade Area.
The EFTA Statistical Office in Luxembourg contributes to the development of a broad and integrated European Statistical System. The EFTA Statistical Office (ESO) is located in the premises of Eurostat, the Statistical Office of the European Union, in Luxembourg, and functions as a liaison office between Eurostat and the EFTA National Statistical Institutes. ESO's main objective is to promote the full inclusion of the EFTA States in the European Statistical System, thus providing harmonised and comparable statistics to support the general cooperation process between EFTA and the EU within and outside the EEA Agreement. The cooperation also entails technical cooperation programmes with third countries and training of European statisticians.
European Economic Area.
A Joint Committee consisting of the EEA States plus the European Commission (representing the EU) has the function of extending relevant EU law to the non EU members. An EEA Council meets twice yearly to govern the overall relationship between the EEA members.
Rather than setting up pan-EEA institutions, the activities of the EEA are regulated by the EFTA Surveillance Authority and the EFTA Court. The EFTA Surveillance Authority and the EFTA Court regulate the activities of the EFTA members in respect of their obligations in the European Economic Area (EEA). Since Switzerland is not an EEA member, it does not participate in these institutions.
The EFTA Surveillance Authority performs the European Commission's role as "guardian of the treaties" for the EFTA countries, while the EFTA Court performs the European Court of Justice's role for those countries.
The original plan for the EEA lacked the EFTA Court or the EFTA Surveillance Authority, the European Court of Justice and the European Commission were to exercise those roles. However, during the negotiations for the EEA agreement, the European Court of Justice informed the Council of the European Union by way of letter that they considered that giving the EU institutions powers with respect to non-EU member states would be a violation of the treaties, and therefore the current arrangement was developed instead.
Norway Grants.
The EEA and Norway Grants are the financial contributions of Iceland, Liechtenstein and Norway to reduce social and economic disparities in Europe. They were established in conjunction with the 2004 enlargement of the European Economic Area (EEA), which brought together the EU, Iceland, Liechtenstein and Norway in the Internal Market. In the period from 2004 to 2009, €1.3 billion of project funding was made available for project funding in the 15 beneficiary states in Central and Southern Europe. The EEA and Norway Grants are administered by the Financial Mechanism Office, which is affiliated to the EFTA Secretariat in Brussels.
Locations.
The EFTA Secretariat is headquartered in Geneva, Switzerland, but also has duty stations in Brussels, Belgium and Luxembourg. The EFTA Surveillance Authority has its headquarters in Brussels, Belgium (the same location as the headquarters of the European Commission), while the EFTA Court has its headquarters in Luxembourg (the same location as the headquarters of the European Court of Justice).
Portugal Fund.
The Portugal Fund was established in 1975 when Portugal was still a member of EFTA, to provide funding for the development and reconstruction of Portugal after the Carnation Revolution. When Portugal left EFTA in 1985 to join the EEC, the remaining EFTA members decided to nonetheless continue the Portugal Fund, so Portugal would continue to benefit from it. The Fund originally took the form of a low-interest loan from the EFTA member states to Portugal, to the value of 100 million US dollars. Repayment was originally to commence in 1988, but EFTA then decided to postpone the start of repayments until 1998. The Portugal Fund has now been dissolved by the Member States.
European Union.
Except for Switzerland, the EFTA members are also members of the European Economic Area (EEA). The EEA comprises three member states of the European Free Trade Association (EFTA) and 28 member states of the European Union (EU), including Croatia which is provisionally applying the agreement pending its ratification by all EEA countries. It was established on 1 January 1994 following an agreement with the European Community (which had become the EU two months earlier). It allows the EFTA-EEA states to participate in the EU's Internal Market without being members of the EU. They adopt almost all EU legislation related to the single market, except laws on agriculture and fisheries. However, they also contribute to and influence the formation of new EEA relevant policies and legislation at an early stage as part of a formal decision-shaping process. One EFTA member, Switzerland, has not joined the EEA but has a series of bilateral agreements, including a free trade agreement, with the EU.
The following table summarises the various components of EU laws applied in the EFTA countries and their sovereign territories. Some territories of EU member states also have a special status in regard to EU laws applied as is the case with some European microstates.
International conventions.
EFTA also originated the Hallmarking Convention and the Pharmaceutical Inspection Convention, both of which are open to non-EFTA states.
International relationships.
EFTA has several free trade agreements with non-EU countries as well as declarations on cooperation and joint workgroups to improve trade. Currently, the EFTA States have established preferential trade relations with 24 states and territories, in addition to the 28 member states of the European Union.
Travel freedom of EFTA citizens and visa policy of EFTA countries.
A citizen of an EFTA country can live and work in all other EFTA countries and in all EU countries, and a citizen of an EU country can live and work in all EFTA countries (but for voting and working in sensitive fields, such as government / police / military, citizenship is often required, and non-citizens may not have the same rights to welfare and unemployment benefits as citizens).
Since each EFTA and EU country can make its own citizenship laws, dual citizenship is not always possible. Of the EFTA countries, Iceland and Switzerland allow it (in Switzerland, conditions for the naturalization of immigrants vary regionally), but Norway only in exceptional cases, and Liechtenstein only for citizens by descent, but not for foreigners wanting to naturalize.
Some non-EFTA/non-EU countries do not allow dual citizenship either, so immigrants wanting to naturalize must sometimes renounce their old citizenship. See also Multiple citizenship and the nationality laws of the countries in question for more details.
Like EU citizens, EFTA citizens enjoy a high degree of travel freedom. In the ""Henley Visa Restrictions Index 2016"," the rankings of the EFTA passports were as follows (For details, click on the name of the country):
The four EFTA countries belong to the Schengen Area and use its visa policy.

</doc>
<doc id="9581" url="https://en.wikipedia.org/wiki?curid=9581" title="European Parliament">
European Parliament

The European Parliament (EP) is the directly elected parliamentary institution of the European Union (EU). Together with the Council of the European Union (the Council) and the European Commission, it exercises the legislative function of the EU. The Parliament is composed of 751 (previously 766) members, who represent the second largest democratic electorate in the world (after the Parliament of India) and the largest trans-national democratic electorate in the world (375 million eligible voters in 2009).
It has been directly elected every five years by universal suffrage since 1979. However, turnout at European Parliament elections has fallen consecutively at each election since that date, and has been under 50% since 1999. Turnout in 2014 stood at 42.54% of all European voters.
Although the European Parliament has legislative power that the Council and Commission do not possess, it does not formally possess legislative initiative, as most national parliaments of European Union member states do. The Parliament is the "first institution" of the EU (mentioned first in the treaties, having ceremonial precedence over all authority at European level), and shares equal legislative and budgetary powers with the Council (except in a few areas where the special legislative procedures apply). It likewise has equal control over the EU budget. Finally, the European Commission, the executive body of the EU, is accountable to Parliament. In particular, Parliament elects the President of the Commission, and approves (or rejects) the appointment of the Commission as a whole. It can subsequently force the Commission as a body to resign by adopting a motion of censure.
The President of the European Parliament (Parliament's speaker) is Martin Schulz (S&D), elected in January 2012. He presides over a multi-party chamber, the two largest groups being the Group of the European People's Party (EPP) and the Progressive Alliance of Socialists and Democrats (S&D). The last union-wide elections were the 2014 elections. The European Parliament has three places of work – Brussels (Belgium), the city of Luxembourg (Luxembourg) and Strasbourg (France).
Luxembourg is home to the administrative offices (the 'General Secretariat'). Meetings of the whole Parliament ('plenary sessions') take place in Strasbourg and in Brussels. Committee meetings are held in Brussels.
History.
The Parliament, like the other institutions, was not designed in its current form when it first met on 10 September 1952. One of the oldest common institutions, it began as the "Common Assembly" of the European Coal and Steel Community (ECSC). It was a consultative assembly of 78 appointed parliamentarians drawn from the national parliaments of member states (see dual mandate), having no legislative powers. The change since its foundation was highlighted by Professor David Farrell of the University of Manchester;
Its development since its foundation is a testament to the evolution of the Union's structures without one clear "master plan". Some such as Tom Reid of the "Washington Post" said of the union, "nobody would have deliberately designed a government as complex and as redundant as the EU". Even the Parliament's two seats, which have switched several times, are a result of various agreements or lack of agreements. Although most MEPs would prefer to be based just in Brussels, at John Major's 1992 Edinburgh summit, France engineered a treaty amendment to maintain Parliament's plenary seat permanently at Strasbourg.
Consultative assembly.
The body was not mentioned in the original Schuman Declaration. It was assumed or hoped that difficulties with the British would be resolved to allow the Council of Europe's Assembly to perform the task. A separate Assembly was introduced during negotiations on the Treaty as an institution which would counterbalance and monitor the executive while providing democratic legitimacy. The wording of the ECSC Treaty demonstrated the leaders' desire for more than a normal consultative assembly by using the term "representatives of the people" and allowed for direct election. Its early importance was highlighted when the Assembly was given the task of drawing up the draft treaty to establish a European Political Community. In this, the Ad Hoc Assembly was established on 13 September 1952 with extra members but after the failure of the proposed European Defence Community the project was dropped. 
Despite this the European Economic Community and Euratom were established in 1958 by the Treaties of Rome. The Common Assembly was shared by all three communities (which had separate executives) and it renamed itself the "European Parliamentary Assembly". The first meeting was held on 19 March 1958 having been set up in Luxembourg, it elected Schuman as its president and on 13 May it rearranged itself to sit according to political ideology rather than nationality. This is seen as the birth of the modern European Parliament, with Parliament's 50 years celebrations being held in March 2008 rather than 2002.
The three communities merged their remaining organs as the European Communities in 1967 and the body was renamed to the current "European Parliament" in 1962. In 1970 the Parliament was granted power over areas of the Community's budget, which were expanded to the whole budget in 1975. Under the Rome Treaties, the Parliament should have become elected. However, the Council was required to agree a uniform voting system beforehand, which it failed to do. The Parliament threatened to take the Council to the European Court of Justice leading to a compromise whereby the Council would agree to elections, but the issue of voting systems would be put off till a later date.
Elected Parliament.
In 1979, its members were directly elected for the first time. This sets it apart from similar institutions such as those of the Parliamentary Assembly of the Council of Europe or Pan-African Parliament which are appointed. After that first election, the parliament held its first session on 11 July 1979, electing Simone Veil MEP as its President. Veil was also the first female President of the Parliament since it was formed as the Common Assembly.
As an elected body, the Parliament began to draft proposals addressing the functioning of the EU. For example, in 1984, inspired by its previous work on the Political Community, it drafted the "draft Treaty establishing the European Union" (also known as the 'Spinelli Plan' after its rapporteur Altiero Spinelli MEP). Although it was not adopted, many ideas were later implemented by other treaties. Furthermore, the Parliament began holding votes on proposed Commission Presidents from the 1980s, before it was given any formal right to veto.
Since the election the membership of the European Parliament has simply expanded whenever new nations have joined (the membership was also adjusted upwards in 1994 after German reunification). Following this the Treaty of Nice imposed a cap on the number of members to be elected, 732. 
Like the other institutions, the Parliament's seat was not yet fixed. The provisional arrangements placed Parliament in Strasbourg, while the Commission and Council had their seats in Brussels. In 1985 the Parliament, wishing to be closer to these institutions, built a second chamber in Brussels and moved some of its work there despite protests from some states. A final agreement was eventually reached by the European Council in 1992. It stated the Parliament would retain its formal seat in Strasbourg, where twelve sessions a year would be held, but with all other parliamentary activity in Brussels. This two seat arrangement was contested by Parliament but was later enshrined in the Treaty of Amsterdam. To this day the institution's locations are a source of contention.
The Parliament gained more powers from successive treaties, namely through the extension of the ordinary legislative procedure (then called the codecision procedure), and in 1999, the Parliament forced the resignation of the Santer Commission. The Parliament had refused to approve the Community budget over allegations of fraud and mis-management in the Commission. The two main parties took on a government-opposition dynamic for the first time during the crisis which ended in the Commission resigning en masse, the first of any forced resignation, in the face of an impending censure from the Parliament.
Barroso I.
In 2004, following the largest trans-national election in history, despite the European Council choosing a President from the largest political group (the EPP), the Parliament again exerted pressure on the Commission. During the Parliament's hearings of the proposed Commissioners MEPs raised doubts about some nominees with the Civil liberties committee rejecting Rocco Buttiglione from the post of Commissioner for Justice, Freedom and Security over his views on homosexuality. That was the first time the Parliament had ever voted against an incoming Commissioner and despite Barroso's insistence upon Buttiglione the Parliament forced Buttiglione to be withdrawn. A number of other Commissioners also had to be withdrawn or reassigned before Parliament allowed the Barroso Commission to take office.
Along with the extension of the ordinary legislative procedure, the Parliament's democratic mandate has given it greater control over legislation against the other institutions. In voting on the Bolkestein directive in 2006, the Parliament voted by a large majority for over 400 amendments that changed the fundamental principle of the law. The "Financial Times" described it in the following terms:
In 2007, for the first time, Justice Commissioner Franco Frattini included Parliament in talks on the second Schengen Information System even though MEPs only needed to be consulted on parts of the package. After that experiment, Frattini indicated he would like to include Parliament in all justice and criminal matters, informally pre-empting the new powers they could gain as part of the Treaty of Lisbon. Between 2007 and 2009, a special working group on parliamentary reform implemented a series of changes to modernise the institution such as more speaking time for rapporteurs, increase committee co-operation and other efficiency reforms.
Recent history.
The Lisbon Treaty finally came into force on 1 December 2009, granting Parliament powers over the entire of the EU budget, making Parliament's legislative powers equal to the Council's in nearly all areas and linking the appointment of the Commission President to Parliament's own elections. Despite some calls for the parties to put forward candidates beforehand, only the EPP (which had re-secured their position as largest party) had one in re-endorsing Barroso.
Barroso gained the support of the European Council for a second term and secured majority support from the Parliament in September 2009. Parliament voted 382 votes in favour and 219 votes against (117 abstentions ) with support of the European People's Party, European Conservatives and Reformists and the Alliance of Liberals and Democrats for Europe. The liberals gave support after Barroso gave them a number of concessions; the liberals previously joined the socialists' call for a delayed vote (the EPP had wanted to approve Barroso in July of that year).
Once Barroso put forward the candidates for his next Commission, another opportunity to gain concessions arose. Bulgarian nominee Rumiana Jeleva was forced to step down by Parliament due to concerns over her experience and financial interests. She only had the support of the EPP which began to retaliate on left wing candidates before Jeleva gave in and was replaced (setting back the final vote further).
Before the final vote, Parliament demanded a number of concessions as part of a future working agreement under the new Lisbon Treaty. The deal includes that Parliament's President will attend high level Commission meetings. Parliament will have a seat in the EU's Commission-lead international negotiations and have a right to information on agreements. However, Parliament secured only an observer seat. Parliament also did not secure a say over the appointment of delegation heads and special representatives for foreign policy. Although they will appear before parliament after they have been appointed by the High Representative. One major internal power was that Parliament wanted a pledge from the Commission that it would put forward legislation when parliament requests. Barroso considered this an infringement on the Commission's powers but did agree to respond within three months. Most requests are already responded to positively.
During the setting up of the European External Action Service (EEAS), Parliament used its control over the EU budget to influence the shape of the EEAS. MEPs had aimed at getting greater oversight over the EEAS by linking it to the Commission and having political deputies to the High Representative. MEPs didn't manage to get everything they demanded. However, they got broader financial control over the new body.
Powers and functions.
The Parliament and Council can be regarded as two chambers in a bicameral legislative branch of the European Union, with law-making power being officially distributed equally between both parliamentary chambers. However, there are some differences from national legislatures; for example, neither the Parliament nor the Council have the power of legislative initiative (except for the fact that the Council has the power in some intergovernmental matters). In Community matters, this is a power uniquely reserved for the European Commission (the executive). Therefore, while Parliament can amend and reject legislation, to make a proposal for legislation, it needs the Commission to draft a bill before anything can become law. The value of such a power has been questioned by noting that in the national legislatures of the member states 85% of initiatives introduced without executive support fail to become law. Yet it has been argued by former Parliament president Hans-Gert Pöttering that as the Parliament does have the right to ask the Commission to draft such legislation, and as the Commission is following Parliament's proposals more and more Parliament does have a "de facto" right of legislative initiative.
The Parliament also has a great deal of indirect influence, through non-binding resolutions and committee hearings, as a "pan-European soapbox" with the ear of thousands of Brussels-based journalists. There is also an indirect effect on foreign policy; the Parliament must approve all development grants, including those overseas. For example, the support for post-war Iraq reconstruction, or incentives for the cessation of Iranian nuclear development, must be supported by the Parliament. Parliamentary support was also required for the transatlantic passenger data-sharing deal with the United States. Finally, Parliament holds a non-binding vote on new EU treaties but cannot veto it. However, when Parliament threatened to vote down the Nice Treaty, the Belgian and Italian Parliaments said they would veto the treaty on the European Parliament's behalf.
Legislative procedure.
With each new treaty, the powers of the Parliament, in terms of its role in the Union's legislative procedures, have expanded. The procedure which has slowly become dominant is the "ordinary legislative procedure" (previously named "codecision procedure"), which provides an equal footing between Parliament and Council. In particular, under the procedure, the Commission presents a proposal to Parliament and the Council which can only become law if both agree on a text, which they do (or not) through successive readings up to a maximum of three. In its first reading, Parliament may send amendments to the Council which can either adopt the text with those amendments or send back a "common position". That position may either be approved by Parliament, or it may reject the text by an absolute majority, causing it to fail, or it may adopt further amendments, also by an absolute majority. If the Council does not approve these, then a "Conciliation Committee" is formed. The Committee is composed of the Council members plus an equal number of MEPs who seek to agree a compromise. Once a position is agreed, it has to be approved by Parliament, by a simple majority. This is also aided by Parliament's mandate as the only directly democratic institution, which has given it leeway to have greater control over legislation than other institutions, for example over its changes to the Bolkestein directive in 2006.
The few other areas that operate the "special legislative procedures" are justice & home affairs, budget and taxation and certain aspects of other policy areas: such as the fiscal aspects of environmental policy. In these areas, the Council or Parliament decide law alone. The procedure also depends upon which type of institutional act is being used. The strongest act is a regulation, an act or law which is directly applicable in its entirety. Then there are directives which bind member states to certain goals which they must achieve. They do this through their own laws and hence have room to manoeuvre in deciding upon them. A decision is an instrument which is focused at a particular person or group and is directly applicable. Institutions may also issue recommendations and opinions which are merely non-binding, declarations. There is a further document which does not follow normal procedures, this is a "written declaration" which is similar to an early day motion used in the Westminster system. It is a document proposed by up to five MEPs on a matter within the EU's activities used to launch a debate on that subject. Having been posted outside the entrance to the hemicycle, members can sign the declaration and if a majority do so it is forwarded to the President and announced to the plenary before being forwarded to the other institutions and formally noted in the minutes.
Budget.
The legislative branch officially holds the Union's budgetary authority with powers gained through the Budgetary Treaties of the 1970s and the Lisbon Treaty. The EU budget is subject to a form of the ordinary legislative procedure with a single reading giving Parliament power over the entire budget (before 2009, its influence was limited to certain areas) on an equal footing to the Council. If there is a disagreement between them, it is taken to a conciliation committee as it is for legislative proposals. If the joint conciliation text is not approved, the Parliament may adopt the budget definitively.
The Parliament is also responsible for discharging the implementation of previous budgets based on the annual report of the European Court of Auditors. It has refused to approve the budget only twice, in 1984 and in 1998. On the latter occasion it led to the resignation of the Santer Commission; highlighting how the budgetary power gives Parliament a great deal of power over the Commission. Parliament also makes extensive use of its budgetary, and other powers, elsewhere; for example in the setting up of the European External Action Service, Parliament has a de facto veto over its design as it has to approve the budgetary and staff changes.
Control of the executive.
Unlike most EU states, which usually operate parliamentary systems, there is a separation of powers between the executive and legislative which makes the European Parliament more akin to the United States Congress than an EU state legislature. The President of the European Commission is proposed by the European Council on the basis of the European elections to Parliament. That proposal has to be approved by the Parliament (by a simple majority) who "elect" the President according to the treaties. Following the approval of the Commission President, the members of the Commission are proposed by the President in accord with the member-states. Each Commissioner comes before a relevant parliamentary committee hearing covering the proposed portfolio. They are then, as a body, approved or rejected by the Parliament. In practice, the Parliament has never voted against a President or his Commission, but it did seem likely when the Barroso Commission was put forward. The resulting pressure forced the proposal to be withdrawn and changed to be more acceptable to parliament. That pressure was seen as an important sign by some of the evolving nature of the Parliament and its ability to make the Commission accountable, rather than being a rubber stamp for candidates. Furthermore, in voting on the Commission, MEPs also voted along party lines, rather than national lines, despite frequent pressure from national governments on their MEPs. This cohesion and willingness to use the Parliament's power ensured greater attention from national leaders, other institutions and the public—who previously gave the lowest ever turnout for the Parliament's elections.
The Parliament also has the power to censure the Commission if they have a two-thirds majority which will force the resignation of the entire Commission from office. As with approval, this power has never been used but it was threatened to the Santer Commission, who subsequently resigned of their own accord. There are a few other controls, such as: the requirement of Commission to submit reports to the Parliament and answer questions from MEPs; the requirement of the President-in-office of the Council to present its programme at the start of their presidency; the obligation on the President of the European Council to report to Parliament after each of its meetings; the right of MEPs to make requests for legislation and policy to the Commission; and the right to question members of those institutions (e.g. "Commission Question Time" every Tuesday). At present, MEPs may ask a question on any topic whatsoever, but in July 2008 MEPs voted to limit questions to those within the EU's mandate and ban offensive or personal questions.
Supervisory powers.
The Parliament also has other powers of general supervision, mainly granted by the Maastricht Treaty. The Parliament has the power to set up a Committee of Inquiry, for example over mad cow disease or CIA detention flights—the former led to the creation of the European veterinary agency. The Parliament can call other institutions to answer questions and if necessary to take them to court if they break EU law or treaties. Furthermore, it has powers over the appointment of the members of the Court of Auditors and the president and executive board of the European Central Bank. The ECB president is also obliged to present an annual report to the parliament.
The European Ombudsman is elected by the Parliament, who deals with public complaints against all institutions. Petitions can also be brought forward by any EU citizen on a matter within the EU's sphere of activities. The Committee on Petitions hears cases, some 1500 each year, sometimes presented by the citizen themselves at the Parliament. While the Parliament attempts to resolve the issue as a mediator they do resort to legal proceedings if it is necessary to resolve the citizens dispute.
Members.
The parliamentarians are known in English as Members of the European Parliament (MEPs). They are elected every five years by universal adult suffrage and sit according to political allegiance; about a third are women. Before 1979 they were appointed by their national parliaments.
Under the Lisbon Treaty, seats are allocated to each state according to population and the maximum number of members is set at 751 (however, as the President cannot vote while in the chair there will only be 750 voting members at any one time).
The seats are distributed according to "degressive proportionality", i.e., the larger the state, the more citizens are represented per MEP. As a result, Maltese and Luxembourgish voters have roughly 10x more influence per voter than citizens of the six large countries.
, Germany (80.9 million inhabitants) has 96 seats (previously 99 seats), i.e. one seat for 843,000 inhabitants. Malta (0.4 million inhabitants) has 6 seats, i.e. one seat for 70,000 inhabitants.
The new system implemented under the Lisbon Treaty, including revising the seating well before elections, was intended to avoid political horse trading when the allocations have to be revised to reflect demographic changes.
Pursuant to this apportionment, the constituencies are formed. In six EU member states (Belgium, France, Ireland, Italy, Poland, and the United Kingdom), the national territory is divided into a number of constituencies. In the remaining member states, the whole country forms a single constituency. All member states hold elections to the European Parliament using various forms of proportional representation.
Transitional arrangements.
Due to the delay in ratifying the Lisbon Treaty, the seventh parliament was elected under the lower Nice Treaty cap. A small scale treaty amendment was ratified on 29 November 2011. This amendment brought in transitional provisions to allow the 18 additional MEPs created under the Lisbon Treaty to be elected or appointed before the 2014 election. Under the Lisbon Treaty reforms, Germany was the only state to lose members from 99 to 96. However, these seats were not removed until the 2014 election.
Salaries and expenses.
Before 2009, members received the same salary as members of their national parliament. However, from 2009 a new members statute came into force, after years of attempts, which gave all members an equal monthly pay, of 8,020.53 euro each in 2014, subject to a European Union tax and which can also be taxed nationally. MEPs are entitled to a pension, paid by Parliament, from the age of 63. Members are also entitled to allowances for office costs and subsistence, and travelling expenses, based on actual cost. Besides their pay, members are granted a number of privileges and immunities. To ensure their free movement to and from the Parliament, they are accorded by their own states the facilities accorded to senior officials travelling abroad and, by other state governments, the status of visiting foreign representatives. When in their own state, they have all the immunities accorded to national parliamentarians, and, in other states, they have immunity from detention and legal proceedings. However, immunity cannot be claimed when a member is found committing a criminal offence and the Parliament also has the right to strip a member of their immunity.
Political groups.
MEPs in Parliament are organised into seven different parliamentary groups, including thirty non-attached members known as "non-inscrits". The two largest groups are the European People's Party (EPP) and the Socialists & Democrats (S&D). These two groups have dominated the Parliament for much of its life, continuously holding between 50 and 70 percent of the seats between them. No single group has ever held a majority in Parliament. As a result of being broad alliances of national parties, European group parties are very decentralised and hence have more in common with parties in federal states like Germany or the United States than unitary states like the majority of the EU states. Nevertheless, the European groups were actually more cohesive than their US counterparts between 2004 and 2009.
Groups are often based on a single European political party such as the socialist group (before 2009). However, they can, like the liberal group, include more than one European party as well as national parties and independents. For a group to be recognised, it needs 25 MEPs from seven different countries. Once recognised, groups receive financial subsidies from the parliament and guaranteed seats on committees, creating an incentive for the formation of groups. However, some controversy occurred with the establishment of the short-lived Identity, Tradition, Sovereignty (ITS) due to its ideology; the members of the group were far-right, so there were concerns about public funds going towards such a group. There were attempts to change the rules to block the formation of ITS, but they never came to fruition. The group was, however, blocked from gaining leading positions on committees — traditionally (by agreement, not a rule) shared among all parties. When this group engaged in infighting, leading to the withdrawal of some members, its size fell below the threshold for recognition causing its collapse.
Grand coalition.
Given that the Parliament does not form the government in the traditional sense of a Parliamentary system, its politics have developed along more consensual lines rather than majority rule of competing parties and coalitions. Indeed, for much of its life it has been dominated by a grand coalition of the European People's Party and the Party of European Socialists. The two major parties tend to co-operate to find a compromise between their two groups leading to proposals endorsed by huge majorities. However, this does not always produce agreement, and each may instead try to build other alliances, the EPP normally with other centre-right or right wing Groups and the PES with centre-left or left wing Groups. Sometimes, the Liberal Group is then in the pivotal position. There are also occasions where very sharp party political divisions have emerged, for example over the resignation of the Santer Commission.
When the initial allegations against the Commission emerged, they were directed primarily against Édith Cresson and Manuel Marín, both socialist members. When the parliament was considering refusing to discharge the Community budget, President Jacques Santer stated that a no vote would be tantamount to a vote of no confidence. The Socialist group supported the Commission and saw the issue as an attempt by the EPP to discredit their party ahead of the 1999 elections. Socialist leader, Pauline Green MEP, attempted a vote of confidence and the EPP put forward counter motions. During this period the two parties took on similar roles to a government-opposition dynamic, with the Socialists supporting the executive and EPP renouncing its previous coalition support and voting it down. Politicisation such as this has been increasing, in 2007 Simon Hix of the London School of Economics noted that:
During the fifth term, 1999 to 2004, there was a break in the grand coalition resulting in a centre-right coalition between the Liberal and People's parties. This was reflected in the Presidency of the Parliament with the terms being shared between the EPP and the ELDR, rather than the EPP and Socialists. In the following term the liberal group grew to hold 88 seats, the largest number of seats held by any third party in Parliament.
Elections.
Elections have taken place, directly in every member-state, every five years since 1979. there have been eight elections. When a nation joins mid-term, a by-election will be held to elect their representatives. This has happened six times, most recently when Croatia joined in 2013 (see below). Elections take place across four days according to local custom and, apart from having to be proportional, the electoral system is chosen by the member-state. This includes allocation of sub-national constituencies; while most members have a national list, some, like the UK and France, divide their allocation between regions. Seats are allocated to member-states according to their population, since 2014 with no state having more than 96, but no fewer than 6, to maintain proportionality.
The most recent Union-wide elections to the European Parliament were the European elections of 2014, held from 22 to 25 May 2014. They were the largest simultaneous transnational elections ever held anywhere in the world.
The eighth term of Parliament started on 1 July 2014.
The proportion of MEPs elected in 2009 who were female was 35%; in 1979 it was just 16.5%.
There have been a number of proposals designed to attract greater public attention to the elections. One such innovation in the 2014 elections was that the pan-European political parties fielded "candidates" for president of the Commission. However, European Union governance is based on a mixture of intergovernmental and supranational features: the President of the European Commission is nominated by the European Council, representing the governments of the member states, and there is no obligation for them to nominate the successful "candidate". The Lisbon Treaty merely states that they should take account of the results of the elections when choosing whom to nominate. The so-called "candidates" were Jean-Claude Juncker for the European People's Party, Martin Schulz for the Party of European Socialists, Guy Verhofstadt for the Alliance of Liberals and Democrats for Europe Party, Ska Keller and José Bové jointly for the European Green Party and Alexis Tsipras for the Party of the European Left.
Turnout has dropped consistently every year since the first election, and from 1999 it has been below 50%. In 2007 both Bulgaria and Romania elected their MEPs in by-elections, having joined at the beginning of 2007. The Bulgarian and Romanian elections saw two of the lowest turnouts for European elections, just 28.6% and 28.3% respectively.
In England, Scotland and Wales, EP elections were originally held for a constituency MEP on a first-past-the-post basis. In 1999 the system was changed to a form of PR where a large group of candidates would stand for a post within a very large regional constituency. One could vote for a party, but not a candidate (unless that party had a single candidate).
Proceedings.
Each year the activities of the Parliament cycle between committee weeks where reports are discussed in committees and interparliamentary delegations meet, political group weeks for members to discuss work within their political groups and session weeks where members spend 3½ days in Strasbourg for part-sessions. In addition six 2-day part-sessions are organised in Brussels throughout the year. Four weeks are allocated as constituency week to allow members to do exclusively constituency work. Finally there are no meetings planned during the summer weeks. The Parliament has the power to meet without being convened by another authority. Its meetings are partly controlled by the treaties but are otherwise up to Parliament according to its own "Rules of Procedure" (the regulations governing the parliament).
During sessions, members may speak after being called on by the President. Members of the Council or Commission may also attend and speak in debates. Partly due to the need for translation, and the politics of consensus in the chamber, debates tend to be calmer and more polite than, say, the Westminster system. Voting is conducted primarily by a show of hands, that may be checked on request by electronic voting. Votes of MEPs are not recorded in either case, however; that only occurs when there is a roll-call ballot. This is required for the final votes on legislation and also whenever a political group or 30 MEPs request it. The number of roll-call votes has increased with time. Votes can also be a completely secret ballot (for example, when the president is elected). All recorded votes, along with minutes and legislation, are recorded in the Official Journal of the European Union and can be accessed online. Votes usually do not follow a debate, but rather they are grouped with other due votes on specific occasions, usually at noon on Tuesdays, Wednesdays or Thursdays. This is because the length of the vote is unpredictable and if it continues for longer than allocated it can disrupt other debates and meetings later in the day.
Members are arranged in a hemicycle according to their political groups (in the Common Assembly, prior to 1958, members sat alphabetically) who are ordered mainly by left to right, but some smaller groups are placed towards the outer ring of the Parliament. All desks are equipped with microphones, headphones for translation and electronic voting equipment. The leaders of the groups sit on the front benches at the centre, and in the very centre is a podium for guest speakers. The remaining half of the circular chamber is primarily composed of the raised area where the President and staff sit. Further benches are provided between the sides of this area and the MEPs, these are taken up by the Council on the far left and the Commission on the far right. Both the Brussels and Strasbourg hemicycle roughly follow this layout with only minor differences. The hemicycle design is a compromise between the different Parliamentary systems. The British-based system has the different groups directly facing each other while the French-based system is a semicircle (and the traditional German system had all members in rows facing a rostrum for speeches). Although the design is mainly based on a semicircle, the opposite ends of the spectrum do still face each other. With access to the chamber limited, entrance is controlled by ushers who aid MEPs in the chamber (for example in delivering documents). The ushers can also occasionally act as a form of police in enforcing the President, for example in ejecting an MEP who is disrupting the session (although this is rare). The first head of protocol in the Parliament was French, so many of the duties in the Parliament are based on the French model first developed following the French Revolution. The 180 ushers are highly visible in the Parliament, dressed in black tails and wearing a silver chain, and are recruited in the same manner as the European civil service. The President is allocated a personal usher.
President and organisation.
The President is essentially the speaker of the Parliament and presides over the plenary when it is in session. The President's signature is required for all acts adopted by co-decision, including the EU budget. The President is also responsible for representing the Parliament externally, including in legal matters, and for the application of the rules of procedure. He or she is elected for two-and-a-half-year terms, meaning two elections per parliamentary term. The President is currently Martin Schulz MEP of the S&D.
In most countries, the protocol of the head of state comes before all others; however, in the EU the Parliament is listed as the first institution, and hence the protocol of its President comes before any other European, or national, protocol. The gifts given to numerous visiting dignitaries depend upon the President. President Josep Borrell MEP of Spain gave his counterparts a crystal cup created by an artist from Barcelona who had engraved upon it parts of the Charter of Fundamental Rights among other things.
A number of notable figures have been President of the Parliament and its predecessors. The first President was Paul-Henri Spaak MEP, one of the founding fathers of the Union. Other founding fathers include Alcide de Gasperi MEP and Robert Schuman MEP. The two female Presidents were Simone Veil MEP in 1979 (first President of the elected Parliament) and Nicole Fontaine MEP in 1999, both Frenchwomen. The previous president, Jerzy Buzek was the first East-Central European to lead an EU institution, a former Prime Minister of Poland who rose out of the Solidarity movement in Poland that helped overthrow communism in the Eastern Bloc.
During the election of a President, the previous President (or, if unable to, one of the previous Vice-Presidents) presides over the chamber. Prior to 2009, the oldest member fulfilled this role but the rule was changed to prevent far-right French MEP Jean-Marie Le Pen taking the chair.
Below the President, there are 14 Vice-Presidents who chair debates when the President is not in the chamber. There are a number of other bodies and posts responsible for the running of parliament besides these speakers. The two main bodies are the Bureau, which is responsible for budgetary and administration issues, and the Conference of Presidents which is a governing body composed of the presidents of each of the parliament's political groups. Looking after the financial and administrative interests of members are five Quaestors. In August 2002, Nichole Robichaux ée Braucksieke became the first American citizen to intern for a member of the European Parliament—Monica Frassoni reen Part.
, the European Parliament budget was EUR 1.756 billion. A 2008 report on the Parliament's finances highlighted certain overspending and miss-payments. Despite some MEPs calling for the report to be published, Parliamentary authorities had refused until an MEP broke confidentiality and leaked it.
Committees and delegations.
The Parliament has 20 Standing Committees consisting of 28 to 86 MEPs each (reflecting the political makeup of the whole Parliament) including a Chairman, a bureau and secretariat. They meet twice a month in public to draw up, amend to adopt legislative proposals and reports to be presented to the plenary. The rapporteurs for a committee are supposed to present the view of the committee, although notably this has not always been the case. In the events leading to the resignation of the Santer Commission, the rapporteur went against the Budgetary Control Committee's narrow vote to discharge the budget, and urged the Parliament to reject it.
Committees can also set up sub-committees (e.g. the Subcommittee on Human Rights) and temporary committees to deal with a specific topic (e.g. on extraordinary rendition). The chairs of the Committees co-ordinate their work through the "Conference of Committee Chairmen". When co-decision was introduced it increased the Parliaments powers in a number of areas, but most notably those covered by the Committee on the Environment, Public Health and Food Safety. Previously this committee was considered by MEPs as a "Cinderella committee"; however, as it gained a new importance, it became more professional and rigorous, attracting increasing attention to its work.
The nature of the committees differ from their national counterparts as, although smaller in comparison to those of the United States Congress, the European Parliament's committees are unusually large by European standards with between eight and twelve dedicated members of staff and three to four support staff. Considerable administration, archives and research resources are also at the disposal of the whole Parliament when needed.
Delegations of the Parliament are formed in a similar manner and are responsible for relations with Parliaments outside the EU. There are 34 delegations made up of around 15 MEPs, chairpersons of the delegations also cooperate in a conference like the committee chairs do. They include "Interparliamentary delegations" (maintain relations with Parliament outside the EU), "joint parliamentary committees" (maintaining relations with parliaments of states which are candidates or associates of the EU), the delegation to the ACP EU Joint Parliamentary Assembly and the delegation to the Euro-Mediterranean Parliamentary Assembly. MEPs also participate in other international activities such as the Euro-Latin American Parliamentary Assembly, the Transatlantic Legislators' Dialogue and through election observation in third countries.
Translation and interpretation.
Speakers in the European Parliament are entitled to speak in any of the 24 official languages of the European Union, ranging from English and German to Maltese and Irish. Simultaneous interpreting is offered in all plenary sessions, and all final texts of legislation are translated. With twenty-four languages, the European Parliament is the most multilingual parliament in the world and the biggest employer of interpreters in the world (employing 350 full-time and 400 free-lancers when there is higher demand). Citizens may also address the Parliament in Basque, Catalan/Valencian and Galician.
Usually a language is translated from a foreign tongue into a translator's native tongue. Due to the large number of languages, some being minor ones, since 1995 interpreting is sometimes done the opposite way, out of an interpreter's native tongue (the "retour" system). In addition, a speech in a minor language may be interpreted through a third language for lack of interpreters ("relay" interpreting) —for example, when interpreting out of Estonian into Maltese. Due to the complexity of the issues, interpretation is not word for word. Instead, interpreters have to convey the political meaning of a speech, regardless of their own views. This requires detailed understanding of the politics and terms of the Parliament, involving a great deal of preparation beforehand (e.g. reading the documents in question). Difficulty can often arise when MEPs use profanities, jokes and word play or speak too fast.
While some see speaking their native language as an important part of their identity, and can speak more fluently in debates, interpretation and its cost has been criticised by some. A 2006 report by Alexander Stubb MEP highlighted that by only using English, French and German costs could be reduced from €118,000 per day (for 21 languages then—Romanian, Bulgarian and Croatian having not yet been included) to €8,900 per day. Many see the ideal single language as being English due to its widespread usage, although there has been a small-scale campaign to make French the reference language for all legal texts, due to the claim that it is more clear and precise for legal purposes.
Because the proceedings are translated into all of the official EU languages, they have been used to make a multilingual corpus known as Europarl. It is widely used to train statistical machine translation systems.
Seat.
The Parliament is based in three different cities with numerous buildings. A protocol attached to the Treaty of Amsterdam requires that 12 plenary sessions be held in Strasbourg (none in August but two in September), which is the Parliament's official seat, while extra part sessions as well as committee meetings are held in Brussels. Luxembourg hosts the Secretariat of the European Parliament. The European Parliament is the only assembly in the world with more than one meeting place and one of the few that does not have the power to decide its own location.
The Strasbourg seat is seen as a symbol of reconciliation between France and Germany, the Strasbourg region having been fought over by the two countries in the past. However, the cost and inconvenience of having two seats is questioned. While Strasbourg is the official seat, and sits alongside the Council of Europe, Brussels is home to nearly all other major EU institutions, with the majority of Parliament's work being carried out there. Critics have described the two-seat arrangement as a "travelling circus", and there is a strong movement to establish Brussels as the sole seat. This is because the other political institutions (the Commission, Council and European Council) are located there, and hence Brussels is treated as the 'capital' of the EU. This movement has received strong backing through numerous figures, including the Commission First-Vice President who stated that "something that was once a very positive symbol of the EU reuniting France and Germany has now become a negative symbol—of wasting money, bureaucracy and the insanity of the Brussels institutions". The Green party has also noted the environmental cost in a study led by Jean Lambert MEP and Caroline Lucas MEP; in addition to the extra 200 million euro spent on the extra seat, there are over 20,268 tonnes of additional carbon dioxide, undermining any environmental stance of the institution and the Union. The campaign is further backed by a million-strong online petition started by Cecilia Malmström MEP. In August 2014, an assessment by the European Court of Auditors calculated that relocating the Strasbourg seat of the European Parliament to Brussels would save €113.8 million per year. In 2006, there were allegations of irregularity in the charges made by the city of Strasbourg on buildings the Parliament rented, thus further harming the case for the Strasbourg seat.
Most MEPs prefer Brussels as a single base. A poll of MEPs found 89% of the respondents wanting a single seat, and 81% preferring Brussels. Another, more academic, survey found 68% support. In July 2011, an absolute majority of MEPs voted in favour of a single seat. In early 2011, the Parliament voted to scrap one of the Strasbourg sessions by holding two within a single week. The mayor of Strasbourg officially reacted by stating "we will counter-attack by upturning the adversary's strength to our own profit, as a judoka would do." However, as Parliament's seat is now fixed by the treaties, it can only be changed by the Council acting unanimously, meaning that France could veto any move. The former French President Nicolas Sarkozy has stated that the Strasbourg seat is "non-negotiable", and that France has no intention of surrendering the only EU Institution on French soil. Given France's declared intention to veto any relocation to Brussels, some MEPs have advocated civil disobedience by refusing to take part in the monthly exodus to Strasbourg.

</doc>
<doc id="9582" url="https://en.wikipedia.org/wiki?curid=9582" title="European Council">
European Council

The European Council is the Institution of the European Union (EU) that comprises the heads of state or government of the member states, along with the council's own president and the president of the Commission. The High Representative of the Union for Foreign Affairs and Security Policy also takes part in its meetings. Established as an informal summit in 1975, the council was formalised as an Institution in 2009 upon the entry into force of the Treaty of Lisbon. The current president of the European Council is Donald Tusk.
Scope.
While the European Council has no formal legislative power, it is a strategic (and crisis-solving) body that provides the union with general political directions and priorities, and acts as a collective presidency. The European Commission remains the sole initiator of legislation, but the European Council is able to provide an impetus to guide legislative policy.
The meetings of the European Council, still commonly referred to as EU summits, are chaired by its president and take place at least twice every six months; usually in the Justus Lipsius building, the headquarters of the Council of the European Union in Brussels. Decisions of the European Council are taken by consensus, except where the Treaties provide otherwise.
History.
The first summits of EU heads of state or government were held in February and July 1961 (in Paris and Bonn respectively). They were informal summits of the leaders of the European Community and were started due to then-French President Charles de Gaulle's resentment at the domination of supranational institutions (e.g. the European Commission) over the integration process, but petered out. The first influential summit held, after the departure of De Gaulle, was The Hague summit of 1969, which reached an agreement on the admittance of the United Kingdom into the Community and initiated foreign policy cooperation (the European Political Cooperation) taking integration beyond economics.
The summits were only formalised in the period between 1974 and 1988. At the December summit in Paris in 1974, following a proposal from then-French president Valéry Giscard d'Estaing, it was agreed that more high level, political input was needed following the "empty chair crisis" and economic problems. The inaugural "European Council", as it became known, was held in Dublin on 10 and 11 March 1975 during Ireland's first Presidency of the Council of Ministers. In 1987, it was included in the treaties for the first time (the Single European Act) and had a defined role for the first time in the Maastricht Treaty. At first only a minimum of two meetings per year were required, which resulted in an average of three meetings per year being held for the 1975-1995 period. Since 1996, the number of meetings were required to be minimum four per year. For the latest 2008-2014 period, this minimum was well exceeded, by an average of seven meetings being held per year. The seat of the Council was formalised in 2002, basing it in Brussels. Three types of European Councils exist: Informal, Scheduled and Extraordinary. While the informal meetings are also scheduled 1½ year in advance, they differ from the scheduled ordinary meetings by not ending with official "Council conclusions", as they instead end by more broad political "Statements" on some cherry picked policy matters. The extraordinary meetings always end with official "Council conclusions" - but differs from the scheduled meetings by not being scheduled more than a year in advance, as for example in 2001 when the European Council gathered to lead the European Union's response to the 11 September attacks.
Some meetings of the European Council are seen by some as turning points in the history of the European Union. For example:
As such, the European Council had already existed before it gained the status as an institution of the European Union with the entering into force of the Treaty of Lisbon. Indeed, Article 214(2) of the Treaty establishing the European Community provided (before it was amended by the Treaty of Lisbon) that ‘the Council, meeting "in the composition of Heads of State or Government" and acting by a qualified majority, shall nominate the person it intends to appoint as President of the Commission’ (emphasis added); this may be seen as an early codification of the European Council in the Treaties. In the event, (amended by the Treaty of Lisbon) officially introduces the term "European Council" as a substitute for the phrase "Council f the European Unio meeting in the composition of the Heads of State or Government", which was previously sometimes used in the treaties to refer to this body.
The Treaty of Lisbon made the European Council a formal institution distinct from the (ordinary) Council of the EU, and created the present longer term and full-time presidency. As an outgrowth of the Council of the EU, the European Council had previously followed the same Presidency, rotating between each member state. While the Council of the EU retains that system, the European Council established, with no change in powers, a system of appointing an individual (without them being a national leader) for a two-and-a-half-year term - which can be renewed for the same person only once. Following the ratification of the treaty in December 2009, the European Council elected the then-Prime Minister of Belgium Herman Van Rompuy as its first permanent president (resigning from Belgian Prime Minister).
Powers and functions.
The European Council is an official institution of the EU, mentioned by the Lisbon Treaty as a body which "shall provide the Union with the necessary impetus for its development". Essentially it defines the EU's policy agenda and has thus been considered to be the motor of European integration. It does this without any formal powers, only the influence it has being composed of national leaders. Beyond the need to provide "impetus", the Council has developed further roles; to "settle issues outstanding from discussions at a lower level", to lead in foreign policy — acting externally as a "collective Head of State", "formal ratification of important documents" and "involvement in the negotiation of the treaty changes".
Since the institution is composed of national leaders, it gathers the executive power of the member states and has thus a great influence in high profile policy areas as for example foreign policy. It also exercises powers of appointment, such as appointment of its own President, the High Representative of the Union for Foreign Affairs and Security Policy, and the President of the European Central Bank. It proposes, to the European Parliament, a candidate for President of the European Commission. Moreover, the European Council influences police and justice planning, the composition of the Commission, matters relating to the organisation of the rotating Council presidency, the suspension of membership rights, and changing the voting systems through the Passerelle Clause. Although the European Council has no direct legislative power, under the "emergency brake" procedure, a state outvoted in the Council of Ministers may refer contentious legislation to the European Council. However, the state may still be outvoted in the European Council. Hence with powers over the supranational executive of the EU, in addition to its other powers, the European Council has been described by some as the Union's "supreme political authority".
Composition.
The European Council consists of the heads of state or government of the member states, alongside its own President and the Commission President (non-voting). The meetings used to be regularly attended by the national foreign minister as well, and the Commission President likewise accompanied by another member of the Commission. However, since the Treaty of Lisbon, this has been discontinued, as the size of the body had become somewhat large following successive accessions of new Member States to the Union.
Meetings can also include other invitees, such as the President of the European Central Bank, as required. The Secretary-General of the Council attends, and is responsible for organisational matters, including minutes. The President of the European Parliament also attends to give an opening speech outlining the European Parliament's position before talks begin.
Additionally, the negotiations involve a large number of other people working behind the scenes. Most of those people, however, are not allowed to the conference room, except for two delegates per state to relay messages. At the push of a button members can also call for advice from a Permanent Representative via the "Antici Group" in an adjacent room. The group is composed of diplomats and assistants who convey information and requests. Interpreters are also required for meetings as members are permitted to speak in their own languages.
As the composition is not precisely defined, some states which have a considerable division of executive power can find it difficult to decide who should attend the meetings. While an MEP, Alexander Stubb argued that there was no need for the President of Finland to attend Council meetings with or instead of the Prime Minister of Finland (who was head of European foreign policy). In 2008, having become Finnish Foreign Minister, Stubb was forced out of the Finnish delegation to the emergency council meeting on the Georgian crisis because the President wanted to attend the high profile summit as well as the Prime Minister (only two people from each country could attend the meetings). This was despite Stubb being head of the Organisation for Security and Co-operation in Europe at the time which was heavily involved in the crisis. Problems also occurred in Poland where the President of Poland and the Prime Minister of Poland were of different parties and had a different foreign policy response to the crisis. A similar situation arose in Romania between President Traian Băsescu and Prime Minister Călin Popescu-Tăriceanu in 2007–2008 and again in 2012 with Prime Minister Victor Ponta, who both opposed the president.
Eurozone summits.
A number of ad hoc meetings of Heads of State or Government of the Euro area countries were held in 2010 and 2011 to discuss the Sovereign Debt crisis. It was agreed in October 2011 that they should meet regularly twice a year (with extra meetings if needed). This will normally be at the end of a European Council meeting and according to the same format (chaired by the President of the European Council and including the President of the Commission), but usually restricted to the (currently 17) Heads of State or Government of countries whose currency is the euro.
President.
The President of the European Council is elected by the European Council by a qualified majority for a once-renewable term of two and a half years. The President must report to the European Parliament after each European Council meeting.
The post was created by the Treaty of Lisbon and was subject to a debate over its exact role. Prior to Lisbon, the Presidency rotated in accordance with the Presidency of the Council of the European Union. The role of that President-in-Office was in no sense (other than protocol) equivalent to an office of a head of state, merely a "primus inter pares" (first among equals) role among other European heads of government. The President-in-Office was primarily responsible for preparing and chairing the Council meetings, and had no executive powers other than the task of representing the Union externally. Now the leader of the Council Presidency country can still act as president when the permanent president is absent.
Members.
 (10) 
Political parties.
Almost all members of the European Council are members of a political party at national level, and most of these are members of a European-level political party. These frequently hold pre-meetings of their European Council members, prior to its meetings. However, the European Council is composed to represent the EU's states rather than political parties and decisions are generally made on these lines, though ideological alignment can colour their political agreements and their choice of appointments (such as their president).
The table below outlines the number of leaders affiliated to each party and their total voting weight. The map to the right indicates the alignment of each individual country.
Seat and meetings.
Meetings of the European Council usually take place four times a year in Brussels. Meetings traditionally last for two days, sometimes even longer when contentious issues were on the agenda. However, former President Van Rompuy preferred to keep the summit to a single day. Until 2002, the venue of the council meeting rotated between member states, as its location was decided by the country holding the rotating presidency. However, the 22nd declaration attached to the Treaty of Nice stated that; "As from 2002, one European Council meeting per Presidency will be held in Brussels. When the Union comprises 18 members, all European Council meetings will be held in Brussels."
Between 2002 and 2004, half the councils were held in Brussels and, after the 2004 enlargement, all were. The European Council uses the same building as the Council of the European Union, i.e., the Justus Lipsius building. However, some extraordinary councils have taken place in the member state holding the Presidency, e.g., 2003 in Rome or 2005 in Hampton Court Palace. A new building (the "Europa building") is currently being built at the northern end of the adjacent historical Résidence Palace complex for use as a purpose built summit building by the European Council and the Council. It had been due to be completed in 2013, but various delays have made late 2016 the opening date.
The choice of a single seat was due to a number of factors, mostly logistical (organising the meetings became ever more onerous with the enlargement of the EU, especially for smaller countries) and security (the experience of the Belgian police in dealing with protesters (a protester in Gothenburg was shot by police)) as well as Brussels having fixed facilities for the Council and journalists at every meeting. Having a permanent seat in Brussels also emphasised that the European Council is an EU institution rather than a summit of sovereign States in the manner of the G20. Some have argued it is the "de facto" EU government, while others underline that it is the Commission that is the EU's day-to-day government and the European Council can best be compared to a collective head of state.
In 2007, the new situation for locating meetings became a source of contention with the Portuguese government wanting to sign the Lisbon Treaty in Lisbon, Portugal. The Belgian government, however, was keen not to set a precedent and insisted that the regular end of year summit took place in Brussels as usual. This meant that after the signing, photo suit, and formal dinner, the attendees of the summit were transferred from Lisbon to Brussels. Mirrored with the "travelling circus" of the European Parliament, this garnered protests from environmental groups describing the hypocrisy of demanding lower carbon emissions while flying across Europe for the same summit for political reasons.
There are no current plans to hold meetings outside of Brussels, except for "force majeure" (for instance a strike by air traffic controllers nearly caused the January 2012 informal meeting to be switched to Luxembourg).
President's cabinet.
Although the European Council is, under the terms of the Lisbon treaty, a separate institution of the EU, it does not have its own administration. The administrative support for both the European Council and its president is provided by the General Secretariat of the Council of the European Union. The president does have, however, his own private office ("cabinet") of close advisers. Van Rompuy chose as his chief of staff ("chef de cabinet") Baron Frans van Daele, formerly Belgian ambassador to, variously, the USA, the UN, the EU and NATO and chief of staff of several Belgian foreign ministers. Also in his team are the former UK Labour MEP Richard Corbett, former Hungarian Ambassador to NATO Zoltán Martinusz, former head of the EU's economic & financial committee Odile Renaud-Basso, and Van Rompuy's long standing press officer Dirk De Backer.

</doc>
<doc id="9587" url="https://en.wikipedia.org/wiki?curid=9587" title="Euthanasia">
Euthanasia

Euthanasia (from ; "good death": εὖ, "eu"; "well" or "good" – θάνατος, "thanatos"; "death") is the practice of intentionally ending a life in order to relieve pain and suffering.
There are different euthanasia laws in each country. The British House of Lords Select Committee on Medical Ethics defines euthanasia as "a deliberate intervention undertaken with the express intention of ending a life, to relieve intractable suffering". In the Netherlands and Flanders, euthanasia is understood as "termination of life by a doctor at the request of a patient".
Euthanasia is categorized in different ways, which include voluntary, non-voluntary, or involuntary. Voluntary euthanasia is legal in some countries. Non-voluntary euthanasia (patient's consent unavailable) is illegal in all countries. Involuntary euthanasia (without asking consent or against the patient's will) is also illegal in all countries and is usually considered murder. As of 2006, euthanasia is the most active area of research in contemporary bioethics.
In some countries there is a divisive public controversy over the moral, ethical, and legal issues of euthanasia. Those who are against euthanasia may argue for the sanctity of life, while proponents of euthanasia rights emphasize alleviating suffering, and preserving bodily integrity, self-determination, and personal autonomy. Jurisdictions where euthanasia is legal include the Netherlands, Canada, Colombia, Belgium, and Luxembourg.
Definition.
Like other terms borrowed from history, "euthanasia" has had different meanings depending on usage. The first apparent usage of the term "euthanasia" belongs to the historian Suetonius, who described how the Emperor Augustus, "dying quickly and without suffering in the arms of his wife, Livia, experienced the 'euthanasia' he had wished for." The word "euthanasia" was first used in a medical context by Francis Bacon in the 17th century, to refer to an easy, painless, happy death, during which it was a "physician's responsibility to alleviate the 'physical sufferings' of the body." Bacon referred to an "outward euthanasia"—the term "outward" he used to distinguish from a spiritual concept—the euthanasia "which regards the preparation of the soul."
In current usage, euthanasia has been defined as the "painless inducement of a quick death". However, it is argued that this approach fails to properly define euthanasia, as it leaves open a number of possible actions which would meet the requirements of the definition, but would not be seen as euthanasia. In particular, these include situations where a person kills another, painlessly, but for no reason beyond that of personal gain; or accidental deaths that are quick and painless, but not intentional.
Another approach incorporates the notion of suffering into the definition. The definition offered by the Oxford English Dictionary incorporates suffering as a necessary condition, with "the painless killing of a patient suffering from an incurable and painful disease or in an irreversible coma", This approach is included in Marvin Khol and Paul Kurtz's definition of it as "a mode or act of inducing or permitting death painlessly as a relief from suffering". Counterexamples can be given: such definitions may encompass killing a person suffering from an incurable disease for personal gain (such as to claim an inheritance), and commentators such as Tom Beauchamp and Arnold Davidson have argued that doing so would constitute "murder simpliciter" rather than euthanasia.
The third element incorporated into many definitions is that of intentionality – the death must be intended, rather than being accidental, and the intent of the action must be a "merciful death". Michael Wreen argued that "the principal thing that distinguishes euthanasia from intentional killing simpliciter is the agent's motive: it must be a good motive insofar as the good of the person killed is concerned." Similarly, Heather Draper speaks to the importance of motive, arguing that "the motive forms a crucial part of arguments for euthanasia, because it must be in the best interests of the person on the receiving end." Definitions such as that offered by the House of Lords Select Committee on Medical Ethics take this path, where euthanasia is defined as "a deliberate intervention undertaken with the express intention of ending a life, to relieve intractable suffering." Beauchamp and Davidson also highlight Baruch Brody's "an act of euthanasia is one in which one person ... (A) kills another person (B) for the benefit of the second person, who actually does benefit from being killed".
Draper argued that any definition of euthanasia must incorporate four elements: an agent and a subject; an intention; a causal proximity, such that the actions of the agent lead to the outcome; and an outcome. Based on this, she offered a definition incorporating those elements, stating that euthanasia "must be defined as death that results from the intention of one person to kill another person, using the most gentle and painless means possible, that is motivated solely by the best interests of the person who dies." Prior to Draper, Beauchamp and Davidson had also offered a definition that includes these elements. Their definition specifically discounts fetuses in order to distinguish between abortions and euthanasia:
Wreen, in part responding to Beauchamp and Davidson, offered a six-part definition:
Wreen also considered a seventh requirement: "(7) The good specified in (6) is, or at least includes, the avoidance of evil", although as Wreen noted in the paper, he was not convinced that the restriction was required.
In discussing his definition, Wreen noted the difficulty of justifying euthanasia when faced with the notion of the subject's "right to life". In response, Wreen argued that euthanasia has to be voluntary, and that "involuntary euthanasia is, as such, a great wrong". Other commentators incorporate consent more directly into their definitions. For example, in a discussion of euthanasia presented in 2003 by the European Association of Palliative Care (EPAC) Ethics Task Force, the authors offered: "Medicalized killing of a person without the person's consent, whether nonvoluntary (where the person in unable to consent) or involuntary (against the person's will) is not euthanasia: it is murder. Hence, euthanasia can be voluntary only." Although the EPAC Ethics Task Force argued that both non-voluntary and involuntary euthanasia could not be included in the definition of euthanasia, there is discussion in the literature about excluding one but not the other.
Classification of euthanasia.
Euthanasia may be classified according to whether a person gives informed consent into three types: voluntary, non-voluntary and involuntary.
There is a debate within the medical and bioethics literature about whether or not the non-voluntary (and by extension, involuntary) killing of patients can be regarded as euthanasia, irrespective of intent or the patient's circumstances. In the definitions offered by Beauchamp and Davidson and, later, by Wreen, consent on the part of the patient was not considered as one of their criteria, although it may have been required to justify euthanasia. However, others see consent as essential.
Voluntary euthanasia.
Euthanasia conducted with the consent of the patient is termed voluntary euthanasia. Active voluntary euthanasia is legal in Belgium, Luxembourg and the Netherlands. Passive voluntary euthanasia is legal throughout the U.S. per "Cruzan v. Director, Missouri Department of Health". When the patient brings about his or her own death with the assistance of a physician, the term assisted suicide is often used instead. Assisted suicide is legal in Switzerland and the U.S. states of California, Oregon, Washington, Montana and Vermont.
Non-voluntary euthanasia.
Euthanasia conducted when the consent of the patient is unavailable is termed non-voluntary euthanasia. Examples include child euthanasia, which is illegal worldwide but decriminalised under certain specific circumstances in the Netherlands under the Groningen Protocol.
Involuntary euthanasia.
Euthanasia conducted against the will of the patient is termed involuntary euthanasia.
Passive and active euthanasia.
Voluntary, non-voluntary and involuntary euthanasia can all be further divided into passive or active variants. Passive euthanasia entails the withholding of common treatments, such as antibiotics, necessary for the continuance of life. Active euthanasia entails the use of lethal substances or forces, such as administering a lethal injection, to kill and is the most controversial means. A number of authors consider these terms to be misleading and unhelpful.
History.
According to the historian N. D. A. Kemp, the origin of the contemporary debate on euthanasia started in 1870. Euthanasia is known to have been debated and practiced long before that date. Euthanasia was practiced in Ancient Greece and Rome: for example, hemlock was employed as a means of hastening death on the island of Kea, a technique also employed in Marseilles. Euthanasia, in the sense of the deliberate hastening of a person's death, was supported by Socrates, Plato and Seneca the Elder in the ancient world, although Hippocrates appears to have spoken against the practice, writing "I will not prescribe a deadly drug to please someone, nor give advice that may cause his death" (noting there is some debate in the literature about whether or not this was intended to encompass euthanasia).
Early modern period.
The term "euthanasia" in the earlier sense of supporting someone as they died was used for the first time by Francis Bacon (1561-1626). In his work, "Euthanasia medica", he chose this ancient Greek word and, in doing so, distinguished between "euthanasia interior", the preparation of the soul for death, and "euthanasia exterior", which was intended to make the end of life easier and painless, in exceptional circumstances by shortening life. That the ancient meaning of an easy death came to the fore again in the early modern period can be seen from its definition in the 18th century "Zedlers Universallexikon":
The concept of euthanasia in the sense of alleviating the process of death goes back to the medical historian, Karl Friedrich Heinrich Marx, who drew on Bacon's philosophical ideas. According to Marx, a doctor had a moral duty to ease the suffering of death through encouragement, support and mitigation using medication. Such an "alleviation of death" reflected the contemporary "Zeitgeist", but was brought into the medical canon of responsibility for the first time by Marx. Marx also stressed the distinction between the theological care of the soul of sick people from the physical care and medical treatment by doctors.
Euthanasia in its modern sense has always been strongly opposed in the Judeo-Christian tradition. Thomas Aquinas opposed both and argued that the practice of euthanasia contradicted our natural human instincts of survival, as did Francois Ranchin (1565–1641), a French physician and professor of medicine, and Michael Boudewijns (1601–1681), a physician and teacher. Other voices argued for euthanasia, such as John Donne in 1624, and euthanasia continued to be practised. In 1678, the publication of Caspar Questel's "De pulvinari morientibus non subtrahend", ("On the pillow of which the dying should not be deprived"), initiated debate on the topic. Questel described various customs which were employed at the time to hasten the death of the dying, (including the sudden removal of a pillow, which was believed to accelerate death), and argued against their use, as doing so was "against the laws of God and Nature". This view was shared by many who followed, including Philipp Jakob Spener, Veit Riedlin and Johann Georg Krünitz. Despite opposition, euthanasia continued to be practised, involving techniques such as bleeding, suffocation, and removing people from their beds to be placed on the cold ground.
Suicide and euthanasia became more accepted during the Age of Enlightenment. Thomas More wrote of euthanasia in "Utopia", although it is not clear if More was intending to endorse the practice. Other cultures have taken different approaches: for example, in Japan suicide has not traditionally been viewed as a sin, as it is used in cases of honor, and accordingly, the perceptions of euthanasia are different from those in other parts of the world.
Beginnings of the contemporary euthanasia debate.
In the mid-1800s, the use of morphine to treat "the pains of death" emerged, with John Warren recommending its use in 1848. A similar use of chloroform was revealed by Joseph Bullar in 1866. However, in neither case was it recommended that the use should be to hasten death. In 1870 Samuel Williams, a schoolteacher, initiated the contemporary euthanasia debate through a speech given at the Birmingham Speculative Club in England, which was subsequently published in a one-off publication entitled "Essays of the Birmingham Speculative Club", the collected works of a number of members of an amateur philosophical society. Williams' proposal was to use chloroform to deliberately hasten the death of terminally ill patients:
The essay was favourably reviewed in "The Saturday Review", but an editorial against the essay appeared in "The Spectator". From there it proved to be influential, and other writers came out in support of such views: Lionel Tollemache wrote in favour of euthanasia, as did Annie Besant, the essayist and reformer who later became involved with the National Secular Society, considering it a duty to society to "die voluntarily and painlessly" when one reaches the point of becoming a 'burden'. "Popular Science" analyzed the issue in May 1873, assessing both sides of the argument. Kemp notes that at the time, medical doctors did not participate in the discussion; it was "essentially a philosophical enterprise ... tied inextricably to a number of objections to the Christian doctrine of the sanctity of human life".
Early euthanasia movement in the United States.
The rise of the euthanasia movement in the United States coincided with the so-called Gilded Age, a time of social and technological change that encompassed an "individualistic conservatism that praised laissez-faire economics, scientific method, and rationalism", along with major depressions, industrialisation and conflict between corporations and labour unions. It was also the period in which the modern hospital system was developed, which has been seen as a factor in the emergence of the euthanasia debate.
Robert Ingersoll argued for euthanasia, stating in 1894 that where someone is suffering from a terminal illness, such as terminal cancer, they should have a right to end their pain through suicide. Felix Adler offered a similar approach, although, unlike Ingersoll, Adler did not reject religion. In fact, he argued from an Ethical Culture framework. In 1891, Alder argued that those suffering from overwhelming pain should have the right to commit suicide, and, furthermore, that it should be permissible for a doctor to assist – thus making Adler the first "prominent American" to argue for suicide in cases where people were suffering from chronic illness. Both Ingersoll and Adler argued for voluntary euthanasia of adults suffering from terminal ailments. Dowbiggin argues that by breaking down prior moral objections to euthanasia and suicide, Ingersoll and Adler enabled others to stretch the definition of euthanasia.
The first attempt to legalise euthanasia took place in the United States, when Henry Hunt introduced legislation into the General Assembly of Ohio in 1906. Hunt did so at the behest of Anna Hall, a wealthy heiress who was a major figure in the euthanasia movement during the early 20th century in the United States. Hall had watched her mother die after an extended battle with liver cancer, and had dedicated herself to ensuring that others would not have to endure the same suffering. Towards this end she engaged in an extensive letter writing campaign, recruited Lurana Sheldon and Maud Ballington Booth, and organised a debate on euthanasia at the annual meeting of the American Humane Association in 1905 – described by Jacob Appel as the first significant public debate on the topic in the 20th century.
Hunt's bill called for the administration of an anesthetic to bring about a patient's death, so long as the person is of lawful age and sound mind, and was suffering from a fatal injury, an irrevocable illness, or great physical pain. It also required that the case be heard by a physician, required informed consent in front of three witnesses, and required the attendance of three physicians who had to agree that the patient's recovery was impossible. A motion to reject the bill outright was voted down, but the bill failed to pass, 79 to 23.
Along with the Ohio euthanasia proposal, in 1906 Assemblyman Ross Gregory introduced a proposal to permit euthanasia to the Iowa legislature. However, the Iowa legislation was far broader in scope than that offered in Ohio. It allowed for the death of any person of at least ten years of age who suffered from an ailment that would prove fatal and cause extreme pain, should they be of sound mind and express a desire to artificially hasten their death. In addition, it allowed for infants to be euthanised if they were sufficiently deformed, and permitted guardians to request euthanasia on behalf of their wards. The proposed legislation also imposed penalties on physicians who refused to perform euthanasia when requested: a 6–12 month prison term and a fine of between $200 and $1000. The proposal proved to be controversial. It engendered considerable debate and failed to pass, having been withdrawn from consideration after being passed to the Committee on Public Health.
After 1906 the euthanasia debate reduced in intensity, resurfacing periodically but not returning to the same level of debate until the 1930s in the United Kingdom.
1930s in Britain.
The Voluntary Euthanasia Legalisation Society was founded in 1935 by Charles Killick Millard (now called Dignity in Dying). The movement campaigned for the legalisation of euthanasia in Great Britain.
In January 1936, King George V was given a fatal dose of morphine and cocaine in order to hasten his death. At the time he was suffering from cardio-respiratory failure, and the decision to end his life was made by his physician, Lord Dawson. Although this event was kept a secret for over 50 years, the death of George V coincided with proposed legislation in the House of Lords to legalise euthanasia. The legislation came through the British Volunteer Euthanasia Legalisation Society.
Euthanasia opponent Ian Dowbiggin argues that the early membership of the Euthanasia Society of America (ESA) reflected how many perceived euthanasia at the time, often seeing it as a eugenics matter rather than an issue concerning individual rights. Dowbiggin argues that not every eugenist joined the ESA "solely for eugenic reasons", but he postulates that there were clear ideological connections between the eugenics and euthanasia movements.
Nazi Euthanasia Program (Action T4).
A 24 July 1939 killing of a severely disabled infant in Nazi Germany was described in a BBC "Genocide Under the Nazis Timeline" as the first "state-sponsored euthanasia". Parties that consented to the killing included Hitler's office, the parents, and the Reich Committee for the Scientific Registration of Serious and Congenitally Based Illnesses. "The Telegraph" noted that the killing of the disabled infant—whose name was Gerhard Kretschmar, born blind, with missing limbs, subject to convulsions, and reportedly "an idiot"— provided "the rationale for a secret Nazi decree that led to 'mercy killings' of almost 300,000 mentally and physically handicapped people". While Kretchmar's killing received parental consent, most of the 5,000 to 8,000 children killed afterwards were forcibly taken from their parents.
The "euthanasia campaign" of mass murder gathered momentum on 14 January 1940 when the "handicapped" were killed with gas vans and killing centres, eventually leading to the deaths of 70,000 adult Germans. Professor Robert Jay Lifton, author of "The Nazi Doctors" and a leading authority on the T4 program, contrasts this program with what he considers to be a genuine euthanasia. He explains that the Nazi version of "euthanasia" was based on the work of Adolf Jost, who published "The Right to Death" (Das Recht auf den Tod) in 1895. Lifton writes: "Jost argued that control over the death of the individual must ultimately belong to the social organism, the state. This concept is in direct opposition to the Anglo-American concept of euthanasia, which emphasizes the "individual's" 'right to die' or 'right to death' or 'right to his or her own death,' as the ultimate human claim. In contrast, Jost was pointing to the state's right to kill. ... Ultimately the argument was biological: 'The rights to death r the key to the fitness of life.' The state must own death—must kill—in order to keep the social organism alive and healthy."
In modern terms, the use of "euthanasia" in the context of Action T4 is seen to be a euphemism to disguise a program of genocide, in which people were killed on the grounds of "disabilities, religious beliefs, and discordant individual values". Compared to the discussions of euthanasia that emerged post-war, the Nazi program may have been worded in terms that appear similar to the modern use of "euthanasia", but there was no "mercy" and the patients were not necessarily terminally ill. Despite these differences, historian and euthanasia opponent Ian Dowbiggin writes that "the origins of Nazi euthanasia, like those of the American euthanasia movement, predate the Third Reich and were intertwined with the history of eugenics and Social Darwinism, and with efforts to discredit traditional morality and ethics."
The 1949 New York State Petition for Euthanasia and Catholic Opposition.
On January 6, 1949, the Euthanasia Society of America presented to the New York State Legislature a petition to legalize euthanasia, signed by 379 leading Protestant and Jewish ministers, the largest group of religious leaders ever to have taken this stance. A similar petition had been sent to the New York State Legislature in 1947, signed by approximately 1,000 New York physicians. Catholic religious leaders criticized the petition, saying that such a bill would "legalize a suicide-murder pact" and a "rationalization of the fifth commandment of God, 'Though Shalt Not Kill.'" The Right Reverend Robert E. McCormick stated that 
The petition brought tensions between the American Euthanasia Society and the Catholic Church to a head that contributed to a climate of anti-Catholic sentiment generally regarding issues such as birth control, eugenics, and population control.
The petition did not lead to a law.
Euthanasia debate.
Historically, the euthanasia debate has tended to focus on a number of key concerns. According to euthanasia opponent Ezekiel Emanuel, proponents of euthanasia have presented four main arguments: a) that people have a right to self-determination, and thus should be allowed to choose their own fate; b) assisting a subject to die might be a better choice than requiring that they continue to suffer; c) the distinction between passive euthanasia, which is often permitted, and active euthanasia, which is not substantive (or that the underlying principle–the doctrine of double effect–is unreasonable or unsound); and d) permitting euthanasia will not necessarily lead to unacceptable consequences. Pro-euthanasia activists often point to countries like the Netherlands and Belgium, and states like Oregon, where euthanasia has been legalized, to argue that it is mostly unproblematic.
Similarly, Emanuel argues that there are four major arguments presented by opponents of euthanasia: a) not all deaths are painful; b) alternatives, such as cessation of active treatment, combined with the use of effective pain relief, are available; c) the distinction between active and passive euthanasia is morally significant; and d) legalising euthanasia will place society on a slippery slope, which will lead to unacceptable consequences. In fact, in Oregon, in 2013, pain wasn't one of the top five reasons people seek euthanasia. Top reasons were a loss of dignity, and a fear of burdening others.
In the United States in 2013, 47% nationwide supported doctor-assisted suicide. This included 32% of Latinos, 29% of African-Americans, and almost nobody with disabilities.
Legal status.
West's "Encyclopedia of American Law" states that "a 'mercy killing' or euthanasia is generally considered to be a criminal homicide" and is normally used as a synonym of homicide committed at a request made by the patient.
The judicial sense of the term "homicide" includes any intervention undertaken with the express intention of ending a life, even to relieve intractable suffering. Not all homicide is unlawful. Two designations of homicide that carry no criminal punishment are justifiable and excusable homicide. In most countries this is not the status of euthanasia. The term "euthanasia" is usually confined to the active variety; the University of Washington website states that "euthanasia generally means that the physician would act directly, for instance by giving a lethal injection, to end the patient's life". Physician-assisted suicide is thus not classified as euthanasia by the US State of Oregon, where it is legal under the Oregon Death with Dignity Act, and despite its name, it is not legally classified as suicide either. Unlike physician-assisted suicide, withholding or withdrawing life-sustaining treatments with patient consent (voluntary) is almost unanimously considered, at least in the United States, to be legal. The use of pain medication in order to relieve suffering, even if it hastens death, has been held as legal in several court decisions.
Some governments around the world have legalized voluntary euthanasia but most commonly it is still considered to be criminal homicide. In the Netherlands and Belgium, where euthanasia has been legalized, it still remains homicide although it is not prosecuted and not punishable if the perpetrator (the doctor) meets certain legal conditions.
Physician sentiment.
A survey in the United States of more than 10,000 physicians came to the result that approximately 16% of physicians would ever consider halting life-sustaining therapy because the family demands it, even if they believed that it was premature. Approximately 55% would not, and for the remaining 29%, it would depend on circumstances.
This study also stated that approximately 46% of physicians agree that physician-assisted suicide should be allowed in some cases; 41% do not, and the remaining 14% think it depends.
In the United Kingdom, the pro-assisted dying group Dignity in Dying cite conflicting research on attitudes by doctors to assisted dying: with a 2009 "Palliative Medicine"-published survey showing 64% support (to 34% oppose) for assisted dying in cases where a patient has an incurable and painful disease, while 49% of doctors in a study published in "BMC Medical Ethics" oppose changing the law on assisted dying to 39% in favour.

</doc>
<doc id="9588" url="https://en.wikipedia.org/wiki?curid=9588" title="Extraterrestrial life">
Extraterrestrial life

Extraterrestrial life is life that does not originate from Earth. It is also called alien life, or, if it is a sentient and/or relatively complex individual, an "extraterrestrial" or "alien" (or, to avoid confusion with the legal sense of "alien", a "space alien"). These as-yet-hypothetical life forms range from simple bacteria-like organisms to beings with civilizations far more advanced than humanity. Although many scientists expect extraterrestrial life to exist, there is no unambiguous evidence for its existence so far.
The science of extraterrestrial life is known as exobiology. The science of astrobiology also considers life on Earth as well, and in the broader astronomical context. Meteorites that have fallen to Earth have sometimes been examined for signs of microscopic extraterrestrial life. In 2015, "remains of biotic life" were found in 4.1 billion-year-old rocks in Western Australia, when the young Earth was about 400 million years old. According to one of the researchers, "If life arose relatively quickly on Earth ... then it could be common in the universe."
Since the mid-20th century, there has been an ongoing search for signs of extraterrestrial intelligence, from radios used to detect possible extraterrestrial signals, to telescopes used to search for potentially habitable extrasolar planets. It has also played a major role in works of science fiction. Over the years, science fiction works, especially Hollywood's involvement, has increased the public's interest in the possibility of extraterrestrial life. Some encourage aggressive methods to try to get in contact with life in outer space, whereas others argue that it might be dangerous to actively call attention to Earth.
Background.
Alien life, such as microorganisms, has been hypothesized to exist in the Solar System and throughout the universe. This hypothesis relies on the vast size and consistent physical laws of the observable universe. According to this argument, made by scientists such as Carl Sagan and Stephen Hawking, it would be improbable for life "not" to exist somewhere other than Earth. This argument is embodied in the Copernican principle, which states that Earth does not occupy a unique position in the Universe, and the mediocrity principle, which states that there is nothing special about life on Earth. The chemistry of life may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the universe was only 10–17 million years old. Life may have emerged independently at many places throughout the universe. Alternatively, life may have formed less frequently, then spread—by meteoroids, for example—between habitable planets in a process called panspermia. In any case, complex organic molecules may have formed in the protoplanetary disk of dust grains surrounding the Sun before the formation of Earth. According to these studies, this process may occur outside Earth on several planets and moons of the Solar System and on planets of other stars.
Since the 1950s, scientists have argued the idea that "habitable zones" around stars are the most likely places to find life. Numerous discoveries in these zones since 2007 have generated estimations of frequencies of Earth-like planets —in terms of composition— numbering in the many billions though as of 2013, only a small number of planets have been discovered in these zones. Nonetheless, on November 4, 2013, astronomers reported, based on "Kepler" space mission data, that there could be as many as 40 billion Earth-sized planets orbiting in the habitable zones of Sun-like stars and red dwarfs in the Milky Way, 11 billion of which may be orbiting Sun-like stars. The nearest such planet may be 12 light-years away, according to the scientists. Astrobiologists have also considered a "follow the energy" view of potential habitats.
Possible basis.
Biochemistry.
It is often hypothesized that life forms elsewhere in the universe would, like life on Earth, be based on carbon chemistry and rely on liquid water. Life forms based on ammonia (rather than water) have been suggested, though this solvent appears less suitable than water. It is also conceivable that there are forms of life whose solvent is a liquid hydrocarbon, such as methane, ethane or propane.
About 29 chemical elements are thought to play an active positive role in living organisms on Earth. About 95% of this living matter is built upon only six elements: carbon, hydrogen, nitrogen, oxygen, phosphorus and sulfur. These six elements form the basic building blocks of virtually all life on Earth, whereas most of the remaining elements are found only in trace amounts. The unique characteristics of carbon made it unlikely that any other element could replace carbon, even on another planet, to generate the biochemistry necessary for life. The carbon atom has the unique ability to make four strong chemical bonds with other atoms, including other carbon atoms. These covalent bonds have a direction in space, so that carbon atoms can form the skeletons of complex 3-dimensional structures with definite architectures such as nucleic acids and proteins. Carbon forms more compounds than all other elements combined. The great versatility of the carbon atom makes it the element most likely to provide the bases—even exotic ones—to the chemical composition of life on other planets.
Life on Earth requires water as its solvent in which biochemical reactions take place. Sufficient quantities of carbon and the other elements along with water, may enable the formation of living organisms on other planets with a chemical make-up and temperature range similar to that of Earth. Terrestrial planets such as Earth are formed in a process that allows for the possibility of having compositions similar to Earth's. The combination of carbon, hydrogen and oxygen in the chemical form of carbohydrates (e.g. sugar) can be a source of chemical energy on which life depends, and can provide structural elements for life. Plants derive energy through the conversion of light energy into chemical energy via photosynthesis. Life, as currently recognized, requires carbon in both reduced (methane derivatives) and partially oxidized (carbon oxides) states. Nitrogen is needed as a reduced ammonia derivative in all proteins, sulfur as a derivative of hydrogen sulfide in some necessary proteins, and phosphorus oxidized to phosphates in genetic material and in energy transfer.
Planetary habitability in the Solar System.
Some bodies in the Solar System have the potential for an environment in which extraterrestrial life can live, particularly those with possible subsurface oceans. Should life be discovered elsewhere in the Solar System, astrobiologists suggest that it will more likely be in the form of extremophile microorganisms.
Mars may have niche subsurface environments where microbial life might exist. A subsurface marine environment on Jupiter's moon Europa might be the most likely habitat in the Solar System, outside Earth, for extremophile microorganisms.
The panspermia hypothesis proposes that life elsewhere in the Solar System may have a common origin. If extraterrestrial life was found on another body in the Solar System, it could have originated from Earth just as life on Earth could have been seeded from elsewhere (exogenesis). The first known mention of the term 'panspermia' was in the writings of the 5th century BC Greek philosopher Anaxagoras. In the nineteenth century it was again revived in modern form by several scientists, including Jöns Jacob Berzelius (1834), Kelvin (1871), Hermann von Helmholtz (1879) and, somewhat later, by Svante Arrhenius (1903).
Sir Fred Hoyle (1915–2001) and Chandra Wickramasinghe (born 1939) are important proponents of the hypothesis who further contended that lifeforms continue to enter Earth's atmosphere, and may be responsible for epidemic outbreaks, new diseases, and the genetic novelty necessary for macroevolution.
Directed panspermia concerns the deliberate transport of microorganisms in space, sent to Earth to start life here, or sent from Earth to seed new stellar systems with life.
The Nobel prize winner Francis Crick, along with Leslie Orgel proposed that seeds of life may have been purposely spread by an advanced extraterrestrial civilization, but considering an early "RNA world" Crick noted later that life may have originated on Earth.
Mars.
Life on Mars has been long speculated. Liquid water is widely thought to have existed on Mars in the past, and now can occasionally be found as low-volume liquid brines in shallow Martian soil. The origin of the potential biosignature of methane observed in Mars atmosphere is unexplained, although abiotic hypotheses have also been proposed. By July 2008, laboratory tests aboard NASA's "Phoenix" Mars lander identified water in a surface soil sample. Photographs from the Mars Global Surveyor from 2006 showed evidence of recent (i.e. within 10 years) flows of a liquid on Mars's frigid surface.
There is evidence that Mars had a warmer and wetter past: dried-up river beds, polar ice caps, volcanos, and minerals that form in the presence of water have all been found. Nevertheless, present conditions on Mars subsurface may support life. Evidence obtained by the "Curiosity" rover studying Aeolis Palus, Gale Crater in 2013, strongly suggest an ancient freshwater lake that could have been a hospitable environment for microbial life.
Current studies on Mars by the "Curiosity" and "Opportunity" rovers are now searching for evidence of ancient life, including a biosphere based on autotrophic, chemotrophic and/or chemolithoautotrophic microorganisms, as well as ancient water, including fluvio-lacustrine environments (plains related to ancient rivers or lakes) that may have been habitable. The search for evidence of habitability, taphonomy (related to fossils), and organic carbon on Mars is now a primary NASA objective.
Ceres.
Ceres, the only dwarf planet in the asteroid belt, was confirmed by the Herschel Space Observatory to have a thin water vapor atmosphere. Frost on the surface may also have been detected in the form of bright spots. The presence of water on Ceres has led to speculation that life may be possible there.
Jupiter system.
Jupiter.
Carl Sagan and others in the 1960s and 1970s computed conditions for hypothetical microorganisms living in the atmosphere of Jupiter, however, the intense radiation and other conditions do not appear to permit encapsulation and molecular biochemistry, so life there is thought unlikely. In contrast, some of Jupiter's moons may have habitats capable of sustaining life. Scientists have indications that heated subsurface oceans of liquid water may exist deep under the crusts of the three outer Galilean moons—Europa, Ganymede, and Callisto. The EJSM/Laplace mission is planned to determine the habitability of these environments.
Europa.
Jupiter's moon Europa has been subject to speculation about the existence of life due to the strong possibility of a liquid water ocean beneath its ice surface. Hydrothermal vents on the bottom of the ocean, if they exist, may warm the ice and could be capable of supporting multicellular microorganisms. It is also possible that Europa could support aerobic macrofauna using oxygen created by cosmic rays impacting its surface ice.
The case for life on Europa was greatly enhanced in 2011 when it was discovered that vast lakes exist within Europa's thick, icy shell. Scientists found that ice shelves surrounding the lakes appear to be collapsing into them, thereby providing a mechanism through which life-forming chemicals created in sunlit areas on Europa's surface could be transferred to its interior.
On December 11, 2013, NASA reported the detection of "clay-like minerals" (specifically, phyllosilicates), often associated with organic materials, on the icy crust of Europa. The presence of the minerals may have been the result of a collision with an asteroid or comet according to the scientists. The "Europa Multiple-Flyby Mission", which would assess the habitability of Europa, is planned for launch in 2025. Europa's subsurface ocean is considered the best target for the discovery of life.
Saturn system.
Titan and Enceladus have been speculated to have possible habitats supportive of life.
Enceladus.
Enceladus, a moon of Saturn, has some of the conditions for life, including geothermal activity and water vapor, as well as possible under-ice oceans heated by tidal effects. The "Cassini–Huygens" probe detected carbon, hydrogen, nitrogen and oxygen—all key elements for supporting life—during its 2005 flyby through one of Enceladus's geysers spewing ice and gas. The temperature and density of the plumes indicate a warmer, watery source beneath the surface.
Titan.
Titan, the largest moon of Saturn, is the only known moon in the Solar System with a significant atmosphere. Data from the "Cassini–Huygens" mission refuted the hypothesis of a global hydrocarbon ocean, but later demonstrated the existence of liquid hydrocarbon lakes in the polar regions—the first stable bodies of surface liquid discovered outside Earth. Analysis of data from the mission has uncovered aspects of atmospheric chemistry near the surface that are consistent with—but do not prove—the hypothesis that organisms there if present, could be consuming hydrogen, acetylene and ethane, and producing methane.
Small Solar System bodies.
Small Solar System bodies have also been speculated to host habitats for extremophiles. Fred Hoyle and Chandra Wickramasinghe have proposed that microbial life might exist on comets and asteroids.
Scientific search.
The scientific search for extraterrestrial life is being carried out both directly and indirectly.
Direct search.
Scientists search for biosignatures within the Solar System by studying planetary surfaces and examining meteorites. Some claim to have identified evidence that microbial life has existed on Mars. An experiment on the two Viking Mars landers reported gas emissions from heated Martian soil samples that some scientists argue are consistent with the presence of living microorganisms. Lack of corroborating evidence from other experiments on the same samples, indicates that a non-biological reaction is a more likely hypothesis. In 1996, a controversial report stated that structures resembling nanobacteria were discovered in a meteorite, ALH84001, formed of rock ejected from Mars.
In February 2005, NASA scientists reported that they may have found some evidence of present life on Mars. The two scientists, Carol Stoker and Larry Lemke of NASA's Ames Research Center, based their claim on methane signatures found in Mars's atmosphere resembling the methane production of some forms of primitive life on Earth, as well as on their own study of primitive life near the Rio Tinto river in Spain. NASA officials soon distanced NASA from the scientists' claims, and Stoker herself backed off from her initial assertions. Though such methane findings are still debated, support among some scientists for the existence of life on Mars seems to be growing.
In November 2011, NASA launched the Mars Science Laboratory that landed the "Curiosity" rover on Mars. It is designed to assess the past and present habitability on Mars using a variety of scientific instruments. The rover landed on Mars at Gale Crater in August 2012.
The Gaia hypothesis stipulates that any planet with a robust population of life will have an atmosphere in chemical disequilibrium, which is relatively easy to determine from a distance by spectroscopy. However, significant advances in the ability to find and resolve light from smaller rocky worlds near their star are necessary before such spectroscopic methods can be used to analyze extrasolar planets. To that effect, the Carl Sagan Institute was founded in 2014 dedicated to the atmospheric characterization of exoplanets in circumstellar habitable zones. Planetary spectroscopic data will be obtained from telescopes like WFIRST and E-ELT.
In August 2011, findings by NASA, based on studies of meteorites found on Earth, suggests DNA and RNA components (adenine, guanine and related organic molecules), building blocks for life as we know it, may be formed extraterrestrially in outer space. In October 2011, scientists reported that cosmic dust contains complex organic matter ("amorphous organic solids with a mixed aromatic-aliphatic structure") that could be created naturally, and rapidly, by stars. One of the scientists suggested that these compounds may have been related to the development of life on Earth and said that, "If this is the case, life on Earth may have had an easier time getting started as these organics can serve as basic ingredients for life."
In August 2012, and in a world first, astronomers at Copenhagen University reported the detection of a specific sugar molecule, glycolaldehyde, in a distant star system. The molecule was found around the protostellar binary "IRAS 16293-2422", which is located 400 light years from Earth. Glycolaldehyde is needed to form ribonucleic acid, or RNA, which is similar in function to DNA. This finding suggests that complex organic molecules may form in stellar systems prior to the formation of planets, eventually arriving on young planets early in their formation.
Indirect search.
Projects such as SETI are monitoring the galaxy for electromagnetic interstellar communications from civilizations on other worlds. If there is an advanced extraterrestrial civilization, there is no guarantee that it is transmitting radio communications in the direction of Earth or that this information could be interpreted as such by humans. The length of time required for a signal to travel across the vastness of space means that any signal detected, would come from the distant past.
Another biosignature of intelligence, would be to find an abundance of anomalies of heavy elements in the light spectrum of a star, which would take place if the star was used as a repository for nuclear waste material.
Extrasolar planets.
Some astronomers search for extrasolar planets that may be conducive to life, narrowing the search to terrestrial planets within the habitable zone of their star. Since 1992 over two thousand exoplanets have been discovered ( planets in planetary systems including multiple planetary systems as of ).
The extrasolar planets so far discovered range in size from that of terrestrial planets similar to Earth's size to that of gas giants larger than Jupiter. The number of observed exoplanets is expected to increase greatly in the coming years.
The "Kepler space telescope" has also detected a few thousand candidate planets, of which about 11% may be false positives.
There is at least one planet on average per star.
About 1 in 5 Sun-like stars have an "Earth-sized" planet in the habitable zone, with the nearest expected to be within 12 light-years distance from Earth. Assuming 200 billion stars in the Milky Way, that would be 11 billion potentially habitable Earth-sized planets in the Milky Way, rising to 40 billion if red dwarfs are included. The rogue planets in the Milky Way possibly number in the trillions.
The nearest known exoplanet, if confirmed, would be Alpha Centauri Bb, located 4.37 light-years from Earth in the southern constellation of Centaurus. , the least massive planet known is PSR B1257+12 A, which is about twice the mass of the Moon. The most massive planet listed on the NASA Exoplanet Archive is DENIS-P J082303.1-491201 b, about 29 times the mass of Jupiter, although according to most definitions of a planet, it is too massive to be a planet and may be a brown dwarf instead. Almost all of the planets detected so far are within the Milky Way, but there have also been a few possible detections of extragalactic planets. The study of planetary habitability also considers a wide range of other factors in determining the suitability of a planet for hosting life.
The Drake equation.
In 1961, University of California, Santa Cruz, astronomer and astrophysicist Frank Drake devised the Drake equation as a way to stimulate scientific dialogue at a meeting on the search for extraterrestrial intelligence (SETI). The Drake equation is a probabilistic argument used to estimate the number of active, communicative extraterrestrial civilizations in the Milky Way galaxy. The equation is best understood not as an equation in the strictly mathematical sense, but to summarize all the various concepts which scientists must contemplate when considering the question of life elsewhere. The Drake equation is:
where:
and
Drake's proposed estimates are as follows, but numbers on the right side of the equation are agreed as speculative and open to substitution:
formula_2
The Drake equation has proved controversial since several of its factors are uncertain and based on conjecture, not allowing conclusions to be made. This has led critics to label the equation a guesstimate, or even meaningless.
Based on observations from the Hubble Space Telescope, there are between 125 and 250 billion galaxies in the observable universe. It is estimated that at least ten percent of all Sun-like stars have a system of planets, i.e. there are 6.25×10 stars with planets orbiting them in the observable universe. Even if we assume that only one out of a billion of these stars have planets supporting life, there would be some 6.25×10 (billion) life-supporting planetary systems in the observable universe.
The apparent contradiction between high estimates of the probability of the existence of extraterrestrial civilizations and the lack of evidence for such civilizations, is known as the Fermi paradox.
Cultural impact.
Cosmic pluralism.
Cosmic pluralism, the plurality of worlds, or simply pluralism, describes the philosophical belief in numerous "worlds" in addition to Earth, which might harbor extraterrestrial life. Before the development of the heliocentric theory and a recognition that our Sun is just one of many stars, the notion of pluralism was largely mythological and philosophical. With the scientific and Copernican revolutions, and later, during the Enlightenment, cosmic pluralism became a mainstream notion, supported by the likes of Bernard le Bovier de Fontenelle in his 1686 work "Entretiens sur la pluralité des mondes". Pluralism was also championed by philosophers such as John Locke, Giordano Bruno and astronomers such as William Herschel. The astronomer Camille Flammarion promoted the notion of cosmic pluralism in his 1862 book "La pluralité des mondes habités". None of these notions of pluralism were based on any specific observation or scientific information.
Early modern period.
There was a dramatic shift in thinking initiated by the invention of the telescope and the Copernican assault on geocentric cosmology. Once it became clear that Earth was merely one planet amongst countless bodies in the universe, the theory of extraterrestrial life started to become a topic in the scientific community. The best known early-modern proponent of such ideas was the Italian philosopher Giordano Bruno, who argued in the 16th century for an infinite universe in which every star is surrounded by its own planetary system. Bruno wrote that other worlds "have no less virtue nor a nature different to that of our earth" and, like Earth, "contain animals and inhabitants".
In the early 17th century, the Czech astronomer Anton Maria Schyrleus of Rheita mused that "if Jupiter has (...) inhabitants (...) they must be larger and more beautiful than the inhabitants of Earth, in proportion to the haracteristic of the two spheres".
In Baroque literature such as "The Other World: The Societies and Governments of the Moon" by Cyrano de Bergerac, extraterrestrial societies are presented as humoristic or ironic parodies of earthly society.
The didactic poet Henry More took up the classical theme of the Greek Democritus in "Democritus Platonissans, or an Essay Upon the Infinity of Worlds" (1647).
In "The Creation: a Philosophical Poem in Seven Books" (1712), Sir Richard Blackmore observed: "We may pronounce each orb sustains a race / Of living things adapted to the place". With the new relative viewpoint that the Copernican revolution had wrought, he suggested "our world's sunne / Becomes a starre elsewhere". Fontanelle's "Conversations on the Plurality of Worlds" (translated into English in 1686) offered similar excursions on the possibility of extraterrestrial life, expanding, rather than denying, the creative sphere of a Maker.
The possibility of extraterrestrials remained a widespread speculation as scientific discovery accelerated. William Herschel, the discoverer of Uranus, was one of many 18th–19th-century astronomers who believed that the Solar System is populated by alien life. Other luminaries of the period who championed "cosmic pluralism" included Immanuel Kant and Benjamin Franklin. At the height of the Enlightenment, even the Sun and Moon were considered candidates for extraterrestrial inhabitants.
19th century.
Speculation about life on Mars increased in the late 19th century, following telescopic observation of apparent Martian canal—which soon, however, turned out to be optical illusions. Despite this, in 1895, American astronomer Percival Lowell published his book "Mars," followed by "Mars and its Canals" in 1906, proposing that the canals were the work of a long-gone civilization. This idea led British writer H. G. Wells to write the novel "The War of the Worlds" in 1897, telling of an invasion by aliens from Mars who were fleeing the planet's desiccation.
Spectroscopic analysis of Mars's atmosphere began in earnest in 1894, when U.S. astronomer William Wallace Campbell showed that neither water nor oxygen was present in the Martian atmosphere.
By 1909 better telescopes and the best perihelic opposition of Mars since 1877 conclusively put an end to the canal hypothesis.
The science fiction genre, although not so named during the time, developed during the late 19th century. Jules Verne's "Around the Moon" (1870) features a discussion of the possibility of life on the Moon, but with the conclusion that it is barren.
Stories involving extraterrestrials are found in e.g. Garrett P. Serviss's "Edison's Conquest of Mars" (1898), an unauthorized sequel to 
"The War of the Worlds" by H. G. Wells was published in 1897 which stands at the beginning of the popular idea of the "Martian invasion" of Earth prominent in 20th-century pop culture.
20th century.
Most unidentified flying objects or UFO sightings can be readily explained as sightings of Earth-based aircraft, known astronomical objects, or as hoaxes. Nonetheless, a certain fraction of the public believe that UFOs might actually be of extraterrestrial origin, and, indeed, the notion has had influence on popular culture.
The possibility of extraterrestrial life on the Moon was ruled out in the 1960s, and during the 1970s it became clear that most of the other bodies of the Solar System do not harbor highly developed life, although the question of primitive life on bodies in the Solar System remains an open question.
Recent history.
The failure so far of the SETI program to detect an intelligent radio signal after decades of effort has at least partially dimmed the prevailing optimism of the beginning of the space age. Notwithstanding, belief in extraterrestrial beings continues to be voiced in pseudoscience, conspiracy theories, and in popular folklore, notably "Area 51" and legends. It has become a pop culture trope given less-than-serious treatment in popular entertainment.
In the words of SETI's Frank Drake, "All we know for sure is that the sky is not littered with powerful microwave transmitters". Drake noted that it is entirely possible that advanced technology results in communication being carried out in some way other than conventional radio transmission. At the same time, the data returned by space probes, and giant strides in detection methods, have allowed science to begin delineating habitability criteria on other worlds, and to confirm that at least other planets are plentiful, though aliens remain a question mark. The Wow! signal, detected in 1977 by a SETI project, remains a subject of speculative debate.
In 2000, geologist and paleontologist Peter Ward and astrobiologist Donald Brownlee published a book entitled "Rare Earth: Why Complex Life is Uncommon in the Universe". In it, they discussed the Rare Earth hypothesis, in which they claim that Earth-like life is rare in the universe, whereas microbial life is common. Ward and Brownlee are open to the idea of evolution on other planets that is not based on essential Earth-like characteristics (such as DNA and carbon).
Theoretical physicist Stephen Hawking in 2010 warned that humans should not try to contact alien life forms. He warned that aliens might pillage Earth for resources. "If aliens visit us, the outcome would be much as when Columbus landed in America, which didn't turn out well for the Native Americans", he said. Jared Diamond has expressed similar concerns.
In November 2011, the White House released an official response to two petitions asking the U.S. government to acknowledge formally that aliens have visited Earth and to disclose any intentional withholding of government interactions with extraterrestrial beings. According to the response, "The U.S. government has no evidence that any life exists outside our planet, or that an extraterrestrial presence has contacted or engaged any member of the human race." Also, according to the response, there is "no credible information to suggest that any evidence is being hidden from the public's eye." The response noted "odds are pretty high" that there may be life on other planets but "the odds of us making contact with any of them—especially any intelligent ones—are extremely small, given the distances involved."
In 2013, the exoplanet Kepler-62f was discovered, along with Kepler-62e and Kepler-62c. A related special issue of the journal "Science", published earlier, described the discovery of the exoplanets.
On 17 April 2014, the discovery of the Earth-size exoplanet Kepler-186f, 500 light-years from Earth, was publicly announced; it is the first Earth-size planet to be discovered in the habitable zone and it has been hypothesized that there may be liquid water on its surface.
On 13 February 2015, scientists (including Geoffrey Marcy, Seth Shostak, Frank Drake and David Brin) at a convention of the American Association for the Advancement of Science, discussed Active SETI and whether transmitting a message to possible intelligent extraterrestrials in the Cosmos was a good idea; one result was a statement, signed by many, that a "worldwide scientific, political and humanitarian discussion must occur before any message is sent".
On 20 July 2015, Stephen Hawking, British physicist, and Yuri Milner, Russian billionaire, along with the SETI Institute, announced a well-funded effort, called the Breakthrough Initiatives, to expand efforts to search for extraterrestrial life. The group contracted the services of the 100-meter Robert C. Byrd Green Bank Telescope in West Virginia in the United States and the 64-meter Parkes Telescope in New South Wales, Australia.

</doc>
<doc id="9589" url="https://en.wikipedia.org/wiki?curid=9589" title="European Strategic Program on Research in Information Technology">
European Strategic Program on Research in Information Technology

European Strategic Program on Research in Information Technology (ESPRIT) was a series of integrated programmes of information technology research and development projects and industrial technology transfer measures. It was a European Union initiative managed by the Directorate General for Industry (DG III) of the European Commission. Five ESPRIT programmes (ESPRIT 0 to ESPRIT 4) ran consecutively from 1983 to 1998. ESPRIT 4 was succeeded by the Information Society Technologies (IST) programme in 1999.
Some of the projects and products supported by ESPRIT are: 

</doc>
<doc id="9591" url="https://en.wikipedia.org/wiki?curid=9591" title="E. E. Cummings">
E. E. Cummings

Edward Estlin Cummings (October 14, 1894 – September 3, 1962), known as E. E. Cummings, with the abbreviated form of his name often written by others in lowercase letters as e e cummings (in the style of some of his poems—see name and capitalization, below), was an American poet, painter, essayist, author, and playwright. His body of work encompasses approximately 2,900 poems, two autobiographical novels, four plays and several essays, as well as numerous drawings and paintings. He is remembered as an eminent voice of 20th century English literature.
Life.
Early years.
Edward Estlin Cummings was born on October 14, 1894, to Edward Cummings and Rebecca Haswell Clarke who were Unitarian. They were a well-known family in Cambridge, Massachusetts. His father was a professor at Harvard University and later the nationally known minister of Old South Church in Boston, Massachusetts. His mother who loved to spend time with her children, played games with Cummings and his sister, Elizabeth. Cummings wrote poems and also drew as a child, and he often played outdoors with the many other children who lived in his neighborhood. He also grew up in the company of such family friends as the philosophers William James (1842–1910) and Josiah Royce (1855–1916). He graduated from Harvard University in 1915 and then received an advanced degree from Harvard in 1916. Many of Cummings' summers were spent on Silver Lake in Madison, New Hampshire where his father had built two houses along the eastern shore. The family ultimately purchased the nearby Joy Farm where Cummings' had his primary summer residence.
He exhibited transcendental leanings his entire life. As he grew in maturity and age, Cummings moved more toward an "I, Thou" relationship with God. His journals are replete with references to "le bon Dieu," as well as prayers for inspiration in his poetry and artwork (such as “Bon Dieu! may I some day do something truly great. amen.”). Cummings "also prayed for strength to be his essential self ('may I be I is the only prayer—not may I be great or good or beautiful or wise or strong'), and for relief of spirit in times of depression ('almighty God! I thank thee for my soul; & may I never die spiritually into a mere mind through disease of loneliness')."
Cummings wanted to be a poet from childhood and wrote poetry daily aged 8 to 22, exploring assorted forms. He went to Harvard and developed an interest in modern poetry which ignored conventional grammar and syntax, aiming for a dynamic use of language. Upon graduating, he worked for a book dealer.
The war years.
In 1917, with the First World War ongoing in Europe, Cummings enlisted in the Norton-Harjes Ambulance Corps, along with his college friend John Dos Passos. Due to an administrative mix-up, Cummings was not assigned to an ambulance unit for five weeks, during which time he stayed in Paris. He fell in love with the city, to which he would return throughout his life.
During their service in the ambulance corps, they sent letters home that drew the attention of the military censors, and were known to prefer the company of French soldiers over fellow ambulance drivers. The two openly expressed anti-war views; Cummings spoke of his lack of hatred for the Germans. On September 21, 1917, just five months after his belated assignment, he and a friend, William Slater Brown, were arrested by the French military on suspicion of espionage and undesirable activities. They were held for 3½ months in a military detention camp at the "Dépôt de Triage", in La Ferté-Macé, Orne, Normandy.
They were imprisoned with other detainees in a large room. Cummings' father failed to obtain his son's release through diplomatic channels and in December 1917 wrote a letter to President Wilson. Cummings was released on December 19, 1917, and Brown was released two months later. Cummings used his prison experience as the basis for his novel, "The Enormous Room" (1922), about which F. Scott Fitzgerald said, "Of all the work by young men who have sprung up since 1920 one book survives—"The Enormous Room" by e e cummings... Those few who cause books to live have not been able to endure the thought of its mortality." 
Cummings returned to the United States on New Year's Day 1918. Later in 1918 he was drafted into the army. He served in the 12th Division at Camp Devens, Massachusetts, until November 1918.
Post-war years.
Cummings returned to Paris in 1921 and remained there for two years before returning to New York. His collection "Tulips and Chimneys" came in 1923 and his inventive use of grammar and syntax is evident. The book was heavily cut by his editor. "XLI Poems", was then published in 1925. With these collections Cummings made his reputation as an avant garde poet.
During the rest of the 1920s and 1930s Cummings returned to Paris a number of times, and traveled throughout Europe, meeting, among others, Pablo Picasso. In 1931 Cummings traveled to the Soviet Union, recounting his experiences in "Eimi", published two years later. During these years Cummings also traveled to Northern Africa and Mexico and worked as an essayist and portrait artist for "Vanity Fair" magazine (1924–1927).
In 1926, Cummings's parents were in a car accident; only his mother survived, although she was severely injured. Cummings later described the accident in the following passage from his "i: six nonlectures" series given at Harvard (as part of the Charles Eliot Norton Lectures) in 1952 and 1953:
A locomotive cut the car in half, killing my father instantly. When two brakemen jumped from the halted train, they saw a woman standing – dazed but erect – beside a mangled machine; with blood spouting (as the older said to me) out of her head. One of her hands (the younger added) kept feeling her dress, as if trying to discover why it was wet. These men took my sixty-six year old mother by the arms and tried to lead her toward a nearby farmhouse; but she threw them off, strode straight to my father's body, and directed a group of scared spectators to cover him. When this had been done (and only then) she let them lead her away.
His father's death had a profound effect on Cummings, who entered a new period in his artistic life. He began to focus on more important aspects of life in his poetry. He started this new period by paying homage to his father in the poem "my father moved through dooms of love"
In the 1930s Samuel Aiwaz Jacobs was Cummings' publisher; he had started the Golden Eagle Press after working as a typographer and publisher.
Final years.
In 1952, his alma mater, Harvard University awarded Cummings an honorary seat as a guest professor. The Charles Eliot Norton Lectures he gave in 1952 and 1955 were later collected as "i: six nonlectures".
Cummings spent the last decade of his life traveling, fulfilling speaking engagements, and spending time at his summer home, Joy Farm, in Silver Lake, New Hampshire. He died of a stroke on September 3, 1962, at the age of 67 in North Conway, New Hampshire at the Memorial Hospital. His cremated remains were buried in Lot 748 Althaeas Path, in Section 6, Forest Hills Cemetery and Crematory in Boston. In 1969, his third wife, model and photographer Marion Morehouse Cummings, died and was buried in an adjoining plot.
Cummings's papers are held at the Houghton Library at Harvard University and the Harry Ransom Center at the University of Texas at Austin.
Personal life.
Marriages.
Cummings was married briefly twice. First to Elaine Orr, then to Anne Minnerly Barton. His longest relationship lasted more than three decades, a common-law marriage to Marion Morehouse.
Cummings's first marriage, to Elaine Orr, began as a love affair in 1918 while she was still married to Scofield Thayer, one of Cummings's friends from Harvard. During this time he wrote a good deal of his erotic poetry. After divorcing Thayer, Elaine married Cummings on March 19, 1924. The couple had a daughter together out of wedlock, Nancy, born on December 20, 1919. Nancy was Cummings's only child. However, the couple separated after only two months of marriage and divorced less than nine months later. Elaine left Cummings for a wealthy Irish banker, moved to Ireland, and took Nancy with her. Under the terms of the divorce Cummings was granted custody of Nancy for three months each year, but Elaine refused to abide by the agreement. Cummings did not see his daughter again until 1946. Nancy later married Joseph Willard Roosevelt, second son of Kermit Roosevelt and Belle Wyatt Willard.
Cummings married his second wife Anne Minnerly Barton on May 1, 1929, and they separated three years later in 1932. That same year, Anne obtained a Mexican divorce; it was not officially recognized in the United States until August 1934.
In 1934, after his separation from his second wife, Cummings met Marion Morehouse, a fashion model and photographer. Although it is not clear whether the two were ever formally married, Morehouse lived with Cummings in a common-law marriage until his death in 1962. She died on May 18, 1969, while living at 4 Patchin Place, Greenwich Village, New York City, where Cummings had resided since September 8, 1924.
Political views.
According to his testimony in "EIMI", Cummings had little interest in politics until his trip to the Soviet Union in 1931, after which he shifted rightward on many political and social issues. Despite his radical and bohemian public image, he was a Republican and, later, an ardent supporter of Joseph McCarthy.
Work.
Poetry.
Despite Cummings's familiarity with avant-garde styles (undoubtedly affected by the "Calligrammes" of Apollinaire, according to a contemporary observation), much of his work is quite traditional. Many of his poems are sonnets, albeit often with a modern twist, and he occasionally made use of the blues form and acrostics. Cummings' poetry often deals with themes of love and nature, as well as the relationship of the individual to the masses and to the world. His poems are also often rife with satire.
While his poetic forms and themes share an affinity with the romantic tradition, Cummings' work universally shows a particular idiosyncrasy of syntax, or way of arranging individual words into larger phrases and sentences. Many of his most striking poems do not involve any typographical or punctuation innovations at all, but purely syntactic ones.
As well as being influenced by notable modernists, including Gertrude Stein and Ezra Pound, Cummings in his early work drew upon the imagist experiments of Amy Lowell. Later, his visits to Paris exposed him to Dada and surrealism, which he reflected in his work. He began to rely on symbolism and allegory where he once used simile and metaphor. In his later work, he rarely used comparisons that required objects that were not previously mentioned in the poem, choosing to use a symbol instead. Due to this, his later poetry is "frequently more lucid, more moving, and more profound than his earlier." Cummings also liked to incorporate imagery of nature and death into much of his poetry.
While some of his poetry is free verse (with no concern for rhyme or meter), many have a recognizable sonnet structure of 14 lines, with an intricate rhyme scheme. A number of his poems feature a typographically exuberant style, with words, parts of words, or punctuation symbols scattered across the page, often making little sense until read aloud, at which point the meaning and emotion become clear. Cummings, who was also a painter, understood the importance of presentation, and used typography to "paint a picture" with some of his poems.
The seeds of Cummings' unconventional style appear well established even in his earliest work. At age six, he wrote to his father:
Following his autobiographical novel, "The Enormous Room", Cummings' first published work was a collection of poems entitled "Tulips and Chimneys" (1923). This work was the public's first encounter with his characteristic eccentric use of grammar and punctuation.
Some of Cummings' most famous poems do not involve much, if any, odd typography or punctuation, but still carry his unmistakable style, particularly in unusual and impressionistic word order.
Cummings' work often does not proceed in accordance with the conventional combinatorial rules that generate typical English sentences (for example, "they sowed their isn't"). His readings of Stein in the early part of the century probably served as a springboard to this aspect of his artistic development. In some respects, Cummings' work is more stylistically continuous with Stein's than with any other poet or writer.
In addition, a number of Cummings' poems feature, in part or in whole, intentional misspellings, and several incorporate phonetic spellings intended to represent particular dialects. Cummings also made use of inventive formations of compound words, as in "in Just" which features words such as "mud-luscious", "puddle-wonderful", and "eddieandbill." This poem is part of a sequence of poems entitled "Chansons Innocentes"; it has many references comparing the "balloonman" to Pan, the mythical creature that is half-goat and half-man. Literary critic R.P. Blackmur has commented that this usage of language is “frequently unintelligible because he disregards the historical accumulation of meaning in words in favour of merely private and personal associations.”
Many of Cummings' poems are satirical and address social issues but have an equal or even stronger bias toward romanticism: time and again his poems celebrate love, sex, and the season of rebirth.
Cummings also wrote children's books and novels. A notable example of his versatility is an introduction he wrote for a collection of the comic strip "Krazy Kat".
Controversy.
Cummings is also known for controversial subject matter, as he has a large collection of erotic poetry. In his 1950 collection "Xaipe: Seventy-One Poems", Cummings published two poems containing words that caused an outrage in some quarters.
</poem>
and
<poem>
</poem>
Cummings biographer Catherine Reef notes of the incident:
Friends begged Cummings to reconsider publishing these poems, and the book's editor pleaded with him to withdraw them, but he insisted that they stay. All the fuss perplexed him. The poems were commenting on prejudice, he pointed out, and not condoning it. He intended to show how derogatory words cause people to see others in terms of stereotypes rather than as individuals. "America (which turns Hungarian into 'hunky' & Irishman into 'mick' and Norwegian into 'square- head') is to blame for 'kike,'" he said.
William Carlos Williams spoke out in his defence.
Plays.
During his lifetime, Cummings published four plays. "HIM", a three-act play, was first produced in 1928 by the Provincetown Players in New York City. The production was directed by James Light. The play's main characters are "Him", a playwright, and "Me", his girlfriend. Cummings said of the unorthodox play:
Relax and give the play a chance to strut its stuff—relax, stop wondering what it is all 'about'—like many strange and familiar things, Life included, this play isn't 'about,' it simply is. . . . Don't try to enjoy it, let it try to enjoy you. "
"Anthropos, or the Future of Art" is a short, one-act play that Cummings contributed to the anthology "Whither, Whither or After Sex, What? A Symposium to End Symposium". The play consists of dialogue between Man, the main character, and three "infrahumans", or inferior beings. The word "anthropos" is the Greek word for "man", in the sense of "mankind".
"Tom, A Ballet" is a ballet based on "Uncle Tom's Cabin". The ballet is detailed in a "synopsis" as well as descriptions of four "episodes", which were published by Cummings in 1935. It has never been performed.
"" was probably Cummings's most successful play. It is an allegorical Christmas fantasy presented in one act of five scenes. The play was inspired by his daughter Nancy, with whom he was reunited in 1946. It was first published in the Harvard College magazine the "Wake". The play's main characters are Santa Claus, his family (Woman and Child), Death, and Mob. At the outset of the play, Santa Claus's family has disintegrated due to their lust for knowledge (Science). After a series of events, however, Santa Claus's faith in love and his rejection of the materialism and disappointment he associates with Science are reaffirmed, and he is reunited with Woman and Child.
Name and capitalization.
Cummings's publishers and others have sometimes echoed the unconventional orthography in his poetry by writing his name in lowercase and without periods (full stops), but normal orthography (uppercase and full stops) is supported by scholarship and preferred by publishers today. Cummings himself used both the lowercase and capitalized versions, though he most often signed his name with capitals.
The use of lowercase for his initials was popularized in part by the title of some books, particularly in the 1960s, printing his name in lower case on the cover and spine. In the preface to "E. E. Cummings: The Growth of a Writer" by Norman Friedman, critic Harry T. Moore notes, "He umming had his name put legally into lower case, and in his later books the titles and his name were always in lower case." According to Cummings's widow, however, this is incorrect. She wrote to Friedman: "You should not have allowed H. Moore to make such a stupid & childish statement about Cummings & his signature." On February 27, 1951, Cummings wrote to his French translator D. Jon Grossman that he preferred the use of upper case for the particular edition they were working on. One Cummings scholar believes that on the rare occasions Cummings signed his name in all lowercase, he may have intended it as a gesture of humility, not as an indication that it was the preferred orthography for others to use.
Adaptations.
In 1943, modern dancer and choreographer Jean Erdman presented "The Transformations of Medusa, Forever and Sunsmell" with a commissioned score by John Cage and a spoken text from the titular poem by E. E. Cummings, sponsored by the Arts Club of Chicago. Erdman also choreographed "Twenty Poems" (1960), a cycle of E. E. Cummings's poems for eight dancers and one actor with a commissioned score by Teiji Ito, performed in the round at the Circle in the Square Theatre in Greenwich Village.
In 1961, Pierre Boulez composed "Cummings ist der dichter" from poems by E.E. Cummings.
Aribert Reimann set Cummings to music in "Impression IV" (1961) for soprano and piano.
The Icelandic singer Björk used lines from Cummings's poem "I Will Wade Out" for the lyrics of "Sun In My Mouth" on her 2001 album "Vespertine." On her next album, "Medúlla" (2004), Björk used his poem "It May Not Always Be So" as the lyrics for the song "Sonnets/Unrealities XI."
The 2007 Ra Ra Riot song, "Dying Is Fine," is based on the Cummings' poem "dying is fine)but death".
American post-hardcore band La Dispute's first spoken-word/experimental album Here, Hear. includes a song based on Cumming's "somewhere i have never traveled".
The Bloc Party song, "Ion Square", on their album "Intimacy", contains lines from Cummings' poem "I carry your heart with me".
The American composer Eric Whitacre wrote a cycle of works for choir entitled "The City and the Sea", which consists of five poems by Cummings set to music.
Numerous composers have set Cummings' poems to music. Among them are Aki Takase, Dominic Argento, Gary Backlund, William Bergsma, Michael Hedges, Leonard Bernstein, Allen Blank, Marc Blitzstein, John Cage, Romeo Cascarino, Aaron Copland, Serge de Gastyne, David Diamond, John Duke, Brian Fennelly, Margaret Garwood, John Gruen, Daron Hagen, Richard Hundley, Barbara Kolb, Robert Manno, Salvatore Martirano, William Mayer, John Musto, Paul Nordoff, Tobias Picker, Vincent Persichetti, Ned Rorem, Peter Schickele, Elie Siegmeister, Hugo Weisgall, Dan Welcher, Eric Whitacre, and James Yannatos, among many others.
Awards.
During his lifetime, Cummings received numerous awards in recognition of his work, including:

</doc>
<doc id="9592" url="https://en.wikipedia.org/wiki?curid=9592" title="East River">
East River

The East River is a salt water tidal strait in New York City. The waterway, which is actually not a river despite its name, connects Upper New York Bay on its south end to Long Island Sound on its north end. It separates Long Island – including the boroughs of Queens and Brooklyn – from the Bronx on the North American mainland, and the island of Manhattan. Because of its connection to Long Island Sound, it was once also known as the "Sound River". The tidal strait changes its flow direction frequently.
Formation.
The strait was formed approximately 11,000 years ago at the end of the Wisconsin glaciation. The distinct change in the shape of the strait between the lower and upper portions is evidence of this glacial activity. The upper portion (from Long Island Sound to Hell Gate), running largely perpendicular to the glacial motion, is wide, meandering, and has deep narrow bays on both banks, scoured out by the glacier's movement. The lower portion (from Hell Gate to New York Bay) runs north-south, parallel to the glacial motion. It is much narrower, with straight banks. The bays that exist (or existed before being filled in by human activity), are largely wide and shallow.
The channel.
Historically, the lower portion of the strait (separating Manhattan from Brooklyn) was one of the busiest and most important channels in the world, particularly during the first three centuries of New York City's history. The Brooklyn Bridge, opened in 1883, was the first bridge to span the strait, replacing frequent ferry service. Some passenger ferry service remains between Manhattan, and Queens and Brooklyn.
Due to heavy pollution, the East River is dangerous to people who fall in or attempt to swim in it, although as of mid-2007 the water was cleaner than it had been in decades. As of 2010, the New York City Department of Environmental Protection categorizes the East River as Use Classification I, meaning it is safe for secondary contact activities such as boating and fishing. According to the marine sciences section of the city Department of Environmental Protection, the channel is swift, with water moving as fast as four knots (just as it does in the Hudson River on the other side of Manhattan). That speed can push casual swimmers out to sea. A few people drown in the waters around New York City each year. The strength of the current foiled an effort in 2007 to tap it for tidal power. However, in February 2012 the federal government announced an agreement with Verdant Power to install 30 tidal turbines in the channel, projected to begin operations in 2015 and produce 1.05 MW of power.
Tributaries.
The Bronx River drains into the East River in the northern section of the strait.
North of Randalls Island, it is joined by the Bronx Kill. Along the east of Wards Island, at approximately the strait's midpoint, it narrows into a channel called Hell Gate, which is spanned by both the Robert F. Kennedy Bridge (formerly the Triborough), and the Hell Gate Bridge. On the south side of Wards Island, it is joined by the Harlem River.
Newtown Creek on Long Island drains into the East River, forming part of the boundary between Queens and Brooklyn.
The East River contains a number of islands, including:
In popular culture.
Music
Television
Games
References.
Notes

</doc>
<doc id="9593" url="https://en.wikipedia.org/wiki?curid=9593" title="Existentialism">
Existentialism

Existentialism () is a term applied to the work of certain late 19th- and 20th-century European philosophers who, despite profound doctrinal differences, shared the belief that philosophical thinking begins with the human subject—not merely the thinking subject, but the acting, feeling, living human individual. While the supreme value of existentialist thought is commonly acknowledged to be freedom, its primary virtue is authenticity. In the view of the existentialist, the individual's starting point is characterized by what has been called "the existential attitude", or a sense of disorientation and confusion in the face of an apparently meaningless or absurd world. Many existentialists have also regarded traditional systematic or academic philosophies, in both style and content, as too abstract and remote from concrete human experience.
Søren Kierkegaard is generally considered to have been the first existentialist philosopher, though he did not use the term existentialism. He proposed that each individual—not society or religion—is solely responsible for giving meaning to life and living it passionately and sincerely ("authentically"). Existentialism became popular in the years following World War II, and strongly influenced many disciplines besides philosophy, including theology, drama, art, literature, and psychology.
Definitional issues and background.
There has never been general agreement on the definition of existentialism. The term is often seen as an historical convenience as it was first applied to many philosophers in hindsight, long after they had died. In fact, while existentialism is generally considered to have originated with Kierkegaard, the first prominent existentialist philosopher to adopt the term as a self-description was Jean-Paul Sartre. Sartre posits the idea that "what all existentialists have in common is the fundamental doctrine that existence precedes essence", as scholar Frederick Copleston explains. According to philosopher Steven Crowell, defining existentialism has been relatively difficult, and he argues that it is better understood as a general approach used to reject certain systematic philosophies rather than as a systematic philosophy itself. Sartre himself, in a lecture delivered in 1945, described existentialism as "the attempt to draw all the consequences from a position of consistent atheism."
Although many outside Scandinavia consider the term existentialism to have originated from Kierkegaard himself, it is more likely that Kierkegaard adopted this term (or at least the term "existential" as a description of his philosophy) from the Norwegian poet and literary critic Johan Sebastian Cammermeyer Welhaven. This assertion comes from two sources. The Norwegian philosopher Erik Lundestad refers to the Danish philosopher Fredrik Christian Sibbern. Sibbern is supposed to have had two conversations in 1841, the first with Welhaven and the second with Kierkegaard. It is in the first conversation that it is believed that Welhaven came up with "a word that he said covered a certain thinking, which had a close and positive attitude to life, a relationship he described as existential". This was then brought to Kierkegaard by Sibbern.
The second claim comes from the Norwegian historian Rune Slagstad, who claims to prove that Kierkegaard himself said the term "existential" was borrowed from the poet. He strongly believes that it was Kierkegaard himself who said that "Hegelians do not study philosophy 'existentially'; to use a phrase by Welhaven from one time when I spoke with him about philosophy". On the other hand, the Norwegian historian Anne-Lise Seip is critical of Slagstad, and believes the statement in fact stems from the Norwegian literary historian Cathrinus Bang.
Concepts.
Existence precedes essence.
A central proposition of Existentialism is that "existence precedes essence", which means that the most important consideration for individuals is that they are individuals—independently acting and responsible, conscious beings ("existence")—rather than what labels, roles, stereotypes, definitions, or other preconceived categories the individuals fit ("essence"). The actual life of the individuals is what constitutes what could be called their "true essence" instead of there being an arbitrarily attributed essence others use to define them. Thus, human beings, through their own consciousness, create their own values and determine a meaning to their life. Although it was Sartre who explicitly coined the phrase, similar notions can be found in the thought of existentialist philosophers such as Heidegger, and Kierkegaard:
Some interpret the imperative to define oneself as meaning that anyone can wish to be anything. However, an existentialist philosopher would say such a wish constitutes an inauthentic existence. Instead, the phrase should be taken to say that people are (1) defined only insofar as they act and (2) that they are responsible for their actions. For example, someone who acts cruelly towards other people is, by that act, defined as a cruel person. Furthermore, by this action of cruelty, such persons are themselves responsible for their new identity (cruel persons). This is as opposed to their genes, or "human nature", bearing the blame.
As Sartre writes in his work "Existentialism is a Humanism": "... man first of all exists, encounters himself, surges up in the world—and defines himself afterwards." Of course, the more positive, therapeutic aspect of this is also implied: A person can choose to act in a different way, and to be a good person instead of a cruel person. Here it is also clear that since humans can choose to be either cruel or good, they are, in fact, neither of these things essentially.
The Absurd.
The notion of the Absurd contains the idea that there is no meaning in the world beyond what meaning we give it. This meaninglessness also encompasses the amorality or "unfairness" of the world. This contrasts with the notion that "bad things don't happen to good people"; to the world, metaphorically speaking, there is no such thing as a good person or a bad person; what happens happens, and it may just as well happen to a "good" person as to a "bad" person.
Because of the world's absurdity, at any point in time, anything can happen to anyone, and a tragic event could plummet someone into direct confrontation with the Absurd. The notion of the Absurd has been prominent in literature throughout history. Many of the literary works of Søren Kierkegaard, Samuel Beckett, Franz Kafka, Fyodor Dostoyevsky, Eugène Ionesco, Miguel de Unamuno, Luigi Pirandello, Jean-Paul Sartre, Joseph Heller and Albert Camus contain descriptions of people who encounter the absurdity of the world.
It is in relation to the concept of the devastating awareness of meaninglessness that Albert Camus claimed that "there is only one truly serious philosophical problem, and that is suicide" in his "The Myth of Sisyphus". Although "prescriptions" against the possibly deleterious consequences of these kinds of encounters vary, from Kierkegaard's religious "stage" to Camus' insistence on persevering in spite of absurdity, the concern with helping people avoid living their lives in ways that put them in the perpetual danger of having everything meaningful break down is common to most existentialist philosophers. The possibility of having everything meaningful break down poses a threat of quietism, which is inherently against the existentialist philosophy. It has been said that the possibility of suicide makes all humans existentialists.
Facticity.
Facticity is a concept defined by Sartre in "Being and Nothingness" as the "in-itself", of which humans are in the mode of not being. This can be more easily understood when considering it in relation to the temporal dimension of past: one's past is what one is, in the sense that it co-constitutes oneself. However, to say that one is only one's past would be to ignore a significant part of reality (the present and the future), while saying that one's past is only what one was, would entirely detach it from oneself now. A denial of one's own concrete past constitutes an inauthentic lifestyle, and the same goes for all other kinds of facticity (having a body—e.g. one that doesn't allow a person to run faster than the speed of sound—identity, values, etc.).
Facticity is both a limitation and a condition of freedom. It is a limitation in that a large part of one's facticity consists of things one couldn't have chosen (birthplace, etc.), but a condition in the sense that one's values most likely depend on it. However, even though one's facticity is "set in stone" (as being past, for instance), it cannot determine a person: The value ascribed to one's facticity is still ascribed to it freely by that person. As an example, consider two men, one of whom has no memory of his past and the other remembers everything. They have both committed many crimes, but the first man, knowing nothing about this, leads a rather normal life while the second man, feeling trapped by his own past, continues a life of crime, blaming his own past for "trapping" him in this life. There is nothing essential about his committing crimes, but he ascribes this meaning to his past.
However, to disregard one's facticity when, in the continual process of self-making, one projects oneself into the future, that would be to put oneself in denial of oneself, and would thus be inauthentic. In other words, the origin of one's projection must still be one's facticity, though in the mode of not being it (essentially). Another aspect of facticity is that it entails angst, both in the sense that freedom "produces" angst when limited by facticity, and in the sense that the lack of the possibility of having facticity to "step in" for one to take responsibility for something one has done also produces angst.
Another aspect of existential freedom is that one can change one's values. Thus, one is responsible for one's values, regardless of society's values. The focus on freedom in existentialism is related to the limits of the responsibility one bears as a result of one's freedom: the relationship between freedom and responsibility is one of interdependency, and a clarification of freedom also clarifies that for which one is responsible.
Authenticity.
Many noted existentialist writers consider the theme of authentic existence important. Authentic existence involves the idea that one has to "create oneself" and then live in accordance with this self. What is meant by authenticity is that in acting, one should act as oneself, not as "one" acts or as "one's genes" or any other essence requires. The authentic act is one that is in accordance with one's freedom. Of course, as a condition of freedom is facticity, this includes one's facticity, but not to the degree that this facticity can in any way determine one's choices (in the sense that one could then blame one's background for making the choice one made). The role of facticity in relation to authenticity involves letting one's actual values come into play when one makes a choice (instead of, like Kierkegaard's Aesthete, "choosing" randomly), so that one also takes responsibility for the act instead of choosing either-or without allowing the options to have different values.
In contrast to this, the inauthentic is the denial to live in accordance with one's freedom. This can take many forms, from pretending choices are meaningless or random, through convincing oneself that some form of determinism is true, to a sort of "mimicry" where one acts as "one should." How "one" should act is often determined by an image one has of how one such as oneself (say, a bank manager, lion tamer, prostitute, etc.) acts. This image usually corresponds to some sort of social norm, but this does not mean that all acting in accordance with social norms is inauthentic: The main point is the attitude one takes to one's own freedom and responsibility, and the extent to which one acts in accordance with this freedom.
The Other and the Look.
The Other (when written with a capital "O") is a concept more properly belonging to phenomenology and its account of intersubjectivity. However, the concept has seen widespread use in existentialist writings, and the conclusions drawn from it differ slightly from the phenomenological accounts. The experience of the Other is the experience of another free subject who inhabits the same world as a person does. In its most basic form, it is this experience of the Other that constitutes intersubjectivity and objectivity. To clarify, when one experiences someone else, and this Other person experiences the world (the same world that a person experiences)--only from "over there"—the world itself is constituted as objective in that it is something that is "there" as identical for both of the subjects; a person experiences the other person as experiencing the same things. This experience of the Other's look is what is termed the Look (sometimes the Gaze).
While this experience, in its basic phenomenological sense, constitutes the world as objective, and oneself as objectively existing subjectivity (one experiences oneself as seen in the Other's Look in precisely the same way that one experiences the Other as seen by him, as subjectivity), in existentialism, it also acts as a kind of limitation of freedom. This is because the Look tends to objectify what it sees. As such, when one experiences oneself in the Look, one doesn't experience oneself as nothing (no thing), but as something. Sartre's own example of a man peeping at someone through a keyhole can help clarify this: at first, this man is entirely caught up in the situation he is in; he is in a pre-reflexive state where his entire consciousness is directed at what goes on in the room. Suddenly, he hears a creaking floorboard behind him, and he becomes aware of himself as seen by the Other. He is thus filled with shame for he perceives himself as he would perceive someone else doing what he was doing, as a Peeping Tom. The Look is then co-constitutive of one's facticity.
Another characteristic feature of the Look is that no Other really needs to have been there: It is quite possible that the creaking floorboard was nothing but the movement of an old house; the Look isn't some kind of mystical telepathic experience of the actual way the other sees one (there may also have been someone there, but he could have not noticed that the person was there). It is only one's perception of the way another might perceive him.
Angst and Dread.
"Existential angst", sometimes called dread, anxiety, or anguish, is a term that is common to many existentialist thinkers. It is generally held to be a negative feeling arising from the experience of human freedom and responsibility. The archetypal example is the experience one has when standing on a cliff where one not only fears falling off it, but also dreads the possibility of throwing oneself off. In this experience that "nothing is holding me back", one senses the lack of anything that predetermines one to either throw oneself off or to stand still, and one experiences one's own freedom.
It can also be seen in relation to the previous point how angst is before nothing, and this is what sets it apart from fear that has an object. While in the case of fear, one can take definitive measures to remove the object of fear, in the case of angst, no such "constructive" measures are possible. The use of the word "nothing" in this context relates both to the inherent insecurity about the consequences of one's actions, and to the fact that, in experiencing freedom as angst, one also realizes that one is fully responsible for these consequences. There is nothing in people (genetically, for instance) that acts in their stead—that they can blame if something goes wrong. Therefore, not every choice is perceived as having dreadful possible consequences (and, it can be claimed, human lives would be unbearable if every choice facilitated dread). However, this doesn't change the fact that freedom remains a condition of every action.
Despair.
Despair, in existentialism, is generally defined as a loss of hope. More specifically, it is a loss of hope in reaction to a breakdown in one or more of the defining qualities of one's self or identity. If a person is invested in being a particular thing, such as a bus driver or an upstanding citizen, and then finds his being-thing compromised, he would normally be found in state of despair — a hopeless state. For example, a singer who loses the ability to sing may despair if she has nothing else to fall back on—nothing to rely on for her identity. She finds herself unable to be what defined her being.
What sets the existentialist notion of despair apart from the conventional definition is that existentialist despair is a state one is in even when he isn't overtly in despair. So long as a person's identity depends on qualities that can crumble, he is in perpetual despair—and as there is, in Sartrean terms, no human essence found in conventional reality on which to constitute the individual's sense of identity, despair is a universal human condition. As Kierkegaard defines it in "Either/Or": "Let each one learn what he can; both of us can learn that a person’s unhappiness never lies in his lack of control over external conditions, since this would only make him completely unhappy." In "Works of Love", he said: 
Opposition to positivism and rationalism.
Existentialists oppose definitions of human beings as primarily rational, and, therefore, oppose positivism and rationalism. Existentialism asserts that people actually make decisions based on subjective meaning rather than pure rationality. The rejection of reason as the source of meaning is a common theme of existentialist thought, as is the focus on the feelings of anxiety and dread that we feel in the face of our own radical freedom and our awareness of death. Kierkegaard advocated rationality as means to interact with the objective world (e.g. in the natural sciences), but when it comes to existential problems, reason is insufficient: "Human reason has boundaries".
Like Kierkegaard, Sartre saw problems with rationality, calling it a form of "bad faith", an attempt by the self to impose structure on a world of phenomena—"the Other"—that is fundamentally irrational and random. According to Sartre, rationality and other forms of bad faith hinder people from finding meaning in freedom. To try to suppress their feelings of anxiety and dread, people confine themselves within everyday experience, Sartre asserts, thereby relinquishing their freedom and acquiescing to being possessed in one form or another by "the Look" of "the Other" (i.e. possessed by another person—or at least one's idea of that other person).
Existentialism and religion.
An existentialist reading of the Bible would demand that the reader recognize that he is an existing subject studying the words more as a recollection of events. This is in contrast to looking at a collection of "truths" that are outside and unrelated to the reader, but may develop a sense of reality/God. Such a reader is not obligated to follow the commandments as if an external agent is forcing them upon him, but as though they are inside him and guiding him from inside. This is the task Kierkegaard takes up when he asks: "Who has the more difficult task: the teacher who lectures on earnest things a meteor's distance from everyday life-or the learner who should put it to use?"
Existentialism and nihilism.
Although nihilism and existentialism are distinct philosophies, they are often confused with one another. A primary cause of confusion is that Friedrich Nietzsche is an important philosopher in both fields, but also the existentialist insistence on the inherent meaninglessness of the world. Existentialist philosophers often stress the importance of Angst as signifying the absolute lack of any objective ground for action, a move that is often reduced to a moral or an existential nihilism. A pervasive theme in the works of existentialist philosophy, however, is to persist through encounters with the absurd, as seen in Camus' "The Myth of Sisyphus" ("One must imagine Sisyphus happy"), and it is only very rarely that existentialist philosophers dismiss morality or one's self-created meaning: Kierkegaard regained a sort of morality in the religious (although he wouldn't himself agree that it was ethical; the religious suspends the ethical), and Sartre's final words in "Being and Nothingness" are "All these questions, which refer us to a pure and not an accessory (or impure) reflection, can find their reply only on the ethical plane. We shall devote to them a future work."
Etymology.
The term "existentialism" was coined by the French Catholic philosopher Gabriel Marcel in the mid-1940s. At first, when Marcel applied the term to him at a colloquium in 1945, Jean-Paul Sartre rejected it. Sartre subsequently changed his mind and, on October 29, 1945, publicly adopted the existentialist label in a lecture to the "Club Maintenant" in Paris. The lecture was published as "L'existentialisme est un humanisme" (Existentialism is a Humanism), a short book that did much to popularize existentialist thought.
Some scholars argue that the term should be used only to refer to the cultural movement in Europe in the 1940s and 1950s associated with the works of the philosophers Jean-Paul Sartre, Simone de Beauvoir, Maurice Merleau-Ponty, and Albert Camus. Other scholars extend the term to Kierkegaard, and yet others extend it as far back as Socrates. However, the term is often identified with the philosophical views of Jean-Paul Sartre.
History.
19th century.
Kierkegaard and Nietzsche.
Søren Kierkegaard and Friedrich Nietzsche were two of the first philosophers considered fundamental to the existentialist movement, though neither used the term "existentialism" and it is unclear whether they would have supported the existentialism of the 20th century. They focused on subjective human experience rather than the objective truths of mathematics and science, which they believed were too detached or observational to truly get at the human experience. Like Pascal, they were interested in people's quiet struggle with the apparent meaninglessness of life and the use of diversion to escape from boredom. Unlike Pascal, Kierkegaard and Nietzsche also considered the role of making free choices, particularly regarding fundamental values and beliefs, and how such choices change the nature and identity of the chooser. Kierkegaard's knight of faith and Nietzsche's Übermensch are representative of people who exhibit Freedom, in that they define the nature of their own existence. Nietzsche's idealized individual invents his own values and creates the very terms they excel under. By contrast, Kierkegaard, opposed to the level of abstraction in Hegel, and not nearly as hostile (actually welcoming) to Christianity as Nietzsche, argues through a pseudonym that the objective certainty of religious truths (specifically Christian) is not only impossible, but even founded on logical paradoxes. Yet he continues to imply that a leap of faith is a possible means for an individual to reach a higher stage of existence that transcends and contains both an aesthetic and ethical value of life. Kierkegaard and Nietzsche were also precursors to other intellectual movements, including postmodernism, and various strands of psychology. However, Kierkegaard believed that individuals should live in accordance with their thinking.
Dostoyevsky.
The first important literary author also important to existentialism was the Russian Fyodor Dostoyevsky. Dostoyevsky's "Notes from Underground" portrays a man unable to fit into society and unhappy with the identities he creates for himself. Jean-Paul Sartre, in his book on existentialism "Existentialism is a Humanism", quoted Dostoyevsky's "The Brothers Karamazov" as an example of existential crisis. Sartre attributes Ivan Karamazov's claim, "If God did not exist, everything would be permitted" to Dostoyevsky himself, though this quote does not appear in the novel. However, a similar sentiment is explicitly stated when Alyosha visits Dimitri in prison. Dimitri mentions his conversations with Rakitin in which the idea that "Then, if He doesn't exist, man is king of the earth, of the universe" allowing the inference contained in Sartre's attribution to remain a valid idea contested within the novel. Other Dostoyevsky novels covered issues raised in existentialist philosophy while presenting story lines divergent from secular existentialism: for example, in "Crime and Punishment", the protagonist Raskolnikov experiences an existential crisis and then moves toward a Christian Orthodox worldview similar to that advocated by Dostoyevsky himself.
Early 20th century.
In the first decades of the 20th century, a number of philosophers and writers explored existentialist ideas. The Spanish philosopher Miguel de Unamuno y Jugo, in his 1913 book "The Tragic Sense of Life in Men and Nations", emphasized the life of "flesh and bone" as opposed to that of abstract rationalism. Unamuno rejected systematic philosophy in favor of the individual's quest for faith. He retained a sense of the tragic, even absurd nature of the quest, symbolized by his enduring interest in Cervantes' fictional character Don Quixote. A novelist, poet and dramatist as well as philosophy professor at the University of Salamanca, Unamuno wrote a short story about a priest's crisis of faith, "Saint Manuel the Good, Martyr", which has been collected in anthologies of existentialist fiction. Another Spanish thinker, Ortega y Gasset, writing in 1914, held that human existence must always be defined as the individual person combined with the concrete circumstances of his life: "Yo soy yo y mi circunstancia" ("I am myself and my circumstances"). Sartre likewise believed that human existence is not an abstract matter, but is always situated ("en situation").
Although Martin Buber wrote his major philosophical works in German, and studied and taught at the Universities of Berlin and Frankfurt, he stands apart from the mainstream of German philosophy. Born into a Jewish family in Vienna in 1878, he was also a scholar of Jewish culture and involved at various times in Zionism and Hasidism. In 1938, he moved permanently to Jerusalem. His best-known philosophical work was the short book "I and Thou", published in 1922. For Buber, the fundamental fact of human existence, too readily overlooked by scientific rationalism and abstract philosophical thought, is "man with man", a dialogue that takes place in the so-called "sphere of between" ("das Zwischenmenschliche").
Two Ukrainian/Russian thinkers, Lev Shestov and Nikolai Berdyaev, became well known as existentialist thinkers during their post-Revolutionary exiles in Paris. Shestov, born into a Ukrainian-Jewish family in Kiev, had launched an attack on rationalism and systematization in philosophy as early as 1905 in his book of aphorisms "All Things Are Possible".
Berdyaev, also from Kiev but with a background in the Eastern Orthodox Church, drew a radical distinction between the world of spirit and the everyday world of objects. Human freedom, for Berdyaev, is rooted in the realm of spirit, a realm independent of scientific notions of causation. To the extent the individual human being lives in the objective world, he is estranged from authentic spiritual freedom. "Man" is not to be interpreted naturalistically, but as a being created in God's image, an originator of free, creative acts. He published a major work on these themes, "The Destiny of Man", in 1931.
Gabriel Marcel, long before coining the term "existentialism", introduced important existentialist themes to a French audience in his early essay "Existence and Objectivity" (1925) and in his "Metaphysical Journal" (1927). A dramatist as well as a philosopher, Marcel found his philosophical starting point in a condition of metaphysical alienation: the human individual searching for harmony in a transient life. Harmony, for Marcel, was to be sought through "secondary reflection", a "dialogical" rather than "dialectical" approach to the world, characterized by "wonder and astonishment" and open to the "presence" of other people and of God rather than merely to "information" about them. For Marcel, such presence implied more than simply being there (as one thing might be in the presence of another thing); it connoted "extravagant" availability, and the willingness to put oneself at the disposal of the other.
Marcel contrasted "secondary reflection" with abstract, scientific-technical "primary reflection", which he associated with the activity of the abstract Cartesian ego. For Marcel, philosophy was a concrete activity undertaken by a sensing, feeling human being incarnate — embodied — in a concrete world. Although Jean-Paul Sartre adopted the term "existentialism" for his own philosophy in the 1940s, Marcel's thought has been described as "almost diametrically opposed" to that of Sartre. Unlike Sartre, Marcel was a Christian, and became a Catholic convert in 1929.
In Germany, the psychologist and philosopher Karl Jaspers — who later described existentialism as a "phantom" created by the public — called his own thought, heavily influenced by Kierkegaard and Nietzsche, "Existenzphilosophie". For Jaspers, ""Existenz"-philosophy is the way of thought by means of which man seeks to become himself...This way of thought does not cognize objects, but elucidates and makes actual the being of the thinker."
Jaspers, a professor at the University of Heidelberg, was acquainted with Martin Heidegger, who held a professorship at Marburg before acceding to Husserl's chair at Freiburg in 1928. They held many philosophical discussions, but later became estranged over Heidegger's support of National Socialism. They shared an admiration for Kierkegaard, and in the 1930s, Heidegger lectured extensively on Nietzsche. Nevertheless, the extent to which Heidegger should be considered an existentialist is debatable. In "Being and Time" he presented a method of rooting philosophical explanations in human existence ("Dasein") to be analysed in terms of existential categories ("existentiale"); and this has led many commentators to treat him as an important figure in the existentialist movement.
After the Second World War.
Following the Second World War, existentialism became a well-known and significant philosophical and cultural movement, mainly through the public prominence of two French writers, Jean-Paul Sartre and Albert Camus, who wrote best-selling novels, plays and widely read journalism as well as theoretical texts. These years also saw the growing reputation of Heidegger's book "Being and Time" outside Germany.
Sartre dealt with existentialist themes in his 1938 novel "Nausea" and the short stories in his 1939 collection "The Wall", and had published his treatise on existentialism, "Being and Nothingness", in 1943, but it was in the two years following the liberation of Paris from the German occupying forces that he and his close associates — Camus, Simone de Beauvoir, Maurice Merleau-Ponty, and others — became internationally famous as the leading figures of a movement known as existentialism. In a very short period of time, Camus and Sartre in particular became the leading public intellectuals of post-war France, achieving by the end of 1945 "a fame that reached across all audiences." Camus was an editor of the most popular leftist (former French Resistance) newspaper "Combat"; Sartre launched his journal of leftist thought, "Les Temps Modernes", and two weeks later gave the widely reported lecture on existentialism and secular humanism to a packed meeting of the Club Maintenant. Beauvoir wrote that "not a week passed without the newspapers discussing us"; existentialism became "the first media craze of the postwar era."
By the end of 1947, Camus' earlier fiction and plays had been reprinted, his new play "Caligula" had been performed and his novel "The Plague" published; the first two novels of Sartre's "The Roads to Freedom" trilogy had appeared, as had Beauvoir's novel "The Blood of Others". Works by Camus and Sartre were already appearing in foreign editions. The Paris-based existentialists had become famous.
Sartre had traveled to Germany in 1930 to study the phenomenology of Edmund Husserl and Martin Heidegger, and he included critical comments on their work in his major treatise "Being and Nothingness". Heidegger's thought had also become known in French philosophical circles through its use by Alexandre Kojève in explicating Hegel in a series of lectures given in Paris in the 1930s. The lectures were highly influential; members of the audience included not only Sartre and Merleau-Ponty, but Raymond Queneau, Georges Bataille, Louis Althusser, André Breton, and Jacques Lacan. A selection from Heidegger's "Being and Time" was published in French in 1938, and his essays began to appear in French philosophy journals.
Heidegger read Sartre's work and was initially impressed, commenting: "Here for the first time I encountered an independent thinker who, from the foundations up, has experienced the area out of which I think. Your work shows such an immediate comprehension of my philosophy as I have never before encountered." Later, however, in response to a question posed by his French follower Jean Beaufret, Heidegger distanced himself from Sartre's position and existentialism in general in his "Letter on Humanism". Heidegger's reputation continued to grow in France during the 1950s and 1960s. In the 1960s, Sartre attempted to reconcile existentialism and Marxism in his work "Critique of Dialectical Reason". A major theme throughout his writings was freedom and responsibility.
Camus was a friend of Sartre, until their falling-out, and wrote several works with existential themes including "The Rebel", "Summer in Algiers", "The Myth of Sisyphus", and "The Stranger", the latter being "considered—to what would have been Camus's irritation—the exemplary existentialist novel." Camus, like many others, rejected the existentialist label, and considered his works concerned with facing the absurd. In the titular book, Camus uses the analogy of the Greek myth of Sisyphus to demonstrate the futility of existence. In the myth, Sisyphus is condemned for eternity to roll a rock up a hill, but when he reaches the summit, the rock will roll to the bottom again. Camus believes that this existence is pointless but that Sisyphus ultimately finds meaning and purpose in his task, simply by continually applying himself to it. The first half of the book contains an extended rebuttal of what Camus took to be existentialist philosophy in the works of Kierkegaard, Shestov, Heidegger, and Jaspers.
Simone de Beauvoir, an important existentialist who spent much of her life as Sartre's partner, wrote about feminist and existentialist ethics in her works, including "The Second Sex" and "The Ethics of Ambiguity". Although often overlooked due to her relationship with Sartre, de Beauvoir integrated existentialism with other forms of thinking such as feminism, unheard of at the time, resulting in alienation from fellow writers such as Camus.
Paul Tillich, an important existentialist theologian following Kierkegaard and Karl Barth, applied existentialist concepts to Christian theology, and helped introduce existential theology to the general public. His seminal work "The Courage to Be" follows Kierkegaard's analysis of anxiety and life's absurdity, but puts forward the thesis that modern humans must, via God, achieve selfhood in spite of life's absurdity. Rudolf Bultmann used Kierkegaard's and Heidegger's philosophy of existence to demythologize Christianity by interpreting Christian mythical concepts into existentialist concepts.
Maurice Merleau-Ponty, an existential phenomenologist, was for a time a companion of Sartre. His understanding of Husserl's phenomenology was far greater than that of Merleau-Ponty's fellow existentialists. It has been said that his work "Humanism and Terror" greatly influenced Sartre. However, in later years they were to disagree irreparably, dividing many existentialists such as de Beauvoir, who sided with Sartre.
Colin Wilson, an English writer, published his study "The Outsider" in 1956, initially to critical acclaim. In this book and others (e.g. "Introduction to the New Existentialism"), he attempted to reinvigorate what he perceived as a pessimistic philosophy and bring it to a wider audience. He was not, however, academically trained, and his work was attacked by professional philosophers for lack of rigor and critical standards.
Influence outside philosophy.
Art.
Film and television.
The French director Jean Genet's 1950 fantasy-erotic film "Un chant d'amour" shows two inmates in solitary cells whose only contact is through a hole in their cell wall, who are spied on by the prison warden. Reviewer James Travers calls the film a, "...visual poem evoking homosexual desire and existentialist suffering," which "... conveys the bleakness of an existence in a godless universe with painful believability"; he calls it "... probably the most effective fusion of existentialist philosophy and cinema."
Stanley Kubrick's 1957 anti-war film "Paths of Glory" "illustrates, and even illuminates...existentialism" by examining the "necessary absurdity of the human condition" and the "horror of war". The film tells the story of a fictional World War I French army regiment ordered to attack an impregnable German stronghold; when the attack fails, three soldiers are chosen at random, court-martialed by a "kangaroo court", and executed by firing squad. The film examines existentialist ethics, such as the issue of whether objectivity is possible and the "problem of authenticity".
Orson Welles' 1962 film "- The Trial" based upon Franz Kafka's book of the same name (Der Process) is characteristic of both existentialist and absurdist themes in its depiction of a man (Joseph K.) arrested for a crime for which the charges are neither revealed to him nor to the reader.
Neon Genesis Evangelion, commonly referred to as Evangelion or Eva, is a Japanese science-fiction animation series created by the anime studio Gainax and was both directed and written by Hideaki Anno. Existential themes of individuality, consciousness, freedom, choice, and responsibility are heavily relied upon throughout the entire series, particularly through the philosophies of Jean-Paul Sartre and Søren Kierkegaard. Episode 16's title, is a reference to Kierkegaard's book, "The Sickness Unto Death".
On the lighter side, the British comedy troupe Monty Python have explored existentialist themes throughout their works, from many of the sketches in their original television show, "Monty Python's Flying Circus", to their 1983 film "Monty Python's The Meaning of Life".
Some contemporary films dealing with existentialist issues include "Fight Club", "I ♥ Huckabees", "Waking Life", "The Matrix", "Ordinary People", and "Life in a Day". Likewise, films throughout the 20th century such as "The Seventh Seal", "Ikiru", "Taxi Driver"," Toy Story", "Ghost in the Shell", "Harold and Maude", "High Noon", "Easy Rider", "One Flew Over the Cuckoo's Nest", "A Clockwork Orange", "Groundhog Day", "Apocalypse Now", "Badlands", and "Blade Runner" also have existentialist qualities.
"The Matrix" has been compared with another movie, "Dark City" where the issues of identity and reality are raised. In "Dark City", the inhabitants of the city are situated in a world controlled by demiurges, much like the prisoners in Plato's cave, in which prisoners see a world of shadows reflected onto a cave wall, rather than the world as it actually is.
Musician-Popular Film Artist John Lennon's "God" models existentialist ideals. Lennon says, "God is a concept by which we measure our pain...I just believe in me."
Notable directors known for their existentialist films include Ingmar Bergman, François Truffaut, Jean-Luc Godard, Michelangelo Antonioni, Akira Kurosawa, Terrence Malick, Stanley Kubrick, Andrei Tarkovsky, Hideaki Anno, Wes Anderson, Woody Allen, and Christopher Nolan. Charlie Kaufman's "Synecdoche, New York" focuses on the protagonist's desire to find existential meaning. Similarly, in Kurosawa's "Red Beard", the protagonist's experiences as an intern in a rural health clinic in Japan lead him to an existential crisis whereby he questions his reason for being. This, in turn, leads him to a better understanding of humanity.
Recently released French film, "Mood Indigo" (directed by Michel Gondry) embraced various elements of existentialism.
The film "The Shawshank Redemption", released in 1994, depicts life in a prison in Maine, USA to explore several existentialist concepts.
Literature.
Existential perspectives are also found in modern literature to varying degrees, especially since the 1920s. Louis-Ferdinand Céline's "Journey to the End of the Night" (Voyage au bout de la nuit, 1932) celebrated by both Sartre and Beauvoir, contained many of the themes that would be found in later existential literature, and is in some ways, the proto-existential novel. Jean-Paul Sartre's 1938 novel "Nausea" was "steeped in Existential ideas", and is considered an accessible way of grasping his philosophical stance. Between 1900 and 1960, other authors such as Albert Camus, Franz Kafka, Rainer Maria Rilke, T.S. Eliot, Herman Hesse, Luigi Pirandello, Ralph Ellison, and Jack Kerouac, composed literature or poetry that contained, to varying degrees, elements of existential or proto-existential thought. The philosophy's influence even reached pulp literature shortly after the turn of the 20th century, as seen in the existential disparity witnessed in Man's lack of control of his fate in the works of H.P. Lovecraft. Since the late 1960s, a great deal of cultural activity in literature contains postmodernist as well as existential elements. Books such as "Do Androids Dream of Electric Sheep?" (1968) (now republished as "Blade Runner") by Philip K. Dick, "Slaughterhouse-Five" by Kurt Vonnegut, and "Fight Club" by Chuck Palahniuk all distort the line between reality and appearance while simultaneously espousing existential themes. Ideas from such writers as Fyodor Dostoyevsky, Michel Foucault, Franz Kafka, Friedrich Nietzsche, Søren Kierkegaard, Herbert Marcuse, Gilles Deleuze, Arthur Schopenhauer, and Eduard von Hartmann permeate the works of modern novelists such as Chuck Palahniuk, Crispin Glover, Andrew Hussie, David Foster Wallace, and Charles Bukowski, and one often finds in their works a delicate balance between distastefulness and beauty.
Theatre.
Jean-Paul Sartre wrote "No Exit" in 1944, an existentialist play originally published in French as "Huis Clos" (meaning "In Camera" or "behind closed doors"), which is the source of the popular quote, "Hell is other people." (In French, "L'enfer, c'est les autres"). The play begins with a Valet leading a man into a room that the audience soon realizes is in hell. Eventually he is joined by two women. After their entry, the Valet leaves and the door is shut and locked. All three expect to be tortured, but no torturer arrives. Instead, they realize they are there to torture each other, which they do effectively by probing each other's sins, desires, and unpleasant memories.
Existentialist themes are displayed in the Theatre of the Absurd, notably in Samuel Beckett's "Waiting for Godot", in which two men divert themselves while they wait expectantly for someone (or something) named Godot who never arrives. They claim Godot is an acquaintance, but in fact, hardly know him, admitting they would not recognize him if they saw him. Samuel Beckett, once asked who or what Godot is, replied, "If I knew, I would have said so in the play." To occupy themselves, the men eat, sleep, talk, argue, sing, play games, exercise, swap hats, and contemplate suicide—anything "to hold the terrible silence at bay". The play "exploits several archetypal forms and situations, all of which lend themselves to both comedy and pathos." The play also illustrates an attitude toward human experience on earth: the poignancy, oppression, camaraderie, hope, corruption, and bewilderment of human experience that can be reconciled only in the mind and art of the absurdist. The play examines questions such as death, the meaning of human existence and the place of God in human existence.
Tom Stoppard's "Rosencrantz & Guildenstern Are Dead" is an absurdist tragicomedy first staged at the Edinburgh Festival Fringe in 1966. The play expands upon the exploits of two minor characters from Shakespeare's "Hamlet". Comparisons have also been drawn to Samuel Beckett's "Waiting For Godot", for the presence of two central characters who appear almost as two halves of a single character. Many plot features are similar as well: the characters pass time by playing Questions, impersonating other characters, and interrupting each other or remaining silent for long periods of time. The two characters are portrayed as two clowns or fools in a world beyond their understanding. They stumble through philosophical arguments while not realizing the implications, and muse on the irrationality and randomness of the world.
Jean Anouilh's "Antigone" also presents arguments founded on existentialist ideas. It is a tragedy inspired by Greek mythology and the play of the same name (Antigone, by Sophocles) from the 5th century BC. In English, it is often distinguished from its antecedent by being pronounced in its original French form, approximately "Ante-GŌN." The play was first performed in Paris on 6 February 1944, during the Nazi occupation of France. Produced under Nazi censorship, the play is purposefully ambiguous with regards to the rejection of authority (represented by Antigone) and the acceptance of it (represented by Creon). The parallels to the French Resistance and the Nazi occupation have been drawn. Antigone rejects life as desperately meaningless but without affirmatively choosing a noble death. The crux of the play is the lengthy dialogue concerning the nature of power, fate, and choice, during which Antigone says that she is, "... disgusted with h...promise of a humdrum happiness." She states that she would rather die than live a mediocre existence.
Critic Martin Esslin in his book "Theatre of the Absurd" pointed out how many contemporary playwrights such as Samuel Beckett, Eugène Ionesco, Jean Genet, and Arthur Adamov wove into their plays the existentialist belief that we are absurd beings loose in a universe empty of real meaning. Esslin noted that many of these playwrights demonstrated the philosophy better than did the plays by Sartre and Camus. Though most of such playwrights, subsequently labeled "Absurdist" (based on Esslin's book), denied affiliations with existentialism and were often staunchly anti-philosophical (for example Ionesco often claimed he identified more with 'Pataphysics or with Surrealism than with existentialism), the playwrights are often linked to existentialism based on Esslin's observation.
Psychoanalysis and psychotherapy.
A major offshoot of existentialism as a philosophy is existentialist psychology and psychoanalysis, which first crystallized in the work of Otto Rank, Freud's closest associate for 20 years. Without awareness of the writings of Rank, Ludwig Binswanger was influenced by Freud, Edmund Husserl, Heidegger, and Sartre. A later figure was Viktor Frankl, who briefly met Freud and studied with Jung as a young man. His logotherapy can be regarded as a form of existentialist therapy. The existentialists would also influence social psychology, antipositivist micro-sociology, symbolic interactionism, and post-structuralism, with the work of thinkers such as Georg Simmel and Michel Foucault. Foucault was a great reader of Kierkegaard even though he almost never refers this author, who nonetheless had for him an importance as secret as it was decisive.
An early contributor to existentialist psychology in the United States was Rollo May, who was strongly influenced by Kierkegaard and Otto Rank. One of the most prolific writers on techniques and theory of existentialist psychology in the USA is Irvin D. Yalom. Yalom states that
Aside from their reaction against Freud's mechanistic, deterministic model of the mind and their assumption of a phenomenological approach in therapy, the existentialist analysts have little in common and have never been regarded as a cohesive ideological school. These thinkers - who include Ludwig Binswanger, Medard Boss, Eugène Minkowski, V.E. Gebsattel, Roland Kuhn, G. Caruso, F.T. Buytendijk, G. Bally and Victor Frankl - were almost entirely unknown to the American psychotherapeutic community until Rollo May's highly influential 1985 book "Existence" - and especially his introductory essay - introduced their work into this country.
A more recent contributor to the development of a European version of existentialist psychotherapy is the British-based Emmy van Deurzen.
Anxiety's importance in existentialism makes it a popular topic in psychotherapy. Therapists often offer existentialist philosophy as an explanation for anxiety. The assertion is that anxiety is manifested of an individual's complete freedom to decide, and complete responsibility for the outcome of such decisions. Psychotherapists using an existentialist approach believe that a patient can harness his anxiety and use it constructively. Instead of suppressing anxiety, patients are advised to use it as grounds for change. By embracing anxiety as inevitable, a person can use it to achieve his full potential in life. Humanistic psychology also had major impetus from existentialist psychology and shares many of the fundamental tenets. Terror management theory, based on the writings of Ernest Becker and Otto Rank, is a developing area of study within the academic study of psychology. It looks at what researchers claim are implicit emotional reactions of people confronted with the knowledge that they will eventually die.
Also, Gerd B. Achenbach has refreshed the socratic tradition with his own blend of philosophical counseling. So did Michel Weber with his Chromatiques Center in Belgium.
Criticisms.
General criticisms.
Walter Kaufmann criticized 'the profoundly unsound methods and the dangerous contempt for reason that have been so prominent in existentialism.'
Logical positivist philosophers, such as Rudolf Carnap and A. J. Ayer, assert that existentialists are often confused about the verb "to be" in their analyses of "being". Specifically, they argue that the verb is transitive and pre-fixed to a predicate (e.g., an apple "is red") (without a predicate, the word is meaningless), and that existentialists frequently misuse the term in this manner.
Colin Wilson has stated in his book "The Angry Years" that existentialism has created many of its own difficulties: "we can see how this question of freedom of the will has been vitiated by post-romantic philosophy, with its inbuilt tendency to laziness and boredom, we can also see how it came about that existentialism found itself in a hole of its own digging, and how the philosophical developments since then have amounted to walking in circles round that hole".
Sartre's philosophy.
Many critics argue Sartre's philosophy is contradictory. Specifically, they argue that Sartre makes metaphysical arguments despite his claiming that his philosophical views ignore metaphysics. Herbert Marcuse criticized "Being and Nothingness" (1943) by Jean-Paul Sartre for projecting anxiety and meaninglessness onto the nature of existence itself: "Insofar as Existentialism is a philosophical doctrine, it remains an idealistic doctrine: it hypostatizes specific historical conditions of human existence into ontological and metaphysical characteristics. Existentialism thus becomes part of the very ideology which it attacks, and its radicalism is illusory".
In "Letter on Humanism", Heidegger criticized Sartre's existentialism:
Existentialism says existence precedes essence. In this statement he is taking "existentia" and "essentia" according to their metaphysical meaning, which, from Plato's time on, has said that "essentia" precedes "existentia". Sartre reverses this statement. But the reversal of a metaphysical statement remains a metaphysical statement. With it, he stays with metaphysics, in oblivion of the truth of Being.

</doc>
<doc id="9596" url="https://en.wikipedia.org/wiki?curid=9596" title="Ellipsis">
Ellipsis

Ellipsis (plural ellipses; from the , "élleipsis", "omission" or "falling short") is a series of dots (typically three, such as "…") that usually indicates an intentional omission of a word, sentence, or whole section from a text without altering its original meaning. 
Background.
Depending on their context and placement in a sentence, ellipses can indicate an unfinished thought, a leading statement, a slight pause, a mysterious or echoing voice, or a nervous or awkward silence. Aposiopesis is the use of an ellipsis to trail off into silence—for example: "But I thought he was When placed at the beginning or end of a sentence, the ellipsis can also inspire a feeling of melancholy or longing.
The most common form of an ellipsis is a row of three periods or full stops (. . .) or a precomposed triple-dot glyph (…). The usage of the em dash (—) can overlap the usage of the ellipsis, especially in dialogue. Style guides often have their own rules governing the use of ellipses. For example, "the Chicago Manual of Style" recommends that an ellipsis be formed by typing three periods, each with a space on both sides.
Some believe that the use of four or more dots or simply two dots, which is often referred to as being more "informal" instead of the well established three dots, is acceptable as an ellipsis. Others believe it is used as an indicator of something that was missed in the message that should not have been; an unnecessary re-iteration due to lack of intelligence or understanding.
The triple-dot punctuation mark is also called a suspension point, points of ellipsis, periods of ellipsis, or colloquially, "dot-dot-dot".
In writing.
In her book on the ellipsis, "Ellipsis in English Literature: Signs of Omission" (Cambridge University Press, 2015), Anne Toner suggests that the first use of the punctuation in the English language dates to a 1588 translation of Terence's Andria, by Maurice Kyffin. In this case, however, the ellipsis consists not of dots but of short dashes.
In the 19th and early 20th centuries, an ellipsis was often used when a writer intentionally omitted a specific proper noun, such as a location: "Jan was born on . . . Street in Warsaw."
As commonly used, this juxtaposition of characters is referred to as "dots of ellipsis" in the English language.
Occasionally, it would be used in pulp fiction and other works of early 20th-century fiction to denote expletives that would otherwise have been censored.
An ellipsis may also imply an unstated alternative indicated by context. For example, when Sue says "I never drink wine . . . ", the implication is that she does drink something elsesuch as vodka.
In reported speech, the ellipsis can be used to represent an intentional silence.
In poetry, an ellipsis is used as a thought-pause or line break at the caesura or this is used to highlight sarcasm or make the reader think about the last points in the poem
In news reporting, often associated with brackets, it is used to indicate that a quotation has been condensed for space, brevity or relevance.
Herb Caen, Pulitzer-prize-winning columnist for the San Francisco Chronicle, became famous for his "Three-dot journalism".
In different languages.
In English.
"The Chicago Manual of Style" suggests the use of an ellipsis for any omitted word, phrase, line, or paragraph from within but not at the end of a quoted passage. There are two commonly used methods of using ellipses: one uses three dots for any omission, while the second one makes a distinction between omissions within a sentence (using three dots: . . .) and omissions between sentences (using a period and a space followed by three dots: . ...). An ellipsis at the end of a sentence with a sentence following should be preceded by a period (for a total of four dots).
The Modern Language Association (MLA), however, used to indicate that an ellipsis must include spaces before and after each dot in all uses. If an ellipsis is meant to represent an omission, square brackets must surround the ellipsis to make it clear that there was no pause in the original quote: . . .. Currently, the MLA has removed the requirement of brackets in its style handbooks. However, some maintain that the use of brackets is still correct because it clears confusion.
The MLA now indicates that a three-dot, spaced ellipsis ( … ) should be used for removing material from within one sentence within a quote. When crossing sentences (when the omitted text contains a period, so that omitting the end of a sentence counts), a four-dot, spaced (except for before the first dot) ellipsis (. . . . ) should be used. When ellipsis points are used in the original text, ellipsis points that are not in the original text should be distinguished by enclosing them in square brackets (e.g. "text text").
According to the Associated Press, the ellipsis should be used to condense quotations. It is less commonly used to indicate a pause in speech or an unfinished thought or to separate items in material such as show business gossip. The stylebook indicates that if the shortened sentence before the mark can stand as a sentence, it should do so, with an ellipsis placed after the period or other ending punctuation. When material is omitted at the end of a paragraph and also immediately following it, an ellipsis goes both at the end of that paragraph and in front of the beginning of the next, according to this style.
According to Robert Bringhurst's "Elements of Typographic Style", the details of typesetting ellipses depend on the character and size of the font being set and the typographer's preference. Bringhurst writes that a full space between each dot is "another Victorian eccentricity. In most contexts, the Chicago ellipsis is much too wide"—he recommends using flush dots, or "thin"-spaced dots (up to one-fifth of an em), or the prefabricated ellipsis character (Unicode U+2026, Latin entity codice_1). Bringhurst suggests that normally an ellipsis should be spaced fore-and-aft to separate it from the text, but when it combines with other punctuation, the leading space disappears and the other punctuation follows. This is the usual practice in typesetting. He provides the following examples:
In legal writing in the United States, Rule 5.3 in the "Bluebook" citation guide governs the use of ellipses and requires a space before the first dot and between the two subsequent dots. If an ellipsis ends the sentence, then there are three dots, each separated by a space, followed by the final punctuation. In some legal writing, an ellipsis is written as three asterisks (*** or * * *) to make it obvious that text has been omitted.
(...) is also used for awkward silence.
In Polish.
When applied in Polish language syntax, the ellipsis is called , which means "multidot". The word "wielokropek" distinguishes the ellipsis of Polish syntax from that of mathematical notation, in which it is known as an .
When an ellipsis replaces a fragment omitted from a quotation, the ellipsis is enclosed in parentheses or square brackets. An unbracketed ellipsis indicates an interruption or pause in speech.
The syntactical rules for ellipses are standardized by the 1983 Polska Norma document PN-83/P-55366, ("Rules for setting texts in the Polish Language").
In Russian.
The combination "ellipsis+period" is replaced by the ellipsis. The combinations "ellipsis+exclamation mark" and "ellipsis+question mark" are written in this way: !.. ?..
In Japanese.
The most common character corresponding to an ellipsis is called "3"-ten rīdā (""3"-dot leaders", ). 2-ten rīdā exists as a character, but it is used less commonly. In writing, the ellipsis consists usually of six dots (two "3"-ten rīdā characters, ). Three dots (one "3"-ten rīdā character) may be used where space is limited, such as in a header. However, variations in the number of dots exist. In horizontally written text the dots are commonly vertically centered within the text height (between the baseline and the ascent line), as in the standard Japanese Windows fonts; in vertically written text the dots are always centered horizontally. 
In manga, the ellipsis by itself represents speechlessness, or a "pregnant pause". Given the context, this could be anything from an admission of guilt to an expression of being dumbfounded at another person's words or actions. As a device, the "ten-ten-ten" is intended to focus the reader on a character while allowing the character to not speak any dialogue. This conveys to the reader a focus of the narrative "camera" on the silent subject, implying an expectation of some motion or action. It is not unheard of to see inanimate objects "speaking" the ellipsis.
In Chinese.
In Chinese, the ellipsis is six dots (in two groups of three dots, occupying the same horizontal or vertical space as two characters) (i.e. ……). The dots are always centered within the baseline and the ascender when horizontal (on the baseline has become acceptable) and centered horizontally when vertical.
Sometimes people will use three dots instead of six to save typing on the Internet.
In Spanish.
In Spanish, ellipsis is commonly used as a substitute of "et cetera" at the end of unfinished lists. So it means "and so forth" or "and other things".
Other use is the suspension of a part of a text, or a paragraph, or a phrase or a part of a word because it is obvious, or unnecessary, or implied. For instance, sometimes the ellipsis is used to avoid the complete use of expletives.
When the ellipse is placed alone into a parenthesis (...) or—less often—between brackets , which is what happens usually within a text transcription, it means the original text had more contents on the same position but are not useful to our target in the transcription. When the suppressed text is at the beginning or at the end of a text, the ellipse does not need to be placed in a parenthesis.
In French.
In French, the ellipsis is commonly used at the end of lists to represent "et cetera".
However, any omitted word, phrase or line at the end of a quoted passage would be indicated like this: . (space before and after the square brackets but not inside).
In German.
In German, the ellipsis in general is surrounded by spaces, if it stands for one or more omitted words. On the other side there is no space between a letter or (part of) a word and an ellipsis, if it stands for one or more omitted letters, that should stick to the written letter or letters.
Example for both cases, using German style: "The first el...is stands for omitted letters, the second ... for an omitted word."
If the ellipsis is at the end of a sentence (or between two sentences), the final full stop is omitted.
Example: "I think that ..."
In mathematical notation.
An ellipsis is also often used in mathematics to mean "and so forth". In a list, between commas, or following a comma, a normal ellipsis is used, as in:
To indicate the omission of values in a repeated operation, an ellipsis raised to the center of the line is used between two operation symbols or following the last operation symbol, as in:
(though sometimes, for example, in Russian mathematical texts, normal, non-raised, ellipses are used even in repeated summations).
The latter formula means the sum of all natural numbers from 1 to 100. However, it is not a formally defined mathematical symbol. Repeated summations or products may similarly be denoted using capital sigma and capital pi notation, respectively:
Normally dots should be used only where the pattern to be followed is clear, the exception being to show the indefinite continuation of an irrational number such as:
Sometimes, it is useful to display a formula compactly, for example:
Another example is the set of zeros of the cosine function.
There are many related uses of the ellipsis in set notation.
The diagonal and vertical forms of the ellipsis are particularly useful for showing missing terms in matrices, such as the size-"n" identity matrix
The use of ellipses in mathematical proofs is often discouraged because of the potential for ambiguity. For this reason, and because the ellipsis supports no systematic rules for symbolic calculation, in recent years some authors have recommended avoiding its use in mathematics altogether.
Computer interfaces.
Ellipses are often used in an operating system's taskbars or web browser tabs to indicate that a user interface string is longer than what can fit in the screen. Hovering the cursor over the tab often displays a tooltip of the full title. When many programs are open, or during a "tab explosion" in web browsing, the tabs may be reduced in size so much that no characters from the actual titles show, and ellipses take up all the space besides the program icon or favicon.
In many user interface guidelines, a "…" after the name of a command implies that the user will need to provide further information, for example in a subsequent dialog box, before the action can be completed. A typical example is the "Save As…" command, which after being clicked will usually require the user to enter a filename, as opposed to "Save" where the file will usually be saved under its existing name.
An ellipsis character after a status message signifies that an operation is in progress and may take some time, as in "Downloading updates…".
Programming languages.
The ellipsis is used as an operator in some programming languages. The precise meaning varies by language, but it generally involves something dealing with multiple items. One of its most common uses is in defining variadic functions which can take an unknown number of arguments in the C, C++ and Java languages. "See Ellipsis (programming operator)".
On the Internet and in text messaging.
The ellipsis is a non-verbal cue that is often used in computer-mediated interactions, in particular in synchronous genres, such as chat. The reason behind its popularity is the fact that it allows people to indicate in writing several functions:
Although an ellipsis is technically complete with three periods (...), its rise in popularity as a "trailing-off" or "silence" indicator, particularly in mid-20th-century comic strip and comic book prose writing, has led to expanded uses online. Today, extended ellipsis anywhere from two to dozens of periods have become common constructions in Internet chat rooms and text messages. The extent of repetition in itself might serve as an additional contextualization or paralinguistic cue, to "extend the lexical meaning of the words, add character to the sentences, and allow fine-tuning and personalisation of the message"
Computer representations.
In computing, several ellipsis characters have been codified, depending on the system used.
In the Unicode standard, there are the following characters:
In Windows, it can be inserted with .
In OS X, it can be inserted with (on an English language keyboard).
In some Linux distributions, it can be inserted with , or alternatively sequence can be used.
In Chinese and sometimes in Japanese, ellipsis characters are made by entering two consecutive "horizontal ellipsis" (U+2026). In vertical texts, the application should rotate the symbol accordingly.
Unicode recognizes a series of three period characters (U+002E) as compatibility equivalent (though not canonical) to the horizontal ellipsis character.
In HTML, the horizontal ellipsis character may be represented by the entity reference codice_1 (since HTML 4.0), and the vertical ellipsis character by the entity reference codice_3 (since HTML 5.0). Alternatively, in HTML, XML, and SGML, a numeric character reference such as codice_4 or codice_5 can be used.
In the TeX typesetting system, the following types of ellipsis are available:
In LaTeX with the codice_6 package from AMS-Latex more specific ellipses are provided for math mode.
The horizontal ellipsis character also appears in the following older character maps:
Note that ISO/IEC 8859 encoding series provides no code point for ellipsis.
As with all characters, especially those outside the ASCII range, the author, sender and receiver of an encoded ellipsis must be in agreement upon what bytes are being used to represent the character. Naive text processing software may improperly assume that a particular encoding is being used, resulting in mojibake.
The Chicago Style Q&A recommends to avoid the use of … (U+2026) character in manuscripts and to place three periods plus two nonbreaking spaces (. . .) instead, so that an editor, publisher, or designer can replace them later.
In Abstract Syntax Notation One (ASN.1), the ellipsis is used as an extension marker to indicate the possibility of type extensions in future revisions of a protocol specification. In a type constraint expression like A ::= INTEGER (0..127, ..., 256..511) an ellipsis is used to separate the extension root from extension additions. The definition of type A in version 1 system of the form A ::= INTEGER (0..127, ...) and the definition of type A in version 2 system of the form A ::= INTEGER (0..127, ..., 256..511) constitute an extension series of the same type A in different versions of the same specification. The ellipsis can also be used in compound type definitions to separate the set of fields belonging to the extension root from the set of fields constituting extension additions. Here is an example: B ::= SEQUENCE { a INTEGER, b INTEGER, ..., c INTEGER } 

</doc>
<doc id="9597" url="https://en.wikipedia.org/wiki?curid=9597" title="Enola Gay">
Enola Gay

The Enola Gay () is a Boeing B-29 Superfortress bomber, named for Enola Gay Tibbets, the mother of the pilot, Colonel Paul Tibbets, who selected the aircraft while it was still on the assembly line. On 6 August 1945, during the final stages of World War II, it became the first aircraft to drop an atomic bomb. The bomb, code-named "Little Boy", was targeted at the city of Hiroshima, Japan, and caused unprecedented destruction. "Enola Gay" participated in the second atomic attack as the weather reconnaissance aircraft for the primary target of Kokura. Clouds and drifting smoke resulted in Nagasaki being bombed instead.
After the war, the "Enola Gay" returned to the United States, where it was operated from Roswell Army Air Field, New Mexico. It was flown to Kwajalein for the Operation Crossroads nuclear tests in the Pacific, but was not chosen to make the test drop at Bikini Atoll. Later that year it was transferred to the Smithsonian Institution, and spent many years parked at air bases exposed to the weather and souvenir hunters, before being disassembled and transported to the Smithsonian's storage facility at Suitland, Maryland, in 1961.
In the 1980s, veterans groups engaged in a call for the Smithsonian to put the aircraft on display, leading to an acrimonious debate about exhibiting the aircraft without a proper historical context. The cockpit and nose section of the aircraft were exhibited at the National Air and Space Museum (NASM) in downtown Washington, D.C., for the bombing's 50th anniversary in 1995, amid a storm of controversy. Since 2003, the entire restored B-29 has been on display at NASM's Steven F. Udvar-Hazy Center. The last survivor of its crew, Theodore Van Kirk, died on July 28, 2014, at the age of 93.
World War II.
Early history.
The "Enola Gay" (Model number B-29-45-MO, Serial number 44-86292, Victor number 82) was built by the Glenn L. Martin Company (now Lockheed Martin) at its Bellevue, Nebraska plant, located at what is now known as Offutt Air Force Base. The bomber was one of 15 B-29s with the "Silverplate" modifications necessary to deliver atomic weapons. These modifications included an extensively modified bomb bay with pneumatic doors and British bomb attachment and release systems, reversible pitch propellers that gave more braking power on landing, improved engines with fuel injection and better cooling, and the removal of protective armor and gun turrets.
"Enola Gay" was personally selected by Colonel Paul W. Tibbets, Jr., the commander of the 509th Composite Group, on 9 May 1945, while still on the assembly line. The aircraft was accepted by the United States Army Air Forces (USAAF) on 18 May 1945 and assigned to the 393d Bombardment Squadron, Heavy, 509th Composite Group. Crew B-9, commanded by Captain Robert A. Lewis, took delivery of the bomber and flew it from Omaha to the 509th's base at Wendover Army Air Field, Utah, on 14 June 1945.
Thirteen days later, the aircraft left Wendover for Guam, where it received a bomb-bay modification, and flew to North Field, Tinian, on 6 July. It was initially given the Victor (squadron-assigned identification) number 12, but on 1 August, was given the circle R tail markings of the 6th Bombardment Group as a security measure and had its Victor number changed to 82 to avoid misidentification with actual 6th Bombardment Group aircraft. During July, the bomber made eight practice or training flights, and flew two missions, on 24 and 26 July, to drop pumpkin bombs on industrial targets at Kobe and Nagoya. "Enola Gay" was used on 31 July on a rehearsal flight for the actual mission.
The partially assembled Little Boy gun-type nuclear weapon L-11 was contained inside a x x wooden crate weighing that was secured to the deck of the . Unlike the six Uranium-235 target discs, which were later flown to Tinian on three separate aircraft arriving 28 and 29 July, the assembled projectile with the nine Uranium-235 rings installed was shipped in a single lead-lined steel container weighing that was securely locked to brackets welded to the deck of Captain Charles B. McVay III's quarters. Both the L-11 and projectile were dropped off at Tinian on 26 July 1945.
Hiroshima mission.
On 5 August 1945, during preparation for the first atomic mission, Tibbets assumed command of the aircraft and named it after his mother, Enola Gay Tibbets, who had herself been named for the heroine of a novel. When it came to selecting a name for the plane, Tibbets later recalled that: 
The name was painted on the aircraft on 5 August by Allan L. Karl, an enlisted man in the 509th. Regularly assigned aircraft commander Robert Lewis was unhappy to be displaced by Tibbets for this important mission, and became furious when he arrived at the aircraft on the morning of 6 August to see it painted with the now-famous nose art.
Hiroshima was the primary target of the first nuclear bombing mission on 6 August, with Kokura and Nagasaki as alternative targets. "Enola Gay", piloted by Tibbets, took off from North Field, in the Mariana Islands, about six hours' flight time from Japan, accompanied by two other B-29s, "The Great Artiste", carrying instrumentation, and a then-nameless aircraft later called "Necessary Evil", commanded by Captain George Marquardt, to take photographs. The director of the Manhattan Project, Major General Leslie R. Groves, Jr., wanted the event recorded for posterity, so the takeoff was illuminated by floodlights. When he wanted to taxi, Tibbets leaned out the window to direct the bystanders out of the way. On request, he gave a friendly wave for the cameras.
After leaving Tinian, the aircraft made their way separately to Iwo Jima, where they rendezvoused at and set course for Japan. The aircraft arrived over the target in clear visibility at . Captain William S. "Deak" Parsons of Project Alberta, who was in command of the mission, armed the bomb during the flight to minimize the risks during takeoff. His assistant, Second Lieutenant Morris R. Jeppson, removed the safety devices 30 minutes before reaching the target area.
The release at 08:15 (Hiroshima time) went as planned, and the Little Boy took 43 seconds to fall from the aircraft flying at to the predetermined detonation height about above the city. "Enola Gay" traveled before it felt the shock waves from the blast. Although buffeted by the shock, neither "Enola Gay" nor "The Great Artiste" was damaged.
The detonation created a blast equivalent to . The U-235 weapon was considered very inefficient, with only 1.7% of its fissile material fissioning. The radius of total destruction was about one mile (1.6 km), with resulting fires across . Americans estimated that of the city were destroyed. Japanese officials determined that 69% of Hiroshima's buildings were destroyed and another 6–7% damaged. Some 70,000–80,000 people, or some 30% of the city's population, were killed by the blast and resultant firestorm, and another 70,000 injured. Out of those killed, 20,000 were soldiers.
"Enola Gay" returned safely to its base on Tinian to great fanfare, touching down at 2:58 pm, after 12 hours 13 minutes. "The Great Artiste" and "Necessary Evil" followed at short intervals. Several hundred people, including journalists and photographers, had gathered to watch the planes return. Tibbets was the first to disembark, and was presented with the Distinguished Service Cross on the spot.
Nagasaki mission.
The Hiroshima mission was followed by another atomic strike. Originally scheduled for 11 August, it was brought forward by two days to 9 August owing to bad weather. This time, a Fat Man nuclear weapon was carried by B-29 "Bockscar", piloted by Major Charles W. Sweeney. "Enola Gay", flown by Captain George Marquardt's Crew B-10, was the weather reconnaissance aircraft for Kokura, the primary target. "Enola Gay" reported clear skies over Kokura, but by the time "Bockscar" arrived, the city was obscured by smoke from fires from the conventional bombing of Yawata by 224 B-29s the day before. After three unsuccessful passes, "Bockscar" diverted to its secondary target, Nagasaki, where it dropped its bomb. In contrast to the Hiroshima mission, the Nagasaki mission has been described as tactically botched, although the mission did meet its objectives. The crew encountered a number of problems in execution, and had very little fuel by the time they landed at the emergency backup landing site Yontan Airfield on Okinawa.
Crews.
Hiroshima mission.
"Enola Gay"'s crew on 6 August 1945, consisted of 12 men. The crew was:
Of mission commander Parsons, it was said: "There is no one more responsible for getting this bomb out of the laboratory and into some form useful for combat operations than Captain Parsons, by his plain genius in the ordnance business."
Nagasaki mission.
For the Nagasaki mission, "Enola Gay" was flown by Crew B-10, normally assigned to "Up An' Atom":
Subsequent history.
On 6 November 1945, Lewis flew the "Enola Gay" back to the United States, arriving at the 509th's new base at Roswell Army Air Field, New Mexico, on 8 November. On 29 April 1946, "Enola Gay" left Roswell as part of Operation Crossroads nuclear tests in the Pacific. It flew to Kwajalein on 1 May. It was not chosen to make the test drop at Bikini Atoll and left Kwajalein on 1 July, the date of the test, reaching Fairfield-Suisun Army Air Field, California, the next day.
The decision was made to preserve the "Enola Gay", and on 24 July 1946, the aircraft was flown to Davis-Monthan Air Force Base, Arizona, in preparation for storage. On 30 August 1946, the title to the aircraft was transferred to the Smithsonian Institution and the "Enola Gay" was removed from the USAAF inventory. From 1946 to 1961, the "Enola Gay" was put into temporary storage at a number of locations. It was at Davis-Monthan from 1 September 1946 until 3 July 1949, when it was flown to Orchard Place Air Field, Park Ridge, Illinois, by Tibbets for acceptance by the Smithsonian. It was moved to Pyote Air Force Base, Texas, on 12 January 1952, and then to Andrews Air Force Base, Maryland, on 2 December 1953, because the Smithsonian had no storage space for the aircraft.
It was hoped that the Air Force would guard the plane but, lacking hangar space, it was left outdoors on a remote part of the air base, exposed to the elements. Souvenir hunters broke in and removed parts. Insects and birds then gained access to the aircraft. Paul E. Garber, the first head of the National Air Museum of the Smithsonian Institution, became concerned about the "Enola Gay"s condition, and on 10 August 1960, Smithsonian staff began dismantling the aircraft. The components were transported to the Smithsonian storage facility at Suitland, Maryland, on 21 July 1961.
"Enola Gay" remained at Suitland for many years. By the early 1980s, two veterans of the 509th, Don Rehl and his former navigator in the 509th, Frank B. Stewart, began lobbying for the aircraft to be restored and put on display. They enlisted Tibbets and Senator Barry Goldwater in their campaign. In 1983, Walter Boyne, a former B-52 pilot with the Strategic Air Command, became director of the National Air and Space Museum, and he made the "Enola Gay"s restoration a priority. Looking at the aircraft, Tibbets recalled, was a "sad meeting. fond memories, and I don't mean the dropping of the bomb, were the numerous occasions I flew the airplane... I pushed it very, very hard and it never failed me... It was probably the most beautiful piece of machinery that any pilot ever flew."
Restoration of the bomber began on 5 December 1984, at the Paul E. Garber Preservation, Restoration, and Storage Facility in Suitland-Silver Hill, Maryland. The propellers that were used on the bombing mission were later shipped to Texas A&M University. One of these propellers was trimmed to for use in the university's Oran W. Nicks Low Speed Wind Tunnel. The lightweight aluminium variable-pitch propeller is powered by a 1,250 kVA electric motor providing a wind speed up to . Two engines were rebuilt at Garber and two at San Diego Air & Space Museum. The work was slow and meticulous. Every component was carefully cleaned. Some parts and instruments had been removed and could not be located. Replacements were found or fabricated, and marked so that future curators could distinguish them from the original components.
Restoration.
Exhibition controversy.
"Enola Gay" became the center of a controversy at the Smithsonian Institution when the museum planned to put its fuselage on public display in 1995 as part of an exhibit commemorating the 50th anniversary of the atomic bombing of Hiroshima. The exhibit, "The Crossroads: The End of World War II, the Atomic Bomb and the Cold War", was drafted by the Smithsonian's National Air and Space Museum staff, and arranged around the restored "Enola Gay".
Critics of the planned exhibit, especially those of the American Legion and the Air Force Association, charged that the exhibit focused too much attention on the Japanese casualties inflicted by the nuclear bomb, rather than on the motivations for the bombing or the discussion of the bomb's role in ending the conflict with Japan. The exhibit brought to national attention many long-standing academic and political issues related to retrospective views of the bombings. As a result, after various failed attempts to revise the exhibit in order to meet the satisfaction of competing interest groups, the exhibit was canceled on 30 January 1995. Martin O. Harwit, Director of the National Air and Space Museum, was compelled to resign over the controversy.
The forward fuselage did go on display on 28 June 1995. On 2 July 1995, three people were arrested for throwing ash and human blood on the aircraft's fuselage, following an earlier incident in which a protester had thrown red paint over the gallery's carpeting. The exhibition closed on 18 May 1998, and the fuselage was returned to the Garber Facility for final restoration.
Complete restoration and display.
Restoration work began in 1984, and would eventually require 300,000 staff hours. While the fuselage was on display, from 1995 to 1998, work continued on the remaining unrestored components. The aircraft was shipped in pieces to the National Air and Space Museum's Steven F. Udvar-Hazy Center in Chantilly, Virginia from March–June 2003, with the fuselage and wings reunited for the first time since 1960 on 10 April 2003 and assembly completed on 8 August 2003. The aircraft is currently at Washington Dulles International Airport in the Steven F. Udvar-Hazy Center, since the museum annex opened on 15 December 2003.

</doc>
<doc id="9598" url="https://en.wikipedia.org/wiki?curid=9598" title="Electronvolt">
Electronvolt

In physics, the electronvolt (symbol eV; also written electron volt) is a unit of energy equal to approximately 160 zeptojoules (symbol zJ) or joules (symbol J). By definition, it is the amount of energy gained (or lost) by the charge of a single electron moving across an electric potential difference of one volt. Thus it is 1 volt (1 joule per coulomb, ) multiplied by the elementary charge ("e", or ). Therefore, one electron volt is equal to Historically, the electron volt was devised as a standard unit of measure through its usefulness in electrostatic particle accelerator sciences because a particle with charge "q" has an energy after passing through the potential "V"; if "q" is quoted in integer units of the elementary charge and the terminal bias in volts, one gets an energy in eV.
The electron volt is not an SI unit, and its definition is empirical (unlike the litre, the light year and other such non-SI units), thus its value in SI units must be obtained experimentally. Like the elementary charge on which it is based, it is not an independent quantity but is equal to . It is a common unit of energy within physics, widely used in solid state, atomic, nuclear, and particle physics. It is commonly used with the metric prefixes milli-, kilo-, mega-, giga-, tera-, peta- or exa- (meV, keV, MeV, GeV, TeV, PeV and EeV respectively). Thus meV stands for milli-electron volt.
In some older documents, and in the name Bevatron, the symbol BeV is used, which stands for billion electron volts; it is equivalent to the GeV.
Mass.
By mass–energy equivalence, the electronvolt is also a unit of mass. It is common in particle physics, where units of mass and energy are often interchanged, to express mass in units of eV/"c", where "c" is the speed of light in vacuum (from ). It is common to simply express mass in terms of "eV" as a unit of mass, effectively using a system of natural units with "c" set to 1. The mass equivalent of is
For example, an electron and a positron, each with a mass of , can annihilate to yield of energy. The proton has a mass of . In general, the masses of all hadrons are of the order of , which makes the GeV (gigaelectronvolt) a convenient unit of mass for particle physics:
The atomic mass unit, 1 gram divided by Avogadro's number, is almost the mass of a hydrogen atom, which is mostly the mass of the proton. To convert to megaelectronvolts, use the formula:
Momentum.
In high-energy physics, the electron volt is often used as a unit of momentum. A potential difference of 1 volt causes an electron to gain an amount of energy (i.e., ). This gives rise to usage of eV (and keV, MeV, GeV or TeV) as units of momentum, for the energy supplied results in acceleration of the particle.
The dimensions of momentum units are . The dimensions of energy units are . Then, dividing the units of energy (such as eV) by a fundamental constant that has units of velocity (), facilitates the required conversion of using energy units to describe momentum. In the field of high-energy particle physics, the fundamental velocity unit is the speed of light in vacuum "c". Thus, dividing energy in eV by the speed of light, one can describe the momentum of an electron in units of eV/"c".
The fundamental velocity constant "c" is often "dropped" from the units of momentum by way of defining units of length such that the value of "c" is unity. For example, if the momentum "p" of an electron is said to be , then the conversion to MKS can be achieved by:
Distance.
In particle physics, a system of "natural units" in which the speed of light in vacuum "c" and the reduced Planck constant "ħ" are dimensionless and equal to unity is widely used: . In these units, both distances and times are expressed in inverse energy units (while energy and mass are expressed in the same units, see mass–energy equivalence). In particular, particle scattering lengths are often presented in units of inverse particle masses.
Outside this system of units, the conversion factors between electronvolt, second, and nanometer are the following:
The above relations also allow expressing the mean lifetime "τ" of an unstable particle (in seconds) in terms of its decay width "Γ" (in eV) via . For example, the B meson has a lifetime of 1.530(9) picoseconds, mean decay length is , or a decay width of .
Conversely, the tiny meson mass differences responsible for meson oscillations are often expressed in the more convenient inverse picoseconds.
Temperature.
In certain fields, such as plasma physics, it is convenient to use the electronvolt as a unit of temperature. The conversion to the Kelvin scale is defined by using "k", the Boltzmann constant:
For example, a typical magnetic confinement fusion plasma is , or 170 megakelvin.
As an approximation: "k""T" is about (≈ ) at a temperature of .
Properties.
The energy "E", frequency "v", and wavelength λ of a photon are related by
where "h" is the Planck constant, "c" is the speed of light. This reduces to
A photon with a wavelength of (green light) would have an energy of approximately . Similarly, would correspond to an infrared photon of wavelength or frequency .
Scattering experiments.
In a low-energy nuclear scattering experiment, it is conventional to refer to the nuclear recoil energy in units of eVr, keVr, etc. This distinguishes the nuclear recoil energy from the "electron equivalent" recoil energy (eVee, keVee, etc.) measured by scintillation light. For example, the yield of a phototube is measured in phe/keVee (photoelectrons per keV electron-equivalent energy). The relationship between eV, eVr, and eVee depends on the medium the scattering takes place in, and must be established empirically for each material.

</doc>
<doc id="9601" url="https://en.wikipedia.org/wiki?curid=9601" title="Electrochemistry">
Electrochemistry

Electrochemistry is the branch of physical chemistry that studies chemical reactions which take place at the interface of an electrode, usually a solid metal or a semiconductor, and an ionic conductor, the electrolyte. These reactions involve electric charges moving between the electrodes and the electrolyte (or ionic species in a solution). Thus electrochemistry deals with the interaction between electrical energy and chemical change.
When a chemical reaction is caused by an externally supplied current, as in electrolysis, or if an electric current is produced by a spontaneous chemical reaction as in a battery, it is called an "electrochemical" reaction. Chemical reactions where electrons are transferred directly between molecules and/or atoms are called oxidation-reduction or (redox) reactions. In general, electrochemistry describes the overall reactions when individual redox reactions are separate but connected by an external electric circuit and an intervening electrolyte.
History.
16th to 18th century developments.
Understanding of electrical matters began in the sixteenth century. During this century, the English scientist William Gilbert spent 17 years experimenting with magnetism and, to a lesser extent, electricity. For his work on magnets, Gilbert became known as the "Father of Magnetism." He discovered various methods for producing and strengthening magnets.
In 1663, the German physicist Otto von Guericke created the first electric generator, which produced static electricity by applying friction in the machine. The generator was made of a large sulfur ball cast inside a glass globe, mounted on a shaft. The ball was rotated by means of a crank and an electric spark was produced when a pad was rubbed against the ball as it rotated. The globe could be removed and used as source for experiments with electricity.
By the mid—18th century the French chemist Charles François de Cisternay du Fay had discovered two types of static electricity, and that like charges repel each other whilst unlike charges attract. Du Fay announced that electricity consisted of two fluids: "vitreous" (from the Latin for "glass"), or positive, electricity; and "resinous," or negative, electricity. This was the "two-fluid theory" of electricity, which was to be opposed by Benjamin Franklin's "one-fluid theory" later in the century.
In 1785, Charles-Augustin de Coulomb developed the law of electrostatic attraction as an outgrowth of his attempt to investigate the law of electrical repulsions as stated by Joseph Priestley in England.
In the late 18th century the Italian physician and anatomist Luigi Galvani marked the birth of electrochemistry by establishing a bridge between chemical reactions and electricity on his essay "De Viribus Electricitatis in Motu Musculari Commentarius" (Latin for Commentary on the Effect of Electricity on Muscular Motion) in 1791 where he proposed a "nerveo-electrical substance" on biological life forms.
In his essay Galvani concluded that animal tissue contained a here-to-fore neglected innate, vital force, which he termed "animal electricity," which activated nerves and muscles spanned by metal probes. He believed that this new force was a form of electricity in addition to the "natural" form produced by lightning or by the electric eel and torpedo ray as well as the "artificial" form produced by friction (i.e., static electricity).
Galvani's scientific colleagues generally accepted his views, but Alessandro Volta rejected the idea of an "animal electric fluid," replying that the frog's legs responded to differences in metal temper, composition, and bulk. Galvani refuted this by obtaining muscular action with two pieces of the same material.
19th century.
In 1800, William Nicholson and Johann Wilhelm Ritter succeeded in decomposing water into hydrogen and oxygen by electrolysis. Soon thereafter Ritter discovered the process of electroplating. He also observed that the amount of metal deposited and the amount of oxygen produced during an electrolytic process depended on the distance between the electrodes. By 1801, Ritter observed thermoelectric currents and anticipated the discovery of thermoelectricity by Thomas Johann Seebeck.
By the 1810s, William Hyde Wollaston made improvements to the galvanic cell.
Sir Humphry Davy's work with electrolysis led to the conclusion that the production of electricity in simple electrolytic cells resulted from chemical action and that chemical combination occurred between substances of opposite charge. This work led directly to the isolation of sodium and potassium from their compounds and of the alkaline earth metals from theirs in 1808.
Hans Christian Ørsted's discovery of the magnetic effect of electric currents in 1820 was immediately recognized as an epoch-making advance, although he left further work on electromagnetism to others. André-Marie Ampère quickly repeated Ørsted's experiment, and formulated them mathematically.
In 1821, Estonian-German physicist Thomas Johann Seebeck demonstrated the electrical potential in the juncture points of two dissimilar metals when there is a heat difference between the joints.
In 1827, the German scientist Georg Ohm expressed his law in this famous book "Die galvanische Kette, mathematisch bearbeitet" (The Galvanic Circuit Investigated Mathematically) in which he gave his complete theory of electricity.
In 1832, Michael Faraday's experiments led him to state his two laws of electrochemistry. In 1836, John Daniell invented a primary cell which solved the problem of polarization by eliminating hydrogen gas generation at the positive electrode. Later results revealed that alloying the amalgamated zinc with mercury would produce a higher voltage.
William Grove produced the first fuel cell in 1839. In 1846, Wilhelm Weber developed the electrodynamometer. In 1868, Georges Leclanché patented a new cell which eventually became the forerunner to the world's first widely used battery, the zinc carbon cell.
Svante Arrhenius published his thesis in 1884 on "Recherches sur la conductibilité galvanique des électrolytes" (Investigations on the galvanic conductivity of electrolytes). From his results the author concluded that electrolytes, when dissolved in water, become to varying degrees split or dissociated into electrically opposite positive and negative ions.
In 1886, Paul Héroult and Charles M. Hall developed an efficient method (the Hall–Héroult process) to obtain aluminium using electrolysis of molten alumina.
In 1894, Friedrich Ostwald concluded important studies of the conductivity and electrolytic dissociation of organic acids.
Walther Hermann Nernst developed the theory of the electromotive force of the voltaic cell in 1888. In 1889, he showed how the characteristics of the current produced could be used to calculate the free energy change in the chemical reaction producing the current. He constructed an equation, known as Nernst equation, which related the voltage of a cell to its properties.
In 1898, Fritz Haber showed that definite reduction products can result from electrolytic processes if the potential at the cathode is kept constant. In 1898, he explained the reduction of nitrobenzene in stages at the cathode and this became the model for other similar reduction processes.
20th century and recent developments.
In 1902, The Electrochemical Society (ECS) was founded.
In 1909, Robert Andrews Millikan began a series of experiments (see oil drop experiment) to determine the electric charge carried by a single electron.
In 1923, Johannes Nicolaus Brønsted and Martin Lowry published essentially the same theory about how acids and bases behave, using an electrochemical basis.
In 1937, Arne Tiselius developed the first sophisticated electrophoretic apparatus. Some years later, he was awarded the 1948 Nobel Prize for his work in protein electrophoresis.
A year later, in 1949, the International Society of Electrochemistry (ISE) was founded.
By the 1960s–1970s quantum electrochemistry was developed by Revaz Dogonadze and his pupils.
Principles.
Oxidation and reduction.
The term "redox" stands for reduction-oxidation. It refers to electrochemical processes involving electron transfer to or from a molecule or ion changing its oxidation state. This reaction can occur through the application of an external voltage or through the release of chemical energy. Oxidation and reduction describe the change of oxidation state that takes place in the atoms, ions or molecules involved in an electrochemical reaction. Formally, oxidation state is the hypothetical charge that an atom would have if all bonds to atoms of different elements were 100% ionic. An atom or ion that gives up an electron to another atom or ion has its oxidation state increase, and the recipient of the negatively charged electron has its oxidation state decrease.
For example, when atomic sodium reacts with atomic chlorine, sodium donates one electron and attains an oxidation state of +1. Chlorine accepts the electron and its oxidation state is reduced to −1. The sign of the oxidation state (positive/negative) actually corresponds to the value of each ion's electronic charge. The attraction of the differently charged sodium and chlorine ions is the reason they then form an ionic bond.
The loss of electrons from an atom or molecule is called oxidation, and the gain of electrons is reduction. This can be easily remembered through the use of mnemonic devices. Two of the most popular are "OIL RIG" (Oxidation Is Loss, Reduction Is Gain) and "LEO" the lion says "GER" (Lose Electrons: Oxidation, Gain Electrons: Reduction). Oxidation and reduction always occur in a paired fashion such that one species is oxidized when another is reduced. For cases where electrons are shared (covalent bonds) between atoms with large differences in electronegativity, the electron is assigned to the atom with the largest electronegativity in determining the oxidation state.
The atom or molecule which loses electrons is known as the "reducing agent", or "reductant", and the substance which accepts the electrons is called the "oxidizing agent", or "oxidant". Thus, the oxidizing agent is always being reduced in a reaction; the reducing agent is always being oxidized. Oxygen is a common oxidizing agent, but not the only one. Despite the name, an oxidation reaction does not necessarily need to involve oxygen. In fact, a fire can be fed by an oxidant other than oxygen; fluorine fires are often unquenchable, as fluorine is an even stronger oxidant (it has a higher electronegativity and thus accepts electrons even better) than oxygen.
For reactions involving oxygen, the gain of oxygen implies the oxidation of the atom or molecule to which the oxygen is added (and the oxygen is reduced). In organic compounds, such as butane or ethanol, the loss of hydrogen implies oxidation of the molecule from which it is lost (and the hydrogen is reduced). This follows because the hydrogen donates its electron in covalent bonds with non-metals but it takes the electron along when it is lost. Conversely, loss of oxygen or gain of hydrogen implies reduction.
Balancing redox reactions.
Electrochemical reactions in water are better understood by balancing redox reactions using the ion-electron method where H, OH ion, HO and electrons (to compensate the oxidation changes) are added to cell's half-reactions for oxidation and reduction.
Acidic medium.
In acid medium H ions and water are added to half-reactions to balance the overall reaction.
For example, when manganese reacts with sodium bismuthate.
Finally, the reaction is balanced by multiplying the number of electrons from the reduction half reaction to oxidation half reaction and vice versa and adding both half reactions, thus solving the equation.
Reaction balanced:
Basic medium.
In basic medium OH ions and water are added to half reactions to balance the overall reaction. For example, on reaction between potassium permanganate and sodium sulfite.
The same procedure as followed on acid medium by multiplying electrons to opposite half reactions solve the equation thus balancing the overall reaction.
Equation balanced:
Neutral medium.
The same procedure as used on acid medium is applied, for example on balancing using electron ion method to complete combustion of propane.
As in acid and basic medium, electrons which were used to compensate oxidation changes are multiplied to opposite half reactions, thus solving the equation.
Equation balanced:
Electrochemical cells.
An electrochemical cell is a device that produces an electric current from energy released by a spontaneous redox reaction. This kind of cell includes the Galvanic cell or Voltaic cell, named after Luigi Galvani and Alessandro Volta, both scientists who conducted several experiments on chemical reactions and electric current during the late 18th century.
Electrochemical cells have two conductive electrodes (the anode and the cathode). The anode is defined as the electrode where oxidation occurs and the cathode is the electrode where the reduction takes place. Electrodes can be made from any sufficiently conductive materials, such as metals, semiconductors, graphite, and even conductive polymers. In between these electrodes is the electrolyte, which contains ions that can freely move.
The galvanic cell uses two different metal electrodes, each in an electrolyte where the positively charged ions are the oxidized form of the electrode metal. One electrode will undergo oxidation (the anode) and the other will undergo reduction (the cathode). The metal of the anode will oxidize, going from an oxidation state of 0 (in the solid form) to a positive oxidation state and become an ion. At the cathode, the metal ion in solution will accept one or more electrons from the cathode and the ion's oxidation state is reduced to 0. This forms a solid metal that electrodeposits on the cathode. The two electrodes must be electrically connected to each other, allowing for a flow of electrons that leave the metal of the anode and flow through this connection to the ions at the surface of the cathode. This flow of electrons is an electric current that can be used to do work, such as turn a motor or power a light.
A galvanic cell whose electrodes are zinc and copper submerged in zinc sulfate and copper sulfate, respectively, is known as a Daniell cell.
Half reactions for a Daniell cell are these:
In this example, the anode is zinc metal which oxidizes (loses electrons) to form zinc ions in solution, and copper ions accept electrons from the copper metal electrode and the ions deposit at the copper cathode as an electrodeposit. This cell forms a simple battery as it will spontaneously generate a flow of electric current from the anode to the cathode through the external connection. This reaction can be driven in reverse by applying a voltage, resulting in the deposition of zinc metal at the anode and formation of copper ions at the cathode.
To provide a complete electric circuit, there must also be an ionic conduction path between the anode and cathode electrolytes in addition to the electron conduction path. The simplest ionic conduction path is to provide a liquid junction. To avoid mixing between the two electrolytes, the liquid junction can be provided through a porous plug that allows ion flow while reducing electrolyte mixing. To further minimize mixing of the electrolytes, a salt bridge can be used which consists of an electrolyte saturated gel in an inverted U-tube. As the negatively charged electrons flow in one direction around this circuit, the positively charged metal ions flow in the opposite direction in the electrolyte.
A voltmeter is capable of measuring the change of electrical potential between the anode and the cathode.
Electrochemical cell voltage is also referred to as electromotive force or emf.
A cell diagram can be used to trace the path of the electrons in the electrochemical cell. For example, here is a cell diagram of a Daniell cell:
First, the reduced form of the metal to be oxidized at the anode (Zn) is written. This is separated from its oxidized form by a vertical line, which represents the limit between the phases (oxidation changes). The double vertical lines represent the saline bridge on the cell. Finally, the oxidized form of the metal to be reduced at the cathode, is written, separated from its reduced form by the vertical line. The electrolyte concentration is given as it is an important variable in determining the cell potential.
Standard electrode potential.
To allow prediction of the cell potential, tabulations of standard electrode potential are available. Such tabulations are referenced to the standard hydrogen electrode (SHE). The standard hydrogen electrode undergoes the reaction
which is shown as reduction but, in fact, the SHE can act as either the anode or the cathode, depending on the relative oxidation/reduction potential of the other electrode/electrolyte combination. The term standard in SHE requires a supply of hydrogen gas bubbled through the electrolyte at a pressure of 1 atm and an acidic electrolyte with H activity equal to 1 (usually assumed to be ions (whose concentration increases). The reaction also shows the production of gaseous hydrogen, chlorine and aqueous sodium hydroxide.
Quantitative electrolysis and Faraday's laws.
Quantitative aspects of electrolysis were originally developed by Michael Faraday in 1834. Faraday is also credited to have coined the terms "electrolyte", electrolysis, among many others while he studied quantitative analysis of electrochemical reactions. Also he was an advocate of the law of conservation of energy.
First law.
Faraday concluded after several experiments on electric current in non-spontaneous process, the mass of the products yielded on the electrodes was proportional to the value of current supplied to the cell, the length of time the current existed, and the molar mass of the substance analyzed. In other words, the amount of a substance deposited on each electrode of an electrolytic cell is directly proportional to the quantity of electricity passed through the cell.
Below is a simplified equation of Faraday's first law:
Where
Second law.
Faraday devised the laws of chemical electrodeposition of metals from solutions in 1857. He formulated the second law of electrolysis stating "the amounts of bodies which are equivalent to each other in their ordinary chemical action have equal quantities of electricity naturally associated with them." In other words, the quantities of different elements deposited by a given amount of electricity are in the ratio of their chemical equivalent weights.
An important aspect of the second law of electrolysis is electroplating which together with the first law of electrolysis, has a significant number of applications in the industry, as when used to protect metals to avoid corrosion.
Applications.
There are various extremely important electrochemical processes in both nature and industry, like the coating of objects with metals or metal oxides through electrodeposition and the detection of alcohol in drunken drivers through the redox reaction of ethanol. The generation of chemical energy through photosynthesis is inherently an electrochemical process, as is production of metals like aluminum and titanium from their ores. Certain diabetes blood sugar meters measure the amount of glucose in the blood through its redox potential. As well as the established electrochemical technologies (like deep cycle lead acid batteries) there is also a wide range of new emerging technologies such as fuel cells, large format lithium ion batteries, electrochemical reactors and super-capacitors that are becoming increasingly commercial
The action potentials that travel down neurons are based on electric current generated by the movement of sodium and potassium ions into and out of cells. Specialized cells in certain animals like the electric eel can generate electric currents powerful enough to disable much larger animals.

</doc>
<doc id="9602" url="https://en.wikipedia.org/wiki?curid=9602" title="Edinburgh">
Edinburgh

Edinburgh (; ) is the capital city of Scotland, located in Lothian on the southern shore of the Firth of Forth. It is the second most populous city in Scotland and the seventh most populous in the United Kingdom. The most recent official population estimates are 464,990 for the city of Edinburgh itself and 492,680 for the local authority area. Edinburgh lies at the heart of the Edinburgh & South East Scotland City region with a population in 2014 of 1,339,380. Recognised as the capital of Scotland since at least the 15th century, Edinburgh is home to the Scottish Parliament and the seat of the monarchy in Scotland. The city is also the annual venue of the General Assembly of the Church of Scotland and home to national institutions such as the National Museum of Scotland, the National Library of Scotland and the Scottish National Gallery. It is the largest financial centre in the UK after London.
Historically part of Midlothian, the city has long been known as a centre of education, particularly in the fields of medicine, Scots law, literature, the sciences and engineering. The University of Edinburgh, founded in 1582 and now one of four in the city, was placed 17th in the QS World University Rankings in 2013 and 2014. The city is also famous for the Edinburgh International Festival and the Fringe, the latter being the largest annual international arts festival in the world. The city's historical and cultural attractions have made it the second most popular tourist destination in the United Kingdom after London, attracting over one million overseas visitors each year . Historic sites in Edinburgh include Edinburgh Castle, Holyrood Palace, the churches of St. Giles, Greyfriars and the Canongate, and the extensive Georgian New Town, built in the 18th century. Edinburgh's Old Town and New Town together are listed as a UNESCO World Heritage Site, which has been managed by Edinburgh World Heritage since 1999.
Etymology.
"Edin", the root of the city's name, is most likely of Brittonic Celtic origin, from the Cumbric language or a variation of it that would have been spoken by the earliest known people of the area, an Iron Age tribe known to the Romans as the "Votadini", and latterly in sub-Roman history as the Gododdin. It appears to derive from the place name "Eidyn" mentioned in the Old Welsh epic poem "Y Gododdin".
The poem names "Din Eidyn" as a hill fort ("Din" meaning "dun") in the territory of the Gododdin. The change in nomenclature, from "Din Eidyn" to "Edinburgh", reflects changes in the local language from Cumbric to Old English, the Germanic language of the Anglian kingdom of Bernicia that permeated the area from the mid-7th century and is regarded as the ancestor of modern Scots. The Celtic element "din" was dropped and replaced by the Old English "burh". The first documentary evidence of the medieval burgh is a royal charter, c.1124–1127, by King David I granting a toft in "burgo meo de Edenesburg" to the Priory of Dunfermline.
History.
Early history.
The earliest known human habitation in the Edinburgh area is from Cramond where evidence was found of a Mesolithic camp-site dated to c. 8500 BC. Traces of later Bronze Age and Iron Age settlements have been found on Castle Rock, Arthur's Seat, Craiglockhart Hill and the Pentland Hills.
When the Romans arrived in Lothian at the end of the 1st century AD, they discovered a Celtic Britonnic tribe whose name they recorded as the Votadini. At some point before the 7th century AD, the Gododdin, who were presumably descendants of the Votadini, built the hill fort of "Din Eidyn" or "Etin". Although its exact location has not been identified, it seems more than likely they would have chosen a commanding position like the Castle Rock or Arthur's Seat or Calton Hill.
In 638 AD the Gododdin stronghold was besieged by forces loyal to King Oswald of Northumbria, and around this time control of Lothian passed to the Angles. Their influence continued for the next three centuries until around 950 AD, when, during the reign of Indulf, son of Constantine II, the "burh" (fortress), named in the 10th-century "Pictish Chronicle" as "oppidum Eden", fell to the Scots and thenceforth remained under their jurisdiction.
The royal burgh was founded by King David I in the early 12th century on land belonging to the Crown, though the precise date is unknown. By the middle of the 14th century, the French chronicler Jean Froissart was describing it as the capital of Scotland (c.1365), and James III (1451–88) referred to it in the 15th century as "the principal burgh of our kingdom". Despite the destruction caused by an English assault in 1544, the town slowly recovered, and was at the centre of events in the 16th-century Scottish Reformation and 17th-century Wars of the Covenant.
17th century.
In 1603, King James VI of Scotland succeeded to the English throne, uniting the crowns of Scotland and England in a personal union known as the Union of the Crowns, though Scotland remained, in all other respects, a separate kingdom. In 1638, King Charles I's attempt to introduce Anglican church forms in Scotland encountered stiff Presbyterian opposition culminating in the conflicts of the Wars of the Three Kingdoms. Subsequent Scottish support for Charles Stuart's restoration to the throne of England resulted in Edinburgh's occupation by Oliver Cromwell's Commonwealth of England forces – the New Model Army – in 1650.
In the 17th century, the boundaries of Edinburgh were still defined by the city's defensive town walls. As a result, expansion took the form of the houses increasing in height to accommodate a growing population. Buildings of 11 storeys or more were common, and have been described as forerunners of the modern-day skyscraper. Most of these old structures were later replaced by the predominantly Victorian buildings seen in today's Old Town.
18th century.
In 1706 and 1707, the Acts of Union were passed by the Parliaments of England and Scotland uniting the two kingdoms into the Kingdom of Great Britain. As a consequence, the Parliament of Scotland merged with the Parliament of England to form the Parliament of Great Britain, which sat at Westminster in London. The Union was opposed by many Scots at the time, resulting in riots in the city.
By the first half of the 18th century, despite rising prosperity evidenced by its growing importance as a banking centre, Edinburgh was being described as one of the most densely populated, overcrowded and unsanitary towns in Europe. Visitors were struck by the fact that the various social classes shared the same urban space, even inhabiting the same tenement buildings; although here a form of social segregation did prevail, whereby shopkeepers and tradesmen tended to occupy the cheaper-to-rent cellars and garrets, while the more well-to-do professional classes occupied the more expensive middle storeys.
During the Jacobite rising of 1745, Edinburgh was briefly occupied by the Jacobite "Highland Army" before its march into England. After its eventual defeat at Culloden, there followed a period of reprisals and pacification, largely directed at the rebellious clans. In Edinburgh, the Town Council, keen to emulate London by initiating city improvements and expansion to the north of the castle, re-affirmed its belief in the Union and loyalty to the Hanoverian monarch George III by its choice of names for the streets of the New Town, for example, Rose Street and Thistle Street, and for the royal family: George Street, Queen Street, Hanover Street, Frederick Street and Princes Street (in honour of George's two sons).
In the second half of the century, the city was at the heart of the Scottish Enlightenment, when thinkers like David Hume, Adam Smith, James Hutton and Joseph Black were familiar figures in its streets. Edinburgh became a major intellectual centre, earning it the nickname "Athens of the North" because of its many neo-classical buildings and reputation for learning, similar to Ancient Athens. In the 18th century novel "The Expedition of Humphry Clinker" by Tobias Smollett one character describes Edinburgh as a "hotbed of genius".
From the 1770s onwards, the professional and business classes gradually deserted the Old Town in favour of the more elegant "one-family" residences of the New Town, a migration that changed the social character of the city. According to the foremost historian of this development, "Unity of social feeling was one of the most valuable heritages of old Edinburgh, and its disappearance was widely and properly lamented."
19th and 20th centuries.
Although Edinburgh's traditional industries of printing, brewing and distilling continued to grow in the 19th century and were joined by new rubber works and engineering works there was little industrialisation compared with other cities in Britain. By 1821, Edinburgh had been overtaken by Glasgow as Scotland's largest city. The city centre between Princes Street and George Street became a major commercial and shopping district, a development partly stimulated by the arrival of railways in the 1840s. The Old Town became an increasingly dilapidated, overcrowded slum with high mortality rates. Improvements carried out under Lord Provost William Chambers in the 1860s began the transformation of the area into the predominantly Victorian Old Town seen today. More improvements followed in the early 20th century as a result of the work of Patrick Geddes, but relative economic stagnation during the two world wars and beyond saw the Old Town deteriorate further before major slum clearance in the 1960s and 1970s began to reverse the process. University building developments which transformed the George Square and Potterrow areas proved highly controversial.
Since the 1990s a new "financial district", including a new Edinburgh International Conference Centre, has grown mainly on demolished railway property to the west of the castle, stretching into Fountainbridge, a run-down 19th-century industrial suburb which has undergone radical change since the 1980s with the demise of industrial and brewery premises. This ongoing development has enabled Edinburgh to maintain its place as the second largest financial and administrative centre in the United Kingdom after London. Financial services now account for a third of all commercial office space in the city. The development of Edinburgh Park, a new business and technology park covering , west of the city centre, has also contributed to the District Council's strategy for the city's major economic regeneration.
In 1998, the Scotland Act, which came into force the following year, established a devolved Scottish Parliament and Scottish Executive (renamed the Scottish Government since September 2007). Both based in Edinburgh, they are responsible for governing Scotland while reserved matters such as defence, taxation and foreign affairs remain the responsibility of the Parliament of the United Kingdom in London.
Geography.
Cityscape.
Situated in Scotland's Central Belt, Edinburgh lies on the southern shore of the Firth of Forth. The city centre is southwest of the shoreline of Leith and inland, as the crow flies, from the east coast of Scotland and the North Sea at Dunbar. While the early burgh grew up in close proximity to the prominent Castle Rock, the modern city is often said to be built on seven hills, namely Calton Hill, Corstorphine Hill, Craiglockhart Hill, Braid Hill, Blackford Hill, Arthur's Seat and the Castle Rock, giving rise to allusions to the seven hills of Rome.
Occupying a narrow gap between the Firth of Forth to the north and the Pentland Hills and their outrunners to the south, the city sprawls over a landscape which is the product of early volcanic activity and later periods of intensive glaciation.
Other prominent landforms such as Calton Hill and Corstorphine Hill are similarly products of glacial erosion. The Braid Hills and Blackford Hill are a series of small summits to the south west of the city commanding expansive views looking northwards over the urban area to the Forth.
Edinburgh is drained by the river named the Water of Leith, which rises at the Colzium Springs in the Pentland Hills and runs for through the south and west of the city, emptying into the Firth of Forth at Leith. The nearest the river gets to the city centre is at Dean Village on the north-western edge of the New Town, where a deep gorge is spanned by Thomas Telford's Dean Bridge, built in 1832 for the road to Queensferry. The Water of Leith Walkway is a mixed use trail that follows the course of the river for from Balerno to Leith.
Excepting the shoreline of the Firth of Forth, Edinburgh is encircled by a green belt, designated in 1957, which stretches from Dalmeny in the west to Prestongrange in the east. With an average width of the principal objectives of the green belt were to contain the outward expansion of the city and to prevent the agglomeration of urban areas. Expansion affecting the green belt is strictly controlled but developments such as Edinburgh Airport and the Royal Highland Showground at Ingliston lie within the zone. Similarly, outlying suburbs such as Juniper Green and Balerno are situated on green belt land. One feature of the Edinburgh green belt is the inclusion of parcels of land within the city which are designated green belt, even though they do not connect with the peripheral ring. Examples of these independent wedges of green belt include Holyrood Park and Corstorphine Hill.
Areas.
Edinburgh is divided into distinct areas that retain much of their original character as settlements in existence before they were absorbed into the sprawling city of the nineteenth century. Many residences are multi-occupancy buildings known as tenements, although the more southern and western parts of the city have traditionally been more affluent with a greater number of detached and semi-detached villas.
The historic centre of Edinburgh is divided in two by the broad green swathe of Princes Street Gardens. To the south the view is dominated by Edinburgh Castle, built high on the castle rock, and the long sweep of the Old Town descending towards Holyrood Palace. To the north lie Princes Street and the New Town.
The West End includes the financial district, with insurance and banking offices as well as the Edinburgh International Conference Centre.
The Old and New Towns of Edinburgh were listed as a UNESCO World Heritage Site in 1995 in recognition of the unique character of the Old Town with its medieval street layout and the planned Georgian New Town, including the adjoining Dean Village and Calton Hill areas. There are over 4,500 listed buildings within the city, a higher proportion relative to area than any other city in the United Kingdom.
The Old Town runs downhill and terminates at Holyrood Palace. Minor streets (called closes or wynds) lie on either side of the main spine forming a herringbone pattern. The street has several fine public buildings such as the church of St Giles, the City Chambers and the Law Courts. Other places of historical interest nearby are Greyfriars Kirkyard and the Grassmarket. The street layout is typical of the old quarters of many northern European cities.
The castle perches on top of a rocky crag (the remnant of an extinct volcano) and the Royal Mile runs down the crest of a ridge from it. Due to space restrictions imposed by the narrowness of this landform, the Old Town became home to some of the earliest "high rise" residential buildings. Multi-storey dwellings known as "lands" were the norm from the 16th century onwards with ten and eleven storeys being typical and one even reaching fourteen or fifteen storeys.
The New Town was an 18th-century solution to the problem of an increasingly crowded city which had been confined to the ridge sloping down from the castle. In 1766 a competition to design a "New Town" was won by James Craig, a 27-year-old architect. The plan was a rigid, ordered grid, which fitted in well with Enlightenment ideas of rationality. The principal street is George Street, running along the natural ridge to the north of what became known as the "Old Town". To either side of it are two other main streets: Princes Street and Queen Street. Princes Street has become the main shopping street in Edinburgh and now has few of its original Georgian buildings. The three main streets are connected by a series of streets running perpendicular to them. The east and west ends of George Street are terminated by St Andrew Square and Charlotte Square respectively. The latter, designed by Robert Adam, influenced the architectural style of the New Town into the early 19th century. Bute House, the official residence of the First Minister of Scotland, is on the north side of Charlotte Square.
The hollow between the Old and New Towns was formerly the Nor Loch, which was originally created for the town's defence but came to be used by the inhabitants for dumping their sewage. It was drained by the 1820s as part of the city's northward expansion. Craig's original plan included an ornamental canal on the site of the loch, but this idea was abandoned. Soil excavated while laying the foundations of buildings in the New Town was dumped on the site of the loch to create the slope connecting the Old and New Towns known as The Mound.
In the middle of the 19th century the National Gallery of Scotland and Royal Scottish Academy Building were built on The Mound, and tunnels for the railway line between Haymarket and Waverley stations were driven through it.
The Southside is a popular residential part of the city, which includes the districts of St Leonards, Marchmont, Newington, Sciennes, the Grange and Blackford. The Southside is broadly analogous to the area covered formerly by the Burgh Muir, and grew in popularity as a residential area after the opening of the South Bridge in the 1780s. The Southside is particularly popular with families (many state and private schools are here), young professionals and students (the central University of Edinburgh campus is based around George Square just north of Marchmont and the Meadows), and Napier University (with major campuses around Merchiston and Morningside). The area is also well provided with hotel and "bed and breakfast" accommodation for visiting festival-goers. These districts often feature in works of fiction. For example, Church Hill in Morningside, was the home of Muriel Spark's Miss Jean Brodie,
and Ian Rankin's Inspector Rebus lives in Marchmont and works in St Leonards.
Leith was historically the port of Edinburgh, an arrangement of unknown date that was reconfirmed by the royal charter Robert the Bruce granted to the city in 1329. The port developed a separate identity from Edinburgh, which to some extent it still retains, and it was a matter of great resentment when the two burghs merged in 1920 into the City of Edinburgh. Even today the parliamentary seat is known as "Edinburgh North and Leith". The loss of traditional industries and commerce (the last shipyard closed in 1983) resulted in economic decline. The Edinburgh Waterfront development which has transformed old dockland areas from Leith to Granton into residential areas with shopping and leisure facilities, has helped rejuvenate the area. With the redevelopment, Edinburgh has gained the business of cruise liner companies which now provide cruises to Norway, Sweden, Denmark, Germany, and the Netherlands.
The coastal suburb of Portobello is characterised by Georgian villas, Victorian tenements, a popular beach and promenade and cafes, bars, restaurants and independent shops. There are rowing and sailing clubs and a restored Victorian swimming pool, including Turkish baths.
The urban area of Edinburgh is almost entirely contained within the City of Edinburgh Council boundary, merging with Musselburgh in East Lothian. Towns within easy reach of the city boundary include Dalkeith, Bonnyrigg, Loanhead, Newtongrange, Prestonpans, Tranent, Penicuik, Haddington, Livingston, Broxburn and Dunfermline. Edinburgh lies at the heart of the Edinburgh & South East Scotland City region with a population in 2014 of 1,339,380.
Climate.
Like most of Scotland, Edinburgh has a temperate, maritime climate which is relatively mild despite its northerly latitude. Winter daytime temperatures rarely fall below freezing and are milder than places such as Moscow and Newfoundland which lie at similar latitudes. Summer temperatures are normally moderate, rarely exceeding . The highest temperature ever recorded in the city was on 4 August 1975 at Turnhouse Airport. The lowest temperature recorded in recent years was during December 2010 at Gogarbank.
The proximity of the city to the sea mitigates any large variations in temperature or extremes of climate. Given Edinburgh's position between the coast and hills, it is renowned as "the windy city", with the prevailing wind direction coming from the south west, which is frequently associated with warm, unstable air from the North Atlantic Current that can give rise to rainfall – although considerably less than cities to the west, such as Glasgow. Rainfall is distributed fairly evenly throughout the year. Winds from an easterly direction are usually drier but considerably colder, and may be accompanied by haar, a persistent coastal fog. Vigorous Atlantic depressions, known as European windstorms, can affect the city between October and May.
There is also a weather station in Gogarbank on the outskirts of the city. This slightly inland station has a slightly wider temperature span between seasons, is cloudier and somewhat wetter, but differences are minor.
Demography.
Current.
The most recent official population estimates are 464,990 for the city of Edinburgh itself and 492,680 for the local authority area. Edinburgh lies at the heart of the Edinburgh & South East Scotland City region with a population in 2014 of 1,339,380. This makes Edinburgh the second largest city in Scotland after Glasgow and the seventh largest in Britain.
Edinburgh has a high proportion of young adults, with 19.5% of the population in their 20s (exceeded only by Aberdeen) and 15.2% in their 30s which is the highest in Scotland. The proportion of Edinburgh's population who were born in the UK fell from 92% to 84% between 2001 and 2011, while the proportion born in Scotland fell from 78% to 70%. Of those Edinburgh residents born in the UK, 335,000 or 83% were born in Scotland, with 58,000 or 14% being born in England.
The proportion of people born outside the UK was 15.9% comparing with 8% in 2001. Countries accounting for the largest number of Edinburgh citizens born overseas are: Poland (13,000), Republic of Ireland (8,603), China (8,076), India (6,470), Pakistan (5,858), United States (3,700), Germany (3,500), Australia (2,100), France (2,000) Spain (2,000), South Africa (1,800) and Canada (1,800). 47% of the non-UK born population in Edinburgh is of European origin, which is amongst the highest for any city in the UK.
Some 13,000 people or 2.7% of the city's total population are Polish. 39,500 people or 8.2% of Edinburgh's population class themselves as Non-White which is an increase from 4% in 2001. Of the Non-White population, the largest group by far are Asian, totalling 26,264 people. Within the Asian population, the Chinese are now the largest sub-group, with 8,076 people, amounting to about 1.7% of the city's total population. The city's Indian population amounts to 6,470 (1.4% of the total population), while there are some 5,858 Pakistanis (1.2% of the total population). Although they account for only 1,277 people or 0.3% of the city's population, Edinburgh has the highest number and proportion of Bangladeshis in Scotland. Over 7,000 people were born in African countries (1.6% of the total population) and nearly 7,000 in the Americas. With the notable exception of Inner London, Edinburgh has a higher number of people born in the United States (over 3,700) than any other city in the UK.
Historical.
A census conducted by the Edinburgh presbytery in 1592 recorded a population of 8,003 adults spread equally north and south of the High Street which runs along the spine of the ridge sloping down from the Castle. In the 18th and 19th centuries, the population expanded rapidly, rising from 49,000 in 1751 to 136,000 in 1831, primarily due to migration from rural areas. As the population grew, problems of overcrowding in the Old Town, particularly in the cramped tenements that lined the present day Royal Mile and the Cowgate, were exacerbated. Poor sanitary arrangements resulted in a high incidence of disease, with outbreaks of cholera occurring in 1832, 1848 and 1866.
The construction of the New Town from 1767 onwards witnessed the migration of the professional and business classes from the difficult living conditions in the Old Town to the lower density, higher quality surroundings taking shape on land to the north.
Early 20th century population growth coincided with lower-density suburban development. As the city expanded to the south and west, detached and semi-detached villas with large gardens replaced tenements as the predominant building style. Nonetheless, the 2001 census revealed that over 55% of Edinburgh's population were still living in tenements or blocks of flats, a figure in line with other Scottish cities, but much higher than other British cities, and even central London.
From the early to mid 20th century the growth in population, together with slum clearance in the Old Town and other areas, such as Dumbiedykes, Leith, and Fountainbridge, led to the creation of new estates such as Stenhouse and Saughton, Craigmillar and Niddrie, Pilton and Muirhouse, Piershill, and Sighthill.
Religion.
The Church of Scotland claims the largest membership of any single religious denomination in Edinburgh. In 2010 there were 83 congregations in the Presbytery of Edinburgh. Its most prominent church is St Giles on the Royal Mile, first dedicated in 1243 but believed to date from before the 12th century. Saint Giles is historically the patron saint of Edinburgh. St Cuthbert's, situated at the west end of Princes Street Gardens in the shadow of Edinburgh Castle and St Giles' can lay claim to being the oldest Christian sites in the city, though the present St Cuthbert's, designed by Hippolyte Blanc, was dedicated in 1894.
Other Church of Scotland churches include Greyfriars Kirk, the Canongate Kirk, St Andrew's and St George's West Church and the Barclay Church. The Church of Scotland Offices are in Edinburgh, as is the Assembly Hall where the annual General Assembly is held.
The Roman Catholic Archdiocese of St Andrews and Edinburgh has 27 parishes across the city. The Archbishop of the Archdiocese of St Andrew's and Edinburgh has his official residence in Greenhill, and the diocesan offices are in nearby Marchmont. The Diocese of Edinburgh of the Scottish Episcopal Church has over 50 churches, half of them in the city. Its centre is the late 19th century Gothic style St Mary's Cathedral in the West End's Palmerston Place. There are several independent churches in the city, both Catholic and Protestant, including Charlotte Chapel, Carrubbers Christian Centre, Morningside Baptist Church, Bellevue Chapel and Sacred Heart. There are also churches belonging to Quakers, Christadelphians, Seventh-day Adventists, Church of Christ, Scientist and The Church of Jesus Christ of Latter-day Saints (LDS Church).
Edinburgh Central Mosque – Edinburgh's main mosque and Islamic Centre – is in Potterrow, on the city's Southside, near Bristo Square. Construction was largely financed by a gift from King Fahd of Saudi Arabia and was completed in 1998. There are other mosques in Annandale Street Lane, off Leith Walk, and in Queensferry Road, Blackhall as well as other Islamic centres across the city. There is also an active presence of the Ahmadiyya Muslim community.
The first recorded presence of a Jewish community in Edinburgh dates back to the late 18th century. Edinburgh's Orthodox synagogue, opened in 1932, is in Salisbury Road and can accommodate a congregation of 2000. A Liberal Jewish congregation also meets in the city. There are a Sikh gurdwara and a Hindu mandir, both in Leith, and a Brahma Kumaris centre in the Polwarth area. The Edinburgh Buddhist Centre, run by the Triratna Buddhist Community, formerly situated in Melville Terrace, now runs sessions at the Healthy Life Centre, Bread Street. Other Buddhist traditions are represented by groups which meet in the capital: the Community of Interbeing (followers of Thich Nhat Hanh), Rigpa, Samye Dzong, Theravadin, Pure Land and Shambala. There is a Sōtō Zen Priory in Portobello and a Theravadin Thai Buddhist Monastery in Slateford Road. Edinburgh is home to an active Bahá'í Community, and a Theosophical Society meets in Great King Street. Edinburgh has an active Inter-Faith Association.
Economy.
Edinburgh has the strongest economy of any city in the United Kingdom outside London and the highest percentage of professionals in the UK with 43% of the population holding a degree-level or professional qualification. According to the Centre for International Competitiveness, it is the most competitive large city in the United Kingdom. It also has the highest gross value added per employee of any city in the UK outside London, measuring £57,594 in 2010. It was named European "Best Large City of the Future for Foreign Direct Investment" and "Best Large City for Foreign Direct Investment Strategy" in the" Financial Times "fDi magazine awards 2012/13.
Known primarily for brewing and distilling, banking and insurance and printing and publishing in the 19th century, Edinburgh's economy is now based mainly on financial services, scientific research, higher education, and tourism. In March 2010 unemployment in Edinburgh was comparatively low at 3.6%, and it remains consistently below the Scottish average of 4.5%. Edinburgh is the 2nd most visited city by foreign visitors in the UK after London.
Banking has been a mainstay of the Edinburgh economy for over 300 years, since the Bank of Scotland (now part of the Lloyds Banking Group) was established by an act of the Scottish Parliament in 1695. Today, the financial services industry, with its particularly strong insurance and investment sectors, and underpinned by Edinburgh-based firms such as Scottish Widows and Standard Life, accounts for the city being the UK's second financial centre after London and Europe's fourth in terms of equity assets. The Royal Bank of Scotland opened new global headquarters at Gogarburn in the west of the city in October 2005, and Edinburgh is home to the headquarters of Bank of Scotland, Sainsbury's Bank, Tesco Bank and Virgin Money.
Tourism is also an important element in the city's economy. As a World Heritage Site, tourists come to visit historical sites such as Edinburgh Castle, the Palace of Holyroodhouse and view the Old and New Towns. Their numbers are augmented in August each year during the Edinburgh Festivals, which attracts 4.4 million visitors, and generates in excess of £100m for the local economy.
As the centre of Scotland's government and legal system, the public sector plays a central role in the economy of Edinburgh. Many departments of the Scottish Government are located in the city. Other major employers include NHS Scotland and local government administration.
Culture.
Festivals and celebrations.
Edinburgh festival.
The city hosts the annual Edinburgh International Festival, which is one of many events that run between the end of July and early September each year. The best known of these events are the Edinburgh Festival Fringe, the Edinburgh International Festival, the Edinburgh Military Tattoo and the Edinburgh International Book Festival.
The longest established of these festivals is the Edinburgh International Festival, which was first held in 1947 and consists mainly of a programme of high-profile theatre productions and classical music performances, featuring international directors, conductors, theatre companies and orchestras.
This has since been overtaken both in size and popularity by the Edinburgh Fringe which began as a programme of marginal acts alongside the "official" Festival and has become the largest performing arts festival in the world. In 2006, 1867 different shows were staged in 261 venues across the city. Comedy has become one of the mainstays of the Fringe, with numerous well-known comedians getting their first 'break' here, often by being chosen to receive the Edinburgh Comedy Award. In 2008, the largest comedy venues "on the Fringe" launched the Edinburgh Comedy Festival as a festival within a festival.
Other festivals include the Edinburgh Mountain Film Festival which takes place in February, Edinburgh Art Festival, Edinburgh International Film Festival, which takes place in June, the Edinburgh Jazz and Blues Festival, and the Edinburgh International Book Festival. The Edge Festival (formerly known as T on the Fringe), a popular music offshoot of the Fringe, began in 2000, replacing the smaller Flux and Planet Pop series of shows.
The Edinburgh Military Tattoo, one of the centrepieces of the "official" Festival, occupies the Castle Esplanade every night, with massed pipers and military bands drawn from around the world. Performances end with a short fireworks display. As well as the various summer festivals the Edinburgh International Science Festival is held annually in April and is one of the largest of its kind in Europe.
Edinburgh's Hogmanay.
The annual Edinburgh Hogmanay celebration was originally an informal street party focused on the Tron Kirk in the High Street of the Old Town. Since 1993 it has been officially organised with the focus moved to Princes Street. In 1996, over 300,000 people attended, leading to ticketing of the main street party in later years up to a limit of 100,000 tickets. Hogmanay now covers four days of processions, concerts and fireworks, with the street party beginning on Hogmanay. Alternative tickets are available for entrance into the Princes Street Gardens concert and Céilidh, where well-known artists perform and ticket holders can participate in traditional Scottish céilidh dancing. The event attracts thousands of people from all over the world.
On the night of 30 April the Beltane Fire Festival takes place on Calton Hill, involving a procession followed by scenes inspired by pagan old spring fertility celebrations. At the beginning of October each year the Dussehra Hindu Festival is also held on Calton Hill.
Music, theatre and film.
Outside the Festival season, Edinburgh supports several theatres and production companies. The Royal Lyceum Theatre has its own company, while the King's Theatre, Edinburgh Festival Theatre and Edinburgh Playhouse stage large touring shows. The Traverse Theatre presents a more contemporary repertoire. Amateur theatre companies productions are staged at the Bedlam Theatre, Church Hill Theatre and King's Theatre among others.
The Usher Hall is Edinburgh's premier venue for classical music, as well as occasional popular music concerts. It was the venue for the Eurovision Song Contest 1972. Other halls staging music and theatre include The Hub, the Assembly Rooms and the Queen's Hall. The Scottish Chamber Orchestra is based in Edinburgh.
Edinburgh has two repertory cinemas, the Edinburgh Filmhouse and The Cameo, as well as the independent Dominion Cinema and a range of multiplexes.
Edinburgh has a healthy popular music scene. Occasionally large concerts are staged at Murrayfield and Meadowbank, while mid-sized events take place at smaller venues such as the Corn Exchange, the Liquid Rooms and the Bongo Club. In 2010, PRS for Music listed Edinburgh among the UK's top ten 'most musical' cities. Several city pubs are well known for their live performances of folk music. They include 'Sandy Bell's' in Forrest Road, 'The Captain's Bar' in South College Street, and 'Whistlebinkies' in Niddry Street.
Edinburgh is home to a flourishing group of contemporary composers such as Nigel Osborne, Peter Nelson, Lyell Cresswell, Hafliði Hallgrímsson, Edward Harper, Robert Crawford, Robert Dow and John McLeod. McLeod's music is heard regularly on BBC Radio 3 and throughout the UK.
Rockstar North, formerly DMA Design, known for creating the Grand Theft Auto series, is based in Edinburgh.
Media.
The "Edinburgh Evening News" is based in Edinburgh and published every day except Sunday. Johnston Press owns the title and "The Scotsman"; their corporate headquarters are in Edinburgh and their national newspaper is the only one published in the city. "The Herald" newspaper, published in Glasgow, also covers Edinburgh.
The city has two commercial radio stations: Forth 1, a station which broadcasts mainstream chart music, and Forth 2 on medium wave which plays classic hits. Capital Radio Scotland and Eklipse Sports Radio also have transmitters covering Edinburgh. Along with the UK national radio stations, Radio Scotland and the Gaelic language service BBC Radio nan Gàidheal are also broadcast. DAB digital radio is broadcast over two local multiplexes. BFBS Radio broadcasts from studios on the base at Dreghorn Barracks across the city on 98.5FM as part of its UK Bases network
STV Edinburgh, a local TV channel for the city, launched on 12 January 2015. Television, along with most radio services, is broadcast to the city from the Craigkelly transmitting station situated in Fife on the opposite side of the Firth of Forth.
Museums, libraries and galleries.
Edinburgh has many museums and libraries. These include the National Museum of Scotland, the National Library of Scotland, National War Museum, the Museum of Edinburgh, Surgeons' Hall Museum, the Grand Lodge of Scotland Museum and Library, the Museum of Childhood, and Our Dynamic Earth.
Edinburgh Zoo, covering on Corstorphine Hill, is the second most popular paid tourist attraction in Scotland, and currently home to two giant pandas, Tian Tian and Yang Guang, on loan from the People's Republic of China.
Edinburgh is also home to The Royal Yacht Britannia, decommissioned in 1997 and now a five-star visitor attraction and evening events venue permanently berthed at Ocean Terminal.
Edinburgh contains Scotland's five National Galleries of Art as well as numerous smaller art galleries. The national collection is housed in the National Gallery of Scotland, located on the Mound, now linked to the Royal Scottish Academy which holds regular major exhibitions of paintings. Contemporary collections are shown in the Scottish National Gallery of Modern Art which occupies a split site at Belford. The Scottish National Portrait Gallery in Queen Street focuses on portraits and photography.
The council-owned City Art Centre in Market Street mounts regular art exhibitions. Across the road, The Fruitmarket Gallery offers world class exhibitions of contemporary art, featuring work by British and international artists with both emerging and established international reputations.
There are many small private galleries, including the Ingleby Gallery. This provides a varied programme including shows by Callum Innes, Peter Liversidge, Ellsworth Kelly, Richard Forster, and Sean Scully.
The city hosts several of Scotland's galleries and organisations dedicated to contemporary visual art. Significant strands of this infrastructure include: The Scottish Arts Council, Edinburgh College of Art, Talbot Rice Gallery (University of Edinburgh) and the Edinburgh Annuale.
Shopping.
The locale around Princes Street is the main shopping area in the city centre, with souvenir shops, chain stores such as Boots the Chemist, H&M and Jenners. George Street, north of Princes Street, is the preferred location for some upmarket shops and independent stores. The St. James Centre at the east end of Princes Street hosts national chains including a large John Lewis store. Multrees Walk, adjacent to the St. James Centre, is a recent addition to the central shopping district, dominated by the presence of Harvey Nichols. Shops here include Louis Vuitton, Emporio Armani, Mulberry and Calvin Klein.
Edinburgh also has substantial retail parks outside the city centre. These include The Gyle Shopping Centre and Hermiston Gait in the west of the city, Cameron Toll Shopping Centre, Straiton Retail Park and Fort Kinnaird in the south and east, and Ocean Terminal in the north on the Leith waterfront.
Governance.
Following local government reorganisation in 1996, Edinburgh constitutes one of the 32 council areas of Scotland. Like all other local authorities of Scotland, the council has powers over most matters of local administration such as housing, planning, local transport, parks, economic development and regeneration. The council comprises 58 elected councillors, returned from 17 multi-member electoral wards in the city. Following the 2007 Scottish Local Elections the incumbent Labour Party lost majority control of the council after 23 years to a Liberal Democrat/SNP coalition.
The city's coat of arms was registered by the Lord Lyon King of Arms in 1732.
Edinburgh is represented in the Scottish Parliament. For electoral purposes, the city is divided into six of the nine constituencies in the Lothians electoral region. Each constituency elects one Member of the Scottish Parliament (MSP) by the first past the post system of election, and the region elects seven additional MSPs to produce a result based on a form of proportional representation.
Edinburgh is also represented in the British House of Commons by five Members of Parliament. The city is divided into Edinburgh North and Leith, Edinburgh East, Edinburgh South, Edinburgh South West, and Edinburgh West, each constituency electing one member by the first past the post system.
In the 2014 Scottish independence referendum, voters in Edinburgh rejected independence by a margin of 61.1% No to 38.9% Yes. Turnout was at 84.4%. Numerically, Edinburgh had the largest number of No votes out of all 32 council areas in Scotland with 194,638 No votes to 123,927 Yes votes. Further to this, the difference between the number of Yes and No votes was largest in Edinburgh by comparison to any other council area at 70,711.
Transport.
Edinburgh Airport is Scotland's busiest and biggest airport and the principal international gateway to the capital, handling around 9 million passengers in 2012. In anticipation of rising passenger numbers, the airport operator BAA outlined a draft masterplan in 2011 to provide for the expansion of the airfield and the terminal building. The airport has since been sold, in June 2012, to Global Infrastructure Partners (GIP). The possibility of building a second runway to cope with an increased number of aircraft movements has also been mooted.
Travel in Edinburgh is undertaken predominantly by bus. Lothian Buses operate the majority of city bus services within the city and to surrounding suburbs, with the most routes running via Princes Street. Services further afield operate from the Edinburgh Bus Station off St Andrew Square and Waterloo Place and are operated mainly by Stagecoach, Scottish Citylink, National Express Coaches, First Scotland East & Perryman's Buses.
Lothian Buses, as the successor company to the Edinburgh Corporation Transport Department, also operates all of the city's branded public tour buses, night bus service and airport bus link.
Edinburgh Waverley Station is the second-busiest railway station in Scotland, with only Glasgow Central handling more passengers. On the evidence of passenger entries and exits between April 2010 and March 2011, Edinburgh Waverley is the fifth-busiest station outside London; it is also the UK's second biggest station in terms of the number of platforms. Waverley is the terminus for most trains arriving from London King's Cross and the departure point for many rail services within Scotland operated by Abellio ScotRail.
To the west of the city centre lies Haymarket Station which is an important commuter stop. Opened in 2003, Edinburgh Park station serves the Gyle business park in the west of the city and the nearby Gogarburn headquarters of the Royal Bank of Scotland. The Edinburgh Crossrail route connects Edinburgh Park with Haymarket, Edinburgh Waverley and the suburban stations of Brunstane and Newcraighall in the east of the city. There are also commuter lines to South Gyle and Dalmeny, the latter serving South Queensferry by the Forth Bridges, and to Wester Hailes and Curriehill in the south west of the city.
To tackle traffic congestion, Edinburgh is now served by six park and ride sites on the periphery of the city at Sheriffhall (in Midlothian), Ingliston, Riccarton, Inverkeithing (in Fife), Newcraighall and Straiton (in Midlothian). A referendum of Edinburgh residents in February 2005 rejected a proposal to introduce congestion charging in the city.
Edinburgh Trams became operational on 31 May 2014. The city had been without a tram system since Edinburgh Corporation Tramways ceased on 16 November 1956. Following parliamentary approval in 2007, construction began in early 2008. The first stage of the project was expected to be completed by July 2011 but, following delays caused by extra utility work and a long-running contractual dispute between the Council and the main contractor, Bilfinger SE, the project was rescheduled. The cost of the project rose from the original projection of £545 million to £750 million in mid-2011 and some suggest that it could eventually exceed £1 billion. The completed line is in length, running from Edinburgh Airport, west of the city, to its current terminus at York Place in the city centre's East End. It was originally planned to continue down Leith Walk to Ocean Terminal and where it would terminate at Newhaven.
Should the original plan be resumed and taken to completion, trams will also run from Haymarket through Ravelston and Craigleith to Granton Square on the Waterfront Edinburgh. Long-term proposals envisage a line running west from the airport to Ratho and Newbridge and another connecting Granton Square to Newhaven via Lower Granton Road, thus completing the Line 1 (North Edinburgh) loop. A further line serving the south of the city has also been suggested.
Transport for Edinburgh released a statement on 7 July 2014 that average weekly tram passengers are currently in excess of 90,000 after the first week of service saw 130,000 journeys taken.
Education.
There are four universities in Edinburgh (including Queen Margaret University which lies just outwith the city boundary) with students making up around one-fifth of the population. Established by Royal Charter in 1583, the University of Edinburgh is one of Scotland's ancient universities and is the fourth oldest in the country after St Andrews, Glasgow and Aberdeen. Originally centred on Old College the university expanded to premises on The Mound, the Royal Mile and George Square. Today, the King's Buildings in the south of the city contain most of the schools within the College of Science and Engineering. In 2002, the medical school moved to purpose built accommodation adjacent to the new Edinburgh Royal Infirmary at Little France. The University was placed 17th in the QS World University Rankings for 2013.
Heriot-Watt University and Napier Technical College were established in the 1960s. Heriot-Watt began as the world's first Mechanics' Institute, tracing its origins to 1821 when it opened as a school for the technical education of the working classes. The former Napier College was renamed Napier Polytechnic in 1986 and gained university status in 1992. Edinburgh Napier University has campuses in the south and west of the city, including the former Merchiston Tower and Craiglockhart Hydropathic. It is home to the Screen Academy Scotland.
Queen Margaret University was located in Edinburgh before it moved to a new campus near Musselburgh in 2008.
Until 2012 further education colleges in the city included Jewel and Esk College (incorporating Leith Nautical College founded in 1903), Telford College, opened in 1968, and Stevenson College, opened in 1970. These have now been amalgamated to form Edinburgh College. The Scottish Agricultural College also has a campus in south Edinburgh. Other institutions include the Royal College of Surgeons of Edinburgh and the Royal College of Physicians of Edinburgh which were established by Royal Charter in 1506 and 1681 respectively. The Trustees Drawing Academy of Edinburgh, founded in 1760, became the Edinburgh College of Art in 1907.
There are 18 nursery, 94 primary and 23 secondary schools administered by the City of Edinburgh Council.
Edinburgh is home to The Royal High School, one of the oldest schools in the country and the world. The city also has several independent, fee-paying schools including Edinburgh Academy, Fettes College, George Heriot's School, George Watson's College, Merchiston Castle School, Stewart's Melville College and The Mary Erskine School. In 2009, the proportion of pupils attending independent schools was 24.2%, far above the Scottish national average of just over 7% and higher than in any other region of Scotland. In August 2013, Edinburgh City Council opened the city's first stand-alone Gaelic primary school, Bun-sgoil Taobh na Pàirce.
Healthcare.
The main NHS Lothian hospitals serving the Edinburgh area are the Royal Infirmary of Edinburgh, which includes the University of Edinburgh Medical School, and the Western General Hospital, which has a large cancer treatment centre and nurse-led Minor Injuries Clinic. The Royal Edinburgh Hospital in Morningside specialises in mental health. The Royal Hospital for Sick Children, popularly referred to as 'the Sick Kids', is a specialist paediatrics hospital.
There are two private hospitals: Murrayfield Hospital in the west of the city and Shawfield Hospital in the south. Both are owned by Spire Healthcare.
Sport.
Football.
Edinburgh has two professional football clubs: Heart of Midlothian, founded in 1874, and Hibernian, founded in 1875. Known locally as "Hearts" and "Hibs", the former plays in the Scottish Premiership and the latter in the Scottish Championship. They are the oldest city rivals in Scotland and the Edinburgh derby is one of the oldest derby matches in world football. Both clubs have won the Scottish league championship four times. Hearts have won the Scottish Cup eight times and the Scottish League Cup four times. Hibs have won the Scottish Cup twice and the Scottish League Cup three times.
Edinburgh was also home to four other former Scottish Football League clubs: Edinburgh City, Leith Athletic, Meadowbank Thistle and St Bernard's. Meadowbank Thistle played at Meadowbank Stadium until 1995, when the club moved to Livingston and became Livingston F.C. The Scottish national team has very occasionally played at Easter Road and Tynecastle, although its normal home stadium is Hampden Park in Glasgow. St Bernard's' New Logie Green was used to host the 1896 Scottish Cup Final, the only time the match has been played outside Glasgow.
The city also plays host to Lowland Football League clubs Edinburgh City, Edinburgh University and Spartans as well as junior club Edinburgh United and East of Scotland League clubs Civil Service Strollers, Craigroyston, Heriot-Watt University, Leith Athletic, Lothian Thistle Hutchison Vale and Tynecastle.
Rugby.
The Scotland national rugby union team and the professional Edinburgh Rugby team play at Murrayfield Stadium, which is owned by the Scottish Rugby Union and also used for other events, including music concerts. It is the largest capacity stadium in Scotland, seating 67,144 spectators. Edinburgh is also home to RBS Premier One rugby teams Heriot's Rugby Club, Boroughmuir RFC, the Edinburgh Academicals and Currie RFC.
Rugby league is represented by the Edinburgh Eagles who play in the Rugby League Conference Scotland Division. Murrayfield Stadium has hosted the Magic Weekend where all Super League matches are played in the stadium over one weekend.
Other sports.
The Scottish cricket team, which represents Scotland internationally, play their home matches at the Grange cricket club.
The Edinburgh Capitals are the latest of a succession of ice hockey clubs in the Scottish capital. Previously Edinburgh was represented by the Murrayfield Racers and the Edinburgh Racers. The club play their home games at the Murrayfield Ice Rink and have competed in the ten-team professional Elite Ice Hockey League since the 2005–06 season.
Right next door to Murrayfield Ice Rink is a 7-sheeter dedicated curling facility where curling is played from October to March each season.
The Edinburgh Diamond Devils is a baseball club which won its first Scottish Championship in 1991 as the "Reivers." 1992 saw the team repeat the achievement, becoming the first team to do so in league history. The same year saw the start of their first youth team, the Blue Jays. The club adopted its present name in 1999.
Edinburgh has also hosted national and international sports events including the World Student Games, the 1970 British Commonwealth Games, the 1986 Commonwealth Games and the inaugural 2000 Commonwealth Youth Games. For the 1970 Games the city built Olympic standard venues and facilities including Meadowbank Stadium and the Royal Commonwealth Pool. The Pool underwent refurbishment in 2012 and is due to host the Diving competition in the 2014 Commonwealth Games which will be held in Glasgow.
In American football, the Scottish Claymores played WLAF/NFL Europe games at Murrayfield, including their World Bowl 96 victory. From 1995 to 1997 they played all their games there, from 1998 to 2000 they split their home matches between Murrayfield and Glasgow's Hampden Park, then moved to Glasgow full-time, with one final Murrayfield appearance in 2002. The city's most successful non-professional team are the Edinburgh Wolves who play at Meadowbank Stadium.
The Edinburgh Marathon has been held annually in the city since 2003 with more than 16,000 runners taking part on each occasion. Its organisers have called it "the fastest marathon in the UK" due to the elevation drop of . The city also organises a half-marathon, as well as 10 km () and 5 km () races, including a race on 1 January each year.
Edinburgh has a speedway team, the Edinburgh Monarchs, which, since the loss of its stadium in the city, has raced at the Lothian Arena in Armadale, West Lothian. The Monarchs have won the Premier League championship five times in their history, in 2003 and again in 2008, 2010, 2014 and 2015.
Notable residents.
Edinburgh has a long literary tradition, which became especially evident during the Scottish Enlightenment. This heritage and the city's lively literary life in the present led to it being declared the first UNESCO City of Literature in 2004. Famous authors who have lived in Edinburgh include the economist Adam Smith, born in Kirkcaldy and author of "The Wealth of Nations",
Scotland has a rich history of science and engineering, with Edinburgh producing a number of famous names. John Napier, inventor of logarithms, was born in Merchiston Tower and lived and died in the city. His house now forms part the original campus of Napier University which was named in his honour. He lies buried under St. Cuthbert's Church. James Clerk Maxwell, founder of the modern theory of electromagnetism, was born in the city and educated at the Edinburgh Academy and the University of Edinburgh, as was the engineer and telephone pioneer Alexander Graham Bell. James Braidwood, who organised Britain's first municipal fire brigade was also born in the city and began his career there.
Other names connected with the city include Max Born, physicist and Nobel laureate; Charles Darwin, the biologist who propounded the theory of natural selection; David Hume, philosopher, economist and historian; James Hutton, regarded as the "Father of Geology"; Joseph Black, the chemist and one of the founders of thermodynamics; pioneering medical researchers Joseph Lister and James Young Simpson; chemist and discoverer of the element nitrogen Daniel Rutherford, Colin Maclaurin, mathematician and developer of the Maclaurin series, and Ian Wilmut, the geneticist involved in the cloning of Dolly the sheep just outside Edinburgh. The stuffed carcass of Dolly the sheep is now on display in the National Museum of Scotland. The latest in a long line of science celebrities associated with the city is theoretical physicist and Nobel Prizewinner Professor Emeritus Peter Higgs, born in Newcastle but resident in Edinburgh for most of his academic career, after whom the Higgs boson particle has been named.
Edinburgh has been the birthplace of actors like Alastair Sim and Sir Sean Connery, famed as the first cinematic James Bond, the comedian and actor Ronnie Corbett, best known as one of The Two Ronnies, and the impressionist Rory Bremner. Famous artists from the city include the portrait painters Sir Henry Raeburn, Sir David Wilkie and Allan Ramsay.
The city has produced or been home to some very successful musicians in recent decades, particularly Ian Anderson, front man of the band Jethro Tull, The Incredible String Band, the folk duo The Corries, Wattie Buchan, lead singer and founding member of punk band The Exploited, Shirley Manson, lead singer of the band Garbage, the Bay City Rollers, The Proclaimers, Boards of Canada and Idlewild.
Edinburgh is the birthplace of former British Prime Minister Tony Blair who attended the city's Fettes College.
Notorious criminals from Edinburgh's past include Deacon Brodie, head of a trades guild and Edinburgh city councillor by day but a burglar by night, who is said to have been the inspiration for Robert Louis Stevenson's story, the "Strange Case of Dr Jekyll and Mr Hyde", and murderers Burke and Hare who delivered fresh corpses for dissection to the famous anatomist Robert Knox.
Another well-known Edinburgh resident was Greyfriars Bobby. The small Skye Terrier reputedly kept vigil over his dead master's grave in Greyfriars Kirkyard for 14 years in the 1860s and 1870s, giving rise to a story of canine devotion which plays a part in attracting visitors to the city.
International relations.
Twin towns and sister cities.
The City of Edinburgh has entered into 14 international twinning arrangements since 1954. Most of the arrangements are styled as 'Twin Cities' but the agreement with Kraków is designated as a 'Partner City', and the agreement with Kyoto Prefecture is officially styled as a 'Friendship Link', reflecting its status as the only region to be twinned with Edinburgh.
For a list of consulates in Edinburgh see List of diplomatic missions in Scotland.

</doc>
<doc id="9603" url="https://en.wikipedia.org/wiki?curid=9603" title="Ernest Rutherford">
Ernest Rutherford

Ernest Rutherford, 1st Baron Rutherford of Nelson, (30 August 1871 – 19 October 1937) was a New Zealand-born British physicist who became known as the father of nuclear physics. "Encyclopædia Britannica" considers him to be the greatest experimentalist since Michael Faraday (1791–1867).
In early work he discovered the concept of radioactive half-life, proved that radioactivity involved the nuclear transmutation of one chemical element to another, and also differentiated and named alpha and beta radiation. This work was done at McGill University in Canada. It is the basis for the Nobel Prize in Chemistry he was awarded in 1908 "for his investigations into the disintegration of the elements, and the chemistry of radioactive substances", for which he is the first Canadian and Oceanian Nobel laureate, and remains the only laureate born in the South Island.
Rutherford moved in 1907 to the Victoria University of Manchester (today University of Manchester) in the UK, where he and Thomas Royds proved that alpha radiation is helium nuclei. Rutherford performed his most famous work after he became a Nobel laureate. In 1911, although he could not prove that it was positive or negative,
he theorized that atoms have their charge concentrated in a very small nucleus,
and thereby pioneered the Rutherford model of the atom, through his discovery and interpretation of Rutherford scattering in his gold foil experiment. He is widely credited with first "splitting the atom" in 1917 in a nuclear reaction between nitrogen and alpha particles, in which he also discovered (and named) the proton.
Rutherford became Director of the Cavendish Laboratory at the University of Cambridge in 1919. Under his leadership the neutron was discovered by James Chadwick in 1932 and in the same year the first experiment to split the nucleus in a fully controlled manner, performed by students working under his direction, John Cockcroft and Ernest Walton. After his death in 1937, he was honoured by being interred with the greatest scientists of the United Kingdom, near Sir Isaac Newton's tomb in Westminster Abbey. The chemical element rutherfordium (element 104) was named after him in 1997.
Biography.
Early life and education.
Ernest Rutherford was the son of James Rutherford, a farmer, and his wife Martha Thompson, originally from Hornchurch, Essex, England. James had emigrated to New Zealand from Perth, Scotland, "to raise a little flax and a lot of children". Ernest was born at Brightwater, near Nelson, New Zealand. His first name was mistakenly spelled 'Earnest' when his birth was registered.
He studied at Havelock School and then Nelson College and won a scholarship to study at Canterbury College, University of New Zealand, where he participated in the debating society and played rugby. After gaining his BA, MA and BSc, and doing two years of research during which he invented a new form of radio receiver, in 1895 Rutherford was awarded an 1851 Research Fellowship from the Royal Commission for the Exhibition of 1851, to travel to England for postgraduate study at the Cavendish Laboratory, University of Cambridge. He was among the first of the 'aliens' (those without a Cambridge degree) allowed to do research at the university, under the inspiring leadership of J. J. Thomson, and the newcomers aroused jealousies from the more conservative members of the Cavendish fraternity. With Thomson's encouragement, he managed to detect radio waves at half a mile and briefly held the world record for the distance over which electromagnetic waves could be detected, though when he presented his results at the British Association meeting in 1896, he discovered he had been outdone by another lecturer, by the name of Marconi.
In 1898 Thomson recommended Rutherford for a position at McGill University in Montreal, Canada. He was to replace Hugh Longbourne Callendar who held the chair of Macdonald Professor of physics and was coming to Cambridge. Rutherford was accepted, which meant that in 1900 he could marry Mary Georgina Newton (1876–1954) to whom he had become engaged before leaving New Zealand; they had one daughter, Eileen Mary (1901–1930), who married Ralph Fowler. In 1900 he gained a DSc from the University of New Zealand. In 1907 Rutherford returned to Britain to take the chair of physics at the University of Manchester.
Later years and honours.
He was knighted in 1914. During World War I, he worked on a top secret project to solve the practical problems of submarine detection by sonar. In 1916 he was awarded the Hector Memorial Medal. In 1919 he returned to the Cavendish succeeding J. J. Thomson as the Cavendish professor and Director. Under him, Nobel Prizes were awarded to James Chadwick for discovering the neutron (in 1932), John Cockcroft and Ernest Walton for an experiment which was to be known as "splitting the atom" using a particle accelerator, and Edward Appleton for demonstrating the existence of the ionosphere. In 1925, Rutherford pushed calls to the Government of New Zealand to support education and research, which led to the formation of the Department of Scientific and Industrial Research (DSIR) in the following year. Between 1925 and 1930 he served as President of the Royal Society, and later as president of the Academic Assistance Council which helped almost 1,000 university refugees from Germany. He was admitted to the Order of Merit in 1925 and raised to the peerage as Baron Rutherford of Nelson, in 1931, a title that became extinct upon his unexpected death in 1937. In 1933, Rutherford was one of the two inaugural recipients of the T. K. Sidey Medal, set up by the Royal Society of New Zealand as an award for outstanding scientific research.
For some time before his death, Rutherford had a small hernia, which he had neglected to have fixed, and it became strangulated, causing him to be violently ill. Despite an emergency operation in London, he died four days afterwards of what physicians termed "intestinal paralysis", at Cambridge. After cremation at Golders Green Crematorium, he was given the high honour of burial in Westminster Abbey, near Isaac Newton and other illustrious British scientists.
Scientific research.
At Cambridge, Rutherford started to work with J. J. Thomson on the conductive effects of X-rays on gases, work which led to the discovery of the electron which Thomson presented to the world in 1897. Hearing of Becquerel's experience with uranium, Rutherford started to explore its radioactivity, discovering two types that differed from X-rays in their penetrating power. Continuing his research in Canada, he coined the terms alpha ray and beta ray in 1899 to describe the two distinct types of radiation. He then discovered that thorium gave off a gas which produced an emanation which was itself radioactive and would coat other substances. He found that a sample of this radioactive material of any size invariably took the same amount of time for half the sample to decay – its "half-life" (11½ minutes in this case).
From 1900 to 1903, he was joined at McGill by the young chemist Frederick Soddy (Nobel Prize in Chemistry, 1921) for whom he set the problem of identifying the thorium emanations. Once he had eliminated all the normal chemical reactions, Soddy suggested that it must be one of the inert gases, which they named thoron (later found to be an isotope of radon). They also found another type of thorium they called Thorium X, and kept on finding traces of helium. They also worked with samples of "Uranium X" from William Crookes and radium from Marie Curie.
In 1902, they produced a "Theory of Atomic Disintegration" to account for all their experiments. Up till then atoms were assumed to be the indestructable basis of all matter and although Curie had suggested that radioactivity was an atomic phenomenon, the idea of the atoms of radioactive substances breaking up was a radically new idea. Rutherford and Soddy demonstrated that radioactivity involved the spontaneous disintegration of atoms into other types of atoms (one element spontaneously being changed to another).
In 1903, Rutherford considered a type of radiation discovered (but not named) by French chemist Paul Villard in 1900, as an emission from radium, and realised that this observation must represent something different from his own alpha and beta rays, due to its very much greater penetrating power. Rutherford therefore gave this third type of radiation the name of gamma ray. All three of Rutherford's terms are in standard use today – other types of radioactive decay have since been discovered, but Rutherford's three types are among the most common.
In Manchester, he continued to work with alpha radiation. In conjunction with Hans Geiger, he developed zinc sulfide scintillation screens and ionisation chambers to count alphas. By dividing the total charge they produced by the number counted, Rutherford decided that the charge on the alpha was two. In late 1907, Ernest Rutherford and Thomas Royds allowed alphas to penetrate a very thin window into an evacuated tube. As they sparked the tube into discharge, the spectrum obtained from it changed, as the alphas accumulated in the tube. Eventually, the clear spectrum of helium gas appeared, proving that alphas were at least ionised helium atoms, and probably helium nuclei.
Gold foil experiment.
Rutherford performed his most famous work "after" receiving the Nobel prize in 1908. Along with Hans Geiger and Ernest Marsden in 1909, he carried out the Geiger–Marsden experiment, which demonstrated the nuclear nature of atoms by deflecting alpha particles passing through a thin gold foil. Rutherford was inspired to ask Geiger and Marsden in this experiment to look for alpha particles with very high deflection angles, of a type not expected from any theory of matter at that time. Such deflections, though rare, were found, and proved to be a smooth but high-order function of the deflection angle. It was Rutherford's interpretation of this data that led him to formulate the Rutherford model of the atom in 1911 – that a very small charged nucleus, containing much of the atom's mass, was orbited by low-mass electrons.
Before leaving Manchester in 1919 to take over the Cavendish laboratory in Cambridge, Rutherford became, in 1919, the first person to deliberately transmute one element into another. In this experiment, he had discovered peculiar radiations when alphas were projected into air, and narrowed the effect down to the nitrogen, not the oxygen in the air. Using pure nitrogen, Rutherford used alpha radiation to convert nitrogen into oxygen through the nuclear reaction N + α → O + proton. The proton was not then known. In the products of this reaction Rutherford simply identified hydrogen nuclei, by their similarity to the particle radiation from earlier experiments in which he had bombarded hydrogen gas with alpha particles to knock hydrogen nuclei out of hydrogen atoms. This result showed Rutherford that hydrogen nuclei were a part of nitrogen nuclei (and by inference, probably other nuclei as well). Such a construction had been suspected for many years on the basis of atomic weights which were whole numbers of that of hydrogen; see Prout's hypothesis. Hydrogen was known to be the lightest element, and its nuclei presumably the lightest nuclei. Now, because of all these considerations, Rutherford decided that a hydrogen nucleus was possibly a fundamental building block of all nuclei, and also possibly a new fundamental particle as well, since nothing was known from the nucleus that was lighter. Thus, Rutherford postulated the hydrogen nucleus to be a new particle in 1920, which he dubbed the "proton".
In 1921, while working with Niels Bohr (who postulated that electrons moved in specific orbits), Rutherford theorized about the existence of neutrons, (which he had christened in his 1920 Bakerian Lecture), which could somehow compensate for the repelling effect of the positive charges of protons by causing an attractive nuclear force and thus keep the nuclei from flying apart from the repulsion between protons. The only alternative to neutrons was the existence of "nuclear electrons" which would counteract some of the proton charges in the nucleus, since by then it was known that nuclei had about twice the mass that could be accounted for if they were simply assembled from hydrogen nuclei (protons). But how these nuclear electrons could be trapped in the nucleus, was a mystery.
Rutherford's theory of neutrons was proved in 1932 by his associate James Chadwick, who recognized neutrons immediately when they were produced by other scientists and later himself, in bombarding beryllium with alpha particles. In 1935, Chadwick was awarded the Nobel Prize in Physics for this discovery.
Legacy.
Nuclear physics.
Rutherford's research, and work done under him as laboratory director, established the nuclear structure of the atom and the essential nature of radioactive decay as a nuclear process. Rutherford's team, using natural alpha particles, demonstrated "induced" nuclear transmutation, and later, using protons from an accelerator, demonstrated "artificially-induced" nuclear reactions and transmutation. He is known as the father of nuclear physics. Rutherford died too early to see Leó Szilárd's idea of controlled nuclear chain reactions come into being. However, a speech of Rutherford's about his artificially-induced transmutation in lithium, printed in 12 September 1933 London paper "The Times", was reported by Szilárd to have been his inspiration for thinking of the possibility of a controlled energy-producing nuclear chain reaction. Szilard had this idea while walking in London, on the same day.
Rutherford's speech touched on the 1932 work of his students John Cockcroft and Ernest Walton in "splitting" lithium into alpha particles by bombardment with protons from a particle accelerator they had constructed. Rutherford realized that the energy released from the split lithium atoms was enormous, but he also realized that the energy needed for the accelerator, and its essential inefficiency in splitting atoms in this fashion, made the project an impossibility as a practical source of energy (accelerator-induced fission of light elements remains too inefficient to be used in this way, even today). Rutherford's speech in part, read:
Incidences of cancer at Rutherford's former laboratory.
The Coupland Building at Manchester University, at which Rutherford conducted many of his experiments, has been the subject of a cancer cluster investigation. There has been a statistically high incidence of pancreatic cancer, brain cancer, and motor neuron disease occurring in and around Rutherford's former laboratories and, since 1984, a total of six workers have been stricken with these ailments. In 2009, an independent commission concluded that the very slightly elevated levels of various radiation related to Rutherford's experiments decades earlier are not the likely cause of such cancers and ruled the illnesses a coincidence.

</doc>
<doc id="9604" url="https://en.wikipedia.org/wiki?curid=9604" title="Many-worlds interpretation">
Many-worlds interpretation

The many-worlds interpretation is an interpretation of quantum mechanics that asserts the objective reality of the universal wavefunction and denies the actuality of wavefunction collapse. Many-worlds implies that all possible alternate histories and futures are real, each representing an actual "world" (or "universe"). In lay terms, the hypothesis states there is a very large—perhaps infinite—number of universes, and everything that could possibly have happened in our past, but did not, has occurred in the past of some other universe or universes. The theory is also referred to as MWI, the relative state formulation, the Everett interpretation, the theory of the universal wavefunction, many-universes interpretation, or just many-worlds.
The original relative state formulation is due to Hugh Everett in 1957. Later, this formulation was popularized and renamed "many-worlds" by Bryce Seligman DeWitt in the 1960s and 1970s. The decoherence approaches to interpreting quantum theory have been further explored and developed, becoming quite popular. MWI is one of many multiverse hypotheses in physics and philosophy. It is currently considered a mainstream interpretation along with the other decoherence interpretations, collapse theories (including the historical Copenhagen interpretation), and hidden variable theories such as the Bohmian mechanics.
Before many-worlds, reality had always been viewed as a single unfolding history. Many-worlds, however, views reality as a many-branched tree, wherein every possible quantum outcome is realised. Many-worlds reconciles the observation of non-deterministic events, such as random radioactive decay, with the fully deterministic equations of quantum physics.
In many-worlds, the subjective appearance of wavefunction collapse is explained by the mechanism of quantum decoherence, and this is supposed to resolve all of the correlation paradoxes of quantum theory, such as the EPR paradox and Schrödinger's cat, since every possible outcome of every event defines or exists in its own "history" or "world".
Outline.
Although several versions of many-worlds have been proposed since Hugh Everett's original work, they all contain one key idea: the equations of physics that model the time evolution of systems "without" embedded observers are sufficient for modelling systems which "do" contain observers; in particular there is no observation-triggered wave function collapse which the Copenhagen interpretation proposes. Provided the theory is linear with respect to the wavefunction, the exact form of the quantum dynamics modelled, be it the non-relativistic Schrödinger equation, relativistic quantum field theory or some form of quantum gravity or string theory, does not alter the validity of MWI since MWI is a metatheory applicable to all linear quantum theories, and there is no experimental evidence for any non-linearity of the wavefunction in physics. MWI's main conclusion is that the universe (or multiverse in this context) is composed of a quantum superposition of very many, possibly even non-denumerably infinitely many, increasingly divergent, non-communicating parallel universes or quantum worlds.
The idea of MWI originated in Everett's Princeton Ph.D. thesis "The Theory of the Universal Wavefunction", developed under his thesis advisor John Archibald Wheeler, a shorter summary of which was published in 1957 entitled "Relative State Formulation of Quantum Mechanics" (Wheeler contributed the title "relative state"; Everett originally called his approach the "Correlation Interpretation", where "correlation" refers to quantum entanglement). The phrase "many-worlds" is due to Bryce DeWitt, who was responsible for the wider popularisation of Everett's theory, which had been largely ignored for the first decade after publication. DeWitt's phrase "many-worlds" has become so much more popular than Everett's "Universal Wavefunction" or Everett–Wheeler's "Relative State Formulation" that many forget that this is only a difference of terminology; the content of both of Everett's papers and DeWitt's popular article is the same.
The many-worlds interpretation shares many similarities with later, other "post-Everett" interpretations of quantum mechanics which also use decoherence to explain the process of measurement or wavefunction collapse. MWI treats the other histories or worlds as real since it regards the universal wavefunction as the "basic physical entity" or "the fundamental entity, obeying at all times a deterministic wave equation". The other decoherent interpretations, such as consistent histories, the Existential Interpretation etc., either regard the extra quantum worlds as metaphorical in some sense, or are agnostic about their reality; it is sometimes hard to distinguish between the different varieties. MWI is distinguished by two qualities: it assumes realism, which it assigns to the wavefunction, and it has the minimal formal structure possible, rejecting any hidden variables, quantum potential, any form of a collapse postulate (i.e., Copenhagenism) or mental postulates (such as the many-minds interpretation makes).
Decoherent interpretations of many-worlds using einselection to explain how a small number of classical pointer states can emerge from the enormous Hilbert space of superpositions have been proposed by Wojciech H. Zurek. "Under scrutiny of the environment, only pointer states remain unchanged. Other states decohere into mixtures of stable pointer states that can persist, and, in this sense, exist: They are einselected." These ideas complement MWI and bring the interpretation in line with our perception of reality.
Many-worlds is often referred to as a theory, rather than just an interpretation, by those who propose that many-worlds can make testable predictions (such as David Deutsch) or is falsifiable (such as Everett) or by those who propose that all the other, non-MW interpretations, are inconsistent, illogical or unscientific in their handling of measurements; Hugh Everett argued that his formulation was a metatheory, since it made statements about other interpretations of quantum theory; that it was the "only completely coherent approach to explaining both the contents of quantum mechanics and the appearance of the world." Deutsch is dismissive that many-worlds is an "interpretation", saying that calling it an interpretation "is like talking about dinosaurs as an 'interpretation' of fossil records."
Interpreting wavefunction collapse.
As with the other interpretations of quantum mechanics, the many-worlds interpretation is motivated by behavior that can be illustrated by the double-slit experiment. When particles of light (or anything else) are passed through the double slit, a calculation assuming wave-like behavior of light can be used to identify where the particles are likely to be observed. Yet when the particles are observed in this experiment, they appear as particles (i.e., at definite places) and not as non-localized waves.
Some versions of the Copenhagen interpretation of quantum mechanics proposed a process of "collapse" in which an indeterminate quantum system would probabilistically collapse down onto, or select, just one determinate outcome to "explain" this phenomenon of observation. Wavefunction collapse was widely regarded as artificial and "ad hoc", so an alternative interpretation in which the behavior of measurement could be understood from more fundamental physical principles was considered desirable.
Everett's Ph.D. work provided such an alternative interpretation. Everett stated that for a composite system – for example a subject (the "observer" or measuring apparatus) observing an object (the "observed" system, such as a particle) – the statement that either the observer or the observed has a well-defined state is meaningless; in modern parlance, the observer and the observed have become entangled; we can only specify the state of one "relative" to the other, i.e., the state of the observer and the observed are correlated "after" the observation is made. This led Everett to derive from the unitary, deterministic dynamics alone (i.e., without assuming wavefunction collapse) the notion of a "relativity of states".
Everett noticed that the unitary, deterministic dynamics alone decreed that after an observation is made each element of the quantum superposition of the combined subject–object wavefunction contains two "relative states": a "collapsed" object state and an associated observer who has observed the same collapsed outcome; what the observer sees and the state of the object have become correlated by the act of measurement or observation. The subsequent evolution of each pair of relative subject–object states proceeds with complete indifference as to the presence or absence of the other elements, "as if" wavefunction collapse has occurred, which has the consequence that later observations are always consistent with the earlier observations. Thus the "appearance" of the object's wavefunction's collapse has emerged from the unitary, deterministic theory itself. (This answered Einstein's early criticism of quantum theory, that the theory should define what is observed, not for the observables to define the theory). Since the wavefunction merely appears to have collapsed then, Everett reasoned, there was no need to actually assume that it had collapsed. And so, invoking Occam's razor, he removed the postulate of wavefunction collapse from the theory.
Probability.
A consequence of removing wavefunction collapse from the quantum formalism is that the Born rule requires derivation, since many-worlds derives its interpretation from the formalism. Attempts have been made, by many-world advocates and others, over the years to "derive" the Born rule, rather than just conventionally "assume" it, so as to reproduce all the required statistical behaviour associated with quantum mechanics. There is no consensus on whether this has been successful.
Frequency-Based Approaches.
Everett (1957) briefly derived the Born rule by showing that the Born rule was the only possible rule, and that its derivation was as justified as the procedure for defining probability in classical mechanics. Everett stopped doing research in theoretical physics shortly after obtaining his Ph.D., but his work on probability has been extended by a number of people. Andrew Gleason (1957) and James Hartle (1965) independently reproduced Everett's work which was later extended. These results are closely related to Gleason's theorem, a mathematical result according to which the Born probability measure is the only one on Hilbert space that can be constructed purely from the quantum state vector.
Bryce DeWitt and his doctoral student R. Neill Graham later provided alternative (and longer) derivations to Everett's derivation of the Born rule. They demonstrated that the norm of the worlds where the usual statistical rules of quantum theory broke down vanished, in the limit where the number of measurements went to infinity.
Decision Theory.
A decision-theoretic derivation of the Born rule from Everettarian assumptions, was produced by David Deutsch (1999) and refined by Wallace (2002–2009) and Saunders (2004). Deutsch's derivation is a two-stage proof: first he shows that the number of orthonormal Everett-worlds after a branching is proportional to the conventional probability density. Then he uses game theory to show that these are all equally likely to be observed. The last step in particular has been criticised for circularity. Some other reviews have been positive, although the status of these arguments remains highly controversial; some theoretical physicists have taken them as supporting the case for parallel universes. In the "New Scientist" article, reviewing their presentation at a September 2007 conference, Andy Albrecht, a physicist at the University of California at Davis, is quoted as saying "This work will go down as one of the most important developments in the history of science."
The Born rule and the collapse of the wave function have been obtained in the framework of the relative-state formulation of quantum mechanics by Armando V.D.B. Assis. He has proved that the Born rule and the collapse of the wave function follow from a game-theoretical strategy, namely the Nash equilibrium within a von Neumann zero-sum game between nature and observer.
Symmetries and Envariance.
Wojciech H. Zurek (2005) has produced a derivation of the Born rule, where decoherence has replaced Deutsch's informatic assumptions. Lutz Polley (2000) has produced Born rule derivations where the informatic assumptions are replaced by symmetry arguments.
Charles Sebens and Sean M. Carroll, building on work by Lev Vaidman, proposed a similar approach based on self-locating uncertainty. In this approach, decoherence creates multiple identical copies of observers, who can assign credences to being on different branches using the Born rule.
Brief overview.
In Everett's formulation, a measuring apparatus M and an object system S form a composite system, each of which prior to measurement exists in well-defined (but time-dependent) states. Measurement is regarded as causing M and S to interact. After S interacts with M, it is no longer possible to describe either system by an independent state. According to Everett, the only meaningful descriptions of each system are relative states: for example the relative state of S given the state of M or the relative state of M given the state of S. In DeWitt's formulation, the state of S after a sequence of measurements is given by a quantum superposition of states, each one corresponding to an alternative measurement history of S.
For example, consider the smallest possible truly quantum system S, as shown in the illustration. This describes for instance, the spin-state of an electron. Considering a specific axis (say the "z"-axis) the north pole represents spin "up" and the south pole, spin "down". The superposition states of the system are described by (the surface of) a sphere called the Bloch sphere. To perform a measurement on S, it is made to interact with another similar system M. After the interaction, the combined system is described by a state that ranges over a six-dimensional space (the reason for the number six is explained in the article on the Bloch sphere). This six-dimensional object can also be regarded as a quantum superposition of two "alternative histories" of the original system S, one in which "up" was observed and the other in which "down" was observed. Each subsequent binary measurement (that is interaction with a system M) causes a similar split in the history tree. Thus after three measurements, the system can be regarded as a quantum superposition of 8 = 2 × 2 × 2 copies of the original system S.
The accepted terminology is somewhat misleading because it is incorrect to regard the universe as splitting at certain times; at any given instant there is one state in one universe.
Relative state.
In his 1957 doctoral dissertation, Everett proposed that rather than modeling an isolated quantum system subject to external observation, one could mathematically model an object as well as its observers as purely physical systems within the mathematical framework developed by Paul Dirac, von Neumann and others, discarding altogether the "ad hoc" mechanism of wave function collapse. Since Everett's original work, there have appeared a number of similar formalisms in the literature. One such idea is discussed in the next section.
The relative state formulation makes two assumptions. The first is that the wavefunction is not simply a description of the object's state, but that it actually is entirely equivalent to the object, a claim it has in common with some other interpretations. The second is that observation or measurement has no special laws or mechanics, unlike in the Copenhagen interpretation which considers the wavefunction collapse as a special kind of event which occurs as a result of observation. Instead, measurement in the relative state formulation is the consequence of a configuration change in the memory of an observer described by the same basic wave physics as the object being modeled.
The many-worlds interpretation is DeWitt's popularisation of Everett's work, who had referred to the combined observer–object system as being split by an observation, each split corresponding to the different or multiple possible outcomes of an observation. These splits generate a possible tree as shown in the graphic below. Subsequently DeWitt introduced the term "world" to describe a complete measurement history of an observer, which corresponds roughly to a single branch of that tree. Note that "splitting" in this sense, is hardly new or even quantum mechanical. The idea of a space of complete alternative histories had already been used in the theory of probability since the mid-1930s for instance to model Brownian motion. 
Under the many-worlds interpretation, the Schrödinger equation, or relativistic analog, holds all the time everywhere. An observation or measurement of an object by an observer is modeled by applying the wave equation to the entire system comprising the observer "and" the object. One consequence is that every observation can be thought of as causing the combined observer–object's wavefunction to change into a quantum superposition of two or more non-interacting branches, or split into many "worlds". Since many observation-like events have happened, and are constantly happening, there are an enormous and growing number of simultaneously existing states.
If a system is composed of two or more subsystems, the system's state will be a superposition of products of the subsystems' states. Once the subsystems interact, their states are no longer independent. Each product of subsystem states in the overall superposition evolves over time independently of other products. The subsystems states have become correlated or entangled and it is no longer possible to consider them independent of one another. In Everett's terminology each subsystem state was now "correlated" with its "relative state", since each subsystem must now be considered relative to the other subsystems with which it has interacted.
Comparative properties and possible experimental tests.
One of the salient properties of the many-worlds interpretation is that it does not require an exceptional method of wave function collapse to explain it. "It seems that there is no experiment distinguishing the MWI from other no-collapse theories such as Bohmian mechanics or other variants of MWI... In most no-collapse interpretations, the evolution of the quantum state of the Universe is the same. Still, one might imagine that there is an experiment distinguishing the MWI from another no-collapse interpretation based on the difference in the correspondence between the formalism and the experience (the results of experiments)."
However, in 1985, David Deutsch published three related thought experiments which could test the theory vs the Copenhagen interpretation. The experiments require macroscopic quantum state preparation and quantum erasure by a hypothetical quantum computer which is currently outside experimental possibility. Since then Lockwood (1989), Vaidman and others have made similar proposals. These proposals also require an advanced technology which is able to place a macroscopic object in a coherent superposition, another task for which it is uncertain whether it will ever be possible. Many other controversial ideas have been put forward though, such as a recent claim that cosmological observations could test the theory, and another claim by Rainer Plaga (1997), published in "Foundations of Physics", that communication might be possible between worlds.
Copenhagen interpretation.
In the Copenhagen interpretation, the mathematics of quantum mechanics allows one to predict probabilities for the occurrence of various events. When an event occurs, it becomes part of the definite reality, and alternative possibilities do not. There is no necessity to say anything definite about what is not observed.
The universe decaying to a new vacuum state.
Any event that changes the number of observers in the universe may have experimental consequences. Quantum tunnelling to a new vacuum state would reduce the number of observers to zero (i.e., kill all life). Some cosmologists argue that the universe is in a false vacuum state and that consequently the universe should have already experienced quantum tunnelling to a true vacuum state. This has not happened and is cited as evidence in favor of many-worlds. In some worlds, quantum tunnelling to a true vacuum state has happened but most other worlds escape this tunneling and remain viable. This can be thought of as a variation on quantum suicide.
Many-minds.
The "many-minds" interpretation is a multi-world interpretation that defines the splitting of reality on the level of the observers' minds. In this, it differs from Everett's many-worlds interpretation, in which there is no special role for the observer's mind.
Reception.
There is a wide range of claims that are considered "many-worlds" interpretations. It was often claimed by those who do not believe in MWI that Everett himself was not entirely clear as to what he believed; however, MWI adherents (such as DeWitt, Tegmark, Deutsch and others) believe they fully understand Everett's meaning as implying the literal existence of the other worlds. Additionally, recent biographical sources make it clear that Everett believed in the literal reality of the other quantum worlds. Everett's son reported that Hugh Everett "never wavered in his belief over his many-worlds theory". Also Everett was reported to believe "his many-worlds theory guaranteed him immortality".
One of MWI's strongest advocates is David Deutsch. According to Deutsch, the single photon interference pattern observed in the double slit experiment can be explained by interference of photons in multiple universes. Viewed in this way, the single photon interference experiment is indistinguishable from the multiple photon interference experiment. In a more practical vein, in one of the earliest papers on quantum computing, he suggested that parallelism that results from the validity of MWI could lead to "a method by which certain probabilistic tasks can be performed faster by a universal quantum computer than by any classical restriction of it". Deutsch has also proposed that when reversible computers become conscious that MWI will be testable (at least against "naive" Copenhagenism) via the reversible observation of spin.
Asher Peres was an outspoken critic of MWI. For example, a section in his 1993 textbook had the title "Everett's interpretation and other bizarre theories". Peres not only questioned whether MWI is really an "interpretation", but rather, if "any" interpretations of quantum mechanics are needed at all. An interpretation can be regarded as a purely formal transformation, which adds nothing to the rules of the quantum mechanics. Peres seems to suggest that positing the existence of an infinite number of non-communicating parallel universes is highly suspect per those who interpret it as a violation of Occam's razor, i.e., that it does not minimize the number of hypothesized entities. However, it is understood that the number of elementary particles are not a gross violation of Occam's Razor, one counts the types, not the tokens. Max Tegmark remarks that the alternative to many-worlds is "many words", an allusion to the complexity of von Neumann's collapse postulate. On the other hand, the same derogatory qualification "many words" is often applied to MWI by its critics who see it as a word game which obfuscates rather than clarifies by confounding the von Neumann branching of possible worlds with the Schrödinger parallelism of many worlds in superposition.
MWI is considered by some to be unfalsifiable and hence unscientific because the multiple parallel universes are non-communicating, in the sense that no information can be passed between them. Others claim MWI is directly testable. Everett regarded MWI as falsifiable since any test that falsifies conventional quantum theory would also falsify MWI.
According to Martin Gardner, the "other" worlds of MWI have two different interpretations: real or unreal; he claims that Stephen Hawking and Steve Weinberg both favour the unreal interpretation. Gardner also claims that the nonreal interpretation is favoured by the majority of physicists, whereas the "realist" view is only supported by MWI experts such as Deutsch and Bryce DeWitt. Hawking has said that "according to Feynman's idea", all the other histories are as "equally real" as our own, and Martin Gardner reports Hawking saying that MWI is "trivially true". In a 1983 interview, Hawking also said he regarded the MWI as "self-evidently correct" but was dismissive towards questions about the interpretation of quantum mechanics, saying, "When I hear of Schrödinger's cat, I reach for my gun." In the same interview, he also said, "But, look: All that one does, really, is to calculate conditional probabilities—in other words, the probability of A happening, given B. I think that that's all the many worlds interpretation is. Some people overlay it with a lot of mysticism about the wave function splitting into different parts. But all that you're calculating is conditional probabilities." Elsewhere Hawking contrasted his attitude towards the "reality" of physical theories with that of his colleague Roger Penrose, saying, "He's a Platonist and I'm a positivist. He's worried that Schrödinger's cat is in a quantum state, where it is half alive and half dead. He feels that can't correspond to reality. But that doesn't bother me. I don't demand that a theory correspond to reality because I don't know what it is. Reality is not a quality you can test with litmus paper. All I'm concerned with is that the theory should predict the results of measurements. Quantum theory does this very successfully." For his own part, Penrose agrees with Hawking that QM applied to the universe implies MW, although he considers the current lack of a successful theory of quantum gravity negates the claimed universality of conventional QM.
Polls.
Advocates of MWI often cite a poll of 72 "leading cosmologists and other quantum field theorists" conducted by the American political scientist David Raub in 1995 showing 58% agreement with "Yes, I think MWI is true".
The poll is controversial: for example, Victor J. Stenger remarks that Murray Gell-Mann's published work explicitly rejects the existence of simultaneous parallel universes. Collaborating with James Hartle, Gell-Mann is working toward the development a more "palatable" "post-Everett quantum mechanics". Stenger thinks it's fair to say that most physicists dismiss the many-world interpretation as too extreme, while noting it "has merit in finding a place for the observer inside the system being analyzed and doing away with the troublesome notion of wave function collapse".
Max Tegmark also reports the result of a "highly unscientific" poll taken at a 1997 quantum mechanics workshop. According to Tegmark, "The many worlds interpretation (MWI) scored second, comfortably ahead of the consistent histories and Bohm interpretations." Such polls have been taken at other conferences, for example, in response to Sean Carroll's observation, "As crazy as it sounds, most working physicists buy into the many-worlds theory" Michael Nielsen counters: "at a quantum computing conference at Cambridge in 1998, a many-worlder surveyed the audience of approximately 200 people... Many-worlds did just fine, garnering support on a level comparable to, but somewhat below, Copenhagen and decoherence." However, Nielsen notes that it seemed most attendees found it to be a waste of time: Asher Peres "got a huge and sustained round of applause… when he got up at the end of the polling and asked 'And who here believes the laws of physics are decided by a democratic vote?'"
A 2005 poll of fewer than 40 students and researchers taken after a course on the Interpretation of Quantum Mechanics at the Institute for Quantum Computing University of Waterloo found "Many Worlds (and decoherence)" to be the least favored.
A 2011 poll of 33 participants at an Austrian conference found 6 endorsed MWI, 8 "Information-based/information-theoretical", and 14 Copenhagen; the authors remark that the results are similar to Tegmark's 1998 poll.
Speculative implications.
Speculative physics deals with questions which are also discussed in science fiction.
Quantum suicide thought experiment.
Quantum suicide, as a thought experiment, was published independently by Hans Moravec in 1987 and Bruno Marchal in 1988 and was independently developed further by Max Tegmark in 1998. It attempts to distinguish between the Copenhagen interpretation of quantum mechanics and the Everett many-worlds interpretation by means of a variation of the Schrödinger's cat thought experiment, from the cat's point of view. Quantum immortality refers to the subjective experience of surviving quantum suicide regardless of the odds.
Weak coupling.
Another speculation is that the separate worlds remain weakly coupled (e.g., by gravity) permitting "communication between parallel universes". A possible test of this using quantum-optical equipment is described in a 1997 "Foundations of Physics" article by Rainer Plaga. It involves an isolated ion in an ion trap, a quantum measurement that would yield two parallel worlds (their difference just being in the detection of a single photon), and the excitation of the ion from only one of these worlds. If the excited ion can be detected from the other parallel universe, then this would constitute direct evidence in support of the many-worlds interpretation and would automatically exclude the orthodox, "logical", and "many-histories" interpretations. The reason the ion is isolated is to make it not participate immediately in the decoherence which insulates the parallel world branches, therefore allowing it to act as a gateway between the two worlds, and if the measure apparatus could perform the measurements quickly enough before the gateway ion is decoupled then the test would succeed (with electronic computers the necessary time window between the two worlds would be in a time scale of milliseconds or nanoseconds, and if the measurements are taken by humans then a few seconds would still be enough). R. Plaga shows that macroscopic decoherence timescales are a possibility. The proposed test is based on technical equipment described in a 1993 "Physical Review" article by Itano et al. and R. Plaga says that this level of technology is enough to realize the proposed inter-world communication experiment. The necessary technology for precision measurements of single ions already exists since the 1970s, and the ion recommended for excitation is Hg. The excitation methodology is described by Itano et al. and the time needed for it is given by the Rabi flopping formula
Such a test as described by R. Plaga would mean that energy transfer is possible between parallel worlds. This does not violate the fundamental principles of physics because these require energy conservation only for the whole universe and not for the single parallel branches. Neither the excitation of the single ion (which is a degree of freedom of the proposed system) leads to decoherence, something which is proven by Welcher Weg detectors which can excite atoms without momentum transfer (which causes the loss of coherence).
The proposed test would allow for low-bandwidth inter-world communication, the limiting factors of bandwidth and time being dependent on the technology of the equipment. Because of the time needed to determine the state of the partially decohered isolated excited ion based on Itano et al.'s methodology, the ion would decohere by the time its state is determined during the experiment, so Plaga's proposal would pass just enough information between the two worlds to confirm their parallel existence and nothing more. The author contemplates that with increased bandwidth, one could even transfer television imagery across the parallel worlds. For example, Itano et al.'s methodology could be improved (by lowering the time needed for state determination of the excited ion) if a more efficient process were found for the detection of fluorescence radiation using 194 nm photons.
A 1991 article by J.Polchinski also supports the view that inter-world communication is a theoretical possibility. Other authors in a 1994 preprint article also contemplated similar ideas.
The reason inter-world communication seems like a possibility is because decoherence which separates the parallel worlds is never fully complete, therefore weak influences from one parallel world to another can still pass between them, and these should be measurable with advanced technology. Deutsch proposed such an experiment in a 1985 "International Journal of Theoretical Physics" article, but the technology it requires involves human-level artificial intelligence.
Similarity to modal realism.
The many-worlds interpretation has some similarity to modal realism in philosophy, which is the view that the possible worlds used to interpret modal claims exist and are of a kind with the actual world. Unlike the possible worlds of philosophy, however, in quantum mechanics counterfactual alternatives can influence the results of experiments, as in the Elitzur–Vaidman bomb-testing problem or the Quantum Zeno effect. Also, while the worlds of the many-worlds interpretation all share the same physical laws, modal realism postulates a world for every way things could conceivably have been.
Time travel.
The many-worlds interpretation could be one possible way to resolve the paradoxes that one would expect to arise "if" time travel turns out to be permitted by physics (permitting closed timelike curves and thus violating causality). Entering the past would itself be a quantum event causing branching, and therefore the timeline accessed by the time traveller simply would be another timeline of many. In that sense, it would make the Novikov self-consistency principle unnecessary.
Many-worlds in literature and science fiction.
The many-worlds interpretation (and the somewhat related concept of possible worlds) has been associated to numerous themes in literature, art and science fiction.
Some of these stories or films violate fundamental principles of causality and relativity, and are extremely misleading since the information-theoretic structure of the path space of multiple universes (that is information flow between different paths) is very likely extraordinarily complex. Also see Michael Clive Price's FAQ referenced in the external links section below where these issues (and other similar ones) are dealt with more decisively.
Another kind of popular illustration of many-worlds splittings, which does not involve information flow between paths, or information flow backwards in time considers alternate outcomes of historical events. According to the many-worlds interpretation, all of the historical speculations entertained within the alternate history genre are realized in parallel universes.
The many-worlds interpretation of reality was anticipated with remarkable fidelity in Olaf Stapledon's 1937 science fiction novel Star Maker, in a paragraph describing one of the many universes created by the Star Maker god of the title. "In one inconceivably complex cosmos, whenever a creature was faced with several possible courses of action, it took them all, thereby creating many distinct temporal dimensions and distinct histories of the cosmos. Since in every evolutionary sequence of the cosmos there were very many creatures, and each was constantly faced with many possible courses, and the combinations of all their courses were innumerable, an infinity of distinct universes exfoliated from every moment of every temporal sequence in this cosmos."

</doc>
<doc id="9611" url="https://en.wikipedia.org/wiki?curid=9611" title="E-commerce">
E-commerce

Electronic commerce, commonly written as e-commerce, is the trading or facilitation of trading in products or services using computer networks, such as the Internet. Electronic commerce draws on technologies such as mobile commerce, electronic funds transfer, supply chain management, Internet marketing, online transaction processing, electronic data interchange (EDI), inventory management systems, and automated data collection systems. Modern electronic commerce typically uses the World Wide Web for at least one part of the transaction's life cycle, although it may also use other technologies such as e-mail.
E-commerce businesses may employ some or all of the following:
Timeline.
A timeline for the development of e-commerce:
Business application.
Some common applications related to electronic commerce are:
Governmental regulation.
In the United States, some electronic commerce activities are regulated by the Federal Trade Commission (FTC). These activities include the use of commercial e-mails, online advertising and consumer privacy. The CAN-SPAM Act of 2003 establishes national standards for direct marketing over e-mail. The Federal Trade Commission Act regulates all forms of advertising, including online advertising, and states that advertising must be truthful and non-deceptive. Using its authority under Section 5 of the FTC Act, which prohibits unfair or deceptive practices, the FTC has brought a number of cases to enforce the promises in corporate privacy statements, including promises about the security of consumers' personal information. As result, any corporate privacy policy related to e-commerce activity may be subject to enforcement by the FTC.
The Ryan Haight Online Pharmacy Consumer Protection Act of 2008, which came into law in 2008, amends the Controlled Substances Act to address online pharmacies.
Conflict of laws in cyberspace is a major hurdle for harmonization of legal framework for e-commerce around the world. In order to give a uniformity to e-commerce law around the world, many countries adopted the UNCITRAL Model Law on Electronic Commerce (1996) <ref name=" http://www.uncitral.org/uncitral/en/uncitral_texts/electronic_commerce/1996Model.html"></ref>
Internationally there is the International Consumer Protection and Enforcement Network (ICPEN), which was formed in 1991 from an informal network of government customer fair trade organisations. The purpose was stated as being to find ways of co-operating on tackling consumer problems connected with cross-border transactions in both goods and services, and to help ensure exchanges of information among the participants for mutual benefit and understanding. From this came Econsumer.gov, an ICPEN initiative since April 2001. It is a portal to report complaints about online and related transactions with foreign companies.
There is also Asia Pacific Economic Cooperation (APEC) was established in 1989 with the vision of achieving stability, security and prosperity for the region through free and open trade and investment. APEC has an Electronic Commerce Steering Group as well as working on common privacy regulations throughout the APEC region.
In Australia, Trade is covered under Australian Treasury Guidelines for electronic commerce, and the Australian Competition and Consumer Commission regulates and offers advice on how to deal with businesses online, and offers specific advice on what happens if things go wrong.
In the United Kingdom, The Financial Services Authority (FSA) was formerly the regulating authority for most aspects of the EU's Payment Services Directive (PSD), until its replacement in 2013 by the Prudential Regulation Authority and the Financial Conduct Authority. The UK implemented the PSD through the Payment Services Regulations 2009 (PSRs), which came into effect on 1 November 2009. The PSR affects firms providing payment services and their customers. These firms include banks, non-bank credit card issuers and non-bank merchant acquirers, e-money issuers, etc. The PSRs created a new class of regulated firms known as payment institutions (PIs), who are subject to prudential requirements. Article 87 of the PSD requires the European Commission to report on the implementation and impact of the PSD by 1 November 2012.
In India, the Information Technology Act 2000 governs the basic applicability of e-commerce.
In China, the Telecommunications Regulations of the People's Republic of China (promulgated on 25 September 2000), stipulated the Ministry of Industry and Information Technology (MIIT) as the government department regulating all telecommunications related activities, including electronic commerce. On the same day, The Administrative Measures on Internet Information Services released, is the first administrative regulation to address profit-generating activities conducted through the Internet, and lay the foundation for future regulations governing e-commerce in China. On 28 August 2004, the eleventh session of the tenth NPC Standing Committee adopted The Electronic Signature Law, which regulates data message, electronic signature authentication and legal liability issues. It is considered the first law in China’s e-commerce legislation. It was a milestone in the course of improving China’s electronic commerce legislation, and also marks the entering of China’s rapid development stage for electronic commerce legislation.
Forms.
Contemporary electronic commerce involves everything from ordering "digital" content for immediate online consumption, to ordering conventional goods and services, to "meta" services to facilitate other types of electronic commerce.
On the institutional level, big corporations and financial institutions use the internet to exchange financial data to facilitate domestic and international business. Data integrity and security are pressing issues for electronic commerce.
Aside from traditional e-Commerce, the terms m-Commerce (mobile commerce) as well (around 2013) t-Commerce have also been used.
Global trends.
In 2010, the United Kingdom had the biggest e-commerce market in the world when measured by the amount spent per capita. As of 2013, the Czech Republic was the European country where ecommerce delivers the biggest contribution to the enterprises´ total revenue. Almost a quarter (24%) of the country’s total turnover is generated via the online channel.
Among emerging economies, China's e-commerce presence continues to expand every year. With 668 million internet users, China's online shopping sales reached $253billion in first half of 2015, according for 10% of total Chinese consumer retail sales in the same period. The Chinese retailers have been able to help consumers feel more comfortable shopping online. E-commerce transactions between China and other countries increased 32% to 2.3 trillion yuan ($375.8 billion) in 2012 and accounted for 9.6% of China's total international trade In 2013, Alibaba had an e-commerce market share of 80% in China.
In 2013, Brazil's eCommerce was growing quickly with retail eCommerce sales expected to grow at a healthy double-digit pace through 2014. By 2016, eMarketer expected retail ecommerce sales in Brazil to reach $17.3 billion. India has an internet user base of about 243.2 million as of January 2014. Despite being third largest user base in world, the penetration of Internet is low compared to markets like the United States, United Kingdom or France but is growing at a much faster rate, adding around 6 million new entrants every month. In India, cash on delivery is the most preferred payment method, accumulating 75% of the e-retail activities.
E-Commerce has become an important tool for small and large businesses worldwide, not only to sell to customers, but also to engage them.
In 2012, ecommerce sales topped $1 trillion for the first time in history.
Mobile devices are playing an increasing role in the mix of eCommerce, this is also commonly called mobile commerce, or m-commerce. In 2014, one estimate saw purchases made on mobile devices making up 25% of the market by 2017.
In 2014, there were 600 million Internet users in China (twice as many than in the US), making it the world's biggest online market.
For traditional businesses, one research stated that information technology and cross-border e-commerce is a good opportunity for the rapid development and growth of enterprises. Many companies have invested enormous volume of investment in mobile applications.The DeLone and McLean Model stated that 3 perspectives are contributed to a successful e-business, including information system quality, service quality and users satisfaction. There is no limit of time and space, there are more opportunities to reach out to customers around the world, and to cut down unnecessary intermediate links, thereby reducing the cost price, and can benefit from one on one large customer data analysis, to achieve a high degree of personal customization strategic plan, in order to fully enhance the core competitiveness of the products in company
Impact on markets and retailers.
Economists have theorized that e-commerce ought to lead to intensified price competition, as it increases consumers' ability to gather information about products and prices. Research by four economists at the University of Chicago has found that the growth of online shopping has also affected industry structure in two areas that have seen significant growth in e-commerce, bookshops and travel agencies. Generally, larger firms are able to use economies of scale and offer lower prices. The lone exception to this pattern has been the very smallest category of bookseller, shops with between one and four employees, which appear to have withstood the trend. Depending on the category, e-commerce may shift the switching costs—procedural, relational, and financial—experienced by customers.
Individual or business involved in e-commerce whether buyers or sellers rely on Internet-based technology in order to accomplish their transactions. E-commerce is recognized for its ability to allow business to communicate and to form transaction anytime and anyplace. Whether an individual is in the US or overseas, business can be conducted through the internet. The power of e-commerce allows geophysical barriers to disappear, making all consumers and businesses on earth potential customers and suppliers. Thus, switching barriers and switching costs my shift. eBay is a good example of e-commerce business individuals and businesses are able to post their items and sell them around the Globe.
In e-commerce activities, supply chain and logistics are two most crucial factors need to be considered. Typically, cross-border logistics need about few weeks time round. Based on this low efficiency of the supply chain service, customer satisfaction will be greatly reduced. Some researcher stated that combining e-commerce competence and IT setup could well enhance company’s overall business worth. Other researcher stated that e-commerce need to consider the establishment of warehouse centers in foreign countries, to create high efficiency of the logistics system, not only improve customers’ satisfaction, but also can improve customers’ loyalty..
Some researcher investigated that if a company want to enhance international customers’ satisfaction, where cultural website need to be adapted in particular country, rather than solely depending on its local country. However, according to this research findings, the researcher found that German company had treated its international website as the same local model, such as in UK and US online marketing. A company could save money and make decision quickly via the identical strategy in different country. However, opportunity cost could be occurred, if the local strategy does not match to a new market, the company could lose its potential customer.
Impact on supply chain management.
For a long time, companies had been troubled by the gap between the benefits which supply chain technology has and the solutions to deliver those benefits. However, the emergence of e-commerce has provided a more practical and effective way of delivering the benefits of the new supply chain technologies.
E-commerce has the capability to integrate all inter-company and intra-company functions, meaning that the three flows (physical flow, financial flow and information flow) of the supply chain could be also affected by e-commerce. The affections on physical flows improved the way of product and inventory movement level for companies. For the information flows, e-commerce optimised the capacity of information processing than companies used to have, and for the financial flows, e-commerce allows companies to have more efficient payment and settlement solutions.
In addition, e-commerce has a more sophisticated level of impact on supply chains: Firstly, the performance gap will be eliminated since companies can identify gaps between different levels of supply chains by electronic means of solutions; Secondly, as a result of e-commerce emergence, new capabilities such implementing ERP systems have helped companies to manage operations with customers and suppliers. Yet these new capabilities are still not fully exploited. Thirdly, technology companies would keep investing on new e-commerce software solutions as they are expecting investment return. Fourthly, e-commerce would help to solve many aspects of issues that companies may feel difficult to cope with, such as political barriers or cross-country changes. Finally, e-commerce provides companies a more efficient and effective way to collaborate with each other within the supply chain.
The social impact of e-commerce.
Along with the e-commerce and its unique charm that has appeared gradually, virtual enterprise, virtual bank, network marketing, online shopping, payment and advertising, such this new vocabulary which is unheard-of and now has become as familiar to people. This reflects that the e-commerce has huge impact on the economy and society from the other side. For instance, B2B is a rapidly growing business in the world that leads to lower cost and then improves the economic efficiency and also bring along the growth of employment.
To understand how the e-commerce has affected the society and economy, this article will mention three issues below:
1. The e-commerce has changed the relative importance of time, but as the pillars of indicator of the country’s economic state that the importance of time should not be ignored.
2. The e-commerce offers the consumer or enterprise various information they need, making information into total transparency, will force enterprise no longer is able to use the mode of space or advertisement to raise their competitive edge. Moreover, in theory, perfect competition between the consumer sovereignty and industry will maximize social welfare.
3. In fact, during the economic activity in the past, large enterprise frequently has advantage of information resource, and thus at the expense of consumers. Nowadays, the transparent and real-time information protects the rights of consumers, because the consumers can use internet to pick out the portfolio to the benefit of themselves. The competitiveness of enterprises will be much more obvious than before, consequently, social welfare would be improved by the development of the e-commerce.
4. The new economy led by the e-commerce change humanistic spirit as well, but above all, is the employee loyalty. Due to the market with competition, the employee’s level of professionalism becomes the crucial for enterprise in the niche market. The enterprises must pay attention to how to build up the enterprises inner culture and a set of interactive mechanisms and it is the prime problem for them. Furthermore, though the mode of e-commerce decrease the information cost and transaction cost, however, its development also makes human being are overly computer literate. In hence, emphasized more humanistic attitude to work is another project for enterprise to development. Life is the root of all and high technology are merely an assistive tool to support our quality of life.
The e-commerce is not a kind of new industry, but it is creating a new economic model. Most of people agree that the e-commerce indeed to be important and significant for economic society in the future, but actually that is a bit of clueless feeling at the beginning, this problem is exactly prove the e-commerce is a sort of incorporeal revolution. Generally speaking, as a type of business active procedure, the e-commerce is going to leading an unprecedented revolution in the world, the influence of this model far exceeded the commercial affair itself. Except the mentioned above, in the area of law, education, culture and also policy, the e-commerce will continue that rise in impact. The e-commerce is truly to take human beings into the information society.
Distribution channels.
E-commerce has grown in importance as companies have adopted pure-click and brick-and-click channel systems. We can distinguish pure-click and brick-and-click channel system adopted by companies.
Examples of new e-commerce systems.
According to eMarketer research company, "by 2017, 65.8 per cent of Britons will use smartphones".

</doc>
<doc id="9613" url="https://en.wikipedia.org/wiki?curid=9613" title="Euler's formula">
Euler's formula

Euler's formula, named after Leonhard Euler, is a mathematical formula in complex analysis that establishes the fundamental relationship between the trigonometric functions and the complex exponential function. Euler's formula states that, for any real number :
where is the base of the natural logarithm, is the imaginary unit, and and are the trigonometric functions cosine and sine respectively, with the argument "x" given in radians. This complex exponential function is sometimes denoted ("cosine plus i sine"). The formula is still valid if is a complex number, and so some authors refer to the more general complex version as Euler's formula.
Euler's formula is ubiquitous in mathematics, physics, and engineering. The physicist Richard Feynman called the equation "our jewel" and "the most remarkable formula in mathematics."
History.
Johann Bernoulli noted that
formula_2
And since
formula_3
the above equation tells us something about complex logarithms. Bernoulli, however, did not evaluate the integral.
Bernoulli's correspondence with Euler (who also knew the above equation) shows that Bernoulli did not fully understand complex logarithms. Euler also suggested that the complex logarithms can have infinitely many values.
Meanwhile, Roger Cotes, in 1714, discovered that
formula_4
(formula_5 is the natural logarithm).
Cotes missed the fact that a complex logarithm can have infinitely many values, differing by multiples of , due to the periodicity of the trigonometric functions.
Around 1740 Euler turned his attention to the exponential function instead of logarithms, and obtained the formula used today that is named after him. It was published in 1748, obtained by comparing the series expansions of the exponential and trigonometric expressions.
None of these mathematicians saw the geometrical interpretation of the formula; the view of complex numbers as points in the complex plane was described some 50 years later by Caspar Wessel.
Applications in complex number theory.
This formula can be interpreted as saying that the function is a unit complex number, i.e., traces out the unit circle in the complex plane as ranges through the real numbers. Here, is the angle that a line connecting the origin with a point on the unit circle makes with the positive real axis, measured counter clockwise and in radians.
The original proof is based on the Taylor series expansions of the exponential function (where is a complex number) and of and for real numbers (see below). In fact, the same proof shows that Euler's formula is even valid for all "complex" numbers .
A point in the complex plane can be represented by a complex number written in
cartesian coordinates. Euler's formula provides a means of conversion between cartesian coordinates and polar coordinates. The polar form simplifies the mathematics when used in multiplication or powers of complex numbers. Any complex number , and its complex conjugate, , can be written as
where
Now, taking this derived formula, we can use Euler's formula to define the logarithm of a complex number. To do this, we also use the definition of the logarithm (as the inverse operator of exponentiation) that
and that
both valid for any complex numbers "a" and "b".
Therefore, one can write:
for any "z" ≠ 0. Taking the logarithm of both sides shows that:
and in fact this can be used as the definition for the complex logarithm. The logarithm of a complex number is thus a multi-valued function, because is multi-valued.
Finally, the other exponential law
which can be seen to hold for all integers "k", together with Euler's formula, implies several trigonometric identities as well as de Moivre's formula.
Relationship to trigonometry.
Euler's formula provides a powerful connection between analysis and trigonometry, and provides an interpretation of the sine and cosine functions as weighted sums of the exponential function:
The two equations above can be derived by adding or subtracting Euler's formulas:
and solving for either cosine or sine.
These formulas can even serve as the definition of the trigonometric functions for complex arguments "x". For example, letting "x" = "iy", we have:
Complex exponentials can simplify trigonometry, because they are easier to manipulate than their sinusoidal components. One technique is simply to convert sinusoids into equivalent expressions in terms of exponentials. After the manipulations, the simplified result is still real-valued. For example:
Another technique is to represent the sinusoids in terms of the real part of a complex expression, and perform the manipulations on the complex expression. For example:
This formula is used for recursive generation of cos("nx") for integer values of "n" and arbitrary "x" (in radians).
See also Phasor arithmetic.
Topological interpretation.
In the language of topology, Euler's formula states that the imaginary exponential function formula_21 is a (surjective) morphism of topological groups from the real line to the unit circle formula_22. In fact, this exhibits as a covering space of formula_22. Similarly, Euler's identity says that the kernel of this map is formula_24, where formula_25. These observations may be combined and summarized in the commutative diagram below:
Other applications.
In differential equations, the function is often used to simplify derivations, even if the final answer is a real function involving sine and cosine. The reason for this is that the complex exponential is the eigenfunction of differentiation. Euler's identity is an easy consequence of Euler's formula.
In electronic engineering and other fields, signals that vary periodically over time are often described as a combination of sine and cosine functions (see Fourier analysis), and these are more conveniently expressed as the real part of exponential functions with imaginary exponents, using Euler's formula. Also, phasor analysis of circuits can include Euler's formula to represent the impedance of a capacitor or an inductor.
Definitions of complex exponentiation.
The exponential function for real values of may be defined in a few different equivalent ways (see Characterizations of the exponential function). Several of these methods may be directly extended to give definitions of for complex values of simply by substituting in place of and using the complex algebraic operations. In particular we may use either of the two following definitions which are equivalent. From a more advanced perspective, each of these definitions may be interpreted as giving the unique analytic continuation of to the complex plane.
Power series definition.
For complex 
Using the ratio test it is possible to show that this power series has an infinite radius of convergence, and so defines for all complex .
Limit definition.
For complex 
Proofs.
Various proofs of the formula are possible.
Using power series.
Here is a proof of Euler's formula using power series expansions
as well as basic facts about the powers of "i":
and so on. Using now the power series definition from above we see that for real values of "x"
In the last step we have simply recognized the Maclaurin series for "cos(x)" and "sin(x)". The rearrangement of terms is justified because each series is absolutely convergent.
Using calculus.
Another proof is based on the fact that all complex numbers can be expressed in polar coordinates. Therefore for some and depending on ,
Now from any of the definitions of the exponential function it can be shown that the derivative of is . Therefore, differentiating both sides gives
Substituting formula_32 for formula_33 and equating real and imaginary parts in this formula gives formula_34 and formula_35. Together with the initial values formula_36 and formula_37 which come from formula_38 this gives formula_39 and formula_40. This proves the formula formula_41.

</doc>
<doc id="9615" url="https://en.wikipedia.org/wiki?curid=9615" title="Édouard Manet">
Édouard Manet

Édouard Manet ( or ; ; 23 January 1832 – 30 April 1883) was a French painter. He was one of the first 19th-century artists to paint modern life, and a pivotal figure in the transition from Realism to Impressionism.
His early masterworks, "The Luncheon on the Grass (Le déjeuner sur l'herbe)" and "Olympia", both 1863, caused great controversy and served as rallying points for the young painters who would create Impressionism. Today, these are considered watershed paintings that mark the genesis of modern art.
Biography.
Born into an upper-class household with strong political connections, Manet rejected the future originally envisioned for him, and became engrossed in the world of painting. He married Suzanne Leenhoff in 1863. The last 20 years of Manet's life saw him form bonds with other great artists of the time, and develop his own style that would be heralded as innovative and serve as a major influence for future painters.
Early life.
Édouard Manet was born in Paris on 23 January 1832, in the ancestral hôtel particulier (mansion) on the rue Bonaparte to an affluent and well-connected family. His mother, Eugénie-Desirée Fournier, was the daughter of a diplomat and goddaughter of the Swedish crown prince Charles Bernadotte, from whom the Swedish monarchs are descended. His father, Auguste Manet, was a French judge who expected Édouard to pursue a career in law. His uncle, Edmond Fournier, encouraged him to pursue painting and took young Manet to the Louvre. In 1841 he enrolled at secondary school, the Collège Rollin. In 1845, at the advice of his uncle, Manet enrolled in a special course of drawing where he met Antonin Proust, future Minister of Fine Arts and subsequent lifelong friend.
At his father's suggestion, in 1848 he sailed on a training vessel to Rio de Janeiro. After he twice failed the examination to join the Navy, his father relented to his wishes to pursue an art education. From 1850 to 1856, Manet studied under the academic painter Thomas Couture. In his spare time, Manet copied the Old Masters in the Louvre.
From 1853 to 1856, Manet visited Germany, Italy, and the Netherlands, during which time he was influenced by the Dutch painter Frans Hals, and the Spanish artists Diego Velázquez and Francisco José de Goya.
In 1856, Manet opened a studio. His style in this period was characterized by loose brush strokes, simplification of details and the suppression of transitional tones. Adopting the current style of realism initiated by Gustave Courbet, he painted "The Absinthe Drinker" (1858–59) and other contemporary subjects such as beggars, singers, Gypsies, people in cafés, and bullfights. After his early career, he rarely painted religious, mythological, or historical subjects; examples include his "Christ Mocked", now in the Art Institute of Chicago, and "Christ with Angels", in the Metropolitan Museum of Art, New York. Manet had two canvases accepted at the Salon in 1861. A portrait of his mother and father, who at the time was paralysed and robbed of speech by a stroke, was ill received by critics. The other, "The Spanish Singer", was admired by Theophile Gautier, and placed in a more conspicuous location as a result of its popularity with Salon-goers. Manet's work, which appeared "slightly slapdash" when compared with the meticulous style of so many other Salon paintings, intrigued some young artists. "The Spanish Singer", painted in a "strange new fashion caused many painters' eyes to open and their jaws to drop."
"Music in the Tuileries".
"Music in the Tuileries" is an early example of Manet's painterly style. Inspired by Hals and Velázquez, it is a harbinger of his lifelong interest in the subject of leisure.
While the picture was regarded as unfinished by some, the suggested atmosphere imparts a sense of what the Tuileries gardens were like at the time; one may imagine the music and conversation.
Here, Manet has depicted his friends, artists, authors, and musicians who take part, and he has included a self-portrait among the subjects.
"Luncheon on the Grass "("Le déjeuner sur l'herbe").
A major early work is "The Luncheon on the Grass (Le déjeuner sur l'herbe)". The Paris Salon rejected it for exhibition in 1863 but Manet exhibited it at the Salon des Refusés (Salon of the Rejected) later in the year. Emperor Napoleon III had initiated The Salon des Refusés after the Paris Salon rejected 2,783 paintings in 1863. Manet employed model Victorine Meurent, his wife Suzanne, future brother-in-law Ferdinand Leenhoff, and one of his brothers to pose. Meurent also posed for several more of Manet's important paintings including "Olympia"; and by the mid-1870s she became an accomplished painter in her own right.
The painting's juxtaposition of fully dressed men and a nude woman was controversial, as was its abbreviated, sketch-like handling, an innovation that distinguished Manet from Courbet. At the same time, Manet's composition reveals his study of the old masters, as the disposition of the main figures is derived from Marcantonio Raimondi's engraving of the "Judgement of Paris" (c. 1515) based on a drawing by Raphael.
Two additional works cited by scholars as important precedents for "Le déjeuner sur l'herbe" are "Pastoral Concert" (c. 1510, The Louvre) and "The Tempest" (Gallerie dell'Accademia, Venice), both of which are attributed variously to Italian Renaissance masters Giorgione or Titian. "The Tempest" is an enigmatic painting featuring a fully dressed man and a nude woman in a rural setting. The man is standing to the left and gazing to the side, apparently at the woman, who is seated and breastfeeding a baby; the relationship between the two figures is unclear. In "Pastoral Concert", two clothed men and a nude woman are seated on the grass, engaged in music making, while a second nude woman stands beside them.
"Olympia".
As he had in "Luncheon on the Grass", Manet again paraphrased a respected work by a Renaissance artist in the painting "Olympia" (1863), a nude portrayed in a style reminiscent of early studio photographs, but whose pose was based on Titian's "Venus of Urbino" (1538). The painting is also reminiscent of Francisco Goya's painting "The Nude Maja" (1800).
Manet embarked on the canvas after being challenged to give the Salon a nude painting to display. His uniquely frank depiction of a self-assured prostitute was accepted by the Paris Salon in 1865, where it created a scandal. According to Antonin Proust, "only the precautions taken by the administration prevented the painting being punctured and torn" by offended viewers. The painting was controversial partly because the nude is wearing some small items of clothing such as an orchid in her hair, a bracelet, a ribbon around her neck, and mule slippers, all of which accentuated her nakedness, sexuality, and comfortable courtesan lifestyle. The orchid, upswept hair, black cat, and bouquet of flowers were all recognized symbols of sexuality at the time. This modern Venus' body is thin, counter to prevailing standards; the painting's lack of idealism rankled viewers. The painting's flatness, inspired by Japanese wood block art, serves to make the nude more human and less voluptuous. A fully dressed black servant is featured, exploiting the then-current theory that black people were hyper-sexed. That she is wearing the clothing of a servant to a courtesan here furthers the sexual tension of the piece.
Olympia's body as well as her gaze is unabashedly confrontational. She defiantly looks out as her servant offers flowers from one of her male suitors. Although her hand rests on her leg, hiding her pubic area, the reference to traditional female virtue is ironic; a notion of modesty is notoriously absent in this work. A contemporary critic denounced Olympia's "shamelessly flexed" left hand, which seemed to him a mockery of the relaxed, shielding hand of Titian's Venus. Likewise, the alert black cat at the foot of the bed strikes a sexually rebellious note in contrast to that of the sleeping dog in Titian's portrayal of the goddess in his Venus of Urbino.
"Olympia" was the subject of caricatures in the popular press, but was championed by the French avant-garde community, and the painting's significance was appreciated by artists such as Gustave Courbet, Paul Cézanne, Claude Monet, and later Paul Gauguin.
As with "Luncheon on the Grass", the painting raised the issue of prostitution within contemporary France and the roles of women within society.
Life and times.
The roughly painted style and photographic lighting in these works was seen as specifically modern, and as a challenge to the Renaissance works Manet copied or used as source material. His work is considered 'early modern', partially because of the black outlining of figures, which draws attention to the surface of the picture plane and the material quality of paint.
He became friends with the Impressionists Edgar Degas, Claude Monet, Pierre-Auguste Renoir, Alfred Sisley, Paul Cézanne and Camille Pissarro through another painter, Berthe Morisot, who was a member of the group and drew him into their activities. The grand niece of the painter Jean-Honoré Fragonard, Morisot had her first painting accepted in the Salon de Paris in 1864, and she continued to show in the salon for the next ten years.
Manet became the friend and colleague of Berthe Morisot in 1868. She is credited with convincing Manet to attempt plein air painting, which she had been practicing since she was introduced to it by another friend of hers, Camille Corot. They had a reciprocating relationship and Manet incorporated some of her techniques into his paintings. In 1874, she became his sister-in-law when she married his brother, Eugène.
Unlike the core Impressionist group, Manet maintained that modern artists should seek to exhibit at the Paris Salon rather than abandon it in favor of independent exhibitions. Nevertheless, when Manet was excluded from the International Exhibition of 1867, he set up his own exhibition. His mother worried that he would waste all his inheritance on this project, which was enormously expensive. While the exhibition earned poor reviews from the major critics, it also provided his first contacts with several future Impressionist painters, including Degas.
Although his own work influenced and anticipated the Impressionist style, he resisted involvement in Impressionist exhibitions, partly because he did not wish to be seen as the representative of a group identity, and partly because he preferred to exhibit at the Salon. Eva Gonzalès was his only formal student.
He was influenced by the Impressionists, especially Monet and Morisot. Their influence is seen in Manet's use of lighter colors, but he retained his distinctive use of black, uncharacteristic of Impressionist painting. He painted many outdoor (plein air) pieces, but always returned to what he considered the serious work of the studio.
Manet enjoyed a close friendship with composer Emmanuel Chabrier, painting two portraits of him; the musician owned 14 of Manet's paintings and dedicated his "Impromptu" to Manet's wife.
Throughout his life, although resisted by art critics, Manet could number as his champions Émile Zola, who supported him publicly in the press, Stéphane Mallarmé, and Charles Baudelaire, who challenged him to depict life as it was. Manet, in turn, drew or painted each of them.
Cafe scenes.
Manet's paintings of café scenes are observations of social life in 19th-century Paris. People are depicted drinking beer, listening to music, flirting, reading, or waiting. Many of these paintings were based on sketches executed on the spot. He often visited the Brasserie Reichshoffen on boulevard de Rochechourt, upon which he based "At the Cafe" in 1878. Several people are at the bar, and one woman confronts the viewer while others wait to be served. Such depictions represent the painted journal of a flâneur. These are painted in a style which is loose, referencing Hals and Velázquez, yet they capture the mood and feeling of Parisian night life. They are painted snapshots of bohemianism, urban working people, as well as some of the bourgeoisie.
In "Corner of a Cafe Concert", a man smokes while behind him a waitress serves drinks. In "The Beer Drinkers" a woman enjoys her beer in the company of a friend. In "The Cafe Concert", shown at right, a sophisticated gentleman sits at a bar while a waitress stands resolutely in the background, sipping her drink. In "The Waitress", a serving woman pauses for a moment behind a seated customer smoking a pipe, while a ballet dancer, with arms extended as she is about to turn, is on stage in the background.
Manet also sat at the restaurant on the Avenue de Clichy called Pere Lathuille's, which had a garden in addition to the dining area. One of the paintings he produced here was "Chez le père Lathuille" (At Pere Lathuille's), in which a man displays an unrequited interest in a woman dining near him.
In "Le Bon Bock" (1873), a large, cheerful, bearded man sits with a pipe in one hand and a glass of beer in the other, looking straight at the viewer.
Paintings of social activities.
Manet painted the upper class enjoying more formal social activities. In "Masked Ball at the Opera", Manet shows a lively crowd of people enjoying a party. Men stand with top hats and long black suits while talking to women with masks and costumes. He included portraits of his friends in this picture.
His 1868 painting "The Luncheon" was posed in the dining room of the Manet house.
Manet depicted other popular activities in his work. In "The Races at Longchamp", an unusual perspective is employed to underscore the furious energy of racehorses as they rush toward the viewer. In "Skating", Manet shows a well dressed woman in the foreground, while others skate behind her. Always there is the sense of active urban life continuing behind the subject, extending outside the frame of the canvas.
In "View of the International Exhibition", soldiers relax, seated and standing, prosperous couples are talking. There is a gardener, a boy with a dog, a woman on horseback—in short, a sample of the classes and ages of the people of Paris.
War.
Manet's response to modern life included works devoted to war, in subjects that may be seen as updated interpretations of the genre of "history painting". The first such work was the "Battle of the Kearsarge and Alabama" (1864), a sea skirmish known as the "Battle of Cherbourg" from the American Civil War which took place off the French coast, and may have been witnessed by the artist.
Of interest next was the French intervention in Mexico; from 1867 to 1869 Manet painted three versions of the "Execution of Emperor Maximilian", an event which raised concerns regarding French foreign and domestic policy. The several versions of the "Execution" are among Manet's largest paintings, which suggests that the theme was one which the painter regarded as most important. Its subject is the execution by Mexican firing squad of a Habsburg emperor who had been installed by Napoleon III. Neither the paintings nor a lithograph of the subject were permitted to be shown in France. As an indictment of formalized slaughter the paintings look back to Goya, and anticipate Picasso's "Guernica".
In January 1871, Manet traveled to Oloron-Sainte-Marie in the Pyrenees. In his absence his friends added his name to the "Fédération des artistes" (see: Courbet) of the Paris Commune. Manet stayed away from Paris, perhaps, until after the "semaine sanglante": in a letter to Berthe Morisot at Cherbourg (10 June 1871) he writes, "We came back to Paris a few days ago..." (the semaine sanglante ended on 28 May).
The Prints and Drawings Collection of the Museum of Fine Arts (Budapest) has a watercolour/gouache ("The Barricade") by Manet, depicting a summary execution of Communards by Versailles troops based on a lithograph of the execution of Maximilian. A similar piece ("The Barricade"), oil on plywood, is held by a private collector.
On 18 March 1871, he wrote to his (confederate) friend Félix Bracquemond in Paris about his visit to Bordeaux, the provisory seat of the French National Assembly of the Third French Republic where Émile Zola introduced him to the sites: "I never imagined that France could be represented by such doddering old fools, not excepting that little twit Thiers..." If this could be interpreted as support of the Commune, a following letter to Bracquemond (21 March 1871) expressed his idea more clearly: "Only party hacks and the ambitious, the Henrys of this world following on the heels of the Milliéres, the grotesque imitators of the Commune of 1793..." He knew the communard Lucien Henry to have been a former painter's model and Millière, an insurance agent. "What an encouragement all these bloodthirsty caperings are for the arts! But there is at least one consolation in our misfortunes: that we're not politicians and have no desire to be elected as deputies".
Paris.
Manet depicted many scenes of the streets of Paris in his works. The "Rue Mosnier Decked with Flags" depicts red, white, and blue pennants covering buildings on either side of the street; another painting of the same title features a one-legged man walking with crutches. Again depicting the same street, but this time in a different context, is "Rue Mosnier with Pavers", in which men repair the roadway while people and horses move past.
"The Railway", widely known as "The Gare Saint-Lazare", was painted in 1873. The setting is the urban landscape of Paris in the late 19th century. Using his favorite model in his last painting of her, a fellow painter, Victorine Meurent, also the model for "Olympia" and the "Luncheon on the Grass", sits before an iron fence holding a sleeping puppy and an open book in her lap. Next to her is a little girl with her back to the painter, watching a train pass beneath them.
Instead of choosing the traditional natural view as background for an outdoor scene, Manet opts for the iron grating which "boldly stretches across the canvas" The only evidence of the train is its white cloud of steam. In the distance, modern apartment buildings are seen. This arrangement compresses the foreground into a narrow focus. The traditional convention of deep space is ignored.
Historian Isabelle Dervaux has described the reception this painting received when it was first exhibited at the official Paris Salon of 1874: "Visitors and critics found its subject baffling, its composition incoherent, and its execution sketchy. Caricaturists ridiculed Manet's picture, in which only a few recognized the symbol of modernity that it has become today". The painting is currently in the National Gallery of Art in Washington, D.C.
Manet painted several boating subjects in 1874. "Boating", now in the Metropolitan Museum of Art, exemplifies in its conciseness the lessons Manet learned from Japanese prints, and the abrupt cropping by the frame of the boat and sail adds to the immediacy of the image. X-rays and pentimenti indicate that the man originally held the rope in his right hand.
Late works.
He completed painting his last major work, "A Bar at the Folies-Bergère (Un Bar aux Folies-Bergère)", in 1882 and it hung in the Salon that year.
In 1875, a book-length French edition of Edgar Allan Poe's "The Raven" included lithographs by Manet and translation by Mallarmé.
In 1881, with pressure from his friend Antonin Proust, the French government awarded Manet the Légion d'honneur.
Personal life.
After the death of his father in 1862, Manet married Suzanne Leenhoff in 1863. Leenhoff was a Dutch-born piano teacher of Manet's age with whom he had been romantically involved for approximately ten years. Leenhoff initially had been employed by Manet's father, Auguste, to teach Manet and his younger brother piano. She also may have been Auguste's mistress. In 1852, Leenhoff gave birth, out of wedlock, to a son, Leon Koella Leenhoff.
Eleven-year-old Leon Leenhoff, whose father may have been either of the Manets, posed often for Manet. Most famously, he is the subject of the "Boy Carrying a Sword" of 1861 (Metropolitan Museum of Art, New York). He also appears as the boy carrying a tray in the background of "The Balcony".
Manet painted his wife in "The Reading", among other paintings.
Death.
In his forties Manet contracted syphilis, for which he received no treatment. He also suffered from rheumatism. In the years before his death, he developed locomotor ataxia, a known side-effect of syphilis, which caused him considerable pain.
In April 1883, his left foot was amputated because of gangrene, and he died eleven days later in Paris. He is buried in the Passy Cemetery in the city.
References.
Further reading.
Short introductory works:
Longer works:

</doc>
<doc id="9616" url="https://en.wikipedia.org/wiki?curid=9616" title="Evolutionarily stable strategy">
Evolutionarily stable strategy

An evolutionarily stable strategy (ESS) is a strategy which, if adopted by a population in a given environment, cannot be invaded by any alternative strategy that is initially rare. It is relevant in game theory, behavioural ecology, and evolutionary psychology. An ESS is an equilibrium refinement of the Nash equilibrium. It is a Nash equilibrium that is "evolutionarily" stable: once it is fixed in a population, natural selection alone is sufficient to prevent alternative (mutant) strategies from invading successfully. The theory is not intended to deal with the possibility of gross external changes to the environment that bring new selective forces to bear.
First published as a specific term in the 1972 book by John Maynard Smith, the ESS is widely used in behavioural ecology and economics, and has been used in anthropology, evolutionary psychology, philosophy, and political science.
History.
Evolutionarily stable strategies were defined and introduced by John Maynard Smith and George R. Price in a 1973 "Nature" paper. Such was the time taken in peer-reviewing the paper for "Nature" that this was preceded by a 1972 essay by Maynard Smith in a book of essays titled "On Evolution". The 1972 essay is sometimes cited instead of the 1973 paper, but university libraries are much more likely to have copies of "Nature". Papers in "Nature" are usually short; in 1974, Maynard Smith published a longer paper in the "Journal of Theoretical Biology". Maynard Smith explains further in his 1982 book "Evolution and the Theory of Games". Sometimes these are cited instead. In fact, the ESS has become so central to game theory that often no citation is given, as the reader is assumed to be familiar with it.
Maynard Smith mathematically formalised a verbal argument made by Price, which he read while peer-reviewing Price's paper. When Maynard Smith realized that the somewhat disorganised Price was not ready to revise his article for publication, he offered to add Price as co-author.
The concept was derived from R. H. MacArthur and W. D. Hamilton's work on sex ratios, derived from Fisher's principle, especially Hamilton's (1967) concept of an unbeatable strategy. Maynard Smith was jointly awarded the 1999 Crafoord Prize for his development of the concept of evolutionarily stable strategies and the application of game theory to the evolution of behaviour.
Uses of ESS:
Motivation.
The Nash equilibrium is the traditional solution concept in game theory. It depends on the cognitive abilities of the players. It is assumed that players are aware of the structure of the game and consciously try to predict the moves of their opponents and to maximize their own payoffs. In addition, it is presumed that all the players know this (see common knowledge). These assumptions are then used to explain why players choose Nash equilibrium strategies.
Evolutionarily stable strategies are motivated entirely differently. Here, it is presumed that the players' strategies are biologically encoded and heritable. Individuals have no control over their strategy and need not be aware of the game. They reproduce and are subject to the forces of natural selection (with the payoffs of the game representing reproductive success (biological fitness)). It is imagined that alternative strategies of the game occasionally occur, via a process like mutation. To be an ESS, a strategy must be resistant to these alternatives.
Given the radically different motivating assumptions, it may come as a surprise that ESSes and Nash equilibria often coincide. In fact, every ESS corresponds to a Nash equilibrium, but some Nash equilibria are not ESSes.
Nash equilibria and ESS.
An ESS is a refined or modified form of a Nash equilibrium. (See the next section for examples which contrast the two.) In a Nash equilibrium, if all players adopt their respective parts, no player can "benefit" by switching to any alternative strategy. In a two player game, it is a strategy pair. Let E("S","T") represent the payoff for playing strategy "S" against strategy "T". The strategy pair ("S", "S") is a Nash equilibrium in a two player game if and only if this is true for both players and for all "T"≠"S":
In this definition, strategy "T" can be a neutral alternative to "S" (scoring equally well, but not better). 
A Nash equilibrium is presumed to be stable even if "T" scores equally, on the assumption that there is no long-term incentive for players to adopt "T" instead of "S". This fact represents the point of departure of the ESS.
Maynard Smith and Price specify two conditions for a strategy "S" to be an ESS. For all "T"≠"S", either
The first condition is sometimes called a "strict" Nash equilibrium. The second is sometimes called "Maynard Smith's second condition". The second condition means that although strategy "T" is neutral with respect to the payoff against strategy "S", the population of players who continue to play strategy "S" has an advantage when playing against "T".
There is also an alternative, stronger definition of ESS, due to Thomas. This places a different emphasis on the role of the Nash equilibrium concept in the ESS concept. Following the terminology given in the first definition above, this definition requires that for all "T"≠"S"
In this formulation, the first condition specifies that the strategy is a Nash equilibrium, and the second specifies that Maynard Smith's second condition is met. Note that the two definitions are not precisely equivalent: for example, each pure strategy in the coordination game below is an ESS by the first definition but not the second.
In words, this definition looks like this: The payoff of the first player when both players play strategy S is higher than (or equal to) the payoff of the first player when he changes to another strategy T and the second players keeps his strategy S. *AND* The payoff of the first player when only his opponent changes his strategy to T is higher than his payoff in case that both of players change their strategies to T.
This formulation more clearly highlights the role of the Nash equilibrium condition in the ESS. It also allows for a natural definition of related concepts such as a weak ESS or an evolutionarily stable set.
Examples of differences between Nash Equilibria and ESSes.
In most simple games, the ESSes and Nash equilibria coincide perfectly. For instance, in the Prisoner's Dilemma there is only one Nash equilibrium, and its strategy ("Defect") is also an ESS.
Some games may have Nash equilibria that are not ESSes. For example, in Harm thy neighbor both ("A", "A") and ("B", "B") are Nash equilibria, since players cannot do better by switching away from either. However, only "B" is an ESS (and a strong Nash). "A" is not an ESS, so "B" can neutrally invade a population of "A" strategists and predominate, because "B" scores higher against "B" than "A" does against "B". This dynamic is captured by Maynard Smith's second condition, since E("A", "A") = E("B", "A"), but it is not the case that E("A","B") > E("B","B").
Nash equilibria with equally scoring alternatives can be ESSes. For example, in the game "Harm everyone", "C" is an ESS because it satisfies Maynard Smith's second condition. "D" strategists may temporarily invade a population of "C" strategists by scoring equally well against "C", but they pay a price when they begin to play against each other; "C" scores better against "D" than does "D". So here although E("C", "C") = E("D", "C"), it is also the case that E("C","D") > E("D","D"). As a result, "C" is an ESS.
Even if a game has pure strategy Nash equilibria, it might be that none of those pure strategies are ESS. Consider the Game of chicken. There are two pure strategy Nash equilibria in this game ("Swerve", "Stay") and ("Stay", "Swerve"). However, in the absence of an uncorrelated asymmetry, neither "Swerve" nor "Stay" are ESSes. There is a third Nash equilibrium, a mixed strategy which is an ESS for this game (see Hawk-dove game and Best response for explanation).
This last example points to an important difference between Nash equilibria and ESS. Nash equilibria are defined on "strategy sets" (a specification of a strategy for each player), while ESS are defined in terms of strategies themselves. The equilibria defined by ESS must always be symmetric, and thus have fewer equilibrium points.
ESS vs. Evolutionarily Stable State.
In population biology, the two concepts of an "evolutionarily stable strategy" (ESS) and an "evolutionarily stable state" are closely linked but describe different situations.
Thomas (1984) applies the term ESS to an individual strategy which may be mixed, and evolutionarily stable population state to a population mixture of pure strategies which may be formally equivalent to the mixed ESS.
Whether a population is evolutionarily stable does not relate to its genetic diversity: it can be genetically monomorphic or polymorphic.
Stochastic ESS.
In the classic definition of an ESS, no mutant strategy can invade. In finite populations, any mutant could in principle invade, albeit at low probability, implying that no ESS can exist. In a finite population, an ESS can instead be defined as a strategy which, should it become invaded by a new mutant strategy with probability p, would be able to counterinvade from a single starting individual with probability >p.
Prisoner's dilemma and ESS.
A common model of altruism and social cooperation is the Prisoner's dilemma. Here a group of players would collectively be better off if they could play "Cooperate", but since "Defect" fares better each individual player has an incentive to play "Defect". One solution to this problem is to introduce the possibility of retaliation by having individuals play the game repeatedly against the same player. In the so-called "iterated" Prisoner's dilemma, the same two individuals play the prisoner's dilemma over and over. While the Prisoner's dilemma has only two strategies ("Cooperate" and "Defect"), the iterated Prisoner's dilemma has a huge number of possible strategies. Since an individual can have different contingency plan for each history and the game may be repeated an indefinite number of times, there may in fact be an infinite number of such contingency plans.
Three simple contingency plans which have received substantial attention are "Always Defect", "Always Cooperate", and "Tit for Tat". The first two strategies do the same thing regardless of the other player's actions, while the later responds on the next round by doing what was done to it on the previous round—it responds to "Cooperate" with "Cooperate" and "Defect" with "Defect".
If the entire population plays "Tit-for-Tat" and a mutant arises who plays "Always Defect", "Tit-for-Tat" will outperform "Always Defect". If the population of the mutant becomes too large — the percentage of the mutant will be kept small. "Tit for Tat" is therefore an ESS, "with respect to only these two strategies". On the other hand, an island of "Always Defect" players will be stable against the invasion of a few "Tit-for-Tat" players, but not against a large number of them. If we introduce "Always Cooperate", a population of "Tit-for-Tat" is no longer an ESS. Since a population of "Tit-for-Tat" players always cooperates, the strategy "Always Cooperate" behaves identically in this population. As a result, a mutant who plays "Always Cooperate" will not be eliminated. However, even though a population of "Always Cooperate" and "Tit-for-Tat" can coexist, if there is a small percentage of the population that is "Always Defect", the selective pressure is against "Always Cooperate", and in favour of "Tit-for-Tat". This is due to the lower payoffs of cooperating than those of defecting in case the opponent defects.
This demonstrates the difficulties in applying the formal definition of an ESS to games with large strategy spaces, and has motivated some to consider alternatives.
ESS and human behavior.
The fields of sociobiology and evolutionary psychology attempt to explain animal and human behavior and social structures, largely in terms of evolutionarily stable strategies. Sociopathy (chronic antisocial or criminal behavior) may be a result of a combination of two such strategies.
Evolutionarily stable strategies were originally considered for biological evolution, but they can apply to other contexts. In fact, there are stable states for a large class of adaptive dynamics. As a result, they can be used to explain human behaviours that lack any genetic influences.

</doc>
<doc id="9617" url="https://en.wikipedia.org/wiki?curid=9617" title="Element">
Element

Element or elements may refer to:

</doc>
<doc id="9619" url="https://en.wikipedia.org/wiki?curid=9619" title="Extremophile">
Extremophile

An extremophile (from Latin ' meaning "extreme" and Greek ' () meaning "love") is an organism that thrives in physically or geochemically extreme conditions that are detrimental to most life on Earth. In contrast, organisms that live in more moderate environments may be termed mesophiles or neutrophiles.
Characteristics.
In the 1980s and 1990s, biologists found that microbial life has an amazing flexibility for surviving in extreme environments — niches that are extraordinarily hot, or acidic, for example — that would be completely inhospitable to complex organisms. Some scientists even concluded that life may have begun on Earth in hydrothermal vents far under the ocean's surface. According to astrophysicist Dr. Steinn Sigurdsson, "There are viable bacterial spores that have been found that are 40 million years old on Earth — and we know they're very hardened to radiation." On 6 February 2013, scientists reported that bacteria were found living in the cold and dark in a lake buried a half-mile deep under the ice in Antarctica. On 17 March 2013, researchers reported data that suggested microbial life forms thrive in the Mariana Trench, the deepest spot on the Earth. Other researchers reported related studies that microbes thrive inside rocks up to 1900 feet below the sea floor under 8500 feet of ocean off the coast of the northwestern United States. According to one of the researchers,"You can find microbes everywhere — they're extremely adaptable to conditions, and survive wherever they are."
Morphology.
Most known extremophiles are microbes. The domain Archaea contains renowned examples, but extremophiles are present in numerous and diverse genetic lineages of bacteria and archaeans. Furthermore, it is erroneous to use the term extremophile to encompass all archaeans, as some are mesophilic. Neither are all extremophiles unicellular; protostome animals found in similar environments include the Pompeii worm, the psychrophilic Grylloblattidae (insects) and Antarctic krill (a crustacean). Many would also classify tardigrades (water bears) as extremophiles but while tardigrades can survive in extreme environments, they are not considered extremophiles because they are not adapted to live in these conditions. Their chances of dying increase the longer they are exposed to the extreme environment.
Classifications.
There are many classes of extremophiles that range all around the globe, each corresponding to the way its environmental niche differs from mesophilic conditions. These classifications are not exclusive. Many extremophiles fall under multiple categories and are classified as polyextremophiles. For example, organisms living inside hot rocks deep under Earth's surface are thermophilic and barophilic such as "Thermococcus barophilus". A polyextremophile living at the summit of a mountain in the Atacama Desert might be a radioresistant xerophile, a psychrophile, and an oligotroph. Polyextremophiles are well known for their ability to tolerate both high and low pH levels.
In astrobiology.
Astrobiology is the field concerned with forming theories, such as panspermia, about the distribution, nature, and future of life in the universe. In it, microbial ecologists, astronomers, planetary scientists, geochemists, philosophers, and explorers cooperate constructively to guide the search for life on other planets. Astrobiologists are particularly interested in studying extremophiles, as many organisms of this type are capable of surviving in environments similar to those known to exist on other planets. For example, Mars may have regions in its deep subsurface permafrost that could harbor endolith communities. The subsurface water ocean of Jupiter's moon Europa may harbor life, especially at hypothesized hydrothermal vents at the ocean floor.
Recent research carried out on extremophiles in Japan involved a variety of bacteria including "Escherichia coli" and "Paracoccus denitrificans" being subject to conditions of extreme gravity. The bacteria were cultivated while being rotated in an ultracentrifuge at high speeds corresponding to 403,627 g (i.e. 403,627 times the gravity experienced on Earth). "Paracoccus denitrificans" was one of the bacteria which displayed not only survival but also robust cellular growth under these conditions of hyperacceleration which are usually found only in cosmic environments, such as on very massive stars or in the shock waves of supernovas. Analysis showed that the small size of prokaryotic cells is essential for successful growth under hypergravity. The research has implications on the feasibility of panspermia.
On 26 April 2012, scientists reported that lichen survived and showed remarkable results on the adaptation capacity of photosynthetic activity within the simulation time of 34 days under Martian conditions in the Mars Simulation Laboratory (MSL) maintained by the German Aerospace Center (DLR).
On 29 April 2013, scientists at Rensselaer Polytechnic Institute, funded by NASA, reported that, during spaceflight on the International Space Station, microbes seem to adapt to the space environment in ways "not observed on Earth" and in ways that "can lead to increases in growth and virulence".
On 19 May 2014, scientists announced that numerous microbes, like "Tersicoccus phoenicis", may be resistant to methods usually used in spacecraft assembly clean rooms. It's not currently known if such resistant microbes could have withstood space travel and are present on the "Curiosity" rover now on the planet Mars.
On 20 August 2014, scientists confirmed the existence of microorganisms living half a mile below the ice of Antarctica.
Examples.
New sub-types of -philes are identified frequently and the sub-category list for extremophiles is always growing. For example, microbial life lives in the liquid asphalt lake, Pitch Lake. Research indicates that extremophiles inhabit the asphalt lake in populations ranging between 10 to 10 cells/gram. Likewise, until recently boron tolerance was unknown but a strong borophile was discovered in bacteria. With the recent isolation of "Bacillus boroniphilus", borophiles came into discussion. Studying these borophiles may help illuminate the mechanisms of both boron toxicity and boron deficiency.
Industrial uses.
The thermoalkaliphilic catalase, which initiates the breakdown of hydrogen peroxide into oxygen and water, was isolated from an organism, "Thermus brockianus", found in Yellowstone National Park by Idaho National Laboratory researchers. The catalase operates over a temperature range from 30 °C to over 94 °C and a pH range from 6-10. This catalase is extremely stable compared to other catalases at high temperatures and pH. In a comparative study, the "T. brockianus" catalase exhibited a half life of 15 days at 80 °C and pH 10 while a catalase derived from "Aspergillus niger" had a half life of 15 seconds under the same conditions. The catalase will have applications for removal of hydrogen peroxide in industrial processes such as pulp and paper bleaching, textile bleaching, food pasteurization, and surface decontamination of food packaging.
DNA modifying enzymes such as "Taq" DNA polymerase and some "Bacillus" enzymes used in clinical diagnostics and starch liquefaction are produced commercially by several biotechnology companies.
DNA transfer.
Over 65 prokaryotic species are known to be naturally competent for genetic transformation, the ability to transfer DNA from one cell to another cell followed by integration of the donor DNA into the recipient cell’s chromosome. Several extremophiles are able to carry out species-specific DNA transfer, as described below. However, it is not yet clear how common such a capability is among extremophiles.
The bacterium "Deinococcus radiodurans" is one of the most radioresistant organisms known. This bacterium can also survive cold, dehydration, vacuum and acid and is thus known as a polyextremophile. "D. radiodurans" is competent to perform genetic transformation. Recipient cells are able to repair DNA damage in donor transforming DNA that had been UV irradiated as efficiently as they repair cellular DNA when the cells themselves are irradiated. The extreme thermophilic bacterium "Thermus thermophilus" and other related "Thermus" species are also capable of genetic transformation.
"Halobacterium volcanii", an extreme halophilic (saline tolerant) archaeon, is capable of natural genetic transformation. Cytoplasmic bridges are formed between cells that appear to be used for DNA transfer from one cell to another in either direction.
"Sulfolobus solfataricus" and "Sulfolobus acidocaldarius" are hyperthermophilic archaea. Exposure of these organisms to the DNA damaging agents UV irradiation, bleomycin or mitomycin C induces species-specific cellular aggregation. UV-induced cellular aggregation of "S. acidocaldarius" mediates chromosomal marker exchange with high frequency. Recombination rates exceed those of uninduced cultures by up to three orders of magnitude. Frols et al. and Ajon et al. hypothesized that cellular aggregation enhances species-specific DNA transfer between "Sulfolobus" cells in order to repair damaged DNA by means of homologous recombination. Van Wolferen et al. noted that this DNA exchange process may be crucial under DNA damaging conditions such as high temperatures. It has also been suggested that DNA transfer in "Sulfolobus" may be an early form of sexual interaction similar to the more well-studied bacterial transformation systems that involve species-specific DNA transfer leading to homologous recombinational repair of DNA damage (and see Transformation (genetics)).
Extracellular membrane vesicles (MVs) might be involved in DNA transfer between different hyperthermophilic archaeal species. It has been shown that both plasmids and viral genomes can be transferred via MVs. Notably, a horizontal plasmid transfer has been documented between hyperthermophilic "Thermococcus" and "Methanocaldococcus" species, respectively belonging to the orders "Thermococcales" and "Methanococcales".

</doc>
<doc id="9620" url="https://en.wikipedia.org/wiki?curid=9620" title="Education reform">
Education reform

Education reform is the name given to the goal of changing public education. Historically, reforms have taken different forms because the motivations of reformers have differed. However, since the 1980s, education reform has been focused on changing the existing system from one focused on inputs to one focused on outputs (i.e., student achievement). In the United States, education reform acknowledges and encourages public education as the primary source of K-12 education for American youth. Education reformers desire to make public education into a market (in the form of an input-output system), where accountability creates high-stakes from curriculum standards tied to standardized tests.
This conceptualization of education reform is based on the market-logic of competition. As a consequence, competition creates inequality which has continued to drive the market-logic of equality at an end point by reproduce the achievement gap among diverse youth. Overall, education reform has and continues to be used as a substitute for needed economic reforms in the United States.
The one constant for all forms of education reform includes the idea that small changes in education will have large social returns in citizen health, wealth and well-being. For example, a stated motivation has been to reduce cost to students and society. From ancient times until the 1800s, one goal was to reduce the expense of a classical education. Ideally, classical education is undertaken with a highly educated full-time (extremely expensive) personal tutor. Historically, this was available only to the most wealthy. Encyclopedias, public libraries and grammar schools are examples of innovations intended to lower the cost of a classical education.
Related reforms attempted to develop similar classical results by concentrating on "why", and "which" questions neglected by classical education. Abstract, introspective answers to these questions can theoretically compress large amounts of facts into relatively few principles. This path was taken by some Transcendentalist educators, such as Amos Bronson Alcott.
In the early modern age, Victorian schools were reformed to teach commercially useful topics, such as modern languages and mathematics, rather than classical subjects, such as Latin and Greek.
Many reformers focused on reforming society by reforming education on more scientific, humanistic, pragmatic or democratic principles. John Dewey and Anton Makarenko are prominent examples of such reformers. Some reformers incorporated several motivations, e.g. Maria Montessori, who both "educated for peace" (a social goal), and to "meet the needs of the child" (A humanistic goal). In historic Prussia, an important motivation for the invention of Kindergarten was to foster national unity by teaching a national language while children were young enough that learning a language was easy.
Reform has taken many forms and directions. Throughout history and the present day, the meaning and methods of education have changed through debates over what content or experiences result in an educated individual or an educated society. Changes may be implemented by individual educators and/or by broad-based school organization and/or by curriculum changes with performance evaluations.
History.
Classical times.
Plato believed that children would never learn unless they wanted to learn. In "The Republic", he said, " . . compulsory learning never sticks in the mind."
An educational debate in the time of the Roman Empire arose after Christianity had achieved broad acceptance. The question concerned the educational value of pre-Christian classical thought: "Given that the body of knowledge of the pre-Christian Romans was heathen in origin, was it safe to teach it to Christian children?" 
Modern reforms.
Though educational reform occurred on a local level at various points throughout history, the modern notion of education reform is tied with the spread of Compulsory education - education reforms did not become widespread until after organized schooling was sufficiently systematized to be 'reformed.'
In the modern world, economic growth and the spread of democracy have raised the value of education and increased the importance of ensuring that all children and adults have access to high quality and effective education. Modern education reforms are increasingly driven by a growing understanding of what works in education and how to go about successfully improving teaching and learning in schools.
Reforms of classical education.
Western classical education as taught from the 18th to the 19th century has missing features that inspired reformers. Classical education is most concerned with answering the who, what, where, and when? questions that concern a majority of students. Unless carefully taught, group instruction naturally neglects the theoretical "why" and "which" questions that strongly concern fewer students.
Classical education in this period also did not teach local (vernacular) languages and cultures. Instead it taught high-status ancient languages (Greek and Latin) and their cultures. This produced odd social effects in which an intellectual class might be more loyal to ancient cultures and institutions than to their native vernacular languages and their actual governing authorities.
England in the 19th century.
Before there were government-funded public schools, education of the lower classes was by the charity school, pioneered in the 19th century by Protestant organizations and adapted by the Roman Catholic Church and governments. Because these schools operated on very small budgets and attempted to serve as many needy children as possible, they were designed to be inexpensive.
The basic program was to develop "grammar" schools. These taught only grammar and bookkeeping. This program permitted people to start businesses to make money, and gave them the skills to continue their education inexpensively from books. "Grammar" was the first third of the then-prevalent system of Classical Education.
The ultimate development of the grammar school was by Joseph Lancaster and Andrew Bell who developed the monitorial system. Lancaster started as a poor Quaker in early 19th century London. Bell started the Madras School of India. The monitorial system uses slightly more-advanced students to teach less-advanced students, achieving student-teacher ratios as small as 2, while educating more than a thousand students per adult. Lancaster promoted his system in a piece called Improvements in Education that spread widely throughout the English-speaking world.
Discipline and labor in a Lancaster school were provided by an economic system. Scrip, a form of money meaningless outside the school, was created at a fixed exchange rate from a student's tuition. Every job of the school was bid-for by students in scrip, with the largest bid winning. However, "any" student tutor could auction positions in his or her classes. Besides tutoring, students could use scrip to buy food, school supplies, books, and childish luxuries in a school store. The adult supervisors were paid from the bids on jobs.
With fully developed internal economies, Lancaster schools provided a grammar-school education for a cost per student near $40 per year in 1999 U.S. dollars. The students were very clever at reducing their costs, and once invented, improvements were widely adopted in a school. For example, Lancaster students, motivated to save scrip, ultimately rented individual pages of textbooks from the school library, and read them in groups around music stands to reduce textbook costs. Students commonly exchanged tutoring, and paid for items and services with receipts from "down tutoring."
Lancaster schools usually lacked sufficient adult supervision. As a result, the older children acting as disciplinary monitors tended to become brutal task masters. Also, the schools did not teach submission to orthodox Christian beliefs or government authorities. As a result, most English-speaking countries developed mandatory publicly paid education explicitly to keep public education in "responsible" hands. These elites said that Lancaster schools might become dishonest, provide poor education and were not accountable to established authorities.
Lancaster's supporters responded that any schoolchild could avoid cheats, given the opportunity, and that the government was not paying for the education, and thus deserved no say in their composition.
Lancaster, though motivated by charity, claimed in his pamphlets to be surprised to find that he lived well on the income of his school, even while the low costs made it available to the poorest street-children.
Ironically, Lancaster lived on the charity of friends in his later life.
Progressive reforms in Europe and the United States.
The term "progressive" in education has been used somewhat indiscriminately; there are a number of kinds of educational progressivism, most of the historically significant kinds peaking in the period between the late 19th and the middle of the 20th centuries.
Child-study.
Jean-Jacques Rousseau has been called the father of the child-study movement. It has been said that Rousseau "discovered" the child (as an object of study).
Rousseau's principal work on education is "", in which he lays out an educational program for a hypothetical newborn's education to adulthood. Rousseau provided a dual critique of both the vision of education set forth in Plato's Republic and also of the society of his contemporary Europe and the educational methods he regarded as contributing to it; he held that a person can either be a man or a citizen, and that while Plato's plan could have brought the latter at the expense of the former, contemporary education failed at both tasks. He advocated a radical withdrawal of the child from society and an educational process that utilized the natural potential of the child and its curiosity, teaching it by confronting it with simulated real-life obstacles and conditioning it by experience rather than teaching it intellectually. His ideas were rarely implemented directly, but were influential on later thinkers, particularly Johann Heinrich Pestalozzi and Friedrich Wilhelm August Fröbel, the inventor of the kindergarten.
Horace Mann.
In the United States, Horace Mann (1796 – 1859) of Massachusetts used his political base and role as Secretary of the Massachusetts State Board of Education to promote public education in his home state and nationwide. His crusading style attracted wide middle class support. Historian Ellwood P. Cubberley asserts:
National identity.
Education is often seen in Europe and Asia as an important system to maintain national, cultural and linguistic unity. Prussia instituted primary school reforms expressly to teach a unified version of the national language, "Hochdeutsch". One significant reform was kindergarten, whose purpose was to have the children spend time in supervised activities in the national language, when the children were young enough that they could easily learn new language skills.
Since most modern schools copy the Prussian models, children start school at an age when their language skills remain plastic, and they find it easy to learn the national language. This was an intentional design on the part of the Prussians.
In the U.S. over the last twenty years, more than 70% of non-English-speaking school-age immigrants have arrived in the U.S. before they were 6 years old. At this age, they could have been taught English in school, and achieved a proficiency indistinguishable from a native speaker. In other countries, such as the Soviet Union, France, Spain, and Germany this approach has dramatically improved reading and math test scores for linguistic minorities.
Dewey.
John Dewey, a philosopher and educator based in Chicago and New York, helped conceptualize the role of American and international education during the first four decades of the 20th century. An important member of the American Pragmatist movement, he carried the subordination of knowledge to action into the educational world by arguing for experiential education that would enable children to learn theory and practice simultaneously; a well-known example is the practice of teaching elementary physics and biology to students while preparing a meal. He was a harsh critic of "dead" knowledge disconnected from practical human life.
Dewey criticized the rigidity and volume of humanistic education, and the emotional idealizations of education based on the child-study movement that had been inspired by Bill Joel and those who followed him. He presented his educational theories as a synthesis of the two views. His slogan was that schools should encourage children to "Learn by doing." He wanted people to realize that children are naturally active and curious. Dewey's understanding of logic is best presented in his "Logic, the Theory of Inquiry" (1938). His educational theories were presented in "My Pedagogic Creed", "The School and Society", "The Child and Curriculum", and "Democracy and Education" (1916).
The question of the history of Deweyan educational practice is a difficult one. He was a widely known and influential thinker, but his views and suggestions were often misunderstood by those who sought to apply them, leading some historians to suggest that there was never an actual implementation on any considerable scale of Deweyan progressive education. The schools with which Dewey himself was most closely associated (though the most famous, the "Laboratory School", was really run by his wife) had considerable ups and downs, and Dewey left the University of Chicago in 1904 over issues relating to the Dewey School.
Dewey's influence began to decline in the time after the Second World War and particularly in the Cold War era, as more conservative educational policies came to the fore.
The administrative progressives.
The form of educational progressivism which was most successful in having its policies implemented has been dubbed "administrative progressivism" by historians. This began to be implemented in the early 20th century. While influenced particularly in its rhetoric by Dewey and even more by his popularizers, administrative progressivism was in its practice much more influenced by the industrial revolution and the concept economies of scale.
The administrative progressives are responsible for many features of modern American education, especially American high schools: counseling programs, the move from many small local high schools to large centralized high schools, curricular differentiation in the form of electives and tracking, curricular, professional, and other forms of standardization, and an increase in state and federal regulation and bureaucracy, with a corresponding reduction of local control at the school board level. (Cf. "State, federal, and local control of education in the United States", below) (Tyack and Cuban, pp. 17–26)
These reforms have since become heavily entrenched, and many today who identify themselves as progressives are opposed to many of them, while conservative education reform during the Cold War embraced them as a framework for strengthening traditional curriculum and standards.
In more recent times, groups such as the think tank Reform's education division, and S.E.R. have attempted to pressure the government of the U.K. into more modernist educational reform, though this has met with limited success.
Critiques of progressive and classical reforms.
Many progressive reforms failed to transfer learned skills. Evidence suggests that higher-order thinking skills are unused by many people (cf. Jean Piaget, Isabel Myers, and Katharine Cook Briggs). Some authorities say that this refutes key assumptions of progressive thinkers such as Dewey.
Jean Piaget was a Swiss psychologist who studied people's developmental stages. He showed by widely reproduced experiments that most young children do not analyze or synthesize as Dewey expected. Some authorities therefore say that Dewey's reforms do not apply to the primary education of young children.
Katherine Briggs and her daughter Isabel Myers developed a psychological test that reproducibly identifies sixteen distinct human temperaments, building on work by Jung. A wide class of temperaments ("Sensors", half by category, 60% of the general population) prefer to use concrete information such as facts and procedures. They prefer not to use abstract theories or logic. In terms of education, some authorities interpret this to mean that 60% of the general population only use, and therefore would prefer to learn answers to concrete "Who, what, when, where", and "how" questions, rather than answers to the theoretical "which" and "why" questions advocated by progressives. This information was confirmed (on another research track) by Jean Piaget, who discovered that nearly 60% of adults never habitually use what he called "formal operational reasoning", a term for the development and use of theories and explicit logic. If this criticism is true, then schools that teach only "principles" would fail to educate 60% of the general population.
The data from Piaget, Myers and Briggs can also be used to criticize classical teaching styles that "never" teach theory or principle. In particular, a wide class of temperaments ("Intuitives", half by category, 40% of the general population) prefer to reason from trusted first principles, and then apply that theory to predict concrete facts. In terms of education, some authorities interpret this to mean that 40% of the general population prefer to use, and therefore want to learn, answers to theoretical "Which and "Why" questions, rather than answers to the concrete "Who, what, when, where" and "How" questions.
The synthesis resulting from this two-part critique is a "neoclassical" learning theory similar to that practiced by Marva Collins, in which both learning styles are accommodated. The classroom is filled with facts, that are organized with theories, providing a rich environment to feed children's natural preferences. To reduce the limitations of depending only on natural preferences, all children are required to learn both important facts, and important forms of reasoning.
Diane Ravitch argues that "progressive" reformers have replaced a challenging liberal arts curriculum with ever-lower standards and indoctrination, particularly in inner-city schools, thereby preventing vast numbers of students from achieving their full potential.
Late-20th century (United states).
Reforms arising from the civil rights era.
From the 1950s to the 1970s, many of the proposed and implemented reforms in U.S. education stemmed from the Civil Rights Movement and related trends; examples include ending racial segregation, and busing for the purpose of desegregation, affirmative action, and banning of school prayer.
1980s.
In the 1980s, some of the momentum of education reform moved from the left to the right, with the release of "A Nation at Risk", Ronald Reagan's efforts to reduce or eliminate the United States Department of Education. "he federal government and virtually all state governments, teacher training institutions, teachers' unions, major foundations, and the mass media have all pushed strenuously for higher standards, greater accountability, more "time on task," and more impressive academic results".
This shift to the right caused many families to seek alternatives, including "charter schools, progressive schools, Montessori schools, Waldorf schools, Afrocentric schools, religious schools - or teaching them at home and in their communities."
In the latter half of the decade, E. D. Hirsch put forth an influential attack on one or more versions of progressive education, advocating an emphasis on "cultural literacy"—the facts, phrases, and texts that Hirsch asserted every American had once known and that now only some knew, but was still essential for decoding basic texts and maintaining communication. Hirsch's ideas remain significant through the 1990s and into the 21st century, and are incorporated into classroom practice through textbooks and curricula published under his own imprint.
1990s and 2000s.
Most states and districts in the 1990s adopted Outcome-Based Education (OBE) in some form or another. A state would create a committee to adopt standards, and choose a quantitative instrument to assess whether the students knew the required content or could perform the required tasks. The standards-based National Education Goals (Goals 2000) were set by the U.S. Congress in the 1990s. Many of these goals were based on the principles of outcomes-based education, and not all of the goals were attained by the year 2000 as was intended. The standards-based reform movement culminated in the No Child Left Behind Act of 2001, which as of 2016 is still an active nationwide mandate in the United States.
OBE reforms usually had other disputed methods, such as constructivist mathematics and whole language, added onto them. Some proponents advocated replacing the traditional high school diploma with a Certificate of Initial Mastery. Other reform movements were school-to-work, which would require all students except those in a university track to spend substantial class time on a job site. See also Uncommon Schools.
Contemporary issues (United States).
Overview.
In the first decade of the 21st century, several issues are salient in debates over further education reform:
Funding levels.
According to a 2005 report from the OECD, the United States is tied for first place with Switzerland when it comes to annual spending per student on its public schools, with each of those two countries spending more than $11,000 (in U.S. currency).
Despite this high level of funding, U.S. public schools lag behind the schools of other rich countries in the areas of reading, math, and science. A further analysis of developed countries shows no correlation between per student spending and student performance, suggesting that there are other factors influencing education. Top performers include Singapore, Finland and Korea, all with relatively low spending on education, while high spenders including Norway and Luxembourg have relatively low performance. One possible factor is the distribution of the funding. In the US, schools in wealthy areas tend to be over-funded while schools in poorer areas tend to be underfunded. These differences in spending between schools or districts may accentuate inequalities, if they result in the best teachers moving to teach in the most wealthy areas. It has also been shown that the socioeconomic situation of the students family has the most influence in determining success; suggesting that even if increased funds in a low income area increase performance, they may still perform worse than their peers from wealthier districts.
Starting in the early 1980s, a series of analyses by Eric Hanushek indicated that the amount spent on schools bore little relationship to student learning. This controversial argument, which focused attention on how money was spent instead of how much was spent, led to lengthy scholarly exchanges. In part the arguments fed into the class size debates and other discussions of "input policies." It also moved reform efforts towards issues of school accountability (including No Child Left Behind) and the use of merit pay and other incentives.
There have been studies that show smaller class sizes and newer buildings (both of which require higher funding to implement) lead to academic improvements. It should also be noted that many of the reform ideas that stray from the traditional format require greater funding.
It has been shown that some school districts do not use their funds in the most productive way. For example, according to a 2007 article in the "Washington Post", the Washington, D.C. public school district spends $12,979 per student per year. This is the third highest level of funding per student out of the 100 biggest school districts in the United States. Despite this high level of funding, the school district provides outcomes that are lower than the national average. In reading and math, the district's students score the lowest among 11 major school districts—even when poor children are compared only with other poor children. 33% of poor fourth graders in the United States lack basic skills in math, but in Washington, D.C., it's 62%. According to a 2006 study by the Goldwater Institute, Arizona's public schools spend 50% more per student than Arizona's private schools. The study also says that while teachers constitute 72% of the employees at private schools, they make up less than half of the staff at public schools. According to the study, if Arizona's public schools wanted to be like private schools, they would have to hire approximately 25,000 more teachers, and eliminate 21,210 administration employees. The study also said that public school teachers are paid about 50% more than private school teachers.
In 1985 in Kansas City, Missouri, a judge ordered the school district to raise taxes and spend more money on public education. Spending was increased so much, that the school district was spending more money per student than any of the country's other 280 largest school districts. Although this very high level of spending continued for more than a decade, there was no improvement in the school district's academic performance.
According to a 1999 article, William J. Bennett, former U.S. Secretary of Education, argued that increased levels of spending on public education have not made the schools better, citing the following statistics:
Alternatives to public education.
In the United States, Private schools (independent schools) have long been an alternative to public education for those with the ability to pay tuition. These include religious schools, preparatory and boarding schools, and schools based on alternative philosophies such as Montessori education. Over 4 million students, about one in twelve children attend religious schools in the United States, most of them Christian.
Montessori pre- and primary school programs employ alternative theories of guided exploration which seek to embrace children's natural curiosity rather than, for instance, scolding them for falling out of rank.
Home education is favored by a growing number of parents who take direct responsibility for their children's education rather than enrolling them in local public schools seen as not meeting expectations.
School choice.
Economists such as Nobel laureate Milton Friedman advocate school choice to promote excellence in education through competition and choice. A competitive "market" for schools eliminates the need to otherwise attempt a workable method of accountability for results. Public education vouchers permit guardians to select and pay any school, public or private, with public funds currently allocated to local public schools. The theory is that children's guardians will naturally shop for the best schools, much as is already done at college level.
Though appealing in theory, many reforms based on school choice have led to slight to moderate improvements—which some teachers' union members see as insufficient to offset the decreased teacher pay and job security. For instance, New Zealand's landmark reform in 1989, during which schools were granted substantial autonomy, funding was devolved to schools, and parents were given a free choice of which school their children would attend, led to moderate improvements in most schools. It was argued that the associated increases in inequity and greater racial stratification in schools nullified the educational gains. Others, however, argued that the original system created more inequity (due to lower income students being required to attend poorer performing inner city schools and not being allowed school choice or better educations that are available to higher income inhabitants of suburbs). Instead, it was argued that the school choice promoted social mobility and increased test scores especially in the cases of low income students. Similar results have been found in other jurisdictions. Though discouraging, the merely slight improvements of some school choice policies often seems to reflect weaknesses in the way that choice is implemented rather than a failure of the basic principle itself.
Teacher tenure.
In October 2010 Apple Inc. CEO Steve Jobs had a consequential meeting with U.S. President Barack Obama to discuss U.S. competitiveness and the nation's education system. During the meeting Jobs recommended pursuing policies that would make it easier for school principals to hire and fire teachers based on merit.
In 2012 tenure for school teachers was challenged in a California lawsuit called "Vergara v. California". The primary issue in the case was the impact of tenure on student outcomes and on equity in education. On June 10, 2014, the trial judge ruled that California's teacher tenure statute produced disparities that " shock the conscience" and violate the equal protection clause of the California Constitution. On July 7, 2014, U.S. Secretary of Education Arne Duncan commented on the "Vergara" decision during a meeting with President Barack Obama and representatives of teacher's unions. Duncan said that tenure for school teachers "should be earned through demonstrated effectiveness" and should not be granted too quickly. Specifically, he criticized the 18-month tenure period at the heart of the "Vergara" case as being too short to be a "meaningful bar."
Barriers to reform.
A study by the Fordham Institute found that some labor agreements with teachers' unions may restrict the ability of school systems to implement merit pay and other reforms. Contracts were more restrictive in districts with high concentrations of poor and minority students. The methodology and conclusions of the study have been criticized by teachers' unions.
Another barrier to reform is assuming that schools are like businesses—when in fact they are very different.
Legal barriers to reform are low in the United States compared to other countries: State and local governance of education creates "wiggle room for educational innovators" who can change local laws or move somewhere more favourable. Cultural barriers to reform are also relatively low, because the question of who should control education is still open.
Internationally.
Taiwan.
In other parts of the world, educational reform has had a number of different meanings. In Taiwan in the 1990s and first decade of the 21st century a movement tried to prioritize reasoning over mere facts, reduce the emphasis on central control and standardized testing. There was consensus on the problems. Efforts were limited because there was little consensus on the goals of educational reforms, and therefore on how to fix the problems. By 2003, the push for education reform had declined.
Motivations.
Education reform has been pursued for a variety of specific reasons, but generally most reforms aim at redressing some societal ills, such as poverty-, gender-, or class-based inequities, or perceived ineffectiveness. Current education trends in the United States represent multiple achievement gaps across ethnicities, income levels, and geographies. As McKinsey and Company reported in a 2009 analysis, “These educational gaps impose on the United States the economic equivalent of a permanent national recession.” Reforms are usually proposed by thinkers who aim to redress societal ills or institute societal changes, most often through a change in the education of the members of a class of people—the preparation of a ruling class to rule or a working class to work, the social hygiene of a lower or immigrant class, the preparation of citizens in a democracy or republic, etc. The idea that all children should be provided with a high level of education is a relatively recent idea, and has arisen largely in the context of Western democracy in the 20th century.
The "beliefs" of school districts are optimistic that quite literally "all students will succeed", which in the context of high school graduation examination in the United States, all students in all groups, regardless of heritage or income will pass tests that in the introduction typically fall beyond the ability of all but the top 20 to 30 percent of students. The claims clearly renounce historical research that shows that all ethnic and income groups score differently on all standardized tests and standards based assessments and that students will achieve on a bell curve. Instead, education officials across the world believe that by setting clear, achievable, higher standards, aligning the curriculum, and assessing outcomes, learning can be increased for all students, and more students can succeed than the 50 percent who are defined to be above or below grade level by norm referenced standards.
States have tried to use state schools to increase state power, especially to make better soldiers and workers. This strategy was first adopted to unify related linguistic groups in Europe, including France, Germany and Italy. Exact mechanisms are unclear, but it often fails in areas where populations are culturally segregated, as when the U.S. Indian school service failed to suppress Lakota and Navaho, or when a culture has widely respected autonomous cultural institutions, as when the Spanish failed to suppress Catalan.
Many students of democracy have desired to improve education in order to improve the quality of governance in democratic societies; the necessity of good public education follows logically if one believes that the quality of democratic governance depends on the ability of citizens to make informed, intelligent choices, and that education can improve these abilities.
Politically motivated educational reforms of the democratic type are recorded as far back as Plato in "The Republic". In the United States, this lineage of democratic education reform was continued by Thomas Jefferson, who advocated ambitious reforms partly along Platonic lines for public schooling in Virginia.
Another motivation for reform is the desire to address socio-economic problems, which many people see as having significant roots in lack of education. Starting in the 20th century, people have attempted to argue that small improvements in education can have large returns in such areas as health, wealth and well-being. For example, in Kerala, India in the 1950s, increases in women's health were correlated with increases in female literacy rates. In Iran, increased primary education was correlated with increased farming efficiencies and income. In both cases some researchers have concluded these correlations as representing an underlying causal relationship: education causes socio-economic benefits. In the case of Iran, researchers concluded that the improvements were due to farmers gaining reliable access to national crop prices and scientific farming information.
Strategies.
Reforms can be based on bringing education into alignment with a society's core values. Reforms that attempt to change a society's core values can connect alternative education initiatives with a network of other alternative institutions.
Digital Education.
The movement to use computers more in education naturally includes many unrelated ideas, methods, and pedagogies since there are many uses for digital computers. For example, the fact that computers are naturally good at math leads to the question of the use of calculators in math education. The Internet's communication capabilities make it potentially useful for collaboration, and foreign language learning. The computer's ability to simulate physical systems makes it potentially useful in teaching science. More often, however, debate of digital education reform centers around more general applications of computers to education, such as electronic test-taking and online classes.
The idea of creating artificial intelligence led some computer scientists to believe that teachers could be replaced by computers, through something like an expert system; however, attempts to accomplish this have predictably proved inflexible. The computer is now more understood to be a tool or assistant for the teacher and students.
Harnessing the richness of the Internet is another goal. In some cases classrooms have been moved entirely online, while in other instances the goal is more to learn how the Internet can be more than a classroom.
Web-based international educational software is under development by students at New York University, based on the belief that current educational institutions are too rigid: effective teaching is not routine, students are not passive, and questions of practice are not predictable or standardized. The software allows for courses tailored to an individual's abilities through frequent and automatic multiple intelligences assessments. Ultimate goals include assisting students to be intrinsically motivated to educate themselves, and aiding the student in self-actualization. Courses typically taught only in college are being reformatted so that they can be taught to any level of student, whereby elementary school students may learn the foundations of any topic they desire. Such a program has the potential to remove the bureaucratic inefficiencies of education in modern countries, and with the decreasing digital divide, help developing nations rapidly achieve a similar quality of education. With an open format similar to Wikipedia, any teacher may upload their courses online and a feedback system will help students choose relevant courses of the highest quality. Teachers can provide links in their digital courses to webcast videos of their lectures. Students will have personal academic profiles and a forum will allow students to pose complex questions, while simpler questions will be automatically answered by the software, which will bring you to a solution by searching through the knowledge database, which includes all available courses and topics.
The 21st century ushered in the acceptance and encouragement of internet research conducted on college and university campuses, in homes, and even in gathering areas of shopping centers. Addition of cyber cafes on campuses and coffee shops, loaning of communication devices from libraries, and availability of more portable technology devices, opened up a world of educational resources. Availability of knowledge to the elite had always been obvious, yet provision of networking devices, even wireless gadget sign-outs from libraries, made availability of information an expectation of most persons. Cassandra B. Whyte researched the future of computer use on higher education campuses focusing on student affairs. Though at first seen as a data collection and outcome reporting tool, the use of computer technology in the classrooms, meeting areas, and homes continued to unfold. The sole dependence on paper resources for subject information diminished and e-books and articles, as well as on-line courses, were anticipated to become increasingly staple and affordable choices provided by higher education institutions according to Whyte in a 2002 presentation.
Digitally "flipping" classrooms is a trend in digital education that has gained significant momentum. Will Richardson, author and visionary for the digital education realm, points to the not-so-distant future and the seemingly infinite possibilities for digital communication linked to improved education. Education on the whole, as a stand-alone entity, has been slow to embrace these changes. The use of web tools such as wikis, blogs, and social networking sites is tied to increasing overall effectiveness of digital education in schools. Examples exist of teacher and student success stories where learning has transcended the classroom and has reached far out into society.
Creativity is of the utmost importance when improving education. The "creative teachers" must have the confidence through training and availability of support and resources. These creative teachers are strongly encouraged to embrace a person-centered approach that develops the psychology of the educator ahead or in conjunction with the deployment of machines. Creative teachers have been also been inspired through Crowd-Accelerated Innovation. Crowd-Accelerated Innovation has pushed people to transition between media types and their understanding thereof at record-breaking paces. This process serves as a catalyst for creative direction and new methods of innovation. Innovation without desire and drive inevitably flat lines.
Mainstream media continues to be both very influential and the medium where Crowd-Accelerated Innovation gains its leverage. Media is in direct competition with formal educational institutions in shaping the minds of today and those of tomorrow. uchanan, Rachel footnot The media has been instrumental in pushing formal educational institutions to become savvier in their methods. Additionally, advertising has been (and continues to be) a vital force in shaping students and parents thought patterns.
Technology is a dynamic entity that is constantly in flux. As time presses on, new technologies will continue to break paradigms that will reshape human thinking regarding technological innovation. This concept stresses a certain disconnect between teachers and learners and the growing chasm that started some time ago. Richardson asserts that traditional classroom’s will essentially enter entropy unless teachers increase their comfort and proficiency with technology.
Administrators are not exempt from the technological disconnect. They must recognize the existence of a younger generation of teachers who were born during the Digital Age and are very comfortable with technology. However, when old meets new, especially in a mentoring situation, conflict seems inevitable. Ironically, the answer to the outdated mentor may be digital collaboration with worldwide mentor webs; composed of individuals with creative ideas for the classroom.
Another viable addition to digital education has been blended learning. In 2009, over 3 million K-12 students took an online course, compared to 2000 when 45,000 took an online course. Blended learning examples include pure online, blended, and traditional education. Research results show that the most effective learning takes place in a blended format. This allows children to view the lecture ahead of time and then spend class time practicing, refining, and applying what they have previously learned.
Notable reforms.
Some of the methods and reforms have gained permanent advocates, and are widely utilized.
Many educators now believe that anything that more precisely meets the needs of the child will work better. This was initiated by M. Montessori and is still utilized in Montessori schools. The teaching method must be teachable! This is a lesson from both Montessori and Dewey. This view now has very wide currency, and is used to select much of the curricula of teachers' colleges.
New programs based on modern learning theories that test individual learning, and teach to mastery of a subject have been proved by the Kentucky Education Reform Act (KERA) to be far more effective than group instruction with compromise schedules, or even class-size reduction.
Schools with limited resources, such as most public schools and most third-world and missionary schools, use a grammar-school approach. The evidence of Lancaster schools suggests using students as teachers. If the culture supports it, perhaps the economic discipline of the Lancaster school can reduce costs even further. However, much of the success of Lancaster's "school economy" was that the children were natives of an intensely mercantile culture.
In order to be effective, classroom instruction needs to change subjects at times near a typical student's attention span, which can be as frequently as every two minutes for young children. This is an important part of Marva Collins' method.
Substantial resources and time can be saved by permitting students to test out of classes. This also increases motivation, directs individual study, and reduces boredom and disciplinary problems.
To support inexpensive continuing adult education a community needs a free public library. It can start modestly as shelves in an attended shop or government building, with donated books. Attendants are essential to protect the books from vandalism. Adult education repays itself many times over by providing direct opportunity to adults. Free libraries are also powerful resources for schools and businesses.
A notable reform of the education system of Massachusetts occurred in 1993.
The current 'student voice' effort echoes past school reform initiatives focusing on parent involvement, community involvement, and other forms of participation in schools. However, it is finding a significant amount of success in schools because of the inherent differences: student voice is central to the daily schooling experience because students spend all day there. Many educators today strive for meaningful student involvement in their classrooms, while school administrators, school board members, and elected officials each lurch to hear what students have to say.

</doc>
<doc id="9621" url="https://en.wikipedia.org/wiki?curid=9621" title="Ellensburg, Washington">
Ellensburg, Washington

Ellensburg is a city in, and the county seat of, Kittitas County, Washington, United States. The population was 18,174 at the 2010 census. Ellensburg is located just east of the Cascade Range on Interstate 90 and is known as the most centrally located city in the state. Ellensburg is the home of Central Washington University (CWU).
The surrounding Kittitas Valley is internationally known for the timothy-hay that it produces. There are several local hay brokering and processing operations that ship to Pacific Rim countries. Downtown Ellensburg has many historic buildings, many of which were constructed in the late 19th century. This is a legacy of its bid to be the state capital, which it lost to Olympia. CWU being placed there is another product of that legacy. The state legislature selected Ellensburg as the location for the then Normal School as a consolation prize. Eastern Washington has a much drier climate than Western Washington, and some Seattle-area residents have moved to the city and commute over Snoqualmie Pass on Interstate 90 to jobs located in the Puget Sound region.
History.
Ellensburg was officially incorporated on November 26, 1883. John Alden Shoudy came to the Kittitas Valley in 1871, and purchased a small trading post from Andrew Jackson "A.J." Splawn, called "Robber's Roost." Robber's Roost was the first business in the valley, other than the early trading that occurred among American Indians, cattle drivers, trappers, and miners. Robber's Roost was located on the present-day 3rd Avenue, just west of Main Street near the alley. There is a placard on the wall commemorating the location, as well as a small stone monument against the wall on the sidewalk. Shoudy named the town after his wife, Mary Ellen Shoudy, thus officially began the city of Ellensburgh around 1872. Shoudy was not the first settler in the Kittitas Valley, nor was he the first businessperson, but he was responsible for platting the city of Ellensburgh in the 1870s, and he was the person who named the streets in the downtown district.
The city was originally named Ellensburgh, until the final -"h" was dropped under standardization pressure from the United States Postal Service and Board of Geography Names in 1894.
There were several early newspapers in Ellensburg. The Daily Record, however, began in 1909 and is the name of the local newspaper today.
Concerns over the state of Ellensburg's historic downtown led to the formation of the Ellensburg Downtown Association to work on revitalizing the area.
Arts and culture.
The City of Ellensburg is home to a number of local art museums and galleries: 
Every first Friday of each month, Ellensburg hosts First Friday Art Walk from 5:00 to 7:00 pm. This downtown event showcases art of all forms. The local businesses, galleries and museums come alive with art, music, wine and people as they celebrate art in the community.
Geography.
According to the United States Census Bureau, the city has a total area of , of which is land and is water.
Climate.
Ellensburg experiences a semi-arid climate (Köppen "BSk").
Demographics.
2010 census.
As of the census of 2010, there were 18,174 people, 7,301 households, and 2,889 families residing in the city. The population density was . There were 7,867 housing units at an average density of . The racial makeup of the city was 85.7% White, 1.5% African American, 1.0% Native American, 3.2% Asian, 0.2% Pacific Islander, 4.6% from other races, and 3.7% from two or more races. Hispanic or Latino of any race were 9.7% of the population.
There were 7,301 households, of which 19.3% had children under the age of 18 living with them, 28.2% were married couples living together, 8.2% had a female householder with no husband present, 3.1% had a male householder with no wife present, and 60.4% were non-families. 35.1% of all households were made up of individuals and 9.6% had someone living alone who was 65 years of age or older. The average household size was 2.16 and the average family size was 2.86.
The median age in the city was 23.5 years. 14.2% of residents were under the age of 18; 41.2% were between the ages of 18 and 24; 21.8% were from 25 to 44; 13.9% were from 45 to 64; and 8.9% were 65 years of age or older. The gender makeup of the city was 50.1% male and 49.9% female.
2000 census.
As of the census of 2000, there were 15,414 people, 6,249 households, and 2,649 families residing in the city. The population density was 2,338.9 people per square mile (903.1/km²). There were 6,732 housing units at an average density of 1,021.5 per square mile (394.4/km²). The racial makeup of the city was 88.07% White, 1.17% Black or African American, 0.95% Native American, 4.09% Asian, 0.16% Pacific Islander, 2.86% from other races, and 2.69% from two or more races. 6.33% of the population were Hispanic or Latino of any race.
There were 6,249 households, of which 20.8% had children under the age of 18 living with them, 31.4% were married couples living together, 8.1% had a female householder with no husband present, and 57.6% were non-families. 35.5% of all households were made up of individuals and 9.1% had someone living alone who was 65 years of age or older. The average household size was 2.12 and the average family size was 2.84.
In the city, the population was spread out with 15.8% under the age of 18, 39.3% from 18 to 24, 22.7% from 25 to 44, 12.8% from 45 to 64, and 9.4% who were 65 years of age or older. The median age was 24 years. For every 100 females there were 95.0 males. For every 100 females age 18 and over, there were 93.1 males.
The median income for a household in the city was $20,034, and the median income for a family was $37,625. Males had a median income of $31,022 versus $22,829 for females. The per capita income for the city was $13,662. About 18.8% of families and 34.3% of the population were below the poverty line, including 29.0% of those under age 18 and 11.2% of those age 65 or over.
Politics and government.
The City of Ellensburg uses the Manager/Council form of government with a City Manager hired by the City Council. The seven-member City Council is elected at large and serve 4-year terms. The City Council elects a Mayor and Deputy Mayor from the Council to serve 2-year terms. The Council meets the first and third Monday of each month, at 7:00 pm, in the City Council Chambers at City Hall.
On the state legislative level, Ellensburg is in the 13th district. As of 2013, its state senator is Republican Janéa Holmquist Newbry, and its two state representatives are Republicans Judy Warnick and Matt Manweller. On the congressional level, Ellensburg is located in Washington's 8th congressional district and is represented by Republican Dave Reichert.
Kittitas County supported Senator John McCain of Arizona over then-Senator Barack Obama of Illinois in the 2008 presidential election, with both receiving 53 and 45 percent of the vote, respectively. Then, in the 2010 Senate race, Republican Dino Rossi carried the city over Democratic Senator Patty Murray with 50.7 percent of the vote to Murray's 49.3 percent.
Education.
Public schools.
Public schools are operated by Ellensburg School District 401. The district includes one high school (Ellensburg High School), one middle school, and three elementary schools.

</doc>
<doc id="9623" url="https://en.wikipedia.org/wiki?curid=9623" title="Eugene, Oregon">
Eugene, Oregon

Eugene ( ) is a city of the Pacific Northwest located in the U.S. state of Oregon. It is located at the south end of the Willamette Valley, near the confluence of the McKenzie and Willamette rivers, about east of the Oregon Coast.
As of the 2010 census, Eugene had a population of 156,185; It is the second most populous city in the state (after Portland and before Salem) and the county seat of Lane County. The Eugene-Springfield, Oregon metropolitan statistical area (MSA) is the 146th largest metropolitan statistical area in the U.S., and the third-largest in the state, behind the Portland Metropolitan Area and the Salem Metropolitan Area. The city's population for 2014 was estimated to be 160,561 by the U.S. Census.
Eugene is home to the University of Oregon and Lane Community College. The city is also noted for its natural beauty, recreational opportunities (especially bicycling, running/jogging, rafting, kayaking), and focus on the arts. Eugene's official slogan is "A Great City for the Arts and Outdoors". It is also referred to as the "Emerald City", and as "Track Town, USA". The Nike corporation had its beginnings in Eugene. In 2021, the city will host the 18th Track and Field World Championships.
History.
Eugene is named after its founder, Eugene Franklin Skinner. Until 1889, it was named Eugene City. In 1846, Skinner erected the first cabin in the area. It was used as a trading post and was registered as an official post office on January 8, 1850. At this time the settlement was known as Skinner's Mudhole. It was relocated in 1853 and named Eugene City, but was not formally incorporated as a city until 1862. Skinner later ran a ferry service across the Willamette River where the Ferry Street Bridge now stands.
The first major educational institution in the area was Columbia College, founded a few years earlier than the University of Oregon. It fell victim to two major fires in four years, and after the second fire, the college decided not to rebuild again. The part of south Eugene known as College Hill was the former location of Columbia College. There is no college there today.
The town raised the initial funding to start a public university, which later became the University of Oregon, with the hope of turning the small town into a center of learning. In 1872, the Legislative Assembly passed a bill creating the University of Oregon as a state institution. Eugene bested the nearby town of Albany in the competition for the state university. In 1873, community member J.H.D. Henderson donated the hilltop land for the campus, overlooking the city.
The university first opened in 1876 with the regents electing the first faculty and naming John Wesley Johnson as president. The first students registered on October 16, 1876. The first building was completed in 1877; it was named Deady Hall in honor of the first Board of Regents President and community leader Judge Matthew P. Deady. The city's name was shortened from Eugene City to Eugene in 1889.
Eugene grew rapidly throughout most of the twentieth century, with the exception being the early 1980s when a downturn in the timber industry caused high unemployment. By 1985, the industry had recovered and Eugene began to attract more high-tech industries.
Geography and climate.
Geography.
According to the United States Census Bureau, the city has a total area of , of which, is land and is water. Eugene is located at an elevation of .
To the north of downtown is Skinner Butte. Northeast of the city are the Coburg Hills. Spencer Butte is a prominent landmark south of the city. Mount Pisgah is southeast of Eugene and includes Mount Pisgah Arboretum and Howard Buford Recreation Area, a Lane County Park. Eugene is surrounded by foothills and forests to the south, east and west, while to the north the land levels out into the Willamette Valley and consists of mostly farmland.
The Willamette and McKenzie rivers run through Eugene and neighboring city, Springfield. Another important stream is Amazon Creek, whose headwaters are near Spencer Butte. The creek discharges west of the city into Fern Ridge Reservoir, maintained for winter flood control by the Army Corps of Engineers. Eugene Yacht Club hosts a sail school and sailing regattas at Fern Ridge during summer months.
Neighborhoods.
Eugene has 23 neighborhood associations:
Climate.
Like the rest of the Willamette Valley, Eugene lies in the Marine West Coast climate zone, with Mediterranean characteristics. Under the Köppen climate classification scheme, Eugene has a subtropical dry summer climate (Köppen "Csb"). Temperatures can vary from cool to warm, with warm, dry summers and cool, wet winters. Spring and fall are also moist seasons, with light rain falling for long periods. Winter snowfall does occur, but it is sporadic and rarely accumulates in large amounts: the average seasonal amount is , and the median is 0. The record snowfall was deep due to a pineapple express in late January 1969. The record snowfall for March was deep in 2012.
The hottest months are July and August, with average highs of around , with an average of 15 days per year above . The coolest month is December, with the average daytime high in the mid-40s°F (7–8 °C), and nights averaging just above freezing. There are 54 nights per year with a low below freezing, and about three days with highs not exceeding freezing. The record high low was in 2006.
Eugene's average annual temperature is , and annual precipitation at . Eugene is slightly cooler on average than Portland. Despite being located about south and having only a slightly higher elevation, Eugene has a more continental climate, less subject to the maritime air that blows inland from the Pacific Ocean via the Columbia River. Eugene's average August low is , while Portland's average August low is . Average winter temperatures (and summer high temperatures) are similar for the two cities. This disparity may be additionally caused by Portland's urban heat island, where the combination of black pavement and urban energy use raises nighttime temperatures. A lesser heat island may also exist in the immediate downtown of Eugene.
Extreme temperatures range from , recorded on December 8, 1972, to on August 9, 1981.
Air quality and allergies.
Eugene is downwind of Willamette Valley grass seed farms, a $500 million industry. The combination of summer grass pollen and the confining shape of the hills around Eugene make it "the area of the highest grass pollen counts in the USA (>1,500 pollen grains/m of air)." These high pollen counts have led to difficulties for some track athletes who compete in Eugene. In the Olympic trials in 1972, "Jim Ryun won the 1,500 after being flown in by helicopter because he was allergic to Eugene's grass seed pollen." Further, six-time Olympian Maria Mutola abandoned Eugene as a training area "in part to avoid allergies".
Demographics.
2010 census.
According to the 2010 census, Eugene's population was 156,185. The population density was 3,572.2 people per square mile. There were 69,951 housing units at an average density of 1,600 per square mile. Those age 18 and over accounted for 81.8% of the total population.
The racial makeup of the city was 85.8% White, 4.0% Asian, 1.4% Black or African American, 1.0% Native American, 0.2% Pacific Islander, and 4.7% from other races.
Hispanics and Latinos of any race accounted for 7.8% of the total population. Of the non-Hispanics, 82% were White, 1.3% Black or African American, 0.8% Native American, 4% Asian, 0.2% Pacific Islander, 0.2% some other race alone, and 3.4% were of two or more races.
Females represented 51.1% of the total population, and males represented 48.9%. The median age in the city was 33.8 years.
2000 census.
The census of 2000 showed that there were 137,893 people, 58,110 households, and 31,321 families residing in the city of Eugene. The population density was 3,404.8 people per square mile (1,314.5/km²). There were 61,444 housing units at an average density of 1,516.4 per square mile (585.5/km²). The racial makeup of the city was 88.15% White, down from 99.5% in 1950, 3.57% Asian, 1.25% Black or African American, 0.93% Native American, 0.21% Pacific Islander, 2.18% from other races, and 3.72% from two or more races. 4.96% of the population were Hispanic or Latino of any race.
There were 58,110 households, of which 25.8% had children under the age of 18 living with them, 40.6% were married couples living together, 9.7% had a female householder with no husband present, and 46.1% were non-families. 31.7% of all households were made up of individuals and 9.4% had someone living alone who was 65 years of age or older. The average household size was 2.27 and the average family size was 2.87. In the city, the population was 20.3% under the age of 18, 17.3% from 18 to 24, 28.5% from 25 to 44, 21.8% from 45 to 64, and 12.1% who were 65 years of age or older. The median age was 33 years. For every 100 females, there were 96.0 males. For every 100 females age 18 and over, there were 94.0 males. The median income for a household in the city was $35,850, and the median income for a family was $48,527. Males had a median income of $35,549 versus $26,721 for females. The per capita income for the city was $21,315. About 8.7% of families and 17.1% of the population were below the poverty line, including 14.8% of those under age 18 and 7.1% of those age 65 or over.
Religion.
Religious institutions of higher learning in Eugene include Northwest Christian University and New Hope Christian College. Northwest Christian University (formerly Northwest Christian College), founded in 1895, has ties with the Christian Church (Disciples of Christ). New Hope Christian College (formerly Eugene Bible College) originated with the Bible Standard Conference in 1915, which joined with Open Bible Evangelistic Association to create Open Bible Standard Churches in 1932. Eugene Bible College was started from this movement by Fred Hornshuh in 1925.
There are two Eastern Orthodox Church parishes in Eugene: St John the Wonderworker Orthodox Christian Church in the Historic Whiteaker Neighborhood and Saint George Greek Orthodox Church.
There are six Roman Catholic parishes in Eugene as well: St. Mary Catholic Church, St. Jude Catholic Church, St. Mark Catholic Church, St. Peter Catholic Church, St. Paul Catholic Church, and St. Thomas More Catholic Church.
Eugene also has a Ukrainian Catholic Church named Nativity of the Mother of God
There is a mainline Protestant contingency in the city as well—such as the largest of the Lutheran Churches, Central Lutheran near the U of O Campus and the Episcopal Church of the Resurrection.
The Eugene area has a sizeable LDS Church presence, with three stakes, consisting of 23 congregations (wards and branches). The Portland Oregon Temple is the nearest temple.
The greater Eugene-Springfield area also has a Jehovah's Witnesses presence with five Kingdom Halls, several having multiple congregations in one Kingdom Hall.
The Reconstructionist Temple Beth Israel is Eugene's largest Jewish congregation. It was also, for many decades, Eugene's only synagogue, until Orthodox members broke away in 1992 and formed "Congregation Ahavas Torah".
Eugene has a community of some 140 Sikhs, who have established a Sikh temple.
The 340-member congregation of the Unitarian Universalist Church in Eugene (UUCE) purchased the former Eugene Scottish Rite Temple in May 2010, renovated it, and began services there in September 2012.
Saraha Nyingma Buddhist Temple in Eugene opened in 2012 in the former site of the Unitarian Universalist Church.
Anarchism.
Some Eugene anarchists gained international notoriety in 1999 for their perceived role in the violent protests at the WTO Conference in Seattle. Eugene resident John Zerzan, an editor of the "Green Anarchy" magazine, has been associated with the growth of the green anarchist movement and with the philosophy behind black bloc tactics of the Seattle riots. During a Reclaim the Streets event in 1999, some protesters blocked downtown streets and smashed the windows of three stores and threw stones and bottles at police. Following those protests, then-mayor Jim Torrey described the city as "the anarchist capital of the United States."
In January 2006, the FBI conducted Operation Backfire, leading to federal indictment of eleven people, all members of a Eugene-based cell of the Earth Liberation Front (ELF). Operation Backfire was the largest investigation into radical underground environmental groups in United States history. Ongoing trials of accused eco-terrorists kept Eugene in the spotlight for a few years.
Economy.
Eugene's largest employers are PeaceHealth Medical Group, the University of Oregon and the Eugene School District. Eugene's largest industries are wood products manufacturing and recreational vehicle manufacturing.
Luckey's Club Cigar Store is one of the oldest bars in Oregon. Tad Luckey, Sr., purchased it in 1911, making it one of the oldest businesses in Eugene. The “Club Cigar,” as it was called in the late 19th century, was for many years a men-only salon. It survived both the Great Depression and Prohibition, partly because Eugene was a dry town before the end of Prohibition.
Corporate headquarters for the employee-owned Bi-Mart corporation and family-owned Market of Choice remain located in Eugene. Emporium Department Stores, which was founded in North Bend, Oregon, had its headquarters in Eugene, but closed all stores in 2002.
Organically Grown Company, the largest distributor of organic fruits and vegetables in the northwest, started in Eugene in 1978 as a non-profit co-op for organic farmers. Notable local food processors, many of whom manufacture certified organic products, include Golden Temple (Yogi Tea), Merry Hempsters and Springfield Creamery (Nancy's Yogurt & owned by the Kesey Family), and Mountain Rose Herbs.
Until July 2008, Hynix Semiconductor America had operated a large semiconductor plant in west Eugene. In late September 2009, Uni-Chem of South Korea announced its intention to purchase the Hynix site for solar cell manufacturing. However, this deal fell through and as of late 2012 is no longer planned.
The footwear repair product Shoe Goo is manufactured by Eclectic Products, based in Eugene.
Burley Design LLC produces bicycle trailers, and was founded in Eugene by Alan Scholz out of a Saturday Market business in 1978. Eugene is also the birthplace and home of Bike Friday bicycle manufacturer, Green Gear Cycling.
Many multinational businesses were launched in Eugene. Some of the most famous include Nike, Taco Time, and Brøderbund Software.
Top employers.
According to Eugene's 2014 Comprehensive Annual Financial Report, the city's top employers are:
Arts and culture.
Eugene has a significant population of people in pursuit of alternative ideas, and a large original hippie population. Beginning in the 1960s, the countercultural ideas and viewpoints espoused by Ken Kesey became established as the seminal elements of the vibrant social tapestry that continue to define Eugene. The Merry Prankster, as Kesey was known, has arguably left the most indelible imprint of any cultural icon in his hometown. He is best known as the author of "One Flew Over the Cuckoo's Nest" and as the male protagonist in Tom Wolfe's "The Electric Kool-Aid Acid Test".
In 2005, the city council unanimously approved a new slogan for the city, "World's Greatest City for the Arts & Outdoors." While Eugene has a vibrant arts community for a city its size, and is well situated near many outdoor opportunities, this slogan was frequently criticized by locals as embarrassing and ludicrous. In early 2010, the slogan was changed to "A Great City for the Arts & Outdoors."
Eugene's Saturday Market, open every Saturday from April through November, was founded in 1970 as the first "Saturday Market" in the United States. It is adjacent to the Lane County Farmer's Market in downtown Eugene. All vendors must create or grow all their own products. The market reappears as the "Holiday Market" between Thanksgiving and New Years in the Lane County Events Center at the fairgrounds.
Community.
Eugene is noted for its "community inventiveness." Many U.S. trends in community development originated in Eugene. The University of Oregon's participatory planning process, known as The Oregon Experiment, was the result of student protests in the early 1970s. The book of the same name is a major document in modern enlightenment thinking in planning and architectural circles. The process, still used by the university in modified form, was created by Christopher Alexander, whose works also directly inspired the creation of the Wiki. Some research for the book "A Pattern Language", which inspired the Design Patterns movement and Extreme Programming, was done by Alexander in Eugene. Not coincidentally, those engineering movements also had origins here. Decades after its publication, "A Pattern Language" is still one of the best-selling books on urban design.
In the 1970s, Eugene was packed with cooperative and community projects. It still has small natural food stores in many neighborhoods, some of the oldest student cooperatives in the country, and alternative schools have been part of the school district since 1971. The old Grower's Market, downtown near the Amtrak depot, is the only food cooperative in the U.S. with no employees. It is possible to see Eugene's trend-setting non-profit tendencies in much newer projects, such as the Tango Center and the Center for Appropriate Transport. In 2006, an initiative began to create a tenant-run development process for downtown Eugene.
In the fall of 2003, neighbors noticed that "an unassuming two-acre remnant orchard tucked into the Friendly Area Neighborhood" had been put up for sale by its owner, a resident of New York City. Learning that a prospective buyer had plans to build several houses on the property, they formed a nonprofit organization called Madison Meadow in June 2004 in order to buy the property and "preserve it as undeveloped space in perpetuity." In 2007 their effort was named Third Best Community Effort by the "Eugene Weekly", and by the end of 2008 they had raised enough money to purchase the property.
The City of Eugene has an active Neighborhood Program. Several neighborhoods are known for their green activism. Friendly Neighborhood has a highly popular neighborhood garden established on the right of way of a street never built. There are a number of community gardens on public property. Amazon Neighborhood has a former church turned into a community center. Whiteaker hosts a housing co-op that dates from the early 1970s that has re-purposed both their parking lots into food production and play space. An unusual eco-village with natural building techniques and large shared garden can be found in Jefferson Westside neighborhood. A several block area in the River Road Neighborhood is known as a permaculture hotspot with an increasing number of suburban homes trading grass for garden, installing rain water catchment systems, food producing landscapes and solar retrofits. Several sites have planted gardens by removing driveways. A 65-tree filbert grove on public property is being restored by citizen volunteers in cooperation with the city of Eugene. There are deepening social and economic networks in the neighborhood.
Museums.
Eugene museums include the University of Oregon's Jordan Schnitzer Museum of Art and Museum of Natural and Cultural History, the Oregon Air and Space Museum, Conger Street Clock Museum, Lane County Historical Museum, Maude Kerns Art Center, Shelton McMurphey Johnson House, and the Science Factory Children's Museum & Planetarium.
Performing arts.
Eugene is home to numerous cultural organizations, including the Eugene Symphony, the Eugene Ballet, the Eugene Opera, the Eugene Concert Choir, the Northwest Christian University Community Choir, the Oregon Mozart Players, the Oregon Bach Festival, the Oregon Children's Choir, the Eugene Youth Symphony, Ballet Fantastique and Oregon Festival of American Music. Principal performing arts venues include the Hult Center for the Performing Arts, The John G. Shedd Institute for the Arts ("The Shedd"), Matthew Knight Arena, Beall Concert Hall and the Erb Memorial Union ballroom on the University of Oregon campus, the McDonald Theatre, and W.O.W. Hall.
A number of live theater groups are based in Eugene, including Free Shakespeare in the Park, Oregon Contemporary Theatre, The Very Little Theatre, Actors Cabaret, LCC Theatre, and University Theatre. Each has its own performance venue.
Music.
Because of its status as a college town, Eugene has been home to many music genres, musicians and bands, ranging from electronic dance music such as dubstep and drum and bass to garage rock, hip hop, folk and heavy metal. Eugene also has a growing reggae and street-performing bluegrass and jug band scene. Multi-genre act the Cherry Poppin' Daddies became a prominent figure in Eugene's music scene and became the house band at Eugene's W.O.W. Hall. In the late 1990s, their contributions to the swing revival movement propelled them to national stardom. Rock band Floater originated in Eugene. Doom metal band YOB is among the leaders of the Eugene heavy music scene.
Eugene is home to "Classical Gas" Composer and two-time Grammy award winner Mason Williams who spent his years as a youth living between his parents in Oakridge, Oregon and Oklahoma. Mason Williams puts on a yearly Christmas show at the Hult center for performing arts with a full orchestra produced by author, audio engineer and University of Oregon professor Don Latarski.
Dick Hyman, noted jazz pianist and musical director for many of Woody Allen's films, designs and hosts the annual Now Hear This! jazz festival at the Oregon Festival of American Music (OFAM). OFAM and the Hult Center routinely draw major jazz talent for concerts.
Eugene is also home to a large Zimbabwean music community. Kutsinhira Cultural Arts Center, which is "dedicated to the music and people of Zimbabwe," is based in Eugene.
Visual arts.
Eugene's visual arts community is supported by over 20 private art galleries and several organizations, including Maude Kerns Art Center, Lane Arts Council, DIVA (the Downtown Initiative for the Visual Arts), the Hult Center's Jacobs Gallery, and the Eugene Glass School.
Annual visual arts events include the Mayor's Art Show and Art and the Vineyard.
Film.
The Eugene area has been used as a filming location for several Hollywood films, most famously for 1978's "National Lampoon's Animal House", which was also filmed in nearby Cottage Grove. John Belushi had the idea for the film "The Blues Brothers" during filming of "Animal House" when he happened to meet Curtis Salgado at what was then the Eugene Hotel.
"Getting Straight", starring Elliott Gould and Candice Bergen, was filmed at Lane Community College in 1969. As the campus was still under construction at the time, the "occupation scenes" were easier to shoot.
The "Chicken Salad on Toast" scene in the 1970 Jack Nicholson movie "Five Easy Pieces" was filmed at the Denny's restaurant at the southern I-5 freeway interchange near Glenwood. Nicholson directed the 1971 film "Drive, He Said" in Eugene.
"How to Beat the High Co$t of Living", starring Jane Curtin, Jessica Lange and Susan St. James, was filmed in Eugene in the fall of 1979. Locations visible in the film include Valley River Center (which is a driving force in the plot), Skinner Butte and Ya-Po-Ah Terrace, the Willamette River and River Road Hardware.
Several track and field movies have used Eugene as a setting and/or a filming location. "Personal Best", starring Mariel Hemingway, was filmed in Eugene in 1982. The film centered on a group of women who are trying to qualify for the Olympic track and field team. Two track and field movies about the life of Steve Prefontaine, "Prefontaine" and "Without Limits" were released within a year of each other in 1997–1998. Kenny Moore, Eugene-trained Olympic runner and co-star in "Prefontaine", co-wrote the screenplay for "Without Limits". "Prefontaine" was filmed in Washington because the "Without Limits" production bought out Hayward Field for the summer to prevent its competition from shooting there. Kenny Moore also wrote a biography of Bill Bowerman, played in "Without Limits" by Donald Sutherland back in Eugene 20 years after he had appeared in "Animal House". Moore had also had a role in "Personal Best".
"Stealing Time", a 2003 independent film, was partially filmed in Eugene. When the film premiered in June 2001 at the Seattle International Film Festival, it was titled "Rennie's Landing" after a popular bar near the University of Oregon campus. The title was changed for its DVD release. "Zerophilia" was filmed in Eugene in 2006.
Sports.
Eugene's Oregon Ducks are part of the Pac-12 Conference (Pac-12). American football is especially popular, with intense rivalries between the Ducks and both the Oregon State University Beavers and the University of Washington Huskies. Autzen Stadium is home to Duck football, with a seating capacity of 54,000 but has had over 60,000 with standing room only.
The basketball arena, McArthur Court, was built in 1926. The arena was replaced by the Matthew Knight Arena in late 2010.
For nearly 40 years, Eugene has been the "Track and Field Capital of the World." Oregon's most famous track icon is the late world-class distance runner Steve Prefontaine, who was killed in a car crash in 1975.
Eugene's jogging trails include Pre's Trail in Alton Baker Park, Rexius Trail, the Adidas Oregon Trail, and the Ridgeline Trail. Jogging was introduced to the U.S. through Eugene, brought from New Zealand by Bill Bowerman, who wrote the best-selling book "Jogging", and coached the champion University of Oregon track and cross country teams. During Bowerman's tenure, his "Men of Oregon" won 24 individual NCAA titles, including titles in 15 out of the 19 events contested. During Bowerman's 24 years at Oregon, his track teams finished in the top ten at the NCAA championships 16 times, including four team titles (1962, '64, '65, '70), and two second-place trophies. His teams also posted a dual meet record of 114–20.
Bowerman also invented the waffle sole for running shoes in Eugene, and with Oregon alumnus Phil Knight founded shoe giant Nike. Eugene's miles of running trails, through its unusually large park system, are the most extensive in the U.S. The city has dozens of running clubs. The climate is cool and temperate, good both for jogging and record-setting. Eugene is home to the University of Oregon's Hayward Field track, which hosts numerous collegiate and amateur track and field meets throughout the year, most notably the Prefontaine Classic. Hayward Field was host to the 2004 AAU Junior Olympic Games, the 1989 World Masters Athletics Championships, the track and field events of the 1998 World Masters Games, the 2006 Pacific-10 track and field championships, the 1971, 1975, 1986, 1993, 1999, 2001, 2009, and 2011 USA Outdoor Track and Field Championships and the 1972, 1976, 1980, 2008, and 2012 U.S. Olympic trials, and is designated to host them again in 2016.
On April 16, 2015, it was announced by the IAAF that Eugene had been awarded the right to host the 2021 World Championships in Athletics. The city had previously bid for the 2019 event but lost narrowly to Doha, Qatar.
Eugene is also home to the Eugene Emeralds, a short-season Class A minor-league baseball team. The "Ems" play their home games in PK Park, also the home of the University of Oregon baseball team.
The Nationwide Tour's golfing event Oregon Classic takes place at Shadow Hills Country Club, just north of Eugene. The event has been played every year since 1998, except in 2001 when it was slated to begin the day after the 9/11 terrorist attacks. The top 20 players from the Nationwide Tour are promoted to the PGA Tour for the following year.
The Eugene Jr. Generals, a Tier III Junior "A" ice hockey team belonging to the Northern Pacific Hockey League (NPHL) consisting of 8 teams throughout Oregon and Washington, plays at the Lane County Ice Center.
The following table lists some sports clubs in Eugene and their usual home venue:
Parks and recreation.
Spencer Butte Park at the southern edge of town provides access to Spencer Butte, a dominant feature of Eugene's skyline. Hendricks Park, situated on a knoll to the east of downtown, is known for its rhododendron garden and nearby memorial to Steve Prefontaine, known as Pre's Rock, where the legendary University of Oregon runner was killed in an auto accident. Alton Baker Park, next to the Willamette River, contains Pre's Trail. Also located next to the Willamette are Skinner Butte Park and the Owen Memorial Rose Garden, which is home to more than 4,500 roses of over 400 varieties, as well as the 150-year-old Black Tartarian Cherry tree, an Oregon Heritage Tree.
The city of Eugene maintains an urban forest. The University of Oregon campus is an arboretum, with over 500 species of trees. The city operates and maintains scenic hiking trails that pass through and across the ridges of a cluster of hills in the southern portion of the city, on the fringe of residential neighborhoods. Some trails allow biking and others are for hikers and runners only.
The nearest ski resort, Willamette Pass, is one hour from Eugene by car. On the way, along Oregon Route 58, are several reservoirs and lakes, the Oakridge mountain bike trails, hot springs, and waterfalls within Willamette National Forest. Eugene residents also frequent Hoodoo and Mount Bachelor ski resorts. The Three Sisters Wilderness, the Oregon Dunes National Recreation Area and Smith Rock are just a short drive away.
Government.
In 1944, Eugene adopted a council-manager form of government, replacing the day-to-day management of city affairs by the part-time mayor and volunteer city council with a full-time professional city manager. The subsequent history of Eugene city government has largely been one of the dynamics—often contentious—between the city manager, the mayor and city council.
The current mayor of Eugene is Kitty Piercy, who has been in office since 2005. Recent mayors include Edwin Cone (1958–69), Les Anderson (1969–77) Gus Keller (1977–84), Brian Obie (1985–88), Jeff Miller (1989–92), Ruth Bascom (1993–96), and Jim Torrey (1997–2004).
Jon Ruiz has been the city manager since April 2008. Ten other people have held the city manager position. They were: Deane Seeger (1945–49), Oren King (1949–53), Robert Finlayson (1953–59), Hugh McKinley (1959–75), Charles Henry (1975–80), Mike Gleason (1981–96), Vicki Elmer (1996–98), Jim Johnson (1998–2002), Dennis Taylor (2002–2007), Angel Jones (2007–2008).
Eugene City Council.
Mayor: Kitty Piercy
Public safety.
The Eugene Police Department (EPD) is the city's law enforcement and public safety agency. The Lane County Sheriff's Office also has its headquarters in Eugene.
The University of Oregon is served by the University of Oregon Police Department (UOPD), and EPD has a police station in the West University District near campus. Lane Community College is served by the Lane Community College Public Safety Department. The Oregon State Police have a presence in the rural areas and highways around the Eugene metro area. The LTD downtown station, and the EmX lines are patrolled by G4S Transit Officers.
Eugene City Hall was abandoned in 2012 for reasons of structural integrity, energy efficiency, and obsolete size. Various offices of city government became tenants in eight other buildings.
Education.
Eugene is home to the University of Oregon. Other institutions of higher learning include Northwest Christian University, Lane Community College, New Hope Christian College, Gutenberg College, and Pacific University's Eugene campus.
Public schools.
The Eugene School District includes four full-service high schools (Churchill, North Eugene, Sheldon, and South Eugene) and many alternative education programs, such as international schools and charter schools. Foreign language immersion programs in the district are available in Spanish, French, and Japanese.
The Bethel School District serves children in the Bethel neighborhood on the northwest edge of Eugene. The district is home to the traditional Willamette High School and the alternative Kalapuya High School. There are 11 schools in this district.
Eugene also has several private schools, including the Eugene Waldorf School, two private Montessori schools: Eugene Montessori and Far Horizon Montessori, Eugene Sudbury School, Wellsprings Friends School, Oak Hill School, and The Little French School.
Parochial schools in Eugene include Marist Catholic High School, O'Hara Catholic Elementary School, and St. Paul Parish School.
Libraries.
The largest library in Oregon is the University of Oregon's Knight Library, with collections totaling more than 3 million volumes and over 100,000 audio and video items. The Eugene Public Library moved into a new, larger building downtown in 2002. The four-story library is an increase from . There are also two branches of the Eugene Public Library, the Sheldon Branch Library in the neighborhood of Cal Young/Sheldon, and the Bethel Branch Library, in the neighborhood of Bethel. Eugene also has the Lane County Law Library.
Media.
Print.
The largest newspaper serving the area is "The Register-Guard", a daily newspaper with a circulation of about 70,000, published independently by the Baker family of Eugene. Other newspapers serving the area include the "Eugene Weekly", the "Emerald", the student-run independent newspaper at the University of Oregon, now published on Mondays and Thursdays;"The Torch", the student-run newspaper at Lane Community College, the "Ignite", the newspaper at New Hope Christian College and "The Mishpat," the student-run newspaper at Northwest Christian University. "Eugene Magazine", "Lifestyle Quarterly", "Eugene Living", and "Sustainable Home and Garden" magazines also serve the area. "Adelante Latino" is a Spanish language newspaper in Eugene that serves all of Lane County.
Television.
Local television stations include KMTR (NBC), KVAL (CBS), KLSR-TV (Fox), KEVU-CD, KEZI (ABC), KEPB (PBS), and KTVC (independent).
Radio.
The local NPR affiliates are KOPB, and KLCC. Radio station KRVM is an affiliate of Jefferson Public Radio, based at Southern Oregon University. The Pacifica Radio affiliate is the University of Oregon student-run radio station, KWVA. Additionally, the community supports two other radio stations: KWAX (classical) and KRVM-FM (alternative).
AM Stations
FM Stations
Infrastructure.
Transportation.
Bus.
Lane Transit District (LTD), a public transportation agency formed in 1970, covers of Lane County, including Creswell, Cottage Grove, Junction City, Veneta, and Blue River. Operating more than 90 buses during peak hours, LTD carries riders on 3.7 million trips every year. LTD also operates a bus rapid transit line that runs between Eugene and Springfield—Emerald Express (EmX)—much of which runs in its own lane. LTD's main terminus in Eugene is at the Eugene Station. LTD also offers paratransit.
Cycling.
Cycling is popular in Eugene and many people commute via bicycle. Summertime events and festivals frequently have bike parking "corrals" that many times are filled to capacity by three hundred or more bikes. Many people commute to work by bicycle every month of the year. Numerous bike shops provide the finest rain gear products, running lights and everything a biker needs to ride and stay comfortable in the damp, misty climate. Bike trails take commuting and recreational bikers along the Willamette River past a scenic rose garden, along Amazon Creek, through the downtown, and through the University of Oregon campus.
In 2009, the League of American Bicyclists cited Eugene as 1 of 10 "Gold-level" cities in the U.S. because of its "remarkable commitments to bicycling." In 2010, "Bicycling" magazine named Eugene the 5th most bike-friendly city in America. The U.S. Census Bureau's annual American Community Survey reported that Eugene had a bicycle commuting mode share of 7.3% in 2011, the fifth highest percentage nationwide among U.S. cities with 65,000 people or more, and 13 times higher than the national average of 0.56%.
Rail.
The 1908 Amtrak depot downtown was restored in 2004; it is the southern terminus for two daily runs of the Amtrak "Cascades", and a stop along the route in each direction for the daily "Coast Starlight".
Air travel.
Air travel is served by the Eugene Airport, also known as Mahlon Sweet Field, which is the fifth largest airport in the Northwest and second largest airport in Oregon. The Eugene Metro area also has numerous private airports. The Eugene Metro area also has several heliports, such as the Sacred Heart Medical Center Heliport and Mahlon Sweet Field Heliport, and many single helipads.
Highways.
Highways traveling within and through Eugene include:
Utilities.
Eugene is the home of Oregon's largest publicly owned water and power utility, the Eugene Water & Electric Board (EWEB). EWEB got its start in the first decade of the 20th century, after an epidemic of typhoid found in the groundwater supply. The City of Eugene condemned Eugene's private water utility and began treating river water (first the Willamette; later the McKenzie) for domestic use. EWEB got into the electric business when power was needed for the water pumps. Excess electricity generated by the EWEB's hydropower plants was used for street lighting.
Natural gas service is provided by NW Natural.
Wastewater treatment services are provided by the Metropolitan Wastewater Management Commission, a partnership between the Cities of Eugene and Springfield and Lane County.
Healthcare.
Three hospitals serve the Eugene-Springfield area. Sacred Heart Medical Center University District is the only one within Eugene city limits. McKenzie-Willamette Medical Center and Sacred Heart Medical Center at RiverBend are in Springfield. Oregon Medical Group, a primary care based multi-specialty group, operates several clinics in Eugene, as does PeaceHealth Medical Group. White Bird Clinic provides a broad range of health and human services, including low-cost clinics. The Volunteers in Medicine Clinic provides free medical and mental care to low-income adults without health insurance.
Notable people.
Notable athletes from Eugene include football players such as Todd Christensen, Quintin Mikell, Kailee Wong, Alex Brink,and Chris Miller. Basketball players have included Danny Ainge and Luke Jackson, while runners include Mary Decker, Marla Runyan, Alberto Salazar, and Steve Prefontaine. Decathlete Ashton Eaton competes for the Oregon Track Club Elite team based in Eugene.
Politicians from Eugene include U.S. Senators Wayne Morse and Paul Martin Simon, as well as Congressmen Jim Weaver and Peter DeFazio. Also born in Eugene was Joan Mondale, wife of Vice President Walter Mondale. Actors of note include Jenny Wade, Howard Hesseman and David Ogden Stiers, while Bryce Zabel chaired the Academy of Television Arts & Sciences. Author Ken Kesey also called the city home, as did Nike co-founder Phil Knight. Other past residents include Eugene Lazowski who saved 8,000 during World War II, New Orleans Saints general manager Mickey Loomis, and astronaut Stanley G. Love, among others. Later in his life L. Ron Hubbard, author and founder of The Church of Scientology, lived in Eugene.
Singer-songwriter Tim Hardin, known for "If I Were a Carpenter" and "Reason to Believe", was born and raised in Eugene. Singer-songwriter Mat Kearney is also from Eugene.
Sister cities.
Eugene has four sister cities:

</doc>
